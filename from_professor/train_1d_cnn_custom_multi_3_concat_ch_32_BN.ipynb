{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path as path\n",
    "import itertools\n",
    "from sklearn.preprocessing import maxabs_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Input, Dense, Activation, BatchNormalization, \\\n",
    "                                    Flatten, Conv1D, MaxPooling1D, Dropout, \\\n",
    "                                    Concatenate, GlobalMaxPool1D, GlobalAvgPool1D\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler, \\\n",
    "                                        EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = path.join('..', 'data')\n",
    "data_dir = path.join(base_dir, 'data_speech_commands_v0.02')\n",
    " \n",
    "train_txt = path.join(data_dir, 'wav_train_16words.txt')\n",
    "val_txt = path.join(data_dir, 'wav_validation_16words.txt')\n",
    "test_txt = path.join(data_dir, 'wav_test_16words.txt')\n",
    "\n",
    "train_data = np.load(path.join(data_dir, 'wav_train_data.npz'))\n",
    "val_data = np.load(path.join(data_dir, 'wav_validation_data.npz'))\n",
    "test_data = np.load(path.join(data_dir, 'wav_test_data.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36805, 16000, 1),\n",
       " (36805,),\n",
       " (4293, 16000, 1),\n",
       " (4293,),\n",
       " (4815, 16000, 1),\n",
       " (4815,),\n",
       " (16, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = train_data['x_train']\n",
    "y_train = train_data['y_train']\n",
    "x_val = val_data['x_val']\n",
    "y_val = val_data['y_val']\n",
    "x_test = test_data['x_test']\n",
    "y_test = test_data['y_test']\n",
    "y_table = test_data['table']\n",
    "\n",
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape, y_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_test[0].shape\n",
    "output_size = y_table.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_abs = np.asarray([maxabs_scale(wav) for wav in x_train])\n",
    "y_train_onehot = np.asarray([to_categorical(label, output_size) for label in y_train])\n",
    "del x_train, y_train\n",
    "\n",
    "x_val_abs = np.asarray([maxabs_scale(wav) for wav in x_val])\n",
    "y_val_onehot = np.asarray([to_categorical(label, output_size) for label in y_val])\n",
    "del x_val, y_val\n",
    "\n",
    "x_test_abs = np.asarray([maxabs_scale(wav) for wav in x_test])\n",
    "y_test_onehot = np.asarray([to_categorical(label, output_size) for label in y_test])\n",
    "del x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn(conv_num=1):\n",
    "    filter_size = 32\n",
    "\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    x = input_layer\n",
    "\n",
    "    layer_outputs = []\n",
    "    for i in range(conv_num):\n",
    "        x = Conv1D (kernel_size=5, filters=filter_size*(2**(i//4)), \n",
    "                          strides=1, padding='same')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('relu')(x)\n",
    "        x = MaxPooling1D(pool_size=3, strides=3)(x)\n",
    "        layer_outputs.append(x)    \n",
    "    \n",
    "    x = Concatenate()([Flatten()(output) for output in layer_outputs[-3:]])\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(output_size, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 16000, 32)    192         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1 (BatchNo (None, 16000, 32)    128         conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 16000, 32)    0           batch_normalization_v1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D)    (None, 5333, 32)     0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 5333, 32)     5152        max_pooling1d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_1 (Batch (None, 5333, 32)     128         conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 5333, 32)     0           batch_normalization_v1_1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 1777, 32)     0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 1777, 32)     5152        max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_2 (Batch (None, 1777, 32)     128         conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 1777, 32)     0           batch_normalization_v1_2[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 592, 32)      0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 170656)       0           max_pooling1d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 56864)        0           max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 18944)        0           max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 246464)       0           flatten[0][0]                    \n",
      "                                                                 flatten_1[0][0]                  \n",
      "                                                                 flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_3 (Batch (None, 246464)       985856      concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 16)           3943440     batch_normalization_v1_3[0][0]   \n",
      "==================================================================================================\n",
      "Total params: 4,940,176\n",
      "Trainable params: 4,447,056\n",
      "Non-trainable params: 493,120\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 16000, 32)    192         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_4 (Batch (None, 16000, 32)    128         conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 16000, 32)    0           batch_normalization_v1_4[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 5333, 32)     0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 5333, 32)     5152        max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_5 (Batch (None, 5333, 32)     128         conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 5333, 32)     0           batch_normalization_v1_5[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 1777, 32)     0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 1777, 32)     5152        max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_6 (Batch (None, 1777, 32)     128         conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 1777, 32)     0           batch_normalization_v1_6[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 592, 32)      0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 592, 32)      5152        max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_7 (Batch (None, 592, 32)      128         conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 592, 32)      0           batch_normalization_v1_7[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1D)  (None, 197, 32)      0           activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 56864)        0           max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 18944)        0           max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, 6304)         0           max_pooling1d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 82112)        0           flatten_3[0][0]                  \n",
      "                                                                 flatten_4[0][0]                  \n",
      "                                                                 flatten_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_8 (Batch (None, 82112)        328448      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 16)           1313808     batch_normalization_v1_8[0][0]   \n",
      "==================================================================================================\n",
      "Total params: 1,658,416\n",
      "Trainable params: 1,493,936\n",
      "Non-trainable params: 164,480\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 16000, 32)    192         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_9 (Batch (None, 16000, 32)    128         conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 16000, 32)    0           batch_normalization_v1_9[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1D)  (None, 5333, 32)     0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 5333, 32)     5152        max_pooling1d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_10 (Batc (None, 5333, 32)     128         conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 5333, 32)     0           batch_normalization_v1_10[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1D)  (None, 1777, 32)     0           activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 1777, 32)     5152        max_pooling1d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_11 (Batc (None, 1777, 32)     128         conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 1777, 32)     0           batch_normalization_v1_11[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1D)  (None, 592, 32)      0           activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_12 (Batc (None, 592, 32)      128         conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 592, 32)      0           batch_normalization_v1_12[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling1D) (None, 197, 32)      0           activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_13 (Batc (None, 197, 64)      256         conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 197, 64)      0           batch_normalization_v1_13[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling1D) (None, 65, 64)       0           activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_6 (Flatten)             (None, 18944)        0           max_pooling1d_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_7 (Flatten)             (None, 6304)         0           max_pooling1d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_8 (Flatten)             (None, 4160)         0           max_pooling1d_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 29408)        0           flatten_6[0][0]                  \n",
      "                                                                 flatten_7[0][0]                  \n",
      "                                                                 flatten_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_14 (Batc (None, 29408)        117632      concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 16)           470544      batch_normalization_v1_14[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 614,896\n",
      "Trainable params: 555,696\n",
      "Non-trainable params: 59,200\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 16000, 32)    192         input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_15 (Batc (None, 16000, 32)    128         conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_15[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling1D) (None, 5333, 32)     0           activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_16 (Batc (None, 5333, 32)     128         conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_16[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling1D) (None, 1777, 32)     0           activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_13[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_17 (Batc (None, 1777, 32)     128         conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_17[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling1D) (None, 592, 32)      0           activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_14[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_18 (Batc (None, 592, 32)      128         conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 592, 32)      0           batch_normalization_v1_18[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling1D) (None, 197, 32)      0           activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_15[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_19 (Batc (None, 197, 64)      256         conv1d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 197, 64)      0           batch_normalization_v1_19[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling1D) (None, 65, 64)       0           activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_17 (Conv1D)              (None, 65, 64)       20544       max_pooling1d_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_20 (Batc (None, 65, 64)       256         conv1d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 65, 64)       0           batch_normalization_v1_20[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling1D) (None, 21, 64)       0           activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_9 (Flatten)             (None, 6304)         0           max_pooling1d_15[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_10 (Flatten)            (None, 4160)         0           max_pooling1d_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_11 (Flatten)            (None, 1344)         0           max_pooling1d_17[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 11808)        0           flatten_9[0][0]                  \n",
      "                                                                 flatten_10[0][0]                 \n",
      "                                                                 flatten_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_21 (Batc (None, 11808)        47232       concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 16)           188944      batch_normalization_v1_21[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 283,696\n",
      "Trainable params: 259,568\n",
      "Non-trainable params: 24,128\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_18 (Conv1D)              (None, 16000, 32)    192         input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_22 (Batc (None, 16000, 32)    128         conv1d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_22[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling1D) (None, 5333, 32)     0           activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_19 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_18[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_23 (Batc (None, 5333, 32)     128         conv1d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_23[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling1D) (None, 1777, 32)     0           activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_20 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_19[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_24 (Batc (None, 1777, 32)     128         conv1d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_24[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling1D) (None, 592, 32)      0           activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_21 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_20[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_25 (Batc (None, 592, 32)      128         conv1d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 592, 32)      0           batch_normalization_v1_25[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling1D) (None, 197, 32)      0           activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_22 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_21[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_26 (Batc (None, 197, 64)      256         conv1d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 197, 64)      0           batch_normalization_v1_26[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling1D) (None, 65, 64)       0           activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_23 (Conv1D)              (None, 65, 64)       20544       max_pooling1d_22[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_27 (Batc (None, 65, 64)       256         conv1d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 65, 64)       0           batch_normalization_v1_27[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling1D) (None, 21, 64)       0           activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_24 (Conv1D)              (None, 21, 64)       20544       max_pooling1d_23[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_28 (Batc (None, 21, 64)       256         conv1d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 21, 64)       0           batch_normalization_v1_28[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling1D) (None, 7, 64)        0           activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_12 (Flatten)            (None, 4160)         0           max_pooling1d_22[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_13 (Flatten)            (None, 1344)         0           max_pooling1d_23[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_14 (Flatten)            (None, 448)          0           max_pooling1d_24[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 5952)         0           flatten_12[0][0]                 \n",
      "                                                                 flatten_13[0][0]                 \n",
      "                                                                 flatten_14[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_29 (Batc (None, 5952)         23808       concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 16)           95248       batch_normalization_v1_29[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 187,376\n",
      "Trainable params: 174,832\n",
      "Non-trainable params: 12,544\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_25 (Conv1D)              (None, 16000, 32)    192         input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_30 (Batc (None, 16000, 32)    128         conv1d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_30[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling1D) (None, 5333, 32)     0           activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_26 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_25[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_31 (Batc (None, 5333, 32)     128         conv1d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_31[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_26 (MaxPooling1D) (None, 1777, 32)     0           activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_27 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_26[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_32 (Batc (None, 1777, 32)     128         conv1d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_32[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_27 (MaxPooling1D) (None, 592, 32)      0           activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_28 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_27[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_33 (Batc (None, 592, 32)      128         conv1d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 592, 32)      0           batch_normalization_v1_33[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_28 (MaxPooling1D) (None, 197, 32)      0           activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_29 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_28[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_34 (Batc (None, 197, 64)      256         conv1d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 197, 64)      0           batch_normalization_v1_34[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_29 (MaxPooling1D) (None, 65, 64)       0           activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_30 (Conv1D)              (None, 65, 64)       20544       max_pooling1d_29[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_35 (Batc (None, 65, 64)       256         conv1d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 65, 64)       0           batch_normalization_v1_35[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_30 (MaxPooling1D) (None, 21, 64)       0           activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_31 (Conv1D)              (None, 21, 64)       20544       max_pooling1d_30[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_36 (Batc (None, 21, 64)       256         conv1d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 21, 64)       0           batch_normalization_v1_36[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_31 (MaxPooling1D) (None, 7, 64)        0           activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_32 (Conv1D)              (None, 7, 64)        20544       max_pooling1d_31[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_37 (Batc (None, 7, 64)        256         conv1d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 7, 64)        0           batch_normalization_v1_37[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_32 (MaxPooling1D) (None, 2, 64)        0           activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_15 (Flatten)            (None, 1344)         0           max_pooling1d_30[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_16 (Flatten)            (None, 448)          0           max_pooling1d_31[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_17 (Flatten)            (None, 128)          0           max_pooling1d_32[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 1920)         0           flatten_15[0][0]                 \n",
      "                                                                 flatten_16[0][0]                 \n",
      "                                                                 flatten_17[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_38 (Batc (None, 1920)         7680        concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 16)           30736       batch_normalization_v1_38[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 127,536\n",
      "Trainable params: 122,928\n",
      "Non-trainable params: 4,608\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in range(3, 9):\n",
    "    model = build_cnn(conv_num=i)\n",
    "    model.summary()\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36805 samples, validate on 4293 samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 4.0107 - acc: 0.3532\n",
      "Epoch 00001: val_loss improved from inf to 3.46988, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_BN_3_conv_checkpoint/001-3.4699.hdf5\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 4.0110 - acc: 0.3532 - val_loss: 3.4699 - val_acc: 0.3897\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.5526 - acc: 0.5894\n",
      "Epoch 00002: val_loss improved from 3.46988 to 3.42486, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_BN_3_conv_checkpoint/002-3.4249.hdf5\n",
      "36805/36805 [==============================] - 34s 913us/sample - loss: 2.5531 - acc: 0.5893 - val_loss: 3.4249 - val_acc: 0.4927\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.8941 - acc: 0.6879\n",
      "Epoch 00003: val_loss did not improve from 3.42486\n",
      "36805/36805 [==============================] - 34s 919us/sample - loss: 1.8940 - acc: 0.6880 - val_loss: 3.6724 - val_acc: 0.4955\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4750 - acc: 0.7567\n",
      "Epoch 00004: val_loss did not improve from 3.42486\n",
      "36805/36805 [==============================] - 34s 921us/sample - loss: 1.4749 - acc: 0.7567 - val_loss: 3.6647 - val_acc: 0.5176\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1988 - acc: 0.7998\n",
      "Epoch 00005: val_loss did not improve from 3.42486\n",
      "36805/36805 [==============================] - 34s 920us/sample - loss: 1.1990 - acc: 0.7998 - val_loss: 3.8808 - val_acc: 0.5199\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0952 - acc: 0.8238\n",
      "Epoch 00006: val_loss did not improve from 3.42486\n",
      "36805/36805 [==============================] - 34s 921us/sample - loss: 1.0952 - acc: 0.8237 - val_loss: 4.1313 - val_acc: 0.5097\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9838 - acc: 0.8433\n",
      "Epoch 00007: val_loss did not improve from 3.42486\n",
      "36805/36805 [==============================] - 34s 914us/sample - loss: 0.9842 - acc: 0.8432 - val_loss: 4.0116 - val_acc: 0.5395\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9038 - acc: 0.8631\n",
      "Epoch 00008: val_loss did not improve from 3.42486\n",
      "36805/36805 [==============================] - 34s 915us/sample - loss: 0.9039 - acc: 0.8631 - val_loss: 4.6391 - val_acc: 0.5129\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8100 - acc: 0.8782\n",
      "Epoch 00009: val_loss did not improve from 3.42486\n",
      "36805/36805 [==============================] - 34s 915us/sample - loss: 0.8101 - acc: 0.8781 - val_loss: 4.5068 - val_acc: 0.5192\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7216 - acc: 0.8937\n",
      "Epoch 00010: val_loss did not improve from 3.42486\n",
      "36805/36805 [==============================] - 34s 919us/sample - loss: 0.7216 - acc: 0.8937 - val_loss: 4.2932 - val_acc: 0.5479\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7208 - acc: 0.8929\n",
      "Epoch 00011: val_loss did not improve from 3.42486\n",
      "36805/36805 [==============================] - 34s 913us/sample - loss: 0.7207 - acc: 0.8929 - val_loss: 4.2623 - val_acc: 0.5570\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7213 - acc: 0.8967\n",
      "Epoch 00012: val_loss did not improve from 3.42486\n",
      "36805/36805 [==============================] - 34s 920us/sample - loss: 0.7221 - acc: 0.8966 - val_loss: 4.2146 - val_acc: 0.5584\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7183 - acc: 0.9004\n",
      "Epoch 00013: val_loss did not improve from 3.42486\n",
      "36805/36805 [==============================] - 34s 914us/sample - loss: 0.7187 - acc: 0.9003 - val_loss: 4.4419 - val_acc: 0.5639\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6771 - acc: 0.9097\n",
      "Epoch 00014: val_loss did not improve from 3.42486\n",
      "36805/36805 [==============================] - 34s 918us/sample - loss: 0.6774 - acc: 0.9097 - val_loss: 4.4237 - val_acc: 0.5618\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5769 - acc: 0.9237\n",
      "Epoch 00015: val_loss did not improve from 3.42486\n",
      "36805/36805 [==============================] - 34s 919us/sample - loss: 0.5776 - acc: 0.9237 - val_loss: 4.6875 - val_acc: 0.5439\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6217 - acc: 0.9196\n",
      "Epoch 00016: val_loss did not improve from 3.42486\n",
      "36805/36805 [==============================] - 34s 915us/sample - loss: 0.6221 - acc: 0.9196 - val_loss: 4.6096 - val_acc: 0.5514\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5710 - acc: 0.9252\n",
      "Epoch 00017: val_loss did not improve from 3.42486\n",
      "36805/36805 [==============================] - 34s 919us/sample - loss: 0.5713 - acc: 0.9251 - val_loss: 4.9163 - val_acc: 0.5460\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5594 - acc: 0.9284\n",
      "Epoch 00018: val_loss did not improve from 3.42486\n",
      "36805/36805 [==============================] - 34s 916us/sample - loss: 0.5603 - acc: 0.9283 - val_loss: 4.5508 - val_acc: 0.5782\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5575 - acc: 0.9288\n",
      "Epoch 00019: val_loss did not improve from 3.42486\n",
      "36805/36805 [==============================] - 34s 916us/sample - loss: 0.5575 - acc: 0.9288 - val_loss: 5.1621 - val_acc: 0.5241\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5047 - acc: 0.9373\n",
      "Epoch 00020: val_loss did not improve from 3.42486\n",
      "36805/36805 [==============================] - 34s 921us/sample - loss: 0.5046 - acc: 0.9373 - val_loss: 4.7699 - val_acc: 0.5639\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5157 - acc: 0.9380\n",
      "Epoch 00021: val_loss did not improve from 3.42486\n",
      "36805/36805 [==============================] - 34s 919us/sample - loss: 0.5162 - acc: 0.9379 - val_loss: 4.6591 - val_acc: 0.5786\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4980 - acc: 0.9398\n",
      "Epoch 00022: val_loss did not improve from 3.42486\n",
      "36805/36805 [==============================] - 34s 920us/sample - loss: 0.4984 - acc: 0.9398 - val_loss: 4.4829 - val_acc: 0.5861\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5254 - acc: 0.9371\n",
      "Epoch 00023: val_loss did not improve from 3.42486\n",
      "36805/36805 [==============================] - 34s 916us/sample - loss: 0.5254 - acc: 0.9371 - val_loss: 4.6620 - val_acc: 0.5765\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4672 - acc: 0.9447\n",
      "Epoch 00024: val_loss did not improve from 3.42486\n",
      "36805/36805 [==============================] - 34s 920us/sample - loss: 0.4678 - acc: 0.9447 - val_loss: 5.2140 - val_acc: 0.5511\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4649 - acc: 0.9449\n",
      "Epoch 00025: val_loss did not improve from 3.42486\n",
      "36805/36805 [==============================] - 34s 916us/sample - loss: 0.4648 - acc: 0.9449 - val_loss: 4.5420 - val_acc: 0.5954\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4504 - acc: 0.9485\n",
      "Epoch 00026: val_loss did not improve from 3.42486\n",
      "36805/36805 [==============================] - 34s 916us/sample - loss: 0.4504 - acc: 0.9485 - val_loss: 4.7021 - val_acc: 0.5912\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4557 - acc: 0.9480\n",
      "Epoch 00027: val_loss did not improve from 3.42486\n",
      "36805/36805 [==============================] - 34s 920us/sample - loss: 0.4568 - acc: 0.9479 - val_loss: 4.9287 - val_acc: 0.5726\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5185 - acc: 0.9400\n",
      "Epoch 00028: val_loss did not improve from 3.42486\n",
      "36805/36805 [==============================] - 34s 919us/sample - loss: 0.5188 - acc: 0.9400 - val_loss: 4.8062 - val_acc: 0.5833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4109 - acc: 0.9540\n",
      "Epoch 00029: val_loss did not improve from 3.42486\n",
      "36805/36805 [==============================] - 34s 921us/sample - loss: 0.4114 - acc: 0.9539 - val_loss: 4.7619 - val_acc: 0.5891\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4361 - acc: 0.9501\n",
      "Epoch 00030: val_loss did not improve from 3.42486\n",
      "36805/36805 [==============================] - 34s 919us/sample - loss: 0.4361 - acc: 0.9500 - val_loss: 4.8155 - val_acc: 0.5882\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3858 - acc: 0.9569\n",
      "Epoch 00031: val_loss did not improve from 3.42486\n",
      "36805/36805 [==============================] - 34s 917us/sample - loss: 0.3857 - acc: 0.9569 - val_loss: 5.1063 - val_acc: 0.5672\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3918 - acc: 0.9568\n",
      "Epoch 00032: val_loss did not improve from 3.42486\n",
      "36805/36805 [==============================] - 34s 920us/sample - loss: 0.3922 - acc: 0.9568 - val_loss: 4.9142 - val_acc: 0.5828\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3922 - acc: 0.9566\n",
      "Epoch 00033: val_loss did not improve from 3.42486\n",
      "36805/36805 [==============================] - 34s 920us/sample - loss: 0.3926 - acc: 0.9565 - val_loss: 5.1742 - val_acc: 0.5644\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4578 - acc: 0.9505\n",
      "Epoch 00034: val_loss did not improve from 3.42486\n",
      "36805/36805 [==============================] - 34s 918us/sample - loss: 0.4594 - acc: 0.9504 - val_loss: 5.1869 - val_acc: 0.5681\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4080 - acc: 0.9544\n",
      "Epoch 00035: val_loss did not improve from 3.42486\n",
      "36805/36805 [==============================] - 34s 916us/sample - loss: 0.4083 - acc: 0.9544 - val_loss: 4.9991 - val_acc: 0.5830\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3761 - acc: 0.9602\n",
      "Epoch 00036: val_loss did not improve from 3.42486\n",
      "36805/36805 [==============================] - 34s 918us/sample - loss: 0.3769 - acc: 0.9602 - val_loss: 4.9137 - val_acc: 0.5851\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3840 - acc: 0.9587\n",
      "Epoch 00037: val_loss did not improve from 3.42486\n",
      "36805/36805 [==============================] - 34s 917us/sample - loss: 0.3852 - acc: 0.9586 - val_loss: 5.4131 - val_acc: 0.5618\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3891 - acc: 0.9600\n",
      "Epoch 00038: val_loss did not improve from 3.42486\n",
      "36805/36805 [==============================] - 34s 914us/sample - loss: 0.3900 - acc: 0.9600 - val_loss: 5.3415 - val_acc: 0.5590\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4230 - acc: 0.9552\n",
      "Epoch 00039: val_loss did not improve from 3.42486\n",
      "36805/36805 [==============================] - 34s 919us/sample - loss: 0.4230 - acc: 0.9551 - val_loss: 4.9231 - val_acc: 0.5914\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3568 - acc: 0.9617\n",
      "Epoch 00040: val_loss did not improve from 3.42486\n",
      "36805/36805 [==============================] - 34s 914us/sample - loss: 0.3570 - acc: 0.9616 - val_loss: 5.7372 - val_acc: 0.5481\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3701 - acc: 0.9623\n",
      "Epoch 00041: val_loss did not improve from 3.42486\n",
      "36805/36805 [==============================] - 34s 919us/sample - loss: 0.3709 - acc: 0.9623 - val_loss: 4.9871 - val_acc: 0.5945\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3801 - acc: 0.9614\n",
      "Epoch 00042: val_loss did not improve from 3.42486\n",
      "36805/36805 [==============================] - 34s 916us/sample - loss: 0.3805 - acc: 0.9614 - val_loss: 4.9114 - val_acc: 0.5919\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3252 - acc: 0.9671\n",
      "Epoch 00043: val_loss did not improve from 3.42486\n",
      "36805/36805 [==============================] - 34s 918us/sample - loss: 0.3252 - acc: 0.9671 - val_loss: 5.1586 - val_acc: 0.5758\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3535 - acc: 0.9637\n",
      "Epoch 00044: val_loss did not improve from 3.42486\n",
      "36805/36805 [==============================] - 34s 915us/sample - loss: 0.3539 - acc: 0.9637 - val_loss: 4.8634 - val_acc: 0.6042\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3370 - acc: 0.9660\n",
      "Epoch 00045: val_loss did not improve from 3.42486\n",
      "36805/36805 [==============================] - 34s 918us/sample - loss: 0.3372 - acc: 0.9660 - val_loss: 5.1071 - val_acc: 0.5889\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3672 - acc: 0.9625\n",
      "Epoch 00046: val_loss did not improve from 3.42486\n",
      "36805/36805 [==============================] - 34s 920us/sample - loss: 0.3671 - acc: 0.9625 - val_loss: 4.9907 - val_acc: 0.5952\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3368 - acc: 0.9661\n",
      "Epoch 00047: val_loss did not improve from 3.42486\n",
      "36805/36805 [==============================] - 34s 919us/sample - loss: 0.3369 - acc: 0.9660 - val_loss: 5.3495 - val_acc: 0.5772\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3430 - acc: 0.9648\n",
      "Epoch 00048: val_loss did not improve from 3.42486\n",
      "36805/36805 [==============================] - 34s 919us/sample - loss: 0.3434 - acc: 0.9648 - val_loss: 4.7879 - val_acc: 0.6112\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3740 - acc: 0.9633\n",
      "Epoch 00049: val_loss did not improve from 3.42486\n",
      "36805/36805 [==============================] - 34s 916us/sample - loss: 0.3739 - acc: 0.9633 - val_loss: 5.5390 - val_acc: 0.5735\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3128 - acc: 0.9690\n",
      "Epoch 00050: val_loss did not improve from 3.42486\n",
      "36805/36805 [==============================] - 34s 918us/sample - loss: 0.3136 - acc: 0.9690 - val_loss: 5.1439 - val_acc: 0.5900\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3237 - acc: 0.9681\n",
      "Epoch 00051: val_loss did not improve from 3.42486\n",
      "36805/36805 [==============================] - 34s 919us/sample - loss: 0.3241 - acc: 0.9680 - val_loss: 5.7392 - val_acc: 0.5532\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3566 - acc: 0.9641\n",
      "Epoch 00052: val_loss did not improve from 3.42486\n",
      "36805/36805 [==============================] - 34s 915us/sample - loss: 0.3568 - acc: 0.9641 - val_loss: 5.1563 - val_acc: 0.5949\n",
      "\n",
      "1D_CNN_custom_multi_3_concat_ch_32_BN_3_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 428us/sample - loss: 3.6859 - acc: 0.4590\n",
      "Loss: 3.68593449231125 Accuracy: 0.45898235\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.1548 - acc: 0.4332\n",
      "Epoch 00001: val_loss improved from inf to 1.92991, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_BN_4_conv_checkpoint/001-1.9299.hdf5\n",
      "36805/36805 [==============================] - 35s 946us/sample - loss: 2.1548 - acc: 0.4332 - val_loss: 1.9299 - val_acc: 0.4326\n",
      "Epoch 2/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1042 - acc: 0.6867\n",
      "Epoch 00002: val_loss improved from 1.92991 to 1.64474, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_BN_4_conv_checkpoint/002-1.6447.hdf5\n",
      "36805/36805 [==============================] - 30s 825us/sample - loss: 1.1041 - acc: 0.6867 - val_loss: 1.6447 - val_acc: 0.5758\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6992 - acc: 0.7935\n",
      "Epoch 00003: val_loss did not improve from 1.64474\n",
      "36805/36805 [==============================] - 30s 821us/sample - loss: 0.6992 - acc: 0.7935 - val_loss: 1.7633 - val_acc: 0.5716\n",
      "Epoch 4/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4568 - acc: 0.8663\n",
      "Epoch 00004: val_loss improved from 1.64474 to 1.61935, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_BN_4_conv_checkpoint/004-1.6193.hdf5\n",
      "36805/36805 [==============================] - 30s 824us/sample - loss: 0.4568 - acc: 0.8662 - val_loss: 1.6193 - val_acc: 0.6229\n",
      "Epoch 5/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3025 - acc: 0.9158\n",
      "Epoch 00005: val_loss did not improve from 1.61935\n",
      "36805/36805 [==============================] - 30s 820us/sample - loss: 0.3031 - acc: 0.9157 - val_loss: 2.0054 - val_acc: 0.5609\n",
      "Epoch 6/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2658 - acc: 0.9257\n",
      "Epoch 00006: val_loss did not improve from 1.61935\n",
      "36805/36805 [==============================] - 30s 821us/sample - loss: 0.2661 - acc: 0.9256 - val_loss: 1.8535 - val_acc: 0.5921\n",
      "Epoch 7/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1920 - acc: 0.9504\n",
      "Epoch 00007: val_loss did not improve from 1.61935\n",
      "36805/36805 [==============================] - 30s 821us/sample - loss: 0.1925 - acc: 0.9503 - val_loss: 1.8715 - val_acc: 0.6122\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1952 - acc: 0.9510\n",
      "Epoch 00008: val_loss did not improve from 1.61935\n",
      "36805/36805 [==============================] - 30s 819us/sample - loss: 0.1953 - acc: 0.9509 - val_loss: 2.0927 - val_acc: 0.5807\n",
      "Epoch 9/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1561 - acc: 0.9595\n",
      "Epoch 00009: val_loss did not improve from 1.61935\n",
      "36805/36805 [==============================] - 30s 819us/sample - loss: 0.1564 - acc: 0.9594 - val_loss: 1.9878 - val_acc: 0.6103\n",
      "Epoch 10/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1413 - acc: 0.9624\n",
      "Epoch 00010: val_loss did not improve from 1.61935\n",
      "36805/36805 [==============================] - 30s 819us/sample - loss: 0.1414 - acc: 0.9623 - val_loss: 2.0298 - val_acc: 0.6091\n",
      "Epoch 11/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1316 - acc: 0.9674\n",
      "Epoch 00011: val_loss did not improve from 1.61935\n",
      "36805/36805 [==============================] - 30s 821us/sample - loss: 0.1317 - acc: 0.9673 - val_loss: 2.3075 - val_acc: 0.5775\n",
      "Epoch 12/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1032 - acc: 0.9755\n",
      "Epoch 00012: val_loss did not improve from 1.61935\n",
      "36805/36805 [==============================] - 30s 821us/sample - loss: 0.1038 - acc: 0.9754 - val_loss: 1.9646 - val_acc: 0.6334\n",
      "Epoch 13/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0952 - acc: 0.9774\n",
      "Epoch 00013: val_loss did not improve from 1.61935\n",
      "36805/36805 [==============================] - 30s 818us/sample - loss: 0.0952 - acc: 0.9773 - val_loss: 2.4649 - val_acc: 0.5812\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0989 - acc: 0.9752\n",
      "Epoch 00014: val_loss did not improve from 1.61935\n",
      "36805/36805 [==============================] - 30s 821us/sample - loss: 0.0989 - acc: 0.9752 - val_loss: 2.4787 - val_acc: 0.5884\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0886 - acc: 0.9782\n",
      "Epoch 00015: val_loss did not improve from 1.61935\n",
      "36805/36805 [==============================] - 30s 818us/sample - loss: 0.0891 - acc: 0.9782 - val_loss: 2.4521 - val_acc: 0.6054\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1046 - acc: 0.9740\n",
      "Epoch 00016: val_loss did not improve from 1.61935\n",
      "36805/36805 [==============================] - 30s 817us/sample - loss: 0.1052 - acc: 0.9740 - val_loss: 2.6180 - val_acc: 0.5775\n",
      "Epoch 17/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1024 - acc: 0.9742\n",
      "Epoch 00017: val_loss did not improve from 1.61935\n",
      "36805/36805 [==============================] - 30s 821us/sample - loss: 0.1023 - acc: 0.9742 - val_loss: 2.5170 - val_acc: 0.5928\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0715 - acc: 0.9838\n",
      "Epoch 00018: val_loss did not improve from 1.61935\n",
      "36805/36805 [==============================] - 30s 820us/sample - loss: 0.0715 - acc: 0.9838 - val_loss: 2.4705 - val_acc: 0.6198\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0644 - acc: 0.9850\n",
      "Epoch 00019: val_loss did not improve from 1.61935\n",
      "36805/36805 [==============================] - 30s 818us/sample - loss: 0.0644 - acc: 0.9849 - val_loss: 2.9242 - val_acc: 0.5607\n",
      "Epoch 20/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0841 - acc: 0.9802\n",
      "Epoch 00020: val_loss did not improve from 1.61935\n",
      "36805/36805 [==============================] - 30s 820us/sample - loss: 0.0840 - acc: 0.9801 - val_loss: 2.3753 - val_acc: 0.6210\n",
      "Epoch 21/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0706 - acc: 0.9830\n",
      "Epoch 00021: val_loss did not improve from 1.61935\n",
      "36805/36805 [==============================] - 30s 819us/sample - loss: 0.0711 - acc: 0.9828 - val_loss: 2.5068 - val_acc: 0.6152\n",
      "Epoch 22/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0856 - acc: 0.9785\n",
      "Epoch 00022: val_loss did not improve from 1.61935\n",
      "36805/36805 [==============================] - 30s 823us/sample - loss: 0.0856 - acc: 0.9785 - val_loss: 2.7108 - val_acc: 0.6021\n",
      "Epoch 23/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0475 - acc: 0.9898\n",
      "Epoch 00023: val_loss did not improve from 1.61935\n",
      "36805/36805 [==============================] - 30s 819us/sample - loss: 0.0475 - acc: 0.9898 - val_loss: 2.6500 - val_acc: 0.6129\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0660 - acc: 0.9844\n",
      "Epoch 00024: val_loss did not improve from 1.61935\n",
      "36805/36805 [==============================] - 30s 816us/sample - loss: 0.0664 - acc: 0.9844 - val_loss: 2.8151 - val_acc: 0.6068\n",
      "Epoch 25/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0744 - acc: 0.9823\n",
      "Epoch 00025: val_loss did not improve from 1.61935\n",
      "36805/36805 [==============================] - 30s 819us/sample - loss: 0.0746 - acc: 0.9823 - val_loss: 2.5340 - val_acc: 0.6338\n",
      "Epoch 26/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0659 - acc: 0.9846\n",
      "Epoch 00026: val_loss did not improve from 1.61935\n",
      "36805/36805 [==============================] - 30s 820us/sample - loss: 0.0659 - acc: 0.9846 - val_loss: 3.2303 - val_acc: 0.5814\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0741 - acc: 0.9829\n",
      "Epoch 00027: val_loss did not improve from 1.61935\n",
      "36805/36805 [==============================] - 30s 820us/sample - loss: 0.0742 - acc: 0.9829 - val_loss: 2.7985 - val_acc: 0.6136\n",
      "Epoch 28/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0697 - acc: 0.9843\n",
      "Epoch 00028: val_loss did not improve from 1.61935\n",
      "36805/36805 [==============================] - 30s 820us/sample - loss: 0.0697 - acc: 0.9843 - val_loss: 2.7858 - val_acc: 0.6210\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0553 - acc: 0.9869\n",
      "Epoch 00029: val_loss did not improve from 1.61935\n",
      "36805/36805 [==============================] - 30s 819us/sample - loss: 0.0561 - acc: 0.9868 - val_loss: 2.7339 - val_acc: 0.6259\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0770 - acc: 0.9812\n",
      "Epoch 00030: val_loss did not improve from 1.61935\n",
      "36805/36805 [==============================] - 30s 823us/sample - loss: 0.0770 - acc: 0.9813 - val_loss: 2.8666 - val_acc: 0.6045\n",
      "Epoch 31/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0507 - acc: 0.9890\n",
      "Epoch 00031: val_loss did not improve from 1.61935\n",
      "36805/36805 [==============================] - 30s 820us/sample - loss: 0.0515 - acc: 0.9889 - val_loss: 2.9510 - val_acc: 0.6140\n",
      "Epoch 32/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0721 - acc: 0.9842\n",
      "Epoch 00032: val_loss did not improve from 1.61935\n",
      "36805/36805 [==============================] - 30s 819us/sample - loss: 0.0721 - acc: 0.9842 - val_loss: 2.8683 - val_acc: 0.6208\n",
      "Epoch 33/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0576 - acc: 0.9867\n",
      "Epoch 00033: val_loss did not improve from 1.61935\n",
      "36805/36805 [==============================] - 30s 818us/sample - loss: 0.0580 - acc: 0.9867 - val_loss: 3.0386 - val_acc: 0.6164\n",
      "Epoch 34/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0725 - acc: 0.9838\n",
      "Epoch 00034: val_loss did not improve from 1.61935\n",
      "36805/36805 [==============================] - 30s 822us/sample - loss: 0.0725 - acc: 0.9838 - val_loss: 3.3542 - val_acc: 0.5896\n",
      "Epoch 35/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0445 - acc: 0.9903\n",
      "Epoch 00035: val_loss did not improve from 1.61935\n",
      "36805/36805 [==============================] - 30s 821us/sample - loss: 0.0445 - acc: 0.9903 - val_loss: 3.0714 - val_acc: 0.6038\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0495 - acc: 0.9891\n",
      "Epoch 00036: val_loss did not improve from 1.61935\n",
      "36805/36805 [==============================] - 30s 820us/sample - loss: 0.0497 - acc: 0.9891 - val_loss: 3.4776 - val_acc: 0.5646\n",
      "Epoch 37/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0519 - acc: 0.9886\n",
      "Epoch 00037: val_loss did not improve from 1.61935\n",
      "36805/36805 [==============================] - 30s 818us/sample - loss: 0.0519 - acc: 0.9886 - val_loss: 3.1487 - val_acc: 0.6077\n",
      "Epoch 38/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0497 - acc: 0.9883\n",
      "Epoch 00038: val_loss did not improve from 1.61935\n",
      "36805/36805 [==============================] - 30s 822us/sample - loss: 0.0500 - acc: 0.9882 - val_loss: 3.0992 - val_acc: 0.6052\n",
      "Epoch 39/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0454 - acc: 0.9900\n",
      "Epoch 00039: val_loss did not improve from 1.61935\n",
      "36805/36805 [==============================] - 30s 817us/sample - loss: 0.0457 - acc: 0.9900 - val_loss: 2.9181 - val_acc: 0.6208\n",
      "Epoch 40/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0547 - acc: 0.9884\n",
      "Epoch 00040: val_loss did not improve from 1.61935\n",
      "36805/36805 [==============================] - 30s 820us/sample - loss: 0.0547 - acc: 0.9883 - val_loss: 2.9440 - val_acc: 0.6287\n",
      "Epoch 41/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0476 - acc: 0.9897\n",
      "Epoch 00041: val_loss did not improve from 1.61935\n",
      "36805/36805 [==============================] - 30s 820us/sample - loss: 0.0477 - acc: 0.9896 - val_loss: 3.1082 - val_acc: 0.6024\n",
      "Epoch 42/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0539 - acc: 0.9878\n",
      "Epoch 00042: val_loss did not improve from 1.61935\n",
      "36805/36805 [==============================] - 30s 818us/sample - loss: 0.0539 - acc: 0.9878 - val_loss: 3.0155 - val_acc: 0.6294\n",
      "Epoch 43/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0418 - acc: 0.9910\n",
      "Epoch 00043: val_loss did not improve from 1.61935\n",
      "36805/36805 [==============================] - 30s 819us/sample - loss: 0.0423 - acc: 0.9910 - val_loss: 2.9749 - val_acc: 0.6268\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0485 - acc: 0.9897\n",
      "Epoch 00044: val_loss did not improve from 1.61935\n",
      "36805/36805 [==============================] - 30s 820us/sample - loss: 0.0488 - acc: 0.9897 - val_loss: 3.1887 - val_acc: 0.6203\n",
      "Epoch 45/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0634 - acc: 0.9862\n",
      "Epoch 00045: val_loss did not improve from 1.61935\n",
      "36805/36805 [==============================] - 30s 815us/sample - loss: 0.0637 - acc: 0.9861 - val_loss: 2.9730 - val_acc: 0.6287\n",
      "Epoch 46/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0565 - acc: 0.9873\n",
      "Epoch 00046: val_loss did not improve from 1.61935\n",
      "36805/36805 [==============================] - 30s 820us/sample - loss: 0.0564 - acc: 0.9873 - val_loss: 3.1017 - val_acc: 0.6254\n",
      "Epoch 47/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0416 - acc: 0.9913\n",
      "Epoch 00047: val_loss did not improve from 1.61935\n",
      "36805/36805 [==============================] - 30s 820us/sample - loss: 0.0420 - acc: 0.9913 - val_loss: 3.1723 - val_acc: 0.6108\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0438 - acc: 0.9913\n",
      "Epoch 00048: val_loss did not improve from 1.61935\n",
      "36805/36805 [==============================] - 30s 818us/sample - loss: 0.0438 - acc: 0.9913 - val_loss: 3.1580 - val_acc: 0.6166\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0476 - acc: 0.9909\n",
      "Epoch 00049: val_loss did not improve from 1.61935\n",
      "36805/36805 [==============================] - 30s 824us/sample - loss: 0.0478 - acc: 0.9909 - val_loss: 3.1403 - val_acc: 0.6171\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0455 - acc: 0.9905\n",
      "Epoch 00050: val_loss did not improve from 1.61935\n",
      "36805/36805 [==============================] - 30s 818us/sample - loss: 0.0455 - acc: 0.9905 - val_loss: 3.1800 - val_acc: 0.6257\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0441 - acc: 0.9915\n",
      "Epoch 00051: val_loss did not improve from 1.61935\n",
      "36805/36805 [==============================] - 30s 822us/sample - loss: 0.0442 - acc: 0.9915 - val_loss: 3.1392 - val_acc: 0.6231\n",
      "Epoch 52/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0446 - acc: 0.9912\n",
      "Epoch 00052: val_loss did not improve from 1.61935\n",
      "36805/36805 [==============================] - 30s 820us/sample - loss: 0.0446 - acc: 0.9911 - val_loss: 3.2228 - val_acc: 0.6196\n",
      "Epoch 53/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0422 - acc: 0.9916\n",
      "Epoch 00053: val_loss did not improve from 1.61935\n",
      "36805/36805 [==============================] - 30s 821us/sample - loss: 0.0427 - acc: 0.9916 - val_loss: 3.2521 - val_acc: 0.6175\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0479 - acc: 0.9898\n",
      "Epoch 00054: val_loss did not improve from 1.61935\n",
      "36805/36805 [==============================] - 30s 818us/sample - loss: 0.0480 - acc: 0.9898 - val_loss: 3.4409 - val_acc: 0.6077\n",
      "\n",
      "1D_CNN_custom_multi_3_concat_ch_32_BN_4_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 426us/sample - loss: 1.7336 - acc: 0.5811\n",
      "Loss: 1.7336489571217808 Accuracy: 0.5811007\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.8416 - acc: 0.4507\n",
      "Epoch 00001: val_loss improved from inf to 1.80991, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_BN_5_conv_checkpoint/001-1.8099.hdf5\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 1.8416 - acc: 0.4507 - val_loss: 1.8099 - val_acc: 0.4433\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1548 - acc: 0.6529\n",
      "Epoch 00002: val_loss improved from 1.80991 to 1.52176, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_BN_5_conv_checkpoint/002-1.5218.hdf5\n",
      "36805/36805 [==============================] - 31s 841us/sample - loss: 1.1549 - acc: 0.6528 - val_loss: 1.5218 - val_acc: 0.5448\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8595 - acc: 0.7406\n",
      "Epoch 00003: val_loss improved from 1.52176 to 1.17127, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_BN_5_conv_checkpoint/003-1.1713.hdf5\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.8597 - acc: 0.7406 - val_loss: 1.1713 - val_acc: 0.6611\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6685 - acc: 0.7996\n",
      "Epoch 00004: val_loss improved from 1.17127 to 1.15386, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_BN_5_conv_checkpoint/004-1.1539.hdf5\n",
      "36805/36805 [==============================] - 31s 844us/sample - loss: 0.6684 - acc: 0.7996 - val_loss: 1.1539 - val_acc: 0.6685\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5207 - acc: 0.8462\n",
      "Epoch 00005: val_loss improved from 1.15386 to 1.09791, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_BN_5_conv_checkpoint/005-1.0979.hdf5\n",
      "36805/36805 [==============================] - 31s 843us/sample - loss: 0.5207 - acc: 0.8462 - val_loss: 1.0979 - val_acc: 0.6909\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4130 - acc: 0.8812\n",
      "Epoch 00006: val_loss improved from 1.09791 to 1.02936, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_BN_5_conv_checkpoint/006-1.0294.hdf5\n",
      "36805/36805 [==============================] - 31s 836us/sample - loss: 0.4130 - acc: 0.8812 - val_loss: 1.0294 - val_acc: 0.7093\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3284 - acc: 0.9088\n",
      "Epoch 00007: val_loss improved from 1.02936 to 1.01736, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_BN_5_conv_checkpoint/007-1.0174.hdf5\n",
      "36805/36805 [==============================] - 31s 843us/sample - loss: 0.3285 - acc: 0.9087 - val_loss: 1.0174 - val_acc: 0.7186\n",
      "Epoch 8/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2697 - acc: 0.9278\n",
      "Epoch 00008: val_loss did not improve from 1.01736\n",
      "36805/36805 [==============================] - 31s 840us/sample - loss: 0.2700 - acc: 0.9278 - val_loss: 1.0612 - val_acc: 0.7133\n",
      "Epoch 9/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2242 - acc: 0.9436\n",
      "Epoch 00009: val_loss did not improve from 1.01736\n",
      "36805/36805 [==============================] - 31s 836us/sample - loss: 0.2243 - acc: 0.9436 - val_loss: 1.1751 - val_acc: 0.6837\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1916 - acc: 0.9540\n",
      "Epoch 00010: val_loss did not improve from 1.01736\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.1920 - acc: 0.9539 - val_loss: 1.1170 - val_acc: 0.7095\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1597 - acc: 0.9643\n",
      "Epoch 00011: val_loss did not improve from 1.01736\n",
      "36805/36805 [==============================] - 31s 840us/sample - loss: 0.1602 - acc: 0.9643 - val_loss: 1.0237 - val_acc: 0.7347\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1404 - acc: 0.9697\n",
      "Epoch 00012: val_loss did not improve from 1.01736\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.1405 - acc: 0.9697 - val_loss: 1.2010 - val_acc: 0.6951\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1114 - acc: 0.9774\n",
      "Epoch 00013: val_loss did not improve from 1.01736\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.1117 - acc: 0.9773 - val_loss: 1.1053 - val_acc: 0.7200\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1212 - acc: 0.9742\n",
      "Epoch 00014: val_loss did not improve from 1.01736\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.1212 - acc: 0.9741 - val_loss: 1.1107 - val_acc: 0.7286\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0827 - acc: 0.9864\n",
      "Epoch 00015: val_loss did not improve from 1.01736\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.0827 - acc: 0.9864 - val_loss: 1.1441 - val_acc: 0.7226\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0791 - acc: 0.9865\n",
      "Epoch 00016: val_loss did not improve from 1.01736\n",
      "36805/36805 [==============================] - 30s 826us/sample - loss: 0.0791 - acc: 0.9865 - val_loss: 1.1967 - val_acc: 0.7163\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0769 - acc: 0.9857\n",
      "Epoch 00017: val_loss did not improve from 1.01736\n",
      "36805/36805 [==============================] - 31s 835us/sample - loss: 0.0770 - acc: 0.9857 - val_loss: 1.1480 - val_acc: 0.7282\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0745 - acc: 0.9857\n",
      "Epoch 00018: val_loss did not improve from 1.01736\n",
      "36805/36805 [==============================] - 31s 833us/sample - loss: 0.0748 - acc: 0.9856 - val_loss: 1.3385 - val_acc: 0.6946\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0718 - acc: 0.9865\n",
      "Epoch 00019: val_loss did not improve from 1.01736\n",
      "36805/36805 [==============================] - 31s 829us/sample - loss: 0.0719 - acc: 0.9864 - val_loss: 1.1637 - val_acc: 0.7258\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0550 - acc: 0.9910\n",
      "Epoch 00020: val_loss did not improve from 1.01736\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.0550 - acc: 0.9910 - val_loss: 1.1780 - val_acc: 0.7356\n",
      "Epoch 21/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0572 - acc: 0.9903\n",
      "Epoch 00021: val_loss did not improve from 1.01736\n",
      "36805/36805 [==============================] - 31s 836us/sample - loss: 0.0572 - acc: 0.9903 - val_loss: 1.2081 - val_acc: 0.7256\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0727 - acc: 0.9854\n",
      "Epoch 00022: val_loss did not improve from 1.01736\n",
      "36805/36805 [==============================] - 31s 830us/sample - loss: 0.0727 - acc: 0.9854 - val_loss: 1.2531 - val_acc: 0.7265\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0432 - acc: 0.9937\n",
      "Epoch 00023: val_loss did not improve from 1.01736\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.0432 - acc: 0.9937 - val_loss: 1.2056 - val_acc: 0.7442\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0457 - acc: 0.9921\n",
      "Epoch 00024: val_loss did not improve from 1.01736\n",
      "36805/36805 [==============================] - 31s 829us/sample - loss: 0.0458 - acc: 0.9921 - val_loss: 1.2520 - val_acc: 0.7293\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0463 - acc: 0.9920\n",
      "Epoch 00025: val_loss did not improve from 1.01736\n",
      "36805/36805 [==============================] - 31s 840us/sample - loss: 0.0464 - acc: 0.9919 - val_loss: 1.3501 - val_acc: 0.7116\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0462 - acc: 0.9918\n",
      "Epoch 00026: val_loss did not improve from 1.01736\n",
      "36805/36805 [==============================] - 31s 841us/sample - loss: 0.0464 - acc: 0.9918 - val_loss: 1.3113 - val_acc: 0.7177\n",
      "Epoch 27/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0479 - acc: 0.9910\n",
      "Epoch 00027: val_loss did not improve from 1.01736\n",
      "36805/36805 [==============================] - 31s 843us/sample - loss: 0.0480 - acc: 0.9909 - val_loss: 1.2956 - val_acc: 0.7188\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0442 - acc: 0.9914\n",
      "Epoch 00028: val_loss did not improve from 1.01736\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.0442 - acc: 0.9914 - val_loss: 1.2875 - val_acc: 0.7331\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0333 - acc: 0.9945\n",
      "Epoch 00029: val_loss did not improve from 1.01736\n",
      "36805/36805 [==============================] - 31s 841us/sample - loss: 0.0334 - acc: 0.9945 - val_loss: 1.4937 - val_acc: 0.7046\n",
      "Epoch 30/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0361 - acc: 0.9940\n",
      "Epoch 00030: val_loss did not improve from 1.01736\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.0361 - acc: 0.9940 - val_loss: 1.5292 - val_acc: 0.7032\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0365 - acc: 0.9929\n",
      "Epoch 00031: val_loss did not improve from 1.01736\n",
      "36805/36805 [==============================] - 31s 841us/sample - loss: 0.0369 - acc: 0.9928 - val_loss: 1.3374 - val_acc: 0.7261\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0434 - acc: 0.9910\n",
      "Epoch 00032: val_loss did not improve from 1.01736\n",
      "36805/36805 [==============================] - 31s 842us/sample - loss: 0.0434 - acc: 0.9910 - val_loss: 1.4156 - val_acc: 0.7193\n",
      "Epoch 33/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0324 - acc: 0.9945\n",
      "Epoch 00033: val_loss did not improve from 1.01736\n",
      "36805/36805 [==============================] - 31s 843us/sample - loss: 0.0325 - acc: 0.9945 - val_loss: 1.3413 - val_acc: 0.7352\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0275 - acc: 0.9955\n",
      "Epoch 00034: val_loss did not improve from 1.01736\n",
      "36805/36805 [==============================] - 31s 841us/sample - loss: 0.0275 - acc: 0.9955 - val_loss: 1.4329 - val_acc: 0.7212\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0350 - acc: 0.9936\n",
      "Epoch 00035: val_loss did not improve from 1.01736\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.0351 - acc: 0.9936 - val_loss: 1.4673 - val_acc: 0.7156\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0365 - acc: 0.9928\n",
      "Epoch 00036: val_loss did not improve from 1.01736\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.0366 - acc: 0.9927 - val_loss: 1.4141 - val_acc: 0.7284\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0287 - acc: 0.9944\n",
      "Epoch 00037: val_loss did not improve from 1.01736\n",
      "36805/36805 [==============================] - 31s 835us/sample - loss: 0.0287 - acc: 0.9944 - val_loss: 1.3671 - val_acc: 0.7391\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0261 - acc: 0.9952\n",
      "Epoch 00038: val_loss did not improve from 1.01736\n",
      "36805/36805 [==============================] - 31s 833us/sample - loss: 0.0261 - acc: 0.9952 - val_loss: 1.4794 - val_acc: 0.7312\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0269 - acc: 0.9955\n",
      "Epoch 00039: val_loss did not improve from 1.01736\n",
      "36805/36805 [==============================] - 31s 833us/sample - loss: 0.0269 - acc: 0.9955 - val_loss: 1.4441 - val_acc: 0.7307\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0341 - acc: 0.9937\n",
      "Epoch 00040: val_loss did not improve from 1.01736\n",
      "36805/36805 [==============================] - 31s 833us/sample - loss: 0.0345 - acc: 0.9936 - val_loss: 1.4608 - val_acc: 0.7326\n",
      "Epoch 41/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0278 - acc: 0.9948\n",
      "Epoch 00041: val_loss did not improve from 1.01736\n",
      "36805/36805 [==============================] - 30s 828us/sample - loss: 0.0278 - acc: 0.9948 - val_loss: 1.4968 - val_acc: 0.7198\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0228 - acc: 0.9964\n",
      "Epoch 00042: val_loss did not improve from 1.01736\n",
      "36805/36805 [==============================] - 31s 832us/sample - loss: 0.0232 - acc: 0.9963 - val_loss: 1.4309 - val_acc: 0.7284\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0496 - acc: 0.9897\n",
      "Epoch 00043: val_loss did not improve from 1.01736\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.0497 - acc: 0.9897 - val_loss: 1.3734 - val_acc: 0.7386\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0231 - acc: 0.9961\n",
      "Epoch 00044: val_loss did not improve from 1.01736\n",
      "36805/36805 [==============================] - 31s 833us/sample - loss: 0.0233 - acc: 0.9961 - val_loss: 1.5258 - val_acc: 0.7130\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0326 - acc: 0.9935\n",
      "Epoch 00045: val_loss did not improve from 1.01736\n",
      "36805/36805 [==============================] - 31s 834us/sample - loss: 0.0326 - acc: 0.9935 - val_loss: 1.3761 - val_acc: 0.7358\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0178 - acc: 0.9976\n",
      "Epoch 00046: val_loss did not improve from 1.01736\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.0179 - acc: 0.9976 - val_loss: 1.5348 - val_acc: 0.7261\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0308 - acc: 0.9936\n",
      "Epoch 00047: val_loss did not improve from 1.01736\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.0310 - acc: 0.9935 - val_loss: 1.5989 - val_acc: 0.7116\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0236 - acc: 0.9954\n",
      "Epoch 00048: val_loss did not improve from 1.01736\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.0236 - acc: 0.9954 - val_loss: 1.4442 - val_acc: 0.7379\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0185 - acc: 0.9975\n",
      "Epoch 00049: val_loss did not improve from 1.01736\n",
      "36805/36805 [==============================] - 31s 840us/sample - loss: 0.0185 - acc: 0.9975 - val_loss: 1.4981 - val_acc: 0.7300\n",
      "Epoch 50/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0242 - acc: 0.9953\n",
      "Epoch 00050: val_loss did not improve from 1.01736\n",
      "36805/36805 [==============================] - 31s 840us/sample - loss: 0.0242 - acc: 0.9952 - val_loss: 1.5078 - val_acc: 0.7333\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0292 - acc: 0.9944\n",
      "Epoch 00051: val_loss did not improve from 1.01736\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.0292 - acc: 0.9944 - val_loss: 1.4621 - val_acc: 0.7384\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0203 - acc: 0.9966\n",
      "Epoch 00052: val_loss did not improve from 1.01736\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.0203 - acc: 0.9966 - val_loss: 1.4409 - val_acc: 0.7396\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0209 - acc: 0.9968\n",
      "Epoch 00053: val_loss did not improve from 1.01736\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.0209 - acc: 0.9968 - val_loss: 1.5418 - val_acc: 0.7237\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0214 - acc: 0.9961\n",
      "Epoch 00054: val_loss did not improve from 1.01736\n",
      "36805/36805 [==============================] - 31s 840us/sample - loss: 0.0215 - acc: 0.9961 - val_loss: 1.5661 - val_acc: 0.7286\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0235 - acc: 0.9953\n",
      "Epoch 00055: val_loss did not improve from 1.01736\n",
      "36805/36805 [==============================] - 31s 841us/sample - loss: 0.0235 - acc: 0.9953 - val_loss: 1.9726 - val_acc: 0.6746\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0204 - acc: 0.9967\n",
      "Epoch 00056: val_loss did not improve from 1.01736\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.0206 - acc: 0.9967 - val_loss: 1.5626 - val_acc: 0.7275\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0284 - acc: 0.9942\n",
      "Epoch 00057: val_loss did not improve from 1.01736\n",
      "36805/36805 [==============================] - 31s 836us/sample - loss: 0.0285 - acc: 0.9942 - val_loss: 1.4814 - val_acc: 0.7347\n",
      "\n",
      "1D_CNN_custom_multi_3_concat_ch_32_BN_5_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 461us/sample - loss: 1.1376 - acc: 0.6818\n",
      "Loss: 1.1375652820274218 Accuracy: 0.6818276\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.8220 - acc: 0.4476\n",
      "Epoch 00001: val_loss improved from inf to 1.47524, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_BN_6_conv_checkpoint/001-1.4752.hdf5\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 1.8219 - acc: 0.4476 - val_loss: 1.4752 - val_acc: 0.5309\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1475 - acc: 0.6543\n",
      "Epoch 00002: val_loss improved from 1.47524 to 1.06220, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_BN_6_conv_checkpoint/002-1.0622.hdf5\n",
      "36805/36805 [==============================] - 32s 862us/sample - loss: 1.1476 - acc: 0.6543 - val_loss: 1.0622 - val_acc: 0.6879\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9106 - acc: 0.7277\n",
      "Epoch 00003: val_loss improved from 1.06220 to 0.96082, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_BN_6_conv_checkpoint/003-0.9608.hdf5\n",
      "36805/36805 [==============================] - 32s 861us/sample - loss: 0.9106 - acc: 0.7277 - val_loss: 0.9608 - val_acc: 0.7188\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7520 - acc: 0.7788\n",
      "Epoch 00004: val_loss improved from 0.96082 to 0.81998, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_BN_6_conv_checkpoint/004-0.8200.hdf5\n",
      "36805/36805 [==============================] - 32s 861us/sample - loss: 0.7520 - acc: 0.7788 - val_loss: 0.8200 - val_acc: 0.7708\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6258 - acc: 0.8163\n",
      "Epoch 00005: val_loss improved from 0.81998 to 0.76767, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_BN_6_conv_checkpoint/005-0.7677.hdf5\n",
      "36805/36805 [==============================] - 32s 859us/sample - loss: 0.6259 - acc: 0.8163 - val_loss: 0.7677 - val_acc: 0.7771\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5289 - acc: 0.8460\n",
      "Epoch 00006: val_loss improved from 0.76767 to 0.73075, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_BN_6_conv_checkpoint/006-0.7308.hdf5\n",
      "36805/36805 [==============================] - 32s 865us/sample - loss: 0.5290 - acc: 0.8460 - val_loss: 0.7308 - val_acc: 0.7985\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4508 - acc: 0.8718\n",
      "Epoch 00007: val_loss improved from 0.73075 to 0.68081, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_BN_6_conv_checkpoint/007-0.6808.hdf5\n",
      "36805/36805 [==============================] - 32s 860us/sample - loss: 0.4508 - acc: 0.8718 - val_loss: 0.6808 - val_acc: 0.8125\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3920 - acc: 0.8894\n",
      "Epoch 00008: val_loss improved from 0.68081 to 0.64820, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_BN_6_conv_checkpoint/008-0.6482.hdf5\n",
      "36805/36805 [==============================] - 32s 861us/sample - loss: 0.3922 - acc: 0.8893 - val_loss: 0.6482 - val_acc: 0.8241\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3350 - acc: 0.9064\n",
      "Epoch 00009: val_loss did not improve from 0.64820\n",
      "36805/36805 [==============================] - 31s 854us/sample - loss: 0.3351 - acc: 0.9064 - val_loss: 0.6520 - val_acc: 0.8181\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2899 - acc: 0.9200\n",
      "Epoch 00010: val_loss improved from 0.64820 to 0.60454, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_BN_6_conv_checkpoint/010-0.6045.hdf5\n",
      "36805/36805 [==============================] - 32s 865us/sample - loss: 0.2901 - acc: 0.9200 - val_loss: 0.6045 - val_acc: 0.8272\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2538 - acc: 0.9330\n",
      "Epoch 00011: val_loss improved from 0.60454 to 0.59223, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_BN_6_conv_checkpoint/011-0.5922.hdf5\n",
      "36805/36805 [==============================] - 32s 856us/sample - loss: 0.2539 - acc: 0.9329 - val_loss: 0.5922 - val_acc: 0.8374\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2221 - acc: 0.9414\n",
      "Epoch 00012: val_loss improved from 0.59223 to 0.55971, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_BN_6_conv_checkpoint/012-0.5597.hdf5\n",
      "36805/36805 [==============================] - 32s 861us/sample - loss: 0.2224 - acc: 0.9413 - val_loss: 0.5597 - val_acc: 0.8477\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1976 - acc: 0.9496\n",
      "Epoch 00013: val_loss improved from 0.55971 to 0.55464, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_BN_6_conv_checkpoint/013-0.5546.hdf5\n",
      "36805/36805 [==============================] - 32s 861us/sample - loss: 0.1976 - acc: 0.9496 - val_loss: 0.5546 - val_acc: 0.8514\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1706 - acc: 0.9569\n",
      "Epoch 00014: val_loss did not improve from 0.55464\n",
      "36805/36805 [==============================] - 32s 856us/sample - loss: 0.1707 - acc: 0.9569 - val_loss: 0.5947 - val_acc: 0.8309\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1512 - acc: 0.9636\n",
      "Epoch 00015: val_loss did not improve from 0.55464\n",
      "36805/36805 [==============================] - 32s 859us/sample - loss: 0.1513 - acc: 0.9635 - val_loss: 0.5836 - val_acc: 0.8446\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1380 - acc: 0.9671\n",
      "Epoch 00016: val_loss did not improve from 0.55464\n",
      "36805/36805 [==============================] - 32s 857us/sample - loss: 0.1382 - acc: 0.9670 - val_loss: 0.5781 - val_acc: 0.8437\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1199 - acc: 0.9726\n",
      "Epoch 00017: val_loss improved from 0.55464 to 0.51934, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_BN_6_conv_checkpoint/017-0.5193.hdf5\n",
      "36805/36805 [==============================] - 32s 861us/sample - loss: 0.1200 - acc: 0.9725 - val_loss: 0.5193 - val_acc: 0.8616\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1080 - acc: 0.9761\n",
      "Epoch 00018: val_loss did not improve from 0.51934\n",
      "36805/36805 [==============================] - 32s 856us/sample - loss: 0.1081 - acc: 0.9760 - val_loss: 0.5499 - val_acc: 0.8537\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1013 - acc: 0.9784\n",
      "Epoch 00019: val_loss did not improve from 0.51934\n",
      "36805/36805 [==============================] - 32s 856us/sample - loss: 0.1013 - acc: 0.9784 - val_loss: 0.6069 - val_acc: 0.8416\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0852 - acc: 0.9831\n",
      "Epoch 00020: val_loss did not improve from 0.51934\n",
      "36805/36805 [==============================] - 32s 856us/sample - loss: 0.0854 - acc: 0.9830 - val_loss: 0.5564 - val_acc: 0.8488\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0832 - acc: 0.9835\n",
      "Epoch 00021: val_loss did not improve from 0.51934\n",
      "36805/36805 [==============================] - 32s 862us/sample - loss: 0.0833 - acc: 0.9834 - val_loss: 0.5559 - val_acc: 0.8572\n",
      "Epoch 22/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0740 - acc: 0.9855\n",
      "Epoch 00022: val_loss did not improve from 0.51934\n",
      "36805/36805 [==============================] - 32s 863us/sample - loss: 0.0740 - acc: 0.9855 - val_loss: 0.5378 - val_acc: 0.8572\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0650 - acc: 0.9881\n",
      "Epoch 00023: val_loss did not improve from 0.51934\n",
      "36805/36805 [==============================] - 32s 859us/sample - loss: 0.0650 - acc: 0.9881 - val_loss: 0.5470 - val_acc: 0.8642\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0621 - acc: 0.9889\n",
      "Epoch 00024: val_loss did not improve from 0.51934\n",
      "36805/36805 [==============================] - 32s 861us/sample - loss: 0.0623 - acc: 0.9889 - val_loss: 0.6428 - val_acc: 0.8374\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0703 - acc: 0.9855\n",
      "Epoch 00025: val_loss did not improve from 0.51934\n",
      "36805/36805 [==============================] - 32s 859us/sample - loss: 0.0704 - acc: 0.9854 - val_loss: 0.5270 - val_acc: 0.8696\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0544 - acc: 0.9907\n",
      "Epoch 00026: val_loss improved from 0.51934 to 0.49186, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_BN_6_conv_checkpoint/026-0.4919.hdf5\n",
      "36805/36805 [==============================] - 32s 864us/sample - loss: 0.0544 - acc: 0.9907 - val_loss: 0.4919 - val_acc: 0.8798\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0494 - acc: 0.9920\n",
      "Epoch 00027: val_loss did not improve from 0.49186\n",
      "36805/36805 [==============================] - 31s 852us/sample - loss: 0.0495 - acc: 0.9919 - val_loss: 0.5324 - val_acc: 0.8665\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0536 - acc: 0.9898\n",
      "Epoch 00028: val_loss did not improve from 0.49186\n",
      "36805/36805 [==============================] - 31s 855us/sample - loss: 0.0536 - acc: 0.9898 - val_loss: 0.5468 - val_acc: 0.8616\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0402 - acc: 0.9943\n",
      "Epoch 00029: val_loss did not improve from 0.49186\n",
      "36805/36805 [==============================] - 31s 855us/sample - loss: 0.0402 - acc: 0.9943 - val_loss: 0.5575 - val_acc: 0.8651\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0392 - acc: 0.9939\n",
      "Epoch 00030: val_loss did not improve from 0.49186\n",
      "36805/36805 [==============================] - 32s 860us/sample - loss: 0.0392 - acc: 0.9939 - val_loss: 0.5912 - val_acc: 0.8623\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0478 - acc: 0.9918\n",
      "Epoch 00031: val_loss did not improve from 0.49186\n",
      "36805/36805 [==============================] - 31s 855us/sample - loss: 0.0479 - acc: 0.9917 - val_loss: 0.5942 - val_acc: 0.8640\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0517 - acc: 0.9907\n",
      "Epoch 00032: val_loss did not improve from 0.49186\n",
      "36805/36805 [==============================] - 32s 864us/sample - loss: 0.0518 - acc: 0.9907 - val_loss: 0.5500 - val_acc: 0.8661\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0351 - acc: 0.9948\n",
      "Epoch 00033: val_loss did not improve from 0.49186\n",
      "36805/36805 [==============================] - 32s 862us/sample - loss: 0.0351 - acc: 0.9948 - val_loss: 0.5371 - val_acc: 0.8747\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0336 - acc: 0.9949\n",
      "Epoch 00034: val_loss did not improve from 0.49186\n",
      "36805/36805 [==============================] - 32s 860us/sample - loss: 0.0337 - acc: 0.9949 - val_loss: 0.5690 - val_acc: 0.8633\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0332 - acc: 0.9944\n",
      "Epoch 00035: val_loss did not improve from 0.49186\n",
      "36805/36805 [==============================] - 32s 861us/sample - loss: 0.0333 - acc: 0.9943 - val_loss: 0.6094 - val_acc: 0.8626\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0366 - acc: 0.9941\n",
      "Epoch 00036: val_loss did not improve from 0.49186\n",
      "36805/36805 [==============================] - 32s 859us/sample - loss: 0.0366 - acc: 0.9941 - val_loss: 0.5559 - val_acc: 0.8737\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0284 - acc: 0.9957\n",
      "Epoch 00037: val_loss did not improve from 0.49186\n",
      "36805/36805 [==============================] - 32s 858us/sample - loss: 0.0287 - acc: 0.9957 - val_loss: 0.5502 - val_acc: 0.8749\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0327 - acc: 0.9947\n",
      "Epoch 00038: val_loss did not improve from 0.49186\n",
      "36805/36805 [==============================] - 32s 861us/sample - loss: 0.0327 - acc: 0.9947 - val_loss: 0.5738 - val_acc: 0.8682\n",
      "Epoch 39/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0262 - acc: 0.9962\n",
      "Epoch 00039: val_loss did not improve from 0.49186\n",
      "36805/36805 [==============================] - 31s 853us/sample - loss: 0.0264 - acc: 0.9961 - val_loss: 0.5892 - val_acc: 0.8623\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0334 - acc: 0.9939\n",
      "Epoch 00040: val_loss did not improve from 0.49186\n",
      "36805/36805 [==============================] - 32s 860us/sample - loss: 0.0334 - acc: 0.9939 - val_loss: 0.7100 - val_acc: 0.8379\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0247 - acc: 0.9969\n",
      "Epoch 00041: val_loss did not improve from 0.49186\n",
      "36805/36805 [==============================] - 32s 867us/sample - loss: 0.0247 - acc: 0.9969 - val_loss: 0.5708 - val_acc: 0.8754\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0257 - acc: 0.9964\n",
      "Epoch 00042: val_loss did not improve from 0.49186\n",
      "36805/36805 [==============================] - 32s 862us/sample - loss: 0.0257 - acc: 0.9964 - val_loss: 0.6096 - val_acc: 0.8546\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0222 - acc: 0.9970\n",
      "Epoch 00043: val_loss did not improve from 0.49186\n",
      "36805/36805 [==============================] - 32s 862us/sample - loss: 0.0226 - acc: 0.9970 - val_loss: 0.5721 - val_acc: 0.8747\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0339 - acc: 0.9933\n",
      "Epoch 00044: val_loss did not improve from 0.49186\n",
      "36805/36805 [==============================] - 32s 864us/sample - loss: 0.0340 - acc: 0.9933 - val_loss: 0.5649 - val_acc: 0.8761\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0248 - acc: 0.9961\n",
      "Epoch 00045: val_loss did not improve from 0.49186\n",
      "36805/36805 [==============================] - 32s 865us/sample - loss: 0.0249 - acc: 0.9961 - val_loss: 0.5481 - val_acc: 0.8810\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0280 - acc: 0.9949\n",
      "Epoch 00046: val_loss did not improve from 0.49186\n",
      "36805/36805 [==============================] - 32s 860us/sample - loss: 0.0281 - acc: 0.9949 - val_loss: 0.6028 - val_acc: 0.8698\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0259 - acc: 0.9958\n",
      "Epoch 00047: val_loss did not improve from 0.49186\n",
      "36805/36805 [==============================] - 32s 862us/sample - loss: 0.0260 - acc: 0.9958 - val_loss: 0.5624 - val_acc: 0.8784\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0224 - acc: 0.9967\n",
      "Epoch 00048: val_loss did not improve from 0.49186\n",
      "36805/36805 [==============================] - 32s 859us/sample - loss: 0.0224 - acc: 0.9967 - val_loss: 0.5488 - val_acc: 0.8791\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0203 - acc: 0.9969\n",
      "Epoch 00049: val_loss did not improve from 0.49186\n",
      "36805/36805 [==============================] - 31s 855us/sample - loss: 0.0205 - acc: 0.9969 - val_loss: 0.5618 - val_acc: 0.8754\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0273 - acc: 0.9950\n",
      "Epoch 00050: val_loss did not improve from 0.49186\n",
      "36805/36805 [==============================] - 32s 860us/sample - loss: 0.0273 - acc: 0.9950 - val_loss: 0.5856 - val_acc: 0.8635\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0156 - acc: 0.9983\n",
      "Epoch 00051: val_loss did not improve from 0.49186\n",
      "36805/36805 [==============================] - 31s 852us/sample - loss: 0.0157 - acc: 0.9982 - val_loss: 0.6108 - val_acc: 0.8754\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0228 - acc: 0.9959\n",
      "Epoch 00052: val_loss did not improve from 0.49186\n",
      "36805/36805 [==============================] - 32s 863us/sample - loss: 0.0228 - acc: 0.9959 - val_loss: 0.5749 - val_acc: 0.8782\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0161 - acc: 0.9979\n",
      "Epoch 00053: val_loss did not improve from 0.49186\n",
      "36805/36805 [==============================] - 32s 859us/sample - loss: 0.0164 - acc: 0.9978 - val_loss: 0.6720 - val_acc: 0.8651\n",
      "Epoch 54/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0280 - acc: 0.9943\n",
      "Epoch 00054: val_loss did not improve from 0.49186\n",
      "36805/36805 [==============================] - 32s 859us/sample - loss: 0.0281 - acc: 0.9942 - val_loss: 0.5649 - val_acc: 0.8768\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0222 - acc: 0.9955\n",
      "Epoch 00055: val_loss did not improve from 0.49186\n",
      "36805/36805 [==============================] - 32s 861us/sample - loss: 0.0224 - acc: 0.9954 - val_loss: 0.6005 - val_acc: 0.8791\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0193 - acc: 0.9968\n",
      "Epoch 00056: val_loss did not improve from 0.49186\n",
      "36805/36805 [==============================] - 32s 860us/sample - loss: 0.0193 - acc: 0.9968 - val_loss: 0.6037 - val_acc: 0.8686\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0154 - acc: 0.9981\n",
      "Epoch 00057: val_loss did not improve from 0.49186\n",
      "36805/36805 [==============================] - 32s 858us/sample - loss: 0.0154 - acc: 0.9980 - val_loss: 0.5849 - val_acc: 0.8805\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0200 - acc: 0.9963\n",
      "Epoch 00058: val_loss did not improve from 0.49186\n",
      "36805/36805 [==============================] - 32s 858us/sample - loss: 0.0200 - acc: 0.9963 - val_loss: 0.6147 - val_acc: 0.8747\n",
      "Epoch 59/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0220 - acc: 0.9961\n",
      "Epoch 00059: val_loss did not improve from 0.49186\n",
      "36805/36805 [==============================] - 32s 860us/sample - loss: 0.0220 - acc: 0.9961 - val_loss: 0.5786 - val_acc: 0.8756\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0171 - acc: 0.9977\n",
      "Epoch 00060: val_loss did not improve from 0.49186\n",
      "36805/36805 [==============================] - 31s 846us/sample - loss: 0.0171 - acc: 0.9977 - val_loss: 0.5705 - val_acc: 0.8791\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0145 - acc: 0.9982\n",
      "Epoch 00061: val_loss did not improve from 0.49186\n",
      "36805/36805 [==============================] - 31s 855us/sample - loss: 0.0145 - acc: 0.9982 - val_loss: 0.5925 - val_acc: 0.8791\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0155 - acc: 0.9978\n",
      "Epoch 00062: val_loss did not improve from 0.49186\n",
      "36805/36805 [==============================] - 31s 852us/sample - loss: 0.0156 - acc: 0.9978 - val_loss: 0.5835 - val_acc: 0.8796\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0228 - acc: 0.9955\n",
      "Epoch 00063: val_loss did not improve from 0.49186\n",
      "36805/36805 [==============================] - 32s 862us/sample - loss: 0.0228 - acc: 0.9954 - val_loss: 0.5777 - val_acc: 0.8735\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0206 - acc: 0.9962\n",
      "Epoch 00064: val_loss did not improve from 0.49186\n",
      "36805/36805 [==============================] - 31s 854us/sample - loss: 0.0206 - acc: 0.9962 - val_loss: 0.5839 - val_acc: 0.8761\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0151 - acc: 0.9976\n",
      "Epoch 00065: val_loss did not improve from 0.49186\n",
      "36805/36805 [==============================] - 31s 852us/sample - loss: 0.0151 - acc: 0.9976 - val_loss: 0.6441 - val_acc: 0.8682\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0212 - acc: 0.9958\n",
      "Epoch 00066: val_loss did not improve from 0.49186\n",
      "36805/36805 [==============================] - 31s 854us/sample - loss: 0.0213 - acc: 0.9958 - val_loss: 0.5718 - val_acc: 0.8803\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0136 - acc: 0.9981\n",
      "Epoch 00067: val_loss did not improve from 0.49186\n",
      "36805/36805 [==============================] - 31s 849us/sample - loss: 0.0137 - acc: 0.9981 - val_loss: 0.5900 - val_acc: 0.8796\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0186 - acc: 0.9963\n",
      "Epoch 00068: val_loss did not improve from 0.49186\n",
      "36805/36805 [==============================] - 32s 857us/sample - loss: 0.0186 - acc: 0.9963 - val_loss: 0.6568 - val_acc: 0.8705\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0123 - acc: 0.9989\n",
      "Epoch 00069: val_loss did not improve from 0.49186\n",
      "36805/36805 [==============================] - 31s 854us/sample - loss: 0.0124 - acc: 0.9988 - val_loss: 0.5638 - val_acc: 0.8880\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0177 - acc: 0.9969\n",
      "Epoch 00070: val_loss did not improve from 0.49186\n",
      "36805/36805 [==============================] - 32s 868us/sample - loss: 0.0178 - acc: 0.9969 - val_loss: 0.5761 - val_acc: 0.8812\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0133 - acc: 0.9978\n",
      "Epoch 00071: val_loss did not improve from 0.49186\n",
      "36805/36805 [==============================] - 32s 861us/sample - loss: 0.0135 - acc: 0.9978 - val_loss: 0.5835 - val_acc: 0.8821\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0232 - acc: 0.9952\n",
      "Epoch 00072: val_loss did not improve from 0.49186\n",
      "36805/36805 [==============================] - 32s 861us/sample - loss: 0.0232 - acc: 0.9951 - val_loss: 0.5901 - val_acc: 0.8796\n",
      "Epoch 73/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0134 - acc: 0.9978\n",
      "Epoch 00073: val_loss did not improve from 0.49186\n",
      "36805/36805 [==============================] - 32s 858us/sample - loss: 0.0134 - acc: 0.9978 - val_loss: 0.5431 - val_acc: 0.8845\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0127 - acc: 0.9978\n",
      "Epoch 00074: val_loss did not improve from 0.49186\n",
      "36805/36805 [==============================] - 31s 855us/sample - loss: 0.0127 - acc: 0.9978 - val_loss: 0.6242 - val_acc: 0.8754\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0162 - acc: 0.9973\n",
      "Epoch 00075: val_loss did not improve from 0.49186\n",
      "36805/36805 [==============================] - 32s 856us/sample - loss: 0.0167 - acc: 0.9972 - val_loss: 0.6166 - val_acc: 0.8735\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0254 - acc: 0.9948\n",
      "Epoch 00076: val_loss did not improve from 0.49186\n",
      "36805/36805 [==============================] - 32s 864us/sample - loss: 0.0257 - acc: 0.9947 - val_loss: 0.5948 - val_acc: 0.8775\n",
      "\n",
      "1D_CNN_custom_multi_3_concat_ch_32_BN_6_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 464us/sample - loss: 0.5799 - acc: 0.8513\n",
      "Loss: 0.5799085622635958 Accuracy: 0.85129803\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.7707 - acc: 0.4525\n",
      "Epoch 00001: val_loss improved from inf to 1.67224, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_BN_7_conv_checkpoint/001-1.6722.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 1.7707 - acc: 0.4525 - val_loss: 1.6722 - val_acc: 0.4314\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0409 - acc: 0.6885\n",
      "Epoch 00002: val_loss improved from 1.67224 to 0.86598, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_BN_7_conv_checkpoint/002-0.8660.hdf5\n",
      "36805/36805 [==============================] - 33s 903us/sample - loss: 1.0411 - acc: 0.6884 - val_loss: 0.8660 - val_acc: 0.7484\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7737 - acc: 0.7762\n",
      "Epoch 00003: val_loss improved from 0.86598 to 0.69204, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_BN_7_conv_checkpoint/003-0.6920.hdf5\n",
      "36805/36805 [==============================] - 34s 925us/sample - loss: 0.7736 - acc: 0.7763 - val_loss: 0.6920 - val_acc: 0.8011\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5979 - acc: 0.8283\n",
      "Epoch 00004: val_loss improved from 0.69204 to 0.61254, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_BN_7_conv_checkpoint/004-0.6125.hdf5\n",
      "36805/36805 [==============================] - 34s 920us/sample - loss: 0.5978 - acc: 0.8283 - val_loss: 0.6125 - val_acc: 0.8183\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4894 - acc: 0.8587\n",
      "Epoch 00005: val_loss improved from 0.61254 to 0.48346, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_BN_7_conv_checkpoint/005-0.4835.hdf5\n",
      "36805/36805 [==============================] - 33s 902us/sample - loss: 0.4894 - acc: 0.8587 - val_loss: 0.4835 - val_acc: 0.8633\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4122 - acc: 0.8833\n",
      "Epoch 00006: val_loss improved from 0.48346 to 0.45871, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_BN_7_conv_checkpoint/006-0.4587.hdf5\n",
      "36805/36805 [==============================] - 33s 905us/sample - loss: 0.4123 - acc: 0.8833 - val_loss: 0.4587 - val_acc: 0.8668\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3574 - acc: 0.8990\n",
      "Epoch 00007: val_loss improved from 0.45871 to 0.40581, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_BN_7_conv_checkpoint/007-0.4058.hdf5\n",
      "36805/36805 [==============================] - 33s 904us/sample - loss: 0.3573 - acc: 0.8991 - val_loss: 0.4058 - val_acc: 0.8849\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3137 - acc: 0.9087\n",
      "Epoch 00008: val_loss did not improve from 0.40581\n",
      "36805/36805 [==============================] - 33s 894us/sample - loss: 0.3138 - acc: 0.9087 - val_loss: 0.4602 - val_acc: 0.8696\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2757 - acc: 0.9212\n",
      "Epoch 00009: val_loss improved from 0.40581 to 0.35393, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_BN_7_conv_checkpoint/009-0.3539.hdf5\n",
      "36805/36805 [==============================] - 33s 895us/sample - loss: 0.2758 - acc: 0.9212 - val_loss: 0.3539 - val_acc: 0.8982\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2484 - acc: 0.9292\n",
      "Epoch 00010: val_loss did not improve from 0.35393\n",
      "36805/36805 [==============================] - 33s 886us/sample - loss: 0.2484 - acc: 0.9292 - val_loss: 0.3546 - val_acc: 0.9012\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2211 - acc: 0.9361\n",
      "Epoch 00011: val_loss improved from 0.35393 to 0.31120, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_BN_7_conv_checkpoint/011-0.3112.hdf5\n",
      "36805/36805 [==============================] - 33s 906us/sample - loss: 0.2211 - acc: 0.9361 - val_loss: 0.3112 - val_acc: 0.9106\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1985 - acc: 0.9451\n",
      "Epoch 00012: val_loss improved from 0.31120 to 0.30639, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_BN_7_conv_checkpoint/012-0.3064.hdf5\n",
      "36805/36805 [==============================] - 33s 897us/sample - loss: 0.1985 - acc: 0.9451 - val_loss: 0.3064 - val_acc: 0.9150\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1794 - acc: 0.9502\n",
      "Epoch 00013: val_loss improved from 0.30639 to 0.29818, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_BN_7_conv_checkpoint/013-0.2982.hdf5\n",
      "36805/36805 [==============================] - 33s 894us/sample - loss: 0.1795 - acc: 0.9501 - val_loss: 0.2982 - val_acc: 0.9175\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1639 - acc: 0.9548\n",
      "Epoch 00014: val_loss improved from 0.29818 to 0.28437, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_BN_7_conv_checkpoint/014-0.2844.hdf5\n",
      "36805/36805 [==============================] - 33s 905us/sample - loss: 0.1640 - acc: 0.9548 - val_loss: 0.2844 - val_acc: 0.9229\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1472 - acc: 0.9595\n",
      "Epoch 00015: val_loss improved from 0.28437 to 0.28372, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_BN_7_conv_checkpoint/015-0.2837.hdf5\n",
      "36805/36805 [==============================] - 33s 904us/sample - loss: 0.1472 - acc: 0.9595 - val_loss: 0.2837 - val_acc: 0.9220\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1342 - acc: 0.9632\n",
      "Epoch 00016: val_loss did not improve from 0.28372\n",
      "36805/36805 [==============================] - 33s 901us/sample - loss: 0.1343 - acc: 0.9632 - val_loss: 0.2877 - val_acc: 0.9229\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1222 - acc: 0.9667\n",
      "Epoch 00017: val_loss did not improve from 0.28372\n",
      "36805/36805 [==============================] - 34s 914us/sample - loss: 0.1222 - acc: 0.9667 - val_loss: 0.2839 - val_acc: 0.9203\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1107 - acc: 0.9701\n",
      "Epoch 00018: val_loss did not improve from 0.28372\n",
      "36805/36805 [==============================] - 34s 912us/sample - loss: 0.1107 - acc: 0.9701 - val_loss: 0.2947 - val_acc: 0.9180\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1014 - acc: 0.9740\n",
      "Epoch 00019: val_loss did not improve from 0.28372\n",
      "36805/36805 [==============================] - 33s 899us/sample - loss: 0.1015 - acc: 0.9740 - val_loss: 0.2880 - val_acc: 0.9206\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0938 - acc: 0.9759\n",
      "Epoch 00020: val_loss improved from 0.28372 to 0.28361, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_BN_7_conv_checkpoint/020-0.2836.hdf5\n",
      "36805/36805 [==============================] - 33s 901us/sample - loss: 0.0938 - acc: 0.9759 - val_loss: 0.2836 - val_acc: 0.9199\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0847 - acc: 0.9787\n",
      "Epoch 00021: val_loss improved from 0.28361 to 0.26628, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_BN_7_conv_checkpoint/021-0.2663.hdf5\n",
      "36805/36805 [==============================] - 33s 896us/sample - loss: 0.0849 - acc: 0.9786 - val_loss: 0.2663 - val_acc: 0.9294\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0793 - acc: 0.9803\n",
      "Epoch 00022: val_loss did not improve from 0.26628\n",
      "36805/36805 [==============================] - 33s 897us/sample - loss: 0.0793 - acc: 0.9803 - val_loss: 0.2950 - val_acc: 0.9213\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0690 - acc: 0.9842\n",
      "Epoch 00023: val_loss did not improve from 0.26628\n",
      "36805/36805 [==============================] - 33s 892us/sample - loss: 0.0691 - acc: 0.9842 - val_loss: 0.3128 - val_acc: 0.9140\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0685 - acc: 0.9837\n",
      "Epoch 00024: val_loss did not improve from 0.26628\n",
      "36805/36805 [==============================] - 33s 906us/sample - loss: 0.0685 - acc: 0.9837 - val_loss: 0.2763 - val_acc: 0.9259\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0597 - acc: 0.9865\n",
      "Epoch 00025: val_loss did not improve from 0.26628\n",
      "36805/36805 [==============================] - 33s 900us/sample - loss: 0.0597 - acc: 0.9865 - val_loss: 0.2855 - val_acc: 0.9250\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0561 - acc: 0.9875\n",
      "Epoch 00026: val_loss did not improve from 0.26628\n",
      "36805/36805 [==============================] - 33s 895us/sample - loss: 0.0562 - acc: 0.9875 - val_loss: 0.3052 - val_acc: 0.9241\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0529 - acc: 0.9885\n",
      "Epoch 00027: val_loss did not improve from 0.26628\n",
      "36805/36805 [==============================] - 33s 896us/sample - loss: 0.0529 - acc: 0.9885 - val_loss: 0.2728 - val_acc: 0.9317\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0472 - acc: 0.9902\n",
      "Epoch 00028: val_loss did not improve from 0.26628\n",
      "36805/36805 [==============================] - 33s 902us/sample - loss: 0.0472 - acc: 0.9902 - val_loss: 0.2953 - val_acc: 0.9264\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0429 - acc: 0.9917\n",
      "Epoch 00029: val_loss did not improve from 0.26628\n",
      "36805/36805 [==============================] - 33s 900us/sample - loss: 0.0439 - acc: 0.9916 - val_loss: 0.3103 - val_acc: 0.9199\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0473 - acc: 0.9897\n",
      "Epoch 00030: val_loss did not improve from 0.26628\n",
      "36805/36805 [==============================] - 33s 895us/sample - loss: 0.0475 - acc: 0.9896 - val_loss: 0.2978 - val_acc: 0.9262\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0479 - acc: 0.9899\n",
      "Epoch 00031: val_loss did not improve from 0.26628\n",
      "36805/36805 [==============================] - 33s 900us/sample - loss: 0.0479 - acc: 0.9899 - val_loss: 0.2934 - val_acc: 0.9259\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0320 - acc: 0.9948\n",
      "Epoch 00032: val_loss improved from 0.26628 to 0.26472, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_BN_7_conv_checkpoint/032-0.2647.hdf5\n",
      "36805/36805 [==============================] - 33s 902us/sample - loss: 0.0320 - acc: 0.9948 - val_loss: 0.2647 - val_acc: 0.9317\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0322 - acc: 0.9945\n",
      "Epoch 00033: val_loss did not improve from 0.26472\n",
      "36805/36805 [==============================] - 33s 895us/sample - loss: 0.0323 - acc: 0.9945 - val_loss: 0.3098 - val_acc: 0.9185\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0400 - acc: 0.9916\n",
      "Epoch 00034: val_loss did not improve from 0.26472\n",
      "36805/36805 [==============================] - 33s 909us/sample - loss: 0.0400 - acc: 0.9916 - val_loss: 0.2688 - val_acc: 0.9320\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0340 - acc: 0.9935\n",
      "Epoch 00035: val_loss did not improve from 0.26472\n",
      "36805/36805 [==============================] - 33s 898us/sample - loss: 0.0340 - acc: 0.9935 - val_loss: 0.2918 - val_acc: 0.9269\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0271 - acc: 0.9957\n",
      "Epoch 00036: val_loss did not improve from 0.26472\n",
      "36805/36805 [==============================] - 33s 897us/sample - loss: 0.0271 - acc: 0.9957 - val_loss: 0.3227 - val_acc: 0.9159\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0296 - acc: 0.9943\n",
      "Epoch 00037: val_loss did not improve from 0.26472\n",
      "36805/36805 [==============================] - 33s 896us/sample - loss: 0.0296 - acc: 0.9943 - val_loss: 0.2978 - val_acc: 0.9271\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0315 - acc: 0.9936\n",
      "Epoch 00038: val_loss did not improve from 0.26472\n",
      "36805/36805 [==============================] - 33s 899us/sample - loss: 0.0315 - acc: 0.9936 - val_loss: 0.3165 - val_acc: 0.9236\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0234 - acc: 0.9967\n",
      "Epoch 00039: val_loss did not improve from 0.26472\n",
      "36805/36805 [==============================] - 33s 896us/sample - loss: 0.0234 - acc: 0.9967 - val_loss: 0.2918 - val_acc: 0.9297\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0259 - acc: 0.9949\n",
      "Epoch 00040: val_loss did not improve from 0.26472\n",
      "36805/36805 [==============================] - 33s 891us/sample - loss: 0.0260 - acc: 0.9949 - val_loss: 0.3769 - val_acc: 0.9080\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0251 - acc: 0.9957\n",
      "Epoch 00041: val_loss did not improve from 0.26472\n",
      "36805/36805 [==============================] - 33s 900us/sample - loss: 0.0251 - acc: 0.9957 - val_loss: 0.2991 - val_acc: 0.9278\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0247 - acc: 0.9955\n",
      "Epoch 00042: val_loss did not improve from 0.26472\n",
      "36805/36805 [==============================] - 33s 894us/sample - loss: 0.0248 - acc: 0.9955 - val_loss: 0.3326 - val_acc: 0.9250\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0335 - acc: 0.9929\n",
      "Epoch 00043: val_loss did not improve from 0.26472\n",
      "36805/36805 [==============================] - 33s 900us/sample - loss: 0.0336 - acc: 0.9928 - val_loss: 0.2972 - val_acc: 0.9317\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0252 - acc: 0.9955\n",
      "Epoch 00044: val_loss did not improve from 0.26472\n",
      "36805/36805 [==============================] - 33s 890us/sample - loss: 0.0252 - acc: 0.9955 - val_loss: 0.3014 - val_acc: 0.9276\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0194 - acc: 0.9972\n",
      "Epoch 00045: val_loss did not improve from 0.26472\n",
      "36805/36805 [==============================] - 33s 895us/sample - loss: 0.0194 - acc: 0.9972 - val_loss: 0.3005 - val_acc: 0.9308\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0170 - acc: 0.9975\n",
      "Epoch 00046: val_loss did not improve from 0.26472\n",
      "36805/36805 [==============================] - 33s 908us/sample - loss: 0.0171 - acc: 0.9975 - val_loss: 0.3046 - val_acc: 0.9280\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0234 - acc: 0.9951\n",
      "Epoch 00047: val_loss did not improve from 0.26472\n",
      "36805/36805 [==============================] - 33s 906us/sample - loss: 0.0236 - acc: 0.9951 - val_loss: 0.3273 - val_acc: 0.9241\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0217 - acc: 0.9958\n",
      "Epoch 00048: val_loss did not improve from 0.26472\n",
      "36805/36805 [==============================] - 33s 900us/sample - loss: 0.0217 - acc: 0.9958 - val_loss: 0.3529 - val_acc: 0.9192\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0171 - acc: 0.9975\n",
      "Epoch 00049: val_loss did not improve from 0.26472\n",
      "36805/36805 [==============================] - 33s 894us/sample - loss: 0.0171 - acc: 0.9975 - val_loss: 0.3034 - val_acc: 0.9278\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0171 - acc: 0.9973\n",
      "Epoch 00050: val_loss did not improve from 0.26472\n",
      "36805/36805 [==============================] - 33s 891us/sample - loss: 0.0171 - acc: 0.9973 - val_loss: 0.2886 - val_acc: 0.9322\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0180 - acc: 0.9971\n",
      "Epoch 00051: val_loss did not improve from 0.26472\n",
      "36805/36805 [==============================] - 33s 894us/sample - loss: 0.0180 - acc: 0.9971 - val_loss: 0.3089 - val_acc: 0.9306\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0140 - acc: 0.9980\n",
      "Epoch 00052: val_loss did not improve from 0.26472\n",
      "36805/36805 [==============================] - 33s 892us/sample - loss: 0.0140 - acc: 0.9980 - val_loss: 0.2977 - val_acc: 0.9331\n",
      "Epoch 53/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0167 - acc: 0.9971\n",
      "Epoch 00053: val_loss did not improve from 0.26472\n",
      "36805/36805 [==============================] - 33s 895us/sample - loss: 0.0167 - acc: 0.9971 - val_loss: 0.3775 - val_acc: 0.9187\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0181 - acc: 0.9965\n",
      "Epoch 00054: val_loss did not improve from 0.26472\n",
      "36805/36805 [==============================] - 33s 898us/sample - loss: 0.0181 - acc: 0.9965 - val_loss: 0.3367 - val_acc: 0.9257\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0173 - acc: 0.9963\n",
      "Epoch 00055: val_loss did not improve from 0.26472\n",
      "36805/36805 [==============================] - 33s 892us/sample - loss: 0.0173 - acc: 0.9963 - val_loss: 0.2921 - val_acc: 0.9329\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0132 - acc: 0.9982\n",
      "Epoch 00056: val_loss did not improve from 0.26472\n",
      "36805/36805 [==============================] - 33s 891us/sample - loss: 0.0132 - acc: 0.9982 - val_loss: 0.2921 - val_acc: 0.9336\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0122 - acc: 0.9981\n",
      "Epoch 00057: val_loss did not improve from 0.26472\n",
      "36805/36805 [==============================] - 33s 889us/sample - loss: 0.0122 - acc: 0.9981 - val_loss: 0.3467 - val_acc: 0.9229\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0144 - acc: 0.9978\n",
      "Epoch 00058: val_loss did not improve from 0.26472\n",
      "36805/36805 [==============================] - 33s 889us/sample - loss: 0.0146 - acc: 0.9978 - val_loss: 0.3260 - val_acc: 0.9290\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0249 - acc: 0.9942\n",
      "Epoch 00059: val_loss did not improve from 0.26472\n",
      "36805/36805 [==============================] - 33s 895us/sample - loss: 0.0249 - acc: 0.9942 - val_loss: 0.3180 - val_acc: 0.9280\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0125 - acc: 0.9980\n",
      "Epoch 00060: val_loss did not improve from 0.26472\n",
      "36805/36805 [==============================] - 33s 892us/sample - loss: 0.0125 - acc: 0.9980 - val_loss: 0.2956 - val_acc: 0.9373\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0088 - acc: 0.9989\n",
      "Epoch 00061: val_loss did not improve from 0.26472\n",
      "36805/36805 [==============================] - 33s 894us/sample - loss: 0.0088 - acc: 0.9989 - val_loss: 0.3026 - val_acc: 0.9331\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0152 - acc: 0.9973\n",
      "Epoch 00062: val_loss did not improve from 0.26472\n",
      "36805/36805 [==============================] - 33s 885us/sample - loss: 0.0152 - acc: 0.9973 - val_loss: 0.2994 - val_acc: 0.9334\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0117 - acc: 0.9983\n",
      "Epoch 00063: val_loss did not improve from 0.26472\n",
      "36805/36805 [==============================] - 33s 891us/sample - loss: 0.0117 - acc: 0.9983 - val_loss: 0.3052 - val_acc: 0.9308\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0119 - acc: 0.9981\n",
      "Epoch 00064: val_loss did not improve from 0.26472\n",
      "36805/36805 [==============================] - 33s 894us/sample - loss: 0.0119 - acc: 0.9981 - val_loss: 0.3125 - val_acc: 0.9248\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0112 - acc: 0.9984\n",
      "Epoch 00065: val_loss did not improve from 0.26472\n",
      "36805/36805 [==============================] - 33s 894us/sample - loss: 0.0113 - acc: 0.9984 - val_loss: 0.3224 - val_acc: 0.9271\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0195 - acc: 0.9956\n",
      "Epoch 00066: val_loss did not improve from 0.26472\n",
      "36805/36805 [==============================] - 33s 892us/sample - loss: 0.0195 - acc: 0.9956 - val_loss: 0.3068 - val_acc: 0.9280\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0094 - acc: 0.9989\n",
      "Epoch 00067: val_loss did not improve from 0.26472\n",
      "36805/36805 [==============================] - 33s 906us/sample - loss: 0.0094 - acc: 0.9989 - val_loss: 0.3455 - val_acc: 0.9245\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0114 - acc: 0.9983\n",
      "Epoch 00068: val_loss did not improve from 0.26472\n",
      "36805/36805 [==============================] - 33s 890us/sample - loss: 0.0116 - acc: 0.9982 - val_loss: 0.3306 - val_acc: 0.9294\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0182 - acc: 0.9962\n",
      "Epoch 00069: val_loss did not improve from 0.26472\n",
      "36805/36805 [==============================] - 33s 892us/sample - loss: 0.0182 - acc: 0.9962 - val_loss: 0.3140 - val_acc: 0.9304\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0122 - acc: 0.9978\n",
      "Epoch 00070: val_loss did not improve from 0.26472\n",
      "36805/36805 [==============================] - 33s 892us/sample - loss: 0.0124 - acc: 0.9977 - val_loss: 0.2929 - val_acc: 0.9313\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0233 - acc: 0.9944\n",
      "Epoch 00071: val_loss did not improve from 0.26472\n",
      "36805/36805 [==============================] - 33s 891us/sample - loss: 0.0233 - acc: 0.9944 - val_loss: 0.2783 - val_acc: 0.9371\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0091 - acc: 0.9989\n",
      "Epoch 00072: val_loss did not improve from 0.26472\n",
      "36805/36805 [==============================] - 33s 887us/sample - loss: 0.0091 - acc: 0.9989 - val_loss: 0.2881 - val_acc: 0.9366\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0102 - acc: 0.9985\n",
      "Epoch 00073: val_loss did not improve from 0.26472\n",
      "36805/36805 [==============================] - 33s 900us/sample - loss: 0.0102 - acc: 0.9985 - val_loss: 0.2754 - val_acc: 0.9383\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0068 - acc: 0.9993\n",
      "Epoch 00074: val_loss did not improve from 0.26472\n",
      "36805/36805 [==============================] - 33s 894us/sample - loss: 0.0069 - acc: 0.9993 - val_loss: 0.3780 - val_acc: 0.9192\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0198 - acc: 0.9950\n",
      "Epoch 00075: val_loss did not improve from 0.26472\n",
      "36805/36805 [==============================] - 34s 911us/sample - loss: 0.0199 - acc: 0.9949 - val_loss: 0.2950 - val_acc: 0.9366\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0167 - acc: 0.9961\n",
      "Epoch 00076: val_loss did not improve from 0.26472\n",
      "36805/36805 [==============================] - 34s 919us/sample - loss: 0.0168 - acc: 0.9961 - val_loss: 0.2941 - val_acc: 0.9324\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0108 - acc: 0.9980\n",
      "Epoch 00077: val_loss did not improve from 0.26472\n",
      "36805/36805 [==============================] - 33s 896us/sample - loss: 0.0109 - acc: 0.9980 - val_loss: 0.3017 - val_acc: 0.9329\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0160 - acc: 0.9963\n",
      "Epoch 00078: val_loss did not improve from 0.26472\n",
      "36805/36805 [==============================] - 33s 893us/sample - loss: 0.0160 - acc: 0.9963 - val_loss: 0.2990 - val_acc: 0.9315\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0071 - acc: 0.9992\n",
      "Epoch 00079: val_loss did not improve from 0.26472\n",
      "36805/36805 [==============================] - 33s 890us/sample - loss: 0.0071 - acc: 0.9992 - val_loss: 0.2970 - val_acc: 0.9324\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0075 - acc: 0.9990\n",
      "Epoch 00080: val_loss did not improve from 0.26472\n",
      "36805/36805 [==============================] - 33s 897us/sample - loss: 0.0075 - acc: 0.9990 - val_loss: 0.3031 - val_acc: 0.9331\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0093 - acc: 0.9987\n",
      "Epoch 00081: val_loss did not improve from 0.26472\n",
      "36805/36805 [==============================] - 33s 898us/sample - loss: 0.0093 - acc: 0.9987 - val_loss: 0.3172 - val_acc: 0.9334\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0122 - acc: 0.9977\n",
      "Epoch 00082: val_loss did not improve from 0.26472\n",
      "36805/36805 [==============================] - 33s 892us/sample - loss: 0.0122 - acc: 0.9977 - val_loss: 0.3489 - val_acc: 0.9243\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_multi_3_concat_ch_32_BN_7_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 470us/sample - loss: 0.3168 - acc: 0.9161\n",
      "Loss: 0.3168294546883921 Accuracy: 0.91609555\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.8906 - acc: 0.4167\n",
      "Epoch 00001: val_loss improved from inf to 1.61971, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_BN_8_conv_checkpoint/001-1.6197.hdf5\n",
      "36805/36805 [==============================] - 51s 1ms/sample - loss: 1.8905 - acc: 0.4167 - val_loss: 1.6197 - val_acc: 0.4673\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9677 - acc: 0.7104\n",
      "Epoch 00002: val_loss improved from 1.61971 to 0.74473, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_BN_8_conv_checkpoint/002-0.7447.hdf5\n",
      "36805/36805 [==============================] - 34s 919us/sample - loss: 0.9676 - acc: 0.7104 - val_loss: 0.7447 - val_acc: 0.7706\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6572 - acc: 0.8122\n",
      "Epoch 00003: val_loss improved from 0.74473 to 0.53421, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_BN_8_conv_checkpoint/003-0.5342.hdf5\n",
      "36805/36805 [==============================] - 34s 913us/sample - loss: 0.6571 - acc: 0.8123 - val_loss: 0.5342 - val_acc: 0.8491\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4972 - acc: 0.8545\n",
      "Epoch 00004: val_loss improved from 0.53421 to 0.41370, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_BN_8_conv_checkpoint/004-0.4137.hdf5\n",
      "36805/36805 [==============================] - 34s 927us/sample - loss: 0.4975 - acc: 0.8544 - val_loss: 0.4137 - val_acc: 0.8786\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4049 - acc: 0.8828\n",
      "Epoch 00005: val_loss improved from 0.41370 to 0.35495, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_BN_8_conv_checkpoint/005-0.3549.hdf5\n",
      "36805/36805 [==============================] - 34s 922us/sample - loss: 0.4052 - acc: 0.8827 - val_loss: 0.3549 - val_acc: 0.8982\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3437 - acc: 0.8988\n",
      "Epoch 00006: val_loss improved from 0.35495 to 0.32624, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_BN_8_conv_checkpoint/006-0.3262.hdf5\n",
      "36805/36805 [==============================] - 34s 925us/sample - loss: 0.3437 - acc: 0.8988 - val_loss: 0.3262 - val_acc: 0.9015\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2941 - acc: 0.9137\n",
      "Epoch 00007: val_loss improved from 0.32624 to 0.30497, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_BN_8_conv_checkpoint/007-0.3050.hdf5\n",
      "36805/36805 [==============================] - 34s 924us/sample - loss: 0.2942 - acc: 0.9137 - val_loss: 0.3050 - val_acc: 0.9087\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2622 - acc: 0.9235\n",
      "Epoch 00008: val_loss improved from 0.30497 to 0.26646, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_BN_8_conv_checkpoint/008-0.2665.hdf5\n",
      "36805/36805 [==============================] - 34s 930us/sample - loss: 0.2622 - acc: 0.9235 - val_loss: 0.2665 - val_acc: 0.9180\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2293 - acc: 0.9315\n",
      "Epoch 00009: val_loss improved from 0.26646 to 0.24839, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_BN_8_conv_checkpoint/009-0.2484.hdf5\n",
      "36805/36805 [==============================] - 34s 924us/sample - loss: 0.2292 - acc: 0.9315 - val_loss: 0.2484 - val_acc: 0.9269\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2098 - acc: 0.9380\n",
      "Epoch 00010: val_loss improved from 0.24839 to 0.24565, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_BN_8_conv_checkpoint/010-0.2457.hdf5\n",
      "36805/36805 [==============================] - 34s 929us/sample - loss: 0.2098 - acc: 0.9380 - val_loss: 0.2457 - val_acc: 0.9236\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1858 - acc: 0.9458\n",
      "Epoch 00011: val_loss improved from 0.24565 to 0.23353, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_BN_8_conv_checkpoint/011-0.2335.hdf5\n",
      "36805/36805 [==============================] - 34s 927us/sample - loss: 0.1858 - acc: 0.9458 - val_loss: 0.2335 - val_acc: 0.9290\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1722 - acc: 0.9498\n",
      "Epoch 00012: val_loss did not improve from 0.23353\n",
      "36805/36805 [==============================] - 35s 944us/sample - loss: 0.1724 - acc: 0.9498 - val_loss: 0.2373 - val_acc: 0.9252\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1606 - acc: 0.9531\n",
      "Epoch 00013: val_loss did not improve from 0.23353\n",
      "36805/36805 [==============================] - 35s 942us/sample - loss: 0.1608 - acc: 0.9530 - val_loss: 0.2355 - val_acc: 0.9280\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1443 - acc: 0.9581\n",
      "Epoch 00014: val_loss improved from 0.23353 to 0.20968, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_BN_8_conv_checkpoint/014-0.2097.hdf5\n",
      "36805/36805 [==============================] - 34s 922us/sample - loss: 0.1443 - acc: 0.9581 - val_loss: 0.2097 - val_acc: 0.9331\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1277 - acc: 0.9639\n",
      "Epoch 00015: val_loss improved from 0.20968 to 0.20907, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_BN_8_conv_checkpoint/015-0.2091.hdf5\n",
      "36805/36805 [==============================] - 35s 945us/sample - loss: 0.1277 - acc: 0.9639 - val_loss: 0.2091 - val_acc: 0.9345\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1177 - acc: 0.9655\n",
      "Epoch 00016: val_loss improved from 0.20907 to 0.20744, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_BN_8_conv_checkpoint/016-0.2074.hdf5\n",
      "36805/36805 [==============================] - 34s 931us/sample - loss: 0.1177 - acc: 0.9655 - val_loss: 0.2074 - val_acc: 0.9373\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1080 - acc: 0.9699\n",
      "Epoch 00017: val_loss did not improve from 0.20744\n",
      "36805/36805 [==============================] - 34s 922us/sample - loss: 0.1080 - acc: 0.9699 - val_loss: 0.2122 - val_acc: 0.9364\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0990 - acc: 0.9718\n",
      "Epoch 00018: val_loss did not improve from 0.20744\n",
      "36805/36805 [==============================] - 35s 950us/sample - loss: 0.0991 - acc: 0.9717 - val_loss: 0.2919 - val_acc: 0.9122\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0923 - acc: 0.9748\n",
      "Epoch 00019: val_loss improved from 0.20744 to 0.20304, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_BN_8_conv_checkpoint/019-0.2030.hdf5\n",
      "36805/36805 [==============================] - 34s 927us/sample - loss: 0.0923 - acc: 0.9748 - val_loss: 0.2030 - val_acc: 0.9371\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0799 - acc: 0.9785\n",
      "Epoch 00020: val_loss did not improve from 0.20304\n",
      "36805/36805 [==============================] - 34s 935us/sample - loss: 0.0800 - acc: 0.9785 - val_loss: 0.2079 - val_acc: 0.9406\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0752 - acc: 0.9799\n",
      "Epoch 00021: val_loss did not improve from 0.20304\n",
      "36805/36805 [==============================] - 34s 934us/sample - loss: 0.0753 - acc: 0.9799 - val_loss: 0.2209 - val_acc: 0.9359\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0694 - acc: 0.9807\n",
      "Epoch 00022: val_loss did not improve from 0.20304\n",
      "36805/36805 [==============================] - 34s 936us/sample - loss: 0.0694 - acc: 0.9807 - val_loss: 0.2067 - val_acc: 0.9415\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0632 - acc: 0.9831\n",
      "Epoch 00023: val_loss did not improve from 0.20304\n",
      "36805/36805 [==============================] - 34s 921us/sample - loss: 0.0632 - acc: 0.9831 - val_loss: 0.2047 - val_acc: 0.9376\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0574 - acc: 0.9849\n",
      "Epoch 00024: val_loss did not improve from 0.20304\n",
      "36805/36805 [==============================] - 34s 918us/sample - loss: 0.0574 - acc: 0.9849 - val_loss: 0.2088 - val_acc: 0.9399\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0533 - acc: 0.9870\n",
      "Epoch 00025: val_loss did not improve from 0.20304\n",
      "36805/36805 [==============================] - 34s 918us/sample - loss: 0.0534 - acc: 0.9870 - val_loss: 0.2157 - val_acc: 0.9357\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0563 - acc: 0.9850\n",
      "Epoch 00026: val_loss did not improve from 0.20304\n",
      "36805/36805 [==============================] - 34s 927us/sample - loss: 0.0564 - acc: 0.9849 - val_loss: 0.2235 - val_acc: 0.9359\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0469 - acc: 0.9882\n",
      "Epoch 00027: val_loss did not improve from 0.20304\n",
      "36805/36805 [==============================] - 34s 927us/sample - loss: 0.0469 - acc: 0.9882 - val_loss: 0.2082 - val_acc: 0.9406\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0400 - acc: 0.9910\n",
      "Epoch 00028: val_loss did not improve from 0.20304\n",
      "36805/36805 [==============================] - 34s 914us/sample - loss: 0.0400 - acc: 0.9910 - val_loss: 0.2282 - val_acc: 0.9315\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0395 - acc: 0.9906\n",
      "Epoch 00029: val_loss did not improve from 0.20304\n",
      "36805/36805 [==============================] - 34s 923us/sample - loss: 0.0395 - acc: 0.9906 - val_loss: 0.2128 - val_acc: 0.9418\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0377 - acc: 0.9912\n",
      "Epoch 00030: val_loss did not improve from 0.20304\n",
      "36805/36805 [==============================] - 34s 921us/sample - loss: 0.0377 - acc: 0.9912 - val_loss: 0.2263 - val_acc: 0.9418\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0307 - acc: 0.9936\n",
      "Epoch 00031: val_loss did not improve from 0.20304\n",
      "36805/36805 [==============================] - 34s 922us/sample - loss: 0.0307 - acc: 0.9936 - val_loss: 0.2191 - val_acc: 0.9399\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0360 - acc: 0.9919\n",
      "Epoch 00032: val_loss did not improve from 0.20304\n",
      "36805/36805 [==============================] - 34s 917us/sample - loss: 0.0362 - acc: 0.9918 - val_loss: 0.2168 - val_acc: 0.9394\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0401 - acc: 0.9888\n",
      "Epoch 00033: val_loss did not improve from 0.20304\n",
      "36805/36805 [==============================] - 34s 914us/sample - loss: 0.0401 - acc: 0.9888 - val_loss: 0.2044 - val_acc: 0.9436\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0249 - acc: 0.9953\n",
      "Epoch 00034: val_loss did not improve from 0.20304\n",
      "36805/36805 [==============================] - 34s 916us/sample - loss: 0.0249 - acc: 0.9953 - val_loss: 0.2194 - val_acc: 0.9401\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0241 - acc: 0.9946- ETA: 0s - loss: 0.0242 - acc: 0.994\n",
      "Epoch 00035: val_loss did not improve from 0.20304\n",
      "36805/36805 [==============================] - 34s 936us/sample - loss: 0.0242 - acc: 0.9946 - val_loss: 0.2302 - val_acc: 0.9380\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0280 - acc: 0.9934\n",
      "Epoch 00036: val_loss did not improve from 0.20304\n",
      "36805/36805 [==============================] - 35s 941us/sample - loss: 0.0283 - acc: 0.9933 - val_loss: 0.2573 - val_acc: 0.9315\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0303 - acc: 0.9932\n",
      "Epoch 00037: val_loss did not improve from 0.20304\n",
      "36805/36805 [==============================] - 34s 917us/sample - loss: 0.0303 - acc: 0.9932 - val_loss: 0.2386 - val_acc: 0.9413\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0187 - acc: 0.9969\n",
      "Epoch 00038: val_loss did not improve from 0.20304\n",
      "36805/36805 [==============================] - 34s 914us/sample - loss: 0.0188 - acc: 0.9968 - val_loss: 0.2344 - val_acc: 0.9362\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0280 - acc: 0.9928\n",
      "Epoch 00039: val_loss did not improve from 0.20304\n",
      "36805/36805 [==============================] - 34s 917us/sample - loss: 0.0280 - acc: 0.9928 - val_loss: 0.2191 - val_acc: 0.9432\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0166 - acc: 0.9970\n",
      "Epoch 00040: val_loss did not improve from 0.20304\n",
      "36805/36805 [==============================] - 34s 918us/sample - loss: 0.0166 - acc: 0.9970 - val_loss: 0.2355 - val_acc: 0.9397\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0183 - acc: 0.9963\n",
      "Epoch 00041: val_loss did not improve from 0.20304\n",
      "36805/36805 [==============================] - 34s 911us/sample - loss: 0.0183 - acc: 0.9963 - val_loss: 0.2319 - val_acc: 0.9420\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0164 - acc: 0.9971\n",
      "Epoch 00042: val_loss did not improve from 0.20304\n",
      "36805/36805 [==============================] - 34s 915us/sample - loss: 0.0164 - acc: 0.9971 - val_loss: 0.2367 - val_acc: 0.9404\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0212 - acc: 0.9951\n",
      "Epoch 00043: val_loss did not improve from 0.20304\n",
      "36805/36805 [==============================] - 34s 913us/sample - loss: 0.0212 - acc: 0.9951 - val_loss: 0.2486 - val_acc: 0.9350\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0199 - acc: 0.9957\n",
      "Epoch 00044: val_loss did not improve from 0.20304\n",
      "36805/36805 [==============================] - 34s 927us/sample - loss: 0.0199 - acc: 0.9957 - val_loss: 0.2539 - val_acc: 0.9385\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0143 - acc: 0.9975\n",
      "Epoch 00045: val_loss did not improve from 0.20304\n",
      "36805/36805 [==============================] - 34s 933us/sample - loss: 0.0144 - acc: 0.9975 - val_loss: 0.2258 - val_acc: 0.9432\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0244 - acc: 0.9935\n",
      "Epoch 00046: val_loss did not improve from 0.20304\n",
      "36805/36805 [==============================] - 34s 921us/sample - loss: 0.0244 - acc: 0.9935 - val_loss: 0.2240 - val_acc: 0.9406\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0172 - acc: 0.9964\n",
      "Epoch 00047: val_loss did not improve from 0.20304\n",
      "36805/36805 [==============================] - 34s 928us/sample - loss: 0.0172 - acc: 0.9964 - val_loss: 0.2253 - val_acc: 0.9413\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0146 - acc: 0.9971\n",
      "Epoch 00048: val_loss did not improve from 0.20304\n",
      "36805/36805 [==============================] - 34s 923us/sample - loss: 0.0146 - acc: 0.9971 - val_loss: 0.2937 - val_acc: 0.9322\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0177 - acc: 0.9955\n",
      "Epoch 00049: val_loss did not improve from 0.20304\n",
      "36805/36805 [==============================] - 34s 926us/sample - loss: 0.0177 - acc: 0.9955 - val_loss: 0.2292 - val_acc: 0.9443\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0113 - acc: 0.9978\n",
      "Epoch 00050: val_loss did not improve from 0.20304\n",
      "36805/36805 [==============================] - 34s 925us/sample - loss: 0.0113 - acc: 0.9977 - val_loss: 0.2270 - val_acc: 0.9422\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0147 - acc: 0.9967\n",
      "Epoch 00051: val_loss did not improve from 0.20304\n",
      "36805/36805 [==============================] - 34s 929us/sample - loss: 0.0147 - acc: 0.9967 - val_loss: 0.2365 - val_acc: 0.9418\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0098 - acc: 0.9983\n",
      "Epoch 00052: val_loss did not improve from 0.20304\n",
      "36805/36805 [==============================] - 34s 931us/sample - loss: 0.0098 - acc: 0.9983 - val_loss: 0.2392 - val_acc: 0.9455\n",
      "Epoch 53/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0130 - acc: 0.9977\n",
      "Epoch 00053: val_loss did not improve from 0.20304\n",
      "36805/36805 [==============================] - 34s 929us/sample - loss: 0.0131 - acc: 0.9976 - val_loss: 0.2863 - val_acc: 0.9336\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0216 - acc: 0.9940\n",
      "Epoch 00054: val_loss did not improve from 0.20304\n",
      "36805/36805 [==============================] - 34s 933us/sample - loss: 0.0217 - acc: 0.9940 - val_loss: 0.2385 - val_acc: 0.9455\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0177 - acc: 0.9952\n",
      "Epoch 00055: val_loss did not improve from 0.20304\n",
      "36805/36805 [==============================] - 34s 925us/sample - loss: 0.0177 - acc: 0.9951 - val_loss: 0.2327 - val_acc: 0.9434\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0134 - acc: 0.9973\n",
      "Epoch 00056: val_loss did not improve from 0.20304\n",
      "36805/36805 [==============================] - 34s 928us/sample - loss: 0.0135 - acc: 0.9973 - val_loss: 0.2282 - val_acc: 0.9460\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0186 - acc: 0.9951\n",
      "Epoch 00057: val_loss did not improve from 0.20304\n",
      "36805/36805 [==============================] - 34s 925us/sample - loss: 0.0187 - acc: 0.9951 - val_loss: 0.2303 - val_acc: 0.9446\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0127 - acc: 0.9974\n",
      "Epoch 00058: val_loss did not improve from 0.20304\n",
      "36805/36805 [==============================] - 34s 927us/sample - loss: 0.0127 - acc: 0.9974 - val_loss: 0.2349 - val_acc: 0.9406\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0070 - acc: 0.9989\n",
      "Epoch 00059: val_loss did not improve from 0.20304\n",
      "36805/36805 [==============================] - 34s 929us/sample - loss: 0.0070 - acc: 0.9989 - val_loss: 0.2456 - val_acc: 0.9420\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0090 - acc: 0.9981\n",
      "Epoch 00060: val_loss did not improve from 0.20304\n",
      "36805/36805 [==============================] - 34s 929us/sample - loss: 0.0091 - acc: 0.9981 - val_loss: 0.2400 - val_acc: 0.9427\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0204 - acc: 0.9943\n",
      "Epoch 00061: val_loss did not improve from 0.20304\n",
      "36805/36805 [==============================] - 34s 924us/sample - loss: 0.0206 - acc: 0.9942 - val_loss: 0.2469 - val_acc: 0.9446\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0216 - acc: 0.9937\n",
      "Epoch 00062: val_loss did not improve from 0.20304\n",
      "36805/36805 [==============================] - 34s 930us/sample - loss: 0.0216 - acc: 0.9938 - val_loss: 0.2461 - val_acc: 0.9436\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0071 - acc: 0.9989\n",
      "Epoch 00063: val_loss did not improve from 0.20304\n",
      "36805/36805 [==============================] - 34s 929us/sample - loss: 0.0073 - acc: 0.9989 - val_loss: 0.2617 - val_acc: 0.9427\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0175 - acc: 0.9950\n",
      "Epoch 00064: val_loss did not improve from 0.20304\n",
      "36805/36805 [==============================] - 34s 932us/sample - loss: 0.0175 - acc: 0.9950 - val_loss: 0.2551 - val_acc: 0.9406\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0060 - acc: 0.9992\n",
      "Epoch 00065: val_loss did not improve from 0.20304\n",
      "36805/36805 [==============================] - 34s 922us/sample - loss: 0.0060 - acc: 0.9992 - val_loss: 0.2416 - val_acc: 0.9471\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0068 - acc: 0.9990\n",
      "Epoch 00066: val_loss did not improve from 0.20304\n",
      "36805/36805 [==============================] - 34s 922us/sample - loss: 0.0068 - acc: 0.9990 - val_loss: 0.2845 - val_acc: 0.9343\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0160 - acc: 0.9956\n",
      "Epoch 00067: val_loss did not improve from 0.20304\n",
      "36805/36805 [==============================] - 34s 919us/sample - loss: 0.0160 - acc: 0.9956 - val_loss: 0.2462 - val_acc: 0.9425\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0065 - acc: 0.9990\n",
      "Epoch 00068: val_loss did not improve from 0.20304\n",
      "36805/36805 [==============================] - 34s 929us/sample - loss: 0.0066 - acc: 0.9990 - val_loss: 0.2714 - val_acc: 0.9408\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0131 - acc: 0.9966\n",
      "Epoch 00069: val_loss did not improve from 0.20304\n",
      "36805/36805 [==============================] - 34s 928us/sample - loss: 0.0132 - acc: 0.9966 - val_loss: 0.2924 - val_acc: 0.9345\n",
      "\n",
      "1D_CNN_custom_multi_3_concat_ch_32_BN_8_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEKCAYAAAAB0GKPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzs3Xl8VNXd+PHPmT2TmeyBAAHCvkPYsbhWUdQWtVZxa9VWfbrYPtbWVrtp7dPWavvU2urPqqUupS7F/RFFsVJQoRIQBFEh7AkBsmcmM5PMcn5/nJlkgCQEyJAQvu/X67xuctdz78yc7z33nnuu0lojhBBCHI6luzMghBDixCABQwghRKdIwBBCCNEpEjCEEEJ0igQMIYQQnSIBQwghRKdIwBBCCNEpEjCEEEJ0igQMIYQQnWLr7gx0pby8PF1UVNTd2RBCiBPGmjVrqrTW+Z2Zt1cFjKKiIkpKSro7G0IIccJQSu3s7LxySUoIIUSnSMAQQgjRKRIwhBBCdEqvuofRlnA4TFlZGaFQqLuzckJyuVwUFhZit9u7OytCiG7W6wNGWVkZXq+XoqIilFLdnZ0Titaa6upqysrKGDJkSHdnRwjRzXr9JalQKERubq4Ei6OglCI3N1dqZ0II4CQIGIAEi2Mgx04IkXBSBIzDaWraQyRS393ZEEKIHk0CBtDcvJdIpCEl666rq+Ohhx46qmUvuOAC6urqOj3/XXfdxe9+97uj2pYQQhyOBAxAKSsQTcm6OwoYkUikw2UXL15MVlZWKrIlhBBHTAIGABa0jqVkzbfffjtbt26luLiY2267jWXLlnHaaacxb948xo4dC8DFF1/M1KlTGTduHI888kjLskVFRVRVVbFjxw7GjBnDjTfeyLhx4zj33HMJBoMdbnfdunXMmjWLiRMncskll1BbWwvAAw88wNixY5k4cSJXXHEFAP/+978pLi6muLiYyZMn4/P5UnIshBAntl7frDbZli234PevO2R8LNYIWLBY0o54nR5PMSNG3N/u9HvuuYeNGzeybp3Z7rJly1i7di0bN25saaq6YMECcnJyCAaDTJ8+nUsvvZTc3NyD8r6Fp59+mkcffZTLL7+c559/nmuuuabd7X71q1/lT3/6E2eccQY///nP+cUvfsH999/PPffcw/bt23E6nS2Xu373u9/x4IMPMnv2bPx+Py6X64iPgxCi95MaBgAK0MdtazNmzDjguYYHHniASZMmMWvWLHbv3s2WLVsOWWbIkCEUFxcDMHXqVHbs2NHu+uvr66mrq+OMM84A4Nprr2X58uUATJw4kauvvpq///3v2GzmfGH27NnceuutPPDAA9TV1bWMF0KIZCdVydBeTSAQ2IzWUdLTxxyXfKSnp7f8vWzZMpYuXcrKlStxu92ceeaZbT734HQ6W/62Wq2HvSTVntdee43ly5fz6quv8qtf/YoNGzZw++23c+GFF7J48WJmz57NkiVLGD169FGtXwjRe0kNg9Te9PZ6vR3eE6ivryc7Oxu3282nn37KqlWrjnmbmZmZZGdns2LFCgCeeuopzjjjDGKxGLt37+ass87it7/9LfX19fj9frZu3cqECRP40Y9+xPTp0/n000+POQ9CiN7npKphtC91N71zc3OZPXs248eP5/zzz+fCCy88YPrcuXN5+OGHGTNmDKNGjWLWrFldst0nnniCb3zjGwQCAYYOHcrf/vY3otEo11xzDfX19Wit+e53v0tWVhY/+9nPeOedd7BYLIwbN47zzz+/S/IghOhdlNbH79p9qk2bNk0f/AKlTz75hDFjOr7UFArtIhyuwestTmX2TlidOYZCiBOTUmqN1npaZ+aVS1KAOQypuSQlhBC9hQQMQCkLoFN2WUoIIXqDlAUMpdQCpdR+pdTGdqbfppRaF08blVJRpVROfNoOpdSG+LSUv6Tb3PRGAoYQQnQglTWMx4G57U3UWt+ntS7WWhcDdwD/1lrXJM1yVnx6p66tHZvEYZCAIYQQ7UlZwNBaLwdqDjujcSXwdKrycjjmkhRoLfcxhBCiPd1+D0Mp5cbURJ5PGq2BN5VSa5RSN6U+F9b4UGoYQgjRnm4PGMAXgfcOuhx1qtZ6CnA+8G2l1OntLayUukkpVaKUKqmsrDyqDLTWMHpGwPB4PEc0XgghjoeeEDCu4KDLUVrr8vhwP/AiMKO9hbXWj2itp2mtp+Xn5x9VBhI3vaVprRBCtK9bA4ZSKhM4A3g5aVy6Usqb+Bs4F2izpVXXSV0N4/bbb+fBBx9s+T/xkiO/38/ZZ5/NlClTmDBhAi+//HIHazmQ1prbbruN8ePHM2HCBJ599lkAKioqOP300ykuLmb8+PGsWLGCaDTKdddd1zLvH/7why7fRyHEySFlXYMopZ4GzgTylFJlwJ2AHUBr/XB8tkuAN7XWjUmL9gVejL9L2gb8Q2v9Rpdk6pZbYN2h3ZtbiJEWbcRicYGyH9k6i4vh/va7N58/fz633HIL3/72twF47rnnWLJkCS6XixdffJGMjAyqqqqYNWsW8+bN69Q7tF944QXWrVvH+vXrqaqqYvr06Zx++un84x//4LzzzuMnP/kJ0WiUQCDAunXrKC8vZ+NGE3OP5A1+QgiRLGUBQ2t9ZSfmeRzT/DZ53DZgUmpy1Z5EId313aRMnjyZ/fv3s2fPHiorK8nOzmbgwIGEw2F+/OMfs3z5ciwWC+Xl5ezbt4+CgoLDrvPdd9/lyiuvxGq10rdvX8444wxWr17N9OnT+drXvkY4HObiiy+muLiYoUOHsm3bNr7zne9w4YUXcu6553b5PgohTg4nV+eD7dUEdIygfy0OxwCczn5dvtnLLruMRYsWsXfvXubPnw/AwoULqaysZM2aNdjtdoqKitrs1vxInH766SxfvpzXXnuN6667jltvvZWvfvWrrF+/niVLlvDwww/z3HPPsWDBgq7YLSHESaYn3PTudqaVlCJVN73nz5/PM888w6JFi7jssssA0615nz59sNvtvPPOO+zcubPT6zvttNN49tlniUajVFZWsnz5cmbMmMHOnTvp27cvN954IzfccANr166lqqqKWCzGpZdeyv/8z/+wdu3alOyjEKL3O7lqGB1KXRfn48aNw+fzMWDAAPr1MzWYq6++mi9+8YtMmDCBadOmHdELiy655BJWrlzJpEmTUEpx7733UlBQwBNPPMF9992H3W7H4/Hw5JNPUl5ezvXXX08sZvbtN7/5TUr2UQjR+0n35nF+/0dYrRmkpRWlKHcnLuneXIjeS7o3PwrmspQ8hyGEEO2RgNEidZekhBCiN5CAEZfK93oLIURvIAEDIBxGxZTUMIQQogMSMAA++ghbVbMEDCGE6IAEDACrFRUDuSQlhBDtk4ABYLGgYqnpfLCuro6HHnroqJa94IILpO8nIUSPIQEDwGqNvzspSlc/l9JRwIhEIh0uu3jxYrKysro0P0IIcbQkYEC8hpEIFF0bMG6//Xa2bt1KcXExt912G8uWLeO0005j3rx5jB07FoCLL76YqVOnMm7cOB555JGWZYuKiqiqqmLHjh2MGTOGG2+8kXHjxnHuuecSDAYP2darr77KzJkzmTx5Mueccw779u0DwO/3c/311zNhwgQmTpzI88+blxu+8cYbTJkyhUmTJnH22Wd36X4LIXqfk6prkHZ6N4fgYNAxok6N1drG9A4cpndz7rnnHjZu3Mi6+IaXLVvG2rVr2bhxI0OGDAFgwYIF5OTkEAwGmT59Opdeeim5ubkHrGfLli08/fTTPProo1x++eU8//zzXHPNNQfMc+qpp7Jq1SqUUjz22GPce++9/P73v+eXv/wlmZmZbNiwAYDa2loqKyu58cYbWb58OUOGDKGmprOvXxdCnKxOqoDRPpWKns3bNWPGjJZgAfDAAw/w4osvArB79262bNlySMAYMmQIxcXFAEydOpUdO3Ycst6ysjLmz59PRUUFzc3NLdtYunQpzzzzTMt82dnZvPrqq5x++ukt8+Tk5HTpPgohep+TKmC0WxPYvhftq8c/JILbPQ6rNS2l+UhPT2/5e9myZSxdupSVK1fidrs588wz2+zm3Ol0tvxttVrbvCT1ne98h1tvvZV58+axbNky7rrrrpTkXwhxcpJ7GGBuekdNCymtu7ZprdfrxefztTu9vr6e7Oxs3G43n376KatWrTrqbdXX1zNgwAAAnnjiiZbxc+bMOeA1sbW1tcyaNYvly5ezfft2ALkkJYQ4LAkYABYLtNz07tqmtbm5ucyePZvx48dz2223HTJ97ty5RCIRxowZw+23386sWbOOelt33XUXl112GVOnTiUvL69l/E9/+lNqa2sZP348kyZN4p133iE/P59HHnmEL33pS0yaNKnlxU5CCNGelHVvrpRaAHwB2K+1Ht/G9DOBl4Ht8VEvaK3vjk+bC/wRsAKPaa3v6cw2j7p784oKKC/HNxJcacOw27M7s7mThnRvLkTv1VO6N38cmHuYeVZorYvjKREsrMCDwPnAWOBKpdTYFObT1DAgXrmQ7kGEEKItKQsYWuvlwNFcGJ8BlGqtt2mtm4FngIu6NHMHi7elTdXT3kII0Rt09z2MU5RS65VSryulxsXHDQB2J81TFh/XJqXUTUqpEqVUSWVl5dHlIl7DkP6khBCifd0ZMNYCg7XWk4A/AS8dzUq01o9oradprafl5+cfXU4ST+tJDUMIIdrVbQFDa92gtfbH/14M2JVSeUA5MDBp1sL4uNRpqWGoLm9WK4QQvUW3BQylVIFSSsX/nhHPSzWwGhihlBqilHIAVwCvpDQziXsY2oLc9BZCiLal7ElvpdTTwJlAnlKqDLgTsANorR8Gvgx8UykVAYLAFdq08Y0opW4GlmCa1S7QWn+cqnwCB9Uwuj9geDwe/H5/d2dDCCEOkLKAobW+8jDT/wz8uZ1pi4HFqchXm1pqGAotN72FEKJN3d1KqmdIYQ3j9ttvP6Bbjrvuuovf/e53+P1+zj77bKZMmcKECRN4+eWXD7uu9rpBb6ub8va6NBdCiKN1UnU+eMsbt7Bub1v9mwM+H9puIWZXWK3uTq+zuKCY++e237/5/PnzueWWW/j2t78NwHPPPceSJUtwuVy8+OKLZGRkUFVVxaxZs5g3bx7x2zptaqsb9Fgs1mY35W11aS6EEMfipAoYHWopqLu2q5TJkyezf/9+9uzZQ2VlJdnZ2QwcOJBwOMyPf/xjli9fjsVioby8nH379lFQUNDuutrqBr2ysrLNbsrb6tJcCCGOxUkVMDqqCbB+PRGPhVBf8HgmdOl2L7vsMhYtWsTevXtbOvlbuHAhlZWVrFmzBrvdTlFRUZvdmid0tht0IYRIFbmHkWC1puxJ7/nz5/PMM8+waNEiLrvsMsB0Rd6nTx/sdjvvvPMOO3fu7HAd7XWD3l435W11aS6EEMdCAkaCxZKyJ73HjRuHz+djwIAB9OvXD4Crr76akpISJkyYwJNPPsno0aM7XEd73aC31015W12aCyHEsUhZ9+bd4ai7Nwf47DNisSYaC5vxeKZ2ePP5ZCPdmwvRe/WU7s1PLCl8iZIQQvQGEjASrFZUPGBIf1JCCHGokyJgdOqyW1INoyd0D9JT9KZLlkKIY9PrA4bL5aK6uvrwBZ/VCtFEoJCAASZYVFdX43K5ujsrQogeoNc/h1FYWEhZWRmHfblSXR3U1xNS4HB8hsUihSSYgFtYWNjd2RBC9AC9PmDY7faWp6A7dN998MMfsmIxjJ3xOrm5h3sduRBCnFx6/SWpTvN6AbAGIRqVrsWFEOJgEjASPB4ArAEJGEII0RYJGAlSwxBCiA5JwEhIqmHEYo3dnBkhhOh5UhYwlFILlFL7lVIb25l+tVLqI6XUBqXU+0qpSUnTdsTHr1NKlbS1fJdrqWEoqWEIIUQbUlnDeBzoqKnRduAMrfUE4JfAIwdNP0trXdzZPk6OWbyGYQ+5JGAIIUQbUvlO7+VKqaIOpr+f9O8qoHsb+8drGPYmhwQMIYRoQ0+5h/F14PWk/zXwplJqjVLqpo4WVErdpJQqUUqVHPbhvI601DDsEjCEEKIN3f7gnlLqLEzAODVp9Kla63KlVB/gLaXUp1rr5W0tr7V+hPjlrGnTph19x0fxgGGTgCGEEG3q1hqGUmoi8Bhwkda6OjFea10eH+4HXgRmpDwzdjs4ndiCVqJRaSUlhBAH67aAoZQaBLwAfEVrvTlpfLpSypv4GzgXaLOlVZfzerGGLFLDEEKINqTskpRS6mngTCBPKVUG3AnYAbTWDwM/B3KBh+Jvt4vEW0T1BV6Mj7MB/9Bav5GqfB7A48EW0BIwhBCiDalsJXXlYabfANzQxvhtwKRDlzgOvF6swXoJGEII0Yae0kqqZ/B4sEoNQwgh2iQBI5nHgyUYlYAhhBBtkICRzOvF0hhB6zCxWHN350YIIXoUCRjJPB4sgTCANK0VQoiDSMBI5vViaTQ1C7ksJYQQB5KAkczjQTU2ARIwhBDiYBIwknm9qOYIKiwBQwghDiYBI1niJUry1j0hhDiEBIxkiZcoBeSmtxBCHEwCRjKpYQghRLskYCSL1zBsEjCEEOIQEjCSSQ1DCCHaJQEj2QH3MCRgCCFEMgkYyRI1jJBVAoYQQhxEAkayeA3DHnISi0krKSGESCYBI1m8hmEPOaSGIYQQB5GAkcztBqWwBW0SMIQQ4iApDRhKqQVKqf1KqTbfya2MB5RSpUqpj5RSU5KmXauU2hJP16Yyn0kZMq9plXsYQghxiFTXMB4H5nYw/XxgRDzdBPw/AKVUDuYd4DOBGcCdSqnslOY0wevFFrRIwBBCiIN0KmAopf5bKZURrxH8VSm1Vil17uGW01ovB2o6mOUi4EltrAKylFL9gPOAt7TWNVrrWuAtOg48XcfjwRpSEjCEECkXjZqkdXfnpHNsnZzva1rrPyqlzgOyga8ATwFvHuP2BwC7k/4vi49rb3zqeb1YA37pS6qXCIehthbq6yEWA6sVLBYzBPNjjUTMMBwGv9/MW18PDQ1mvNttUnq6SdnZkJMDubmQlgbNzbB1K3z2mUk7dph122xgt5uhwwFOJ7hcZuh0miugycnvh/37Yd8+M2xogLw86NcPCgpMamqCigqT9uyBmhqzLzZba4rFzL5EImaotWnP4fWaocdjjkEk0jpPczMEAtDYaIbBIGRktG63b1+z73V15njW1Ji/m5pa1xGJmP12u81xcbvN/gaDZl98PjMMh1uPh8NhUlNT63YDAXPcE/tjtZp5cnKgTx/IzzdDl8vku6nJDEMhs43kpJTZb6/X7E9amtnHhobWPDU2mu0mUnOz2abd3vr5ZWWZY9C3rzkeubmteUukykrYuRN27TLD2lrz+SWW6dPHbCvx+VVUmO0nU8rkMz/fLJuXZ45TdTVUVZlUXW3GZWRAZqYZFhbCokWp/z11NmCo+PAC4Cmt9cdKKdXRAseLUuomzOUsBg0adOwr9HiwNsakhpECWrcW0IlCJrmAThTSyQVXY6OZx+drHYbDrQWf12sKJr//wB9VTY35wTamOO67XCY/0WjruNxcU4AkF9rNzaYgPxyr1RQsffqYgmDTJnj7bVM4J1gsZnr//mZbWpttNDWZ42CxmILO6TTHKBGMdu1qLUihtTBMBLb0dHMsc3LMfjU0wCefwDvvmOMJZl1ZWSZoZmWZAthmM0O73eQlGDSFZ6IATkszn1NmpinY7PbW/DY3m7/T02HAgNZAY7W2nn1HIma+6mrYvh3+8x+z/mjU7Gsi8DidrcHB6zUFtdZmf3fsMMNg0GwrI8PM07+/OUZpaa3J4Wg9gUikujrYuxc2bIClSw/8PBLsdhg0yKQ5c8wxqqoyJwC7dkFJidm3fv1g0iSYO9cca6VMPmMxk+rrW7/HZWUmEOblwYgRcMopZpnmZvP5JH4ziWCdap0NGGuUUm8CQ4A7lFJeoBNf/8MqBwYm/V8YH1cOnHnQ+GVtrUBr/QjwCMC0adOOvWLn9WKpjBGNBo95Vb1JoiBIFOR1daYQSaTqanNWnJwSP9BEamo6um0nAkPiDNluh/JyUwgmksdjflS5uVBUBFOmmB9WomDLyjKFSyxmCoNYzOxT8lmizdZasCXO3qzWAwOY3996hl1dbZLLBaNHw6hRMHKkWa4tkYj58Tc1maR1a4rFzLYT+TxYKGQKHofDBItEDel4aWpqrXW0lb/jLVG42jpbgqVg+4mAlkiJmltv1tnD/XWgGNimtQ7Eb0pf3wXbfwW4WSn1DOYGd73WukIptQT4ddKN7nOBO7pge4fn8WANRIhG/Wit6SEVqS5XX28uo+zaZQrB5MsBVVXmckd5uRnu22cKzMNdZ/V4Ws+OBw0yBafL1Xrm5nQeWM1PnNVmZrYW0hkZrZd+Epc2estHYLO11oyOlMsFgwd3fZ46K3EZraewWLq3cE5s327vvjx0h84GjFOAdVrrRqXUNcAU4I+HW0gp9TSmppCnlCrDtHyyA2itHwYWYy5zlQIB4kFIa12jlPolsDq+qru11h3dPO86Xi+WQATQhMOVOBx9jstmu1JTkznzrayE3bsPvK66fbsJFNXV7S/vcpnLAwMGwPTppmrv9bYW5Onp5kw4J6f1Wn5OjinchRC9V2cDxv8DJimlJgHfBx4DngTO6GghrfWVh5mugW+3M20BsKCT+es6Hg+WxjAAweDWHhcwGhrgo4/g44/N2f/eva1p/35TO/C3cfvF4TBn/UVF8OUvw7BhJg0e3HozMHFG73L1nrP6VGpsbqTCX0E0FiU/PZ8sVxYWlZrTXq01VYEqYjpGrjsXm6Xtn66OVwOPpWYcjobZ17gPm8VGpjMTl811VOtrjjbzYcWHaDQehwevw4vH4SHNnoZVWbFarFiVFY2mNljL/sb97Gvcx/7G/WQ4Mzht0GmkO9IPWW8oEuK9Xe9R4a9oWa/X6SXNloa/2U9dqI66UB31TfXEdAyXzYXL5iLNlobb7qZPeh8KPAXkp+e3eRwjsQiRWIRwNNzydzASpLG5kcZwI/5mP02RJuxWO3aLHYfVgd1qhg6ro2Wc0+bE4/DgtDpbjl8kFmF77XY+q/6Mz6o+IxgJMmfoHKYPmN7mdyemYzQ2N5JmT2v3Mz+eOpuDiNZaK6UuAv6stf6rUurrqcxYt/F6UY1BiEEwWEpm5indko26utZWN5s3mwCxfr2pISQoZVpTJFqyjBzZ2rIiN9cMBw40QaFPH2iKBqkJ1hCKhAhGgoQiIRrDAcpDddRW1VIbqqU2WEtURw/44uek5TB//Hzcdnebed1SvYWnPnqKvf69VAYqqQpUURWowm13U5hRSKG3kMKMQrxOLzvrdrK9bjvbarexo24HTpuTvul9KfAUUOApIMOZga/ZR0NTAw1NDfiafGQ4M+jn7Uc/j0k5aTloNFprNJpoLMr+xv2UNZRR5iujrKGMmI4xrd80ZhbOZMaAGYzNH8v22u0s37mcFbtWsGLXCnbU7ThkX9JsaaQ70km3p5PuSMdlc7UUbokf9P7G/VT4KvA1H9jExaqs5LnzyHPnkeHMwOv0thRoBZ4CRuWOYnTeaEbljSLTmcmmyk28t/s93tv9Hit3ryQcCzM4czCDMgcxOHMw2WnZlNaUsqlyE5sqN1EdNNVChSI7LZt8dz4Zzgwaw40HHC+lVEvh6La7yXBmMDhrMEWZRRRlFTE4azDhaJjKQCWVjZVUBirZ17iP8oZyyhrK2Ovfi6b1+qPD6iDTmYnb7m4tTGNhYjrGiJwRzBxgjvHMwpk4rU7eKH2D10tf5+3tb+NvPvrGIw6rg9MGncZ5w85jVuEs1lasZcnWJSzbsYxg5NjvMSoU+en5OKwOgmHzewhFQkR19PALHwGbxYbH4cFtd1PZWEk4Fj5g+s/e+Rl90/ty4YgLOX/E+fib/XxY8SEf7v2Q9fvW09DUAIDdYm/5TAdlDmJk7khG5o5kRM4IRuaOpLigOOWX0JXuRANgpdS/gTeArwGnAfuB9VrrCSnN3RGaNm2aLikpObaV3Hcf/PCHrFgMhaPvZMiQu7okb+2JRk1QWLcOStaFeLd0HZ/WbMBXmwaBXAjkYWnKZXBRlIETd5AzdAf2/O3E0vdQkJ1BgbcPfdP70ie9DzaLjbpQHbWhWupCdVQHqtnVsItd9bvYWbeTykBlp/JkURZi+sA2DQMzBnLPOfdw5fgrW76UviYf/7P8f/jDqj8Q1VHy3Hnku/PJT88nz51HIBwwhXhDGTVBc0XRZXNRlFXE0OyhFGUWEY6F2evf25IamhrIcGa0JI/DQ0NTA3t8e9jr33vIjy1ZTlqOCVAZhURiEVaXr6Y2VAuYwjxREOS58zht0GmMyRtzwFmdRhMMB1vOIhvDjTRFmojqKDEdIxqLotHku/Pp5+lHf29/+nn7YVXWAwrfqkAVvmYfviYf/mY/vmYfFb6KA/LusDpojjYD0De9L58b+Dk8Dg8763eyq34XZQ1lRGIRctJyGJs/lrF5YxmTPwaH1dGyncpAJQ1NDXgcHjIcGS1BSmtNIBwgEA4QjASpDdW2BOpE4ZP8Weem5dInvU/LsSvMKKS/tz/RWLTlTL0+VE8gEsBusWOz2LBZbGit+bjyY0r2lNAYPrA52qDMQZw//HzmDJ2D2+5uOQ7+Zj/BcJCojhKNRYnqKFprctJy6Osx3+M+6X2o8FWwZOsS3ih9g48rP25Z78jckZw37DzOG3YeI3JH0Njc2HKsg5EgHoeHLFcWmc5MMl2Z2Cw2c4IUDwj+Zj/7G/ez17+XfY37qPBVEIlFTA3EntZSG0neT6vF2uaJRCQWoTnaTDgapinaRDgaJhwLt4xLbC+x3/5mP33S+zAqdxSj8kYxKncUSile3/I6r25+lTdK36C+qR4At93NxL4TmVwwmaKsIpoiTQQjQQLhAP5mPzvqdrC5ejO7G3a3fKcrb+vc7/tgSqk1WutpnZq3kwGjALgKWK21XqGUGgScqbV+8qhymCJdEjAefhi++U1KXh1A+rCzGDPmqa7JXJzfb5oFLn83xltrP+XDylWEcj+A/quh70dgPXz7OJvFRt/0vvib/S1fsLZ4HB4GZgxsOWMdnDWYPHceaba0lh/4edWgAAAgAElEQVRImi2NLFcW2WnZLT80q8VKNBYlHAsTjoZZU7GG77/5fdZWrGXmgJn873n/S2lNKT9a+iP2+vdyffH1/PrsX1PgKWg3L4FwAF+Tj/z0/KO+bBPTMWqCNdSF6lAolFJYlAWLspDnzjukBqS1prSmlP+U/4eP9n3EsOxhnD74dEbnjT7ujRkOvhRR4a9gUt9JzB40myFZQw7JTzQWxdfsI9OZ2aV5rQvVsbNuJw6rg/z0fLJd2Vgtx9bkKhqLsqlyEx+Uf0BjuJE5Q+d06TEuayhjdflqiguKGZI9pEvW2ROFo2FW71lNTloOI3JGdOpzCYQDlNaUUhWo4vNDPn9U2+3ygBFfaV9gevzfD7TW+48qdynUJQHj73+Hr3yFTS/NJDTQwpQp7x9zvup8zTz24mae/dfHrN2zgVjBBzDgP+AyZ3tuSyYTcqdx+vDpzBo4neKCYqKxKFWBKqqD1VQFqrAoC0VZ5pJCf2//luuZoUiIykZzSSEai5Kdlk22K5tMVyYOq+OY854Q0zGeWv8Ud7x9BxX+CgBmDpjJA+c/wIwBM7psO0KI4+tIAkan7mEopS4H7sM8C6GAPymlbtNaH4dnC4+zeJtHd6wftcHOB4tgOMjznzzP9trtVAerqWysYvPuarbs30W9bbOpOQwBVWRlaPoEzhh2JacPncWswlmMzB3Z5ln3iNwRh92uy+ZiYOZABmYOPOy8x8KiLFxbfC2Xjr2UR9c8Sp/0Plw54cqU3eQVQvQ8nb3p/RNgeqJWoZTKB5YCvS9gxF+ilBbpQzi8n0jEh83mbXf26kA1D65+kD9/8OeWewROnUHEl0vUl4c9NJziPhdx4fTxfOm0cYztMwqXzXVcdiUVPA4P3zvle92dDSFEN+hswLAcdAmqmt76Lo14DcMZzgVM01qvt/iQ2fb49vCbFb/hrx/+lWAkyOkFX6BgxQ/YsPgUIjg4/3y47jr4whd61gNPQghxtDobMN6IP339dPz/+ZiH7nqfeA3D2ZwBQCh0aMB4ZuMzfOu1b+Fv9nPV+GtI/+gHPHLzWHJy4L574JprTDNXIYToTToVMLTWtymlLgVmx0c9orV+MXXZ6kbxGoaj2QyDwa0tk2qDtXxr8bd4ZuMzzBwwkztGPcWd3xnB+vVw1VXwwAPm+QchhOiNOv3ooNb6eeD5FOalZ4jXMKyBCHZ7XkvAeHvb21z70rXsa9zHL8/6JZkbbufLn7eRlwcvvQQXXdSdmRZCiNTrMGAopXxAW+1uFaZnj4yU5Ko7JXqG8/tJSxtOMLiV0ppSzl94PsNyhvHyFS+z4rmpfPd78MUvwuOPm36UhBCit+swYGit228e1FslXiTg8+FyDaO+/l1+/faPcVgdvHPtOyz8SwE/+IHpj+kf/zj5eqsUQpy8emdLp2Pl8cRrGMP4sHIX/9z0T75/yvd58iETLC6/XIKFEOLkIwGjLV5vvIYxlL9s0+S7c7H+5wf86EdwxRWwcKEECyHEyaf7+8vtieI1jOV79/FRPdw29hruvNzL5ZfDU09131u+hBCiO0kNoy1eL1FfA3e/v4DCNKh+82psNrj/fgkWQoiTlwSMtng8PJG5nU1Vn3HdQC+LnhvHpZeal7cLIcTJKqUBQyk1Vyn1mVKqVCl1exvT/6CUWhdPm5VSdUnToknTXkllPg8WyEjj58N2MXPATKIbv0dDg5ubbz6eORBCiJ4nZRdYlFJW4EFgDlAGrFZKvaK13pSYR2v9vaT5vwNMTlpFUGt9aCdOx8ETBfsod0dYeM69/NfcgQwf/imzZ4/ujqwIIUSPkcoaxgygVGu9TWvdDDwDdPQ89JW09lXVrdZ5fOQHFLby0/nssyFcdNGfgNhhlxNCiN4slQFjALA76f+y+LhDKKUGA0OAfyWNdimlSpRSq5RSF6cum4cqdTQyvAYefBAyMpo4++zHaWraczyzIIQQPU5Puel9BbBI6wPevj44/haoq4D7lVLD2lpQKXVTPLCUVFYe3TttD7bFWseAaieLFmmuumofaWkBQqGth19QCCF6sVQGjHIg+TVwhfFxbbmCgy5Haa3L48NtmDf9TT50MdBaP6K1nqa1npafn3+seSYYDrKbeqprZhAOK771LfNe4mCw9JjXLYQQJ7JUBozVwAil1BCllAMTFA5p7aSUGg1kAyuTxmUrpZzxv/Mw3apvOnjZVNhetx2AdTVf5tyR2xk3rh9K2Q7o5lwIIU5GKQsYWusIcDOwBPgEeE5r/bFS6m6l1LykWa8AntFaJ/eKOwYoUUqtB94B7kluXZVKpTWmJlFbM5Ob+/wTi8WGy1UkAUMIcdJL6XPLWuvFHPRmPq31zw/6/642lnsfmJDKvLUnETDS6gq5YM9jwA9xuYZJwBBCnPR6yk3vHqO0phR7JJuRGVas27ZAbS1pacPlprcQ4qQnAeMgpTWlWOqGM3JEfMTataSlDSMSqSMcrunWvAkhRHeSgHGQ0ppSmvcOZ8S0+MsES0pISzMteqWllBDiZCYBI0lztJmd9TvR1cMZMSENhg07KGDIZSkhxMlLAkaSHXU7iOkY1AxnxAhg6lQoKcHlGgooAoFPuzuLQgjRbSRgJEm0kGoJGNOmwY4dWGsb8XgmU1e3rDuzJ4QQ3UoCRpJEwPCGh5OfjwkYAGvWkJ09h4aGlUQi/u7LoBBCdCMJGElKa0qxRryMLMxHKWDKFDOhpITs7HPQOkx9/fJuzaMQQnQXCRhJSmtKsdYPZ+QI038UmZkwciSUlJCZeSoWi4va2re6N5NCCNFNJGAk2VIdb1I7ImnktGmwZg1Wq4vMzFOprV3abfkTQojuJAEjLhKLsKNue+sN74Rp02D3bti3j+zsOTQ2bqSpqaLb8imEEN1FAkbcrvpdRHQEaoYzcmTShANufJ8DQG3t28c/g0II0c0kYMS1NKmtHnFgDWPyZFAKSkrweIqx2XLlPoYQ4qQkASMuETCy9XCys5MmeDwwZgyUlKCUhezss6mtXcqBvbELIUTvJwEjrrSmFEvUzcgBBYdOjD/xDZCdfQ7NzXsIBD45zjkUQojuJQEjrrSmFGtdUpPaZNOmQUUF7NlDdvYcAGktJYQ46UjAiNtSXUp430EtpBISN75LSkhLK8LlGiYBQwhx0klpwFBKzVVKfaaUKlVK3d7G9OuUUpVKqXXxdEPStGuVUlvi6dpU5jMai7K1duuhLaQSiovBZoMVKwDIyZlDXd0yYrFwKrMlhBA9SsoChlLKCjwInA+MBa5USo1tY9ZntdbF8fRYfNkc4E5gJjADuFMpld3Gsl2i3FdOONZ86DMYCW43nHcePPssxGJkZ59DNOrD5/sgVVkSQogeJ5U1jBlAqdZ6m9a6GXgGuKiTy54HvKW1rtFa1wJvAXNTlM9De6lty9VXmwf43n2XrKzPA4qaGmleK4Q4eaQyYAwAdif9XxYfd7BLlVIfKaUWKaUGHuGyXSIRMPIsw/F625lp3jxIT4eFC7Hbs/F6p8l9DCHESaW7b3q/ChRprSdiahFPHOkKlFI3KaVKlFIllZWVR5WJ0ppSVNTJqP4dxKT0dLj4YvjnP6G5mZycuTQ0rCQY3H5U2xRCiBNNKgNGOTAw6f/C+LgWWutqrXVT/N/HgKmdXTZpHY9oradprafl5+cfVUZNL7XDGDXyMIfj6quhthbeeIN+/W5CKStlZf97VNsUQogTTSoDxmpghFJqiFLKAVwBvJI8g1KqX9K/84DE03BLgHOVUtnxm93nxselxOaqUiL7O7h/kXDOOZCXBwsX4nIV0rfv1VRU/JXm5qpUZU0IIXqMlAUMrXUEuBlT0H8CPKe1/lgpdbdSal58tu8qpT5WSq0HvgtcF1+2BvglJuisBu6Oj0tFPs09jI5ueCfY7TB/PrzyCjQ0MHDgbcRiQcrL/5yKrAkhRI+ielOfSNOmTdMl8S48OiumY/zi8RXc/cM+rH97DBMnHmaBlSvhc5+DJ56Ar36VDRvmUV//PqecshOrNf3oMy+EEN1AKbVGaz2tM/N2903vbmdRFuzlZ0DVGIYP78QCs2bBkCGwcCEAAwf+kEikmoqKv6U2o0II0c1O+oABsHkzFBaa5/MOSym46ipYuhT27iUr61QyMj5HWdnvicUiKc+rEEJ0FwkYwJYtHP7+RbKrr4ZYDJ57DoBBg35IKLSDysp/piaDQgjRA0jA4CgCxpgxpn+pp54CrcnN/SJu92h2775X3pMhhOi1TvqAEYnAddfB+ecf4YL/9V/mHRmPPopSFgYOvA2/fx01NW+kIptCCNHtTvpWUkctFjNRZvlyKCkhNmY4q1ePJxYLM336R9hsGccnH0IIcQykldTxYLGYprUZGXDllViaYowe/RRNTbspLf3v7s6dEEJ0OQkYx6KgwASNDRvgttvIzJzF4ME/Zu/ex6msfLG7cyeEEF1KAsaxmjsXbr0VHnwQXn6ZwYN/jsczlc2bb6KpaW93504IIbqMBIyu8Otfw5Qp8LWvYdmxmzFjniIa9fPZZzdIqykhRK8hAaMrOJ3w9NOmydXkyaQ/t4qhQ+6hpuY1Kioe6+7cCSFEl5CA0VVGjoQPP4TJk+FrX2PAN98kP3wapaX/TUODvMpVCHHik4DRlYYOhX/9C/74R9S/3mHs5R8x4M10Nqz7AsHgju7OnRBCHBMJGF3NYoHvfhfWr0eNG8+wX1Ux+SvV7LnvVMKh6u7OnRBCHDUJGKkyYoR5qG/RIhyeIobdWU509CBiTz4O0Wh3504IIY6YBIxUsljg0kuxbdhC7WM3E7EGsFx7PfryyyEU6u7cCdH1pFVg92huPi6bkYBxPFgsZH/9T1S++VNKvwXqhRfQ586BurruztmR0xq++U145JHuzonoaX72M5gxAxobu37d27efHMEoFjuy+T/5BK65xrzU7TgcHwkYx1HR0Lux/uCnbPoZ6JXvETt1JpSXd3e2jsySJfDww/DDH0J9fXfnRvQUq1fDr35lOuT8yU+6dt2//a1pUPKjH3XtenuaNWugXz+46SbTRL8jGzfCFVfAuHHw4ovw+c9DU1Pq86i1TlkC5gKfAaXA7W1MvxXYBHwEvA0MTpoWBdbF0yud2d7UqVP1iaC6eone8IdMHU5DRwbkar1pU3dnqXOiUa0nTdI6P19r0PrXv+7uHImeIBzWurhY6/79tb7uOq2V0nrFiq5Z99/+Zr5rgwaZ4R//2DXr7WnWrNE6K0vr7Gyzn1/4gtaNjYfOt3Wr1l/+spnH49H6jju03r//mDYNlOjOlumdnfFIE2AFtgJDAQewHhh70DxnAe74398Enk2a5j/SbZ4oAUNrrUOhcv3Jwim6KRsds1t09KortF61SutYrLuz1r6FC81X5u9/13ruXBM42vpSi47FYlrv2dOzP+sj8fvfm+/FokVa+3xaFxVpPWLEsX83/u//tLZatT7nHK0DAa0vucQEo+ee65p8p0pVldZ//rPW11+v9d13m/x+9JHWwWDb869ZYwLF4MFab9+u9UMPmf2cNUvrykozT2Oj1j/7mdZOp9bp6Vr/9KdaV1d3SXZ7SsA4BViS9P8dwB0dzD8ZeC/p/14dMLTWOhoN653vfkfv/hI6km4xH8e0aVovWKD1Bx9ovXmz+cI0N3d3VrVuatJ66FBTw4hGzRkkaH3//d2dsxNLaakpAEHrKVO0fvJJrUOh47f95mat33vPfMd+/nOtv/pVrc84Q+s5c7T+8MMjX9+OHVq73eaMOBEA337b7N+ttx59Pt9/X+u0NK2nTtW6ocGMCwS0nj1ba4dD62XLjn7dxyoSMQF/3z6ta2pMkPT7tX7hBa0vvlhru93sf16eGSaSxWKCwL33ar1li1nX2rUmWAwapPW2ba3beP55ExxGjdL60Udba1hXXaV1WVmX7k5PCRhfBh5L+v8rwJ87mP/PwE+T/o8AJcAq4OIOlrspPl/JoEGDuvRAHi9VVa/p95dk6S3fc+nwyMIDv2SJNGiQ1jfeaL6U9fUHriAWMz+qqirzo0rFmeuf/mTysXhx67jTT9d6wIDjW+CdqJqbtb7nHq1dLq0zMrS+7Tatx441x7SgQOtf/MIU2Pv2mYDcVWIxU6j/5S/mDD0jo/U7pZTWhYVan3qq1n37am2zmbPYzn6esZgJFG632Uayb3zDrP+99w6/nkjEfHdLS7VevVrrl17SOidH6+HDzfFIVl2t9ZgxWmdmav3vf5vv+7FqbNT6rbe0/slPTEDKyjKfzYUXav2d72j9v/+r9V13aX355VpPmGACVlu/0cRn+f3va71unVm3328+16efNrWCKVNa5x0/3uznwIHmUtPBli83eQGtJ040/6fAkQSMlL1ASSn1ZWCu1vqG+P9fAWZqrW9uY95rgJuBM7TWTfFxA7TW5UqpocC/gLO11ls72uZxfYFSFwsGd/Dxx1/G71vDsIZrGaAuwlLvNy2pamth/Xp46y3w+cBmg2nTzPMc+/bB/v0HNtNVCtxuyMkxPenefLNZ5mj5/TBsmHk17TvvmPUDvPkmnHcePPoo3HDDsR2AVKuthcxM09T5eAoE4N//hjvuMJ/hl74EDzwAAwaYYmPpUvjDH+D111uXsdmgb18oKoJLLoErr4T+/Vunx2LmGZ8FC8y6s7MhPx/69DFDnw92725NiVZLgwaZz+u880wXNoWF4HCYaTU18L3vwZNPmhupf/sbTJ/e8b698AJceincdx/84AcHTvP5YPx4cLnMq4y9XvPuGK/XbGvlSpNWrYJ16yAcPnD5ggJ47z1zs/tgu3bBKafAnj3m//79zfdz2DCzT/37m+M7YIDp583nM99hn8801NizB8rKTNq9GzZtMtu3Ws3vqrjY/K62b4dt28xySsGQITB2rPkdFBWZzy8cbk2TJ8OcOYf/re3cCS+9ZG5WV1bCq6+2vZ8AmzebhgSXX35sv+EOHMkLlFIZME4B7tJanxf//w4ArfVvDprvHOBPmGCxv511PQ78n9Z6UUfbPJEDBkA0GqK09BYqKv6Cw9GfwsLv0q/ff2G3Z5kZwmF4/3144w14911ITzeFRN++Zuh0mgIqEDCFxLp1pquSiRPhoYdg9uyjy9jdd8Odd5of+KxZreO1Ns0oa2vh00/b/0JrbX6UW7bAWWeZgvtYVVWZAnj9erPtyZPNjyo398D5Vq6Ee+6BV14xBebVV8NXvmJ+9GCO05Il5sf7r3+ZYzluXGsaMsQUdInCzmrtOF/RqCno/vUvE1xXrTJt5Pv3N13gX3xx28uVlpp9qahoTRs2mMJCKTj7bLjqKlPYLVhgCrKMDFP4h0Km4Nm/3ww9Hhg4sDUNG2aWHz26Ndi357XXzOuHKyrMvqenm5OP9HSw201LnFDIpNJSU9CtXm2mHWzpUlOAtsftNkFpxgxTuGdlmeCXlQUTJpi/27N3rzm+W7e2pm3bTL470zQ1L88ElwEDzOd81lnm9+H1Hjif1ub7nZZmUi/UUwKGDdgMnA2UA6uBq7TWHyfNMxlYhKmJbEkanw0EtNZNSqk8YCVwkdZ6U0fbPNEDRkJNzVvs2vVb6urexmr10q/fjRQWfg+Xq/DIVqS1KQhvucWcSV1/vXmGwuk0BZ/NZpLb3ZrsdhNwEmdfu3ebrk7OOcecUR7spZfMWfDChaZASwiHTVB75RWTtm0z4x0O8w6Ryy6DefNModeeaNQUgJs3m/bmn3xiAs/HH7eeXYIJQPX1Zl/OP9+0S09Ph3vvNWfiOTnwta+Z5d5806x3yhRTYLz1FgSDZp5zzzWFw8cfm/1vi9cLp50GF1wAF17Yeqb5wQfwj3/As8+as1OLxQSxs84y6cwzzfE9Ups3m2P797+3HsMzzzT7c+mlR7fOw6mvN8du+/bWk4/GRvOZulytKSMDfvrT1uDblk8+aT1Lb2gwQ7cbZs40JzJdfdacqHWXl5sUDpvPzOMxQ6/XBG+Xq2u3ewLrEQEjnpELgPsxLaYWaK1/pZS6G3PN7BWl1FJgAlARX2SX1nqeUupzwF+AGOZZkfu11n893PZ6S8BI8PnWsnv379i//zksFjuFhd9j0KDbj/x94Y2N8Mtfwu9/f/j23VbroV2XZGebms3o0YfOH4uZH35jozlD27XLBJnEj9XpNMFm3jzTXcqrr8I//2kKZIfDnPV7PK1JKXP2uHevOWNOzkt6usnD2LEwaVJrysszZ+cLF5pCOxFMCgvNpZIbbjDLgilMnnnGXCaproYvfMEEvNNPP7Dwqq83wamsrLWwa2gwZ7BvvWXOaMHkpanJ/O9wmPVdcYU5s87KOrLPqSNaw9q15rNo7/KFEEehxwSM4623BYyEYHAHO3b8jH37/o7d3ochQ+6moODrWCxHeHa2bZu5zBGNmsARjZpCPRhsvZQVCJjCNXE5o7DQpI7OyF580ZzV9+ljlhk0yAxnzDBn7YnCOiEWM5dqXnzRBBa/vzVFo+b6db9+rcPhw81ZbGHh4e9BRKOwbJm59/PFL7Zeo+9KWpvLa6+9BosXmzxdcYUJPF0ZJIQ4DiRg9FINDavZuvVW6uvfxe0eS79+N5CXdwlpaUXdnTUhxAnqSAKGdA1yAsnImE5x8XLGjVuEUja2br2V//xnCCUlk9mx4258vg/R+gj7ohFCiE6SGsYJLBAoparqJaqqXqKh4X1AY7fnk509h5ycc8nOnoPT2f+w6xFCnLzkktRJqLl5HzU1b1Jb+yY1NW8SDpsWyllZZ9Gv39fJy/sSVmvvbBYohDh6EjBOclrHaGzcQFXVK+zd+zdCoe1YrZn07XsV/fp9HY9nCupw7fGFECcFCRiihdYx6ur+TUXFX6mqep5YLER6+iT69buBvn2vxm7v4OEoIUSvJwFDtCkcrmP//qepqHgMv38tSjnJy7uYnJxzycw8jbS04VLzEOIkIwFDHJbP9yEVFX+lsvJZwuEqAOz2vmRmnhpPn8PjKcZiScFzDEKIHkMChug0rWMEAp9RX7+C+vp3qa9fQSi0AwCLxYXXO52MjFPIyJiB1zsDp7NQaiFC9CJHEjBS0/2hOGEoZSE9fQzp6WPo3/8mAJqa9tDQsJL6+vdpaHifsrI/oLXpTdRu70tGxnQ8nmLc7tG43WNwu0dhtaZ3tBkhRC8gAUMcwunsT37+peTnXwpALNaE378en281DQ0f4POtprp6MaarL8PlGkpOzrnk5FxAdvbnJYAI0QvJJSlxVGKxJoLBUhobPyEQ+ASfbw21tUuJxRpRykFW1pl4vVNwOgtxOge2DO32PLmkJUQPIpekRMpZLE7S08eRnj6uZVws1kR9/btUV79OTc3r1NW903IpK8Fq9ZKWNgyXaxhpacNISxtBWtpw3O4ROBz9JZgI0YNJwBBdxmJxkp19NtnZZwO/Q+sYzc37aWoqi6edBINbCQa30ti4kerqVw4IKBaLm7S04fFA0hpUnM5CHI4CbLasAwKK1ppYLEgkUo/D0QelDvNyIyHEMZGAIVJGKQtOZwFOZwFwaI1X6yih0G6CwS0tKRDYQiDwKdXVi4m/rTdpfQ4cjgKs1nQikTrC4ZqWeaxWD17vDDIyTiEz8xS83hk4HPnHYzeFOGlIwBDdRikraWlF8e7ZD3yVp9Yxmpr2EApto6lpD83Ne1tSNOrHbs/GZsvBbs/BavXQ2PgJDQ0r2bXrHsC8dMlmyyU9fUxSSy4vSlkxL4O0YrE4sFrTsVjSsVrTsVq9uFyDO+xzKxaLEA5X0ty8j3B4H83N+7Db+5CdfTYWSxuvKRWiF5GAIXokpSy4XIVH/FraaLQRn68En28tgYC5IV9Z+QKRSHVnt4zLNZT09LG43WOxWt2EQtsJBrcTCm2nqamM5NZhCXZ7Pn36zKdPn6vJyJjZ7r0YrWOEw1U0N+8nGvURjfpbhk5nIRkZn8NqbftlVeFwNRaLWzqRFN0m1a9onQv8EfOK1se01vccNN0JPAlMBaqB+VrrHfFpdwBfx5wufldrveRw25NWUqI94XANsVgQraPxFCEWayIWCxCNNsZTffyS2CcEApsIBD5D6zAOR39criGkpQ3B5SrC4eiHw1GAw9EXu70vgcAm9u37O9XVrxKLheKtwfJRytaSolF/vIa0j0QNqC0WSxqZmaeRnT0Hr3cyjY0baWj4Dw0NqwiFtqOUk8zM2WRnn9MyD1iIxYJEowFisQBK2VpqThaLDa01kUht/D7SbpqaytE6hsXixGJxYbE4sVozcLmKcLkGtTzdr7UmGNwcf6DzXZqa9uB2j2lp7JCePg6bLbPdfdFaE436sVo9XdaYIRYL09i4Eb9/LZGIr+VzMMND73MlC4frCIercDoHHHXQ1VrT2Pgxfv9aYrEQsVgzWjcTizXj8UwgO3tOu70jaB0FLD2uYUePeNJbmTuQmzHXGsqA1cCVWutNSfN8C5iotf6GUuoK4BKt9Xyl1FjgaWAG0B9YCozU5oi3SwKG6EqxWAStI+2e8R8sEmmgsvIFampeJxYLoHUkHpjCWK3pSYGmAIejDzZbJlarB6vVg8WSTiDwKbW1b1Fb+xaBwCct63U6C/F6Z5KRMYPm5n3U1i6lsfEjAJSyH9ISLZlSTpQyAaVzLDidA3A6BxIMbiEcrgTM5T2XaxCBwGfEYoHWuS0ubLYsrNbMeGFtJRyuJhKpJhyuBaLxlnEjcbtH4XaPxGbLJhyuIhyujA9rcTjycToH4XQOjAettPi0qniNrAK/fx1+/0eH3Ns6eH8djgKczn44HP3QOkwotJNQaCfRaMMBx9Q0sBgeb1wxNN7QYih2e3Y82DUSjfqIROrx+VbHP5ulNDdXtLt9my2LvLyLyc+/HK93Oj7faurrV1BXtxyfbzUOR0HS+2rOxm7PPWQdoVAZDQ2r8Pn+Q0PDfwiHa/B6p5GZeQoZGaeQnj6OWKyJQGAzgcCnBIOfEY0GGDbst538jA8+Zj0jYJwC3KW1PlNExz4AAAlsSURBVC/+/x0AWuvfJM2zJD7PSmUuLO8F8oHbk+dNnq+jbUrAEL1FKFRGIPAx6ekT2nwJlgkcb+P3r8dicWG1urFY3FgsaUA0fqmrkWjUj9bRliBgnocpRCk7sVgIrZuIxUKEw7WEQjsIhbbH0y5crqKWvsXc7lEopdA6Rii0k8bGjwkENhEOVxGJ1BGJ1BOJ1KF1GLs9F5stNz7MoKmpnEDgMwKBz2hq2gVoQGG352K352OzZREOVxIK7ULr5jaOhpk3PX0CXu9UPJ6peL1TsdtzaG7eF0+Je1wVNDdX0NRkhkrZcbkG43INwukcjN2eR1PTboLB0nhqDYoJFksasVgons9WdntevGZ3zv9v795irKruOI5/f2cuOOOBQWfohIAXrESkieAdKm2spkYJMX2wKd5iGhNfaKJJk1bSW+pbX2p9MK2mtbWtsV4qlvhQi2BobKqIisqlVEs1YtSRiiNQhzgz/z6sNXAcZ8pmhpmzD/P7JCdz9jr7nPzmZM/8z15rn7Xo6FiWx8Ra8xlFhd7ev9LT8zB79jzOwEDv4fRqplo9n46OpfT1vcnevRvy46n7E4KIT4j4hMHBPvr7P8zPa6VaPZeWlk727dt0aM63SuWEnO/w+9PefjYXXrh1TGcvZfkexhzgrZrt3cDFo+0TEf2SeoHO3P7ssOfOmbioZuVypPGb1tZuuruvo7v7uklMlcaW2tpS9xysOOrnDwx8zMDAAVpaTvrMZdBpfOd9+vreZHDwIC0ts2hp6Rpx3yGpkCwcy69ySH//Pvr6dvHxx7vyRRbv0NTUTlPTdJqaptPcPJ329oVUq4uQRl/VurNzOZ2dyxkcPMgHH6zjwIGtzJhxITNmLPnUzAeDg/3s27eZvXvTPlIzlUorUgtSK+3tZzFjxsVUq4uoVKbl9ybo69tFb+/f2b//BZqbO/MZ2wLa2s6ctHGthh/0lnQLcAvAqaeeWuc0Zvb/NDW1jfrPTark8YjuSc3U3DydanUR1eqiY/J6lco0urpW0NU1ckGtVJrp6FhCR8eSwq8p6dD3k+CGY5JzLEYvl+P3NnBKzfbc3DbiPrlLqoM0+F3kuQBExL0RcUFEXDBrlq+7NzObKBNZMJ4H5kuaJ6kVWAmsHbbPWuCmfP8aYEOkQZW1wEpJ0yTNA+YDmyYwq5mZHcGEdUnlMYlvAU+SLqu9LyK2SboD2BwRa4FfAb+T9DrwAamokPd7GNgO9AOrjnSFlJmZTSzPVmtmNoUdzVVSE9klZWZmxxEXDDMzK8QFw8zMCnHBMDOzQo6rQW9J7wNvjvHpXcCeYxhnMjRa5kbLC848WRotc6PlhdEznxYRhb7EdlwVjPGQtLnolQJl0WiZGy0vOPNkabTMjZYXjk1md0mZmVkhLhhmZlaIC8Zh99Y7wBg0WuZGywvOPFkaLXOj5YVjkNljGGZmVojPMMzMrJApXzAkXSlpp6TXJd1e7zwjkXSfpB5JW2vaTpa0TtJr+edJ9cw4nKRTJD0tabukbZJuze2lzS3pBEmbJL2cM/84t8+T9Fw+Rh7Ksy+XhqQmSS9JeiJvlz3vG5JelbRF0ubcVtrjAkDSTEmPSvqHpB2SlpY5s6Sz8vs7dPtI0m3jzTylC0Zed/xu4CpgIXBtXk+8bH4DXDms7XZgfUTMB9bn7TLpB74dEQuBJcCq/N6WOfdB4LKIWAQsBq6UtAT4CXBnRJwJ7AVurmPGkdwK7KjZLntegK9ExOKayzzLfFwA3AX8OSIWAItI73dpM0fEzvz+LgbOB/4LrGG8mSNiyt6ApcCTNdurgdX1zjVK1tOBrTXbO4HZ+f5sYGe9Mx4h/5+ArzZKbqAdeJG0rPAeoHmkY6beN9LiYuuBy4AnAJU5b870BtA1rK20xwVpYbd/k8d8GyHzsJxXAH87Fpmn9BkGI6873ihrh3dHxDv5/rvA5K5reRQknQ6cCzxHyXPn7p0tQA+wDvgX8GFE9OddynaM/Az4DjCYtzspd16AAP4i6YW8xDKU+7iYB7wP/Dp3/f1S0omUO3OtlcCD+f64Mk/1gnFciPRxoZSXu0mqAn8EbouIj2ofK2PuiBiIdBo/F7gIWFDnSKOStALoiYgX6p3lKC2LiPNIXcGrJH259sESHhfNwHnAzyPiXOAAw7pySpgZgDx+dTXwyPDHxpJ5qheMwmuHl9B7kmYD5J89dc7zGZJaSMXigYh4LDeXPjdARHwIPE3q0pmZ15yHch0jlwBXS3oD+AOpW+ouypsXgIh4O//sIfWrX0S5j4vdwO6IeC5vP0oqIGXOPOQq4MWIeC9vjyvzVC8YRdYdL6va9dBvIo0RlIYkkZbg3RERP615qLS5Jc2SNDPfbyONuewgFY5r8m6lyRwRqyNibkScTjp2N0TE9ZQ0L4CkEyVNH7pP6l/fSomPi4h4F3hL0lm56XLS8tGlzVzjWg53R8F4M9d7QKbeN2A58E9SX/X36p1nlIwPAu8An5A+7dxM6qteD7wGPAWcXO+cwzIvI53uvgJsybflZc4NnAO8lDNvBX6Y288ANgGvk07tp9U76wjZLwWeKHvenO3lfNs29DdX5uMi51sMbM7HxuPASQ2Q+UTgP0BHTdu4Mvub3mZmVshU75IyM7OCXDDMzKwQFwwzMyvEBcPMzApxwTAzs0JcMMxKQNKlQ7PNmpWVC4aZmRXigmF2FCTdkNfM2CLpnjxZ4X5Jd+Y1NNZLmpX3XSzpWUmvSFoztPaApDMlPZXX3XhR0ufzy1dr1lx4IH9b3qw0XDDMCpJ0NvAN4JJIExQOANeTvlG7OSK+AGwEfpSf8lvguxFxDvBqTfsDwN2R1t34Iulb/JBm9L2NtDbLGaS5osxKo/nIu5hZdjlpMZrn84f/NtLkbYPAQ3mf3wOPSeoAZkbExtx+P/BInkdpTkSsAYiIPoD8epsiYnfe3kJaA+WZif+1zIpxwTArTsD9EbH6U43SD4btN9b5dg7W3B/Af59WMu6SMituPXCNpM/BoXWoTyP9HQ3NDnsd8ExE9AJ7JX0pt98IbIyIfcBuSV/LrzFNUvuk/hZmY+RPMGYFRcR2Sd8nrRZXIc0evIq0oM5F+bEe0jgHpOmjf5ELwi7gm7n9RuAeSXfk1/j6JP4aZmPm2WrNxknS/oio1juH2URzl5SZmRXiMwwzMyvEZxhmZlaIC4aZmRXigmFmZoW4YJiZWSEuGGZmVogLhpmZFfI/TcgQwBfIKEsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 567us/sample - loss: 0.2759 - acc: 0.9161\n",
      "Loss: 0.2758556693297185 Accuracy: 0.91609555\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base = '1D_CNN_custom_multi_3_concat_ch_32_BN'\n",
    "\n",
    "for i in range(3, 9):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    model = build_cnn(conv_num=i)\n",
    "#         model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4),\n",
    "          metrics=['accuracy'])\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model_filename = model_path+'{epoch:03d}-{val_loss:.4f}.hdf5'\n",
    "    checkpointer = ModelCheckpoint(filepath = model_filename, monitor = \"val_loss\", \n",
    "                                   verbose=1, save_best_only=True)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=50)\n",
    "    hist = model.fit(x_train_abs, y_train_onehot, batch_size=64, epochs=500, \n",
    "                     validation_data=[x_val_abs, y_val_onehot], shuffle=True, \n",
    "                     callbacks = [checkpointer, early_stopping])\n",
    "\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "    ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "    ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "    ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    png_path = 'visualization/learning_curve/'\n",
    "    filename = model_name+'.png'\n",
    "    os.makedirs(png_path, exist_ok=True)\n",
    "    fig.savefig(png_path+filename, transparent=True)\n",
    "\n",
    "    model.save(model_path+'000_last.hdf5')\n",
    "    del(model)\n",
    "\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "    model = load_model(model_filename)\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "    print()\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_multi_3_concat_ch_32_BN_3_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_33 (Conv1D)              (None, 16000, 32)    192         input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_39 (Batc (None, 16000, 32)    128         conv1d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_39[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling1D) (None, 5333, 32)     0           activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_34 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_33[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_40 (Batc (None, 5333, 32)     128         conv1d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_40[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling1D) (None, 1777, 32)     0           activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_35 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_41 (Batc (None, 1777, 32)     128         conv1d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_41[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling1D) (None, 592, 32)      0           activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_18 (Flatten)            (None, 170656)       0           max_pooling1d_33[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_19 (Flatten)            (None, 56864)        0           max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_20 (Flatten)            (None, 18944)        0           max_pooling1d_35[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 246464)       0           flatten_18[0][0]                 \n",
      "                                                                 flatten_19[0][0]                 \n",
      "                                                                 flatten_20[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_42 (Batc (None, 246464)       985856      concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 16)           3943440     batch_normalization_v1_42[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 4,940,176\n",
      "Trainable params: 4,447,056\n",
      "Non-trainable params: 493,120\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 580us/sample - loss: 3.6859 - acc: 0.4590\n",
      "Loss: 3.68593449231125 Accuracy: 0.45898235\n",
      "\n",
      "1D_CNN_custom_multi_3_concat_ch_32_BN_4_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_8 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_36 (Conv1D)              (None, 16000, 32)    192         input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_43 (Batc (None, 16000, 32)    128         conv1d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_43[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling1D) (None, 5333, 32)     0           activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_37 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_36[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_44 (Batc (None, 5333, 32)     128         conv1d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_44[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling1D) (None, 1777, 32)     0           activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_38 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_37[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_45 (Batc (None, 1777, 32)     128         conv1d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_45[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling1D) (None, 592, 32)      0           activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_39 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_46 (Batc (None, 592, 32)      128         conv1d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 592, 32)      0           batch_normalization_v1_46[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling1D) (None, 197, 32)      0           activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_21 (Flatten)            (None, 56864)        0           max_pooling1d_37[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_22 (Flatten)            (None, 18944)        0           max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_23 (Flatten)            (None, 6304)         0           max_pooling1d_39[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 82112)        0           flatten_21[0][0]                 \n",
      "                                                                 flatten_22[0][0]                 \n",
      "                                                                 flatten_23[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_47 (Batc (None, 82112)        328448      concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 16)           1313808     batch_normalization_v1_47[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 1,658,416\n",
      "Trainable params: 1,493,936\n",
      "Non-trainable params: 164,480\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 557us/sample - loss: 1.7336 - acc: 0.5811\n",
      "Loss: 1.7336489571217808 Accuracy: 0.5811007\n",
      "\n",
      "1D_CNN_custom_multi_3_concat_ch_32_BN_5_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_40 (Conv1D)              (None, 16000, 32)    192         input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_48 (Batc (None, 16000, 32)    128         conv1d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_48[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling1D) (None, 5333, 32)     0           activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_41 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_40[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_49 (Batc (None, 5333, 32)     128         conv1d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_49[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling1D) (None, 1777, 32)     0           activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_42 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_41[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_50 (Batc (None, 1777, 32)     128         conv1d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_50[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling1D) (None, 592, 32)      0           activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_43 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_42[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_51 (Batc (None, 592, 32)      128         conv1d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 592, 32)      0           batch_normalization_v1_51[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling1D) (None, 197, 32)      0           activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_44 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_52 (Batc (None, 197, 64)      256         conv1d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 197, 64)      0           batch_normalization_v1_52[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling1D) (None, 65, 64)       0           activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_24 (Flatten)            (None, 18944)        0           max_pooling1d_42[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_25 (Flatten)            (None, 6304)         0           max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_26 (Flatten)            (None, 4160)         0           max_pooling1d_44[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 29408)        0           flatten_24[0][0]                 \n",
      "                                                                 flatten_25[0][0]                 \n",
      "                                                                 flatten_26[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_53 (Batc (None, 29408)        117632      concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 16)           470544      batch_normalization_v1_53[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 614,896\n",
      "Trainable params: 555,696\n",
      "Non-trainable params: 59,200\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 563us/sample - loss: 1.1376 - acc: 0.6818\n",
      "Loss: 1.1375652820274218 Accuracy: 0.6818276\n",
      "\n",
      "1D_CNN_custom_multi_3_concat_ch_32_BN_6_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_10 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_45 (Conv1D)              (None, 16000, 32)    192         input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_54 (Batc (None, 16000, 32)    128         conv1d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_54[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling1D) (None, 5333, 32)     0           activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_46 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_45[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_55 (Batc (None, 5333, 32)     128         conv1d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_55[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling1D) (None, 1777, 32)     0           activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_47 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_46[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_56 (Batc (None, 1777, 32)     128         conv1d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_56[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling1D) (None, 592, 32)      0           activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_48 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_47[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_57 (Batc (None, 592, 32)      128         conv1d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 592, 32)      0           batch_normalization_v1_57[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling1D) (None, 197, 32)      0           activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_49 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_48[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_58 (Batc (None, 197, 64)      256         conv1d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 197, 64)      0           batch_normalization_v1_58[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling1D) (None, 65, 64)       0           activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_50 (Conv1D)              (None, 65, 64)       20544       max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_59 (Batc (None, 65, 64)       256         conv1d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 65, 64)       0           batch_normalization_v1_59[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling1D) (None, 21, 64)       0           activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_27 (Flatten)            (None, 6304)         0           max_pooling1d_48[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_28 (Flatten)            (None, 4160)         0           max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_29 (Flatten)            (None, 1344)         0           max_pooling1d_50[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 11808)        0           flatten_27[0][0]                 \n",
      "                                                                 flatten_28[0][0]                 \n",
      "                                                                 flatten_29[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_60 (Batc (None, 11808)        47232       concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 16)           188944      batch_normalization_v1_60[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 283,696\n",
      "Trainable params: 259,568\n",
      "Non-trainable params: 24,128\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 583us/sample - loss: 0.5799 - acc: 0.8513\n",
      "Loss: 0.5799085622635958 Accuracy: 0.85129803\n",
      "\n",
      "1D_CNN_custom_multi_3_concat_ch_32_BN_7_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_51 (Conv1D)              (None, 16000, 32)    192         input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_61 (Batc (None, 16000, 32)    128         conv1d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_61[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling1D) (None, 5333, 32)     0           activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_52 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_51[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_62 (Batc (None, 5333, 32)     128         conv1d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_62[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling1D) (None, 1777, 32)     0           activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_53 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_52[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_63 (Batc (None, 1777, 32)     128         conv1d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_63[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling1D) (None, 592, 32)      0           activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_54 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_53[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_64 (Batc (None, 592, 32)      128         conv1d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 592, 32)      0           batch_normalization_v1_64[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling1D) (None, 197, 32)      0           activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_55 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_54[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_65 (Batc (None, 197, 64)      256         conv1d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 197, 64)      0           batch_normalization_v1_65[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling1D) (None, 65, 64)       0           activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_56 (Conv1D)              (None, 65, 64)       20544       max_pooling1d_55[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_66 (Batc (None, 65, 64)       256         conv1d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 65, 64)       0           batch_normalization_v1_66[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling1D) (None, 21, 64)       0           activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_57 (Conv1D)              (None, 21, 64)       20544       max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_67 (Batc (None, 21, 64)       256         conv1d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 21, 64)       0           batch_normalization_v1_67[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling1D) (None, 7, 64)        0           activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_30 (Flatten)            (None, 4160)         0           max_pooling1d_55[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_31 (Flatten)            (None, 1344)         0           max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_32 (Flatten)            (None, 448)          0           max_pooling1d_57[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 5952)         0           flatten_30[0][0]                 \n",
      "                                                                 flatten_31[0][0]                 \n",
      "                                                                 flatten_32[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_68 (Batc (None, 5952)         23808       concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 16)           95248       batch_normalization_v1_68[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 187,376\n",
      "Trainable params: 174,832\n",
      "Non-trainable params: 12,544\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 608us/sample - loss: 0.3168 - acc: 0.9161\n",
      "Loss: 0.3168294546883921 Accuracy: 0.91609555\n",
      "\n",
      "1D_CNN_custom_multi_3_concat_ch_32_BN_8_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_12 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_58 (Conv1D)              (None, 16000, 32)    192         input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_69 (Batc (None, 16000, 32)    128         conv1d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_69[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling1D) (None, 5333, 32)     0           activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_59 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_58[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_70 (Batc (None, 5333, 32)     128         conv1d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_70[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling1D) (None, 1777, 32)     0           activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_60 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_59[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_71 (Batc (None, 1777, 32)     128         conv1d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_71[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling1D) (None, 592, 32)      0           activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_61 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_60[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_72 (Batc (None, 592, 32)      128         conv1d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 592, 32)      0           batch_normalization_v1_72[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling1D) (None, 197, 32)      0           activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_62 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_61[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_73 (Batc (None, 197, 64)      256         conv1d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 197, 64)      0           batch_normalization_v1_73[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling1D) (None, 65, 64)       0           activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_63 (Conv1D)              (None, 65, 64)       20544       max_pooling1d_62[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_74 (Batc (None, 65, 64)       256         conv1d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 65, 64)       0           batch_normalization_v1_74[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling1D) (None, 21, 64)       0           activation_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_64 (Conv1D)              (None, 21, 64)       20544       max_pooling1d_63[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_75 (Batc (None, 21, 64)       256         conv1d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 21, 64)       0           batch_normalization_v1_75[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling1D) (None, 7, 64)        0           activation_64[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_65 (Conv1D)              (None, 7, 64)        20544       max_pooling1d_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_76 (Batc (None, 7, 64)        256         conv1d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 7, 64)        0           batch_normalization_v1_76[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling1D) (None, 2, 64)        0           activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_33 (Flatten)            (None, 1344)         0           max_pooling1d_63[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_34 (Flatten)            (None, 448)          0           max_pooling1d_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_35 (Flatten)            (None, 128)          0           max_pooling1d_65[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 1920)         0           flatten_33[0][0]                 \n",
      "                                                                 flatten_34[0][0]                 \n",
      "                                                                 flatten_35[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_77 (Batc (None, 1920)         7680        concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 16)           30736       batch_normalization_v1_77[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 127,536\n",
      "Trainable params: 122,928\n",
      "Non-trainable params: 4,608\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 641us/sample - loss: 0.2759 - acc: 0.9161\n",
      "Loss: 0.2758556693297185 Accuracy: 0.91609555\n"
     ]
    }
   ],
   "source": [
    "log_dir = 'log'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "base = '1D_CNN_custom_multi_3_concat_ch_32_BN'\n",
    "\n",
    "with open(path.join(log_dir, base), 'w') as log_file:\n",
    "    for i in range(3, 9):\n",
    "        model_name = base+'_{}_conv'.format(i)\n",
    "        print()\n",
    "        print(model_name, 'Model')\n",
    "        model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "        model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "\n",
    "        model = load_model(model_filename)\n",
    "        model.summary()\n",
    "\n",
    "        [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "        print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "        del(model)\n",
    "\n",
    "        log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_multi_3_concat_ch_32_BN_3_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_33 (Conv1D)              (None, 16000, 32)    192         input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_39 (Batc (None, 16000, 32)    128         conv1d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_39[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling1D) (None, 5333, 32)     0           activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_34 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_33[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_40 (Batc (None, 5333, 32)     128         conv1d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_40[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling1D) (None, 1777, 32)     0           activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_35 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_41 (Batc (None, 1777, 32)     128         conv1d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_41[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling1D) (None, 592, 32)      0           activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_18 (Flatten)            (None, 170656)       0           max_pooling1d_33[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_19 (Flatten)            (None, 56864)        0           max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_20 (Flatten)            (None, 18944)        0           max_pooling1d_35[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 246464)       0           flatten_18[0][0]                 \n",
      "                                                                 flatten_19[0][0]                 \n",
      "                                                                 flatten_20[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_42 (Batc (None, 246464)       985856      concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 16)           3943440     batch_normalization_v1_42[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 4,940,176\n",
      "Trainable params: 4,447,056\n",
      "Non-trainable params: 493,120\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 671us/sample - loss: 5.5479 - acc: 0.5643\n",
      "Loss: 5.5479217181695955 Accuracy: 0.5642783\n",
      "\n",
      "1D_CNN_custom_multi_3_concat_ch_32_BN_4_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_8 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_36 (Conv1D)              (None, 16000, 32)    192         input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_43 (Batc (None, 16000, 32)    128         conv1d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_43[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling1D) (None, 5333, 32)     0           activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_37 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_36[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_44 (Batc (None, 5333, 32)     128         conv1d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_44[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling1D) (None, 1777, 32)     0           activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_38 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_37[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_45 (Batc (None, 1777, 32)     128         conv1d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_45[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling1D) (None, 592, 32)      0           activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_39 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_46 (Batc (None, 592, 32)      128         conv1d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 592, 32)      0           batch_normalization_v1_46[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling1D) (None, 197, 32)      0           activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_21 (Flatten)            (None, 56864)        0           max_pooling1d_37[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_22 (Flatten)            (None, 18944)        0           max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_23 (Flatten)            (None, 6304)         0           max_pooling1d_39[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 82112)        0           flatten_21[0][0]                 \n",
      "                                                                 flatten_22[0][0]                 \n",
      "                                                                 flatten_23[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_47 (Batc (None, 82112)        328448      concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 16)           1313808     batch_normalization_v1_47[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 1,658,416\n",
      "Trainable params: 1,493,936\n",
      "Non-trainable params: 164,480\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 727us/sample - loss: 3.7941 - acc: 0.5688\n",
      "Loss: 3.794078778774939 Accuracy: 0.56884736\n",
      "\n",
      "1D_CNN_custom_multi_3_concat_ch_32_BN_5_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_40 (Conv1D)              (None, 16000, 32)    192         input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_48 (Batc (None, 16000, 32)    128         conv1d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_48[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling1D) (None, 5333, 32)     0           activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_41 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_40[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_49 (Batc (None, 5333, 32)     128         conv1d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_49[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling1D) (None, 1777, 32)     0           activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_42 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_41[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_50 (Batc (None, 1777, 32)     128         conv1d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_50[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling1D) (None, 592, 32)      0           activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_43 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_42[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_51 (Batc (None, 592, 32)      128         conv1d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 592, 32)      0           batch_normalization_v1_51[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling1D) (None, 197, 32)      0           activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_44 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_52 (Batc (None, 197, 64)      256         conv1d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 197, 64)      0           batch_normalization_v1_52[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling1D) (None, 65, 64)       0           activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_24 (Flatten)            (None, 18944)        0           max_pooling1d_42[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_25 (Flatten)            (None, 6304)         0           max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_26 (Flatten)            (None, 4160)         0           max_pooling1d_44[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 29408)        0           flatten_24[0][0]                 \n",
      "                                                                 flatten_25[0][0]                 \n",
      "                                                                 flatten_26[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_53 (Batc (None, 29408)        117632      concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 16)           470544      batch_normalization_v1_53[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 614,896\n",
      "Trainable params: 555,696\n",
      "Non-trainable params: 59,200\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 665us/sample - loss: 1.6122 - acc: 0.7142\n",
      "Loss: 1.6122075664167712 Accuracy: 0.71422637\n",
      "\n",
      "1D_CNN_custom_multi_3_concat_ch_32_BN_6_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_10 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_45 (Conv1D)              (None, 16000, 32)    192         input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_54 (Batc (None, 16000, 32)    128         conv1d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_54[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling1D) (None, 5333, 32)     0           activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_46 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_45[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_55 (Batc (None, 5333, 32)     128         conv1d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_55[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling1D) (None, 1777, 32)     0           activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_47 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_46[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_56 (Batc (None, 1777, 32)     128         conv1d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_56[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling1D) (None, 592, 32)      0           activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_48 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_47[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_57 (Batc (None, 592, 32)      128         conv1d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 592, 32)      0           batch_normalization_v1_57[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling1D) (None, 197, 32)      0           activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_49 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_48[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_58 (Batc (None, 197, 64)      256         conv1d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 197, 64)      0           batch_normalization_v1_58[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling1D) (None, 65, 64)       0           activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_50 (Conv1D)              (None, 65, 64)       20544       max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_59 (Batc (None, 65, 64)       256         conv1d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 65, 64)       0           batch_normalization_v1_59[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling1D) (None, 21, 64)       0           activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_27 (Flatten)            (None, 6304)         0           max_pooling1d_48[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_28 (Flatten)            (None, 4160)         0           max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_29 (Flatten)            (None, 1344)         0           max_pooling1d_50[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 11808)        0           flatten_27[0][0]                 \n",
      "                                                                 flatten_28[0][0]                 \n",
      "                                                                 flatten_29[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_60 (Batc (None, 11808)        47232       concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 16)           188944      batch_normalization_v1_60[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 283,696\n",
      "Trainable params: 259,568\n",
      "Non-trainable params: 24,128\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 676us/sample - loss: 0.7184 - acc: 0.8569\n",
      "Loss: 0.7183662781091494 Accuracy: 0.8569055\n",
      "\n",
      "1D_CNN_custom_multi_3_concat_ch_32_BN_7_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_51 (Conv1D)              (None, 16000, 32)    192         input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_61 (Batc (None, 16000, 32)    128         conv1d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_61[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling1D) (None, 5333, 32)     0           activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_52 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_51[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_62 (Batc (None, 5333, 32)     128         conv1d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_62[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling1D) (None, 1777, 32)     0           activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_53 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_52[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_63 (Batc (None, 1777, 32)     128         conv1d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_63[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling1D) (None, 592, 32)      0           activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_54 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_53[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_64 (Batc (None, 592, 32)      128         conv1d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 592, 32)      0           batch_normalization_v1_64[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling1D) (None, 197, 32)      0           activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_55 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_54[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_65 (Batc (None, 197, 64)      256         conv1d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 197, 64)      0           batch_normalization_v1_65[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling1D) (None, 65, 64)       0           activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_56 (Conv1D)              (None, 65, 64)       20544       max_pooling1d_55[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_66 (Batc (None, 65, 64)       256         conv1d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 65, 64)       0           batch_normalization_v1_66[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling1D) (None, 21, 64)       0           activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_57 (Conv1D)              (None, 21, 64)       20544       max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_67 (Batc (None, 21, 64)       256         conv1d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 21, 64)       0           batch_normalization_v1_67[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling1D) (None, 7, 64)        0           activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_30 (Flatten)            (None, 4160)         0           max_pooling1d_55[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_31 (Flatten)            (None, 1344)         0           max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_32 (Flatten)            (None, 448)          0           max_pooling1d_57[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 5952)         0           flatten_30[0][0]                 \n",
      "                                                                 flatten_31[0][0]                 \n",
      "                                                                 flatten_32[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_68 (Batc (None, 5952)         23808       concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 16)           95248       batch_normalization_v1_68[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 187,376\n",
      "Trainable params: 174,832\n",
      "Non-trainable params: 12,544\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 704us/sample - loss: 0.4205 - acc: 0.9045\n",
      "Loss: 0.42054748286339355 Accuracy: 0.9044652\n",
      "\n",
      "1D_CNN_custom_multi_3_concat_ch_32_BN_8_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_12 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_58 (Conv1D)              (None, 16000, 32)    192         input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_69 (Batc (None, 16000, 32)    128         conv1d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_69[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling1D) (None, 5333, 32)     0           activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_59 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_58[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_70 (Batc (None, 5333, 32)     128         conv1d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_70[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling1D) (None, 1777, 32)     0           activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_60 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_59[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_71 (Batc (None, 1777, 32)     128         conv1d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_71[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling1D) (None, 592, 32)      0           activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_61 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_60[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_72 (Batc (None, 592, 32)      128         conv1d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 592, 32)      0           batch_normalization_v1_72[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling1D) (None, 197, 32)      0           activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_62 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_61[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_73 (Batc (None, 197, 64)      256         conv1d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 197, 64)      0           batch_normalization_v1_73[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling1D) (None, 65, 64)       0           activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_63 (Conv1D)              (None, 65, 64)       20544       max_pooling1d_62[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_74 (Batc (None, 65, 64)       256         conv1d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 65, 64)       0           batch_normalization_v1_74[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling1D) (None, 21, 64)       0           activation_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_64 (Conv1D)              (None, 21, 64)       20544       max_pooling1d_63[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_75 (Batc (None, 21, 64)       256         conv1d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 21, 64)       0           batch_normalization_v1_75[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling1D) (None, 7, 64)        0           activation_64[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_65 (Conv1D)              (None, 7, 64)        20544       max_pooling1d_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_76 (Batc (None, 7, 64)        256         conv1d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 7, 64)        0           batch_normalization_v1_76[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling1D) (None, 2, 64)        0           activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_33 (Flatten)            (None, 1344)         0           max_pooling1d_63[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_34 (Flatten)            (None, 448)          0           max_pooling1d_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_35 (Flatten)            (None, 128)          0           max_pooling1d_65[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 1920)         0           flatten_33[0][0]                 \n",
      "                                                                 flatten_34[0][0]                 \n",
      "                                                                 flatten_35[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_77 (Batc (None, 1920)         7680        concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 16)           30736       batch_normalization_v1_77[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 127,536\n",
      "Trainable params: 122,928\n",
      "Non-trainable params: 4,608\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 771us/sample - loss: 0.3644 - acc: 0.9207\n",
      "Loss: 0.3644367678576417 Accuracy: 0.9206646\n"
     ]
    }
   ],
   "source": [
    "for i in range(3, 9):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + '000_last.hdf5'\n",
    "\n",
    "    model = load_model(model_filename)\n",
    "    model.summary()\n",
    "\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
