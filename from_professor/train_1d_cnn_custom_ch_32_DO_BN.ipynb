{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path as path\n",
    "import itertools\n",
    "from sklearn.preprocessing import maxabs_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Input,InputLayer, Dense, Activation, BatchNormalization, Flatten, Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D, Dropout\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler, \\\n",
    "                                        EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = path.join('..', 'data')\n",
    "data_dir = path.join(base_dir, 'data_speech_commands_v0.02')\n",
    " \n",
    "train_txt = path.join(data_dir, 'wav_train_16words.txt')\n",
    "val_txt = path.join(data_dir, 'wav_validation_16words.txt')\n",
    "test_txt = path.join(data_dir, 'wav_test_16words.txt')\n",
    "\n",
    "train_data = np.load(path.join(data_dir, 'wav_train_data.npz'))\n",
    "val_data = np.load(path.join(data_dir, 'wav_validation_data.npz'))\n",
    "test_data = np.load(path.join(data_dir, 'wav_test_data.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36805, 16000, 1),\n",
       " (36805,),\n",
       " (4293, 16000, 1),\n",
       " (4293,),\n",
       " (4815, 16000, 1),\n",
       " (4815,),\n",
       " (16, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = train_data['x_train']\n",
    "y_train = train_data['y_train']\n",
    "x_val = val_data['x_val']\n",
    "y_val = val_data['y_val']\n",
    "x_test = test_data['x_test']\n",
    "y_test = test_data['y_test']\n",
    "y_table = test_data['table']\n",
    "\n",
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape, y_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_test[0].shape\n",
    "output_size = y_table.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_abs = np.asarray([maxabs_scale(wav) for wav in x_train])\n",
    "y_train_onehot = np.asarray([to_categorical(label, output_size) for label in y_train])\n",
    "del x_train, y_train\n",
    "\n",
    "x_val_abs = np.asarray([maxabs_scale(wav) for wav in x_val])\n",
    "y_val_onehot = np.asarray([to_categorical(label, output_size) for label in y_val])\n",
    "del x_val, y_val\n",
    "\n",
    "x_test_abs = np.asarray([maxabs_scale(wav) for wav in x_test])\n",
    "y_test_onehot = np.asarray([to_categorical(label, output_size) for label in y_test])\n",
    "del x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_1d_cnn_custom_ch_32_DO_BN(conv_num=1):\n",
    "    model=Sequential()\n",
    "    model.add(Conv1D (kernel_size=5, filters=32, strides=1, padding='same', input_shape=input_shape)) \n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "#     model.add(MaxPooling1D(pool_size=3, strides=3, padding='same'))\n",
    "    \n",
    "    for i in range(conv_num-1):\n",
    "        model.add(Conv1D (kernel_size=5, filters=32*(2**int((i+1)/4)), \n",
    "                          strides=1, padding='same'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(MaxPooling1D(pool_size=3, strides=3))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(output_size, activation='softmax' ))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1 (Batc (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_1 (Ba (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_2 (Ba (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 56864)             0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 56864)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                909840    \n",
      "=================================================================\n",
      "Total params: 920,720\n",
      "Trainable params: 920,528\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_3 (Conv1D)            (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_3 (Ba (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_4 (Ba (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_5 (Ba (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 1777, 32)          5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_6 (Ba (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 18944)             0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 18944)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                303120    \n",
      "=================================================================\n",
      "Total params: 319,280\n",
      "Trainable params: 319,024\n",
      "Non-trainable params: 256\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_7 (Conv1D)            (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_7 (Ba (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_8 (Ba (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_9 (Ba (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, 1777, 32)          5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_10 (B (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 592, 64)           10304     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_11 (B (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 12608)             0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 12608)             0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                201744    \n",
      "=================================================================\n",
      "Total params: 228,464\n",
      "Trainable params: 228,080\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_12 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_12 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_13 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1 (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_14 (B (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_15 (Conv1D)           (None, 1777, 32)          5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_15 (B (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_16 (Conv1D)           (None, 592, 64)           10304     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_16 (B (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_17 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_17 (B (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 4160)              0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 4160)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                66576     \n",
      "=================================================================\n",
      "Total params: 114,096\n",
      "Trainable params: 113,584\n",
      "Non-trainable params: 512\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_18 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_18 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_19 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_19 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_20 (Conv1D)           (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_20 (B (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_21 (Conv1D)           (None, 1777, 32)          5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_21 (B (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_22 (Conv1D)           (None, 592, 64)           10304     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_22 (B (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_23 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_23 (B (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_24 (Conv1D)           (None, 65, 64)            20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_24 (B (None, 65, 64)            256       \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 1344)              0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 1344)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                21520     \n",
      "=================================================================\n",
      "Total params: 89,840\n",
      "Trainable params: 89,200\n",
      "Non-trainable params: 640\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_25 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_25 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_26 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_26 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_27 (Conv1D)           (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_27 (B (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_27 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_28 (Conv1D)           (None, 1777, 32)          5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_28 (B (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_28 (Activation)   (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_29 (Conv1D)           (None, 592, 64)           10304     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_29 (B (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_29 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_30 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_30 (B (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_30 (Activation)   (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_31 (Conv1D)           (None, 65, 64)            20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_31 (B (None, 65, 64)            256       \n",
      "_________________________________________________________________\n",
      "activation_31 (Activation)   (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_32 (Conv1D)           (None, 21, 64)            20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_32 (B (None, 21, 64)            256       \n",
      "_________________________________________________________________\n",
      "activation_32 (Activation)   (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_26 (MaxPooling (None, 7, 64)             0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 448)               0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 448)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 16)                7184      \n",
      "=================================================================\n",
      "Total params: 96,304\n",
      "Trainable params: 95,536\n",
      "Non-trainable params: 768\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_33 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_33 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_33 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_34 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_34 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_34 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_27 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_35 (Conv1D)           (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_35 (B (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_35 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_28 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_36 (Conv1D)           (None, 1777, 32)          5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_36 (B (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_36 (Activation)   (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_29 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_37 (Conv1D)           (None, 592, 64)           10304     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_37 (B (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_37 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_30 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_38 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_38 (B (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_38 (Activation)   (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_31 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_39 (Conv1D)           (None, 65, 64)            20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_39 (B (None, 65, 64)            256       \n",
      "_________________________________________________________________\n",
      "activation_39 (Activation)   (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_32 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_40 (Conv1D)           (None, 21, 64)            20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_40 (B (None, 21, 64)            256       \n",
      "_________________________________________________________________\n",
      "activation_40 (Activation)   (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling (None, 7, 64)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_41 (Conv1D)           (None, 7, 128)            41088     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_41 (B (None, 7, 128)            512       \n",
      "_________________________________________________________________\n",
      "activation_41 (Activation)   (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling (None, 2, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 16)                4112      \n",
      "=================================================================\n",
      "Total params: 134,832\n",
      "Trainable params: 133,808\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in range(3, 10):\n",
    "    model = build_1d_cnn_custom_ch_32_DO_BN(conv_num=i)\n",
    "    model.summary()\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36805 samples, validate on 4293 samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.4370 - acc: 0.2875\n",
      "Epoch 00001: val_loss improved from inf to 2.18721, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_BN_3_conv_checkpoint/001-2.1872.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 2.4368 - acc: 0.2876 - val_loss: 2.1872 - val_acc: 0.2681\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.7331 - acc: 0.4631\n",
      "Epoch 00002: val_loss improved from 2.18721 to 1.51603, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_BN_3_conv_checkpoint/002-1.5160.hdf5\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 1.7330 - acc: 0.4631 - val_loss: 1.5160 - val_acc: 0.5218\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4354 - acc: 0.5512\n",
      "Epoch 00003: val_loss improved from 1.51603 to 1.39472, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_BN_3_conv_checkpoint/003-1.3947.hdf5\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 1.4359 - acc: 0.5511 - val_loss: 1.3947 - val_acc: 0.5858\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2461 - acc: 0.6089\n",
      "Epoch 00004: val_loss improved from 1.39472 to 1.35384, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_BN_3_conv_checkpoint/004-1.3538.hdf5\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 1.2461 - acc: 0.6088 - val_loss: 1.3538 - val_acc: 0.5765\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0926 - acc: 0.6528\n",
      "Epoch 00005: val_loss did not improve from 1.35384\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 1.0928 - acc: 0.6528 - val_loss: 1.4920 - val_acc: 0.5330\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9741 - acc: 0.6886\n",
      "Epoch 00006: val_loss did not improve from 1.35384\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.9745 - acc: 0.6886 - val_loss: 2.2373 - val_acc: 0.3976\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8678 - acc: 0.7217\n",
      "Epoch 00007: val_loss did not improve from 1.35384\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.8679 - acc: 0.7217 - val_loss: 1.5363 - val_acc: 0.5637\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7724 - acc: 0.7511\n",
      "Epoch 00008: val_loss did not improve from 1.35384\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.7723 - acc: 0.7511 - val_loss: 1.4202 - val_acc: 0.5949\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7182 - acc: 0.7689\n",
      "Epoch 00009: val_loss improved from 1.35384 to 1.34823, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_BN_3_conv_checkpoint/009-1.3482.hdf5\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.7182 - acc: 0.7688 - val_loss: 1.3482 - val_acc: 0.6166\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6425 - acc: 0.7905\n",
      "Epoch 00010: val_loss did not improve from 1.34823\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.6426 - acc: 0.7905 - val_loss: 1.3527 - val_acc: 0.6219\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5958 - acc: 0.8059\n",
      "Epoch 00011: val_loss did not improve from 1.34823\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.5958 - acc: 0.8059 - val_loss: 1.4760 - val_acc: 0.5900\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5546 - acc: 0.8163\n",
      "Epoch 00012: val_loss did not improve from 1.34823\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.5546 - acc: 0.8163 - val_loss: 1.6891 - val_acc: 0.5637\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5089 - acc: 0.8338\n",
      "Epoch 00013: val_loss did not improve from 1.34823\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.5090 - acc: 0.8338 - val_loss: 1.8117 - val_acc: 0.5276\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4701 - acc: 0.8466\n",
      "Epoch 00014: val_loss did not improve from 1.34823\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.4701 - acc: 0.8466 - val_loss: 1.4344 - val_acc: 0.6203\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4394 - acc: 0.8563\n",
      "Epoch 00015: val_loss did not improve from 1.34823\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.4393 - acc: 0.8563 - val_loss: 1.8247 - val_acc: 0.5523\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4178 - acc: 0.8635\n",
      "Epoch 00016: val_loss did not improve from 1.34823\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.4181 - acc: 0.8634 - val_loss: 1.9312 - val_acc: 0.5390\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3970 - acc: 0.8718\n",
      "Epoch 00017: val_loss did not improve from 1.34823\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.3970 - acc: 0.8718 - val_loss: 1.5610 - val_acc: 0.6031\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3793 - acc: 0.8733\n",
      "Epoch 00018: val_loss did not improve from 1.34823\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.3794 - acc: 0.8733 - val_loss: 1.6866 - val_acc: 0.5940\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3567 - acc: 0.8818\n",
      "Epoch 00019: val_loss did not improve from 1.34823\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.3569 - acc: 0.8818 - val_loss: 1.6786 - val_acc: 0.5886\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3362 - acc: 0.8887\n",
      "Epoch 00020: val_loss did not improve from 1.34823\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.3362 - acc: 0.8887 - val_loss: 1.5705 - val_acc: 0.6152\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3172 - acc: 0.8954\n",
      "Epoch 00021: val_loss did not improve from 1.34823\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.3172 - acc: 0.8954 - val_loss: 1.5120 - val_acc: 0.6285\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3102 - acc: 0.8968\n",
      "Epoch 00022: val_loss did not improve from 1.34823\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.3102 - acc: 0.8969 - val_loss: 1.5013 - val_acc: 0.6266\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3009 - acc: 0.9008\n",
      "Epoch 00023: val_loss did not improve from 1.34823\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.3009 - acc: 0.9008 - val_loss: 1.8855 - val_acc: 0.5763\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2844 - acc: 0.9063\n",
      "Epoch 00024: val_loss did not improve from 1.34823\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.2844 - acc: 0.9063 - val_loss: 2.0730 - val_acc: 0.5509\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2861 - acc: 0.9057\n",
      "Epoch 00025: val_loss did not improve from 1.34823\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.2863 - acc: 0.9056 - val_loss: 1.7490 - val_acc: 0.6017\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2709 - acc: 0.9116\n",
      "Epoch 00026: val_loss did not improve from 1.34823\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.2709 - acc: 0.9116 - val_loss: 1.7973 - val_acc: 0.5935\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2490 - acc: 0.9190\n",
      "Epoch 00027: val_loss did not improve from 1.34823\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.2490 - acc: 0.9190 - val_loss: 1.9708 - val_acc: 0.5712\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2423 - acc: 0.9207\n",
      "Epoch 00028: val_loss did not improve from 1.34823\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.2423 - acc: 0.9207 - val_loss: 1.7274 - val_acc: 0.6052\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2425 - acc: 0.9213\n",
      "Epoch 00029: val_loss did not improve from 1.34823\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.2424 - acc: 0.9213 - val_loss: 1.8164 - val_acc: 0.6012\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2308 - acc: 0.9244\n",
      "Epoch 00030: val_loss did not improve from 1.34823\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.2308 - acc: 0.9244 - val_loss: 1.7489 - val_acc: 0.6087\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2248 - acc: 0.9249\n",
      "Epoch 00031: val_loss did not improve from 1.34823\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.2248 - acc: 0.9250 - val_loss: 2.0325 - val_acc: 0.5544\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2139 - acc: 0.9299\n",
      "Epoch 00032: val_loss did not improve from 1.34823\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.2139 - acc: 0.9299 - val_loss: 1.8218 - val_acc: 0.6052\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2164 - acc: 0.9290\n",
      "Epoch 00033: val_loss did not improve from 1.34823\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.2164 - acc: 0.9290 - val_loss: 2.0462 - val_acc: 0.5663\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2036 - acc: 0.9315\n",
      "Epoch 00034: val_loss did not improve from 1.34823\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.2035 - acc: 0.9315 - val_loss: 1.8680 - val_acc: 0.6024\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2008 - acc: 0.9342\n",
      "Epoch 00035: val_loss did not improve from 1.34823\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.2008 - acc: 0.9342 - val_loss: 1.9127 - val_acc: 0.5891\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2029 - acc: 0.9347\n",
      "Epoch 00036: val_loss did not improve from 1.34823\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.2029 - acc: 0.9347 - val_loss: 1.8049 - val_acc: 0.6136\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1874 - acc: 0.9406\n",
      "Epoch 00037: val_loss did not improve from 1.34823\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.1874 - acc: 0.9406 - val_loss: 1.9868 - val_acc: 0.5949\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1750 - acc: 0.9443\n",
      "Epoch 00038: val_loss did not improve from 1.34823\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.1750 - acc: 0.9443 - val_loss: 1.7516 - val_acc: 0.6268\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1879 - acc: 0.9383\n",
      "Epoch 00039: val_loss did not improve from 1.34823\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.1880 - acc: 0.9382 - val_loss: 1.8054 - val_acc: 0.6236\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1847 - acc: 0.9402\n",
      "Epoch 00040: val_loss did not improve from 1.34823\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.1846 - acc: 0.9403 - val_loss: 2.0763 - val_acc: 0.5828\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1699 - acc: 0.9446\n",
      "Epoch 00041: val_loss did not improve from 1.34823\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.1699 - acc: 0.9447 - val_loss: 1.7985 - val_acc: 0.6219\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1665 - acc: 0.9467\n",
      "Epoch 00042: val_loss did not improve from 1.34823\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.1666 - acc: 0.9466 - val_loss: 1.7695 - val_acc: 0.6329\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1655 - acc: 0.9473\n",
      "Epoch 00043: val_loss did not improve from 1.34823\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.1658 - acc: 0.9473 - val_loss: 1.9795 - val_acc: 0.6112\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1699 - acc: 0.9461\n",
      "Epoch 00044: val_loss did not improve from 1.34823\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.1699 - acc: 0.9461 - val_loss: 1.8658 - val_acc: 0.6264\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1584 - acc: 0.9489\n",
      "Epoch 00045: val_loss did not improve from 1.34823\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.1584 - acc: 0.9489 - val_loss: 1.8324 - val_acc: 0.6350\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1546 - acc: 0.9507\n",
      "Epoch 00046: val_loss did not improve from 1.34823\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.1546 - acc: 0.9507 - val_loss: 2.2274 - val_acc: 0.5761\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1556 - acc: 0.9514\n",
      "Epoch 00047: val_loss did not improve from 1.34823\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.1556 - acc: 0.9514 - val_loss: 2.1519 - val_acc: 0.5879\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1482 - acc: 0.9522\n",
      "Epoch 00048: val_loss did not improve from 1.34823\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.1481 - acc: 0.9522 - val_loss: 1.9318 - val_acc: 0.6126\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1503 - acc: 0.9521\n",
      "Epoch 00049: val_loss did not improve from 1.34823\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.1503 - acc: 0.9521 - val_loss: 2.4682 - val_acc: 0.5495\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1466 - acc: 0.9544\n",
      "Epoch 00050: val_loss did not improve from 1.34823\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.1467 - acc: 0.9544 - val_loss: 2.5027 - val_acc: 0.5546\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1418 - acc: 0.9544\n",
      "Epoch 00051: val_loss did not improve from 1.34823\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.1418 - acc: 0.9544 - val_loss: 2.4763 - val_acc: 0.5516\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1463 - acc: 0.9537\n",
      "Epoch 00052: val_loss did not improve from 1.34823\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.1463 - acc: 0.9537 - val_loss: 1.8773 - val_acc: 0.6280\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1285 - acc: 0.9577\n",
      "Epoch 00053: val_loss did not improve from 1.34823\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.1285 - acc: 0.9577 - val_loss: 2.4120 - val_acc: 0.5586\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1415 - acc: 0.9546\n",
      "Epoch 00054: val_loss did not improve from 1.34823\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.1415 - acc: 0.9546 - val_loss: 1.8905 - val_acc: 0.6229\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1346 - acc: 0.9576\n",
      "Epoch 00055: val_loss did not improve from 1.34823\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.1346 - acc: 0.9576 - val_loss: 1.7648 - val_acc: 0.6520\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1290 - acc: 0.9590\n",
      "Epoch 00056: val_loss did not improve from 1.34823\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.1290 - acc: 0.9590 - val_loss: 2.5687 - val_acc: 0.5381\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1347 - acc: 0.9566\n",
      "Epoch 00057: val_loss did not improve from 1.34823\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.1348 - acc: 0.9566 - val_loss: 1.9021 - val_acc: 0.6252\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1246 - acc: 0.9606\n",
      "Epoch 00058: val_loss did not improve from 1.34823\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.1247 - acc: 0.9605 - val_loss: 2.0085 - val_acc: 0.6159\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1245 - acc: 0.9610\n",
      "Epoch 00059: val_loss did not improve from 1.34823\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.1245 - acc: 0.9610 - val_loss: 2.3300 - val_acc: 0.5686\n",
      "\n",
      "1D_CNN_custom_ch_32_DO_BN_3_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 451us/sample - loss: 1.4817 - acc: 0.5676\n",
      "Loss: 1.4816579678100712 Accuracy: 0.56760126\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.4669 - acc: 0.2734\n",
      "Epoch 00001: val_loss improved from inf to 1.80859, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_BN_4_conv_checkpoint/001-1.8086.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 2.4670 - acc: 0.2734 - val_loss: 1.8086 - val_acc: 0.3932\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.7003 - acc: 0.4728\n",
      "Epoch 00002: val_loss improved from 1.80859 to 1.32835, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_BN_4_conv_checkpoint/002-1.3284.hdf5\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 1.7003 - acc: 0.4728 - val_loss: 1.3284 - val_acc: 0.5830\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4365 - acc: 0.5492\n",
      "Epoch 00003: val_loss improved from 1.32835 to 1.29170, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_BN_4_conv_checkpoint/003-1.2917.hdf5\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 1.4366 - acc: 0.5491 - val_loss: 1.2917 - val_acc: 0.5982\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2850 - acc: 0.5993\n",
      "Epoch 00004: val_loss improved from 1.29170 to 1.14488, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_BN_4_conv_checkpoint/004-1.1449.hdf5\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 1.2851 - acc: 0.5993 - val_loss: 1.1449 - val_acc: 0.6394\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1862 - acc: 0.6301\n",
      "Epoch 00005: val_loss improved from 1.14488 to 1.07834, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_BN_4_conv_checkpoint/005-1.0783.hdf5\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 1.1864 - acc: 0.6301 - val_loss: 1.0783 - val_acc: 0.6751\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0955 - acc: 0.6609\n",
      "Epoch 00006: val_loss improved from 1.07834 to 1.01610, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_BN_4_conv_checkpoint/006-1.0161.hdf5\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 1.0955 - acc: 0.6609 - val_loss: 1.0161 - val_acc: 0.6993\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0335 - acc: 0.6820\n",
      "Epoch 00007: val_loss improved from 1.01610 to 0.99235, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_BN_4_conv_checkpoint/007-0.9924.hdf5\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 1.0337 - acc: 0.6819 - val_loss: 0.9924 - val_acc: 0.7105\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9703 - acc: 0.6983\n",
      "Epoch 00008: val_loss did not improve from 0.99235\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.9702 - acc: 0.6982 - val_loss: 0.9928 - val_acc: 0.7025\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9183 - acc: 0.7148\n",
      "Epoch 00009: val_loss improved from 0.99235 to 0.96324, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_BN_4_conv_checkpoint/009-0.9632.hdf5\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.9183 - acc: 0.7148 - val_loss: 0.9632 - val_acc: 0.7167\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8721 - acc: 0.7323\n",
      "Epoch 00010: val_loss improved from 0.96324 to 0.92044, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_BN_4_conv_checkpoint/010-0.9204.hdf5\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.8721 - acc: 0.7323 - val_loss: 0.9204 - val_acc: 0.7247\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8389 - acc: 0.7406\n",
      "Epoch 00011: val_loss did not improve from 0.92044\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.8395 - acc: 0.7405 - val_loss: 0.9240 - val_acc: 0.7251\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8081 - acc: 0.7486\n",
      "Epoch 00012: val_loss improved from 0.92044 to 0.90643, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_BN_4_conv_checkpoint/012-0.9064.hdf5\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.8082 - acc: 0.7486 - val_loss: 0.9064 - val_acc: 0.7261\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7697 - acc: 0.7608\n",
      "Epoch 00013: val_loss did not improve from 0.90643\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.7698 - acc: 0.7608 - val_loss: 0.9790 - val_acc: 0.7067\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7362 - acc: 0.7717\n",
      "Epoch 00014: val_loss did not improve from 0.90643\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.7362 - acc: 0.7717 - val_loss: 0.9209 - val_acc: 0.7286\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7001 - acc: 0.7838\n",
      "Epoch 00015: val_loss did not improve from 0.90643\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.7002 - acc: 0.7838 - val_loss: 0.9780 - val_acc: 0.7144\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6780 - acc: 0.7898\n",
      "Epoch 00016: val_loss improved from 0.90643 to 0.88883, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_BN_4_conv_checkpoint/016-0.8888.hdf5\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.6782 - acc: 0.7898 - val_loss: 0.8888 - val_acc: 0.7358\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6589 - acc: 0.7940\n",
      "Epoch 00017: val_loss improved from 0.88883 to 0.88687, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_BN_4_conv_checkpoint/017-0.8869.hdf5\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.6588 - acc: 0.7940 - val_loss: 0.8869 - val_acc: 0.7379\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6286 - acc: 0.8040\n",
      "Epoch 00018: val_loss did not improve from 0.88687\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.6286 - acc: 0.8040 - val_loss: 0.9231 - val_acc: 0.7356\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6092 - acc: 0.8100\n",
      "Epoch 00019: val_loss did not improve from 0.88687\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.6091 - acc: 0.8100 - val_loss: 0.9169 - val_acc: 0.7321\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5957 - acc: 0.8114\n",
      "Epoch 00020: val_loss did not improve from 0.88687\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.5957 - acc: 0.8114 - val_loss: 0.9062 - val_acc: 0.7368\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5638 - acc: 0.8234\n",
      "Epoch 00021: val_loss did not improve from 0.88687\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.5638 - acc: 0.8234 - val_loss: 0.9018 - val_acc: 0.7400\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5502 - acc: 0.8279\n",
      "Epoch 00022: val_loss did not improve from 0.88687\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.5502 - acc: 0.8279 - val_loss: 0.9369 - val_acc: 0.7338\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5359 - acc: 0.8300\n",
      "Epoch 00023: val_loss did not improve from 0.88687\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.5359 - acc: 0.8300 - val_loss: 0.9367 - val_acc: 0.7372\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5230 - acc: 0.8347\n",
      "Epoch 00024: val_loss improved from 0.88687 to 0.86484, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_BN_4_conv_checkpoint/024-0.8648.hdf5\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.5232 - acc: 0.8347 - val_loss: 0.8648 - val_acc: 0.7545\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5063 - acc: 0.8395\n",
      "Epoch 00025: val_loss did not improve from 0.86484\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.5063 - acc: 0.8395 - val_loss: 0.8743 - val_acc: 0.7503\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4892 - acc: 0.8443\n",
      "Epoch 00026: val_loss did not improve from 0.86484\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.4892 - acc: 0.8443 - val_loss: 0.9481 - val_acc: 0.7358\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4824 - acc: 0.8464\n",
      "Epoch 00027: val_loss did not improve from 0.86484\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.4824 - acc: 0.8464 - val_loss: 0.8898 - val_acc: 0.7540\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4548 - acc: 0.8533\n",
      "Epoch 00028: val_loss did not improve from 0.86484\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.4548 - acc: 0.8533 - val_loss: 0.9129 - val_acc: 0.7473\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4548 - acc: 0.8567\n",
      "Epoch 00029: val_loss did not improve from 0.86484\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.4549 - acc: 0.8566 - val_loss: 0.9206 - val_acc: 0.7494\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4372 - acc: 0.8606\n",
      "Epoch 00030: val_loss did not improve from 0.86484\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.4374 - acc: 0.8605 - val_loss: 0.9434 - val_acc: 0.7286\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4318 - acc: 0.8615\n",
      "Epoch 00031: val_loss improved from 0.86484 to 0.84548, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_BN_4_conv_checkpoint/031-0.8455.hdf5\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.4317 - acc: 0.8615 - val_loss: 0.8455 - val_acc: 0.7682\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4151 - acc: 0.8666\n",
      "Epoch 00032: val_loss did not improve from 0.84548\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.4151 - acc: 0.8666 - val_loss: 1.2090 - val_acc: 0.6760\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4124 - acc: 0.8674\n",
      "Epoch 00033: val_loss did not improve from 0.84548\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.4124 - acc: 0.8674 - val_loss: 1.0061 - val_acc: 0.7282\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4037 - acc: 0.8700\n",
      "Epoch 00034: val_loss did not improve from 0.84548\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.4037 - acc: 0.8700 - val_loss: 1.0162 - val_acc: 0.7256\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3933 - acc: 0.8730\n",
      "Epoch 00035: val_loss did not improve from 0.84548\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.3933 - acc: 0.8731 - val_loss: 0.9198 - val_acc: 0.7536\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3872 - acc: 0.8752\n",
      "Epoch 00036: val_loss did not improve from 0.84548\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.3872 - acc: 0.8753 - val_loss: 0.9105 - val_acc: 0.7536\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3754 - acc: 0.8776\n",
      "Epoch 00037: val_loss did not improve from 0.84548\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3754 - acc: 0.8776 - val_loss: 0.9165 - val_acc: 0.7398\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3705 - acc: 0.8801\n",
      "Epoch 00038: val_loss did not improve from 0.84548\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3704 - acc: 0.8801 - val_loss: 0.9254 - val_acc: 0.7580\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3605 - acc: 0.8809\n",
      "Epoch 00039: val_loss did not improve from 0.84548\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.3606 - acc: 0.8809 - val_loss: 0.8553 - val_acc: 0.7682\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3509 - acc: 0.8846\n",
      "Epoch 00040: val_loss did not improve from 0.84548\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.3509 - acc: 0.8846 - val_loss: 0.9409 - val_acc: 0.7335\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3486 - acc: 0.8877\n",
      "Epoch 00041: val_loss did not improve from 0.84548\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.3486 - acc: 0.8877 - val_loss: 0.9263 - val_acc: 0.7503\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3400 - acc: 0.8878\n",
      "Epoch 00042: val_loss did not improve from 0.84548\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.3401 - acc: 0.8877 - val_loss: 0.9133 - val_acc: 0.7496\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3371 - acc: 0.8893\n",
      "Epoch 00043: val_loss did not improve from 0.84548\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.3371 - acc: 0.8894 - val_loss: 1.0759 - val_acc: 0.7160\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3275 - acc: 0.8959\n",
      "Epoch 00044: val_loss did not improve from 0.84548\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.3275 - acc: 0.8959 - val_loss: 0.9385 - val_acc: 0.7540\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3218 - acc: 0.8950\n",
      "Epoch 00045: val_loss did not improve from 0.84548\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.3219 - acc: 0.8950 - val_loss: 0.9348 - val_acc: 0.7503\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3228 - acc: 0.8943\n",
      "Epoch 00046: val_loss did not improve from 0.84548\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.3229 - acc: 0.8943 - val_loss: 0.9262 - val_acc: 0.7517\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3112 - acc: 0.8986\n",
      "Epoch 00047: val_loss did not improve from 0.84548\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.3112 - acc: 0.8986 - val_loss: 0.9214 - val_acc: 0.7547\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3052 - acc: 0.9018\n",
      "Epoch 00048: val_loss did not improve from 0.84548\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.3051 - acc: 0.9018 - val_loss: 0.8631 - val_acc: 0.7713\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3031 - acc: 0.9016\n",
      "Epoch 00049: val_loss did not improve from 0.84548\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.3031 - acc: 0.9016 - val_loss: 0.9328 - val_acc: 0.7568\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3005 - acc: 0.9016\n",
      "Epoch 00050: val_loss did not improve from 0.84548\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.3005 - acc: 0.9016 - val_loss: 0.9215 - val_acc: 0.7629\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2906 - acc: 0.9043\n",
      "Epoch 00051: val_loss did not improve from 0.84548\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.2908 - acc: 0.9043 - val_loss: 0.8847 - val_acc: 0.7666\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2904 - acc: 0.9052\n",
      "Epoch 00052: val_loss did not improve from 0.84548\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.2903 - acc: 0.9052 - val_loss: 0.9097 - val_acc: 0.7671\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2831 - acc: 0.9079\n",
      "Epoch 00053: val_loss did not improve from 0.84548\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.2832 - acc: 0.9079 - val_loss: 1.0201 - val_acc: 0.7386\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2794 - acc: 0.9094\n",
      "Epoch 00054: val_loss did not improve from 0.84548\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.2793 - acc: 0.9094 - val_loss: 1.0710 - val_acc: 0.7179\n",
      "Epoch 55/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2713 - acc: 0.9128\n",
      "Epoch 00055: val_loss did not improve from 0.84548\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.2714 - acc: 0.9128 - val_loss: 0.9201 - val_acc: 0.7722\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2741 - acc: 0.9099\n",
      "Epoch 00056: val_loss did not improve from 0.84548\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.2742 - acc: 0.9098 - val_loss: 0.8726 - val_acc: 0.7738\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2713 - acc: 0.9093\n",
      "Epoch 00057: val_loss did not improve from 0.84548\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.2714 - acc: 0.9093 - val_loss: 0.9224 - val_acc: 0.7645\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2663 - acc: 0.9115\n",
      "Epoch 00058: val_loss did not improve from 0.84548\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.2663 - acc: 0.9115 - val_loss: 0.9676 - val_acc: 0.7512\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2549 - acc: 0.9165\n",
      "Epoch 00059: val_loss did not improve from 0.84548\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.2550 - acc: 0.9165 - val_loss: 1.0142 - val_acc: 0.7426\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2572 - acc: 0.9153\n",
      "Epoch 00060: val_loss did not improve from 0.84548\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.2572 - acc: 0.9153 - val_loss: 0.9478 - val_acc: 0.7631\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2471 - acc: 0.9205\n",
      "Epoch 00061: val_loss did not improve from 0.84548\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.2471 - acc: 0.9205 - val_loss: 1.0559 - val_acc: 0.7326\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2505 - acc: 0.9173\n",
      "Epoch 00062: val_loss did not improve from 0.84548\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.2505 - acc: 0.9173 - val_loss: 0.9318 - val_acc: 0.7706\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2478 - acc: 0.9185\n",
      "Epoch 00063: val_loss did not improve from 0.84548\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.2477 - acc: 0.9185 - val_loss: 0.9487 - val_acc: 0.7512\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2504 - acc: 0.9167\n",
      "Epoch 00064: val_loss did not improve from 0.84548\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.2504 - acc: 0.9167 - val_loss: 0.9225 - val_acc: 0.7703\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2414 - acc: 0.9207\n",
      "Epoch 00065: val_loss did not improve from 0.84548\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.2414 - acc: 0.9207 - val_loss: 1.0175 - val_acc: 0.7547\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2377 - acc: 0.9227\n",
      "Epoch 00066: val_loss did not improve from 0.84548\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.2376 - acc: 0.9227 - val_loss: 0.9506 - val_acc: 0.7615\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2393 - acc: 0.9230\n",
      "Epoch 00067: val_loss did not improve from 0.84548\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.2393 - acc: 0.9230 - val_loss: 0.9278 - val_acc: 0.7645\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2297 - acc: 0.9259\n",
      "Epoch 00068: val_loss did not improve from 0.84548\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.2298 - acc: 0.9259 - val_loss: 1.0317 - val_acc: 0.7531\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2306 - acc: 0.9259\n",
      "Epoch 00069: val_loss did not improve from 0.84548\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.2307 - acc: 0.9258 - val_loss: 1.1316 - val_acc: 0.7205\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2303 - acc: 0.9254\n",
      "Epoch 00070: val_loss did not improve from 0.84548\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.2303 - acc: 0.9254 - val_loss: 0.9091 - val_acc: 0.7754\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2232 - acc: 0.9261\n",
      "Epoch 00071: val_loss did not improve from 0.84548\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.2232 - acc: 0.9261 - val_loss: 0.9773 - val_acc: 0.7543\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2215 - acc: 0.9281\n",
      "Epoch 00072: val_loss did not improve from 0.84548\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.2215 - acc: 0.9281 - val_loss: 0.9237 - val_acc: 0.7745\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2167 - acc: 0.9279\n",
      "Epoch 00073: val_loss did not improve from 0.84548\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.2168 - acc: 0.9279 - val_loss: 0.9655 - val_acc: 0.7603\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2198 - acc: 0.9293\n",
      "Epoch 00074: val_loss did not improve from 0.84548\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.2198 - acc: 0.9293 - val_loss: 0.8974 - val_acc: 0.7778\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2124 - acc: 0.9317\n",
      "Epoch 00075: val_loss did not improve from 0.84548\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.2124 - acc: 0.9317 - val_loss: 0.9207 - val_acc: 0.7722\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2150 - acc: 0.9313\n",
      "Epoch 00076: val_loss did not improve from 0.84548\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.2150 - acc: 0.9313 - val_loss: 0.9862 - val_acc: 0.7598\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2128 - acc: 0.9312\n",
      "Epoch 00077: val_loss did not improve from 0.84548\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.2128 - acc: 0.9313 - val_loss: 0.9416 - val_acc: 0.7736\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2081 - acc: 0.9332\n",
      "Epoch 00078: val_loss did not improve from 0.84548\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.2081 - acc: 0.9332 - val_loss: 0.9154 - val_acc: 0.7773\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2028 - acc: 0.9347\n",
      "Epoch 00079: val_loss did not improve from 0.84548\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.2027 - acc: 0.9347 - val_loss: 1.0314 - val_acc: 0.7580\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2056 - acc: 0.9329\n",
      "Epoch 00080: val_loss did not improve from 0.84548\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.2056 - acc: 0.9328 - val_loss: 0.9661 - val_acc: 0.7715\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2056 - acc: 0.9329\n",
      "Epoch 00081: val_loss did not improve from 0.84548\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.2056 - acc: 0.9329 - val_loss: 1.0240 - val_acc: 0.7526\n",
      "\n",
      "1D_CNN_custom_ch_32_DO_BN_4_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 485us/sample - loss: 0.9394 - acc: 0.7377\n",
      "Loss: 0.9393778567249778 Accuracy: 0.7376947\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.4058 - acc: 0.2757\n",
      "Epoch 00001: val_loss improved from inf to 1.72411, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_BN_5_conv_checkpoint/001-1.7241.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 2.4056 - acc: 0.2757 - val_loss: 1.7241 - val_acc: 0.4393\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5712 - acc: 0.4979\n",
      "Epoch 00002: val_loss improved from 1.72411 to 1.17531, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_BN_5_conv_checkpoint/002-1.1753.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 1.5713 - acc: 0.4979 - val_loss: 1.1753 - val_acc: 0.6394\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3067 - acc: 0.5874\n",
      "Epoch 00003: val_loss improved from 1.17531 to 1.05016, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_BN_5_conv_checkpoint/003-1.0502.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 1.3068 - acc: 0.5874 - val_loss: 1.0502 - val_acc: 0.6804\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1474 - acc: 0.6430\n",
      "Epoch 00004: val_loss improved from 1.05016 to 1.00533, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_BN_5_conv_checkpoint/004-1.0053.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 1.1473 - acc: 0.6430 - val_loss: 1.0053 - val_acc: 0.7004\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0442 - acc: 0.6757\n",
      "Epoch 00005: val_loss improved from 1.00533 to 0.86946, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_BN_5_conv_checkpoint/005-0.8695.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 1.0441 - acc: 0.6757 - val_loss: 0.8695 - val_acc: 0.7398\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9551 - acc: 0.7037\n",
      "Epoch 00006: val_loss did not improve from 0.86946\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.9553 - acc: 0.7037 - val_loss: 0.8700 - val_acc: 0.7384\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8879 - acc: 0.7281\n",
      "Epoch 00007: val_loss improved from 0.86946 to 0.77762, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_BN_5_conv_checkpoint/007-0.7776.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.8882 - acc: 0.7281 - val_loss: 0.7776 - val_acc: 0.7731\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8376 - acc: 0.7419\n",
      "Epoch 00008: val_loss did not improve from 0.77762\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.8375 - acc: 0.7419 - val_loss: 0.8567 - val_acc: 0.7393\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7891 - acc: 0.7584\n",
      "Epoch 00009: val_loss improved from 0.77762 to 0.74683, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_BN_5_conv_checkpoint/009-0.7468.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.7894 - acc: 0.7583 - val_loss: 0.7468 - val_acc: 0.7796\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7639 - acc: 0.7651\n",
      "Epoch 00010: val_loss did not improve from 0.74683\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.7639 - acc: 0.7651 - val_loss: 0.7850 - val_acc: 0.7743\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7244 - acc: 0.7792\n",
      "Epoch 00011: val_loss improved from 0.74683 to 0.72825, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_BN_5_conv_checkpoint/011-0.7282.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.7244 - acc: 0.7791 - val_loss: 0.7282 - val_acc: 0.7878\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6959 - acc: 0.7887\n",
      "Epoch 00012: val_loss did not improve from 0.72825\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.6963 - acc: 0.7886 - val_loss: 0.7462 - val_acc: 0.7789\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6634 - acc: 0.8003\n",
      "Epoch 00013: val_loss did not improve from 0.72825\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.6634 - acc: 0.8003 - val_loss: 0.7459 - val_acc: 0.7757\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6436 - acc: 0.8031\n",
      "Epoch 00014: val_loss did not improve from 0.72825\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.6436 - acc: 0.8031 - val_loss: 0.8300 - val_acc: 0.7489\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6191 - acc: 0.8137\n",
      "Epoch 00015: val_loss did not improve from 0.72825\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.6191 - acc: 0.8137 - val_loss: 0.7854 - val_acc: 0.7678\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6037 - acc: 0.8160\n",
      "Epoch 00016: val_loss did not improve from 0.72825\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.6037 - acc: 0.8160 - val_loss: 0.7602 - val_acc: 0.7722\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5873 - acc: 0.8211\n",
      "Epoch 00017: val_loss did not improve from 0.72825\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.5875 - acc: 0.8211 - val_loss: 0.7371 - val_acc: 0.7771\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5670 - acc: 0.8267\n",
      "Epoch 00018: val_loss did not improve from 0.72825\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.5670 - acc: 0.8267 - val_loss: 0.8489 - val_acc: 0.7566\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5495 - acc: 0.8325\n",
      "Epoch 00019: val_loss improved from 0.72825 to 0.69130, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_BN_5_conv_checkpoint/019-0.6913.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.5496 - acc: 0.8325 - val_loss: 0.6913 - val_acc: 0.7957\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5329 - acc: 0.8369\n",
      "Epoch 00020: val_loss improved from 0.69130 to 0.67680, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_BN_5_conv_checkpoint/020-0.6768.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.5329 - acc: 0.8369 - val_loss: 0.6768 - val_acc: 0.8018\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5234 - acc: 0.8400\n",
      "Epoch 00021: val_loss did not improve from 0.67680\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.5234 - acc: 0.8400 - val_loss: 0.7035 - val_acc: 0.7957\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5033 - acc: 0.8458\n",
      "Epoch 00022: val_loss did not improve from 0.67680\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.5033 - acc: 0.8458 - val_loss: 0.6818 - val_acc: 0.8013\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4989 - acc: 0.8446\n",
      "Epoch 00023: val_loss did not improve from 0.67680\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.4989 - acc: 0.8445 - val_loss: 0.7389 - val_acc: 0.7843\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4831 - acc: 0.8490\n",
      "Epoch 00024: val_loss did not improve from 0.67680\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.4831 - acc: 0.8490 - val_loss: 0.7789 - val_acc: 0.7729\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4631 - acc: 0.8582\n",
      "Epoch 00025: val_loss improved from 0.67680 to 0.64859, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_BN_5_conv_checkpoint/025-0.6486.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.4631 - acc: 0.8582 - val_loss: 0.6486 - val_acc: 0.8185\n",
      "Epoch 26/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4557 - acc: 0.8598\n",
      "Epoch 00026: val_loss did not improve from 0.64859\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.4558 - acc: 0.8597 - val_loss: 0.7026 - val_acc: 0.7997\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4512 - acc: 0.8598\n",
      "Epoch 00027: val_loss improved from 0.64859 to 0.64543, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_BN_5_conv_checkpoint/027-0.6454.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.4511 - acc: 0.8599 - val_loss: 0.6454 - val_acc: 0.8111\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4383 - acc: 0.8656\n",
      "Epoch 00028: val_loss did not improve from 0.64543\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.4384 - acc: 0.8656 - val_loss: 1.1574 - val_acc: 0.6930\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4285 - acc: 0.8678\n",
      "Epoch 00029: val_loss did not improve from 0.64543\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.4285 - acc: 0.8678 - val_loss: 1.3499 - val_acc: 0.6564\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4213 - acc: 0.8682\n",
      "Epoch 00030: val_loss did not improve from 0.64543\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.4213 - acc: 0.8681 - val_loss: 0.6638 - val_acc: 0.8062\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4169 - acc: 0.8678\n",
      "Epoch 00031: val_loss did not improve from 0.64543\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.4169 - acc: 0.8678 - val_loss: 0.6559 - val_acc: 0.8155\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4036 - acc: 0.8742\n",
      "Epoch 00032: val_loss did not improve from 0.64543\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.4036 - acc: 0.8742 - val_loss: 0.6804 - val_acc: 0.8018\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3982 - acc: 0.8740\n",
      "Epoch 00033: val_loss did not improve from 0.64543\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3982 - acc: 0.8740 - val_loss: 0.6492 - val_acc: 0.8164\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3844 - acc: 0.8785\n",
      "Epoch 00034: val_loss did not improve from 0.64543\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3845 - acc: 0.8785 - val_loss: 0.8537 - val_acc: 0.7689\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3749 - acc: 0.8821\n",
      "Epoch 00035: val_loss did not improve from 0.64543\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3749 - acc: 0.8821 - val_loss: 0.6902 - val_acc: 0.7976\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3703 - acc: 0.8837\n",
      "Epoch 00036: val_loss did not improve from 0.64543\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.3706 - acc: 0.8837 - val_loss: 0.7295 - val_acc: 0.7976\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3745 - acc: 0.8821\n",
      "Epoch 00037: val_loss did not improve from 0.64543\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.3745 - acc: 0.8821 - val_loss: 0.6650 - val_acc: 0.8139\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3568 - acc: 0.8877\n",
      "Epoch 00038: val_loss did not improve from 0.64543\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.3568 - acc: 0.8877 - val_loss: 0.7365 - val_acc: 0.7955\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3542 - acc: 0.8900\n",
      "Epoch 00039: val_loss did not improve from 0.64543\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.3542 - acc: 0.8900 - val_loss: 0.6597 - val_acc: 0.8125\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3420 - acc: 0.8915\n",
      "Epoch 00040: val_loss did not improve from 0.64543\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.3420 - acc: 0.8915 - val_loss: 0.6831 - val_acc: 0.8076\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3394 - acc: 0.8922\n",
      "Epoch 00041: val_loss improved from 0.64543 to 0.63717, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_BN_5_conv_checkpoint/041-0.6372.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.3394 - acc: 0.8922 - val_loss: 0.6372 - val_acc: 0.8223\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3313 - acc: 0.8968\n",
      "Epoch 00042: val_loss did not improve from 0.63717\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.3316 - acc: 0.8967 - val_loss: 0.7389 - val_acc: 0.8001\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3299 - acc: 0.8949\n",
      "Epoch 00043: val_loss did not improve from 0.63717\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.3300 - acc: 0.8949 - val_loss: 0.6402 - val_acc: 0.8328\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3215 - acc: 0.8980\n",
      "Epoch 00044: val_loss did not improve from 0.63717\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.3218 - acc: 0.8980 - val_loss: 0.9200 - val_acc: 0.7577\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3218 - acc: 0.8971\n",
      "Epoch 00045: val_loss did not improve from 0.63717\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.3217 - acc: 0.8971 - val_loss: 0.6794 - val_acc: 0.8174\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3047 - acc: 0.9027\n",
      "Epoch 00046: val_loss did not improve from 0.63717\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.3047 - acc: 0.9028 - val_loss: 0.7673 - val_acc: 0.7920\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3048 - acc: 0.9033\n",
      "Epoch 00047: val_loss did not improve from 0.63717\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.3048 - acc: 0.9033 - val_loss: 0.7416 - val_acc: 0.8090\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3064 - acc: 0.9021\n",
      "Epoch 00048: val_loss did not improve from 0.63717\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.3063 - acc: 0.9021 - val_loss: 0.7116 - val_acc: 0.8106\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2977 - acc: 0.9036\n",
      "Epoch 00049: val_loss improved from 0.63717 to 0.63707, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_BN_5_conv_checkpoint/049-0.6371.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2978 - acc: 0.9036 - val_loss: 0.6371 - val_acc: 0.8251\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2962 - acc: 0.9067\n",
      "Epoch 00050: val_loss did not improve from 0.63707\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2962 - acc: 0.9066 - val_loss: 0.7039 - val_acc: 0.8171\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2908 - acc: 0.9065\n",
      "Epoch 00051: val_loss did not improve from 0.63707\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2909 - acc: 0.9065 - val_loss: 0.7066 - val_acc: 0.8099\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2922 - acc: 0.9074\n",
      "Epoch 00052: val_loss did not improve from 0.63707\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2922 - acc: 0.9073 - val_loss: 0.7030 - val_acc: 0.8062\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2827 - acc: 0.9095\n",
      "Epoch 00053: val_loss did not improve from 0.63707\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2828 - acc: 0.9095 - val_loss: 0.7326 - val_acc: 0.8076\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2814 - acc: 0.9097\n",
      "Epoch 00054: val_loss did not improve from 0.63707\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2813 - acc: 0.9097 - val_loss: 0.7064 - val_acc: 0.8157\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2694 - acc: 0.9141\n",
      "Epoch 00055: val_loss did not improve from 0.63707\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2695 - acc: 0.9141 - val_loss: 0.7486 - val_acc: 0.8022\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2747 - acc: 0.9107\n",
      "Epoch 00056: val_loss did not improve from 0.63707\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2748 - acc: 0.9107 - val_loss: 0.7905 - val_acc: 0.7911\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2722 - acc: 0.9136\n",
      "Epoch 00057: val_loss did not improve from 0.63707\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2723 - acc: 0.9136 - val_loss: 0.7953 - val_acc: 0.7959\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2678 - acc: 0.9149\n",
      "Epoch 00058: val_loss did not improve from 0.63707\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2678 - acc: 0.9149 - val_loss: 0.7263 - val_acc: 0.8116\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2572 - acc: 0.9177\n",
      "Epoch 00059: val_loss did not improve from 0.63707\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2572 - acc: 0.9177 - val_loss: 0.6438 - val_acc: 0.8283\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2513 - acc: 0.9202\n",
      "Epoch 00060: val_loss did not improve from 0.63707\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2513 - acc: 0.9202 - val_loss: 0.6545 - val_acc: 0.8362\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2547 - acc: 0.9195\n",
      "Epoch 00061: val_loss did not improve from 0.63707\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2548 - acc: 0.9195 - val_loss: 1.0164 - val_acc: 0.7512\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2560 - acc: 0.9187\n",
      "Epoch 00062: val_loss did not improve from 0.63707\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2560 - acc: 0.9188 - val_loss: 0.6417 - val_acc: 0.8318\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2475 - acc: 0.9195\n",
      "Epoch 00063: val_loss did not improve from 0.63707\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2476 - acc: 0.9195 - val_loss: 0.6737 - val_acc: 0.8225\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2452 - acc: 0.9211\n",
      "Epoch 00064: val_loss did not improve from 0.63707\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2452 - acc: 0.9211 - val_loss: 0.7359 - val_acc: 0.8092\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2421 - acc: 0.9212\n",
      "Epoch 00065: val_loss did not improve from 0.63707\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2421 - acc: 0.9212 - val_loss: 0.6618 - val_acc: 0.8223\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2421 - acc: 0.9217\n",
      "Epoch 00066: val_loss did not improve from 0.63707\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2421 - acc: 0.9217 - val_loss: 0.6776 - val_acc: 0.8244\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2343 - acc: 0.9244\n",
      "Epoch 00067: val_loss did not improve from 0.63707\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2343 - acc: 0.9244 - val_loss: 0.6760 - val_acc: 0.8246\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2327 - acc: 0.9258\n",
      "Epoch 00068: val_loss did not improve from 0.63707\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2327 - acc: 0.9258 - val_loss: 0.7927 - val_acc: 0.7941\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2365 - acc: 0.9234\n",
      "Epoch 00069: val_loss did not improve from 0.63707\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2365 - acc: 0.9234 - val_loss: 0.7785 - val_acc: 0.8048\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2320 - acc: 0.9255\n",
      "Epoch 00070: val_loss did not improve from 0.63707\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2320 - acc: 0.9256 - val_loss: 0.6979 - val_acc: 0.8258\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2220 - acc: 0.9296\n",
      "Epoch 00071: val_loss did not improve from 0.63707\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2219 - acc: 0.9297 - val_loss: 0.6684 - val_acc: 0.8274\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2228 - acc: 0.9268\n",
      "Epoch 00072: val_loss did not improve from 0.63707\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2228 - acc: 0.9267 - val_loss: 0.6886 - val_acc: 0.8321\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2204 - acc: 0.9291\n",
      "Epoch 00073: val_loss did not improve from 0.63707\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2204 - acc: 0.9291 - val_loss: 0.7237 - val_acc: 0.8113\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2231 - acc: 0.9279\n",
      "Epoch 00074: val_loss did not improve from 0.63707\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2232 - acc: 0.9279 - val_loss: 0.7331 - val_acc: 0.8141\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2123 - acc: 0.9304\n",
      "Epoch 00075: val_loss did not improve from 0.63707\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2124 - acc: 0.9304 - val_loss: 0.9131 - val_acc: 0.7806\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2191 - acc: 0.9299\n",
      "Epoch 00076: val_loss did not improve from 0.63707\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2191 - acc: 0.9299 - val_loss: 0.7327 - val_acc: 0.8104\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2039 - acc: 0.9351\n",
      "Epoch 00077: val_loss did not improve from 0.63707\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2040 - acc: 0.9351 - val_loss: 0.7419 - val_acc: 0.8125\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2089 - acc: 0.9321\n",
      "Epoch 00078: val_loss did not improve from 0.63707\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2090 - acc: 0.9321 - val_loss: 0.7834 - val_acc: 0.8102\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2124 - acc: 0.9314\n",
      "Epoch 00079: val_loss did not improve from 0.63707\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2124 - acc: 0.9314 - val_loss: 0.8521 - val_acc: 0.7831\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2027 - acc: 0.9358\n",
      "Epoch 00080: val_loss did not improve from 0.63707\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2027 - acc: 0.9358 - val_loss: 0.7606 - val_acc: 0.8150\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2031 - acc: 0.9349\n",
      "Epoch 00081: val_loss did not improve from 0.63707\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2031 - acc: 0.9349 - val_loss: 0.6722 - val_acc: 0.8332\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1984 - acc: 0.9365\n",
      "Epoch 00082: val_loss did not improve from 0.63707\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1984 - acc: 0.9364 - val_loss: 0.6823 - val_acc: 0.8281\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1971 - acc: 0.9357\n",
      "Epoch 00083: val_loss did not improve from 0.63707\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1973 - acc: 0.9357 - val_loss: 0.7820 - val_acc: 0.8041\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1933 - acc: 0.9380\n",
      "Epoch 00084: val_loss did not improve from 0.63707\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1933 - acc: 0.9379 - val_loss: 0.8373 - val_acc: 0.7994\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1978 - acc: 0.9379\n",
      "Epoch 00085: val_loss did not improve from 0.63707\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1978 - acc: 0.9379 - val_loss: 0.7269 - val_acc: 0.8234\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1903 - acc: 0.9387\n",
      "Epoch 00086: val_loss did not improve from 0.63707\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1904 - acc: 0.9387 - val_loss: 0.7332 - val_acc: 0.8127\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1920 - acc: 0.9382\n",
      "Epoch 00087: val_loss did not improve from 0.63707\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1920 - acc: 0.9382 - val_loss: 0.7482 - val_acc: 0.8195\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1852 - acc: 0.9400\n",
      "Epoch 00088: val_loss did not improve from 0.63707\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1852 - acc: 0.9400 - val_loss: 0.7520 - val_acc: 0.8253\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1910 - acc: 0.9389\n",
      "Epoch 00089: val_loss did not improve from 0.63707\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1910 - acc: 0.9388 - val_loss: 0.6983 - val_acc: 0.8255\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1865 - acc: 0.9403\n",
      "Epoch 00090: val_loss did not improve from 0.63707\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1865 - acc: 0.9403 - val_loss: 0.8198 - val_acc: 0.7964\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1827 - acc: 0.9410\n",
      "Epoch 00091: val_loss did not improve from 0.63707\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1827 - acc: 0.9410 - val_loss: 0.6868 - val_acc: 0.8283\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1802 - acc: 0.9424\n",
      "Epoch 00092: val_loss did not improve from 0.63707\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1802 - acc: 0.9424 - val_loss: 0.8026 - val_acc: 0.8001\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1848 - acc: 0.9408\n",
      "Epoch 00093: val_loss did not improve from 0.63707\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1851 - acc: 0.9408 - val_loss: 0.6989 - val_acc: 0.8318\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1821 - acc: 0.9411\n",
      "Epoch 00094: val_loss did not improve from 0.63707\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1820 - acc: 0.9411 - val_loss: 0.7105 - val_acc: 0.8262\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1768 - acc: 0.9436\n",
      "Epoch 00095: val_loss did not improve from 0.63707\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1769 - acc: 0.9436 - val_loss: 0.7158 - val_acc: 0.8237\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1787 - acc: 0.9432\n",
      "Epoch 00096: val_loss did not improve from 0.63707\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1787 - acc: 0.9432 - val_loss: 0.7234 - val_acc: 0.8241\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1722 - acc: 0.9452\n",
      "Epoch 00097: val_loss did not improve from 0.63707\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1722 - acc: 0.9452 - val_loss: 0.7757 - val_acc: 0.8199\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1723 - acc: 0.9452\n",
      "Epoch 00098: val_loss did not improve from 0.63707\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1723 - acc: 0.9452 - val_loss: 0.7356 - val_acc: 0.8218\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1674 - acc: 0.9457\n",
      "Epoch 00099: val_loss did not improve from 0.63707\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1675 - acc: 0.9457 - val_loss: 0.6882 - val_acc: 0.8381\n",
      "\n",
      "1D_CNN_custom_ch_32_DO_BN_5_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 532us/sample - loss: 0.6981 - acc: 0.8039\n",
      "Loss: 0.6980502425819667 Accuracy: 0.803946\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.5072 - acc: 0.2537\n",
      "Epoch 00001: val_loss improved from inf to 1.69430, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_BN_6_conv_checkpoint/001-1.6943.hdf5\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 2.5071 - acc: 0.2537 - val_loss: 1.6943 - val_acc: 0.4486\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6385 - acc: 0.4740\n",
      "Epoch 00002: val_loss improved from 1.69430 to 1.14882, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_BN_6_conv_checkpoint/002-1.1488.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 1.6384 - acc: 0.4740 - val_loss: 1.1488 - val_acc: 0.6571\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3322 - acc: 0.5774\n",
      "Epoch 00003: val_loss improved from 1.14882 to 0.98784, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_BN_6_conv_checkpoint/003-0.9878.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 1.3322 - acc: 0.5774 - val_loss: 0.9878 - val_acc: 0.7053\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1513 - acc: 0.6394\n",
      "Epoch 00004: val_loss improved from 0.98784 to 0.87573, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_BN_6_conv_checkpoint/004-0.8757.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 1.1513 - acc: 0.6394 - val_loss: 0.8757 - val_acc: 0.7400\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0316 - acc: 0.6822\n",
      "Epoch 00005: val_loss improved from 0.87573 to 0.86127, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_BN_6_conv_checkpoint/005-0.8613.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 1.0315 - acc: 0.6822 - val_loss: 0.8613 - val_acc: 0.7335\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9390 - acc: 0.7125\n",
      "Epoch 00006: val_loss improved from 0.86127 to 0.76254, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_BN_6_conv_checkpoint/006-0.7625.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.9390 - acc: 0.7125 - val_loss: 0.7625 - val_acc: 0.7778\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8720 - acc: 0.7365\n",
      "Epoch 00007: val_loss improved from 0.76254 to 0.70765, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_BN_6_conv_checkpoint/007-0.7076.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.8719 - acc: 0.7365 - val_loss: 0.7076 - val_acc: 0.7983\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8150 - acc: 0.7529\n",
      "Epoch 00008: val_loss did not improve from 0.70765\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.8150 - acc: 0.7529 - val_loss: 0.7101 - val_acc: 0.7992\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7697 - acc: 0.7681\n",
      "Epoch 00009: val_loss improved from 0.70765 to 0.67318, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_BN_6_conv_checkpoint/009-0.6732.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.7699 - acc: 0.7681 - val_loss: 0.6732 - val_acc: 0.8109\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7315 - acc: 0.7786\n",
      "Epoch 00010: val_loss improved from 0.67318 to 0.66270, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_BN_6_conv_checkpoint/010-0.6627.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.7318 - acc: 0.7785 - val_loss: 0.6627 - val_acc: 0.8148\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6984 - acc: 0.7905\n",
      "Epoch 00011: val_loss improved from 0.66270 to 0.60463, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_BN_6_conv_checkpoint/011-0.6046.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.6984 - acc: 0.7904 - val_loss: 0.6046 - val_acc: 0.8355\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6662 - acc: 0.8020\n",
      "Epoch 00012: val_loss did not improve from 0.60463\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.6666 - acc: 0.8020 - val_loss: 0.6269 - val_acc: 0.8216\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6408 - acc: 0.8115\n",
      "Epoch 00013: val_loss improved from 0.60463 to 0.57226, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_BN_6_conv_checkpoint/013-0.5723.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.6408 - acc: 0.8115 - val_loss: 0.5723 - val_acc: 0.8484\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6156 - acc: 0.8148\n",
      "Epoch 00014: val_loss improved from 0.57226 to 0.57167, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_BN_6_conv_checkpoint/014-0.5717.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.6156 - acc: 0.8148 - val_loss: 0.5717 - val_acc: 0.8451\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5941 - acc: 0.8237\n",
      "Epoch 00015: val_loss did not improve from 0.57167\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.5943 - acc: 0.8236 - val_loss: 0.7385 - val_acc: 0.7820\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5706 - acc: 0.8336\n",
      "Epoch 00016: val_loss improved from 0.57167 to 0.54310, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_BN_6_conv_checkpoint/016-0.5431.hdf5\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.5706 - acc: 0.8336 - val_loss: 0.5431 - val_acc: 0.8514\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5577 - acc: 0.8350\n",
      "Epoch 00017: val_loss did not improve from 0.54310\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.5578 - acc: 0.8349 - val_loss: 0.5599 - val_acc: 0.8465\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5368 - acc: 0.8400\n",
      "Epoch 00018: val_loss improved from 0.54310 to 0.54035, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_BN_6_conv_checkpoint/018-0.5404.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.5368 - acc: 0.8400 - val_loss: 0.5404 - val_acc: 0.8528\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5258 - acc: 0.8443\n",
      "Epoch 00019: val_loss improved from 0.54035 to 0.51491, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_BN_6_conv_checkpoint/019-0.5149.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.5258 - acc: 0.8443 - val_loss: 0.5149 - val_acc: 0.8614\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5095 - acc: 0.8492\n",
      "Epoch 00020: val_loss did not improve from 0.51491\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.5096 - acc: 0.8492 - val_loss: 0.6015 - val_acc: 0.8334\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4938 - acc: 0.8547\n",
      "Epoch 00021: val_loss did not improve from 0.51491\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.4938 - acc: 0.8547 - val_loss: 0.5213 - val_acc: 0.8546\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4797 - acc: 0.8589\n",
      "Epoch 00022: val_loss improved from 0.51491 to 0.50025, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_BN_6_conv_checkpoint/022-0.5002.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.4798 - acc: 0.8589 - val_loss: 0.5002 - val_acc: 0.8584\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4706 - acc: 0.8608\n",
      "Epoch 00023: val_loss improved from 0.50025 to 0.47159, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_BN_6_conv_checkpoint/023-0.4716.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.4706 - acc: 0.8608 - val_loss: 0.4716 - val_acc: 0.8724\n",
      "Epoch 24/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4563 - acc: 0.8651\n",
      "Epoch 00024: val_loss did not improve from 0.47159\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.4564 - acc: 0.8650 - val_loss: 0.4804 - val_acc: 0.8735\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4491 - acc: 0.8660\n",
      "Epoch 00025: val_loss did not improve from 0.47159\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.4491 - acc: 0.8659 - val_loss: 0.5440 - val_acc: 0.8456\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4341 - acc: 0.8711\n",
      "Epoch 00026: val_loss improved from 0.47159 to 0.47020, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_BN_6_conv_checkpoint/026-0.4702.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.4343 - acc: 0.8711 - val_loss: 0.4702 - val_acc: 0.8675\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4240 - acc: 0.8742\n",
      "Epoch 00027: val_loss did not improve from 0.47020\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.4241 - acc: 0.8742 - val_loss: 0.5468 - val_acc: 0.8432\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4257 - acc: 0.8733\n",
      "Epoch 00028: val_loss did not improve from 0.47020\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.4256 - acc: 0.8734 - val_loss: 0.4719 - val_acc: 0.8691\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4113 - acc: 0.8783\n",
      "Epoch 00029: val_loss did not improve from 0.47020\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.4114 - acc: 0.8782 - val_loss: 0.4973 - val_acc: 0.8640\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4027 - acc: 0.8784\n",
      "Epoch 00030: val_loss did not improve from 0.47020\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.4026 - acc: 0.8784 - val_loss: 0.4925 - val_acc: 0.8628\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3919 - acc: 0.8829\n",
      "Epoch 00031: val_loss did not improve from 0.47020\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.3920 - acc: 0.8829 - val_loss: 0.5785 - val_acc: 0.8353\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3907 - acc: 0.8830\n",
      "Epoch 00032: val_loss improved from 0.47020 to 0.46994, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_BN_6_conv_checkpoint/032-0.4699.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.3907 - acc: 0.8830 - val_loss: 0.4699 - val_acc: 0.8726\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3818 - acc: 0.8853\n",
      "Epoch 00033: val_loss did not improve from 0.46994\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.3819 - acc: 0.8853 - val_loss: 0.4791 - val_acc: 0.8733\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3716 - acc: 0.8888\n",
      "Epoch 00034: val_loss improved from 0.46994 to 0.46845, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_BN_6_conv_checkpoint/034-0.4684.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.3716 - acc: 0.8888 - val_loss: 0.4684 - val_acc: 0.8707\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3711 - acc: 0.8878\n",
      "Epoch 00035: val_loss improved from 0.46845 to 0.45566, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_BN_6_conv_checkpoint/035-0.4557.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.3711 - acc: 0.8878 - val_loss: 0.4557 - val_acc: 0.8737\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3632 - acc: 0.8907\n",
      "Epoch 00036: val_loss improved from 0.45566 to 0.43123, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_BN_6_conv_checkpoint/036-0.4312.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.3632 - acc: 0.8907 - val_loss: 0.4312 - val_acc: 0.8798\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3548 - acc: 0.8927\n",
      "Epoch 00037: val_loss did not improve from 0.43123\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.3548 - acc: 0.8927 - val_loss: 0.4886 - val_acc: 0.8635\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3533 - acc: 0.8934\n",
      "Epoch 00038: val_loss improved from 0.43123 to 0.40256, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_BN_6_conv_checkpoint/038-0.4026.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.3533 - acc: 0.8934 - val_loss: 0.4026 - val_acc: 0.8919\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3444 - acc: 0.8965\n",
      "Epoch 00039: val_loss did not improve from 0.40256\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.3444 - acc: 0.8966 - val_loss: 0.4390 - val_acc: 0.8761\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3385 - acc: 0.8983\n",
      "Epoch 00040: val_loss did not improve from 0.40256\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.3385 - acc: 0.8983 - val_loss: 0.4139 - val_acc: 0.8793\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3312 - acc: 0.8998\n",
      "Epoch 00041: val_loss did not improve from 0.40256\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.3312 - acc: 0.8998 - val_loss: 0.4237 - val_acc: 0.8863\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3268 - acc: 0.9009\n",
      "Epoch 00042: val_loss did not improve from 0.40256\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.3269 - acc: 0.9009 - val_loss: 0.4425 - val_acc: 0.8768\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3226 - acc: 0.9023\n",
      "Epoch 00043: val_loss did not improve from 0.40256\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.3228 - acc: 0.9022 - val_loss: 0.4133 - val_acc: 0.8880\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3197 - acc: 0.9026\n",
      "Epoch 00044: val_loss did not improve from 0.40256\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.3197 - acc: 0.9026 - val_loss: 0.4162 - val_acc: 0.8868\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3161 - acc: 0.9049\n",
      "Epoch 00045: val_loss did not improve from 0.40256\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.3161 - acc: 0.9049 - val_loss: 0.7285 - val_acc: 0.8032\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3115 - acc: 0.9042\n",
      "Epoch 00046: val_loss improved from 0.40256 to 0.39495, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_BN_6_conv_checkpoint/046-0.3950.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.3115 - acc: 0.9042 - val_loss: 0.3950 - val_acc: 0.8954\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3033 - acc: 0.9083\n",
      "Epoch 00047: val_loss did not improve from 0.39495\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.3033 - acc: 0.9082 - val_loss: 0.4563 - val_acc: 0.8807\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3008 - acc: 0.9082\n",
      "Epoch 00048: val_loss did not improve from 0.39495\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.3009 - acc: 0.9081 - val_loss: 0.4083 - val_acc: 0.8921\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2940 - acc: 0.9108\n",
      "Epoch 00049: val_loss did not improve from 0.39495\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2940 - acc: 0.9108 - val_loss: 0.4224 - val_acc: 0.8819\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2957 - acc: 0.9095\n",
      "Epoch 00050: val_loss improved from 0.39495 to 0.38519, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_BN_6_conv_checkpoint/050-0.3852.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2958 - acc: 0.9095 - val_loss: 0.3852 - val_acc: 0.8933\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2881 - acc: 0.9114\n",
      "Epoch 00051: val_loss did not improve from 0.38519\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2881 - acc: 0.9115 - val_loss: 0.4588 - val_acc: 0.8698\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2837 - acc: 0.9112\n",
      "Epoch 00052: val_loss did not improve from 0.38519\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2840 - acc: 0.9111 - val_loss: 0.5301 - val_acc: 0.8553\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2796 - acc: 0.9140\n",
      "Epoch 00053: val_loss did not improve from 0.38519\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2796 - acc: 0.9140 - val_loss: 0.5769 - val_acc: 0.8463\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2724 - acc: 0.9164\n",
      "Epoch 00054: val_loss did not improve from 0.38519\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2725 - acc: 0.9164 - val_loss: 0.4476 - val_acc: 0.8698\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2727 - acc: 0.9155\n",
      "Epoch 00055: val_loss did not improve from 0.38519\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2727 - acc: 0.9155 - val_loss: 0.4682 - val_acc: 0.8689\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2704 - acc: 0.9166\n",
      "Epoch 00056: val_loss did not improve from 0.38519\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2706 - acc: 0.9165 - val_loss: 0.3989 - val_acc: 0.8947\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2695 - acc: 0.9163\n",
      "Epoch 00057: val_loss did not improve from 0.38519\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2695 - acc: 0.9163 - val_loss: 0.3923 - val_acc: 0.8949\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2624 - acc: 0.9175\n",
      "Epoch 00058: val_loss did not improve from 0.38519\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2625 - acc: 0.9174 - val_loss: 0.4345 - val_acc: 0.8847\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2630 - acc: 0.9191\n",
      "Epoch 00059: val_loss did not improve from 0.38519\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2631 - acc: 0.9191 - val_loss: 0.4291 - val_acc: 0.8761\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2583 - acc: 0.9197\n",
      "Epoch 00060: val_loss did not improve from 0.38519\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2583 - acc: 0.9197 - val_loss: 0.3943 - val_acc: 0.8952\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2497 - acc: 0.9220\n",
      "Epoch 00061: val_loss did not improve from 0.38519\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2497 - acc: 0.9219 - val_loss: 0.4153 - val_acc: 0.8887\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2533 - acc: 0.9195\n",
      "Epoch 00062: val_loss did not improve from 0.38519\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2534 - acc: 0.9195 - val_loss: 0.3913 - val_acc: 0.8919\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2494 - acc: 0.9229\n",
      "Epoch 00063: val_loss did not improve from 0.38519\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2494 - acc: 0.9229 - val_loss: 0.4013 - val_acc: 0.8931\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2426 - acc: 0.9247\n",
      "Epoch 00064: val_loss did not improve from 0.38519\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2426 - acc: 0.9247 - val_loss: 0.4489 - val_acc: 0.8814\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2436 - acc: 0.9245\n",
      "Epoch 00065: val_loss improved from 0.38519 to 0.37092, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_BN_6_conv_checkpoint/065-0.3709.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2436 - acc: 0.9245 - val_loss: 0.3709 - val_acc: 0.9022\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2419 - acc: 0.9254\n",
      "Epoch 00066: val_loss did not improve from 0.37092\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2419 - acc: 0.9254 - val_loss: 0.4206 - val_acc: 0.8870\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2371 - acc: 0.9257\n",
      "Epoch 00067: val_loss did not improve from 0.37092\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2372 - acc: 0.9256 - val_loss: 0.3876 - val_acc: 0.8940\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2314 - acc: 0.9282\n",
      "Epoch 00068: val_loss did not improve from 0.37092\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2314 - acc: 0.9282 - val_loss: 0.4187 - val_acc: 0.8880\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2336 - acc: 0.9272\n",
      "Epoch 00069: val_loss did not improve from 0.37092\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2337 - acc: 0.9271 - val_loss: 0.4064 - val_acc: 0.8898\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2308 - acc: 0.9284\n",
      "Epoch 00070: val_loss did not improve from 0.37092\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2309 - acc: 0.9284 - val_loss: 0.3883 - val_acc: 0.8987\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2243 - acc: 0.9297\n",
      "Epoch 00071: val_loss did not improve from 0.37092\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2243 - acc: 0.9297 - val_loss: 0.4584 - val_acc: 0.8838\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2249 - acc: 0.9284\n",
      "Epoch 00072: val_loss improved from 0.37092 to 0.35431, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_BN_6_conv_checkpoint/072-0.3543.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2250 - acc: 0.9284 - val_loss: 0.3543 - val_acc: 0.9068\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2246 - acc: 0.9293\n",
      "Epoch 00073: val_loss did not improve from 0.35431\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2246 - acc: 0.9293 - val_loss: 0.3814 - val_acc: 0.8994\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2191 - acc: 0.9326\n",
      "Epoch 00074: val_loss did not improve from 0.35431\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2190 - acc: 0.9326 - val_loss: 0.4514 - val_acc: 0.8812\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2137 - acc: 0.9326\n",
      "Epoch 00075: val_loss did not improve from 0.35431\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2138 - acc: 0.9325 - val_loss: 0.3752 - val_acc: 0.9024\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2196 - acc: 0.9311\n",
      "Epoch 00076: val_loss did not improve from 0.35431\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2198 - acc: 0.9311 - val_loss: 0.3897 - val_acc: 0.9033\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2186 - acc: 0.9312\n",
      "Epoch 00077: val_loss did not improve from 0.35431\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2188 - acc: 0.9312 - val_loss: 0.4433 - val_acc: 0.8887\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2117 - acc: 0.9336\n",
      "Epoch 00078: val_loss did not improve from 0.35431\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2117 - acc: 0.9337 - val_loss: 0.3688 - val_acc: 0.9068\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2072 - acc: 0.9343\n",
      "Epoch 00079: val_loss did not improve from 0.35431\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2072 - acc: 0.9343 - val_loss: 0.4095 - val_acc: 0.8933\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2090 - acc: 0.9344\n",
      "Epoch 00080: val_loss did not improve from 0.35431\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2090 - acc: 0.9344 - val_loss: 0.3824 - val_acc: 0.9026\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2015 - acc: 0.9368\n",
      "Epoch 00081: val_loss did not improve from 0.35431\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2018 - acc: 0.9367 - val_loss: 0.3900 - val_acc: 0.8966\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2042 - acc: 0.9365\n",
      "Epoch 00082: val_loss did not improve from 0.35431\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.2042 - acc: 0.9364 - val_loss: 0.3861 - val_acc: 0.8919\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1982 - acc: 0.9362\n",
      "Epoch 00083: val_loss did not improve from 0.35431\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1982 - acc: 0.9362 - val_loss: 0.3865 - val_acc: 0.9019\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2016 - acc: 0.9377\n",
      "Epoch 00084: val_loss did not improve from 0.35431\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2016 - acc: 0.9377 - val_loss: 0.3964 - val_acc: 0.9001\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1971 - acc: 0.9385\n",
      "Epoch 00085: val_loss did not improve from 0.35431\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1971 - acc: 0.9385 - val_loss: 0.3880 - val_acc: 0.8989\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1909 - acc: 0.9379\n",
      "Epoch 00086: val_loss did not improve from 0.35431\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1909 - acc: 0.9379 - val_loss: 0.3705 - val_acc: 0.9068\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1956 - acc: 0.9379\n",
      "Epoch 00087: val_loss did not improve from 0.35431\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1956 - acc: 0.9379 - val_loss: 0.4758 - val_acc: 0.8761\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1927 - acc: 0.9391\n",
      "Epoch 00088: val_loss did not improve from 0.35431\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1927 - acc: 0.9391 - val_loss: 0.3615 - val_acc: 0.9064\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1894 - acc: 0.9396\n",
      "Epoch 00089: val_loss did not improve from 0.35431\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1895 - acc: 0.9396 - val_loss: 0.3825 - val_acc: 0.8984\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1923 - acc: 0.9384\n",
      "Epoch 00090: val_loss did not improve from 0.35431\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1923 - acc: 0.9384 - val_loss: 0.3712 - val_acc: 0.9043\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1861 - acc: 0.9409\n",
      "Epoch 00091: val_loss did not improve from 0.35431\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1862 - acc: 0.9409 - val_loss: 0.3882 - val_acc: 0.9043\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1889 - acc: 0.9398\n",
      "Epoch 00092: val_loss did not improve from 0.35431\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1888 - acc: 0.9398 - val_loss: 0.3691 - val_acc: 0.9040\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1838 - acc: 0.9406\n",
      "Epoch 00093: val_loss did not improve from 0.35431\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1838 - acc: 0.9406 - val_loss: 0.3634 - val_acc: 0.9043\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1839 - acc: 0.9413\n",
      "Epoch 00094: val_loss did not improve from 0.35431\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1839 - acc: 0.9413 - val_loss: 0.3718 - val_acc: 0.9036\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1773 - acc: 0.9442\n",
      "Epoch 00095: val_loss did not improve from 0.35431\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1773 - acc: 0.9442 - val_loss: 0.3706 - val_acc: 0.9068\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1781 - acc: 0.9443\n",
      "Epoch 00096: val_loss did not improve from 0.35431\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1781 - acc: 0.9443 - val_loss: 0.5306 - val_acc: 0.8588\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1777 - acc: 0.9436\n",
      "Epoch 00097: val_loss did not improve from 0.35431\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1778 - acc: 0.9435 - val_loss: 0.3663 - val_acc: 0.9059\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1745 - acc: 0.9443\n",
      "Epoch 00098: val_loss did not improve from 0.35431\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1745 - acc: 0.9443 - val_loss: 0.4294 - val_acc: 0.8884\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1746 - acc: 0.9449\n",
      "Epoch 00099: val_loss did not improve from 0.35431\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1746 - acc: 0.9450 - val_loss: 0.3697 - val_acc: 0.9075\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1723 - acc: 0.9440\n",
      "Epoch 00100: val_loss did not improve from 0.35431\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1725 - acc: 0.9440 - val_loss: 0.3731 - val_acc: 0.9029\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1701 - acc: 0.9447\n",
      "Epoch 00101: val_loss did not improve from 0.35431\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1701 - acc: 0.9447 - val_loss: 0.3922 - val_acc: 0.9008\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1681 - acc: 0.9461\n",
      "Epoch 00102: val_loss did not improve from 0.35431\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1681 - acc: 0.9461 - val_loss: 0.3858 - val_acc: 0.9040\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1661 - acc: 0.9474\n",
      "Epoch 00103: val_loss did not improve from 0.35431\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1662 - acc: 0.9474 - val_loss: 0.4121 - val_acc: 0.8952\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1679 - acc: 0.9457\n",
      "Epoch 00104: val_loss did not improve from 0.35431\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1680 - acc: 0.9457 - val_loss: 0.3604 - val_acc: 0.9057\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1595 - acc: 0.9490\n",
      "Epoch 00105: val_loss did not improve from 0.35431\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1598 - acc: 0.9490 - val_loss: 0.4564 - val_acc: 0.8810\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1687 - acc: 0.9451\n",
      "Epoch 00106: val_loss did not improve from 0.35431\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1688 - acc: 0.9451 - val_loss: 0.4091 - val_acc: 0.8973\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1645 - acc: 0.9464\n",
      "Epoch 00107: val_loss did not improve from 0.35431\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1646 - acc: 0.9463 - val_loss: 0.3795 - val_acc: 0.9071\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1617 - acc: 0.9490\n",
      "Epoch 00108: val_loss did not improve from 0.35431\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1617 - acc: 0.9490 - val_loss: 0.3705 - val_acc: 0.9073\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1596 - acc: 0.9486\n",
      "Epoch 00109: val_loss did not improve from 0.35431\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1598 - acc: 0.9486 - val_loss: 0.3829 - val_acc: 0.9052\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1561 - acc: 0.9492\n",
      "Epoch 00110: val_loss did not improve from 0.35431\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1562 - acc: 0.9492 - val_loss: 0.4234 - val_acc: 0.9001\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1556 - acc: 0.9497\n",
      "Epoch 00111: val_loss did not improve from 0.35431\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1557 - acc: 0.9496 - val_loss: 0.4074 - val_acc: 0.8954\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1586 - acc: 0.9484\n",
      "Epoch 00112: val_loss did not improve from 0.35431\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1587 - acc: 0.9483 - val_loss: 0.4065 - val_acc: 0.9001\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1555 - acc: 0.9510\n",
      "Epoch 00113: val_loss did not improve from 0.35431\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1555 - acc: 0.9509 - val_loss: 0.4081 - val_acc: 0.9012\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1565 - acc: 0.9498\n",
      "Epoch 00114: val_loss did not improve from 0.35431\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1565 - acc: 0.9498 - val_loss: 0.3905 - val_acc: 0.9085\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1495 - acc: 0.9527\n",
      "Epoch 00115: val_loss did not improve from 0.35431\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1495 - acc: 0.9527 - val_loss: 0.3859 - val_acc: 0.9054\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1520 - acc: 0.9512\n",
      "Epoch 00116: val_loss did not improve from 0.35431\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1521 - acc: 0.9512 - val_loss: 0.3719 - val_acc: 0.9122\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1509 - acc: 0.9521\n",
      "Epoch 00117: val_loss did not improve from 0.35431\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1509 - acc: 0.9521 - val_loss: 0.4248 - val_acc: 0.8903\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1491 - acc: 0.9523\n",
      "Epoch 00118: val_loss did not improve from 0.35431\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1491 - acc: 0.9523 - val_loss: 0.3815 - val_acc: 0.9066\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1484 - acc: 0.9518\n",
      "Epoch 00119: val_loss did not improve from 0.35431\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1484 - acc: 0.9519 - val_loss: 0.3930 - val_acc: 0.8973\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1478 - acc: 0.9536\n",
      "Epoch 00120: val_loss did not improve from 0.35431\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1478 - acc: 0.9536 - val_loss: 0.3982 - val_acc: 0.8959\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1434 - acc: 0.9537\n",
      "Epoch 00121: val_loss did not improve from 0.35431\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1436 - acc: 0.9536 - val_loss: 0.3770 - val_acc: 0.9057\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1491 - acc: 0.9522\n",
      "Epoch 00122: val_loss did not improve from 0.35431\n",
      "36805/36805 [==============================] - 45s 1ms/sample - loss: 0.1491 - acc: 0.9522 - val_loss: 0.3601 - val_acc: 0.9113\n",
      "\n",
      "1D_CNN_custom_ch_32_DO_BN_6_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 558us/sample - loss: 0.4171 - acc: 0.8827\n",
      "Loss: 0.4171265884970826 Accuracy: 0.88265836\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.6796 - acc: 0.2001\n",
      "Epoch 00001: val_loss improved from inf to 1.94170, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_BN_7_conv_checkpoint/001-1.9417.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 2.6795 - acc: 0.2002 - val_loss: 1.9417 - val_acc: 0.3988\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.7998 - acc: 0.4182\n",
      "Epoch 00002: val_loss improved from 1.94170 to 1.26337, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_BN_7_conv_checkpoint/002-1.2634.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 1.7999 - acc: 0.4182 - val_loss: 1.2634 - val_acc: 0.6201\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4211 - acc: 0.5375\n",
      "Epoch 00003: val_loss improved from 1.26337 to 1.04946, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_BN_7_conv_checkpoint/003-1.0495.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 1.4211 - acc: 0.5375 - val_loss: 1.0495 - val_acc: 0.6930\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1855 - acc: 0.6218\n",
      "Epoch 00004: val_loss improved from 1.04946 to 0.87118, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_BN_7_conv_checkpoint/004-0.8712.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 1.1855 - acc: 0.6218 - val_loss: 0.8712 - val_acc: 0.7475\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0261 - acc: 0.6769\n",
      "Epoch 00005: val_loss improved from 0.87118 to 0.83058, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_BN_7_conv_checkpoint/005-0.8306.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 1.0261 - acc: 0.6769 - val_loss: 0.8306 - val_acc: 0.7603\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8993 - acc: 0.7193\n",
      "Epoch 00006: val_loss improved from 0.83058 to 0.74415, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_BN_7_conv_checkpoint/006-0.7441.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.8994 - acc: 0.7193 - val_loss: 0.7441 - val_acc: 0.7862\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8132 - acc: 0.7505\n",
      "Epoch 00007: val_loss did not improve from 0.74415\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.8132 - acc: 0.7505 - val_loss: 0.8199 - val_acc: 0.7468\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7410 - acc: 0.7718\n",
      "Epoch 00008: val_loss improved from 0.74415 to 0.57712, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_BN_7_conv_checkpoint/008-0.5771.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.7411 - acc: 0.7717 - val_loss: 0.5771 - val_acc: 0.8365\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6857 - acc: 0.7914\n",
      "Epoch 00009: val_loss improved from 0.57712 to 0.55471, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_BN_7_conv_checkpoint/009-0.5547.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.6857 - acc: 0.7914 - val_loss: 0.5547 - val_acc: 0.8472\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6337 - acc: 0.8098\n",
      "Epoch 00010: val_loss improved from 0.55471 to 0.50022, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_BN_7_conv_checkpoint/010-0.5002.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.6337 - acc: 0.8098 - val_loss: 0.5002 - val_acc: 0.8661\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5919 - acc: 0.8222\n",
      "Epoch 00011: val_loss improved from 0.50022 to 0.48212, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_BN_7_conv_checkpoint/011-0.4821.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.5920 - acc: 0.8222 - val_loss: 0.4821 - val_acc: 0.8668\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5525 - acc: 0.8344\n",
      "Epoch 00012: val_loss did not improve from 0.48212\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.5524 - acc: 0.8344 - val_loss: 0.5869 - val_acc: 0.8211\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5307 - acc: 0.8396\n",
      "Epoch 00013: val_loss improved from 0.48212 to 0.45148, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_BN_7_conv_checkpoint/013-0.4515.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.5307 - acc: 0.8395 - val_loss: 0.4515 - val_acc: 0.8714\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4989 - acc: 0.8491\n",
      "Epoch 00014: val_loss improved from 0.45148 to 0.41922, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_BN_7_conv_checkpoint/014-0.4192.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.4989 - acc: 0.8492 - val_loss: 0.4192 - val_acc: 0.8833\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4708 - acc: 0.8564\n",
      "Epoch 00015: val_loss did not improve from 0.41922\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.4709 - acc: 0.8563 - val_loss: 0.4240 - val_acc: 0.8824\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4508 - acc: 0.8639\n",
      "Epoch 00016: val_loss did not improve from 0.41922\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.4507 - acc: 0.8640 - val_loss: 0.5000 - val_acc: 0.8507\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4306 - acc: 0.8702\n",
      "Epoch 00017: val_loss improved from 0.41922 to 0.41569, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_BN_7_conv_checkpoint/017-0.4157.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.4308 - acc: 0.8702 - val_loss: 0.4157 - val_acc: 0.8756\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4151 - acc: 0.8761\n",
      "Epoch 00018: val_loss improved from 0.41569 to 0.34868, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_BN_7_conv_checkpoint/018-0.3487.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.4151 - acc: 0.8762 - val_loss: 0.3487 - val_acc: 0.9078\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4025 - acc: 0.8774\n",
      "Epoch 00019: val_loss did not improve from 0.34868\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.4025 - acc: 0.8775 - val_loss: 0.4102 - val_acc: 0.8765\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3854 - acc: 0.8846\n",
      "Epoch 00020: val_loss improved from 0.34868 to 0.33131, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_BN_7_conv_checkpoint/020-0.3313.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.3854 - acc: 0.8846 - val_loss: 0.3313 - val_acc: 0.9101\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3707 - acc: 0.8867\n",
      "Epoch 00021: val_loss did not improve from 0.33131\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.3707 - acc: 0.8867 - val_loss: 0.3314 - val_acc: 0.9140\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3626 - acc: 0.8909\n",
      "Epoch 00022: val_loss did not improve from 0.33131\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.3626 - acc: 0.8909 - val_loss: 0.4156 - val_acc: 0.8803\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3501 - acc: 0.8935\n",
      "Epoch 00023: val_loss did not improve from 0.33131\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.3501 - acc: 0.8935 - val_loss: 0.3579 - val_acc: 0.9019\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3379 - acc: 0.8969\n",
      "Epoch 00024: val_loss did not improve from 0.33131\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.3382 - acc: 0.8968 - val_loss: 0.4914 - val_acc: 0.8549\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3359 - acc: 0.8971\n",
      "Epoch 00025: val_loss improved from 0.33131 to 0.31853, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_BN_7_conv_checkpoint/025-0.3185.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.3360 - acc: 0.8971 - val_loss: 0.3185 - val_acc: 0.9173\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3228 - acc: 0.9015\n",
      "Epoch 00026: val_loss did not improve from 0.31853\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.3228 - acc: 0.9015 - val_loss: 0.3440 - val_acc: 0.9045\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3097 - acc: 0.9072\n",
      "Epoch 00027: val_loss improved from 0.31853 to 0.29228, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_BN_7_conv_checkpoint/027-0.2923.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.3098 - acc: 0.9072 - val_loss: 0.2923 - val_acc: 0.9168\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3005 - acc: 0.9079\n",
      "Epoch 00028: val_loss improved from 0.29228 to 0.28316, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_BN_7_conv_checkpoint/028-0.2832.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.3007 - acc: 0.9079 - val_loss: 0.2832 - val_acc: 0.9217\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2925 - acc: 0.9111\n",
      "Epoch 00029: val_loss improved from 0.28316 to 0.25266, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_BN_7_conv_checkpoint/029-0.2527.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2926 - acc: 0.9111 - val_loss: 0.2527 - val_acc: 0.9315\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2898 - acc: 0.9121\n",
      "Epoch 00030: val_loss did not improve from 0.25266\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2898 - acc: 0.9121 - val_loss: 0.2928 - val_acc: 0.9220\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2806 - acc: 0.9148\n",
      "Epoch 00031: val_loss did not improve from 0.25266\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2806 - acc: 0.9148 - val_loss: 0.2556 - val_acc: 0.9315\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2784 - acc: 0.9152\n",
      "Epoch 00032: val_loss did not improve from 0.25266\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2784 - acc: 0.9152 - val_loss: 0.2601 - val_acc: 0.9297\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2712 - acc: 0.9160\n",
      "Epoch 00033: val_loss did not improve from 0.25266\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2711 - acc: 0.9160 - val_loss: 0.2883 - val_acc: 0.9178\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2615 - acc: 0.9201\n",
      "Epoch 00034: val_loss did not improve from 0.25266\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2616 - acc: 0.9201 - val_loss: 0.2683 - val_acc: 0.9271\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2592 - acc: 0.9215\n",
      "Epoch 00035: val_loss did not improve from 0.25266\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2593 - acc: 0.9215 - val_loss: 0.2953 - val_acc: 0.9210\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2547 - acc: 0.9224\n",
      "Epoch 00036: val_loss did not improve from 0.25266\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.2549 - acc: 0.9224 - val_loss: 0.2865 - val_acc: 0.9227\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2506 - acc: 0.9231\n",
      "Epoch 00037: val_loss did not improve from 0.25266\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2506 - acc: 0.9231 - val_loss: 0.3547 - val_acc: 0.9017\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2429 - acc: 0.9243\n",
      "Epoch 00038: val_loss did not improve from 0.25266\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2429 - acc: 0.9242 - val_loss: 0.2968 - val_acc: 0.9201\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2393 - acc: 0.9254\n",
      "Epoch 00039: val_loss improved from 0.25266 to 0.24863, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_BN_7_conv_checkpoint/039-0.2486.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2393 - acc: 0.9254 - val_loss: 0.2486 - val_acc: 0.9345\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2316 - acc: 0.9293\n",
      "Epoch 00040: val_loss improved from 0.24863 to 0.24792, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_BN_7_conv_checkpoint/040-0.2479.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2316 - acc: 0.9293 - val_loss: 0.2479 - val_acc: 0.9355\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2259 - acc: 0.9312\n",
      "Epoch 00041: val_loss did not improve from 0.24792\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2261 - acc: 0.9312 - val_loss: 0.2650 - val_acc: 0.9306\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2263 - acc: 0.9293\n",
      "Epoch 00042: val_loss did not improve from 0.24792\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2266 - acc: 0.9292 - val_loss: 0.2736 - val_acc: 0.9280\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2237 - acc: 0.9302\n",
      "Epoch 00043: val_loss improved from 0.24792 to 0.23996, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_BN_7_conv_checkpoint/043-0.2400.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2237 - acc: 0.9303 - val_loss: 0.2400 - val_acc: 0.9297\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2193 - acc: 0.9309\n",
      "Epoch 00044: val_loss did not improve from 0.23996\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2193 - acc: 0.9309 - val_loss: 0.2585 - val_acc: 0.9269\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2124 - acc: 0.9330\n",
      "Epoch 00045: val_loss did not improve from 0.23996\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2124 - acc: 0.9330 - val_loss: 0.2624 - val_acc: 0.9313\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2051 - acc: 0.9364\n",
      "Epoch 00046: val_loss did not improve from 0.23996\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2051 - acc: 0.9364 - val_loss: 0.3286 - val_acc: 0.9147\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2054 - acc: 0.9365\n",
      "Epoch 00047: val_loss did not improve from 0.23996\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2054 - acc: 0.9365 - val_loss: 0.3000 - val_acc: 0.9187\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2005 - acc: 0.9373\n",
      "Epoch 00048: val_loss improved from 0.23996 to 0.22345, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_BN_7_conv_checkpoint/048-0.2234.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2004 - acc: 0.9373 - val_loss: 0.2234 - val_acc: 0.9434\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1979 - acc: 0.9368\n",
      "Epoch 00049: val_loss did not improve from 0.22345\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1981 - acc: 0.9368 - val_loss: 0.2783 - val_acc: 0.9245\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1999 - acc: 0.9369\n",
      "Epoch 00050: val_loss improved from 0.22345 to 0.22322, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_BN_7_conv_checkpoint/050-0.2232.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2000 - acc: 0.9369 - val_loss: 0.2232 - val_acc: 0.9394\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1976 - acc: 0.9380\n",
      "Epoch 00051: val_loss did not improve from 0.22322\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1976 - acc: 0.9379 - val_loss: 0.2370 - val_acc: 0.9413\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1896 - acc: 0.9404\n",
      "Epoch 00052: val_loss did not improve from 0.22322\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1896 - acc: 0.9404 - val_loss: 0.2250 - val_acc: 0.9390\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1853 - acc: 0.9413\n",
      "Epoch 00053: val_loss did not improve from 0.22322\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1853 - acc: 0.9413 - val_loss: 0.2587 - val_acc: 0.9245\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1857 - acc: 0.9412\n",
      "Epoch 00054: val_loss improved from 0.22322 to 0.22008, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_BN_7_conv_checkpoint/054-0.2201.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1858 - acc: 0.9411 - val_loss: 0.2201 - val_acc: 0.9397\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1803 - acc: 0.9434\n",
      "Epoch 00055: val_loss did not improve from 0.22008\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1803 - acc: 0.9434 - val_loss: 0.2319 - val_acc: 0.9371\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1805 - acc: 0.9433\n",
      "Epoch 00056: val_loss did not improve from 0.22008\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1805 - acc: 0.9434 - val_loss: 0.2249 - val_acc: 0.9413\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1724 - acc: 0.9453\n",
      "Epoch 00057: val_loss did not improve from 0.22008\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1724 - acc: 0.9453 - val_loss: 0.2351 - val_acc: 0.9324\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1773 - acc: 0.9438\n",
      "Epoch 00058: val_loss did not improve from 0.22008\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1774 - acc: 0.9438 - val_loss: 0.2364 - val_acc: 0.9331\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1668 - acc: 0.9467\n",
      "Epoch 00059: val_loss did not improve from 0.22008\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1670 - acc: 0.9467 - val_loss: 0.2333 - val_acc: 0.9380\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1702 - acc: 0.9442\n",
      "Epoch 00060: val_loss did not improve from 0.22008\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1703 - acc: 0.9442 - val_loss: 0.2532 - val_acc: 0.9276\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1692 - acc: 0.9455\n",
      "Epoch 00061: val_loss did not improve from 0.22008\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1692 - acc: 0.9455 - val_loss: 0.2801 - val_acc: 0.9259\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1640 - acc: 0.9480\n",
      "Epoch 00062: val_loss did not improve from 0.22008\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1640 - acc: 0.9480 - val_loss: 0.2285 - val_acc: 0.9369\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1575 - acc: 0.9499\n",
      "Epoch 00063: val_loss improved from 0.22008 to 0.21274, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_BN_7_conv_checkpoint/063-0.2127.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1575 - acc: 0.9500 - val_loss: 0.2127 - val_acc: 0.9418\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1588 - acc: 0.9493\n",
      "Epoch 00064: val_loss did not improve from 0.21274\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1588 - acc: 0.9493 - val_loss: 0.2706 - val_acc: 0.9297\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1560 - acc: 0.9503\n",
      "Epoch 00065: val_loss did not improve from 0.21274\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1559 - acc: 0.9503 - val_loss: 0.2435 - val_acc: 0.9350\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1508 - acc: 0.9513\n",
      "Epoch 00066: val_loss did not improve from 0.21274\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1509 - acc: 0.9513 - val_loss: 0.3061 - val_acc: 0.9161\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1552 - acc: 0.9512\n",
      "Epoch 00067: val_loss did not improve from 0.21274\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1554 - acc: 0.9511 - val_loss: 0.2562 - val_acc: 0.9327\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1573 - acc: 0.9500\n",
      "Epoch 00068: val_loss did not improve from 0.21274\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1572 - acc: 0.9500 - val_loss: 0.2506 - val_acc: 0.9357\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1490 - acc: 0.9524\n",
      "Epoch 00069: val_loss did not improve from 0.21274\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1491 - acc: 0.9524 - val_loss: 0.2312 - val_acc: 0.9425\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1556 - acc: 0.9508\n",
      "Epoch 00070: val_loss did not improve from 0.21274\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1556 - acc: 0.9508 - val_loss: 0.2275 - val_acc: 0.9364\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1447 - acc: 0.9541\n",
      "Epoch 00071: val_loss did not improve from 0.21274\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1447 - acc: 0.9541 - val_loss: 0.2139 - val_acc: 0.9420\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1440 - acc: 0.9530\n",
      "Epoch 00072: val_loss did not improve from 0.21274\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1440 - acc: 0.9530 - val_loss: 0.2782 - val_acc: 0.9257\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1387 - acc: 0.9555\n",
      "Epoch 00073: val_loss did not improve from 0.21274\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1387 - acc: 0.9554 - val_loss: 0.2406 - val_acc: 0.9359\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1438 - acc: 0.9523\n",
      "Epoch 00074: val_loss did not improve from 0.21274\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1439 - acc: 0.9522 - val_loss: 0.2160 - val_acc: 0.9397\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1355 - acc: 0.9571\n",
      "Epoch 00075: val_loss did not improve from 0.21274\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1356 - acc: 0.9571 - val_loss: 0.2274 - val_acc: 0.9362\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1349 - acc: 0.9566\n",
      "Epoch 00076: val_loss did not improve from 0.21274\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1349 - acc: 0.9566 - val_loss: 0.2180 - val_acc: 0.9411\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1305 - acc: 0.9577\n",
      "Epoch 00077: val_loss did not improve from 0.21274\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1305 - acc: 0.9577 - val_loss: 0.2434 - val_acc: 0.9345\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1333 - acc: 0.9569\n",
      "Epoch 00078: val_loss did not improve from 0.21274\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1333 - acc: 0.9569 - val_loss: 0.2301 - val_acc: 0.9352\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1334 - acc: 0.9565\n",
      "Epoch 00079: val_loss did not improve from 0.21274\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1334 - acc: 0.9565 - val_loss: 0.2349 - val_acc: 0.9373\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1310 - acc: 0.9574\n",
      "Epoch 00080: val_loss did not improve from 0.21274\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1311 - acc: 0.9574 - val_loss: 0.2463 - val_acc: 0.9359\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1328 - acc: 0.9561\n",
      "Epoch 00081: val_loss did not improve from 0.21274\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1330 - acc: 0.9561 - val_loss: 0.2312 - val_acc: 0.9404\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1293 - acc: 0.9582\n",
      "Epoch 00082: val_loss did not improve from 0.21274\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.1296 - acc: 0.9581 - val_loss: 0.2355 - val_acc: 0.9413\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1298 - acc: 0.9575\n",
      "Epoch 00083: val_loss did not improve from 0.21274\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1298 - acc: 0.9575 - val_loss: 0.2190 - val_acc: 0.9434\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1238 - acc: 0.9589\n",
      "Epoch 00084: val_loss did not improve from 0.21274\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1238 - acc: 0.9589 - val_loss: 0.2299 - val_acc: 0.9415\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1212 - acc: 0.9612\n",
      "Epoch 00085: val_loss did not improve from 0.21274\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1212 - acc: 0.9612 - val_loss: 0.2708 - val_acc: 0.9320\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1212 - acc: 0.9601\n",
      "Epoch 00086: val_loss did not improve from 0.21274\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1212 - acc: 0.9600 - val_loss: 0.2359 - val_acc: 0.9420\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1221 - acc: 0.9612\n",
      "Epoch 00087: val_loss did not improve from 0.21274\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1222 - acc: 0.9612 - val_loss: 0.2686 - val_acc: 0.9376\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1221 - acc: 0.9595\n",
      "Epoch 00088: val_loss did not improve from 0.21274\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1221 - acc: 0.9594 - val_loss: 0.2503 - val_acc: 0.9378\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1188 - acc: 0.9617\n",
      "Epoch 00089: val_loss did not improve from 0.21274\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1188 - acc: 0.9617 - val_loss: 0.2228 - val_acc: 0.9436\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1182 - acc: 0.9610\n",
      "Epoch 00090: val_loss did not improve from 0.21274\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1182 - acc: 0.9610 - val_loss: 0.2517 - val_acc: 0.9392\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1125 - acc: 0.9634\n",
      "Epoch 00091: val_loss did not improve from 0.21274\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1125 - acc: 0.9634 - val_loss: 0.2246 - val_acc: 0.9432\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1124 - acc: 0.9637\n",
      "Epoch 00092: val_loss did not improve from 0.21274\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1124 - acc: 0.9637 - val_loss: 0.2435 - val_acc: 0.9338\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1086 - acc: 0.9649\n",
      "Epoch 00093: val_loss did not improve from 0.21274\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1088 - acc: 0.9648 - val_loss: 0.2678 - val_acc: 0.9338\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1144 - acc: 0.9622\n",
      "Epoch 00094: val_loss did not improve from 0.21274\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1144 - acc: 0.9622 - val_loss: 0.2267 - val_acc: 0.9408\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1084 - acc: 0.9652\n",
      "Epoch 00095: val_loss did not improve from 0.21274\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1084 - acc: 0.9652 - val_loss: 0.2790 - val_acc: 0.9306\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1097 - acc: 0.9643\n",
      "Epoch 00096: val_loss did not improve from 0.21274\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1097 - acc: 0.9643 - val_loss: 0.3078 - val_acc: 0.9301\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1076 - acc: 0.9646\n",
      "Epoch 00097: val_loss improved from 0.21274 to 0.20918, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_BN_7_conv_checkpoint/097-0.2092.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1078 - acc: 0.9646 - val_loss: 0.2092 - val_acc: 0.9425\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1107 - acc: 0.9645\n",
      "Epoch 00098: val_loss did not improve from 0.20918\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1108 - acc: 0.9645 - val_loss: 0.2396 - val_acc: 0.9399\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1083 - acc: 0.9649\n",
      "Epoch 00099: val_loss did not improve from 0.20918\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1084 - acc: 0.9649 - val_loss: 0.2539 - val_acc: 0.9343\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1022 - acc: 0.9663\n",
      "Epoch 00100: val_loss did not improve from 0.20918\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1022 - acc: 0.9663 - val_loss: 0.2519 - val_acc: 0.9357\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0999 - acc: 0.9678\n",
      "Epoch 00101: val_loss did not improve from 0.20918\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1001 - acc: 0.9678 - val_loss: 0.2208 - val_acc: 0.9462\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1093 - acc: 0.9639\n",
      "Epoch 00102: val_loss did not improve from 0.20918\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1095 - acc: 0.9639 - val_loss: 0.2612 - val_acc: 0.9355\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1023 - acc: 0.9668\n",
      "Epoch 00103: val_loss did not improve from 0.20918\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1023 - acc: 0.9668 - val_loss: 0.2449 - val_acc: 0.9390\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0978 - acc: 0.9676\n",
      "Epoch 00104: val_loss did not improve from 0.20918\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0977 - acc: 0.9676 - val_loss: 0.2386 - val_acc: 0.9406\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0940 - acc: 0.9689\n",
      "Epoch 00105: val_loss did not improve from 0.20918\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0940 - acc: 0.9689 - val_loss: 0.2462 - val_acc: 0.9434\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0971 - acc: 0.9685\n",
      "Epoch 00106: val_loss did not improve from 0.20918\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0971 - acc: 0.9685 - val_loss: 0.2221 - val_acc: 0.9420\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0930 - acc: 0.9704\n",
      "Epoch 00107: val_loss did not improve from 0.20918\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0931 - acc: 0.9704 - val_loss: 0.2518 - val_acc: 0.9401\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1021 - acc: 0.9677\n",
      "Epoch 00108: val_loss did not improve from 0.20918\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1023 - acc: 0.9676 - val_loss: 0.2457 - val_acc: 0.9369\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0961 - acc: 0.9695\n",
      "Epoch 00109: val_loss did not improve from 0.20918\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0962 - acc: 0.9694 - val_loss: 0.2420 - val_acc: 0.9413\n",
      "Epoch 110/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1007 - acc: 0.9665\n",
      "Epoch 00110: val_loss did not improve from 0.20918\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.1007 - acc: 0.9665 - val_loss: 0.2267 - val_acc: 0.9401\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0918 - acc: 0.9699\n",
      "Epoch 00111: val_loss did not improve from 0.20918\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0918 - acc: 0.9699 - val_loss: 0.2408 - val_acc: 0.9408\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0973 - acc: 0.9675\n",
      "Epoch 00112: val_loss did not improve from 0.20918\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0973 - acc: 0.9675 - val_loss: 0.2347 - val_acc: 0.9427\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0915 - acc: 0.9708\n",
      "Epoch 00113: val_loss did not improve from 0.20918\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0915 - acc: 0.9708 - val_loss: 0.2619 - val_acc: 0.9387\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0907 - acc: 0.9702\n",
      "Epoch 00114: val_loss did not improve from 0.20918\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0907 - acc: 0.9702 - val_loss: 0.2305 - val_acc: 0.9425\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0866 - acc: 0.9717\n",
      "Epoch 00115: val_loss did not improve from 0.20918\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0866 - acc: 0.9717 - val_loss: 0.2458 - val_acc: 0.9397\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0875 - acc: 0.9708\n",
      "Epoch 00116: val_loss did not improve from 0.20918\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 0.0876 - acc: 0.9708 - val_loss: 0.2343 - val_acc: 0.9439\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0894 - acc: 0.9706\n",
      "Epoch 00117: val_loss did not improve from 0.20918\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0894 - acc: 0.9706 - val_loss: 0.2328 - val_acc: 0.9467\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0883 - acc: 0.9706\n",
      "Epoch 00118: val_loss did not improve from 0.20918\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0883 - acc: 0.9706 - val_loss: 0.2332 - val_acc: 0.9404\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0864 - acc: 0.9719\n",
      "Epoch 00119: val_loss did not improve from 0.20918\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0865 - acc: 0.9719 - val_loss: 0.2549 - val_acc: 0.9394\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0863 - acc: 0.9723\n",
      "Epoch 00120: val_loss did not improve from 0.20918\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0863 - acc: 0.9723 - val_loss: 0.2327 - val_acc: 0.9439\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0856 - acc: 0.9718\n",
      "Epoch 00121: val_loss did not improve from 0.20918\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0857 - acc: 0.9718 - val_loss: 0.2502 - val_acc: 0.9392\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0866 - acc: 0.9711\n",
      "Epoch 00122: val_loss did not improve from 0.20918\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0866 - acc: 0.9711 - val_loss: 0.2496 - val_acc: 0.9401\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0817 - acc: 0.9727\n",
      "Epoch 00123: val_loss did not improve from 0.20918\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0816 - acc: 0.9727 - val_loss: 0.2290 - val_acc: 0.9464\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0782 - acc: 0.9748\n",
      "Epoch 00124: val_loss did not improve from 0.20918\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0782 - acc: 0.9748 - val_loss: 0.2339 - val_acc: 0.9411\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0827 - acc: 0.9736\n",
      "Epoch 00125: val_loss did not improve from 0.20918\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0827 - acc: 0.9736 - val_loss: 0.2507 - val_acc: 0.9420\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0834 - acc: 0.9727\n",
      "Epoch 00126: val_loss did not improve from 0.20918\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0834 - acc: 0.9727 - val_loss: 0.2305 - val_acc: 0.9446\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0846 - acc: 0.9722\n",
      "Epoch 00127: val_loss did not improve from 0.20918\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0845 - acc: 0.9722 - val_loss: 0.2342 - val_acc: 0.9441\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0781 - acc: 0.9736\n",
      "Epoch 00128: val_loss did not improve from 0.20918\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0780 - acc: 0.9736 - val_loss: 0.2415 - val_acc: 0.9422\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0804 - acc: 0.9738\n",
      "Epoch 00129: val_loss did not improve from 0.20918\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0804 - acc: 0.9738 - val_loss: 0.2752 - val_acc: 0.9352\n",
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0786 - acc: 0.9745\n",
      "Epoch 00130: val_loss did not improve from 0.20918\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0786 - acc: 0.9745 - val_loss: 0.2328 - val_acc: 0.9457\n",
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0795 - acc: 0.9736\n",
      "Epoch 00131: val_loss did not improve from 0.20918\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0795 - acc: 0.9736 - val_loss: 0.2385 - val_acc: 0.9443\n",
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0747 - acc: 0.9752\n",
      "Epoch 00132: val_loss did not improve from 0.20918\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0748 - acc: 0.9752 - val_loss: 0.2444 - val_acc: 0.9418\n",
      "Epoch 133/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0771 - acc: 0.9752\n",
      "Epoch 00133: val_loss did not improve from 0.20918\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0771 - acc: 0.9752 - val_loss: 0.2540 - val_acc: 0.9408\n",
      "Epoch 134/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0736 - acc: 0.9760\n",
      "Epoch 00134: val_loss did not improve from 0.20918\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0737 - acc: 0.9759 - val_loss: 0.2357 - val_acc: 0.9462\n",
      "Epoch 135/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0741 - acc: 0.9758\n",
      "Epoch 00135: val_loss did not improve from 0.20918\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0741 - acc: 0.9758 - val_loss: 0.2540 - val_acc: 0.9401\n",
      "Epoch 136/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0764 - acc: 0.9746\n",
      "Epoch 00136: val_loss did not improve from 0.20918\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0765 - acc: 0.9745 - val_loss: 0.2293 - val_acc: 0.9415\n",
      "Epoch 137/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0781 - acc: 0.9740\n",
      "Epoch 00137: val_loss did not improve from 0.20918\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0781 - acc: 0.9741 - val_loss: 0.2383 - val_acc: 0.9390\n",
      "Epoch 138/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0709 - acc: 0.9770\n",
      "Epoch 00138: val_loss did not improve from 0.20918\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0709 - acc: 0.9770 - val_loss: 0.2365 - val_acc: 0.9371\n",
      "Epoch 139/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0744 - acc: 0.9749\n",
      "Epoch 00139: val_loss did not improve from 0.20918\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0744 - acc: 0.9749 - val_loss: 0.2382 - val_acc: 0.9455\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 140/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0713 - acc: 0.9765\n",
      "Epoch 00140: val_loss did not improve from 0.20918\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0713 - acc: 0.9765 - val_loss: 0.2663 - val_acc: 0.9411\n",
      "Epoch 141/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0705 - acc: 0.9770\n",
      "Epoch 00141: val_loss did not improve from 0.20918\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0706 - acc: 0.9770 - val_loss: 0.2913 - val_acc: 0.9345\n",
      "Epoch 142/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0782 - acc: 0.9742\n",
      "Epoch 00142: val_loss did not improve from 0.20918\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0783 - acc: 0.9741 - val_loss: 0.2475 - val_acc: 0.9462\n",
      "Epoch 143/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0717 - acc: 0.9769\n",
      "Epoch 00143: val_loss did not improve from 0.20918\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0717 - acc: 0.9769 - val_loss: 0.2292 - val_acc: 0.9439\n",
      "Epoch 144/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0724 - acc: 0.9764\n",
      "Epoch 00144: val_loss did not improve from 0.20918\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0724 - acc: 0.9764 - val_loss: 0.2526 - val_acc: 0.9434\n",
      "Epoch 145/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0704 - acc: 0.9773\n",
      "Epoch 00145: val_loss did not improve from 0.20918\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0704 - acc: 0.9773 - val_loss: 0.2468 - val_acc: 0.9448\n",
      "Epoch 146/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0679 - acc: 0.9781\n",
      "Epoch 00146: val_loss did not improve from 0.20918\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0679 - acc: 0.9781 - val_loss: 0.2594 - val_acc: 0.9404\n",
      "Epoch 147/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0680 - acc: 0.9777\n",
      "Epoch 00147: val_loss did not improve from 0.20918\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.0680 - acc: 0.9777 - val_loss: 0.2289 - val_acc: 0.9401\n",
      "\n",
      "1D_CNN_custom_ch_32_DO_BN_7_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 619us/sample - loss: 0.2484 - acc: 0.9294\n",
      "Loss: 0.2484203779499355 Accuracy: 0.92938733\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.8902 - acc: 0.1701\n",
      "Epoch 00001: val_loss improved from inf to 2.09563, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_BN_8_conv_checkpoint/001-2.0956.hdf5\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 2.8902 - acc: 0.1701 - val_loss: 2.0956 - val_acc: 0.3527\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9450 - acc: 0.3808\n",
      "Epoch 00002: val_loss improved from 2.09563 to 1.32522, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_BN_8_conv_checkpoint/002-1.3252.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 1.9450 - acc: 0.3808 - val_loss: 1.3252 - val_acc: 0.6089\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4929 - acc: 0.5225\n",
      "Epoch 00003: val_loss improved from 1.32522 to 1.03278, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_BN_8_conv_checkpoint/003-1.0328.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 1.4928 - acc: 0.5225 - val_loss: 1.0328 - val_acc: 0.6953\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2068 - acc: 0.6174\n",
      "Epoch 00004: val_loss improved from 1.03278 to 0.80452, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_BN_8_conv_checkpoint/004-0.8045.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 1.2069 - acc: 0.6174 - val_loss: 0.8045 - val_acc: 0.7780\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0057 - acc: 0.6847\n",
      "Epoch 00005: val_loss improved from 0.80452 to 0.66519, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_BN_8_conv_checkpoint/005-0.6652.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 1.0057 - acc: 0.6847 - val_loss: 0.6652 - val_acc: 0.8241\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8567 - acc: 0.7352\n",
      "Epoch 00006: val_loss improved from 0.66519 to 0.64260, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_BN_8_conv_checkpoint/006-0.6426.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.8566 - acc: 0.7352 - val_loss: 0.6426 - val_acc: 0.8183\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7469 - acc: 0.7711\n",
      "Epoch 00007: val_loss improved from 0.64260 to 0.51107, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_BN_8_conv_checkpoint/007-0.5111.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.7469 - acc: 0.7711 - val_loss: 0.5111 - val_acc: 0.8612\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6522 - acc: 0.7991\n",
      "Epoch 00008: val_loss improved from 0.51107 to 0.47870, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_BN_8_conv_checkpoint/008-0.4787.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.6525 - acc: 0.7991 - val_loss: 0.4787 - val_acc: 0.8679\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5791 - acc: 0.8237\n",
      "Epoch 00009: val_loss improved from 0.47870 to 0.36842, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_BN_8_conv_checkpoint/009-0.3684.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.5792 - acc: 0.8237 - val_loss: 0.3684 - val_acc: 0.9064\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5227 - acc: 0.8431\n",
      "Epoch 00010: val_loss did not improve from 0.36842\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.5227 - acc: 0.8431 - val_loss: 0.3692 - val_acc: 0.8996\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4749 - acc: 0.8561\n",
      "Epoch 00011: val_loss improved from 0.36842 to 0.31739, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_BN_8_conv_checkpoint/011-0.3174.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4754 - acc: 0.8561 - val_loss: 0.3174 - val_acc: 0.9157\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4385 - acc: 0.8674\n",
      "Epoch 00012: val_loss did not improve from 0.31739\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4386 - acc: 0.8674 - val_loss: 0.3350 - val_acc: 0.9059\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4051 - acc: 0.8769\n",
      "Epoch 00013: val_loss improved from 0.31739 to 0.28501, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_BN_8_conv_checkpoint/013-0.2850.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.4052 - acc: 0.8768 - val_loss: 0.2850 - val_acc: 0.9189\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3797 - acc: 0.8843\n",
      "Epoch 00014: val_loss improved from 0.28501 to 0.26613, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_BN_8_conv_checkpoint/014-0.2661.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3797 - acc: 0.8843 - val_loss: 0.2661 - val_acc: 0.9292\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3546 - acc: 0.8939\n",
      "Epoch 00015: val_loss improved from 0.26613 to 0.24766, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_BN_8_conv_checkpoint/015-0.2477.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3547 - acc: 0.8939 - val_loss: 0.2477 - val_acc: 0.9306\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3344 - acc: 0.8985\n",
      "Epoch 00016: val_loss improved from 0.24766 to 0.23569, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_BN_8_conv_checkpoint/016-0.2357.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3345 - acc: 0.8985 - val_loss: 0.2357 - val_acc: 0.9315\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3137 - acc: 0.9040\n",
      "Epoch 00017: val_loss improved from 0.23569 to 0.23206, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_BN_8_conv_checkpoint/017-0.2321.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.3138 - acc: 0.9040 - val_loss: 0.2321 - val_acc: 0.9373\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2990 - acc: 0.9103\n",
      "Epoch 00018: val_loss improved from 0.23206 to 0.22593, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_BN_8_conv_checkpoint/018-0.2259.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2990 - acc: 0.9103 - val_loss: 0.2259 - val_acc: 0.9394\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2828 - acc: 0.9143\n",
      "Epoch 00019: val_loss improved from 0.22593 to 0.22284, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_BN_8_conv_checkpoint/019-0.2228.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2828 - acc: 0.9143 - val_loss: 0.2228 - val_acc: 0.9383\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2714 - acc: 0.9188\n",
      "Epoch 00020: val_loss improved from 0.22284 to 0.21433, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_BN_8_conv_checkpoint/020-0.2143.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2715 - acc: 0.9187 - val_loss: 0.2143 - val_acc: 0.9387\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2647 - acc: 0.9187\n",
      "Epoch 00021: val_loss improved from 0.21433 to 0.21128, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_BN_8_conv_checkpoint/021-0.2113.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2647 - acc: 0.9187 - val_loss: 0.2113 - val_acc: 0.9385\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2486 - acc: 0.9248\n",
      "Epoch 00022: val_loss did not improve from 0.21128\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2486 - acc: 0.9248 - val_loss: 0.2163 - val_acc: 0.9411\n",
      "Epoch 23/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2356 - acc: 0.9277\n",
      "Epoch 00023: val_loss improved from 0.21128 to 0.20086, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_BN_8_conv_checkpoint/023-0.2009.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2358 - acc: 0.9277 - val_loss: 0.2009 - val_acc: 0.9443\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2367 - acc: 0.9274\n",
      "Epoch 00024: val_loss did not improve from 0.20086\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2368 - acc: 0.9273 - val_loss: 0.2260 - val_acc: 0.9324\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2263 - acc: 0.9295\n",
      "Epoch 00025: val_loss improved from 0.20086 to 0.19231, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_BN_8_conv_checkpoint/025-0.1923.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2263 - acc: 0.9295 - val_loss: 0.1923 - val_acc: 0.9455\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2140 - acc: 0.9339\n",
      "Epoch 00026: val_loss improved from 0.19231 to 0.18763, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_BN_8_conv_checkpoint/026-0.1876.hdf5\n",
      "36805/36805 [==============================] - 47s 1ms/sample - loss: 0.2140 - acc: 0.9339 - val_loss: 0.1876 - val_acc: 0.9434\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2078 - acc: 0.9355\n",
      "Epoch 00027: val_loss improved from 0.18763 to 0.18076, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_BN_8_conv_checkpoint/027-0.1808.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2080 - acc: 0.9354 - val_loss: 0.1808 - val_acc: 0.9515\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2022 - acc: 0.9371\n",
      "Epoch 00028: val_loss did not improve from 0.18076\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.2023 - acc: 0.9370 - val_loss: 0.1914 - val_acc: 0.9488\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1935 - acc: 0.9402\n",
      "Epoch 00029: val_loss improved from 0.18076 to 0.16614, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_BN_8_conv_checkpoint/029-0.1661.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1935 - acc: 0.9402 - val_loss: 0.1661 - val_acc: 0.9509\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1906 - acc: 0.9409\n",
      "Epoch 00030: val_loss did not improve from 0.16614\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1907 - acc: 0.9409 - val_loss: 0.2066 - val_acc: 0.9420\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1835 - acc: 0.9437\n",
      "Epoch 00031: val_loss did not improve from 0.16614\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1835 - acc: 0.9437 - val_loss: 0.1798 - val_acc: 0.9506\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1764 - acc: 0.9448\n",
      "Epoch 00032: val_loss did not improve from 0.16614\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1765 - acc: 0.9448 - val_loss: 0.1749 - val_acc: 0.9497\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1760 - acc: 0.9442\n",
      "Epoch 00033: val_loss did not improve from 0.16614\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1759 - acc: 0.9442 - val_loss: 0.1958 - val_acc: 0.9392\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1679 - acc: 0.9472\n",
      "Epoch 00034: val_loss did not improve from 0.16614\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1679 - acc: 0.9472 - val_loss: 0.1729 - val_acc: 0.9511\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1671 - acc: 0.9480\n",
      "Epoch 00035: val_loss improved from 0.16614 to 0.16388, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_BN_8_conv_checkpoint/035-0.1639.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1676 - acc: 0.9479 - val_loss: 0.1639 - val_acc: 0.9488\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1671 - acc: 0.9477\n",
      "Epoch 00036: val_loss improved from 0.16388 to 0.16124, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_BN_8_conv_checkpoint/036-0.1612.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1672 - acc: 0.9476 - val_loss: 0.1612 - val_acc: 0.9534\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1599 - acc: 0.9504\n",
      "Epoch 00037: val_loss did not improve from 0.16124\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1599 - acc: 0.9504 - val_loss: 0.1718 - val_acc: 0.9553\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1506 - acc: 0.9512\n",
      "Epoch 00038: val_loss did not improve from 0.16124\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1506 - acc: 0.9512 - val_loss: 0.1640 - val_acc: 0.9534\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1513 - acc: 0.9518\n",
      "Epoch 00039: val_loss did not improve from 0.16124\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1513 - acc: 0.9518 - val_loss: 0.1723 - val_acc: 0.9525\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1436 - acc: 0.9550\n",
      "Epoch 00040: val_loss did not improve from 0.16124\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1436 - acc: 0.9550 - val_loss: 0.1851 - val_acc: 0.9490\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1397 - acc: 0.9574\n",
      "Epoch 00041: val_loss did not improve from 0.16124\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1398 - acc: 0.9574 - val_loss: 0.1648 - val_acc: 0.9541\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1400 - acc: 0.9568\n",
      "Epoch 00042: val_loss did not improve from 0.16124\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1402 - acc: 0.9568 - val_loss: 0.1763 - val_acc: 0.9504\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1406 - acc: 0.9563\n",
      "Epoch 00043: val_loss did not improve from 0.16124\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1406 - acc: 0.9563 - val_loss: 0.1736 - val_acc: 0.9490\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1326 - acc: 0.9599\n",
      "Epoch 00044: val_loss did not improve from 0.16124\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1326 - acc: 0.9599 - val_loss: 0.1639 - val_acc: 0.9564\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1285 - acc: 0.9593\n",
      "Epoch 00045: val_loss did not improve from 0.16124\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1287 - acc: 0.9593 - val_loss: 0.1779 - val_acc: 0.9509\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1289 - acc: 0.9598\n",
      "Epoch 00046: val_loss improved from 0.16124 to 0.15633, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_BN_8_conv_checkpoint/046-0.1563.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1289 - acc: 0.9598 - val_loss: 0.1563 - val_acc: 0.9534\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1211 - acc: 0.9611\n",
      "Epoch 00047: val_loss did not improve from 0.15633\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1211 - acc: 0.9611 - val_loss: 0.1676 - val_acc: 0.9527\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1237 - acc: 0.9613\n",
      "Epoch 00048: val_loss did not improve from 0.15633\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1237 - acc: 0.9613 - val_loss: 0.1580 - val_acc: 0.9529\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1185 - acc: 0.9617\n",
      "Epoch 00049: val_loss improved from 0.15633 to 0.15293, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_BN_8_conv_checkpoint/049-0.1529.hdf5\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1185 - acc: 0.9617 - val_loss: 0.1529 - val_acc: 0.9548\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1130 - acc: 0.9642\n",
      "Epoch 00050: val_loss did not improve from 0.15293\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1130 - acc: 0.9642 - val_loss: 0.1729 - val_acc: 0.9555\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1155 - acc: 0.9638\n",
      "Epoch 00051: val_loss did not improve from 0.15293\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1156 - acc: 0.9638 - val_loss: 0.1942 - val_acc: 0.9464\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1121 - acc: 0.9651\n",
      "Epoch 00052: val_loss did not improve from 0.15293\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1121 - acc: 0.9651 - val_loss: 0.1754 - val_acc: 0.9488\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1047 - acc: 0.9673\n",
      "Epoch 00053: val_loss did not improve from 0.15293\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1047 - acc: 0.9673 - val_loss: 0.1628 - val_acc: 0.9520\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1027 - acc: 0.9671\n",
      "Epoch 00054: val_loss did not improve from 0.15293\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1028 - acc: 0.9671 - val_loss: 0.1588 - val_acc: 0.9569\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1067 - acc: 0.9662\n",
      "Epoch 00055: val_loss did not improve from 0.15293\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1067 - acc: 0.9662 - val_loss: 0.1580 - val_acc: 0.9564\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1020 - acc: 0.9674\n",
      "Epoch 00056: val_loss did not improve from 0.15293\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1020 - acc: 0.9675 - val_loss: 0.1940 - val_acc: 0.9481\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0970 - acc: 0.9708\n",
      "Epoch 00057: val_loss did not improve from 0.15293\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0970 - acc: 0.9708 - val_loss: 0.1648 - val_acc: 0.9571\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1001 - acc: 0.9679\n",
      "Epoch 00058: val_loss did not improve from 0.15293\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.1001 - acc: 0.9679 - val_loss: 0.1703 - val_acc: 0.9536\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0998 - acc: 0.9674\n",
      "Epoch 00059: val_loss did not improve from 0.15293\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0998 - acc: 0.9674 - val_loss: 0.1663 - val_acc: 0.9509\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0928 - acc: 0.9705\n",
      "Epoch 00060: val_loss did not improve from 0.15293\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0928 - acc: 0.9705 - val_loss: 0.1785 - val_acc: 0.9515\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0929 - acc: 0.9697\n",
      "Epoch 00061: val_loss did not improve from 0.15293\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0929 - acc: 0.9697 - val_loss: 0.1594 - val_acc: 0.9569\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0882 - acc: 0.9723\n",
      "Epoch 00062: val_loss did not improve from 0.15293\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0883 - acc: 0.9722 - val_loss: 0.1753 - val_acc: 0.9536\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0953 - acc: 0.9699\n",
      "Epoch 00063: val_loss did not improve from 0.15293\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0953 - acc: 0.9699 - val_loss: 0.1715 - val_acc: 0.9536\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0834 - acc: 0.9723\n",
      "Epoch 00064: val_loss did not improve from 0.15293\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0836 - acc: 0.9722 - val_loss: 0.1818 - val_acc: 0.9513\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0876 - acc: 0.9721\n",
      "Epoch 00065: val_loss did not improve from 0.15293\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0877 - acc: 0.9721 - val_loss: 0.1534 - val_acc: 0.9592\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0860 - acc: 0.9721\n",
      "Epoch 00066: val_loss did not improve from 0.15293\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0860 - acc: 0.9721 - val_loss: 0.1584 - val_acc: 0.9562\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0840 - acc: 0.9727\n",
      "Epoch 00067: val_loss did not improve from 0.15293\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0841 - acc: 0.9727 - val_loss: 0.2065 - val_acc: 0.9464\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0852 - acc: 0.9728\n",
      "Epoch 00068: val_loss did not improve from 0.15293\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0851 - acc: 0.9728 - val_loss: 0.1649 - val_acc: 0.9574\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0758 - acc: 0.9760\n",
      "Epoch 00069: val_loss did not improve from 0.15293\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0760 - acc: 0.9760 - val_loss: 0.1687 - val_acc: 0.9585\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0803 - acc: 0.9737\n",
      "Epoch 00070: val_loss did not improve from 0.15293\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0803 - acc: 0.9737 - val_loss: 0.1928 - val_acc: 0.9476\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0745 - acc: 0.9755\n",
      "Epoch 00071: val_loss did not improve from 0.15293\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0745 - acc: 0.9755 - val_loss: 0.1537 - val_acc: 0.9583\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0758 - acc: 0.9758\n",
      "Epoch 00072: val_loss did not improve from 0.15293\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0760 - acc: 0.9758 - val_loss: 0.1675 - val_acc: 0.9553\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0844 - acc: 0.9726\n",
      "Epoch 00073: val_loss did not improve from 0.15293\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0844 - acc: 0.9726 - val_loss: 0.1754 - val_acc: 0.9539\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0694 - acc: 0.9782\n",
      "Epoch 00074: val_loss did not improve from 0.15293\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0694 - acc: 0.9782 - val_loss: 0.1635 - val_acc: 0.9578\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0730 - acc: 0.9766\n",
      "Epoch 00075: val_loss did not improve from 0.15293\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0730 - acc: 0.9766 - val_loss: 0.1592 - val_acc: 0.9571\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0670 - acc: 0.9783\n",
      "Epoch 00076: val_loss did not improve from 0.15293\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0670 - acc: 0.9783 - val_loss: 0.2069 - val_acc: 0.9464\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0695 - acc: 0.9777\n",
      "Epoch 00077: val_loss did not improve from 0.15293\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0695 - acc: 0.9777 - val_loss: 0.1987 - val_acc: 0.9499\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0676 - acc: 0.9780\n",
      "Epoch 00078: val_loss did not improve from 0.15293\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0676 - acc: 0.9780 - val_loss: 0.1949 - val_acc: 0.9504\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0676 - acc: 0.9791\n",
      "Epoch 00079: val_loss did not improve from 0.15293\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0677 - acc: 0.9791 - val_loss: 0.1702 - val_acc: 0.9562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0709 - acc: 0.9765\n",
      "Epoch 00080: val_loss did not improve from 0.15293\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0709 - acc: 0.9766 - val_loss: 0.1835 - val_acc: 0.9532\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0677 - acc: 0.9784\n",
      "Epoch 00081: val_loss did not improve from 0.15293\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0677 - acc: 0.9784 - val_loss: 0.1626 - val_acc: 0.9569\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0624 - acc: 0.9799\n",
      "Epoch 00082: val_loss did not improve from 0.15293\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0624 - acc: 0.9799 - val_loss: 0.1826 - val_acc: 0.9529\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0574 - acc: 0.9821\n",
      "Epoch 00083: val_loss did not improve from 0.15293\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0574 - acc: 0.9821 - val_loss: 0.1827 - val_acc: 0.9518\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0633 - acc: 0.9795\n",
      "Epoch 00084: val_loss did not improve from 0.15293\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0633 - acc: 0.9795 - val_loss: 0.1783 - val_acc: 0.9525\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0621 - acc: 0.9804\n",
      "Epoch 00085: val_loss did not improve from 0.15293\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0621 - acc: 0.9804 - val_loss: 0.1592 - val_acc: 0.9602\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0590 - acc: 0.9809\n",
      "Epoch 00086: val_loss did not improve from 0.15293\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0590 - acc: 0.9809 - val_loss: 0.1885 - val_acc: 0.9483\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0589 - acc: 0.9812\n",
      "Epoch 00087: val_loss did not improve from 0.15293\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0589 - acc: 0.9813 - val_loss: 0.1673 - val_acc: 0.9555\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0581 - acc: 0.9815\n",
      "Epoch 00088: val_loss did not improve from 0.15293\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0581 - acc: 0.9816 - val_loss: 0.1980 - val_acc: 0.9515\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0573 - acc: 0.9817\n",
      "Epoch 00089: val_loss did not improve from 0.15293\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0573 - acc: 0.9817 - val_loss: 0.1684 - val_acc: 0.9576\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0576 - acc: 0.9813\n",
      "Epoch 00090: val_loss did not improve from 0.15293\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0576 - acc: 0.9813 - val_loss: 0.1735 - val_acc: 0.9546\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0568 - acc: 0.9818\n",
      "Epoch 00091: val_loss did not improve from 0.15293\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0568 - acc: 0.9818 - val_loss: 0.1802 - val_acc: 0.9553\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0537 - acc: 0.9825\n",
      "Epoch 00092: val_loss did not improve from 0.15293\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0537 - acc: 0.9825 - val_loss: 0.1977 - val_acc: 0.9527\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0557 - acc: 0.9821\n",
      "Epoch 00093: val_loss did not improve from 0.15293\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0557 - acc: 0.9821 - val_loss: 0.1694 - val_acc: 0.9571\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0504 - acc: 0.9839\n",
      "Epoch 00094: val_loss did not improve from 0.15293\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0504 - acc: 0.9838 - val_loss: 0.1826 - val_acc: 0.9560\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0534 - acc: 0.9824\n",
      "Epoch 00095: val_loss did not improve from 0.15293\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0534 - acc: 0.9824 - val_loss: 0.1714 - val_acc: 0.9553\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0514 - acc: 0.9828\n",
      "Epoch 00096: val_loss did not improve from 0.15293\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0515 - acc: 0.9827 - val_loss: 0.1905 - val_acc: 0.9569\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0529 - acc: 0.9825\n",
      "Epoch 00097: val_loss did not improve from 0.15293\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0529 - acc: 0.9825 - val_loss: 0.1676 - val_acc: 0.9576\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0465 - acc: 0.9855\n",
      "Epoch 00098: val_loss did not improve from 0.15293\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0465 - acc: 0.9855 - val_loss: 0.1868 - val_acc: 0.9529\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0479 - acc: 0.9848\n",
      "Epoch 00099: val_loss did not improve from 0.15293\n",
      "36805/36805 [==============================] - 48s 1ms/sample - loss: 0.0479 - acc: 0.9848 - val_loss: 0.1936 - val_acc: 0.9532\n",
      "\n",
      "1D_CNN_custom_ch_32_DO_BN_8_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 693us/sample - loss: 0.1976 - acc: 0.9391\n",
      "Loss: 0.1975665055052885 Accuracy: 0.9391485\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.4341 - acc: 0.2964\n",
      "Epoch 00001: val_loss improved from inf to 1.53652, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_BN_9_conv_checkpoint/001-1.5365.hdf5\n",
      "36805/36805 [==============================] - 71s 2ms/sample - loss: 2.4339 - acc: 0.2964 - val_loss: 1.5365 - val_acc: 0.5222\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3253 - acc: 0.5812\n",
      "Epoch 00002: val_loss improved from 1.53652 to 0.76083, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_BN_9_conv_checkpoint/002-0.7608.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 1.3254 - acc: 0.5811 - val_loss: 0.7608 - val_acc: 0.7820\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9344 - acc: 0.7112\n",
      "Epoch 00003: val_loss improved from 0.76083 to 0.50482, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_BN_9_conv_checkpoint/003-0.5048.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.9344 - acc: 0.7112 - val_loss: 0.5048 - val_acc: 0.8556\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7158 - acc: 0.7784\n",
      "Epoch 00004: val_loss improved from 0.50482 to 0.42440, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_BN_9_conv_checkpoint/004-0.4244.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.7158 - acc: 0.7783 - val_loss: 0.4244 - val_acc: 0.8796\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5878 - acc: 0.8185\n",
      "Epoch 00005: val_loss improved from 0.42440 to 0.34919, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_BN_9_conv_checkpoint/005-0.3492.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.5877 - acc: 0.8185 - val_loss: 0.3492 - val_acc: 0.9017\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5023 - acc: 0.8465\n",
      "Epoch 00006: val_loss improved from 0.34919 to 0.28997, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_BN_9_conv_checkpoint/006-0.2900.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.5023 - acc: 0.8465 - val_loss: 0.2900 - val_acc: 0.9166\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4335 - acc: 0.8682\n",
      "Epoch 00007: val_loss improved from 0.28997 to 0.26200, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_BN_9_conv_checkpoint/007-0.2620.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.4335 - acc: 0.8682 - val_loss: 0.2620 - val_acc: 0.9241\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3798 - acc: 0.8845\n",
      "Epoch 00008: val_loss did not improve from 0.26200\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3797 - acc: 0.8845 - val_loss: 0.2643 - val_acc: 0.9243\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3424 - acc: 0.8959\n",
      "Epoch 00009: val_loss improved from 0.26200 to 0.26022, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_BN_9_conv_checkpoint/009-0.2602.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3424 - acc: 0.8959 - val_loss: 0.2602 - val_acc: 0.9222\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3119 - acc: 0.9048\n",
      "Epoch 00010: val_loss improved from 0.26022 to 0.20650, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_BN_9_conv_checkpoint/010-0.2065.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.3121 - acc: 0.9048 - val_loss: 0.2065 - val_acc: 0.9401\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2925 - acc: 0.9113\n",
      "Epoch 00011: val_loss did not improve from 0.20650\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2925 - acc: 0.9113 - val_loss: 0.2085 - val_acc: 0.9397\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2659 - acc: 0.9191\n",
      "Epoch 00012: val_loss improved from 0.20650 to 0.19100, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_BN_9_conv_checkpoint/012-0.1910.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2659 - acc: 0.9191 - val_loss: 0.1910 - val_acc: 0.9418\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2489 - acc: 0.9242\n",
      "Epoch 00013: val_loss improved from 0.19100 to 0.17933, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_BN_9_conv_checkpoint/013-0.1793.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2489 - acc: 0.9242 - val_loss: 0.1793 - val_acc: 0.9455\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2294 - acc: 0.9298\n",
      "Epoch 00014: val_loss improved from 0.17933 to 0.16378, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_BN_9_conv_checkpoint/014-0.1638.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2295 - acc: 0.9298 - val_loss: 0.1638 - val_acc: 0.9525\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2171 - acc: 0.9337\n",
      "Epoch 00015: val_loss did not improve from 0.16378\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2171 - acc: 0.9337 - val_loss: 0.1801 - val_acc: 0.9453\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2016 - acc: 0.9391\n",
      "Epoch 00016: val_loss did not improve from 0.16378\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.2016 - acc: 0.9391 - val_loss: 0.1672 - val_acc: 0.9481\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1929 - acc: 0.9403\n",
      "Epoch 00017: val_loss did not improve from 0.16378\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1930 - acc: 0.9403 - val_loss: 0.1856 - val_acc: 0.9443\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1828 - acc: 0.9441\n",
      "Epoch 00018: val_loss did not improve from 0.16378\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1829 - acc: 0.9440 - val_loss: 0.1799 - val_acc: 0.9483\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1733 - acc: 0.9464\n",
      "Epoch 00019: val_loss improved from 0.16378 to 0.14799, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_BN_9_conv_checkpoint/019-0.1480.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1733 - acc: 0.9464 - val_loss: 0.1480 - val_acc: 0.9569\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1624 - acc: 0.9499\n",
      "Epoch 00020: val_loss did not improve from 0.14799\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1625 - acc: 0.9499 - val_loss: 0.1670 - val_acc: 0.9506\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1532 - acc: 0.9531\n",
      "Epoch 00021: val_loss did not improve from 0.14799\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1534 - acc: 0.9530 - val_loss: 0.1533 - val_acc: 0.9553\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1473 - acc: 0.9542\n",
      "Epoch 00022: val_loss did not improve from 0.14799\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1474 - acc: 0.9541 - val_loss: 0.1840 - val_acc: 0.9422\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1388 - acc: 0.9568\n",
      "Epoch 00023: val_loss improved from 0.14799 to 0.14682, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_BN_9_conv_checkpoint/023-0.1468.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1388 - acc: 0.9569 - val_loss: 0.1468 - val_acc: 0.9555\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1326 - acc: 0.9594\n",
      "Epoch 00024: val_loss improved from 0.14682 to 0.14572, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_BN_9_conv_checkpoint/024-0.1457.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1326 - acc: 0.9594 - val_loss: 0.1457 - val_acc: 0.9564\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1276 - acc: 0.9604\n",
      "Epoch 00025: val_loss did not improve from 0.14572\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1276 - acc: 0.9604 - val_loss: 0.1869 - val_acc: 0.9434\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1213 - acc: 0.9618\n",
      "Epoch 00026: val_loss did not improve from 0.14572\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1213 - acc: 0.9618 - val_loss: 0.1497 - val_acc: 0.9546\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1129 - acc: 0.9651\n",
      "Epoch 00027: val_loss improved from 0.14572 to 0.14147, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_BN_9_conv_checkpoint/027-0.1415.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1129 - acc: 0.9651 - val_loss: 0.1415 - val_acc: 0.9578\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1115 - acc: 0.9659\n",
      "Epoch 00028: val_loss improved from 0.14147 to 0.13641, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_BN_9_conv_checkpoint/028-0.1364.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1115 - acc: 0.9659 - val_loss: 0.1364 - val_acc: 0.9597\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1062 - acc: 0.9668\n",
      "Epoch 00029: val_loss did not improve from 0.13641\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1063 - acc: 0.9668 - val_loss: 0.1558 - val_acc: 0.9520\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1054 - acc: 0.9667\n",
      "Epoch 00030: val_loss did not improve from 0.13641\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.1055 - acc: 0.9667 - val_loss: 0.1613 - val_acc: 0.9499\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0992 - acc: 0.9690\n",
      "Epoch 00031: val_loss did not improve from 0.13641\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0993 - acc: 0.9690 - val_loss: 0.1491 - val_acc: 0.9543\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0956 - acc: 0.9704\n",
      "Epoch 00032: val_loss improved from 0.13641 to 0.13080, saving model to model/checkpoint/1D_CNN_custom_ch_32_DO_BN_9_conv_checkpoint/032-0.1308.hdf5\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0956 - acc: 0.9704 - val_loss: 0.1308 - val_acc: 0.9609\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0889 - acc: 0.9721\n",
      "Epoch 00033: val_loss did not improve from 0.13080\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0889 - acc: 0.9721 - val_loss: 0.1514 - val_acc: 0.9553\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0872 - acc: 0.9730\n",
      "Epoch 00034: val_loss did not improve from 0.13080\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0872 - acc: 0.9730 - val_loss: 0.1365 - val_acc: 0.9581\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0792 - acc: 0.9749\n",
      "Epoch 00035: val_loss did not improve from 0.13080\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0793 - acc: 0.9749 - val_loss: 0.1645 - val_acc: 0.9525\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0865 - acc: 0.9732\n",
      "Epoch 00036: val_loss did not improve from 0.13080\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0865 - acc: 0.9732 - val_loss: 0.1440 - val_acc: 0.9583\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0759 - acc: 0.9768\n",
      "Epoch 00037: val_loss did not improve from 0.13080\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0758 - acc: 0.9769 - val_loss: 0.1494 - val_acc: 0.9560\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0718 - acc: 0.9769\n",
      "Epoch 00038: val_loss did not improve from 0.13080\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0720 - acc: 0.9769 - val_loss: 0.1525 - val_acc: 0.9578\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0775 - acc: 0.9753\n",
      "Epoch 00039: val_loss did not improve from 0.13080\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0775 - acc: 0.9753 - val_loss: 0.1479 - val_acc: 0.9571\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0661 - acc: 0.9796\n",
      "Epoch 00040: val_loss did not improve from 0.13080\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0661 - acc: 0.9796 - val_loss: 0.1398 - val_acc: 0.9604\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0679 - acc: 0.9787\n",
      "Epoch 00041: val_loss did not improve from 0.13080\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0679 - acc: 0.9787 - val_loss: 0.1513 - val_acc: 0.9560\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0665 - acc: 0.9794\n",
      "Epoch 00042: val_loss did not improve from 0.13080\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0665 - acc: 0.9794 - val_loss: 0.1606 - val_acc: 0.9553\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0604 - acc: 0.9805\n",
      "Epoch 00043: val_loss did not improve from 0.13080\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0604 - acc: 0.9805 - val_loss: 0.1344 - val_acc: 0.9585\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0613 - acc: 0.9799\n",
      "Epoch 00044: val_loss did not improve from 0.13080\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0613 - acc: 0.9799 - val_loss: 0.1562 - val_acc: 0.9562\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0626 - acc: 0.9797\n",
      "Epoch 00045: val_loss did not improve from 0.13080\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0626 - acc: 0.9796 - val_loss: 0.1496 - val_acc: 0.9583\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0577 - acc: 0.9828\n",
      "Epoch 00046: val_loss did not improve from 0.13080\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0577 - acc: 0.9828 - val_loss: 0.1681 - val_acc: 0.9557\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0502 - acc: 0.9846\n",
      "Epoch 00047: val_loss did not improve from 0.13080\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0502 - acc: 0.9846 - val_loss: 0.1485 - val_acc: 0.9602\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0494 - acc: 0.9851\n",
      "Epoch 00048: val_loss did not improve from 0.13080\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0494 - acc: 0.9851 - val_loss: 0.1405 - val_acc: 0.9604\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0489 - acc: 0.9854\n",
      "Epoch 00049: val_loss did not improve from 0.13080\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0489 - acc: 0.9854 - val_loss: 0.1489 - val_acc: 0.9583\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0519 - acc: 0.9837\n",
      "Epoch 00050: val_loss did not improve from 0.13080\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0519 - acc: 0.9838 - val_loss: 0.1535 - val_acc: 0.9585\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0476 - acc: 0.9848\n",
      "Epoch 00051: val_loss did not improve from 0.13080\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0476 - acc: 0.9847 - val_loss: 0.1724 - val_acc: 0.9560\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0464 - acc: 0.9858\n",
      "Epoch 00052: val_loss did not improve from 0.13080\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0464 - acc: 0.9858 - val_loss: 0.1481 - val_acc: 0.9574\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0448 - acc: 0.9860\n",
      "Epoch 00053: val_loss did not improve from 0.13080\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0448 - acc: 0.9860 - val_loss: 0.1417 - val_acc: 0.9609\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0439 - acc: 0.9865\n",
      "Epoch 00054: val_loss did not improve from 0.13080\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0439 - acc: 0.9865 - val_loss: 0.1529 - val_acc: 0.9592\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0428 - acc: 0.9867\n",
      "Epoch 00055: val_loss did not improve from 0.13080\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0428 - acc: 0.9867 - val_loss: 0.1719 - val_acc: 0.9541\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0437 - acc: 0.9859\n",
      "Epoch 00056: val_loss did not improve from 0.13080\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0437 - acc: 0.9859 - val_loss: 0.1876 - val_acc: 0.9513\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0415 - acc: 0.9870\n",
      "Epoch 00057: val_loss did not improve from 0.13080\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0415 - acc: 0.9870 - val_loss: 0.1666 - val_acc: 0.9590\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0375 - acc: 0.9885\n",
      "Epoch 00058: val_loss did not improve from 0.13080\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0377 - acc: 0.9885 - val_loss: 0.1783 - val_acc: 0.9550\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0480 - acc: 0.9843\n",
      "Epoch 00059: val_loss did not improve from 0.13080\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0481 - acc: 0.9843 - val_loss: 0.1517 - val_acc: 0.9592\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0382 - acc: 0.9880\n",
      "Epoch 00060: val_loss did not improve from 0.13080\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0384 - acc: 0.9880 - val_loss: 0.1560 - val_acc: 0.9623\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0403 - acc: 0.9878\n",
      "Epoch 00061: val_loss did not improve from 0.13080\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0403 - acc: 0.9878 - val_loss: 0.1490 - val_acc: 0.9578\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0307 - acc: 0.9912\n",
      "Epoch 00062: val_loss did not improve from 0.13080\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0307 - acc: 0.9912 - val_loss: 0.1633 - val_acc: 0.9555\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0316 - acc: 0.9904\n",
      "Epoch 00063: val_loss did not improve from 0.13080\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0316 - acc: 0.9904 - val_loss: 0.1657 - val_acc: 0.9567\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0369 - acc: 0.9887\n",
      "Epoch 00064: val_loss did not improve from 0.13080\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0370 - acc: 0.9887 - val_loss: 0.1580 - val_acc: 0.9574\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0366 - acc: 0.9886\n",
      "Epoch 00065: val_loss did not improve from 0.13080\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0366 - acc: 0.9886 - val_loss: 0.1596 - val_acc: 0.9588\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0307 - acc: 0.9911\n",
      "Epoch 00066: val_loss did not improve from 0.13080\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0310 - acc: 0.9911 - val_loss: 0.1739 - val_acc: 0.9567\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0375 - acc: 0.9886\n",
      "Epoch 00067: val_loss did not improve from 0.13080\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0376 - acc: 0.9886 - val_loss: 0.1540 - val_acc: 0.9611\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0406 - acc: 0.9877\n",
      "Epoch 00068: val_loss did not improve from 0.13080\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0406 - acc: 0.9877 - val_loss: 0.1500 - val_acc: 0.9641\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0292 - acc: 0.9910\n",
      "Epoch 00069: val_loss did not improve from 0.13080\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0293 - acc: 0.9910 - val_loss: 0.1515 - val_acc: 0.9620\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0322 - acc: 0.9903\n",
      "Epoch 00070: val_loss did not improve from 0.13080\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0322 - acc: 0.9903 - val_loss: 0.1439 - val_acc: 0.9651\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0254 - acc: 0.9927\n",
      "Epoch 00071: val_loss did not improve from 0.13080\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0254 - acc: 0.9927 - val_loss: 0.1797 - val_acc: 0.9578\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0255 - acc: 0.9929\n",
      "Epoch 00072: val_loss did not improve from 0.13080\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0256 - acc: 0.9929 - val_loss: 0.1588 - val_acc: 0.9599\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0311 - acc: 0.9905\n",
      "Epoch 00073: val_loss did not improve from 0.13080\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0311 - acc: 0.9905 - val_loss: 0.1564 - val_acc: 0.9588\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0270 - acc: 0.9918\n",
      "Epoch 00074: val_loss did not improve from 0.13080\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0270 - acc: 0.9918 - val_loss: 0.1646 - val_acc: 0.9606\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0272 - acc: 0.9912\n",
      "Epoch 00075: val_loss did not improve from 0.13080\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0273 - acc: 0.9911 - val_loss: 0.1642 - val_acc: 0.9564\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0317 - acc: 0.9901\n",
      "Epoch 00076: val_loss did not improve from 0.13080\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0318 - acc: 0.9901 - val_loss: 0.1604 - val_acc: 0.9574\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0357 - acc: 0.9886\n",
      "Epoch 00077: val_loss did not improve from 0.13080\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0357 - acc: 0.9886 - val_loss: 0.1897 - val_acc: 0.9583\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0256 - acc: 0.9924\n",
      "Epoch 00078: val_loss did not improve from 0.13080\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0256 - acc: 0.9924 - val_loss: 0.1872 - val_acc: 0.9550\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0220 - acc: 0.9937\n",
      "Epoch 00079: val_loss did not improve from 0.13080\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0223 - acc: 0.9936 - val_loss: 0.2252 - val_acc: 0.9506\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0342 - acc: 0.9892\n",
      "Epoch 00080: val_loss did not improve from 0.13080\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0342 - acc: 0.9892 - val_loss: 0.1584 - val_acc: 0.9609\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0205 - acc: 0.9942\n",
      "Epoch 00081: val_loss did not improve from 0.13080\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0205 - acc: 0.9942 - val_loss: 0.1647 - val_acc: 0.9625\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0201 - acc: 0.9938\n",
      "Epoch 00082: val_loss did not improve from 0.13080\n",
      "36805/36805 [==============================] - 49s 1ms/sample - loss: 0.0201 - acc: 0.9938 - val_loss: 0.1738 - val_acc: 0.9557\n",
      "\n",
      "1D_CNN_custom_ch_32_DO_BN_9_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 702us/sample - loss: 0.1950 - acc: 0.9425\n",
      "Loss: 0.19495002571203254 Accuracy: 0.94247144\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(3, 10):\n",
    "    base = '1D_CNN_custom_ch_32_DO_BN'\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    model = build_1d_cnn_custom_ch_32_DO_BN(conv_num=i)\n",
    "#         model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4),\n",
    "          metrics=['accuracy'])\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model_filename = model_path+'{epoch:03d}-{val_loss:.4f}.hdf5'\n",
    "    checkpointer = ModelCheckpoint(filepath = model_filename, monitor = \"val_loss\", \n",
    "                                   verbose=1, save_best_only=True)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=50)\n",
    "    hist = model.fit(x_train_abs, y_train_onehot, batch_size=64, epochs=500, \n",
    "                     validation_data=[x_val_abs, y_val_onehot], shuffle=True, \n",
    "                     callbacks = [checkpointer, early_stopping])\n",
    "\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "    ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "    ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "    ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    png_path = 'visualization/learning_curve/'\n",
    "    filename = model_name+'.png'\n",
    "    os.makedirs(png_path, exist_ok=True)\n",
    "    fig.savefig(png_path+filename, transparent=True)\n",
    "\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "    model = load_model(model_filename)\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "    print()\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_ch_32_DO_BN_3_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_42 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_42 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_42 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_43 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_43 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_43 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_44 (Conv1D)           (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_44 (B (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_44 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 56864)             0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 56864)             0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 16)                909840    \n",
      "=================================================================\n",
      "Total params: 920,720\n",
      "Trainable params: 920,528\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 575us/sample - loss: 1.4817 - acc: 0.5676\n",
      "Loss: 1.4816579678100712 Accuracy: 0.56760126\n",
      "\n",
      "1D_CNN_custom_ch_32_DO_BN_4_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_45 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_45 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_45 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_46 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_46 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_46 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_47 (Conv1D)           (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_47 (B (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_47 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_48 (Conv1D)           (None, 1777, 32)          5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_48 (B (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_48 (Activation)   (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 18944)             0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 18944)             0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 16)                303120    \n",
      "=================================================================\n",
      "Total params: 319,280\n",
      "Trainable params: 319,024\n",
      "Non-trainable params: 256\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 636us/sample - loss: 0.9394 - acc: 0.7377\n",
      "Loss: 0.9393778567249778 Accuracy: 0.7376947\n",
      "\n",
      "1D_CNN_custom_ch_32_DO_BN_5_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_49 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_49 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_49 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_50 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_50 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_50 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_51 (Conv1D)           (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_51 (B (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_51 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_52 (Conv1D)           (None, 1777, 32)          5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_52 (B (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_52 (Activation)   (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_53 (Conv1D)           (None, 592, 64)           10304     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_53 (B (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_53 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 12608)             0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 12608)             0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 16)                201744    \n",
      "=================================================================\n",
      "Total params: 228,464\n",
      "Trainable params: 228,080\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 683us/sample - loss: 0.6981 - acc: 0.8039\n",
      "Loss: 0.6980502425819667 Accuracy: 0.803946\n",
      "\n",
      "1D_CNN_custom_ch_32_DO_BN_6_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_54 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_54 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_54 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_55 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_55 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_55 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_56 (Conv1D)           (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_56 (B (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_56 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_57 (Conv1D)           (None, 1777, 32)          5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_57 (B (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_57 (Activation)   (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_58 (Conv1D)           (None, 592, 64)           10304     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_58 (B (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_58 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_59 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_59 (B (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_59 (Activation)   (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 4160)              0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 4160)              0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 16)                66576     \n",
      "=================================================================\n",
      "Total params: 114,096\n",
      "Trainable params: 113,584\n",
      "Non-trainable params: 512\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 694us/sample - loss: 0.4171 - acc: 0.8827\n",
      "Loss: 0.4171265884970826 Accuracy: 0.88265836\n",
      "\n",
      "1D_CNN_custom_ch_32_DO_BN_7_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_60 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_60 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_60 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_61 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_61 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_61 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_62 (Conv1D)           (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_62 (B (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_62 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_63 (Conv1D)           (None, 1777, 32)          5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_63 (B (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_63 (Activation)   (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_64 (Conv1D)           (None, 592, 64)           10304     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_64 (B (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_64 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_65 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_65 (B (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_65 (Activation)   (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_66 (Conv1D)           (None, 65, 64)            20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_66 (B (None, 65, 64)            256       \n",
      "_________________________________________________________________\n",
      "activation_66 (Activation)   (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 1344)              0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 1344)              0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 16)                21520     \n",
      "=================================================================\n",
      "Total params: 89,840\n",
      "Trainable params: 89,200\n",
      "Non-trainable params: 640\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 744us/sample - loss: 0.2484 - acc: 0.9294\n",
      "Loss: 0.2484203779499355 Accuracy: 0.92938733\n",
      "\n",
      "1D_CNN_custom_ch_32_DO_BN_8_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_67 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_67 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_67 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_68 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_68 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_68 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_69 (Conv1D)           (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_69 (B (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_69 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_70 (Conv1D)           (None, 1777, 32)          5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_70 (B (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_70 (Activation)   (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_71 (Conv1D)           (None, 592, 64)           10304     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_71 (B (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_71 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_72 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_72 (B (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_72 (Activation)   (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_73 (Conv1D)           (None, 65, 64)            20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_73 (B (None, 65, 64)            256       \n",
      "_________________________________________________________________\n",
      "activation_73 (Activation)   (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_74 (Conv1D)           (None, 21, 64)            20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_74 (B (None, 21, 64)            256       \n",
      "_________________________________________________________________\n",
      "activation_74 (Activation)   (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling (None, 7, 64)             0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 448)               0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 448)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 16)                7184      \n",
      "=================================================================\n",
      "Total params: 96,304\n",
      "Trainable params: 95,536\n",
      "Non-trainable params: 768\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 697us/sample - loss: 0.1976 - acc: 0.9391\n",
      "Loss: 0.1975665055052885 Accuracy: 0.9391485\n",
      "\n",
      "1D_CNN_custom_ch_32_DO_BN_9_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_75 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_75 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_75 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_76 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_76 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_76 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_77 (Conv1D)           (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_77 (B (None, 5333, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_77 (Activation)   (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_78 (Conv1D)           (None, 1777, 32)          5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_78 (B (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_78 (Activation)   (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_79 (Conv1D)           (None, 592, 64)           10304     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_79 (B (None, 592, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_79 (Activation)   (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_80 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_80 (B (None, 197, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_80 (Activation)   (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_66 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_81 (Conv1D)           (None, 65, 64)            20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_81 (B (None, 65, 64)            256       \n",
      "_________________________________________________________________\n",
      "activation_81 (Activation)   (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_67 (MaxPooling (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_82 (Conv1D)           (None, 21, 64)            20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_82 (B (None, 21, 64)            256       \n",
      "_________________________________________________________________\n",
      "activation_82 (Activation)   (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_68 (MaxPooling (None, 7, 64)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_83 (Conv1D)           (None, 7, 128)            41088     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_83 (B (None, 7, 128)            512       \n",
      "_________________________________________________________________\n",
      "activation_83 (Activation)   (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_69 (MaxPooling (None, 2, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 16)                4112      \n",
      "=================================================================\n",
      "Total params: 134,832\n",
      "Trainable params: 133,808\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 814us/sample - loss: 0.1950 - acc: 0.9425\n",
      "Loss: 0.19495002571203254 Accuracy: 0.94247144\n"
     ]
    }
   ],
   "source": [
    "log_dir = 'log'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "base = '1D_CNN_custom_ch_32_DO_BN'\n",
    "\n",
    "with open(path.join(log_dir, base), 'w') as log_file:\n",
    "    for i in range(3, 10):\n",
    "        model_name = base+'_{}_conv'.format(i)\n",
    "        print()\n",
    "        print(model_name, 'Model')\n",
    "        model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "        model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "\n",
    "        model = load_model(model_filename)\n",
    "        model.summary()\n",
    "\n",
    "        [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "        print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "        del(model)\n",
    "\n",
    "        log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
