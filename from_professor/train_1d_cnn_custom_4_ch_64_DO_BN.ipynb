{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path as path\n",
    "import itertools\n",
    "from sklearn.preprocessing import maxabs_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Input,InputLayer, Dense, Activation, BatchNormalization, Flatten, Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D, Dropout\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler, \\\n",
    "                                        EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '6'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = path.join('..', 'data')\n",
    "data_dir = path.join(base_dir, 'data_speech_commands_v0.02')\n",
    " \n",
    "train_txt = path.join(data_dir, 'wav_train_16words.txt')\n",
    "val_txt = path.join(data_dir, 'wav_validation_16words.txt')\n",
    "test_txt = path.join(data_dir, 'wav_test_16words.txt')\n",
    "\n",
    "train_data = np.load(path.join(data_dir, 'wav_train_data.npz'))\n",
    "val_data = np.load(path.join(data_dir, 'wav_validation_data.npz'))\n",
    "test_data = np.load(path.join(data_dir, 'wav_test_data.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36805, 16000, 1),\n",
       " (36805,),\n",
       " (4293, 16000, 1),\n",
       " (4293,),\n",
       " (4815, 16000, 1),\n",
       " (4815,),\n",
       " (16, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = train_data['x_train']\n",
    "y_train = train_data['y_train']\n",
    "x_val = val_data['x_val']\n",
    "y_val = val_data['y_val']\n",
    "x_test = test_data['x_test']\n",
    "y_test = test_data['y_test']\n",
    "y_table = test_data['table']\n",
    "\n",
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape, y_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_test[0].shape\n",
    "output_size = y_table.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_abs = np.asarray([maxabs_scale(wav) for wav in x_train])\n",
    "y_train_onehot = np.asarray([to_categorical(label, output_size) for label in y_train])\n",
    "del x_train, y_train\n",
    "\n",
    "x_val_abs = np.asarray([maxabs_scale(wav) for wav in x_val])\n",
    "y_val_onehot = np.asarray([to_categorical(label, output_size) for label in y_val])\n",
    "del x_val, y_val\n",
    "\n",
    "x_test_abs = np.asarray([maxabs_scale(wav) for wav in x_test])\n",
    "y_test_onehot = np.asarray([to_categorical(label, output_size) for label in y_test])\n",
    "del x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_1d_cnn_custom_DO_BN(conv_num=1):\n",
    "    init_channel = 64\n",
    "    \n",
    "    model=Sequential()\n",
    "    model.add(Conv1D (kernel_size=5, filters=init_channel, strides=1, \n",
    "                      padding='same', input_shape=input_shape))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    for i in range(conv_num-1):\n",
    "        model.add(Conv1D (kernel_size=5, filters=int(init_channel/(2**int((i+1)/3))), \n",
    "                          strides=1, padding='same'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(MaxPooling1D(pool_size=3, strides=3))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(output_size, activation='softmax' ))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1 (Batc (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1024000)           0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1024000)           0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                16384016  \n",
      "=================================================================\n",
      "Total params: 16,384,656\n",
      "Trainable params: 16,384,528\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_1 (Ba (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_2 (Ba (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 341312)            0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 341312)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                5461008   \n",
      "=================================================================\n",
      "Total params: 5,482,448\n",
      "Trainable params: 5,482,192\n",
      "Non-trainable params: 256\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_3 (Conv1D)            (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_3 (Ba (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_4 (Ba (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_5 (Ba (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                1819664   \n",
      "=================================================================\n",
      "Total params: 1,861,904\n",
      "Trainable params: 1,861,520\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_6 (Conv1D)            (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_6 (Ba (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_7 (Ba (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_8 (Ba (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 1777, 32)          10272     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_9 (Ba (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 18944)             0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 18944)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                303120    \n",
      "=================================================================\n",
      "Total params: 355,760\n",
      "Trainable params: 355,312\n",
      "Non-trainable params: 448\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_10 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_10 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_11 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_12 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_12 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 1777, 32)          10272     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_13 (B (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 592, 32)           5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_14 (B (None, 592, 32)           128       \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1 (None, 197, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 6304)              0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 6304)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                100880    \n",
      "=================================================================\n",
      "Total params: 158,800\n",
      "Trainable params: 158,288\n",
      "Non-trainable params: 512\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_15 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_15 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_16 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_16 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_17 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_17 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_18 (Conv1D)           (None, 1777, 32)          10272     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_18 (B (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_19 (Conv1D)           (None, 592, 32)           5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_19 (B (None, 592, 32)           128       \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling (None, 197, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_20 (Conv1D)           (None, 197, 32)           5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_20 (B (None, 197, 32)           128       \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 197, 32)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling (None, 65, 32)            0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 2080)              0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 2080)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 16)                33296     \n",
      "=================================================================\n",
      "Total params: 96,496\n",
      "Trainable params: 95,920\n",
      "Non-trainable params: 576\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_21 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_21 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_22 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_22 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_23 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_23 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_24 (Conv1D)           (None, 1777, 32)          10272     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_24 (B (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_25 (Conv1D)           (None, 592, 32)           5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_25 (B (None, 592, 32)           128       \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling (None, 197, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_26 (Conv1D)           (None, 197, 32)           5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_26 (B (None, 197, 32)           128       \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 197, 32)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling (None, 65, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_27 (Conv1D)           (None, 65, 16)            2576      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_27 (B (None, 65, 16)            64        \n",
      "_________________________________________________________________\n",
      "activation_27 (Activation)   (None, 65, 16)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling (None, 21, 16)            0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 336)               0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 336)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 16)                5392      \n",
      "=================================================================\n",
      "Total params: 71,232\n",
      "Trainable params: 70,624\n",
      "Non-trainable params: 608\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_28 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_28 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_28 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_29 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_29 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_29 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_30 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_30 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_30 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_31 (Conv1D)           (None, 1777, 32)          10272     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_31 (B (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_31 (Activation)   (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_32 (Conv1D)           (None, 592, 32)           5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_32 (B (None, 592, 32)           128       \n",
      "_________________________________________________________________\n",
      "activation_32 (Activation)   (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling (None, 197, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_33 (Conv1D)           (None, 197, 32)           5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_33 (B (None, 197, 32)           128       \n",
      "_________________________________________________________________\n",
      "activation_33 (Activation)   (None, 197, 32)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling (None, 65, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_34 (Conv1D)           (None, 65, 16)            2576      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_34 (B (None, 65, 16)            64        \n",
      "_________________________________________________________________\n",
      "activation_34 (Activation)   (None, 65, 16)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_26 (MaxPooling (None, 21, 16)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_35 (Conv1D)           (None, 21, 16)            1296      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_35 (B (None, 21, 16)            64        \n",
      "_________________________________________________________________\n",
      "activation_35 (Activation)   (None, 21, 16)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_27 (MaxPooling (None, 7, 16)             0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 112)               0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 112)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 16)                1808      \n",
      "=================================================================\n",
      "Total params: 69,008\n",
      "Trainable params: 68,368\n",
      "Non-trainable params: 640\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_36 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_36 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_36 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_37 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_37 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_37 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_28 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_38 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_38 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_38 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_29 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_39 (Conv1D)           (None, 1777, 32)          10272     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_39 (B (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_39 (Activation)   (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_30 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_40 (Conv1D)           (None, 592, 32)           5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_40 (B (None, 592, 32)           128       \n",
      "_________________________________________________________________\n",
      "activation_40 (Activation)   (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_31 (MaxPooling (None, 197, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_41 (Conv1D)           (None, 197, 32)           5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_41 (B (None, 197, 32)           128       \n",
      "_________________________________________________________________\n",
      "activation_41 (Activation)   (None, 197, 32)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_32 (MaxPooling (None, 65, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_42 (Conv1D)           (None, 65, 16)            2576      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_42 (B (None, 65, 16)            64        \n",
      "_________________________________________________________________\n",
      "activation_42 (Activation)   (None, 65, 16)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling (None, 21, 16)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_43 (Conv1D)           (None, 21, 16)            1296      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_43 (B (None, 21, 16)            64        \n",
      "_________________________________________________________________\n",
      "activation_43 (Activation)   (None, 21, 16)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling (None, 7, 16)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_44 (Conv1D)           (None, 7, 16)             1296      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_44 (B (None, 7, 16)             64        \n",
      "_________________________________________________________________\n",
      "activation_44 (Activation)   (None, 7, 16)             0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling (None, 2, 16)             0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 16)                528       \n",
      "=================================================================\n",
      "Total params: 69,088\n",
      "Trainable params: 68,416\n",
      "Non-trainable params: 672\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 10):\n",
    "    model = build_1d_cnn_custom_DO_BN(conv_num=i)\n",
    "    model.summary()\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36805 samples, validate on 4293 samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.1614 - acc: 0.3296\n",
      "Epoch 00001: val_loss improved from inf to 1.62868, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_4_conv_checkpoint/001-1.6287.hdf5\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 2.1612 - acc: 0.3296 - val_loss: 1.6287 - val_acc: 0.4771\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4707 - acc: 0.5331\n",
      "Epoch 00002: val_loss improved from 1.62868 to 1.30408, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_4_conv_checkpoint/002-1.3041.hdf5\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 1.4708 - acc: 0.5331 - val_loss: 1.3041 - val_acc: 0.5903\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2576 - acc: 0.6054\n",
      "Epoch 00003: val_loss improved from 1.30408 to 1.08315, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_4_conv_checkpoint/003-1.0831.hdf5\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 1.2575 - acc: 0.6054 - val_loss: 1.0831 - val_acc: 0.6646\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1296 - acc: 0.6471\n",
      "Epoch 00004: val_loss improved from 1.08315 to 1.07871, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_4_conv_checkpoint/004-1.0787.hdf5\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 1.1297 - acc: 0.6471 - val_loss: 1.0787 - val_acc: 0.6783\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0316 - acc: 0.6781\n",
      "Epoch 00005: val_loss improved from 1.07871 to 1.01836, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_4_conv_checkpoint/005-1.0184.hdf5\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 1.0315 - acc: 0.6781 - val_loss: 1.0184 - val_acc: 0.6872\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9559 - acc: 0.7065\n",
      "Epoch 00006: val_loss improved from 1.01836 to 0.94887, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_4_conv_checkpoint/006-0.9489.hdf5\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.9558 - acc: 0.7065 - val_loss: 0.9489 - val_acc: 0.7114\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8987 - acc: 0.7210\n",
      "Epoch 00007: val_loss did not improve from 0.94887\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.8986 - acc: 0.7210 - val_loss: 0.9671 - val_acc: 0.7109\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8427 - acc: 0.7391\n",
      "Epoch 00008: val_loss improved from 0.94887 to 0.94841, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_4_conv_checkpoint/008-0.9484.hdf5\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.8426 - acc: 0.7391 - val_loss: 0.9484 - val_acc: 0.7095\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7955 - acc: 0.7545\n",
      "Epoch 00009: val_loss improved from 0.94841 to 0.87647, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_4_conv_checkpoint/009-0.8765.hdf5\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.7957 - acc: 0.7544 - val_loss: 0.8765 - val_acc: 0.7370\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7574 - acc: 0.7660\n",
      "Epoch 00010: val_loss did not improve from 0.87647\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.7574 - acc: 0.7661 - val_loss: 0.9458 - val_acc: 0.7112\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7187 - acc: 0.7780\n",
      "Epoch 00011: val_loss did not improve from 0.87647\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.7189 - acc: 0.7780 - val_loss: 0.9202 - val_acc: 0.7249\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6873 - acc: 0.7895\n",
      "Epoch 00012: val_loss did not improve from 0.87647\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.6874 - acc: 0.7895 - val_loss: 0.8890 - val_acc: 0.7326\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6575 - acc: 0.7964\n",
      "Epoch 00013: val_loss improved from 0.87647 to 0.85292, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_4_conv_checkpoint/013-0.8529.hdf5\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.6575 - acc: 0.7964 - val_loss: 0.8529 - val_acc: 0.7482\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6204 - acc: 0.8076\n",
      "Epoch 00014: val_loss improved from 0.85292 to 0.84071, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_4_conv_checkpoint/014-0.8407.hdf5\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.6203 - acc: 0.8076 - val_loss: 0.8407 - val_acc: 0.7515\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5883 - acc: 0.8178\n",
      "Epoch 00015: val_loss did not improve from 0.84071\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.5884 - acc: 0.8178 - val_loss: 0.9480 - val_acc: 0.7240\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5715 - acc: 0.8236\n",
      "Epoch 00016: val_loss did not improve from 0.84071\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.5715 - acc: 0.8236 - val_loss: 1.0096 - val_acc: 0.7025\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5477 - acc: 0.8296\n",
      "Epoch 00017: val_loss improved from 0.84071 to 0.80977, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_4_conv_checkpoint/017-0.8098.hdf5\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.5477 - acc: 0.8296 - val_loss: 0.8098 - val_acc: 0.7619\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5297 - acc: 0.8352\n",
      "Epoch 00018: val_loss did not improve from 0.80977\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.5297 - acc: 0.8352 - val_loss: 0.8535 - val_acc: 0.7510\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4983 - acc: 0.8426\n",
      "Epoch 00019: val_loss did not improve from 0.80977\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.4984 - acc: 0.8425 - val_loss: 0.9048 - val_acc: 0.7470\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4826 - acc: 0.8477\n",
      "Epoch 00020: val_loss did not improve from 0.80977\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.4828 - acc: 0.8476 - val_loss: 0.8411 - val_acc: 0.7603\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4714 - acc: 0.8529\n",
      "Epoch 00021: val_loss did not improve from 0.80977\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.4715 - acc: 0.8529 - val_loss: 1.0766 - val_acc: 0.7114\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4580 - acc: 0.8552\n",
      "Epoch 00022: val_loss did not improve from 0.80977\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.4580 - acc: 0.8551 - val_loss: 0.9531 - val_acc: 0.7400\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4373 - acc: 0.8610\n",
      "Epoch 00023: val_loss did not improve from 0.80977\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.4373 - acc: 0.8610 - val_loss: 0.8313 - val_acc: 0.7650\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4254 - acc: 0.8642\n",
      "Epoch 00024: val_loss did not improve from 0.80977\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.4254 - acc: 0.8643 - val_loss: 0.8499 - val_acc: 0.7626\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4152 - acc: 0.8673\n",
      "Epoch 00025: val_loss did not improve from 0.80977\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.4152 - acc: 0.8673 - val_loss: 0.9978 - val_acc: 0.7170\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3999 - acc: 0.8712\n",
      "Epoch 00026: val_loss did not improve from 0.80977\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.4000 - acc: 0.8711 - val_loss: 0.8424 - val_acc: 0.7629\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3919 - acc: 0.8743\n",
      "Epoch 00027: val_loss did not improve from 0.80977\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.3920 - acc: 0.8743 - val_loss: 0.9036 - val_acc: 0.7517\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3716 - acc: 0.8813\n",
      "Epoch 00028: val_loss did not improve from 0.80977\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.3716 - acc: 0.8812 - val_loss: 1.1530 - val_acc: 0.6944\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3616 - acc: 0.8840\n",
      "Epoch 00029: val_loss did not improve from 0.80977\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.3617 - acc: 0.8840 - val_loss: 1.0699 - val_acc: 0.7130\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3600 - acc: 0.8833\n",
      "Epoch 00030: val_loss did not improve from 0.80977\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.3600 - acc: 0.8833 - val_loss: 1.0073 - val_acc: 0.7142\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3458 - acc: 0.8886\n",
      "Epoch 00031: val_loss did not improve from 0.80977\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.3457 - acc: 0.8886 - val_loss: 0.8651 - val_acc: 0.7720\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3353 - acc: 0.8913\n",
      "Epoch 00032: val_loss did not improve from 0.80977\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.3354 - acc: 0.8913 - val_loss: 0.9699 - val_acc: 0.7314\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3299 - acc: 0.8930\n",
      "Epoch 00033: val_loss did not improve from 0.80977\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.3301 - acc: 0.8930 - val_loss: 1.0662 - val_acc: 0.7079\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3306 - acc: 0.8935\n",
      "Epoch 00034: val_loss did not improve from 0.80977\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.3306 - acc: 0.8935 - val_loss: 1.1790 - val_acc: 0.7053\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3199 - acc: 0.8978\n",
      "Epoch 00035: val_loss did not improve from 0.80977\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.3199 - acc: 0.8978 - val_loss: 0.8375 - val_acc: 0.7713\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2995 - acc: 0.9007\n",
      "Epoch 00036: val_loss did not improve from 0.80977\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.2995 - acc: 0.9007 - val_loss: 0.8642 - val_acc: 0.7731\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2975 - acc: 0.9040\n",
      "Epoch 00037: val_loss did not improve from 0.80977\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.2975 - acc: 0.9040 - val_loss: 0.9161 - val_acc: 0.7605\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2955 - acc: 0.9034\n",
      "Epoch 00038: val_loss did not improve from 0.80977\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.2956 - acc: 0.9034 - val_loss: 1.0180 - val_acc: 0.7307\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2804 - acc: 0.9096\n",
      "Epoch 00039: val_loss did not improve from 0.80977\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.2804 - acc: 0.9096 - val_loss: 0.9307 - val_acc: 0.7622\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2810 - acc: 0.9093\n",
      "Epoch 00040: val_loss did not improve from 0.80977\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.2810 - acc: 0.9093 - val_loss: 0.9429 - val_acc: 0.7556\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2709 - acc: 0.9126\n",
      "Epoch 00041: val_loss did not improve from 0.80977\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.2710 - acc: 0.9126 - val_loss: 0.9444 - val_acc: 0.7487\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2781 - acc: 0.9092\n",
      "Epoch 00042: val_loss did not improve from 0.80977\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.2782 - acc: 0.9092 - val_loss: 0.9256 - val_acc: 0.7552\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2694 - acc: 0.9103\n",
      "Epoch 00043: val_loss did not improve from 0.80977\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.2693 - acc: 0.9103 - val_loss: 0.8554 - val_acc: 0.7750\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2609 - acc: 0.9148\n",
      "Epoch 00044: val_loss did not improve from 0.80977\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.2610 - acc: 0.9147 - val_loss: 0.8717 - val_acc: 0.7671\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2608 - acc: 0.9152\n",
      "Epoch 00045: val_loss did not improve from 0.80977\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.2607 - acc: 0.9152 - val_loss: 0.8653 - val_acc: 0.7789\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2518 - acc: 0.9187\n",
      "Epoch 00046: val_loss did not improve from 0.80977\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.2518 - acc: 0.9187 - val_loss: 0.8442 - val_acc: 0.7745\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2487 - acc: 0.9179\n",
      "Epoch 00047: val_loss did not improve from 0.80977\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.2487 - acc: 0.9179 - val_loss: 0.8493 - val_acc: 0.7727\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2423 - acc: 0.9202\n",
      "Epoch 00048: val_loss did not improve from 0.80977\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.2423 - acc: 0.9202 - val_loss: 1.0478 - val_acc: 0.7419\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2384 - acc: 0.9204\n",
      "Epoch 00049: val_loss did not improve from 0.80977\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.2384 - acc: 0.9204 - val_loss: 0.9375 - val_acc: 0.7654\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2380 - acc: 0.9238\n",
      "Epoch 00050: val_loss did not improve from 0.80977\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.2381 - acc: 0.9237 - val_loss: 1.2483 - val_acc: 0.7114\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2426 - acc: 0.9213\n",
      "Epoch 00051: val_loss did not improve from 0.80977\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.2426 - acc: 0.9213 - val_loss: 0.8667 - val_acc: 0.7796\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2248 - acc: 0.9265\n",
      "Epoch 00052: val_loss did not improve from 0.80977\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.2250 - acc: 0.9265 - val_loss: 0.9705 - val_acc: 0.7582\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2289 - acc: 0.9250\n",
      "Epoch 00053: val_loss did not improve from 0.80977\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.2289 - acc: 0.9250 - val_loss: 0.9293 - val_acc: 0.7692\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2217 - acc: 0.9261\n",
      "Epoch 00054: val_loss did not improve from 0.80977\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.2219 - acc: 0.9260 - val_loss: 1.0127 - val_acc: 0.7522\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2213 - acc: 0.9274\n",
      "Epoch 00055: val_loss did not improve from 0.80977\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.2213 - acc: 0.9275 - val_loss: 0.9207 - val_acc: 0.7710\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2116 - acc: 0.9307\n",
      "Epoch 00056: val_loss did not improve from 0.80977\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.2119 - acc: 0.9307 - val_loss: 0.9014 - val_acc: 0.7782\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2231 - acc: 0.9248\n",
      "Epoch 00057: val_loss did not improve from 0.80977\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.2232 - acc: 0.9248 - val_loss: 0.8905 - val_acc: 0.7801\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2140 - acc: 0.9302\n",
      "Epoch 00058: val_loss did not improve from 0.80977\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.2141 - acc: 0.9301 - val_loss: 0.8705 - val_acc: 0.7857\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2120 - acc: 0.9307\n",
      "Epoch 00059: val_loss did not improve from 0.80977\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.2120 - acc: 0.9307 - val_loss: 0.9253 - val_acc: 0.7657\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2023 - acc: 0.9341\n",
      "Epoch 00060: val_loss did not improve from 0.80977\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.2023 - acc: 0.9341 - val_loss: 0.9118 - val_acc: 0.7696\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2065 - acc: 0.9322\n",
      "Epoch 00061: val_loss did not improve from 0.80977\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.2066 - acc: 0.9322 - val_loss: 0.9203 - val_acc: 0.7864\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1965 - acc: 0.9351\n",
      "Epoch 00062: val_loss did not improve from 0.80977\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.1965 - acc: 0.9350 - val_loss: 0.9560 - val_acc: 0.7710\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1995 - acc: 0.9346\n",
      "Epoch 00063: val_loss did not improve from 0.80977\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.1995 - acc: 0.9346 - val_loss: 1.0522 - val_acc: 0.7519\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1908 - acc: 0.9371\n",
      "Epoch 00064: val_loss did not improve from 0.80977\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.1908 - acc: 0.9371 - val_loss: 0.9717 - val_acc: 0.7710\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1900 - acc: 0.9392\n",
      "Epoch 00065: val_loss did not improve from 0.80977\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.1900 - acc: 0.9392 - val_loss: 0.9432 - val_acc: 0.7673\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1879 - acc: 0.9385\n",
      "Epoch 00066: val_loss did not improve from 0.80977\n",
      "36805/36805 [==============================] - 82s 2ms/sample - loss: 0.1879 - acc: 0.9385 - val_loss: 1.0527 - val_acc: 0.7563\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1867 - acc: 0.9380\n",
      "Epoch 00067: val_loss did not improve from 0.80977\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.1868 - acc: 0.9380 - val_loss: 0.9922 - val_acc: 0.7640\n",
      "\n",
      "1D_CNN_custom_4_ch_64_DO_BN_4_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 886us/sample - loss: 0.9074 - acc: 0.7277\n",
      "Loss: 0.9074451053130169 Accuracy: 0.72772586\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.4172 - acc: 0.2611\n",
      "Epoch 00001: val_loss improved from inf to 1.73950, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_5_conv_checkpoint/001-1.7395.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 2.4171 - acc: 0.2611 - val_loss: 1.7395 - val_acc: 0.4274\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5803 - acc: 0.4891\n",
      "Epoch 00002: val_loss improved from 1.73950 to 1.17258, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_5_conv_checkpoint/002-1.1726.hdf5\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 1.5804 - acc: 0.4890 - val_loss: 1.1726 - val_acc: 0.6389\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3101 - acc: 0.5820\n",
      "Epoch 00003: val_loss improved from 1.17258 to 1.13212, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_5_conv_checkpoint/003-1.1321.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 1.3103 - acc: 0.5820 - val_loss: 1.1321 - val_acc: 0.6452\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1680 - acc: 0.6302\n",
      "Epoch 00004: val_loss improved from 1.13212 to 1.00718, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_5_conv_checkpoint/004-1.0072.hdf5\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 1.1680 - acc: 0.6302 - val_loss: 1.0072 - val_acc: 0.6962\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0500 - acc: 0.6729\n",
      "Epoch 00005: val_loss improved from 1.00718 to 0.94242, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_5_conv_checkpoint/005-0.9424.hdf5\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 1.0500 - acc: 0.6729 - val_loss: 0.9424 - val_acc: 0.7116\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9749 - acc: 0.6978\n",
      "Epoch 00006: val_loss improved from 0.94242 to 0.93723, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_5_conv_checkpoint/006-0.9372.hdf5\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.9749 - acc: 0.6978 - val_loss: 0.9372 - val_acc: 0.7074\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9142 - acc: 0.7177\n",
      "Epoch 00007: val_loss improved from 0.93723 to 0.79736, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_5_conv_checkpoint/007-0.7974.hdf5\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.9142 - acc: 0.7177 - val_loss: 0.7974 - val_acc: 0.7757\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8609 - acc: 0.7364\n",
      "Epoch 00008: val_loss improved from 0.79736 to 0.77548, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_5_conv_checkpoint/008-0.7755.hdf5\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.8610 - acc: 0.7364 - val_loss: 0.7755 - val_acc: 0.7722\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8269 - acc: 0.7483\n",
      "Epoch 00009: val_loss did not improve from 0.77548\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.8268 - acc: 0.7483 - val_loss: 0.8221 - val_acc: 0.7512\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7943 - acc: 0.7576\n",
      "Epoch 00010: val_loss did not improve from 0.77548\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.7945 - acc: 0.7576 - val_loss: 0.7874 - val_acc: 0.7768\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7715 - acc: 0.7658\n",
      "Epoch 00011: val_loss did not improve from 0.77548\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.7716 - acc: 0.7658 - val_loss: 0.8179 - val_acc: 0.7610\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7398 - acc: 0.7768\n",
      "Epoch 00012: val_loss improved from 0.77548 to 0.69462, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_5_conv_checkpoint/012-0.6946.hdf5\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.7397 - acc: 0.7768 - val_loss: 0.6946 - val_acc: 0.8025\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7127 - acc: 0.7833\n",
      "Epoch 00013: val_loss did not improve from 0.69462\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.7127 - acc: 0.7833 - val_loss: 0.8534 - val_acc: 0.7545\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6959 - acc: 0.7902\n",
      "Epoch 00014: val_loss did not improve from 0.69462\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.6959 - acc: 0.7902 - val_loss: 0.7298 - val_acc: 0.7855\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6742 - acc: 0.7965\n",
      "Epoch 00015: val_loss did not improve from 0.69462\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.6741 - acc: 0.7965 - val_loss: 0.7011 - val_acc: 0.7936\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6553 - acc: 0.8016\n",
      "Epoch 00016: val_loss improved from 0.69462 to 0.66012, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_5_conv_checkpoint/016-0.6601.hdf5\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.6554 - acc: 0.8016 - val_loss: 0.6601 - val_acc: 0.8181\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6448 - acc: 0.8048\n",
      "Epoch 00017: val_loss did not improve from 0.66012\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.6448 - acc: 0.8049 - val_loss: 0.6901 - val_acc: 0.8018\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6278 - acc: 0.8128\n",
      "Epoch 00018: val_loss did not improve from 0.66012\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.6279 - acc: 0.8128 - val_loss: 0.7124 - val_acc: 0.7934\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6116 - acc: 0.8153\n",
      "Epoch 00019: val_loss did not improve from 0.66012\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.6116 - acc: 0.8152 - val_loss: 0.7112 - val_acc: 0.7927\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5966 - acc: 0.8190\n",
      "Epoch 00020: val_loss did not improve from 0.66012\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.5966 - acc: 0.8190 - val_loss: 0.7526 - val_acc: 0.7843\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5841 - acc: 0.8221\n",
      "Epoch 00021: val_loss improved from 0.66012 to 0.64317, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_5_conv_checkpoint/021-0.6432.hdf5\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.5842 - acc: 0.8220 - val_loss: 0.6432 - val_acc: 0.8185\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5775 - acc: 0.8264\n",
      "Epoch 00022: val_loss did not improve from 0.64317\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.5776 - acc: 0.8264 - val_loss: 0.6530 - val_acc: 0.8146\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5659 - acc: 0.8292\n",
      "Epoch 00023: val_loss improved from 0.64317 to 0.63098, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_5_conv_checkpoint/023-0.6310.hdf5\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.5660 - acc: 0.8291 - val_loss: 0.6310 - val_acc: 0.8241\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5553 - acc: 0.8315\n",
      "Epoch 00024: val_loss did not improve from 0.63098\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.5553 - acc: 0.8316 - val_loss: 0.7346 - val_acc: 0.7883\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5442 - acc: 0.8355\n",
      "Epoch 00025: val_loss did not improve from 0.63098\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.5442 - acc: 0.8355 - val_loss: 0.7138 - val_acc: 0.7983\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5376 - acc: 0.8370\n",
      "Epoch 00026: val_loss did not improve from 0.63098\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.5375 - acc: 0.8370 - val_loss: 0.6590 - val_acc: 0.8146\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5228 - acc: 0.8426\n",
      "Epoch 00027: val_loss did not improve from 0.63098\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.5229 - acc: 0.8425 - val_loss: 0.6603 - val_acc: 0.8081\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5182 - acc: 0.8428\n",
      "Epoch 00028: val_loss did not improve from 0.63098\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.5182 - acc: 0.8428 - val_loss: 0.6903 - val_acc: 0.7966\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5043 - acc: 0.8466\n",
      "Epoch 00029: val_loss did not improve from 0.63098\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.5043 - acc: 0.8466 - val_loss: 0.7232 - val_acc: 0.7966\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5014 - acc: 0.8483\n",
      "Epoch 00030: val_loss did not improve from 0.63098\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.5014 - acc: 0.8483 - val_loss: 0.7625 - val_acc: 0.7741\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4972 - acc: 0.8482\n",
      "Epoch 00031: val_loss improved from 0.63098 to 0.61698, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_5_conv_checkpoint/031-0.6170.hdf5\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.4973 - acc: 0.8482 - val_loss: 0.6170 - val_acc: 0.8369\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4870 - acc: 0.8528\n",
      "Epoch 00032: val_loss did not improve from 0.61698\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.4871 - acc: 0.8527 - val_loss: 0.6663 - val_acc: 0.8088\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4821 - acc: 0.8512\n",
      "Epoch 00033: val_loss did not improve from 0.61698\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.4821 - acc: 0.8512 - val_loss: 0.6247 - val_acc: 0.8199\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4732 - acc: 0.8557\n",
      "Epoch 00034: val_loss did not improve from 0.61698\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.4732 - acc: 0.8557 - val_loss: 0.6847 - val_acc: 0.8046\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4672 - acc: 0.8554\n",
      "Epoch 00035: val_loss did not improve from 0.61698\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.4672 - acc: 0.8554 - val_loss: 0.7646 - val_acc: 0.7766\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4541 - acc: 0.8594\n",
      "Epoch 00036: val_loss did not improve from 0.61698\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.4541 - acc: 0.8594 - val_loss: 0.6393 - val_acc: 0.8197\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4555 - acc: 0.8602\n",
      "Epoch 00037: val_loss did not improve from 0.61698\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.4555 - acc: 0.8602 - val_loss: 0.6817 - val_acc: 0.8048\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4499 - acc: 0.8614\n",
      "Epoch 00038: val_loss did not improve from 0.61698\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.4502 - acc: 0.8613 - val_loss: 0.6307 - val_acc: 0.8241\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4422 - acc: 0.8647\n",
      "Epoch 00039: val_loss improved from 0.61698 to 0.57728, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_5_conv_checkpoint/039-0.5773.hdf5\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.4422 - acc: 0.8646 - val_loss: 0.5773 - val_acc: 0.8444\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4288 - acc: 0.8697\n",
      "Epoch 00040: val_loss did not improve from 0.57728\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.4287 - acc: 0.8697 - val_loss: 0.6441 - val_acc: 0.8251\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4312 - acc: 0.8675\n",
      "Epoch 00041: val_loss did not improve from 0.57728\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.4315 - acc: 0.8674 - val_loss: 0.6266 - val_acc: 0.8328\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4224 - acc: 0.8706\n",
      "Epoch 00042: val_loss did not improve from 0.57728\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.4224 - acc: 0.8706 - val_loss: 0.6613 - val_acc: 0.8181\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4178 - acc: 0.8712\n",
      "Epoch 00043: val_loss did not improve from 0.57728\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.4178 - acc: 0.8713 - val_loss: 0.6839 - val_acc: 0.8036\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4078 - acc: 0.8737\n",
      "Epoch 00044: val_loss did not improve from 0.57728\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.4078 - acc: 0.8738 - val_loss: 0.7434 - val_acc: 0.7936\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4052 - acc: 0.8745\n",
      "Epoch 00045: val_loss improved from 0.57728 to 0.56213, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_5_conv_checkpoint/045-0.5621.hdf5\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.4052 - acc: 0.8745 - val_loss: 0.5621 - val_acc: 0.8423\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4065 - acc: 0.8727\n",
      "Epoch 00046: val_loss did not improve from 0.56213\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.4065 - acc: 0.8727 - val_loss: 0.6105 - val_acc: 0.8293\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3999 - acc: 0.8750\n",
      "Epoch 00047: val_loss did not improve from 0.56213\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.4000 - acc: 0.8750 - val_loss: 0.6003 - val_acc: 0.8348\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3948 - acc: 0.8777\n",
      "Epoch 00048: val_loss did not improve from 0.56213\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.3947 - acc: 0.8777 - val_loss: 1.1611 - val_acc: 0.7072\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3923 - acc: 0.8775\n",
      "Epoch 00049: val_loss did not improve from 0.56213\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.3923 - acc: 0.8775 - val_loss: 0.6548 - val_acc: 0.8183\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3821 - acc: 0.8800\n",
      "Epoch 00050: val_loss did not improve from 0.56213\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.3822 - acc: 0.8800 - val_loss: 0.5896 - val_acc: 0.8311\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3820 - acc: 0.8783\n",
      "Epoch 00051: val_loss did not improve from 0.56213\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.3820 - acc: 0.8783 - val_loss: 0.6434 - val_acc: 0.8339\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3778 - acc: 0.8799\n",
      "Epoch 00052: val_loss did not improve from 0.56213\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.3779 - acc: 0.8799 - val_loss: 0.6964 - val_acc: 0.8130\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3673 - acc: 0.8851\n",
      "Epoch 00053: val_loss did not improve from 0.56213\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.3673 - acc: 0.8851 - val_loss: 0.5983 - val_acc: 0.8446\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3704 - acc: 0.8840\n",
      "Epoch 00054: val_loss did not improve from 0.56213\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.3704 - acc: 0.8840 - val_loss: 0.6677 - val_acc: 0.8230\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3627 - acc: 0.8881\n",
      "Epoch 00055: val_loss did not improve from 0.56213\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.3627 - acc: 0.8881 - val_loss: 0.8469 - val_acc: 0.7715\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3618 - acc: 0.8864\n",
      "Epoch 00056: val_loss did not improve from 0.56213\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.3618 - acc: 0.8863 - val_loss: 0.6522 - val_acc: 0.8185\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3593 - acc: 0.8869\n",
      "Epoch 00057: val_loss did not improve from 0.56213\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.3596 - acc: 0.8869 - val_loss: 0.5894 - val_acc: 0.8393\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3596 - acc: 0.8868\n",
      "Epoch 00058: val_loss did not improve from 0.56213\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.3595 - acc: 0.8868 - val_loss: 0.5771 - val_acc: 0.8453\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3467 - acc: 0.8931\n",
      "Epoch 00059: val_loss did not improve from 0.56213\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.3468 - acc: 0.8930 - val_loss: 0.6279 - val_acc: 0.8297\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3435 - acc: 0.8921\n",
      "Epoch 00060: val_loss did not improve from 0.56213\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.3436 - acc: 0.8921 - val_loss: 0.6034 - val_acc: 0.8339\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3420 - acc: 0.8917\n",
      "Epoch 00061: val_loss did not improve from 0.56213\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.3420 - acc: 0.8918 - val_loss: 0.6146 - val_acc: 0.8428\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3419 - acc: 0.8914\n",
      "Epoch 00062: val_loss did not improve from 0.56213\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.3420 - acc: 0.8913 - val_loss: 0.5983 - val_acc: 0.8416\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3382 - acc: 0.8930\n",
      "Epoch 00063: val_loss did not improve from 0.56213\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.3382 - acc: 0.8930 - val_loss: 0.5665 - val_acc: 0.8477\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3299 - acc: 0.8962\n",
      "Epoch 00064: val_loss did not improve from 0.56213\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.3300 - acc: 0.8961 - val_loss: 0.5930 - val_acc: 0.8374\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3324 - acc: 0.8955\n",
      "Epoch 00065: val_loss did not improve from 0.56213\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.3325 - acc: 0.8955 - val_loss: 0.7713 - val_acc: 0.7911\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3248 - acc: 0.8982\n",
      "Epoch 00066: val_loss did not improve from 0.56213\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.3249 - acc: 0.8982 - val_loss: 0.6095 - val_acc: 0.8388\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3207 - acc: 0.8970\n",
      "Epoch 00067: val_loss did not improve from 0.56213\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.3208 - acc: 0.8970 - val_loss: 0.6265 - val_acc: 0.8302\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3160 - acc: 0.8994\n",
      "Epoch 00068: val_loss did not improve from 0.56213\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.3160 - acc: 0.8994 - val_loss: 0.6494 - val_acc: 0.8297\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3196 - acc: 0.8989\n",
      "Epoch 00069: val_loss did not improve from 0.56213\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.3196 - acc: 0.8989 - val_loss: 0.7289 - val_acc: 0.7997\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3145 - acc: 0.9015\n",
      "Epoch 00070: val_loss did not improve from 0.56213\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.3145 - acc: 0.9015 - val_loss: 0.6310 - val_acc: 0.8304\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3087 - acc: 0.9032\n",
      "Epoch 00071: val_loss did not improve from 0.56213\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.3087 - acc: 0.9032 - val_loss: 0.7063 - val_acc: 0.8097\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3110 - acc: 0.8994\n",
      "Epoch 00072: val_loss did not improve from 0.56213\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.3111 - acc: 0.8994 - val_loss: 0.5917 - val_acc: 0.8465\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3047 - acc: 0.9038\n",
      "Epoch 00073: val_loss did not improve from 0.56213\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.3047 - acc: 0.9037 - val_loss: 0.5982 - val_acc: 0.8463\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3044 - acc: 0.9023\n",
      "Epoch 00074: val_loss did not improve from 0.56213\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.3044 - acc: 0.9023 - val_loss: 0.6170 - val_acc: 0.8339\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3057 - acc: 0.9010\n",
      "Epoch 00075: val_loss did not improve from 0.56213\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.3057 - acc: 0.9009 - val_loss: 0.6135 - val_acc: 0.8390\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2992 - acc: 0.9052\n",
      "Epoch 00076: val_loss did not improve from 0.56213\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2992 - acc: 0.9052 - val_loss: 0.5812 - val_acc: 0.8423\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2905 - acc: 0.9068\n",
      "Epoch 00077: val_loss did not improve from 0.56213\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2905 - acc: 0.9068 - val_loss: 0.6167 - val_acc: 0.8304\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2935 - acc: 0.9050\n",
      "Epoch 00078: val_loss did not improve from 0.56213\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2935 - acc: 0.9050 - val_loss: 0.7081 - val_acc: 0.8267\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2918 - acc: 0.9074\n",
      "Epoch 00079: val_loss did not improve from 0.56213\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2918 - acc: 0.9073 - val_loss: 0.6724 - val_acc: 0.8230\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2886 - acc: 0.9082\n",
      "Epoch 00080: val_loss did not improve from 0.56213\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2886 - acc: 0.9082 - val_loss: 0.6158 - val_acc: 0.8423\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2810 - acc: 0.9099\n",
      "Epoch 00081: val_loss did not improve from 0.56213\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2810 - acc: 0.9099 - val_loss: 0.6854 - val_acc: 0.8209\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2845 - acc: 0.9083\n",
      "Epoch 00082: val_loss did not improve from 0.56213\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2846 - acc: 0.9083 - val_loss: 0.6161 - val_acc: 0.8365\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2827 - acc: 0.9081\n",
      "Epoch 00083: val_loss did not improve from 0.56213\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2828 - acc: 0.9081 - val_loss: 0.7957 - val_acc: 0.7897\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2774 - acc: 0.9130\n",
      "Epoch 00084: val_loss did not improve from 0.56213\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2774 - acc: 0.9130 - val_loss: 0.6244 - val_acc: 0.8453\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2744 - acc: 0.9119\n",
      "Epoch 00085: val_loss did not improve from 0.56213\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2743 - acc: 0.9119 - val_loss: 0.5667 - val_acc: 0.8519\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2761 - acc: 0.9111\n",
      "Epoch 00086: val_loss did not improve from 0.56213\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2761 - acc: 0.9111 - val_loss: 0.5834 - val_acc: 0.8549\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2705 - acc: 0.9127\n",
      "Epoch 00087: val_loss did not improve from 0.56213\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2704 - acc: 0.9128 - val_loss: 0.7279 - val_acc: 0.8132\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2699 - acc: 0.9123\n",
      "Epoch 00088: val_loss did not improve from 0.56213\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2699 - acc: 0.9123 - val_loss: 0.6504 - val_acc: 0.8328\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2608 - acc: 0.9161\n",
      "Epoch 00089: val_loss did not improve from 0.56213\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2609 - acc: 0.9161 - val_loss: 0.6239 - val_acc: 0.8418\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2666 - acc: 0.9139\n",
      "Epoch 00090: val_loss did not improve from 0.56213\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2668 - acc: 0.9138 - val_loss: 0.6485 - val_acc: 0.8381\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2640 - acc: 0.9144\n",
      "Epoch 00091: val_loss did not improve from 0.56213\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 0.2640 - acc: 0.9144 - val_loss: 0.6543 - val_acc: 0.8307\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2630 - acc: 0.9152\n",
      "Epoch 00092: val_loss did not improve from 0.56213\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2631 - acc: 0.9152 - val_loss: 0.6671 - val_acc: 0.8265\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2561 - acc: 0.9183\n",
      "Epoch 00093: val_loss did not improve from 0.56213\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2561 - acc: 0.9183 - val_loss: 0.6174 - val_acc: 0.8407\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2544 - acc: 0.9193\n",
      "Epoch 00094: val_loss did not improve from 0.56213\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2545 - acc: 0.9192 - val_loss: 0.6039 - val_acc: 0.8579\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2533 - acc: 0.9187\n",
      "Epoch 00095: val_loss did not improve from 0.56213\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2533 - acc: 0.9187 - val_loss: 0.6136 - val_acc: 0.8467\n",
      "\n",
      "1D_CNN_custom_4_ch_64_DO_BN_5_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 871us/sample - loss: 0.6616 - acc: 0.8075\n",
      "Loss: 0.6616365684403314 Accuracy: 0.80747664\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.8050 - acc: 0.1719\n",
      "Epoch 00001: val_loss improved from inf to 2.16316, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_6_conv_checkpoint/001-2.1632.hdf5\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 2.8049 - acc: 0.1719 - val_loss: 2.1632 - val_acc: 0.3473\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9030 - acc: 0.3825\n",
      "Epoch 00002: val_loss improved from 2.16316 to 1.35837, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_6_conv_checkpoint/002-1.3584.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 1.9030 - acc: 0.3826 - val_loss: 1.3584 - val_acc: 0.5851\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4828 - acc: 0.5209\n",
      "Epoch 00003: val_loss improved from 1.35837 to 1.12118, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_6_conv_checkpoint/003-1.1212.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 1.4831 - acc: 0.5208 - val_loss: 1.1212 - val_acc: 0.6699\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2726 - acc: 0.5931\n",
      "Epoch 00004: val_loss improved from 1.12118 to 1.00333, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_6_conv_checkpoint/004-1.0033.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 1.2726 - acc: 0.5931 - val_loss: 1.0033 - val_acc: 0.7032\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1313 - acc: 0.6406\n",
      "Epoch 00005: val_loss improved from 1.00333 to 0.88054, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_6_conv_checkpoint/005-0.8805.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 1.1313 - acc: 0.6406 - val_loss: 0.8805 - val_acc: 0.7466\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0344 - acc: 0.6732\n",
      "Epoch 00006: val_loss improved from 0.88054 to 0.81264, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_6_conv_checkpoint/006-0.8126.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 1.0343 - acc: 0.6732 - val_loss: 0.8126 - val_acc: 0.7692\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9544 - acc: 0.7088\n",
      "Epoch 00007: val_loss improved from 0.81264 to 0.80047, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_6_conv_checkpoint/007-0.8005.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.9543 - acc: 0.7088 - val_loss: 0.8005 - val_acc: 0.7673\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8976 - acc: 0.7244\n",
      "Epoch 00008: val_loss improved from 0.80047 to 0.76632, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_6_conv_checkpoint/008-0.7663.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.8976 - acc: 0.7244 - val_loss: 0.7663 - val_acc: 0.7789\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8437 - acc: 0.7431\n",
      "Epoch 00009: val_loss did not improve from 0.76632\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.8437 - acc: 0.7431 - val_loss: 0.7683 - val_acc: 0.7857\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8183 - acc: 0.7501\n",
      "Epoch 00010: val_loss improved from 0.76632 to 0.72388, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_6_conv_checkpoint/010-0.7239.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.8182 - acc: 0.7501 - val_loss: 0.7239 - val_acc: 0.7929\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7732 - acc: 0.7668\n",
      "Epoch 00011: val_loss improved from 0.72388 to 0.66017, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_6_conv_checkpoint/011-0.6602.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.7734 - acc: 0.7667 - val_loss: 0.6602 - val_acc: 0.8090\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7487 - acc: 0.7741\n",
      "Epoch 00012: val_loss did not improve from 0.66017\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.7490 - acc: 0.7741 - val_loss: 0.6967 - val_acc: 0.8092\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7198 - acc: 0.7851\n",
      "Epoch 00013: val_loss improved from 0.66017 to 0.63610, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_6_conv_checkpoint/013-0.6361.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.7198 - acc: 0.7851 - val_loss: 0.6361 - val_acc: 0.8199\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6958 - acc: 0.7915\n",
      "Epoch 00014: val_loss improved from 0.63610 to 0.62595, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_6_conv_checkpoint/014-0.6259.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.6959 - acc: 0.7914 - val_loss: 0.6259 - val_acc: 0.8232\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6806 - acc: 0.7959\n",
      "Epoch 00015: val_loss did not improve from 0.62595\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.6806 - acc: 0.7959 - val_loss: 0.6543 - val_acc: 0.8146\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6557 - acc: 0.8054\n",
      "Epoch 00016: val_loss improved from 0.62595 to 0.59254, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_6_conv_checkpoint/016-0.5925.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.6557 - acc: 0.8055 - val_loss: 0.5925 - val_acc: 0.8311\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6338 - acc: 0.8110\n",
      "Epoch 00017: val_loss did not improve from 0.59254\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.6338 - acc: 0.8110 - val_loss: 0.6542 - val_acc: 0.8139\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6225 - acc: 0.8143\n",
      "Epoch 00018: val_loss improved from 0.59254 to 0.55262, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_6_conv_checkpoint/018-0.5526.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.6225 - acc: 0.8143 - val_loss: 0.5526 - val_acc: 0.8446\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6087 - acc: 0.8213\n",
      "Epoch 00019: val_loss improved from 0.55262 to 0.54942, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_6_conv_checkpoint/019-0.5494.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.6090 - acc: 0.8212 - val_loss: 0.5494 - val_acc: 0.8439\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5921 - acc: 0.8228\n",
      "Epoch 00020: val_loss did not improve from 0.54942\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.5921 - acc: 0.8227 - val_loss: 0.5647 - val_acc: 0.8416\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5797 - acc: 0.8274\n",
      "Epoch 00021: val_loss improved from 0.54942 to 0.54781, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_6_conv_checkpoint/021-0.5478.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.5797 - acc: 0.8274 - val_loss: 0.5478 - val_acc: 0.8439\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5677 - acc: 0.8339\n",
      "Epoch 00022: val_loss did not improve from 0.54781\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.5677 - acc: 0.8339 - val_loss: 0.5637 - val_acc: 0.8323\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5587 - acc: 0.8338\n",
      "Epoch 00023: val_loss improved from 0.54781 to 0.53890, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_6_conv_checkpoint/023-0.5389.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.5587 - acc: 0.8338 - val_loss: 0.5389 - val_acc: 0.8514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5465 - acc: 0.8376\n",
      "Epoch 00024: val_loss did not improve from 0.53890\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.5467 - acc: 0.8375 - val_loss: 0.5522 - val_acc: 0.8435\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5382 - acc: 0.8423\n",
      "Epoch 00025: val_loss did not improve from 0.53890\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.5382 - acc: 0.8423 - val_loss: 0.5453 - val_acc: 0.8512\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5313 - acc: 0.8444\n",
      "Epoch 00026: val_loss did not improve from 0.53890\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.5313 - acc: 0.8444 - val_loss: 0.5808 - val_acc: 0.8339\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5114 - acc: 0.8482\n",
      "Epoch 00027: val_loss improved from 0.53890 to 0.52366, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_6_conv_checkpoint/027-0.5237.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.5115 - acc: 0.8482 - val_loss: 0.5237 - val_acc: 0.8616\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5098 - acc: 0.8496\n",
      "Epoch 00028: val_loss improved from 0.52366 to 0.51075, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_6_conv_checkpoint/028-0.5108.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.5098 - acc: 0.8496 - val_loss: 0.5108 - val_acc: 0.8651\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5063 - acc: 0.8490\n",
      "Epoch 00029: val_loss did not improve from 0.51075\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.5063 - acc: 0.8490 - val_loss: 0.5248 - val_acc: 0.8537\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4929 - acc: 0.8532\n",
      "Epoch 00030: val_loss did not improve from 0.51075\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.4929 - acc: 0.8532 - val_loss: 0.6824 - val_acc: 0.7969\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4848 - acc: 0.8552\n",
      "Epoch 00031: val_loss did not improve from 0.51075\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.4853 - acc: 0.8550 - val_loss: 0.5496 - val_acc: 0.8465\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4824 - acc: 0.8558\n",
      "Epoch 00032: val_loss did not improve from 0.51075\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.4826 - acc: 0.8557 - val_loss: 0.5349 - val_acc: 0.8484\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4743 - acc: 0.8587\n",
      "Epoch 00033: val_loss did not improve from 0.51075\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.4743 - acc: 0.8587 - val_loss: 0.7565 - val_acc: 0.7843\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4622 - acc: 0.8635\n",
      "Epoch 00034: val_loss did not improve from 0.51075\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.4623 - acc: 0.8635 - val_loss: 1.0159 - val_acc: 0.7338\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4560 - acc: 0.8648\n",
      "Epoch 00035: val_loss improved from 0.51075 to 0.50358, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_6_conv_checkpoint/035-0.5036.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.4559 - acc: 0.8648 - val_loss: 0.5036 - val_acc: 0.8658\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4557 - acc: 0.8653\n",
      "Epoch 00036: val_loss improved from 0.50358 to 0.47717, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_6_conv_checkpoint/036-0.4772.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.4558 - acc: 0.8652 - val_loss: 0.4772 - val_acc: 0.8740\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4437 - acc: 0.8679\n",
      "Epoch 00037: val_loss did not improve from 0.47717\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.4437 - acc: 0.8679 - val_loss: 0.5022 - val_acc: 0.8640\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4464 - acc: 0.8678\n",
      "Epoch 00038: val_loss improved from 0.47717 to 0.43486, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_6_conv_checkpoint/038-0.4349.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.4465 - acc: 0.8678 - val_loss: 0.4349 - val_acc: 0.8789\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4396 - acc: 0.8698\n",
      "Epoch 00039: val_loss did not improve from 0.43486\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.4396 - acc: 0.8698 - val_loss: 0.5034 - val_acc: 0.8749\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4296 - acc: 0.8708\n",
      "Epoch 00040: val_loss did not improve from 0.43486\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.4297 - acc: 0.8708 - val_loss: 0.4832 - val_acc: 0.8768\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4248 - acc: 0.8722\n",
      "Epoch 00041: val_loss did not improve from 0.43486\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.4248 - acc: 0.8722 - val_loss: 0.4897 - val_acc: 0.8635\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4216 - acc: 0.8738\n",
      "Epoch 00042: val_loss did not improve from 0.43486\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.4217 - acc: 0.8737 - val_loss: 0.5806 - val_acc: 0.8418\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4166 - acc: 0.8745\n",
      "Epoch 00043: val_loss did not improve from 0.43486\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.4167 - acc: 0.8744 - val_loss: 0.4431 - val_acc: 0.8798\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4121 - acc: 0.8769\n",
      "Epoch 00044: val_loss did not improve from 0.43486\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.4120 - acc: 0.8769 - val_loss: 0.4965 - val_acc: 0.8698\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4011 - acc: 0.8796\n",
      "Epoch 00045: val_loss did not improve from 0.43486\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.4011 - acc: 0.8796 - val_loss: 0.5247 - val_acc: 0.8595\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4012 - acc: 0.8793\n",
      "Epoch 00046: val_loss did not improve from 0.43486\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.4013 - acc: 0.8793 - val_loss: 0.5100 - val_acc: 0.8579\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3981 - acc: 0.8823\n",
      "Epoch 00047: val_loss did not improve from 0.43486\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.3982 - acc: 0.8822 - val_loss: 0.4720 - val_acc: 0.8775\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3931 - acc: 0.8825\n",
      "Epoch 00048: val_loss did not improve from 0.43486\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.3931 - acc: 0.8825 - val_loss: 0.4604 - val_acc: 0.8761\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3911 - acc: 0.8825\n",
      "Epoch 00049: val_loss did not improve from 0.43486\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.3911 - acc: 0.8825 - val_loss: 0.5059 - val_acc: 0.8661\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3804 - acc: 0.8875\n",
      "Epoch 00050: val_loss did not improve from 0.43486\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.3804 - acc: 0.8875 - val_loss: 0.4462 - val_acc: 0.8889\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3806 - acc: 0.8851\n",
      "Epoch 00051: val_loss did not improve from 0.43486\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.3806 - acc: 0.8851 - val_loss: 0.4627 - val_acc: 0.8651\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3760 - acc: 0.8873\n",
      "Epoch 00052: val_loss did not improve from 0.43486\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.3760 - acc: 0.8873 - val_loss: 0.4358 - val_acc: 0.8791\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3724 - acc: 0.8887\n",
      "Epoch 00053: val_loss did not improve from 0.43486\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.3725 - acc: 0.8887 - val_loss: 0.4775 - val_acc: 0.8670\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3672 - acc: 0.8871\n",
      "Epoch 00054: val_loss improved from 0.43486 to 0.42394, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_6_conv_checkpoint/054-0.4239.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.3673 - acc: 0.8871 - val_loss: 0.4239 - val_acc: 0.8931\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3704 - acc: 0.8878\n",
      "Epoch 00055: val_loss did not improve from 0.42394\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.3704 - acc: 0.8878 - val_loss: 1.6468 - val_acc: 0.6431\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3619 - acc: 0.8913\n",
      "Epoch 00056: val_loss improved from 0.42394 to 0.42192, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_6_conv_checkpoint/056-0.4219.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.3618 - acc: 0.8913 - val_loss: 0.4219 - val_acc: 0.8863\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3595 - acc: 0.8903\n",
      "Epoch 00057: val_loss did not improve from 0.42192\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.3594 - acc: 0.8903 - val_loss: 0.4290 - val_acc: 0.8859\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3563 - acc: 0.8915\n",
      "Epoch 00058: val_loss did not improve from 0.42192\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.3564 - acc: 0.8914 - val_loss: 0.4493 - val_acc: 0.8828\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3558 - acc: 0.8918\n",
      "Epoch 00059: val_loss improved from 0.42192 to 0.42182, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_6_conv_checkpoint/059-0.4218.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.3558 - acc: 0.8918 - val_loss: 0.4218 - val_acc: 0.8917\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3501 - acc: 0.8950\n",
      "Epoch 00060: val_loss did not improve from 0.42182\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.3500 - acc: 0.8950 - val_loss: 0.4759 - val_acc: 0.8821\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3479 - acc: 0.8948\n",
      "Epoch 00061: val_loss did not improve from 0.42182\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.3479 - acc: 0.8949 - val_loss: 0.4265 - val_acc: 0.8828\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3470 - acc: 0.8942\n",
      "Epoch 00062: val_loss did not improve from 0.42182\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.3470 - acc: 0.8942 - val_loss: 0.4782 - val_acc: 0.8791\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3385 - acc: 0.8994\n",
      "Epoch 00063: val_loss did not improve from 0.42182\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.3385 - acc: 0.8994 - val_loss: 0.5117 - val_acc: 0.8754\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3377 - acc: 0.8979\n",
      "Epoch 00064: val_loss did not improve from 0.42182\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.3378 - acc: 0.8978 - val_loss: 0.5023 - val_acc: 0.8684\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3383 - acc: 0.8972\n",
      "Epoch 00065: val_loss did not improve from 0.42182\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.3384 - acc: 0.8972 - val_loss: 0.4458 - val_acc: 0.8849\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3298 - acc: 0.9018\n",
      "Epoch 00066: val_loss did not improve from 0.42182\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.3298 - acc: 0.9018 - val_loss: 0.4288 - val_acc: 0.8931\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3283 - acc: 0.9015\n",
      "Epoch 00067: val_loss did not improve from 0.42182\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.3283 - acc: 0.9015 - val_loss: 0.4747 - val_acc: 0.8724\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3243 - acc: 0.9017\n",
      "Epoch 00068: val_loss did not improve from 0.42182\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.3243 - acc: 0.9017 - val_loss: 0.4226 - val_acc: 0.8917\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3323 - acc: 0.8981\n",
      "Epoch 00069: val_loss did not improve from 0.42182\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.3323 - acc: 0.8981 - val_loss: 0.4543 - val_acc: 0.8910\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3226 - acc: 0.9022\n",
      "Epoch 00070: val_loss did not improve from 0.42182\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.3226 - acc: 0.9022 - val_loss: 0.4592 - val_acc: 0.8791\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3185 - acc: 0.9038\n",
      "Epoch 00071: val_loss did not improve from 0.42182\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.3188 - acc: 0.9038 - val_loss: 0.5279 - val_acc: 0.8558\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3221 - acc: 0.9016\n",
      "Epoch 00072: val_loss improved from 0.42182 to 0.40855, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_6_conv_checkpoint/072-0.4086.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.3221 - acc: 0.9016 - val_loss: 0.4086 - val_acc: 0.8991\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3141 - acc: 0.9049\n",
      "Epoch 00073: val_loss did not improve from 0.40855\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.3141 - acc: 0.9049 - val_loss: 0.4173 - val_acc: 0.8938\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3103 - acc: 0.9035\n",
      "Epoch 00074: val_loss improved from 0.40855 to 0.40648, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_6_conv_checkpoint/074-0.4065.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.3103 - acc: 0.9035 - val_loss: 0.4065 - val_acc: 0.8975\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3108 - acc: 0.9034\n",
      "Epoch 00075: val_loss did not improve from 0.40648\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.3108 - acc: 0.9034 - val_loss: 0.4319 - val_acc: 0.8889\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3080 - acc: 0.9055\n",
      "Epoch 00076: val_loss did not improve from 0.40648\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.3080 - acc: 0.9055 - val_loss: 0.4540 - val_acc: 0.8742\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3070 - acc: 0.9063\n",
      "Epoch 00077: val_loss did not improve from 0.40648\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.3071 - acc: 0.9063 - val_loss: 0.4281 - val_acc: 0.8889\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3029 - acc: 0.9081\n",
      "Epoch 00078: val_loss did not improve from 0.40648\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.3029 - acc: 0.9081 - val_loss: 0.4906 - val_acc: 0.8728\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3019 - acc: 0.9064\n",
      "Epoch 00079: val_loss did not improve from 0.40648\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.3019 - acc: 0.9064 - val_loss: 0.4105 - val_acc: 0.8891\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2952 - acc: 0.9099\n",
      "Epoch 00080: val_loss did not improve from 0.40648\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2952 - acc: 0.9099 - val_loss: 0.4065 - val_acc: 0.8966\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2947 - acc: 0.9084\n",
      "Epoch 00081: val_loss improved from 0.40648 to 0.40462, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_6_conv_checkpoint/081-0.4046.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2951 - acc: 0.9083 - val_loss: 0.4046 - val_acc: 0.8977\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2976 - acc: 0.9088\n",
      "Epoch 00082: val_loss did not improve from 0.40462\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2977 - acc: 0.9087 - val_loss: 0.4128 - val_acc: 0.8935\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2942 - acc: 0.9099\n",
      "Epoch 00083: val_loss did not improve from 0.40462\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2942 - acc: 0.9099 - val_loss: 0.4379 - val_acc: 0.8903\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2913 - acc: 0.9107\n",
      "Epoch 00084: val_loss improved from 0.40462 to 0.39749, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_6_conv_checkpoint/084-0.3975.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2914 - acc: 0.9107 - val_loss: 0.3975 - val_acc: 0.8938\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2864 - acc: 0.9126\n",
      "Epoch 00085: val_loss did not improve from 0.39749\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2865 - acc: 0.9126 - val_loss: 0.4297 - val_acc: 0.8910\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2871 - acc: 0.9101\n",
      "Epoch 00086: val_loss did not improve from 0.39749\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2871 - acc: 0.9100 - val_loss: 0.4570 - val_acc: 0.8796\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2836 - acc: 0.9127\n",
      "Epoch 00087: val_loss did not improve from 0.39749\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2836 - acc: 0.9126 - val_loss: 0.4314 - val_acc: 0.8894\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2840 - acc: 0.9109\n",
      "Epoch 00088: val_loss did not improve from 0.39749\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2840 - acc: 0.9109 - val_loss: 0.4632 - val_acc: 0.8905\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2784 - acc: 0.9149\n",
      "Epoch 00089: val_loss did not improve from 0.39749\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2784 - acc: 0.9149 - val_loss: 0.4438 - val_acc: 0.8917\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2760 - acc: 0.9159\n",
      "Epoch 00090: val_loss did not improve from 0.39749\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2761 - acc: 0.9159 - val_loss: 0.4404 - val_acc: 0.8824\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2781 - acc: 0.9156\n",
      "Epoch 00091: val_loss did not improve from 0.39749\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2782 - acc: 0.9156 - val_loss: 0.4019 - val_acc: 0.9033\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2793 - acc: 0.9118\n",
      "Epoch 00092: val_loss did not improve from 0.39749\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2793 - acc: 0.9118 - val_loss: 0.4033 - val_acc: 0.8966\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2728 - acc: 0.9160\n",
      "Epoch 00093: val_loss did not improve from 0.39749\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2731 - acc: 0.9159 - val_loss: 0.4098 - val_acc: 0.8940\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2742 - acc: 0.9164\n",
      "Epoch 00094: val_loss improved from 0.39749 to 0.38413, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_6_conv_checkpoint/094-0.3841.hdf5\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2742 - acc: 0.9165 - val_loss: 0.3841 - val_acc: 0.9024\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2676 - acc: 0.9168\n",
      "Epoch 00095: val_loss improved from 0.38413 to 0.37874, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_6_conv_checkpoint/095-0.3787.hdf5\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2676 - acc: 0.9168 - val_loss: 0.3787 - val_acc: 0.9043\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2659 - acc: 0.9168\n",
      "Epoch 00096: val_loss did not improve from 0.37874\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2660 - acc: 0.9168 - val_loss: 0.4942 - val_acc: 0.8828\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2645 - acc: 0.9184\n",
      "Epoch 00097: val_loss did not improve from 0.37874\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2645 - acc: 0.9184 - val_loss: 0.4241 - val_acc: 0.8831\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2654 - acc: 0.9178\n",
      "Epoch 00098: val_loss did not improve from 0.37874\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2654 - acc: 0.9178 - val_loss: 0.4115 - val_acc: 0.8947\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2673 - acc: 0.9169\n",
      "Epoch 00099: val_loss did not improve from 0.37874\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2675 - acc: 0.9169 - val_loss: 0.4542 - val_acc: 0.8882\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2598 - acc: 0.9184\n",
      "Epoch 00100: val_loss did not improve from 0.37874\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2597 - acc: 0.9184 - val_loss: 0.4354 - val_acc: 0.8933\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2565 - acc: 0.9192\n",
      "Epoch 00101: val_loss did not improve from 0.37874\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2565 - acc: 0.9192 - val_loss: 0.4425 - val_acc: 0.8840\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2630 - acc: 0.9175\n",
      "Epoch 00102: val_loss did not improve from 0.37874\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2630 - acc: 0.9175 - val_loss: 1.1121 - val_acc: 0.7370\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2557 - acc: 0.9203\n",
      "Epoch 00103: val_loss did not improve from 0.37874\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2557 - acc: 0.9203 - val_loss: 0.3832 - val_acc: 0.9061\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2569 - acc: 0.9200\n",
      "Epoch 00104: val_loss did not improve from 0.37874\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2575 - acc: 0.9199 - val_loss: 0.4036 - val_acc: 0.9024\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2565 - acc: 0.9206\n",
      "Epoch 00105: val_loss did not improve from 0.37874\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2565 - acc: 0.9206 - val_loss: 0.4134 - val_acc: 0.8961\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2491 - acc: 0.9221\n",
      "Epoch 00106: val_loss did not improve from 0.37874\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2493 - acc: 0.9220 - val_loss: 0.3917 - val_acc: 0.9022\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2565 - acc: 0.9194\n",
      "Epoch 00107: val_loss improved from 0.37874 to 0.37774, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_6_conv_checkpoint/107-0.3777.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2564 - acc: 0.9194 - val_loss: 0.3777 - val_acc: 0.9052\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2462 - acc: 0.9233\n",
      "Epoch 00108: val_loss did not improve from 0.37774\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2463 - acc: 0.9233 - val_loss: 0.4915 - val_acc: 0.8677\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2528 - acc: 0.9196\n",
      "Epoch 00109: val_loss did not improve from 0.37774\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2530 - acc: 0.9195 - val_loss: 0.4513 - val_acc: 0.8824\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2502 - acc: 0.9215\n",
      "Epoch 00110: val_loss did not improve from 0.37774\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2502 - acc: 0.9216 - val_loss: 0.3941 - val_acc: 0.9045\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2454 - acc: 0.9232\n",
      "Epoch 00111: val_loss did not improve from 0.37774\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2454 - acc: 0.9232 - val_loss: 0.4095 - val_acc: 0.8975\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2409 - acc: 0.9248\n",
      "Epoch 00112: val_loss did not improve from 0.37774\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2409 - acc: 0.9248 - val_loss: 0.3968 - val_acc: 0.8989\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2413 - acc: 0.9240\n",
      "Epoch 00113: val_loss did not improve from 0.37774\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2413 - acc: 0.9240 - val_loss: 0.4066 - val_acc: 0.8935\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2385 - acc: 0.9242\n",
      "Epoch 00114: val_loss did not improve from 0.37774\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2385 - acc: 0.9242 - val_loss: 0.4656 - val_acc: 0.8807\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2415 - acc: 0.9241\n",
      "Epoch 00115: val_loss did not improve from 0.37774\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2417 - acc: 0.9240 - val_loss: 0.4248 - val_acc: 0.9019\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2469 - acc: 0.9224\n",
      "Epoch 00116: val_loss did not improve from 0.37774\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2469 - acc: 0.9224 - val_loss: 0.4721 - val_acc: 0.8763\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2342 - acc: 0.9257\n",
      "Epoch 00117: val_loss did not improve from 0.37774\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2342 - acc: 0.9257 - val_loss: 0.4168 - val_acc: 0.8994\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2340 - acc: 0.9266\n",
      "Epoch 00118: val_loss did not improve from 0.37774\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2340 - acc: 0.9266 - val_loss: 0.4215 - val_acc: 0.8949\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2317 - acc: 0.9277\n",
      "Epoch 00119: val_loss did not improve from 0.37774\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2316 - acc: 0.9277 - val_loss: 0.4103 - val_acc: 0.8949\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2286 - acc: 0.9274\n",
      "Epoch 00120: val_loss did not improve from 0.37774\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2287 - acc: 0.9273 - val_loss: 0.4356 - val_acc: 0.8910\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2305 - acc: 0.9257\n",
      "Epoch 00121: val_loss did not improve from 0.37774\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2304 - acc: 0.9257 - val_loss: 0.4052 - val_acc: 0.9031\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2308 - acc: 0.9277\n",
      "Epoch 00122: val_loss improved from 0.37774 to 0.37120, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_6_conv_checkpoint/122-0.3712.hdf5\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2308 - acc: 0.9277 - val_loss: 0.3712 - val_acc: 0.9045\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2261 - acc: 0.9291\n",
      "Epoch 00123: val_loss did not improve from 0.37120\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2261 - acc: 0.9291 - val_loss: 0.3971 - val_acc: 0.9045\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2270 - acc: 0.9276\n",
      "Epoch 00124: val_loss did not improve from 0.37120\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2275 - acc: 0.9275 - val_loss: 0.4380 - val_acc: 0.8863\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2308 - acc: 0.9260\n",
      "Epoch 00125: val_loss did not improve from 0.37120\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2309 - acc: 0.9259 - val_loss: 0.3910 - val_acc: 0.9019\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2232 - acc: 0.9302\n",
      "Epoch 00126: val_loss did not improve from 0.37120\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2232 - acc: 0.9301 - val_loss: 0.4583 - val_acc: 0.8798\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2258 - acc: 0.9301\n",
      "Epoch 00127: val_loss did not improve from 0.37120\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2258 - acc: 0.9301 - val_loss: 0.3965 - val_acc: 0.8994\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2206 - acc: 0.9302\n",
      "Epoch 00128: val_loss did not improve from 0.37120\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2206 - acc: 0.9302 - val_loss: 0.4090 - val_acc: 0.8968\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2173 - acc: 0.9307\n",
      "Epoch 00129: val_loss did not improve from 0.37120\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2172 - acc: 0.9307 - val_loss: 0.4022 - val_acc: 0.8961\n",
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2147 - acc: 0.9337\n",
      "Epoch 00130: val_loss did not improve from 0.37120\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2147 - acc: 0.9337 - val_loss: 0.4038 - val_acc: 0.8968\n",
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2203 - acc: 0.9298\n",
      "Epoch 00131: val_loss did not improve from 0.37120\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2203 - acc: 0.9298 - val_loss: 0.4274 - val_acc: 0.8940\n",
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2148 - acc: 0.9321\n",
      "Epoch 00132: val_loss did not improve from 0.37120\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2148 - acc: 0.9321 - val_loss: 0.4334 - val_acc: 0.8912\n",
      "Epoch 133/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2154 - acc: 0.9322\n",
      "Epoch 00133: val_loss did not improve from 0.37120\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2154 - acc: 0.9322 - val_loss: 0.3924 - val_acc: 0.9050\n",
      "Epoch 134/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2134 - acc: 0.9321\n",
      "Epoch 00134: val_loss did not improve from 0.37120\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2134 - acc: 0.9321 - val_loss: 0.3837 - val_acc: 0.8996\n",
      "Epoch 135/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2144 - acc: 0.9316\n",
      "Epoch 00135: val_loss did not improve from 0.37120\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2144 - acc: 0.9316 - val_loss: 0.4037 - val_acc: 0.8982\n",
      "Epoch 136/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2148 - acc: 0.9320\n",
      "Epoch 00136: val_loss did not improve from 0.37120\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2148 - acc: 0.9320 - val_loss: 0.4545 - val_acc: 0.8889\n",
      "Epoch 137/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2165 - acc: 0.9318\n",
      "Epoch 00137: val_loss did not improve from 0.37120\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2165 - acc: 0.9318 - val_loss: 0.4998 - val_acc: 0.8742\n",
      "Epoch 138/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2145 - acc: 0.9317\n",
      "Epoch 00138: val_loss did not improve from 0.37120\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2144 - acc: 0.9317 - val_loss: 0.4256 - val_acc: 0.8956\n",
      "Epoch 139/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2063 - acc: 0.9347\n",
      "Epoch 00139: val_loss did not improve from 0.37120\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2066 - acc: 0.9346 - val_loss: 0.3961 - val_acc: 0.9043\n",
      "Epoch 140/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2117 - acc: 0.9335\n",
      "Epoch 00140: val_loss did not improve from 0.37120\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2117 - acc: 0.9335 - val_loss: 0.4123 - val_acc: 0.8963\n",
      "Epoch 141/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2103 - acc: 0.9339\n",
      "Epoch 00141: val_loss did not improve from 0.37120\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2104 - acc: 0.9338 - val_loss: 0.3982 - val_acc: 0.9005\n",
      "Epoch 142/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2088 - acc: 0.9329\n",
      "Epoch 00142: val_loss did not improve from 0.37120\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2088 - acc: 0.9329 - val_loss: 0.4170 - val_acc: 0.8991\n",
      "Epoch 143/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2095 - acc: 0.9347\n",
      "Epoch 00143: val_loss did not improve from 0.37120\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2096 - acc: 0.9346 - val_loss: 0.4263 - val_acc: 0.8966\n",
      "Epoch 144/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2095 - acc: 0.9329\n",
      "Epoch 00144: val_loss did not improve from 0.37120\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2094 - acc: 0.9329 - val_loss: 0.5512 - val_acc: 0.8535\n",
      "Epoch 145/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2055 - acc: 0.9336\n",
      "Epoch 00145: val_loss did not improve from 0.37120\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2055 - acc: 0.9337 - val_loss: 0.4188 - val_acc: 0.8970\n",
      "Epoch 146/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1984 - acc: 0.9376\n",
      "Epoch 00146: val_loss did not improve from 0.37120\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1984 - acc: 0.9375 - val_loss: 0.4611 - val_acc: 0.8887\n",
      "Epoch 147/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2087 - acc: 0.9330\n",
      "Epoch 00147: val_loss did not improve from 0.37120\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2087 - acc: 0.9330 - val_loss: 0.4048 - val_acc: 0.9059\n",
      "Epoch 148/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2005 - acc: 0.9369\n",
      "Epoch 00148: val_loss did not improve from 0.37120\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2005 - acc: 0.9369 - val_loss: 0.4013 - val_acc: 0.9066\n",
      "Epoch 149/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1995 - acc: 0.9354\n",
      "Epoch 00149: val_loss did not improve from 0.37120\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1995 - acc: 0.9354 - val_loss: 0.3978 - val_acc: 0.8952\n",
      "Epoch 150/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1957 - acc: 0.9374\n",
      "Epoch 00150: val_loss did not improve from 0.37120\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1958 - acc: 0.9373 - val_loss: 0.4154 - val_acc: 0.9038\n",
      "Epoch 151/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2029 - acc: 0.9350\n",
      "Epoch 00151: val_loss did not improve from 0.37120\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2029 - acc: 0.9350 - val_loss: 0.4220 - val_acc: 0.9026\n",
      "Epoch 152/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1991 - acc: 0.9351\n",
      "Epoch 00152: val_loss did not improve from 0.37120\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1991 - acc: 0.9350 - val_loss: 0.3859 - val_acc: 0.9043\n",
      "Epoch 153/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1952 - acc: 0.9380\n",
      "Epoch 00153: val_loss did not improve from 0.37120\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1951 - acc: 0.9380 - val_loss: 0.4988 - val_acc: 0.8684\n",
      "Epoch 154/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1923 - acc: 0.9400\n",
      "Epoch 00154: val_loss did not improve from 0.37120\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1923 - acc: 0.9400 - val_loss: 0.4284 - val_acc: 0.8919\n",
      "Epoch 155/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1978 - acc: 0.9356\n",
      "Epoch 00155: val_loss did not improve from 0.37120\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1978 - acc: 0.9356 - val_loss: 0.4403 - val_acc: 0.8975\n",
      "Epoch 156/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1905 - acc: 0.9393\n",
      "Epoch 00156: val_loss did not improve from 0.37120\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1906 - acc: 0.9392 - val_loss: 0.5275 - val_acc: 0.8737\n",
      "Epoch 157/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1938 - acc: 0.9375\n",
      "Epoch 00157: val_loss did not improve from 0.37120\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1938 - acc: 0.9375 - val_loss: 0.6154 - val_acc: 0.8400\n",
      "Epoch 158/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1907 - acc: 0.9391\n",
      "Epoch 00158: val_loss did not improve from 0.37120\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1907 - acc: 0.9391 - val_loss: 0.8056 - val_acc: 0.8109\n",
      "Epoch 159/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1925 - acc: 0.9378\n",
      "Epoch 00159: val_loss did not improve from 0.37120\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1926 - acc: 0.9378 - val_loss: 0.4550 - val_acc: 0.8921\n",
      "Epoch 160/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1925 - acc: 0.9388\n",
      "Epoch 00160: val_loss did not improve from 0.37120\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1925 - acc: 0.9388 - val_loss: 0.6038 - val_acc: 0.8437\n",
      "Epoch 161/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1878 - acc: 0.9396\n",
      "Epoch 00161: val_loss did not improve from 0.37120\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1878 - acc: 0.9396 - val_loss: 0.4072 - val_acc: 0.8959\n",
      "Epoch 162/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1864 - acc: 0.9409\n",
      "Epoch 00162: val_loss did not improve from 0.37120\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1865 - acc: 0.9409 - val_loss: 0.5506 - val_acc: 0.8677\n",
      "Epoch 163/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1887 - acc: 0.9390\n",
      "Epoch 00163: val_loss did not improve from 0.37120\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1887 - acc: 0.9390 - val_loss: 0.4252 - val_acc: 0.8959\n",
      "Epoch 164/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1901 - acc: 0.9404\n",
      "Epoch 00164: val_loss did not improve from 0.37120\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1901 - acc: 0.9404 - val_loss: 0.6075 - val_acc: 0.8574\n",
      "Epoch 165/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1868 - acc: 0.9412\n",
      "Epoch 00165: val_loss did not improve from 0.37120\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1869 - acc: 0.9412 - val_loss: 0.5871 - val_acc: 0.8535\n",
      "Epoch 166/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1843 - acc: 0.9393\n",
      "Epoch 00166: val_loss did not improve from 0.37120\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1848 - acc: 0.9392 - val_loss: 0.6922 - val_acc: 0.8362\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 167/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1900 - acc: 0.9386\n",
      "Epoch 00167: val_loss did not improve from 0.37120\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1901 - acc: 0.9385 - val_loss: 0.4159 - val_acc: 0.9010\n",
      "Epoch 168/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1863 - acc: 0.9395\n",
      "Epoch 00168: val_loss did not improve from 0.37120\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1867 - acc: 0.9394 - val_loss: 0.4292 - val_acc: 0.9045\n",
      "Epoch 169/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1844 - acc: 0.9421\n",
      "Epoch 00169: val_loss did not improve from 0.37120\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1844 - acc: 0.9422 - val_loss: 0.4144 - val_acc: 0.8982\n",
      "Epoch 170/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1810 - acc: 0.9414\n",
      "Epoch 00170: val_loss did not improve from 0.37120\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1813 - acc: 0.9414 - val_loss: 0.4865 - val_acc: 0.8896\n",
      "Epoch 171/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1932 - acc: 0.9391\n",
      "Epoch 00171: val_loss did not improve from 0.37120\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1934 - acc: 0.9390 - val_loss: 0.3904 - val_acc: 0.9029\n",
      "Epoch 172/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1832 - acc: 0.9415\n",
      "Epoch 00172: val_loss did not improve from 0.37120\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1835 - acc: 0.9414 - val_loss: 0.4428 - val_acc: 0.8987\n",
      "\n",
      "1D_CNN_custom_4_ch_64_DO_BN_6_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 876us/sample - loss: 0.4401 - acc: 0.8766\n",
      "Loss: 0.44006399147730874 Accuracy: 0.8766355\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 3.1166 - acc: 0.0966\n",
      "Epoch 00001: val_loss improved from inf to 2.54271, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_7_conv_checkpoint/001-2.5427.hdf5\n",
      "36805/36805 [==============================] - 95s 3ms/sample - loss: 3.1164 - acc: 0.0966 - val_loss: 2.5427 - val_acc: 0.1740\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.5054 - acc: 0.2032\n",
      "Epoch 00002: val_loss improved from 2.54271 to 1.97185, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_7_conv_checkpoint/002-1.9718.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 2.5053 - acc: 0.2032 - val_loss: 1.9718 - val_acc: 0.3993\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.1051 - acc: 0.3129\n",
      "Epoch 00003: val_loss improved from 1.97185 to 1.64436, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_7_conv_checkpoint/003-1.6444.hdf5\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 2.1052 - acc: 0.3128 - val_loss: 1.6444 - val_acc: 0.5015\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.8452 - acc: 0.3967\n",
      "Epoch 00004: val_loss improved from 1.64436 to 1.52197, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_7_conv_checkpoint/004-1.5220.hdf5\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 1.8453 - acc: 0.3967 - val_loss: 1.5220 - val_acc: 0.5420\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6476 - acc: 0.4629\n",
      "Epoch 00005: val_loss improved from 1.52197 to 1.33356, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_7_conv_checkpoint/005-1.3336.hdf5\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 1.6476 - acc: 0.4629 - val_loss: 1.3336 - val_acc: 0.6091\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4933 - acc: 0.5182\n",
      "Epoch 00006: val_loss improved from 1.33356 to 1.16565, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_7_conv_checkpoint/006-1.1656.hdf5\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 1.4933 - acc: 0.5181 - val_loss: 1.1656 - val_acc: 0.6723\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3691 - acc: 0.5608\n",
      "Epoch 00007: val_loss improved from 1.16565 to 1.03738, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_7_conv_checkpoint/007-1.0374.hdf5\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 1.3692 - acc: 0.5608 - val_loss: 1.0374 - val_acc: 0.7051\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2553 - acc: 0.6023\n",
      "Epoch 00008: val_loss did not improve from 1.03738\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 1.2553 - acc: 0.6022 - val_loss: 1.0473 - val_acc: 0.7058\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1704 - acc: 0.6325\n",
      "Epoch 00009: val_loss improved from 1.03738 to 0.85868, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_7_conv_checkpoint/009-0.8587.hdf5\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 1.1703 - acc: 0.6325 - val_loss: 0.8587 - val_acc: 0.7601\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0841 - acc: 0.6633\n",
      "Epoch 00010: val_loss improved from 0.85868 to 0.83932, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_7_conv_checkpoint/010-0.8393.hdf5\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 1.0841 - acc: 0.6633 - val_loss: 0.8393 - val_acc: 0.7645\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0179 - acc: 0.6857\n",
      "Epoch 00011: val_loss did not improve from 0.83932\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 1.0181 - acc: 0.6857 - val_loss: 0.8643 - val_acc: 0.7608\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9708 - acc: 0.7002\n",
      "Epoch 00012: val_loss improved from 0.83932 to 0.71691, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_7_conv_checkpoint/012-0.7169.hdf5\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.9708 - acc: 0.7002 - val_loss: 0.7169 - val_acc: 0.8153\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9148 - acc: 0.7214\n",
      "Epoch 00013: val_loss did not improve from 0.71691\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.9148 - acc: 0.7213 - val_loss: 0.7204 - val_acc: 0.8025\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8773 - acc: 0.7318\n",
      "Epoch 00014: val_loss improved from 0.71691 to 0.71646, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_7_conv_checkpoint/014-0.7165.hdf5\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.8773 - acc: 0.7317 - val_loss: 0.7165 - val_acc: 0.7985\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8461 - acc: 0.7455\n",
      "Epoch 00015: val_loss improved from 0.71646 to 0.67317, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_7_conv_checkpoint/015-0.6732.hdf5\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.8461 - acc: 0.7455 - val_loss: 0.6732 - val_acc: 0.8192\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8081 - acc: 0.7583\n",
      "Epoch 00016: val_loss improved from 0.67317 to 0.60368, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_7_conv_checkpoint/016-0.6037.hdf5\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.8081 - acc: 0.7583 - val_loss: 0.6037 - val_acc: 0.8451\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7812 - acc: 0.7638\n",
      "Epoch 00017: val_loss improved from 0.60368 to 0.59826, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_7_conv_checkpoint/017-0.5983.hdf5\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.7813 - acc: 0.7638 - val_loss: 0.5983 - val_acc: 0.8432\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7585 - acc: 0.7713\n",
      "Epoch 00018: val_loss did not improve from 0.59826\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.7585 - acc: 0.7713 - val_loss: 0.6791 - val_acc: 0.8090\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7363 - acc: 0.7796\n",
      "Epoch 00019: val_loss improved from 0.59826 to 0.57076, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_7_conv_checkpoint/019-0.5708.hdf5\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.7363 - acc: 0.7796 - val_loss: 0.5708 - val_acc: 0.8451\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7110 - acc: 0.7909\n",
      "Epoch 00020: val_loss did not improve from 0.57076\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.7110 - acc: 0.7909 - val_loss: 0.6335 - val_acc: 0.8206\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7046 - acc: 0.7931\n",
      "Epoch 00021: val_loss improved from 0.57076 to 0.55797, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_7_conv_checkpoint/021-0.5580.hdf5\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.7046 - acc: 0.7931 - val_loss: 0.5580 - val_acc: 0.8456\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6668 - acc: 0.8002\n",
      "Epoch 00022: val_loss improved from 0.55797 to 0.50766, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_7_conv_checkpoint/022-0.5077.hdf5\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.6669 - acc: 0.8002 - val_loss: 0.5077 - val_acc: 0.8668\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6617 - acc: 0.8015\n",
      "Epoch 00023: val_loss did not improve from 0.50766\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.6618 - acc: 0.8015 - val_loss: 0.5190 - val_acc: 0.8637\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6408 - acc: 0.8085\n",
      "Epoch 00024: val_loss improved from 0.50766 to 0.48764, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_7_conv_checkpoint/024-0.4876.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.6408 - acc: 0.8084 - val_loss: 0.4876 - val_acc: 0.8663\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6279 - acc: 0.8118\n",
      "Epoch 00025: val_loss improved from 0.48764 to 0.47069, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_7_conv_checkpoint/025-0.4707.hdf5\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.6280 - acc: 0.8118 - val_loss: 0.4707 - val_acc: 0.8786\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6198 - acc: 0.8161\n",
      "Epoch 00026: val_loss improved from 0.47069 to 0.46095, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_7_conv_checkpoint/026-0.4610.hdf5\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.6199 - acc: 0.8161 - val_loss: 0.4610 - val_acc: 0.8786\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6067 - acc: 0.8208\n",
      "Epoch 00027: val_loss did not improve from 0.46095\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.6068 - acc: 0.8208 - val_loss: 0.4786 - val_acc: 0.8670\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5914 - acc: 0.8242\n",
      "Epoch 00028: val_loss did not improve from 0.46095\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.5914 - acc: 0.8242 - val_loss: 0.5213 - val_acc: 0.8514\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5843 - acc: 0.8267\n",
      "Epoch 00029: val_loss did not improve from 0.46095\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.5843 - acc: 0.8268 - val_loss: 0.4614 - val_acc: 0.8791\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5704 - acc: 0.8305\n",
      "Epoch 00030: val_loss improved from 0.46095 to 0.41441, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_7_conv_checkpoint/030-0.4144.hdf5\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.5707 - acc: 0.8304 - val_loss: 0.4144 - val_acc: 0.8912\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5652 - acc: 0.8314\n",
      "Epoch 00031: val_loss did not improve from 0.41441\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.5653 - acc: 0.8314 - val_loss: 0.4206 - val_acc: 0.8884\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5547 - acc: 0.8355\n",
      "Epoch 00032: val_loss did not improve from 0.41441\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.5549 - acc: 0.8355 - val_loss: 0.4363 - val_acc: 0.8793\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5489 - acc: 0.8351\n",
      "Epoch 00033: val_loss did not improve from 0.41441\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.5492 - acc: 0.8351 - val_loss: 0.4446 - val_acc: 0.8852\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5324 - acc: 0.8415\n",
      "Epoch 00034: val_loss did not improve from 0.41441\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.5323 - acc: 0.8416 - val_loss: 0.4383 - val_acc: 0.8847\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5298 - acc: 0.8409\n",
      "Epoch 00035: val_loss improved from 0.41441 to 0.38985, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_7_conv_checkpoint/035-0.3898.hdf5\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.5298 - acc: 0.8409 - val_loss: 0.3898 - val_acc: 0.8980\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5184 - acc: 0.8483\n",
      "Epoch 00036: val_loss did not improve from 0.38985\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.5184 - acc: 0.8483 - val_loss: 0.4068 - val_acc: 0.8896\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5174 - acc: 0.8442\n",
      "Epoch 00037: val_loss improved from 0.38985 to 0.38375, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_7_conv_checkpoint/037-0.3838.hdf5\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.5174 - acc: 0.8442 - val_loss: 0.3838 - val_acc: 0.9017\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5039 - acc: 0.8483\n",
      "Epoch 00038: val_loss did not improve from 0.38375\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.5042 - acc: 0.8483 - val_loss: 0.3988 - val_acc: 0.8868\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5095 - acc: 0.8457\n",
      "Epoch 00039: val_loss did not improve from 0.38375\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.5095 - acc: 0.8457 - val_loss: 0.3917 - val_acc: 0.8984\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4937 - acc: 0.8526\n",
      "Epoch 00040: val_loss did not improve from 0.38375\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.4938 - acc: 0.8525 - val_loss: 0.3881 - val_acc: 0.8956\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4866 - acc: 0.8534\n",
      "Epoch 00041: val_loss did not improve from 0.38375\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.4867 - acc: 0.8534 - val_loss: 0.4124 - val_acc: 0.8828\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4830 - acc: 0.8561\n",
      "Epoch 00042: val_loss did not improve from 0.38375\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.4830 - acc: 0.8562 - val_loss: 0.3924 - val_acc: 0.8970\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4767 - acc: 0.8568\n",
      "Epoch 00043: val_loss did not improve from 0.38375\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.4767 - acc: 0.8568 - val_loss: 0.4086 - val_acc: 0.8891\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4727 - acc: 0.8566\n",
      "Epoch 00044: val_loss improved from 0.38375 to 0.36242, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_7_conv_checkpoint/044-0.3624.hdf5\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.4729 - acc: 0.8566 - val_loss: 0.3624 - val_acc: 0.9047\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4625 - acc: 0.8601\n",
      "Epoch 00045: val_loss did not improve from 0.36242\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.4630 - acc: 0.8600 - val_loss: 0.3827 - val_acc: 0.8994\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4634 - acc: 0.8590\n",
      "Epoch 00046: val_loss did not improve from 0.36242\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.4633 - acc: 0.8590 - val_loss: 0.4065 - val_acc: 0.8919\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4514 - acc: 0.8643\n",
      "Epoch 00047: val_loss improved from 0.36242 to 0.34787, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_7_conv_checkpoint/047-0.3479.hdf5\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.4513 - acc: 0.8644 - val_loss: 0.3479 - val_acc: 0.9138\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4505 - acc: 0.8625\n",
      "Epoch 00048: val_loss did not improve from 0.34787\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.4505 - acc: 0.8625 - val_loss: 0.3512 - val_acc: 0.9094\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4406 - acc: 0.8667\n",
      "Epoch 00049: val_loss did not improve from 0.34787\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.4409 - acc: 0.8667 - val_loss: 0.3983 - val_acc: 0.8910\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4383 - acc: 0.8658\n",
      "Epoch 00050: val_loss did not improve from 0.34787\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.4386 - acc: 0.8657 - val_loss: 0.5904 - val_acc: 0.8269\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4468 - acc: 0.8651\n",
      "Epoch 00051: val_loss did not improve from 0.34787\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.4469 - acc: 0.8650 - val_loss: 0.3719 - val_acc: 0.9082\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4375 - acc: 0.8672\n",
      "Epoch 00052: val_loss did not improve from 0.34787\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.4375 - acc: 0.8672 - val_loss: 0.4218 - val_acc: 0.8800\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4260 - acc: 0.8705\n",
      "Epoch 00053: val_loss did not improve from 0.34787\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.4260 - acc: 0.8705 - val_loss: 0.3518 - val_acc: 0.9059\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4232 - acc: 0.8720\n",
      "Epoch 00054: val_loss did not improve from 0.34787\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.4232 - acc: 0.8720 - val_loss: 0.3718 - val_acc: 0.9087\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4267 - acc: 0.8681\n",
      "Epoch 00055: val_loss improved from 0.34787 to 0.33923, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_7_conv_checkpoint/055-0.3392.hdf5\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.4270 - acc: 0.8680 - val_loss: 0.3392 - val_acc: 0.9157\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4266 - acc: 0.8696\n",
      "Epoch 00056: val_loss improved from 0.33923 to 0.32747, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_7_conv_checkpoint/056-0.3275.hdf5\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.4266 - acc: 0.8696 - val_loss: 0.3275 - val_acc: 0.9140\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4134 - acc: 0.8740\n",
      "Epoch 00057: val_loss did not improve from 0.32747\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.4137 - acc: 0.8740 - val_loss: 0.3853 - val_acc: 0.8961\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4180 - acc: 0.8726\n",
      "Epoch 00058: val_loss did not improve from 0.32747\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.4181 - acc: 0.8726 - val_loss: 0.3557 - val_acc: 0.9036\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4088 - acc: 0.8755\n",
      "Epoch 00059: val_loss did not improve from 0.32747\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.4089 - acc: 0.8755 - val_loss: 0.3575 - val_acc: 0.9005\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4025 - acc: 0.8783\n",
      "Epoch 00060: val_loss did not improve from 0.32747\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.4026 - acc: 0.8783 - val_loss: 0.3759 - val_acc: 0.8982\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3997 - acc: 0.8788\n",
      "Epoch 00061: val_loss did not improve from 0.32747\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.3997 - acc: 0.8788 - val_loss: 0.3530 - val_acc: 0.9101\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3960 - acc: 0.8779\n",
      "Epoch 00062: val_loss did not improve from 0.32747\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.3960 - acc: 0.8779 - val_loss: 0.3573 - val_acc: 0.9071\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3945 - acc: 0.8791\n",
      "Epoch 00063: val_loss did not improve from 0.32747\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.3945 - acc: 0.8791 - val_loss: 0.3366 - val_acc: 0.9117\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3962 - acc: 0.8786\n",
      "Epoch 00064: val_loss did not improve from 0.32747\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.3962 - acc: 0.8786 - val_loss: 0.3309 - val_acc: 0.9182\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3902 - acc: 0.8807\n",
      "Epoch 00065: val_loss did not improve from 0.32747\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.3903 - acc: 0.8807 - val_loss: 0.3595 - val_acc: 0.9026\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3846 - acc: 0.8831\n",
      "Epoch 00066: val_loss did not improve from 0.32747\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.3846 - acc: 0.8831 - val_loss: 0.3576 - val_acc: 0.9157\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3815 - acc: 0.8813\n",
      "Epoch 00067: val_loss did not improve from 0.32747\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.3815 - acc: 0.8813 - val_loss: 0.3373 - val_acc: 0.9201\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3812 - acc: 0.8826\n",
      "Epoch 00068: val_loss improved from 0.32747 to 0.32121, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_7_conv_checkpoint/068-0.3212.hdf5\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.3812 - acc: 0.8826 - val_loss: 0.3212 - val_acc: 0.9161\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3752 - acc: 0.8844\n",
      "Epoch 00069: val_loss did not improve from 0.32121\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.3758 - acc: 0.8843 - val_loss: 0.3463 - val_acc: 0.9157\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3864 - acc: 0.8826\n",
      "Epoch 00070: val_loss did not improve from 0.32121\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.3863 - acc: 0.8826 - val_loss: 0.3265 - val_acc: 0.9143\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3683 - acc: 0.8879\n",
      "Epoch 00071: val_loss did not improve from 0.32121\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.3682 - acc: 0.8880 - val_loss: 0.3833 - val_acc: 0.8975\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3676 - acc: 0.8871\n",
      "Epoch 00072: val_loss did not improve from 0.32121\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.3676 - acc: 0.8871 - val_loss: 0.3238 - val_acc: 0.9171\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3669 - acc: 0.8878\n",
      "Epoch 00073: val_loss did not improve from 0.32121\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.3669 - acc: 0.8878 - val_loss: 0.3253 - val_acc: 0.9152\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3593 - acc: 0.8886\n",
      "Epoch 00074: val_loss did not improve from 0.32121\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.3593 - acc: 0.8886 - val_loss: 0.3400 - val_acc: 0.9096\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3664 - acc: 0.8875\n",
      "Epoch 00075: val_loss did not improve from 0.32121\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.3664 - acc: 0.8875 - val_loss: 0.3585 - val_acc: 0.9068\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3569 - acc: 0.8900\n",
      "Epoch 00076: val_loss did not improve from 0.32121\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.3569 - acc: 0.8900 - val_loss: 0.3345 - val_acc: 0.9138\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3544 - acc: 0.8907\n",
      "Epoch 00077: val_loss improved from 0.32121 to 0.31574, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_7_conv_checkpoint/077-0.3157.hdf5\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.3544 - acc: 0.8907 - val_loss: 0.3157 - val_acc: 0.9150\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3561 - acc: 0.8888\n",
      "Epoch 00078: val_loss did not improve from 0.31574\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.3563 - acc: 0.8888 - val_loss: 0.3427 - val_acc: 0.9131\n",
      "Epoch 79/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3456 - acc: 0.8936\n",
      "Epoch 00079: val_loss did not improve from 0.31574\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.3455 - acc: 0.8936 - val_loss: 0.3252 - val_acc: 0.9189\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3459 - acc: 0.8924\n",
      "Epoch 00080: val_loss did not improve from 0.31574\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.3459 - acc: 0.8924 - val_loss: 0.3332 - val_acc: 0.9175\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3498 - acc: 0.8915\n",
      "Epoch 00081: val_loss did not improve from 0.31574\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.3497 - acc: 0.8915 - val_loss: 0.3438 - val_acc: 0.9143\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3394 - acc: 0.8930\n",
      "Epoch 00082: val_loss did not improve from 0.31574\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.3393 - acc: 0.8930 - val_loss: 0.3463 - val_acc: 0.9136\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3452 - acc: 0.8934\n",
      "Epoch 00083: val_loss improved from 0.31574 to 0.31111, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_7_conv_checkpoint/083-0.3111.hdf5\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.3451 - acc: 0.8934 - val_loss: 0.3111 - val_acc: 0.9182\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3391 - acc: 0.8944\n",
      "Epoch 00084: val_loss did not improve from 0.31111\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.3391 - acc: 0.8944 - val_loss: 0.3172 - val_acc: 0.9161\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3365 - acc: 0.8952\n",
      "Epoch 00085: val_loss improved from 0.31111 to 0.30439, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_7_conv_checkpoint/085-0.3044.hdf5\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.3365 - acc: 0.8952 - val_loss: 0.3044 - val_acc: 0.9250\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3379 - acc: 0.8961\n",
      "Epoch 00086: val_loss did not improve from 0.30439\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.3378 - acc: 0.8961 - val_loss: 0.3350 - val_acc: 0.9168\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3347 - acc: 0.8965\n",
      "Epoch 00087: val_loss did not improve from 0.30439\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.3347 - acc: 0.8965 - val_loss: 0.3078 - val_acc: 0.9180\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3272 - acc: 0.8976\n",
      "Epoch 00088: val_loss did not improve from 0.30439\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.3272 - acc: 0.8976 - val_loss: 0.3046 - val_acc: 0.9213\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3269 - acc: 0.8970\n",
      "Epoch 00089: val_loss did not improve from 0.30439\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.3270 - acc: 0.8970 - val_loss: 0.3161 - val_acc: 0.9231\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3271 - acc: 0.8973\n",
      "Epoch 00090: val_loss improved from 0.30439 to 0.30123, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_7_conv_checkpoint/090-0.3012.hdf5\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.3272 - acc: 0.8973 - val_loss: 0.3012 - val_acc: 0.9185\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3224 - acc: 0.9002\n",
      "Epoch 00091: val_loss did not improve from 0.30123\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.3225 - acc: 0.9002 - val_loss: 0.3120 - val_acc: 0.9250\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3234 - acc: 0.8980\n",
      "Epoch 00092: val_loss did not improve from 0.30123\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.3234 - acc: 0.8980 - val_loss: 0.3682 - val_acc: 0.8998\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3164 - acc: 0.9024\n",
      "Epoch 00093: val_loss did not improve from 0.30123\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.3164 - acc: 0.9024 - val_loss: 0.3802 - val_acc: 0.8984\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3207 - acc: 0.8992\n",
      "Epoch 00094: val_loss did not improve from 0.30123\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.3206 - acc: 0.8993 - val_loss: 0.3185 - val_acc: 0.9199\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3154 - acc: 0.9020\n",
      "Epoch 00095: val_loss did not improve from 0.30123\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.3153 - acc: 0.9020 - val_loss: 0.3083 - val_acc: 0.9257\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3145 - acc: 0.9005\n",
      "Epoch 00096: val_loss did not improve from 0.30123\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.3146 - acc: 0.9005 - val_loss: 0.3223 - val_acc: 0.9192\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3102 - acc: 0.9037\n",
      "Epoch 00097: val_loss did not improve from 0.30123\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.3102 - acc: 0.9037 - val_loss: 0.3270 - val_acc: 0.9178\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3137 - acc: 0.9000\n",
      "Epoch 00098: val_loss did not improve from 0.30123\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.3139 - acc: 0.8999 - val_loss: 0.3128 - val_acc: 0.9189\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3061 - acc: 0.9045\n",
      "Epoch 00099: val_loss did not improve from 0.30123\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.3062 - acc: 0.9045 - val_loss: 0.3323 - val_acc: 0.9201\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3100 - acc: 0.9040\n",
      "Epoch 00100: val_loss did not improve from 0.30123\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.3101 - acc: 0.9040 - val_loss: 0.3021 - val_acc: 0.9215\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3084 - acc: 0.9047\n",
      "Epoch 00101: val_loss did not improve from 0.30123\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.3085 - acc: 0.9047 - val_loss: 0.3282 - val_acc: 0.9175\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3041 - acc: 0.9053\n",
      "Epoch 00102: val_loss did not improve from 0.30123\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.3042 - acc: 0.9053 - val_loss: 0.3112 - val_acc: 0.9264\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3013 - acc: 0.9048\n",
      "Epoch 00103: val_loss did not improve from 0.30123\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.3013 - acc: 0.9048 - val_loss: 0.3606 - val_acc: 0.9140\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3034 - acc: 0.9060\n",
      "Epoch 00104: val_loss did not improve from 0.30123\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.3034 - acc: 0.9060 - val_loss: 0.3157 - val_acc: 0.9173\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3050 - acc: 0.9040\n",
      "Epoch 00105: val_loss did not improve from 0.30123\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.3051 - acc: 0.9040 - val_loss: 0.3212 - val_acc: 0.9117\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2998 - acc: 0.9051\n",
      "Epoch 00106: val_loss did not improve from 0.30123\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2999 - acc: 0.9050 - val_loss: 0.3154 - val_acc: 0.9231\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2961 - acc: 0.9058\n",
      "Epoch 00107: val_loss improved from 0.30123 to 0.28987, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_7_conv_checkpoint/107-0.2899.hdf5\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2961 - acc: 0.9059 - val_loss: 0.2899 - val_acc: 0.9248\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2928 - acc: 0.9104\n",
      "Epoch 00108: val_loss did not improve from 0.28987\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2929 - acc: 0.9103 - val_loss: 0.3119 - val_acc: 0.9224\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2989 - acc: 0.9060\n",
      "Epoch 00109: val_loss did not improve from 0.28987\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2989 - acc: 0.9060 - val_loss: 0.3308 - val_acc: 0.9203\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2847 - acc: 0.9107\n",
      "Epoch 00110: val_loss did not improve from 0.28987\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2850 - acc: 0.9106 - val_loss: 0.2939 - val_acc: 0.9262\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2943 - acc: 0.9078\n",
      "Epoch 00111: val_loss did not improve from 0.28987\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2943 - acc: 0.9078 - val_loss: 0.3393 - val_acc: 0.9078\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2876 - acc: 0.9095\n",
      "Epoch 00112: val_loss did not improve from 0.28987\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2876 - acc: 0.9095 - val_loss: 0.2920 - val_acc: 0.9243\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2840 - acc: 0.9090\n",
      "Epoch 00113: val_loss did not improve from 0.28987\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2840 - acc: 0.9091 - val_loss: 0.4195 - val_acc: 0.8805\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2885 - acc: 0.9080\n",
      "Epoch 00114: val_loss did not improve from 0.28987\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2884 - acc: 0.9080 - val_loss: 0.2988 - val_acc: 0.9255\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2817 - acc: 0.9129\n",
      "Epoch 00115: val_loss did not improve from 0.28987\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2817 - acc: 0.9129 - val_loss: 0.3353 - val_acc: 0.9145\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2858 - acc: 0.9108\n",
      "Epoch 00116: val_loss did not improve from 0.28987\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2858 - acc: 0.9108 - val_loss: 0.3228 - val_acc: 0.9292\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2846 - acc: 0.9102\n",
      "Epoch 00117: val_loss did not improve from 0.28987\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2846 - acc: 0.9102 - val_loss: 0.3061 - val_acc: 0.9283\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2797 - acc: 0.9133\n",
      "Epoch 00118: val_loss did not improve from 0.28987\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2797 - acc: 0.9133 - val_loss: 0.3034 - val_acc: 0.9201\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2746 - acc: 0.9123\n",
      "Epoch 00119: val_loss did not improve from 0.28987\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2746 - acc: 0.9123 - val_loss: 0.2985 - val_acc: 0.9313\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2733 - acc: 0.9135\n",
      "Epoch 00120: val_loss did not improve from 0.28987\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2734 - acc: 0.9135 - val_loss: 0.3043 - val_acc: 0.9285\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2781 - acc: 0.9116\n",
      "Epoch 00121: val_loss did not improve from 0.28987\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2782 - acc: 0.9115 - val_loss: 0.2943 - val_acc: 0.9317\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2757 - acc: 0.9114\n",
      "Epoch 00122: val_loss did not improve from 0.28987\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2757 - acc: 0.9114 - val_loss: 0.3180 - val_acc: 0.9152\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2720 - acc: 0.9133\n",
      "Epoch 00123: val_loss improved from 0.28987 to 0.28266, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_7_conv_checkpoint/123-0.2827.hdf5\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2719 - acc: 0.9133 - val_loss: 0.2827 - val_acc: 0.9290\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2736 - acc: 0.9147\n",
      "Epoch 00124: val_loss did not improve from 0.28266\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2739 - acc: 0.9147 - val_loss: 0.2946 - val_acc: 0.9276\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2695 - acc: 0.9155\n",
      "Epoch 00125: val_loss did not improve from 0.28266\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2695 - acc: 0.9156 - val_loss: 0.2885 - val_acc: 0.9238\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2662 - acc: 0.9168\n",
      "Epoch 00126: val_loss did not improve from 0.28266\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2662 - acc: 0.9168 - val_loss: 0.2995 - val_acc: 0.9283\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2684 - acc: 0.9153\n",
      "Epoch 00127: val_loss did not improve from 0.28266\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2684 - acc: 0.9153 - val_loss: 0.3008 - val_acc: 0.9273\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2671 - acc: 0.9155\n",
      "Epoch 00128: val_loss did not improve from 0.28266\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2671 - acc: 0.9155 - val_loss: 0.3062 - val_acc: 0.9287\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2617 - acc: 0.9171\n",
      "Epoch 00129: val_loss did not improve from 0.28266\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2617 - acc: 0.9171 - val_loss: 0.2893 - val_acc: 0.9262\n",
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2555 - acc: 0.9182\n",
      "Epoch 00130: val_loss did not improve from 0.28266\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2555 - acc: 0.9182 - val_loss: 0.2991 - val_acc: 0.9285\n",
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2614 - acc: 0.9183\n",
      "Epoch 00131: val_loss did not improve from 0.28266\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2619 - acc: 0.9182 - val_loss: 0.3138 - val_acc: 0.9245\n",
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2678 - acc: 0.9159\n",
      "Epoch 00132: val_loss did not improve from 0.28266\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2677 - acc: 0.9159 - val_loss: 0.3002 - val_acc: 0.9264\n",
      "Epoch 133/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2588 - acc: 0.9179\n",
      "Epoch 00133: val_loss did not improve from 0.28266\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2588 - acc: 0.9178 - val_loss: 0.2988 - val_acc: 0.9257\n",
      "Epoch 134/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2513 - acc: 0.9220\n",
      "Epoch 00134: val_loss did not improve from 0.28266\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2515 - acc: 0.9219 - val_loss: 0.3194 - val_acc: 0.9215\n",
      "Epoch 135/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2607 - acc: 0.9167\n",
      "Epoch 00135: val_loss did not improve from 0.28266\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2607 - acc: 0.9167 - val_loss: 0.3119 - val_acc: 0.9306\n",
      "Epoch 136/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2517 - acc: 0.9197\n",
      "Epoch 00136: val_loss did not improve from 0.28266\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2516 - acc: 0.9197 - val_loss: 0.3170 - val_acc: 0.9234\n",
      "Epoch 137/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2505 - acc: 0.9212\n",
      "Epoch 00137: val_loss did not improve from 0.28266\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2504 - acc: 0.9213 - val_loss: 0.2892 - val_acc: 0.9273\n",
      "Epoch 138/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2534 - acc: 0.9187\n",
      "Epoch 00138: val_loss did not improve from 0.28266\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2534 - acc: 0.9187 - val_loss: 0.3031 - val_acc: 0.9324\n",
      "Epoch 139/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2474 - acc: 0.9201\n",
      "Epoch 00139: val_loss did not improve from 0.28266\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2475 - acc: 0.9201 - val_loss: 0.3047 - val_acc: 0.9252\n",
      "Epoch 140/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2505 - acc: 0.9196\n",
      "Epoch 00140: val_loss did not improve from 0.28266\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2505 - acc: 0.9196 - val_loss: 0.2922 - val_acc: 0.9308\n",
      "Epoch 141/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2523 - acc: 0.9191\n",
      "Epoch 00141: val_loss did not improve from 0.28266\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2525 - acc: 0.9191 - val_loss: 0.3161 - val_acc: 0.9234\n",
      "Epoch 142/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2562 - acc: 0.9179\n",
      "Epoch 00142: val_loss did not improve from 0.28266\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2563 - acc: 0.9178 - val_loss: 0.3067 - val_acc: 0.9280\n",
      "Epoch 143/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2496 - acc: 0.9202\n",
      "Epoch 00143: val_loss did not improve from 0.28266\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2497 - acc: 0.9202 - val_loss: 0.3120 - val_acc: 0.9257\n",
      "Epoch 144/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2429 - acc: 0.9241\n",
      "Epoch 00144: val_loss did not improve from 0.28266\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2429 - acc: 0.9241 - val_loss: 0.2914 - val_acc: 0.9322\n",
      "Epoch 145/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2458 - acc: 0.9211\n",
      "Epoch 00145: val_loss did not improve from 0.28266\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2458 - acc: 0.9211 - val_loss: 0.2891 - val_acc: 0.9306\n",
      "Epoch 146/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2386 - acc: 0.9243\n",
      "Epoch 00146: val_loss did not improve from 0.28266\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2386 - acc: 0.9244 - val_loss: 0.3039 - val_acc: 0.9264\n",
      "Epoch 147/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2425 - acc: 0.9226\n",
      "Epoch 00147: val_loss did not improve from 0.28266\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2425 - acc: 0.9226 - val_loss: 0.3227 - val_acc: 0.9157\n",
      "Epoch 148/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2380 - acc: 0.9229\n",
      "Epoch 00148: val_loss did not improve from 0.28266\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2380 - acc: 0.9229 - val_loss: 0.3190 - val_acc: 0.9154\n",
      "Epoch 149/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2433 - acc: 0.9222\n",
      "Epoch 00149: val_loss did not improve from 0.28266\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2432 - acc: 0.9222 - val_loss: 0.3038 - val_acc: 0.9227\n",
      "Epoch 150/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2342 - acc: 0.9246\n",
      "Epoch 00150: val_loss did not improve from 0.28266\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2343 - acc: 0.9246 - val_loss: 0.3076 - val_acc: 0.9245\n",
      "Epoch 151/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2546 - acc: 0.9193\n",
      "Epoch 00151: val_loss did not improve from 0.28266\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2549 - acc: 0.9192 - val_loss: 0.3180 - val_acc: 0.9266\n",
      "Epoch 152/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2445 - acc: 0.9218\n",
      "Epoch 00152: val_loss improved from 0.28266 to 0.28126, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_7_conv_checkpoint/152-0.2813.hdf5\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2445 - acc: 0.9218 - val_loss: 0.2813 - val_acc: 0.9280\n",
      "Epoch 153/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2312 - acc: 0.9261\n",
      "Epoch 00153: val_loss did not improve from 0.28126\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2312 - acc: 0.9261 - val_loss: 0.2927 - val_acc: 0.9338\n",
      "Epoch 154/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2289 - acc: 0.9278\n",
      "Epoch 00154: val_loss did not improve from 0.28126\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2290 - acc: 0.9278 - val_loss: 0.3131 - val_acc: 0.9297\n",
      "Epoch 155/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2329 - acc: 0.9266\n",
      "Epoch 00155: val_loss did not improve from 0.28126\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2329 - acc: 0.9266 - val_loss: 0.2966 - val_acc: 0.9304\n",
      "Epoch 156/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2316 - acc: 0.9251\n",
      "Epoch 00156: val_loss did not improve from 0.28126\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2318 - acc: 0.9250 - val_loss: 0.3267 - val_acc: 0.9257\n",
      "Epoch 157/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2375 - acc: 0.9236\n",
      "Epoch 00157: val_loss did not improve from 0.28126\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2375 - acc: 0.9236 - val_loss: 0.2956 - val_acc: 0.9301\n",
      "Epoch 158/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2311 - acc: 0.9252\n",
      "Epoch 00158: val_loss did not improve from 0.28126\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2311 - acc: 0.9252 - val_loss: 0.2862 - val_acc: 0.9336\n",
      "Epoch 159/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2299 - acc: 0.9256\n",
      "Epoch 00159: val_loss did not improve from 0.28126\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2300 - acc: 0.9256 - val_loss: 0.3278 - val_acc: 0.9248\n",
      "Epoch 160/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2269 - acc: 0.9283\n",
      "Epoch 00160: val_loss did not improve from 0.28126\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2268 - acc: 0.9283 - val_loss: 0.2979 - val_acc: 0.9324\n",
      "Epoch 161/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2264 - acc: 0.9280\n",
      "Epoch 00161: val_loss did not improve from 0.28126\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2264 - acc: 0.9280 - val_loss: 0.3192 - val_acc: 0.9292\n",
      "Epoch 162/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2312 - acc: 0.9259\n",
      "Epoch 00162: val_loss did not improve from 0.28126\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2312 - acc: 0.9259 - val_loss: 0.3245 - val_acc: 0.9264\n",
      "Epoch 163/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2300 - acc: 0.9256\n",
      "Epoch 00163: val_loss did not improve from 0.28126\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2300 - acc: 0.9256 - val_loss: 0.2937 - val_acc: 0.9294\n",
      "Epoch 164/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2171 - acc: 0.9291\n",
      "Epoch 00164: val_loss did not improve from 0.28126\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2171 - acc: 0.9291 - val_loss: 0.2890 - val_acc: 0.9292\n",
      "Epoch 165/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2286 - acc: 0.9281\n",
      "Epoch 00165: val_loss did not improve from 0.28126\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2286 - acc: 0.9281 - val_loss: 0.3120 - val_acc: 0.9308\n",
      "Epoch 166/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2245 - acc: 0.9265\n",
      "Epoch 00166: val_loss did not improve from 0.28126\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2248 - acc: 0.9265 - val_loss: 0.3184 - val_acc: 0.9241\n",
      "Epoch 167/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2263 - acc: 0.9279\n",
      "Epoch 00167: val_loss did not improve from 0.28126\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2265 - acc: 0.9278 - val_loss: 0.3047 - val_acc: 0.9245\n",
      "Epoch 168/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2241 - acc: 0.9274\n",
      "Epoch 00168: val_loss did not improve from 0.28126\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2243 - acc: 0.9273 - val_loss: 0.3041 - val_acc: 0.9304\n",
      "Epoch 169/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2220 - acc: 0.9289\n",
      "Epoch 00169: val_loss did not improve from 0.28126\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2220 - acc: 0.9289 - val_loss: 0.2991 - val_acc: 0.9273\n",
      "Epoch 170/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2174 - acc: 0.9293\n",
      "Epoch 00170: val_loss did not improve from 0.28126\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2175 - acc: 0.9293 - val_loss: 0.3189 - val_acc: 0.9215\n",
      "Epoch 171/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2244 - acc: 0.9276\n",
      "Epoch 00171: val_loss did not improve from 0.28126\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2245 - acc: 0.9276 - val_loss: 0.3029 - val_acc: 0.9238\n",
      "Epoch 172/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2193 - acc: 0.9294\n",
      "Epoch 00172: val_loss did not improve from 0.28126\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2193 - acc: 0.9294 - val_loss: 0.3218 - val_acc: 0.9273\n",
      "Epoch 173/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2171 - acc: 0.9293\n",
      "Epoch 00173: val_loss did not improve from 0.28126\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2172 - acc: 0.9293 - val_loss: 0.2972 - val_acc: 0.9297\n",
      "Epoch 174/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2202 - acc: 0.9290\n",
      "Epoch 00174: val_loss did not improve from 0.28126\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2202 - acc: 0.9290 - val_loss: 0.3295 - val_acc: 0.9285\n",
      "Epoch 175/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2142 - acc: 0.9314\n",
      "Epoch 00175: val_loss did not improve from 0.28126\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2148 - acc: 0.9313 - val_loss: 0.3110 - val_acc: 0.9329\n",
      "Epoch 176/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2236 - acc: 0.9267\n",
      "Epoch 00176: val_loss did not improve from 0.28126\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2236 - acc: 0.9267 - val_loss: 0.3260 - val_acc: 0.9264\n",
      "Epoch 177/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2130 - acc: 0.9316\n",
      "Epoch 00177: val_loss did not improve from 0.28126\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2131 - acc: 0.9316 - val_loss: 0.3376 - val_acc: 0.9264\n",
      "Epoch 178/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2182 - acc: 0.9296\n",
      "Epoch 00178: val_loss did not improve from 0.28126\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2182 - acc: 0.9296 - val_loss: 0.3067 - val_acc: 0.9297\n",
      "Epoch 179/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2084 - acc: 0.9326\n",
      "Epoch 00179: val_loss did not improve from 0.28126\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2084 - acc: 0.9326 - val_loss: 0.3053 - val_acc: 0.9290\n",
      "Epoch 180/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2116 - acc: 0.9309\n",
      "Epoch 00180: val_loss did not improve from 0.28126\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2116 - acc: 0.9309 - val_loss: 0.3084 - val_acc: 0.9250\n",
      "Epoch 181/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2069 - acc: 0.9327\n",
      "Epoch 00181: val_loss did not improve from 0.28126\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2068 - acc: 0.9327 - val_loss: 0.3774 - val_acc: 0.9147\n",
      "Epoch 182/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2096 - acc: 0.9324\n",
      "Epoch 00182: val_loss did not improve from 0.28126\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2096 - acc: 0.9324 - val_loss: 0.3044 - val_acc: 0.9276\n",
      "Epoch 183/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2092 - acc: 0.9316\n",
      "Epoch 00183: val_loss did not improve from 0.28126\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2092 - acc: 0.9316 - val_loss: 0.3698 - val_acc: 0.9047\n",
      "Epoch 184/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2094 - acc: 0.9320\n",
      "Epoch 00184: val_loss did not improve from 0.28126\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2094 - acc: 0.9320 - val_loss: 0.3218 - val_acc: 0.9238\n",
      "Epoch 185/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2060 - acc: 0.9340\n",
      "Epoch 00185: val_loss did not improve from 0.28126\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2060 - acc: 0.9340 - val_loss: 0.3547 - val_acc: 0.9117\n",
      "Epoch 186/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2072 - acc: 0.9327\n",
      "Epoch 00186: val_loss did not improve from 0.28126\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2072 - acc: 0.9327 - val_loss: 0.3367 - val_acc: 0.9273\n",
      "Epoch 187/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2055 - acc: 0.9336\n",
      "Epoch 00187: val_loss did not improve from 0.28126\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2054 - acc: 0.9337 - val_loss: 0.3345 - val_acc: 0.9250\n",
      "Epoch 188/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2021 - acc: 0.9356\n",
      "Epoch 00188: val_loss did not improve from 0.28126\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2024 - acc: 0.9356 - val_loss: 0.3434 - val_acc: 0.9269\n",
      "Epoch 189/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2154 - acc: 0.9314\n",
      "Epoch 00189: val_loss did not improve from 0.28126\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2154 - acc: 0.9314 - val_loss: 0.3172 - val_acc: 0.9182\n",
      "Epoch 190/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2045 - acc: 0.9327\n",
      "Epoch 00190: val_loss did not improve from 0.28126\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2045 - acc: 0.9327 - val_loss: 0.2987 - val_acc: 0.9287\n",
      "Epoch 191/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2024 - acc: 0.9360\n",
      "Epoch 00191: val_loss did not improve from 0.28126\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2025 - acc: 0.9360 - val_loss: 0.3509 - val_acc: 0.9157\n",
      "Epoch 192/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2097 - acc: 0.9336\n",
      "Epoch 00192: val_loss did not improve from 0.28126\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2098 - acc: 0.9336 - val_loss: 0.3194 - val_acc: 0.9297\n",
      "Epoch 193/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1997 - acc: 0.9357\n",
      "Epoch 00193: val_loss did not improve from 0.28126\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.1997 - acc: 0.9357 - val_loss: 0.2997 - val_acc: 0.9297\n",
      "Epoch 194/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2027 - acc: 0.9355\n",
      "Epoch 00194: val_loss did not improve from 0.28126\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2027 - acc: 0.9354 - val_loss: 0.3032 - val_acc: 0.9290\n",
      "Epoch 195/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2019 - acc: 0.9339\n",
      "Epoch 00195: val_loss did not improve from 0.28126\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2022 - acc: 0.9339 - val_loss: 0.3049 - val_acc: 0.9273\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 196/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2076 - acc: 0.9324\n",
      "Epoch 00196: val_loss did not improve from 0.28126\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.2078 - acc: 0.9323 - val_loss: 0.3101 - val_acc: 0.9243\n",
      "Epoch 197/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1982 - acc: 0.9354\n",
      "Epoch 00197: val_loss did not improve from 0.28126\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.1982 - acc: 0.9354 - val_loss: 0.3302 - val_acc: 0.9331\n",
      "Epoch 198/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1946 - acc: 0.9362\n",
      "Epoch 00198: val_loss did not improve from 0.28126\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.1946 - acc: 0.9363 - val_loss: 0.3257 - val_acc: 0.9217\n",
      "Epoch 199/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1949 - acc: 0.9374\n",
      "Epoch 00199: val_loss did not improve from 0.28126\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.1950 - acc: 0.9373 - val_loss: 0.2997 - val_acc: 0.9315\n",
      "Epoch 200/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1985 - acc: 0.9347\n",
      "Epoch 00200: val_loss did not improve from 0.28126\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.1985 - acc: 0.9347 - val_loss: 0.3212 - val_acc: 0.9243\n",
      "Epoch 201/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1946 - acc: 0.9366\n",
      "Epoch 00201: val_loss did not improve from 0.28126\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.1947 - acc: 0.9365 - val_loss: 0.3169 - val_acc: 0.9294\n",
      "Epoch 202/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1895 - acc: 0.9376\n",
      "Epoch 00202: val_loss did not improve from 0.28126\n",
      "36805/36805 [==============================] - 86s 2ms/sample - loss: 0.1895 - acc: 0.9376 - val_loss: 0.3100 - val_acc: 0.9283\n",
      "\n",
      "1D_CNN_custom_4_ch_64_DO_BN_7_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 933us/sample - loss: 0.3134 - acc: 0.9128\n",
      "Loss: 0.3134123099741535 Accuracy: 0.9127726\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 3.1916 - acc: 0.1076\n",
      "Epoch 00001: val_loss improved from inf to 2.52090, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_8_conv_checkpoint/001-2.5209.hdf5\n",
      "36805/36805 [==============================] - 100s 3ms/sample - loss: 3.1916 - acc: 0.1076 - val_loss: 2.5209 - val_acc: 0.1866\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.6136 - acc: 0.1793\n",
      "Epoch 00002: val_loss improved from 2.52090 to 2.14385, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_8_conv_checkpoint/002-2.1439.hdf5\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 2.6137 - acc: 0.1792 - val_loss: 2.1439 - val_acc: 0.3343\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.3233 - acc: 0.2445\n",
      "Epoch 00003: val_loss improved from 2.14385 to 1.95190, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_8_conv_checkpoint/003-1.9519.hdf5\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 2.3232 - acc: 0.2445 - val_loss: 1.9519 - val_acc: 0.3976\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.1004 - acc: 0.3040\n",
      "Epoch 00004: val_loss improved from 1.95190 to 1.71189, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_8_conv_checkpoint/004-1.7119.hdf5\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 2.1003 - acc: 0.3041 - val_loss: 1.7119 - val_acc: 0.4836\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9347 - acc: 0.3576\n",
      "Epoch 00005: val_loss improved from 1.71189 to 1.58049, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_8_conv_checkpoint/005-1.5805.hdf5\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 1.9347 - acc: 0.3576 - val_loss: 1.5805 - val_acc: 0.4957\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.7847 - acc: 0.4042\n",
      "Epoch 00006: val_loss improved from 1.58049 to 1.42473, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_8_conv_checkpoint/006-1.4247.hdf5\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 1.7847 - acc: 0.4042 - val_loss: 1.4247 - val_acc: 0.5702\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6509 - acc: 0.4451\n",
      "Epoch 00007: val_loss improved from 1.42473 to 1.29319, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_8_conv_checkpoint/007-1.2932.hdf5\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 1.6510 - acc: 0.4450 - val_loss: 1.2932 - val_acc: 0.6110\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5268 - acc: 0.4895\n",
      "Epoch 00008: val_loss did not improve from 1.29319\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 1.5269 - acc: 0.4894 - val_loss: 1.3825 - val_acc: 0.5430\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4266 - acc: 0.5234\n",
      "Epoch 00009: val_loss improved from 1.29319 to 1.09366, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_8_conv_checkpoint/009-1.0937.hdf5\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 1.4266 - acc: 0.5234 - val_loss: 1.0937 - val_acc: 0.6872\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3399 - acc: 0.5569\n",
      "Epoch 00010: val_loss improved from 1.09366 to 1.05388, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_8_conv_checkpoint/010-1.0539.hdf5\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 1.3399 - acc: 0.5569 - val_loss: 1.0539 - val_acc: 0.6851\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2595 - acc: 0.5870\n",
      "Epoch 00011: val_loss improved from 1.05388 to 0.99774, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_8_conv_checkpoint/011-0.9977.hdf5\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 1.2596 - acc: 0.5870 - val_loss: 0.9977 - val_acc: 0.7123\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1963 - acc: 0.6141\n",
      "Epoch 00012: val_loss improved from 0.99774 to 0.87659, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_8_conv_checkpoint/012-0.8766.hdf5\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 1.1962 - acc: 0.6142 - val_loss: 0.8766 - val_acc: 0.7549\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1233 - acc: 0.6392\n",
      "Epoch 00013: val_loss improved from 0.87659 to 0.83289, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_8_conv_checkpoint/013-0.8329.hdf5\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 1.1233 - acc: 0.6392 - val_loss: 0.8329 - val_acc: 0.7675\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0587 - acc: 0.6654\n",
      "Epoch 00014: val_loss improved from 0.83289 to 0.81861, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_8_conv_checkpoint/014-0.8186.hdf5\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 1.0588 - acc: 0.6654 - val_loss: 0.8186 - val_acc: 0.7722\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0118 - acc: 0.6812\n",
      "Epoch 00015: val_loss improved from 0.81861 to 0.79676, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_8_conv_checkpoint/015-0.7968.hdf5\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 1.0120 - acc: 0.6811 - val_loss: 0.7968 - val_acc: 0.7862\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9672 - acc: 0.6983\n",
      "Epoch 00016: val_loss improved from 0.79676 to 0.72976, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_8_conv_checkpoint/016-0.7298.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.9672 - acc: 0.6983 - val_loss: 0.7298 - val_acc: 0.7957\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9266 - acc: 0.7114\n",
      "Epoch 00017: val_loss did not improve from 0.72976\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.9265 - acc: 0.7114 - val_loss: 0.7534 - val_acc: 0.8036\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8951 - acc: 0.7207\n",
      "Epoch 00018: val_loss improved from 0.72976 to 0.64405, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_8_conv_checkpoint/018-0.6440.hdf5\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.8950 - acc: 0.7207 - val_loss: 0.6440 - val_acc: 0.8283\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8513 - acc: 0.7361\n",
      "Epoch 00019: val_loss did not improve from 0.64405\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.8514 - acc: 0.7360 - val_loss: 0.6731 - val_acc: 0.8034\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8186 - acc: 0.7490\n",
      "Epoch 00020: val_loss improved from 0.64405 to 0.58609, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_8_conv_checkpoint/020-0.5861.hdf5\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.8187 - acc: 0.7490 - val_loss: 0.5861 - val_acc: 0.8451\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7959 - acc: 0.7540\n",
      "Epoch 00021: val_loss improved from 0.58609 to 0.56993, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_8_conv_checkpoint/021-0.5699.hdf5\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.7959 - acc: 0.7540 - val_loss: 0.5699 - val_acc: 0.8514\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7624 - acc: 0.7671\n",
      "Epoch 00022: val_loss improved from 0.56993 to 0.51091, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_8_conv_checkpoint/022-0.5109.hdf5\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.7626 - acc: 0.7670 - val_loss: 0.5109 - val_acc: 0.8642\n",
      "Epoch 23/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7460 - acc: 0.7716\n",
      "Epoch 00023: val_loss did not improve from 0.51091\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.7462 - acc: 0.7716 - val_loss: 0.5211 - val_acc: 0.8565\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7279 - acc: 0.7760\n",
      "Epoch 00024: val_loss did not improve from 0.51091\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.7280 - acc: 0.7760 - val_loss: 0.5405 - val_acc: 0.8484\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7019 - acc: 0.7871\n",
      "Epoch 00025: val_loss improved from 0.51091 to 0.49052, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_8_conv_checkpoint/025-0.4905.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.7018 - acc: 0.7871 - val_loss: 0.4905 - val_acc: 0.8658\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6863 - acc: 0.7921\n",
      "Epoch 00026: val_loss improved from 0.49052 to 0.47244, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_8_conv_checkpoint/026-0.4724.hdf5\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.6864 - acc: 0.7920 - val_loss: 0.4724 - val_acc: 0.8635\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6709 - acc: 0.7973\n",
      "Epoch 00027: val_loss did not improve from 0.47244\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.6710 - acc: 0.7972 - val_loss: 0.5163 - val_acc: 0.8549\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6525 - acc: 0.8011\n",
      "Epoch 00028: val_loss improved from 0.47244 to 0.45332, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_8_conv_checkpoint/028-0.4533.hdf5\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.6526 - acc: 0.8011 - val_loss: 0.4533 - val_acc: 0.8758\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6333 - acc: 0.8093\n",
      "Epoch 00029: val_loss improved from 0.45332 to 0.43958, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_8_conv_checkpoint/029-0.4396.hdf5\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.6336 - acc: 0.8093 - val_loss: 0.4396 - val_acc: 0.8677\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6310 - acc: 0.8081\n",
      "Epoch 00030: val_loss did not improve from 0.43958\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.6310 - acc: 0.8081 - val_loss: 0.4667 - val_acc: 0.8728\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6125 - acc: 0.8129\n",
      "Epoch 00031: val_loss did not improve from 0.43958\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.6124 - acc: 0.8129 - val_loss: 0.4680 - val_acc: 0.8838\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5910 - acc: 0.8221\n",
      "Epoch 00032: val_loss improved from 0.43958 to 0.38619, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_8_conv_checkpoint/032-0.3862.hdf5\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.5910 - acc: 0.8221 - val_loss: 0.3862 - val_acc: 0.9019\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5935 - acc: 0.8210\n",
      "Epoch 00033: val_loss did not improve from 0.38619\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.5936 - acc: 0.8210 - val_loss: 0.3920 - val_acc: 0.8889\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5733 - acc: 0.8272\n",
      "Epoch 00034: val_loss did not improve from 0.38619\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.5735 - acc: 0.8272 - val_loss: 0.4408 - val_acc: 0.8838\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5708 - acc: 0.8273\n",
      "Epoch 00035: val_loss did not improve from 0.38619\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.5708 - acc: 0.8273 - val_loss: 0.3932 - val_acc: 0.9019\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5623 - acc: 0.8307\n",
      "Epoch 00036: val_loss did not improve from 0.38619\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.5625 - acc: 0.8306 - val_loss: 0.4471 - val_acc: 0.8856\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5482 - acc: 0.8333\n",
      "Epoch 00037: val_loss improved from 0.38619 to 0.37973, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_8_conv_checkpoint/037-0.3797.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.5481 - acc: 0.8333 - val_loss: 0.3797 - val_acc: 0.9022\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5373 - acc: 0.8354\n",
      "Epoch 00038: val_loss did not improve from 0.37973\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.5374 - acc: 0.8354 - val_loss: 0.3915 - val_acc: 0.8873\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5293 - acc: 0.8398\n",
      "Epoch 00039: val_loss improved from 0.37973 to 0.36812, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_8_conv_checkpoint/039-0.3681.hdf5\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.5299 - acc: 0.8397 - val_loss: 0.3681 - val_acc: 0.9022\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5245 - acc: 0.8430\n",
      "Epoch 00040: val_loss improved from 0.36812 to 0.36246, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_8_conv_checkpoint/040-0.3625.hdf5\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.5244 - acc: 0.8430 - val_loss: 0.3625 - val_acc: 0.8942\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5167 - acc: 0.8407\n",
      "Epoch 00041: val_loss did not improve from 0.36246\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.5167 - acc: 0.8407 - val_loss: 0.4016 - val_acc: 0.8963\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5039 - acc: 0.8455\n",
      "Epoch 00042: val_loss did not improve from 0.36246\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.5039 - acc: 0.8455 - val_loss: 0.3681 - val_acc: 0.9045\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5042 - acc: 0.8476\n",
      "Epoch 00043: val_loss improved from 0.36246 to 0.34475, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_8_conv_checkpoint/043-0.3447.hdf5\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.5042 - acc: 0.8476 - val_loss: 0.3447 - val_acc: 0.9103\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4962 - acc: 0.8493\n",
      "Epoch 00044: val_loss did not improve from 0.34475\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.4962 - acc: 0.8493 - val_loss: 0.3592 - val_acc: 0.9087\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4895 - acc: 0.8536\n",
      "Epoch 00045: val_loss did not improve from 0.34475\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.4895 - acc: 0.8536 - val_loss: 0.4047 - val_acc: 0.8803\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4843 - acc: 0.8515\n",
      "Epoch 00046: val_loss did not improve from 0.34475\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.4845 - acc: 0.8515 - val_loss: 0.4074 - val_acc: 0.8870\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4795 - acc: 0.8566\n",
      "Epoch 00047: val_loss did not improve from 0.34475\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.4795 - acc: 0.8566 - val_loss: 0.3547 - val_acc: 0.8945\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4646 - acc: 0.8574\n",
      "Epoch 00048: val_loss improved from 0.34475 to 0.33848, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_8_conv_checkpoint/048-0.3385.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.4647 - acc: 0.8574 - val_loss: 0.3385 - val_acc: 0.9166\n",
      "Epoch 49/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4664 - acc: 0.8586\n",
      "Epoch 00049: val_loss did not improve from 0.33848\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.4664 - acc: 0.8586 - val_loss: 0.3625 - val_acc: 0.9026\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4577 - acc: 0.8600\n",
      "Epoch 00050: val_loss improved from 0.33848 to 0.33023, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_8_conv_checkpoint/050-0.3302.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.4580 - acc: 0.8600 - val_loss: 0.3302 - val_acc: 0.9115\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4483 - acc: 0.8639\n",
      "Epoch 00051: val_loss did not improve from 0.33023\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.4483 - acc: 0.8639 - val_loss: 0.3924 - val_acc: 0.8877\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4460 - acc: 0.8639\n",
      "Epoch 00052: val_loss improved from 0.33023 to 0.31665, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_8_conv_checkpoint/052-0.3167.hdf5\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.4460 - acc: 0.8639 - val_loss: 0.3167 - val_acc: 0.9175\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4403 - acc: 0.8645\n",
      "Epoch 00053: val_loss improved from 0.31665 to 0.30578, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_8_conv_checkpoint/053-0.3058.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.4403 - acc: 0.8645 - val_loss: 0.3058 - val_acc: 0.9189\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4362 - acc: 0.8672\n",
      "Epoch 00054: val_loss did not improve from 0.30578\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.4364 - acc: 0.8671 - val_loss: 0.3266 - val_acc: 0.9092\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4362 - acc: 0.8665\n",
      "Epoch 00055: val_loss did not improve from 0.30578\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.4364 - acc: 0.8665 - val_loss: 0.3155 - val_acc: 0.9182\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4240 - acc: 0.8720\n",
      "Epoch 00056: val_loss did not improve from 0.30578\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.4239 - acc: 0.8720 - val_loss: 0.3341 - val_acc: 0.9133\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4201 - acc: 0.8724\n",
      "Epoch 00057: val_loss did not improve from 0.30578\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.4201 - acc: 0.8724 - val_loss: 0.4045 - val_acc: 0.8889\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4172 - acc: 0.8718\n",
      "Epoch 00058: val_loss did not improve from 0.30578\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.4171 - acc: 0.8718 - val_loss: 0.3507 - val_acc: 0.9001\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4125 - acc: 0.8759\n",
      "Epoch 00059: val_loss did not improve from 0.30578\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.4124 - acc: 0.8759 - val_loss: 0.4165 - val_acc: 0.8863\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4093 - acc: 0.8744\n",
      "Epoch 00060: val_loss did not improve from 0.30578\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.4093 - acc: 0.8744 - val_loss: 0.3423 - val_acc: 0.9087\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4106 - acc: 0.8746\n",
      "Epoch 00061: val_loss improved from 0.30578 to 0.29416, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_8_conv_checkpoint/061-0.2942.hdf5\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.4106 - acc: 0.8746 - val_loss: 0.2942 - val_acc: 0.9213\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4026 - acc: 0.8765\n",
      "Epoch 00062: val_loss improved from 0.29416 to 0.27979, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_8_conv_checkpoint/062-0.2798.hdf5\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.4028 - acc: 0.8765 - val_loss: 0.2798 - val_acc: 0.9236\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3996 - acc: 0.8758\n",
      "Epoch 00063: val_loss did not improve from 0.27979\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.3996 - acc: 0.8758 - val_loss: 0.2857 - val_acc: 0.9262\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3950 - acc: 0.8789\n",
      "Epoch 00064: val_loss improved from 0.27979 to 0.27066, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_8_conv_checkpoint/064-0.2707.hdf5\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.3951 - acc: 0.8788 - val_loss: 0.2707 - val_acc: 0.9271\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3949 - acc: 0.8794\n",
      "Epoch 00065: val_loss did not improve from 0.27066\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.3950 - acc: 0.8794 - val_loss: 0.2825 - val_acc: 0.9213\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3883 - acc: 0.8820\n",
      "Epoch 00066: val_loss did not improve from 0.27066\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.3883 - acc: 0.8820 - val_loss: 0.3053 - val_acc: 0.9143\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3826 - acc: 0.8831\n",
      "Epoch 00067: val_loss did not improve from 0.27066\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.3826 - acc: 0.8831 - val_loss: 0.3187 - val_acc: 0.9117\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3818 - acc: 0.8828\n",
      "Epoch 00068: val_loss did not improve from 0.27066\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.3819 - acc: 0.8828 - val_loss: 0.3449 - val_acc: 0.9047\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3786 - acc: 0.8837\n",
      "Epoch 00069: val_loss did not improve from 0.27066\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.3786 - acc: 0.8837 - val_loss: 0.2889 - val_acc: 0.9217\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3769 - acc: 0.8840\n",
      "Epoch 00070: val_loss did not improve from 0.27066\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.3770 - acc: 0.8839 - val_loss: 0.2856 - val_acc: 0.9224\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3739 - acc: 0.8839\n",
      "Epoch 00071: val_loss did not improve from 0.27066\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.3739 - acc: 0.8839 - val_loss: 0.3055 - val_acc: 0.9255\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3652 - acc: 0.8874\n",
      "Epoch 00072: val_loss did not improve from 0.27066\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.3652 - acc: 0.8874 - val_loss: 0.2801 - val_acc: 0.9269\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3624 - acc: 0.8886\n",
      "Epoch 00073: val_loss did not improve from 0.27066\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.3624 - acc: 0.8886 - val_loss: 0.3031 - val_acc: 0.9241\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3645 - acc: 0.8885\n",
      "Epoch 00074: val_loss did not improve from 0.27066\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.3645 - acc: 0.8885 - val_loss: 0.3021 - val_acc: 0.9164\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3558 - acc: 0.8907\n",
      "Epoch 00075: val_loss did not improve from 0.27066\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.3559 - acc: 0.8907 - val_loss: 0.2950 - val_acc: 0.9222\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3553 - acc: 0.8907\n",
      "Epoch 00076: val_loss did not improve from 0.27066\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.3553 - acc: 0.8907 - val_loss: 0.2842 - val_acc: 0.9236\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3507 - acc: 0.8920\n",
      "Epoch 00077: val_loss did not improve from 0.27066\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.3508 - acc: 0.8919 - val_loss: 0.2824 - val_acc: 0.9248\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3483 - acc: 0.8937\n",
      "Epoch 00078: val_loss did not improve from 0.27066\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.3483 - acc: 0.8937 - val_loss: 0.2809 - val_acc: 0.9224\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3455 - acc: 0.8940\n",
      "Epoch 00079: val_loss improved from 0.27066 to 0.24985, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_8_conv_checkpoint/079-0.2498.hdf5\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.3455 - acc: 0.8940 - val_loss: 0.2498 - val_acc: 0.9320\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3417 - acc: 0.8942\n",
      "Epoch 00080: val_loss did not improve from 0.24985\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.3420 - acc: 0.8942 - val_loss: 0.3551 - val_acc: 0.8973\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3453 - acc: 0.8918\n",
      "Epoch 00081: val_loss did not improve from 0.24985\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.3456 - acc: 0.8918 - val_loss: 0.2552 - val_acc: 0.9336\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3471 - acc: 0.8920\n",
      "Epoch 00082: val_loss did not improve from 0.24985\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.3472 - acc: 0.8919 - val_loss: 0.2690 - val_acc: 0.9234\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3334 - acc: 0.8960\n",
      "Epoch 00083: val_loss did not improve from 0.24985\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.3334 - acc: 0.8960 - val_loss: 0.2701 - val_acc: 0.9241\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3317 - acc: 0.8968\n",
      "Epoch 00084: val_loss did not improve from 0.24985\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.3318 - acc: 0.8968 - val_loss: 0.2566 - val_acc: 0.9308\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3381 - acc: 0.8950\n",
      "Epoch 00085: val_loss did not improve from 0.24985\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.3381 - acc: 0.8950 - val_loss: 0.3145 - val_acc: 0.9113\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3212 - acc: 0.9006\n",
      "Epoch 00086: val_loss did not improve from 0.24985\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.3214 - acc: 0.9005 - val_loss: 0.2662 - val_acc: 0.9308\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3349 - acc: 0.8964\n",
      "Epoch 00087: val_loss did not improve from 0.24985\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.3349 - acc: 0.8964 - val_loss: 0.2959 - val_acc: 0.9178\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3259 - acc: 0.8978\n",
      "Epoch 00088: val_loss improved from 0.24985 to 0.24343, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_8_conv_checkpoint/088-0.2434.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.3259 - acc: 0.8978 - val_loss: 0.2434 - val_acc: 0.9320\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3293 - acc: 0.8975\n",
      "Epoch 00089: val_loss did not improve from 0.24343\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.3293 - acc: 0.8975 - val_loss: 0.2697 - val_acc: 0.9315\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3281 - acc: 0.8993\n",
      "Epoch 00090: val_loss did not improve from 0.24343\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.3280 - acc: 0.8994 - val_loss: 0.2789 - val_acc: 0.9194\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3179 - acc: 0.9013\n",
      "Epoch 00091: val_loss did not improve from 0.24343\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.3182 - acc: 0.9012 - val_loss: 0.3102 - val_acc: 0.9180\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3146 - acc: 0.9033\n",
      "Epoch 00092: val_loss did not improve from 0.24343\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.3148 - acc: 0.9033 - val_loss: 0.3315 - val_acc: 0.9096\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3144 - acc: 0.9013\n",
      "Epoch 00093: val_loss did not improve from 0.24343\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.3148 - acc: 0.9013 - val_loss: 0.2679 - val_acc: 0.9271\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3129 - acc: 0.9054\n",
      "Epoch 00094: val_loss did not improve from 0.24343\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.3129 - acc: 0.9054 - val_loss: 0.2816 - val_acc: 0.9245\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3104 - acc: 0.9035\n",
      "Epoch 00095: val_loss did not improve from 0.24343\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.3105 - acc: 0.9034 - val_loss: 0.3075 - val_acc: 0.9129\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3070 - acc: 0.9028\n",
      "Epoch 00096: val_loss did not improve from 0.24343\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.3071 - acc: 0.9028 - val_loss: 0.2835 - val_acc: 0.9222\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3102 - acc: 0.9032\n",
      "Epoch 00097: val_loss did not improve from 0.24343\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.3103 - acc: 0.9032 - val_loss: 0.3066 - val_acc: 0.9203\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3045 - acc: 0.9050\n",
      "Epoch 00098: val_loss did not improve from 0.24343\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.3048 - acc: 0.9049 - val_loss: 0.2657 - val_acc: 0.9271\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3036 - acc: 0.9046\n",
      "Epoch 00099: val_loss improved from 0.24343 to 0.24339, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_8_conv_checkpoint/099-0.2434.hdf5\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.3040 - acc: 0.9046 - val_loss: 0.2434 - val_acc: 0.9343\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3114 - acc: 0.9036\n",
      "Epoch 00100: val_loss did not improve from 0.24339\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.3114 - acc: 0.9036 - val_loss: 0.2567 - val_acc: 0.9262\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2964 - acc: 0.9076\n",
      "Epoch 00101: val_loss did not improve from 0.24339\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2964 - acc: 0.9076 - val_loss: 0.2842 - val_acc: 0.9273\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2951 - acc: 0.9092\n",
      "Epoch 00102: val_loss did not improve from 0.24339\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2951 - acc: 0.9093 - val_loss: 0.2715 - val_acc: 0.9299\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2983 - acc: 0.9070\n",
      "Epoch 00103: val_loss did not improve from 0.24339\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2983 - acc: 0.9070 - val_loss: 0.2623 - val_acc: 0.9245\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2905 - acc: 0.9083\n",
      "Epoch 00104: val_loss did not improve from 0.24339\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2904 - acc: 0.9083 - val_loss: 0.2783 - val_acc: 0.9241\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2887 - acc: 0.9103\n",
      "Epoch 00105: val_loss did not improve from 0.24339\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2888 - acc: 0.9103 - val_loss: 0.3750 - val_acc: 0.8994\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2903 - acc: 0.9094\n",
      "Epoch 00106: val_loss did not improve from 0.24339\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2902 - acc: 0.9094 - val_loss: 0.2446 - val_acc: 0.9317\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2920 - acc: 0.9093\n",
      "Epoch 00107: val_loss did not improve from 0.24339\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2922 - acc: 0.9093 - val_loss: 0.2492 - val_acc: 0.9315\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2895 - acc: 0.9093\n",
      "Epoch 00108: val_loss did not improve from 0.24339\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2894 - acc: 0.9093 - val_loss: 0.2757 - val_acc: 0.9255\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2814 - acc: 0.9124\n",
      "Epoch 00109: val_loss did not improve from 0.24339\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2815 - acc: 0.9124 - val_loss: 0.3264 - val_acc: 0.9068\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2829 - acc: 0.9101\n",
      "Epoch 00110: val_loss did not improve from 0.24339\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2830 - acc: 0.9101 - val_loss: 0.3081 - val_acc: 0.9194\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2842 - acc: 0.9107\n",
      "Epoch 00111: val_loss did not improve from 0.24339\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2843 - acc: 0.9106 - val_loss: 0.2510 - val_acc: 0.9334\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2843 - acc: 0.9110\n",
      "Epoch 00112: val_loss did not improve from 0.24339\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2843 - acc: 0.9110 - val_loss: 0.3154 - val_acc: 0.9101\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2754 - acc: 0.9128\n",
      "Epoch 00113: val_loss did not improve from 0.24339\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2757 - acc: 0.9128 - val_loss: 0.2615 - val_acc: 0.9322\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2784 - acc: 0.9124\n",
      "Epoch 00114: val_loss did not improve from 0.24339\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2785 - acc: 0.9124 - val_loss: 0.2694 - val_acc: 0.9285\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2709 - acc: 0.9136\n",
      "Epoch 00115: val_loss did not improve from 0.24339\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2711 - acc: 0.9135 - val_loss: 0.2631 - val_acc: 0.9322\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2773 - acc: 0.9124\n",
      "Epoch 00116: val_loss did not improve from 0.24339\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2775 - acc: 0.9123 - val_loss: 0.3069 - val_acc: 0.9220\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2777 - acc: 0.9118\n",
      "Epoch 00117: val_loss did not improve from 0.24339\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2778 - acc: 0.9118 - val_loss: 0.2507 - val_acc: 0.9327\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2704 - acc: 0.9146\n",
      "Epoch 00118: val_loss improved from 0.24339 to 0.24307, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_8_conv_checkpoint/118-0.2431.hdf5\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2703 - acc: 0.9146 - val_loss: 0.2431 - val_acc: 0.9385\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2660 - acc: 0.9175\n",
      "Epoch 00119: val_loss did not improve from 0.24307\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2659 - acc: 0.9175 - val_loss: 0.2597 - val_acc: 0.9331\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2681 - acc: 0.9132\n",
      "Epoch 00120: val_loss did not improve from 0.24307\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2681 - acc: 0.9132 - val_loss: 0.2932 - val_acc: 0.9208\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2651 - acc: 0.9160\n",
      "Epoch 00121: val_loss did not improve from 0.24307\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2651 - acc: 0.9160 - val_loss: 0.2537 - val_acc: 0.9331\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2565 - acc: 0.9183\n",
      "Epoch 00122: val_loss did not improve from 0.24307\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2566 - acc: 0.9183 - val_loss: 0.2603 - val_acc: 0.9311\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2675 - acc: 0.9170\n",
      "Epoch 00123: val_loss did not improve from 0.24307\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2676 - acc: 0.9170 - val_loss: 0.2870 - val_acc: 0.9362\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2613 - acc: 0.9156\n",
      "Epoch 00124: val_loss did not improve from 0.24307\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2613 - acc: 0.9156 - val_loss: 0.2733 - val_acc: 0.9271\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2540 - acc: 0.9190\n",
      "Epoch 00125: val_loss did not improve from 0.24307\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2542 - acc: 0.9190 - val_loss: 0.2728 - val_acc: 0.9280\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2628 - acc: 0.9186\n",
      "Epoch 00126: val_loss improved from 0.24307 to 0.24240, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_8_conv_checkpoint/126-0.2424.hdf5\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2628 - acc: 0.9186 - val_loss: 0.2424 - val_acc: 0.9383\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2585 - acc: 0.9173\n",
      "Epoch 00127: val_loss did not improve from 0.24240\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2585 - acc: 0.9172 - val_loss: 0.2548 - val_acc: 0.9324\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2671 - acc: 0.9146\n",
      "Epoch 00128: val_loss did not improve from 0.24240\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2670 - acc: 0.9146 - val_loss: 0.2792 - val_acc: 0.9236\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2531 - acc: 0.9198\n",
      "Epoch 00129: val_loss did not improve from 0.24240\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2530 - acc: 0.9198 - val_loss: 0.2884 - val_acc: 0.9234\n",
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2530 - acc: 0.9216\n",
      "Epoch 00130: val_loss did not improve from 0.24240\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2530 - acc: 0.9216 - val_loss: 0.2913 - val_acc: 0.9306\n",
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2551 - acc: 0.9188\n",
      "Epoch 00131: val_loss did not improve from 0.24240\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2552 - acc: 0.9188 - val_loss: 0.2650 - val_acc: 0.9369\n",
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2479 - acc: 0.9211\n",
      "Epoch 00132: val_loss did not improve from 0.24240\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2479 - acc: 0.9211 - val_loss: 0.2718 - val_acc: 0.9255\n",
      "Epoch 133/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2461 - acc: 0.9230\n",
      "Epoch 00133: val_loss did not improve from 0.24240\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2461 - acc: 0.9230 - val_loss: 0.2583 - val_acc: 0.9311\n",
      "Epoch 134/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2456 - acc: 0.9211\n",
      "Epoch 00134: val_loss did not improve from 0.24240\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2456 - acc: 0.9211 - val_loss: 0.2624 - val_acc: 0.9343\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 135/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2469 - acc: 0.9208\n",
      "Epoch 00135: val_loss did not improve from 0.24240\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2470 - acc: 0.9207 - val_loss: 0.2732 - val_acc: 0.9313\n",
      "Epoch 136/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2488 - acc: 0.9198\n",
      "Epoch 00136: val_loss did not improve from 0.24240\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2488 - acc: 0.9198 - val_loss: 0.2641 - val_acc: 0.9338\n",
      "Epoch 137/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2416 - acc: 0.9209\n",
      "Epoch 00137: val_loss did not improve from 0.24240\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2416 - acc: 0.9209 - val_loss: 0.2535 - val_acc: 0.9373\n",
      "Epoch 138/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2417 - acc: 0.9233\n",
      "Epoch 00138: val_loss did not improve from 0.24240\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2418 - acc: 0.9233 - val_loss: 0.3881 - val_acc: 0.8938\n",
      "Epoch 139/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2490 - acc: 0.9211\n",
      "Epoch 00139: val_loss did not improve from 0.24240\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2491 - acc: 0.9210 - val_loss: 0.3128 - val_acc: 0.9196\n",
      "Epoch 140/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2371 - acc: 0.9249\n",
      "Epoch 00140: val_loss improved from 0.24240 to 0.23472, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_8_conv_checkpoint/140-0.2347.hdf5\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2373 - acc: 0.9248 - val_loss: 0.2347 - val_acc: 0.9408\n",
      "Epoch 141/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2433 - acc: 0.9227\n",
      "Epoch 00141: val_loss did not improve from 0.23472\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2434 - acc: 0.9227 - val_loss: 0.2561 - val_acc: 0.9329\n",
      "Epoch 142/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2386 - acc: 0.9238\n",
      "Epoch 00142: val_loss did not improve from 0.23472\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2388 - acc: 0.9238 - val_loss: 0.2504 - val_acc: 0.9345\n",
      "Epoch 143/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2418 - acc: 0.9220\n",
      "Epoch 00143: val_loss did not improve from 0.23472\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2418 - acc: 0.9220 - val_loss: 0.3988 - val_acc: 0.8889\n",
      "Epoch 144/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2345 - acc: 0.9254\n",
      "Epoch 00144: val_loss did not improve from 0.23472\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2347 - acc: 0.9254 - val_loss: 0.3068 - val_acc: 0.9287\n",
      "Epoch 145/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2432 - acc: 0.9232\n",
      "Epoch 00145: val_loss did not improve from 0.23472\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2432 - acc: 0.9232 - val_loss: 0.2548 - val_acc: 0.9380\n",
      "Epoch 146/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2338 - acc: 0.9266\n",
      "Epoch 00146: val_loss did not improve from 0.23472\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2339 - acc: 0.9266 - val_loss: 0.2566 - val_acc: 0.9348\n",
      "Epoch 147/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2406 - acc: 0.9239\n",
      "Epoch 00147: val_loss did not improve from 0.23472\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2408 - acc: 0.9239 - val_loss: 0.2659 - val_acc: 0.9327\n",
      "Epoch 148/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2307 - acc: 0.9268\n",
      "Epoch 00148: val_loss did not improve from 0.23472\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2309 - acc: 0.9267 - val_loss: 0.2718 - val_acc: 0.9313\n",
      "Epoch 149/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2363 - acc: 0.9241\n",
      "Epoch 00149: val_loss did not improve from 0.23472\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2362 - acc: 0.9241 - val_loss: 0.2564 - val_acc: 0.9359\n",
      "Epoch 150/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2298 - acc: 0.9259\n",
      "Epoch 00150: val_loss did not improve from 0.23472\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2298 - acc: 0.9259 - val_loss: 0.2369 - val_acc: 0.9390\n",
      "Epoch 151/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2207 - acc: 0.9281\n",
      "Epoch 00151: val_loss did not improve from 0.23472\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2210 - acc: 0.9281 - val_loss: 0.2822 - val_acc: 0.9283\n",
      "Epoch 152/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2294 - acc: 0.9259\n",
      "Epoch 00152: val_loss did not improve from 0.23472\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2293 - acc: 0.9259 - val_loss: 0.2648 - val_acc: 0.9322\n",
      "Epoch 153/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2210 - acc: 0.9296\n",
      "Epoch 00153: val_loss did not improve from 0.23472\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2210 - acc: 0.9296 - val_loss: 0.2604 - val_acc: 0.9336\n",
      "Epoch 154/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2204 - acc: 0.9309\n",
      "Epoch 00154: val_loss did not improve from 0.23472\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2205 - acc: 0.9309 - val_loss: 0.2986 - val_acc: 0.9241\n",
      "Epoch 155/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2299 - acc: 0.9277\n",
      "Epoch 00155: val_loss did not improve from 0.23472\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2302 - acc: 0.9276 - val_loss: 0.3260 - val_acc: 0.9262\n",
      "Epoch 156/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2361 - acc: 0.9249\n",
      "Epoch 00156: val_loss did not improve from 0.23472\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2362 - acc: 0.9249 - val_loss: 0.2681 - val_acc: 0.9306\n",
      "Epoch 157/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2241 - acc: 0.9273\n",
      "Epoch 00157: val_loss did not improve from 0.23472\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2242 - acc: 0.9272 - val_loss: 0.2971 - val_acc: 0.9271\n",
      "Epoch 158/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2195 - acc: 0.9290\n",
      "Epoch 00158: val_loss did not improve from 0.23472\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2195 - acc: 0.9290 - val_loss: 0.8779 - val_acc: 0.8032\n",
      "Epoch 159/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2135 - acc: 0.9322\n",
      "Epoch 00159: val_loss did not improve from 0.23472\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2140 - acc: 0.9322 - val_loss: 0.2930 - val_acc: 0.9273\n",
      "Epoch 160/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2290 - acc: 0.9257\n",
      "Epoch 00160: val_loss did not improve from 0.23472\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2291 - acc: 0.9257 - val_loss: 0.2598 - val_acc: 0.9366\n",
      "Epoch 161/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2177 - acc: 0.9299\n",
      "Epoch 00161: val_loss did not improve from 0.23472\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2178 - acc: 0.9298 - val_loss: 0.4500 - val_acc: 0.8870\n",
      "Epoch 162/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2211 - acc: 0.9268\n",
      "Epoch 00162: val_loss did not improve from 0.23472\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2212 - acc: 0.9268 - val_loss: 0.2607 - val_acc: 0.9373\n",
      "Epoch 163/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2138 - acc: 0.9302\n",
      "Epoch 00163: val_loss did not improve from 0.23472\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2138 - acc: 0.9302 - val_loss: 0.2871 - val_acc: 0.9283\n",
      "Epoch 164/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2180 - acc: 0.9290\n",
      "Epoch 00164: val_loss did not improve from 0.23472\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2183 - acc: 0.9289 - val_loss: 0.2673 - val_acc: 0.9299\n",
      "Epoch 165/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2201 - acc: 0.9283\n",
      "Epoch 00165: val_loss did not improve from 0.23472\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2201 - acc: 0.9283 - val_loss: 0.3019 - val_acc: 0.9229\n",
      "Epoch 166/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2092 - acc: 0.9322\n",
      "Epoch 00166: val_loss did not improve from 0.23472\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2092 - acc: 0.9322 - val_loss: 0.2565 - val_acc: 0.9329\n",
      "Epoch 167/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2099 - acc: 0.9312\n",
      "Epoch 00167: val_loss did not improve from 0.23472\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2099 - acc: 0.9312 - val_loss: 0.2688 - val_acc: 0.9362\n",
      "Epoch 168/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2141 - acc: 0.9312\n",
      "Epoch 00168: val_loss did not improve from 0.23472\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2141 - acc: 0.9312 - val_loss: 0.2786 - val_acc: 0.9306\n",
      "Epoch 169/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2084 - acc: 0.9321\n",
      "Epoch 00169: val_loss did not improve from 0.23472\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2084 - acc: 0.9321 - val_loss: 0.3174 - val_acc: 0.9234\n",
      "Epoch 170/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2129 - acc: 0.9310\n",
      "Epoch 00170: val_loss did not improve from 0.23472\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2133 - acc: 0.9309 - val_loss: 0.2569 - val_acc: 0.9320\n",
      "Epoch 171/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2207 - acc: 0.9287\n",
      "Epoch 00171: val_loss did not improve from 0.23472\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2209 - acc: 0.9287 - val_loss: 0.2918 - val_acc: 0.9304\n",
      "Epoch 172/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2141 - acc: 0.9301\n",
      "Epoch 00172: val_loss did not improve from 0.23472\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2148 - acc: 0.9300 - val_loss: 0.2612 - val_acc: 0.9362\n",
      "Epoch 173/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2118 - acc: 0.9321\n",
      "Epoch 00173: val_loss did not improve from 0.23472\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2117 - acc: 0.9321 - val_loss: 0.2642 - val_acc: 0.9336\n",
      "Epoch 174/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2008 - acc: 0.9356\n",
      "Epoch 00174: val_loss did not improve from 0.23472\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2008 - acc: 0.9356 - val_loss: 0.2720 - val_acc: 0.9331\n",
      "Epoch 175/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2047 - acc: 0.9326\n",
      "Epoch 00175: val_loss did not improve from 0.23472\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2047 - acc: 0.9326 - val_loss: 0.2618 - val_acc: 0.9350\n",
      "Epoch 176/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2026 - acc: 0.9348\n",
      "Epoch 00176: val_loss did not improve from 0.23472\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2027 - acc: 0.9348 - val_loss: 0.2861 - val_acc: 0.9262\n",
      "Epoch 177/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2066 - acc: 0.9333\n",
      "Epoch 00177: val_loss did not improve from 0.23472\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2067 - acc: 0.9333 - val_loss: 0.2958 - val_acc: 0.9241\n",
      "Epoch 178/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2053 - acc: 0.9333\n",
      "Epoch 00178: val_loss did not improve from 0.23472\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2054 - acc: 0.9333 - val_loss: 0.2788 - val_acc: 0.9252\n",
      "Epoch 179/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2121 - acc: 0.9309\n",
      "Epoch 00179: val_loss did not improve from 0.23472\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2126 - acc: 0.9308 - val_loss: 0.3127 - val_acc: 0.9173\n",
      "Epoch 180/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2094 - acc: 0.9328\n",
      "Epoch 00180: val_loss did not improve from 0.23472\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2098 - acc: 0.9327 - val_loss: 0.2618 - val_acc: 0.9364\n",
      "Epoch 181/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2056 - acc: 0.9336\n",
      "Epoch 00181: val_loss did not improve from 0.23472\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2060 - acc: 0.9335 - val_loss: 0.2657 - val_acc: 0.9341\n",
      "Epoch 182/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2138 - acc: 0.9328\n",
      "Epoch 00182: val_loss did not improve from 0.23472\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2142 - acc: 0.9327 - val_loss: 0.2968 - val_acc: 0.9217\n",
      "Epoch 183/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2070 - acc: 0.9317\n",
      "Epoch 00183: val_loss did not improve from 0.23472\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2070 - acc: 0.9317 - val_loss: 0.4218 - val_acc: 0.8912\n",
      "Epoch 184/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2020 - acc: 0.9345\n",
      "Epoch 00184: val_loss did not improve from 0.23472\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2019 - acc: 0.9345 - val_loss: 0.3188 - val_acc: 0.9199\n",
      "Epoch 185/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1992 - acc: 0.9364\n",
      "Epoch 00185: val_loss did not improve from 0.23472\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1993 - acc: 0.9364 - val_loss: 0.3078 - val_acc: 0.9234\n",
      "Epoch 186/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1939 - acc: 0.9380\n",
      "Epoch 00186: val_loss did not improve from 0.23472\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1942 - acc: 0.9380 - val_loss: 0.2749 - val_acc: 0.9322\n",
      "Epoch 187/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2057 - acc: 0.9321\n",
      "Epoch 00187: val_loss did not improve from 0.23472\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2058 - acc: 0.9321 - val_loss: 0.2875 - val_acc: 0.9308\n",
      "Epoch 188/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1947 - acc: 0.9365\n",
      "Epoch 00188: val_loss did not improve from 0.23472\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1947 - acc: 0.9365 - val_loss: 0.2737 - val_acc: 0.9329\n",
      "Epoch 189/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1970 - acc: 0.9357\n",
      "Epoch 00189: val_loss did not improve from 0.23472\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1974 - acc: 0.9356 - val_loss: 0.3083 - val_acc: 0.9276\n",
      "Epoch 190/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2041 - acc: 0.9334\n",
      "Epoch 00190: val_loss did not improve from 0.23472\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2041 - acc: 0.9334 - val_loss: 0.2885 - val_acc: 0.9294\n",
      "\n",
      "1D_CNN_custom_4_ch_64_DO_BN_8_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 952us/sample - loss: 0.2910 - acc: 0.9223\n",
      "Loss: 0.29099333845194997 Accuracy: 0.9223261\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 3.0382 - acc: 0.1008\n",
      "Epoch 00001: val_loss improved from inf to 2.55993, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_9_conv_checkpoint/001-2.5599.hdf5\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 3.0381 - acc: 0.1008 - val_loss: 2.5599 - val_acc: 0.1882\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.5121 - acc: 0.1876\n",
      "Epoch 00002: val_loss improved from 2.55993 to 2.10775, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_9_conv_checkpoint/002-2.1078.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.5120 - acc: 0.1876 - val_loss: 2.1078 - val_acc: 0.3692\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.2601 - acc: 0.2574\n",
      "Epoch 00003: val_loss improved from 2.10775 to 1.86174, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_9_conv_checkpoint/003-1.8617.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.2601 - acc: 0.2574 - val_loss: 1.8617 - val_acc: 0.4826\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.0331 - acc: 0.3299\n",
      "Epoch 00004: val_loss improved from 1.86174 to 1.65046, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_9_conv_checkpoint/004-1.6505.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 2.0331 - acc: 0.3299 - val_loss: 1.6505 - val_acc: 0.5267\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.8537 - acc: 0.3872\n",
      "Epoch 00005: val_loss improved from 1.65046 to 1.45590, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_9_conv_checkpoint/005-1.4559.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 1.8538 - acc: 0.3872 - val_loss: 1.4559 - val_acc: 0.5886\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6990 - acc: 0.4392\n",
      "Epoch 00006: val_loss improved from 1.45590 to 1.30994, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_9_conv_checkpoint/006-1.3099.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 1.6992 - acc: 0.4391 - val_loss: 1.3099 - val_acc: 0.6410\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5873 - acc: 0.4792\n",
      "Epoch 00007: val_loss improved from 1.30994 to 1.16750, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_9_conv_checkpoint/007-1.1675.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 1.5874 - acc: 0.4791 - val_loss: 1.1675 - val_acc: 0.6669\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4798 - acc: 0.5134\n",
      "Epoch 00008: val_loss improved from 1.16750 to 1.11449, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_9_conv_checkpoint/008-1.1145.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 1.4798 - acc: 0.5134 - val_loss: 1.1145 - val_acc: 0.7000\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4042 - acc: 0.5395\n",
      "Epoch 00009: val_loss improved from 1.11449 to 1.02737, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_9_conv_checkpoint/009-1.0274.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 1.4042 - acc: 0.5395 - val_loss: 1.0274 - val_acc: 0.7317\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3275 - acc: 0.5624\n",
      "Epoch 00010: val_loss improved from 1.02737 to 0.97946, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_9_conv_checkpoint/010-0.9795.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 1.3274 - acc: 0.5625 - val_loss: 0.9795 - val_acc: 0.7261\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2613 - acc: 0.5842\n",
      "Epoch 00011: val_loss improved from 0.97946 to 0.90732, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_9_conv_checkpoint/011-0.9073.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 1.2614 - acc: 0.5841 - val_loss: 0.9073 - val_acc: 0.7556\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2058 - acc: 0.6079\n",
      "Epoch 00012: val_loss improved from 0.90732 to 0.90206, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_9_conv_checkpoint/012-0.9021.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 1.2058 - acc: 0.6079 - val_loss: 0.9021 - val_acc: 0.7631\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1639 - acc: 0.6221\n",
      "Epoch 00013: val_loss improved from 0.90206 to 0.77803, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_9_conv_checkpoint/013-0.7780.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 1.1639 - acc: 0.6221 - val_loss: 0.7780 - val_acc: 0.7920\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1016 - acc: 0.6426\n",
      "Epoch 00014: val_loss improved from 0.77803 to 0.70919, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_9_conv_checkpoint/014-0.7092.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 1.1015 - acc: 0.6426 - val_loss: 0.7092 - val_acc: 0.8097\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0708 - acc: 0.6502\n",
      "Epoch 00015: val_loss improved from 0.70919 to 0.67683, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_9_conv_checkpoint/015-0.6768.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 1.0710 - acc: 0.6502 - val_loss: 0.6768 - val_acc: 0.8246\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0387 - acc: 0.6627\n",
      "Epoch 00016: val_loss improved from 0.67683 to 0.66175, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_9_conv_checkpoint/016-0.6618.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 1.0386 - acc: 0.6627 - val_loss: 0.6618 - val_acc: 0.8155\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9927 - acc: 0.6811\n",
      "Epoch 00017: val_loss improved from 0.66175 to 0.62857, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_9_conv_checkpoint/017-0.6286.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.9928 - acc: 0.6810 - val_loss: 0.6286 - val_acc: 0.8314\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9656 - acc: 0.6885\n",
      "Epoch 00018: val_loss improved from 0.62857 to 0.61684, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_9_conv_checkpoint/018-0.6168.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.9656 - acc: 0.6885 - val_loss: 0.6168 - val_acc: 0.8383\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9395 - acc: 0.6979\n",
      "Epoch 00019: val_loss improved from 0.61684 to 0.57262, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_9_conv_checkpoint/019-0.5726.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.9397 - acc: 0.6978 - val_loss: 0.5726 - val_acc: 0.8549\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9120 - acc: 0.7067\n",
      "Epoch 00020: val_loss improved from 0.57262 to 0.54349, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_9_conv_checkpoint/020-0.5435.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.9122 - acc: 0.7066 - val_loss: 0.5435 - val_acc: 0.8565\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8827 - acc: 0.7182\n",
      "Epoch 00021: val_loss improved from 0.54349 to 0.53190, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_9_conv_checkpoint/021-0.5319.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.8828 - acc: 0.7182 - val_loss: 0.5319 - val_acc: 0.8491\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8583 - acc: 0.7247\n",
      "Epoch 00022: val_loss improved from 0.53190 to 0.51691, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_9_conv_checkpoint/022-0.5169.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.8583 - acc: 0.7247 - val_loss: 0.5169 - val_acc: 0.8677\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8337 - acc: 0.7349\n",
      "Epoch 00023: val_loss improved from 0.51691 to 0.50968, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_9_conv_checkpoint/023-0.5097.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.8337 - acc: 0.7349 - val_loss: 0.5097 - val_acc: 0.8663\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8175 - acc: 0.7390\n",
      "Epoch 00024: val_loss improved from 0.50968 to 0.47208, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_9_conv_checkpoint/024-0.4721.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.8175 - acc: 0.7390 - val_loss: 0.4721 - val_acc: 0.8775\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7942 - acc: 0.7436\n",
      "Epoch 00025: val_loss improved from 0.47208 to 0.47011, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_9_conv_checkpoint/025-0.4701.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.7942 - acc: 0.7436 - val_loss: 0.4701 - val_acc: 0.8721\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7810 - acc: 0.7500\n",
      "Epoch 00026: val_loss did not improve from 0.47011\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.7811 - acc: 0.7500 - val_loss: 0.5456 - val_acc: 0.8537\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7628 - acc: 0.7596\n",
      "Epoch 00027: val_loss improved from 0.47011 to 0.43949, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_9_conv_checkpoint/027-0.4395.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.7630 - acc: 0.7595 - val_loss: 0.4395 - val_acc: 0.8854\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7465 - acc: 0.7618\n",
      "Epoch 00028: val_loss did not improve from 0.43949\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.7468 - acc: 0.7617 - val_loss: 0.4508 - val_acc: 0.8763\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7297 - acc: 0.7667\n",
      "Epoch 00029: val_loss improved from 0.43949 to 0.40383, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_9_conv_checkpoint/029-0.4038.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.7297 - acc: 0.7667 - val_loss: 0.4038 - val_acc: 0.8917\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7173 - acc: 0.7729\n",
      "Epoch 00030: val_loss did not improve from 0.40383\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.7175 - acc: 0.7729 - val_loss: 0.4258 - val_acc: 0.8833\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6981 - acc: 0.7801\n",
      "Epoch 00031: val_loss improved from 0.40383 to 0.39275, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_9_conv_checkpoint/031-0.3928.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.6981 - acc: 0.7800 - val_loss: 0.3928 - val_acc: 0.8924\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6894 - acc: 0.7824\n",
      "Epoch 00032: val_loss improved from 0.39275 to 0.38188, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_9_conv_checkpoint/032-0.3819.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.6895 - acc: 0.7824 - val_loss: 0.3819 - val_acc: 0.8977\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6760 - acc: 0.7854\n",
      "Epoch 00033: val_loss improved from 0.38188 to 0.36862, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_9_conv_checkpoint/033-0.3686.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.6765 - acc: 0.7854 - val_loss: 0.3686 - val_acc: 0.8984\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6654 - acc: 0.7898\n",
      "Epoch 00034: val_loss did not improve from 0.36862\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.6656 - acc: 0.7898 - val_loss: 0.3759 - val_acc: 0.8903\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6519 - acc: 0.7918\n",
      "Epoch 00035: val_loss improved from 0.36862 to 0.36123, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_9_conv_checkpoint/035-0.3612.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.6519 - acc: 0.7918 - val_loss: 0.3612 - val_acc: 0.8987\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6417 - acc: 0.7962\n",
      "Epoch 00036: val_loss did not improve from 0.36123\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.6421 - acc: 0.7962 - val_loss: 0.3679 - val_acc: 0.8938\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6310 - acc: 0.7998\n",
      "Epoch 00037: val_loss improved from 0.36123 to 0.34976, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_9_conv_checkpoint/037-0.3498.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.6310 - acc: 0.7999 - val_loss: 0.3498 - val_acc: 0.9057\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6170 - acc: 0.8042\n",
      "Epoch 00038: val_loss improved from 0.34976 to 0.33057, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_9_conv_checkpoint/038-0.3306.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.6171 - acc: 0.8042 - val_loss: 0.3306 - val_acc: 0.9131\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6096 - acc: 0.8043\n",
      "Epoch 00039: val_loss did not improve from 0.33057\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.6095 - acc: 0.8043 - val_loss: 0.3413 - val_acc: 0.9012\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5968 - acc: 0.8088\n",
      "Epoch 00040: val_loss did not improve from 0.33057\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.5968 - acc: 0.8088 - val_loss: 0.3543 - val_acc: 0.8973\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5957 - acc: 0.8109\n",
      "Epoch 00041: val_loss improved from 0.33057 to 0.31823, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_9_conv_checkpoint/041-0.3182.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.5958 - acc: 0.8109 - val_loss: 0.3182 - val_acc: 0.9124\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5787 - acc: 0.8152\n",
      "Epoch 00042: val_loss did not improve from 0.31823\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.5789 - acc: 0.8152 - val_loss: 0.3312 - val_acc: 0.9064\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5825 - acc: 0.8149\n",
      "Epoch 00043: val_loss improved from 0.31823 to 0.30318, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_9_conv_checkpoint/043-0.3032.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.5826 - acc: 0.8149 - val_loss: 0.3032 - val_acc: 0.9154\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5677 - acc: 0.8209\n",
      "Epoch 00044: val_loss improved from 0.30318 to 0.29700, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_9_conv_checkpoint/044-0.2970.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.5677 - acc: 0.8209 - val_loss: 0.2970 - val_acc: 0.9164\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5537 - acc: 0.8223\n",
      "Epoch 00045: val_loss did not improve from 0.29700\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.5541 - acc: 0.8222 - val_loss: 0.3690 - val_acc: 0.9026\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5635 - acc: 0.8210\n",
      "Epoch 00046: val_loss did not improve from 0.29700\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.5637 - acc: 0.8209 - val_loss: 0.2987 - val_acc: 0.9101\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5417 - acc: 0.8280\n",
      "Epoch 00047: val_loss did not improve from 0.29700\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.5419 - acc: 0.8280 - val_loss: 0.2986 - val_acc: 0.9159\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5333 - acc: 0.8317\n",
      "Epoch 00048: val_loss improved from 0.29700 to 0.29132, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_9_conv_checkpoint/048-0.2913.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.5335 - acc: 0.8317 - val_loss: 0.2913 - val_acc: 0.9161\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5290 - acc: 0.8306\n",
      "Epoch 00049: val_loss did not improve from 0.29132\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.5290 - acc: 0.8306 - val_loss: 0.2979 - val_acc: 0.9182\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5264 - acc: 0.8320\n",
      "Epoch 00050: val_loss did not improve from 0.29132\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.5267 - acc: 0.8319 - val_loss: 0.3042 - val_acc: 0.9154\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5183 - acc: 0.8358\n",
      "Epoch 00051: val_loss did not improve from 0.29132\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.5183 - acc: 0.8358 - val_loss: 0.3035 - val_acc: 0.9161\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5087 - acc: 0.8386\n",
      "Epoch 00052: val_loss improved from 0.29132 to 0.27855, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_9_conv_checkpoint/052-0.2785.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.5087 - acc: 0.8386 - val_loss: 0.2785 - val_acc: 0.9236\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5083 - acc: 0.8383\n",
      "Epoch 00053: val_loss improved from 0.27855 to 0.27543, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_9_conv_checkpoint/053-0.2754.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.5083 - acc: 0.8383 - val_loss: 0.2754 - val_acc: 0.9271\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5099 - acc: 0.8395\n",
      "Epoch 00054: val_loss did not improve from 0.27543\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.5100 - acc: 0.8394 - val_loss: 0.2937 - val_acc: 0.9194\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4865 - acc: 0.8442\n",
      "Epoch 00055: val_loss improved from 0.27543 to 0.27223, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_9_conv_checkpoint/055-0.2722.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.4867 - acc: 0.8442 - val_loss: 0.2722 - val_acc: 0.9248\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4842 - acc: 0.8435\n",
      "Epoch 00056: val_loss did not improve from 0.27223\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.4842 - acc: 0.8435 - val_loss: 0.2822 - val_acc: 0.9245\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4876 - acc: 0.8430\n",
      "Epoch 00057: val_loss did not improve from 0.27223\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.4878 - acc: 0.8430 - val_loss: 0.2797 - val_acc: 0.9208\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4772 - acc: 0.8483\n",
      "Epoch 00058: val_loss did not improve from 0.27223\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.4772 - acc: 0.8483 - val_loss: 0.2749 - val_acc: 0.9227\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4763 - acc: 0.8497\n",
      "Epoch 00059: val_loss improved from 0.27223 to 0.27105, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_9_conv_checkpoint/059-0.2711.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.4764 - acc: 0.8497 - val_loss: 0.2711 - val_acc: 0.9206\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4754 - acc: 0.8472\n",
      "Epoch 00060: val_loss did not improve from 0.27105\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.4754 - acc: 0.8472 - val_loss: 0.2748 - val_acc: 0.9201\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4632 - acc: 0.8524\n",
      "Epoch 00061: val_loss improved from 0.27105 to 0.26997, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_9_conv_checkpoint/061-0.2700.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.4634 - acc: 0.8524 - val_loss: 0.2700 - val_acc: 0.9238\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4619 - acc: 0.8535\n",
      "Epoch 00062: val_loss did not improve from 0.26997\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.4619 - acc: 0.8535 - val_loss: 0.2959 - val_acc: 0.9136\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4583 - acc: 0.8525\n",
      "Epoch 00063: val_loss did not improve from 0.26997\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.4586 - acc: 0.8525 - val_loss: 0.2935 - val_acc: 0.9140\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4648 - acc: 0.8509\n",
      "Epoch 00064: val_loss improved from 0.26997 to 0.26549, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_9_conv_checkpoint/064-0.2655.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.4648 - acc: 0.8509 - val_loss: 0.2655 - val_acc: 0.9224\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4405 - acc: 0.8589\n",
      "Epoch 00065: val_loss did not improve from 0.26549\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.4407 - acc: 0.8589 - val_loss: 0.2678 - val_acc: 0.9241\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4435 - acc: 0.8594\n",
      "Epoch 00066: val_loss improved from 0.26549 to 0.25401, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_9_conv_checkpoint/066-0.2540.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.4437 - acc: 0.8594 - val_loss: 0.2540 - val_acc: 0.9299\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4419 - acc: 0.8576\n",
      "Epoch 00067: val_loss did not improve from 0.25401\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.4419 - acc: 0.8576 - val_loss: 0.2613 - val_acc: 0.9287\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4322 - acc: 0.8592\n",
      "Epoch 00068: val_loss did not improve from 0.25401\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.4324 - acc: 0.8591 - val_loss: 0.2992 - val_acc: 0.9136\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4370 - acc: 0.8576\n",
      "Epoch 00069: val_loss did not improve from 0.25401\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.4371 - acc: 0.8576 - val_loss: 0.2700 - val_acc: 0.9283\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4264 - acc: 0.8623\n",
      "Epoch 00070: val_loss did not improve from 0.25401\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.4263 - acc: 0.8623 - val_loss: 0.2558 - val_acc: 0.9297\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4205 - acc: 0.8647\n",
      "Epoch 00071: val_loss improved from 0.25401 to 0.24666, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_9_conv_checkpoint/071-0.2467.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.4207 - acc: 0.8647 - val_loss: 0.2467 - val_acc: 0.9313\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4158 - acc: 0.8653\n",
      "Epoch 00072: val_loss did not improve from 0.24666\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.4158 - acc: 0.8653 - val_loss: 0.2591 - val_acc: 0.9280\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4127 - acc: 0.8666\n",
      "Epoch 00073: val_loss did not improve from 0.24666\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.4127 - acc: 0.8666 - val_loss: 0.2629 - val_acc: 0.9320\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4143 - acc: 0.8664\n",
      "Epoch 00074: val_loss did not improve from 0.24666\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.4143 - acc: 0.8663 - val_loss: 0.2521 - val_acc: 0.9280\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4088 - acc: 0.8685\n",
      "Epoch 00075: val_loss did not improve from 0.24666\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.4092 - acc: 0.8684 - val_loss: 0.2513 - val_acc: 0.9308\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4058 - acc: 0.8711\n",
      "Epoch 00076: val_loss did not improve from 0.24666\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.4059 - acc: 0.8711 - val_loss: 0.2641 - val_acc: 0.9273\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4017 - acc: 0.8711\n",
      "Epoch 00077: val_loss did not improve from 0.24666\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.4018 - acc: 0.8711 - val_loss: 0.2560 - val_acc: 0.9276\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3967 - acc: 0.8705\n",
      "Epoch 00078: val_loss did not improve from 0.24666\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.3968 - acc: 0.8705 - val_loss: 0.2650 - val_acc: 0.9269\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3946 - acc: 0.8722\n",
      "Epoch 00079: val_loss did not improve from 0.24666\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.3946 - acc: 0.8722 - val_loss: 0.2527 - val_acc: 0.9278\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3954 - acc: 0.8720\n",
      "Epoch 00080: val_loss did not improve from 0.24666\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.3956 - acc: 0.8719 - val_loss: 0.2629 - val_acc: 0.9341\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3927 - acc: 0.8727\n",
      "Epoch 00081: val_loss did not improve from 0.24666\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.3926 - acc: 0.8727 - val_loss: 0.2489 - val_acc: 0.9297\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3867 - acc: 0.8754\n",
      "Epoch 00082: val_loss did not improve from 0.24666\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.3871 - acc: 0.8754 - val_loss: 0.2593 - val_acc: 0.9331\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3872 - acc: 0.8744\n",
      "Epoch 00083: val_loss did not improve from 0.24666\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.3873 - acc: 0.8744 - val_loss: 0.2899 - val_acc: 0.9236\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3854 - acc: 0.8736\n",
      "Epoch 00084: val_loss did not improve from 0.24666\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.3854 - acc: 0.8735 - val_loss: 0.2554 - val_acc: 0.9301\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3874 - acc: 0.8748\n",
      "Epoch 00085: val_loss did not improve from 0.24666\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.3877 - acc: 0.8747 - val_loss: 0.2520 - val_acc: 0.9278\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3842 - acc: 0.8730\n",
      "Epoch 00086: val_loss did not improve from 0.24666\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.3842 - acc: 0.8730 - val_loss: 0.2615 - val_acc: 0.9285\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3716 - acc: 0.8795\n",
      "Epoch 00087: val_loss did not improve from 0.24666\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.3716 - acc: 0.8795 - val_loss: 0.2684 - val_acc: 0.9294\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3689 - acc: 0.8797\n",
      "Epoch 00088: val_loss did not improve from 0.24666\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.3689 - acc: 0.8797 - val_loss: 0.2486 - val_acc: 0.9329\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3690 - acc: 0.8792\n",
      "Epoch 00089: val_loss did not improve from 0.24666\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.3690 - acc: 0.8792 - val_loss: 0.2708 - val_acc: 0.9266\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3687 - acc: 0.8792\n",
      "Epoch 00090: val_loss did not improve from 0.24666\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.3688 - acc: 0.8792 - val_loss: 0.2748 - val_acc: 0.9266\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3581 - acc: 0.8820\n",
      "Epoch 00091: val_loss did not improve from 0.24666\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.3580 - acc: 0.8820 - val_loss: 0.2731 - val_acc: 0.9271\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3598 - acc: 0.8819\n",
      "Epoch 00092: val_loss did not improve from 0.24666\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.3604 - acc: 0.8819 - val_loss: 0.2764 - val_acc: 0.9271\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3694 - acc: 0.8808\n",
      "Epoch 00093: val_loss did not improve from 0.24666\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.3695 - acc: 0.8808 - val_loss: 0.2478 - val_acc: 0.9345\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3547 - acc: 0.8837\n",
      "Epoch 00094: val_loss did not improve from 0.24666\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.3548 - acc: 0.8837 - val_loss: 0.2471 - val_acc: 0.9366\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3537 - acc: 0.8822\n",
      "Epoch 00095: val_loss did not improve from 0.24666\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.3539 - acc: 0.8822 - val_loss: 0.2542 - val_acc: 0.9327\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3508 - acc: 0.8857\n",
      "Epoch 00096: val_loss did not improve from 0.24666\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.3509 - acc: 0.8856 - val_loss: 0.2516 - val_acc: 0.9343\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3535 - acc: 0.8825\n",
      "Epoch 00097: val_loss did not improve from 0.24666\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.3536 - acc: 0.8824 - val_loss: 0.2560 - val_acc: 0.9257\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3553 - acc: 0.8848\n",
      "Epoch 00098: val_loss did not improve from 0.24666\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.3553 - acc: 0.8848 - val_loss: 0.2526 - val_acc: 0.9315\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3524 - acc: 0.8830\n",
      "Epoch 00099: val_loss did not improve from 0.24666\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.3525 - acc: 0.8830 - val_loss: 0.2716 - val_acc: 0.9217\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3453 - acc: 0.8881\n",
      "Epoch 00100: val_loss improved from 0.24666 to 0.24622, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_9_conv_checkpoint/100-0.2462.hdf5\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.3458 - acc: 0.8880 - val_loss: 0.2462 - val_acc: 0.9343\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3509 - acc: 0.8840\n",
      "Epoch 00101: val_loss did not improve from 0.24622\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.3512 - acc: 0.8840 - val_loss: 0.2553 - val_acc: 0.9329\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3540 - acc: 0.8829\n",
      "Epoch 00102: val_loss did not improve from 0.24622\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.3539 - acc: 0.8829 - val_loss: 0.2560 - val_acc: 0.9287\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3353 - acc: 0.8884\n",
      "Epoch 00103: val_loss did not improve from 0.24622\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.3353 - acc: 0.8884 - val_loss: 0.2627 - val_acc: 0.9290\n",
      "Epoch 104/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3300 - acc: 0.8902\n",
      "Epoch 00104: val_loss did not improve from 0.24622\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.3314 - acc: 0.8901 - val_loss: 0.2596 - val_acc: 0.9329\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3484 - acc: 0.8846\n",
      "Epoch 00105: val_loss did not improve from 0.24622\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.3486 - acc: 0.8846 - val_loss: 0.2633 - val_acc: 0.9304\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3361 - acc: 0.8899\n",
      "Epoch 00106: val_loss did not improve from 0.24622\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.3361 - acc: 0.8899 - val_loss: 0.2515 - val_acc: 0.9350\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3262 - acc: 0.8927\n",
      "Epoch 00107: val_loss did not improve from 0.24622\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.3263 - acc: 0.8927 - val_loss: 0.2519 - val_acc: 0.9348\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3287 - acc: 0.8922\n",
      "Epoch 00108: val_loss did not improve from 0.24622\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.3287 - acc: 0.8921 - val_loss: 0.2508 - val_acc: 0.9336\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3275 - acc: 0.8916\n",
      "Epoch 00109: val_loss did not improve from 0.24622\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.3275 - acc: 0.8916 - val_loss: 0.2570 - val_acc: 0.9308\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3221 - acc: 0.8933\n",
      "Epoch 00110: val_loss did not improve from 0.24622\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.3223 - acc: 0.8933 - val_loss: 0.3032 - val_acc: 0.9213\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3310 - acc: 0.8907\n",
      "Epoch 00111: val_loss improved from 0.24622 to 0.23419, saving model to model/checkpoint/1D_CNN_custom_4_ch_64_DO_BN_9_conv_checkpoint/111-0.2342.hdf5\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.3312 - acc: 0.8906 - val_loss: 0.2342 - val_acc: 0.9371\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3291 - acc: 0.8899\n",
      "Epoch 00112: val_loss did not improve from 0.23419\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.3293 - acc: 0.8898 - val_loss: 0.2461 - val_acc: 0.9359\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3194 - acc: 0.8942\n",
      "Epoch 00113: val_loss did not improve from 0.23419\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.3194 - acc: 0.8942 - val_loss: 0.2779 - val_acc: 0.9248\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3165 - acc: 0.8964\n",
      "Epoch 00114: val_loss did not improve from 0.23419\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.3167 - acc: 0.8963 - val_loss: 0.2690 - val_acc: 0.9301\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3172 - acc: 0.8953\n",
      "Epoch 00115: val_loss did not improve from 0.23419\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.3173 - acc: 0.8953 - val_loss: 0.2443 - val_acc: 0.9331\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3142 - acc: 0.8958\n",
      "Epoch 00116: val_loss did not improve from 0.23419\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.3143 - acc: 0.8957 - val_loss: 0.2626 - val_acc: 0.9273\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3114 - acc: 0.8966\n",
      "Epoch 00117: val_loss did not improve from 0.23419\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.3116 - acc: 0.8965 - val_loss: 0.2422 - val_acc: 0.9359\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3144 - acc: 0.8969\n",
      "Epoch 00118: val_loss did not improve from 0.23419\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.3144 - acc: 0.8969 - val_loss: 0.2446 - val_acc: 0.9371\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3060 - acc: 0.8970\n",
      "Epoch 00119: val_loss did not improve from 0.23419\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.3064 - acc: 0.8970 - val_loss: 0.2415 - val_acc: 0.9385\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3053 - acc: 0.8973\n",
      "Epoch 00120: val_loss did not improve from 0.23419\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.3055 - acc: 0.8973 - val_loss: 0.2721 - val_acc: 0.9255\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3097 - acc: 0.8971\n",
      "Epoch 00121: val_loss did not improve from 0.23419\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.3096 - acc: 0.8971 - val_loss: 0.2619 - val_acc: 0.9299\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3049 - acc: 0.8981\n",
      "Epoch 00122: val_loss did not improve from 0.23419\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.3050 - acc: 0.8981 - val_loss: 0.2813 - val_acc: 0.9280\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3024 - acc: 0.8984\n",
      "Epoch 00123: val_loss did not improve from 0.23419\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.3024 - acc: 0.8984 - val_loss: 0.2442 - val_acc: 0.9350\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2997 - acc: 0.9001\n",
      "Epoch 00124: val_loss did not improve from 0.23419\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2997 - acc: 0.9000 - val_loss: 0.2828 - val_acc: 0.9250\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2971 - acc: 0.9032\n",
      "Epoch 00125: val_loss did not improve from 0.23419\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2972 - acc: 0.9032 - val_loss: 0.2622 - val_acc: 0.9315\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3043 - acc: 0.9004\n",
      "Epoch 00126: val_loss did not improve from 0.23419\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.3043 - acc: 0.9004 - val_loss: 0.2569 - val_acc: 0.9366\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2953 - acc: 0.9010\n",
      "Epoch 00127: val_loss did not improve from 0.23419\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2958 - acc: 0.9009 - val_loss: 0.2675 - val_acc: 0.9311\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2975 - acc: 0.9026\n",
      "Epoch 00128: val_loss did not improve from 0.23419\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2976 - acc: 0.9026 - val_loss: 0.2495 - val_acc: 0.9317\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3004 - acc: 0.9002\n",
      "Epoch 00129: val_loss did not improve from 0.23419\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.3004 - acc: 0.9001 - val_loss: 0.2517 - val_acc: 0.9327\n",
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2967 - acc: 0.8990\n",
      "Epoch 00130: val_loss did not improve from 0.23419\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2969 - acc: 0.8990 - val_loss: 0.2609 - val_acc: 0.9352\n",
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2976 - acc: 0.9008\n",
      "Epoch 00131: val_loss did not improve from 0.23419\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2981 - acc: 0.9008 - val_loss: 0.2930 - val_acc: 0.9220\n",
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2947 - acc: 0.9018\n",
      "Epoch 00132: val_loss did not improve from 0.23419\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2947 - acc: 0.9018 - val_loss: 0.2739 - val_acc: 0.9313\n",
      "Epoch 133/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2856 - acc: 0.9045\n",
      "Epoch 00133: val_loss did not improve from 0.23419\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2855 - acc: 0.9045 - val_loss: 0.2623 - val_acc: 0.9338\n",
      "Epoch 134/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2911 - acc: 0.9014\n",
      "Epoch 00134: val_loss did not improve from 0.23419\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2910 - acc: 0.9014 - val_loss: 0.2575 - val_acc: 0.9350\n",
      "Epoch 135/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2810 - acc: 0.9054\n",
      "Epoch 00135: val_loss did not improve from 0.23419\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2812 - acc: 0.9054 - val_loss: 0.2673 - val_acc: 0.9371\n",
      "Epoch 136/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2927 - acc: 0.9040\n",
      "Epoch 00136: val_loss did not improve from 0.23419\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2931 - acc: 0.9040 - val_loss: 0.2699 - val_acc: 0.9317\n",
      "Epoch 137/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2916 - acc: 0.9033\n",
      "Epoch 00137: val_loss did not improve from 0.23419\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2919 - acc: 0.9033 - val_loss: 0.2529 - val_acc: 0.9350\n",
      "Epoch 138/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2800 - acc: 0.9076\n",
      "Epoch 00138: val_loss did not improve from 0.23419\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2801 - acc: 0.9075 - val_loss: 0.2731 - val_acc: 0.9292\n",
      "Epoch 139/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2776 - acc: 0.9068\n",
      "Epoch 00139: val_loss did not improve from 0.23419\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2776 - acc: 0.9068 - val_loss: 0.2773 - val_acc: 0.9320\n",
      "Epoch 140/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2919 - acc: 0.9018\n",
      "Epoch 00140: val_loss did not improve from 0.23419\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2918 - acc: 0.9018 - val_loss: 0.2663 - val_acc: 0.9334\n",
      "Epoch 141/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2736 - acc: 0.9101\n",
      "Epoch 00141: val_loss did not improve from 0.23419\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2737 - acc: 0.9101 - val_loss: 0.2531 - val_acc: 0.9369\n",
      "Epoch 142/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2782 - acc: 0.9078\n",
      "Epoch 00142: val_loss did not improve from 0.23419\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2785 - acc: 0.9078 - val_loss: 0.2789 - val_acc: 0.9317\n",
      "Epoch 143/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2806 - acc: 0.9072\n",
      "Epoch 00143: val_loss did not improve from 0.23419\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2810 - acc: 0.9071 - val_loss: 0.2662 - val_acc: 0.9320\n",
      "Epoch 144/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2901 - acc: 0.9030\n",
      "Epoch 00144: val_loss did not improve from 0.23419\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2904 - acc: 0.9029 - val_loss: 0.2758 - val_acc: 0.9285\n",
      "Epoch 145/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2879 - acc: 0.9042\n",
      "Epoch 00145: val_loss did not improve from 0.23419\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2880 - acc: 0.9041 - val_loss: 0.2595 - val_acc: 0.9338\n",
      "Epoch 146/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2715 - acc: 0.9102\n",
      "Epoch 00146: val_loss did not improve from 0.23419\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2717 - acc: 0.9102 - val_loss: 0.2458 - val_acc: 0.9392\n",
      "Epoch 147/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2707 - acc: 0.9089\n",
      "Epoch 00147: val_loss did not improve from 0.23419\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2707 - acc: 0.9089 - val_loss: 0.2539 - val_acc: 0.9329\n",
      "Epoch 148/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2654 - acc: 0.9098\n",
      "Epoch 00148: val_loss did not improve from 0.23419\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2655 - acc: 0.9098 - val_loss: 0.2500 - val_acc: 0.9385\n",
      "Epoch 149/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2692 - acc: 0.9076\n",
      "Epoch 00149: val_loss did not improve from 0.23419\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2696 - acc: 0.9076 - val_loss: 0.2594 - val_acc: 0.9329\n",
      "Epoch 150/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2863 - acc: 0.9059\n",
      "Epoch 00150: val_loss did not improve from 0.23419\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2866 - acc: 0.9059 - val_loss: 0.2566 - val_acc: 0.9348\n",
      "Epoch 151/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2634 - acc: 0.9104\n",
      "Epoch 00151: val_loss did not improve from 0.23419\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2634 - acc: 0.9104 - val_loss: 0.2721 - val_acc: 0.9334\n",
      "Epoch 152/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2651 - acc: 0.9102\n",
      "Epoch 00152: val_loss did not improve from 0.23419\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2652 - acc: 0.9101 - val_loss: 0.2705 - val_acc: 0.9331\n",
      "Epoch 153/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2718 - acc: 0.9094\n",
      "Epoch 00153: val_loss did not improve from 0.23419\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2719 - acc: 0.9094 - val_loss: 0.2840 - val_acc: 0.9324\n",
      "Epoch 154/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2789 - acc: 0.9074\n",
      "Epoch 00154: val_loss did not improve from 0.23419\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2790 - acc: 0.9074 - val_loss: 0.2642 - val_acc: 0.9345\n",
      "Epoch 155/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2589 - acc: 0.9130\n",
      "Epoch 00155: val_loss did not improve from 0.23419\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2590 - acc: 0.9129 - val_loss: 0.2645 - val_acc: 0.9336\n",
      "Epoch 156/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2760 - acc: 0.9083\n",
      "Epoch 00156: val_loss did not improve from 0.23419\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2759 - acc: 0.9083 - val_loss: 0.2461 - val_acc: 0.9380\n",
      "Epoch 157/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2576 - acc: 0.9115\n",
      "Epoch 00157: val_loss did not improve from 0.23419\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2576 - acc: 0.9115 - val_loss: 0.2544 - val_acc: 0.9343\n",
      "Epoch 158/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2661 - acc: 0.9104\n",
      "Epoch 00158: val_loss did not improve from 0.23419\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2662 - acc: 0.9104 - val_loss: 0.3038 - val_acc: 0.9287\n",
      "Epoch 159/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2649 - acc: 0.9101\n",
      "Epoch 00159: val_loss did not improve from 0.23419\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2656 - acc: 0.9100 - val_loss: 0.2921 - val_acc: 0.9292\n",
      "Epoch 160/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2703 - acc: 0.9093\n",
      "Epoch 00160: val_loss did not improve from 0.23419\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2706 - acc: 0.9093 - val_loss: 0.2613 - val_acc: 0.9378\n",
      "Epoch 161/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2632 - acc: 0.9116\n",
      "Epoch 00161: val_loss did not improve from 0.23419\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2644 - acc: 0.9115 - val_loss: 0.2742 - val_acc: 0.9324\n",
      "\n",
      "1D_CNN_custom_4_ch_64_DO_BN_9_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 984us/sample - loss: 0.3284 - acc: 0.9094\n",
      "Loss: 0.32841336481420175 Accuracy: 0.90944964\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(4, 10):\n",
    "    base = '1D_CNN_custom_4_ch_64_DO_BN'\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    model = build_1d_cnn_custom_DO_BN(conv_num=i)\n",
    "#         model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4),\n",
    "          metrics=['accuracy'])\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model_filename = model_path+'{epoch:03d}-{val_loss:.4f}.hdf5'\n",
    "    checkpointer = ModelCheckpoint(filepath = model_filename, monitor = \"val_loss\", \n",
    "                                   verbose=1, save_best_only=True)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=50)\n",
    "    hist = model.fit(x_train_abs, y_train_onehot, batch_size=64, epochs=500, \n",
    "                     validation_data=[x_val_abs, y_val_onehot], shuffle=True, \n",
    "                     callbacks = [checkpointer, early_stopping])\n",
    "\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "    ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "    ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "    ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    png_path = 'visualization/learning_curve/'\n",
    "    filename = model_name+'.png'\n",
    "    os.makedirs(png_path, exist_ok=True)\n",
    "    fig.savefig(png_path+filename, transparent=True)\n",
    "\n",
    "    model.save(model_path+'000_last.hdf5')\n",
    "    del(model)\n",
    "    \n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "    model = load_model(model_filename)\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "    print()\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_4_ch_64_DO_BN_4_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_45 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_45 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_45 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_46 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_46 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_46 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_47 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_47 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_47 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_48 (Conv1D)           (None, 1777, 32)          10272     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_48 (B (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_48 (Activation)   (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 18944)             0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 18944)             0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 16)                303120    \n",
      "=================================================================\n",
      "Total params: 355,760\n",
      "Trainable params: 355,312\n",
      "Non-trainable params: 448\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 5s 965us/sample - loss: 0.9074 - acc: 0.7277\n",
      "Loss: 0.9074451053130169 Accuracy: 0.72772586\n",
      "\n",
      "1D_CNN_custom_4_ch_64_DO_BN_5_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_49 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_49 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_49 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_50 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_50 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_50 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_51 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_51 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_51 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_52 (Conv1D)           (None, 1777, 32)          10272     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_52 (B (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_52 (Activation)   (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_53 (Conv1D)           (None, 592, 32)           5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_53 (B (None, 592, 32)           128       \n",
      "_________________________________________________________________\n",
      "activation_53 (Activation)   (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling (None, 197, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 6304)              0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 6304)              0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 16)                100880    \n",
      "=================================================================\n",
      "Total params: 158,800\n",
      "Trainable params: 158,288\n",
      "Non-trainable params: 512\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 5s 989us/sample - loss: 0.6616 - acc: 0.8075\n",
      "Loss: 0.6616365684403314 Accuracy: 0.80747664\n",
      "\n",
      "1D_CNN_custom_4_ch_64_DO_BN_6_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_54 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_54 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_54 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_55 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_55 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_55 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_56 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_56 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_56 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_57 (Conv1D)           (None, 1777, 32)          10272     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_57 (B (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_57 (Activation)   (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_58 (Conv1D)           (None, 592, 32)           5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_58 (B (None, 592, 32)           128       \n",
      "_________________________________________________________________\n",
      "activation_58 (Activation)   (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling (None, 197, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_59 (Conv1D)           (None, 197, 32)           5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_59 (B (None, 197, 32)           128       \n",
      "_________________________________________________________________\n",
      "activation_59 (Activation)   (None, 197, 32)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling (None, 65, 32)            0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 2080)              0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 2080)              0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 16)                33296     \n",
      "=================================================================\n",
      "Total params: 96,496\n",
      "Trainable params: 95,920\n",
      "Non-trainable params: 576\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 0.4401 - acc: 0.8766\n",
      "Loss: 0.44006399147730874 Accuracy: 0.8766355\n",
      "\n",
      "1D_CNN_custom_4_ch_64_DO_BN_7_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_60 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_60 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_60 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_61 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_61 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_61 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_62 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_62 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_62 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_63 (Conv1D)           (None, 1777, 32)          10272     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_63 (B (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_63 (Activation)   (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_64 (Conv1D)           (None, 592, 32)           5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_64 (B (None, 592, 32)           128       \n",
      "_________________________________________________________________\n",
      "activation_64 (Activation)   (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling (None, 197, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_65 (Conv1D)           (None, 197, 32)           5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_65 (B (None, 197, 32)           128       \n",
      "_________________________________________________________________\n",
      "activation_65 (Activation)   (None, 197, 32)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling (None, 65, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_66 (Conv1D)           (None, 65, 16)            2576      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_66 (B (None, 65, 16)            64        \n",
      "_________________________________________________________________\n",
      "activation_66 (Activation)   (None, 65, 16)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling (None, 21, 16)            0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 336)               0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 336)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 16)                5392      \n",
      "=================================================================\n",
      "Total params: 71,232\n",
      "Trainable params: 70,624\n",
      "Non-trainable params: 608\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 0.3134 - acc: 0.9128\n",
      "Loss: 0.3134123099741535 Accuracy: 0.9127726\n",
      "\n",
      "1D_CNN_custom_4_ch_64_DO_BN_8_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_67 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_67 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_67 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_68 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_68 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_68 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_69 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_69 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_69 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_70 (Conv1D)           (None, 1777, 32)          10272     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_70 (B (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_70 (Activation)   (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_71 (Conv1D)           (None, 592, 32)           5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_71 (B (None, 592, 32)           128       \n",
      "_________________________________________________________________\n",
      "activation_71 (Activation)   (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling (None, 197, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_72 (Conv1D)           (None, 197, 32)           5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_72 (B (None, 197, 32)           128       \n",
      "_________________________________________________________________\n",
      "activation_72 (Activation)   (None, 197, 32)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling (None, 65, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_73 (Conv1D)           (None, 65, 16)            2576      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_73 (B (None, 65, 16)            64        \n",
      "_________________________________________________________________\n",
      "activation_73 (Activation)   (None, 65, 16)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling (None, 21, 16)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_74 (Conv1D)           (None, 21, 16)            1296      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_74 (B (None, 21, 16)            64        \n",
      "_________________________________________________________________\n",
      "activation_74 (Activation)   (None, 21, 16)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling (None, 7, 16)             0         \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 112)               0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 112)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 16)                1808      \n",
      "=================================================================\n",
      "Total params: 69,008\n",
      "Trainable params: 68,368\n",
      "Non-trainable params: 640\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 0.2910 - acc: 0.9223\n",
      "Loss: 0.29099333845194997 Accuracy: 0.9223261\n",
      "\n",
      "1D_CNN_custom_4_ch_64_DO_BN_9_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_75 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_75 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_75 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_76 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_76 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_76 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_77 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_77 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_77 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_78 (Conv1D)           (None, 1777, 32)          10272     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_78 (B (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_78 (Activation)   (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_79 (Conv1D)           (None, 592, 32)           5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_79 (B (None, 592, 32)           128       \n",
      "_________________________________________________________________\n",
      "activation_79 (Activation)   (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling (None, 197, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_80 (Conv1D)           (None, 197, 32)           5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_80 (B (None, 197, 32)           128       \n",
      "_________________________________________________________________\n",
      "activation_80 (Activation)   (None, 197, 32)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling (None, 65, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_81 (Conv1D)           (None, 65, 16)            2576      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_81 (B (None, 65, 16)            64        \n",
      "_________________________________________________________________\n",
      "activation_81 (Activation)   (None, 65, 16)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_66 (MaxPooling (None, 21, 16)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_82 (Conv1D)           (None, 21, 16)            1296      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_82 (B (None, 21, 16)            64        \n",
      "_________________________________________________________________\n",
      "activation_82 (Activation)   (None, 21, 16)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_67 (MaxPooling (None, 7, 16)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_83 (Conv1D)           (None, 7, 16)             1296      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_83 (B (None, 7, 16)             64        \n",
      "_________________________________________________________________\n",
      "activation_83 (Activation)   (None, 7, 16)             0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_68 (MaxPooling (None, 2, 16)             0         \n",
      "_________________________________________________________________\n",
      "flatten_14 (Flatten)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 16)                528       \n",
      "=================================================================\n",
      "Total params: 69,088\n",
      "Trainable params: 68,416\n",
      "Non-trainable params: 672\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 0.3284 - acc: 0.9094\n",
      "Loss: 0.32841336481420175 Accuracy: 0.90944964\n"
     ]
    }
   ],
   "source": [
    "log_dir = 'log'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "base = '1D_CNN_custom_4_ch_64_DO_BN'\n",
    "\n",
    "with open(path.join(log_dir, base), 'w') as log_file:\n",
    "    for i in range(4, 10):\n",
    "        model_name = base+'_{}_conv'.format(i)\n",
    "        print()\n",
    "        print(model_name, 'Model')\n",
    "        model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "        model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "\n",
    "        model = load_model(model_filename)\n",
    "        model.summary()\n",
    "\n",
    "        [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "        print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "        del(model)\n",
    "\n",
    "        log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_4_ch_64_DO_BN_4_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_45 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_45 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_45 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_46 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_46 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_46 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_47 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_47 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_47 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_48 (Conv1D)           (None, 1777, 32)          10272     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_48 (B (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_48 (Activation)   (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 18944)             0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 18944)             0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 16)                303120    \n",
      "=================================================================\n",
      "Total params: 355,760\n",
      "Trainable params: 355,312\n",
      "Non-trainable params: 448\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 1.1848 - acc: 0.7109\n",
      "Loss: 1.1848195280984184 Accuracy: 0.7109034\n",
      "\n",
      "1D_CNN_custom_4_ch_64_DO_BN_5_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_49 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_49 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_49 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_50 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_50 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_50 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_51 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_51 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_51 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_52 (Conv1D)           (None, 1777, 32)          10272     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_52 (B (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_52 (Activation)   (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_53 (Conv1D)           (None, 592, 32)           5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_53 (B (None, 592, 32)           128       \n",
      "_________________________________________________________________\n",
      "activation_53 (Activation)   (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling (None, 197, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 6304)              0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 6304)              0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 16)                100880    \n",
      "=================================================================\n",
      "Total params: 158,800\n",
      "Trainable params: 158,288\n",
      "Non-trainable params: 512\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 0.6932 - acc: 0.8131\n",
      "Loss: 0.6931554398804067 Accuracy: 0.8130841\n",
      "\n",
      "1D_CNN_custom_4_ch_64_DO_BN_6_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_54 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_54 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_54 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_55 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_55 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_55 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_56 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_56 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_56 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_57 (Conv1D)           (None, 1777, 32)          10272     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_57 (B (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_57 (Activation)   (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_58 (Conv1D)           (None, 592, 32)           5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_58 (B (None, 592, 32)           128       \n",
      "_________________________________________________________________\n",
      "activation_58 (Activation)   (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling (None, 197, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_59 (Conv1D)           (None, 197, 32)           5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_59 (B (None, 197, 32)           128       \n",
      "_________________________________________________________________\n",
      "activation_59 (Activation)   (None, 197, 32)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling (None, 65, 32)            0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 2080)              0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 2080)              0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 16)                33296     \n",
      "=================================================================\n",
      "Total params: 96,496\n",
      "Trainable params: 95,920\n",
      "Non-trainable params: 576\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.4827 - acc: 0.8754\n",
      "Loss: 0.48269676133479655 Accuracy: 0.8753894\n",
      "\n",
      "1D_CNN_custom_4_ch_64_DO_BN_7_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_60 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_60 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_60 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_61 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_61 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_61 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_62 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_62 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_62 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_63 (Conv1D)           (None, 1777, 32)          10272     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_63 (B (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_63 (Activation)   (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_64 (Conv1D)           (None, 592, 32)           5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_64 (B (None, 592, 32)           128       \n",
      "_________________________________________________________________\n",
      "activation_64 (Activation)   (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling (None, 197, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_65 (Conv1D)           (None, 197, 32)           5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_65 (B (None, 197, 32)           128       \n",
      "_________________________________________________________________\n",
      "activation_65 (Activation)   (None, 197, 32)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling (None, 65, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_66 (Conv1D)           (None, 65, 16)            2576      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_66 (B (None, 65, 16)            64        \n",
      "_________________________________________________________________\n",
      "activation_66 (Activation)   (None, 65, 16)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling (None, 21, 16)            0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 336)               0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 336)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 16)                5392      \n",
      "=================================================================\n",
      "Total params: 71,232\n",
      "Trainable params: 70,624\n",
      "Non-trainable params: 608\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.3330 - acc: 0.9070\n",
      "Loss: 0.3330468311549966 Accuracy: 0.90695745\n",
      "\n",
      "1D_CNN_custom_4_ch_64_DO_BN_8_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_67 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_67 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_67 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_68 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_68 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_68 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_69 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_69 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_69 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_70 (Conv1D)           (None, 1777, 32)          10272     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_70 (B (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_70 (Activation)   (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_71 (Conv1D)           (None, 592, 32)           5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_71 (B (None, 592, 32)           128       \n",
      "_________________________________________________________________\n",
      "activation_71 (Activation)   (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling (None, 197, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_72 (Conv1D)           (None, 197, 32)           5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_72 (B (None, 197, 32)           128       \n",
      "_________________________________________________________________\n",
      "activation_72 (Activation)   (None, 197, 32)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling (None, 65, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_73 (Conv1D)           (None, 65, 16)            2576      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_73 (B (None, 65, 16)            64        \n",
      "_________________________________________________________________\n",
      "activation_73 (Activation)   (None, 65, 16)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling (None, 21, 16)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_74 (Conv1D)           (None, 21, 16)            1296      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_74 (B (None, 21, 16)            64        \n",
      "_________________________________________________________________\n",
      "activation_74 (Activation)   (None, 21, 16)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling (None, 7, 16)             0         \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 112)               0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 112)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 16)                1808      \n",
      "=================================================================\n",
      "Total params: 69,008\n",
      "Trainable params: 68,368\n",
      "Non-trainable params: 640\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.3136 - acc: 0.9184\n",
      "Loss: 0.31362761453428495 Accuracy: 0.9183801\n",
      "\n",
      "1D_CNN_custom_4_ch_64_DO_BN_9_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_75 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_75 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_75 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_76 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_76 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_76 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_77 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_77 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_77 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_78 (Conv1D)           (None, 1777, 32)          10272     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_78 (B (None, 1777, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_78 (Activation)   (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_79 (Conv1D)           (None, 592, 32)           5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_79 (B (None, 592, 32)           128       \n",
      "_________________________________________________________________\n",
      "activation_79 (Activation)   (None, 592, 32)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling (None, 197, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_80 (Conv1D)           (None, 197, 32)           5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_80 (B (None, 197, 32)           128       \n",
      "_________________________________________________________________\n",
      "activation_80 (Activation)   (None, 197, 32)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling (None, 65, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_81 (Conv1D)           (None, 65, 16)            2576      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_81 (B (None, 65, 16)            64        \n",
      "_________________________________________________________________\n",
      "activation_81 (Activation)   (None, 65, 16)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_66 (MaxPooling (None, 21, 16)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_82 (Conv1D)           (None, 21, 16)            1296      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_82 (B (None, 21, 16)            64        \n",
      "_________________________________________________________________\n",
      "activation_82 (Activation)   (None, 21, 16)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_67 (MaxPooling (None, 7, 16)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_83 (Conv1D)           (None, 7, 16)             1296      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_83 (B (None, 7, 16)             64        \n",
      "_________________________________________________________________\n",
      "activation_83 (Activation)   (None, 7, 16)             0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_68 (MaxPooling (None, 2, 16)             0         \n",
      "_________________________________________________________________\n",
      "flatten_14 (Flatten)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 16)                528       \n",
      "=================================================================\n",
      "Total params: 69,088\n",
      "Trainable params: 68,416\n",
      "Non-trainable params: 672\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.3485 - acc: 0.9136\n",
      "Loss: 0.34853348972455733 Accuracy: 0.9136033\n"
     ]
    }
   ],
   "source": [
    "# log_dir = 'log'\n",
    "# os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# with open(path.join(log_dir, base), 'w') as log_file:\n",
    "for i in range(4, 10):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + '000_last.hdf5'\n",
    "\n",
    "    model = load_model(model_filename)\n",
    "    model.summary()\n",
    "\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "    del(model)\n",
    "\n",
    "#         log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
