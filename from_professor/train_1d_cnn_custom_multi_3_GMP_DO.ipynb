{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path as path\n",
    "import itertools\n",
    "from sklearn.preprocessing import maxabs_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Input, Dense, Activation, BatchNormalization, \\\n",
    "                                    Flatten, Conv1D, MaxPooling1D, Dropout, \\\n",
    "                                    Concatenate, GlobalMaxPool1D, GlobalAvgPool1D\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler, \\\n",
    "                                        EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = path.join('..', 'data')\n",
    "data_dir = path.join(base_dir, 'data_speech_commands_v0.02')\n",
    " \n",
    "train_txt = path.join(data_dir, 'wav_train_16words.txt')\n",
    "val_txt = path.join(data_dir, 'wav_validation_16words.txt')\n",
    "test_txt = path.join(data_dir, 'wav_test_16words.txt')\n",
    "\n",
    "train_data = np.load(path.join(data_dir, 'wav_train_data.npz'))\n",
    "val_data = np.load(path.join(data_dir, 'wav_validation_data.npz'))\n",
    "test_data = np.load(path.join(data_dir, 'wav_test_data.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36805, 16000, 1),\n",
       " (36805,),\n",
       " (4293, 16000, 1),\n",
       " (4293,),\n",
       " (4815, 16000, 1),\n",
       " (4815,),\n",
       " (16, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = train_data['x_train']\n",
    "y_train = train_data['y_train']\n",
    "x_val = val_data['x_val']\n",
    "y_val = val_data['y_val']\n",
    "x_test = test_data['x_test']\n",
    "y_test = test_data['y_test']\n",
    "y_table = test_data['table']\n",
    "\n",
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape, y_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_test[0].shape\n",
    "output_size = y_table.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_abs = np.asarray([maxabs_scale(wav) for wav in x_train])\n",
    "y_train_onehot = np.asarray([to_categorical(label, output_size) for label in y_train])\n",
    "del x_train, y_train\n",
    "\n",
    "x_val_abs = np.asarray([maxabs_scale(wav) for wav in x_val])\n",
    "y_val_onehot = np.asarray([to_categorical(label, output_size) for label in y_val])\n",
    "del x_val, y_val\n",
    "\n",
    "x_test_abs = np.asarray([maxabs_scale(wav) for wav in x_test])\n",
    "y_test_onehot = np.asarray([to_categorical(label, output_size) for label in y_test])\n",
    "del x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn(conv_num=1):\n",
    "    filter_size = 64\n",
    "\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    x = input_layer\n",
    "\n",
    "    layer_outputs = []\n",
    "    for i in range(conv_num):\n",
    "        x = Conv1D (kernel_size=5, filters=filter_size*(2**(i//4)), \n",
    "                          strides=1, padding='same')(x)\n",
    "        x = Activation('relu')(x)\n",
    "        x = MaxPooling1D(pool_size=3, strides=3)(x)\n",
    "        layer_outputs.append(x)    \n",
    "    \n",
    "    x = Concatenate()([GlobalMaxPool1D()(output) for output in layer_outputs[-3:]])\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(output_size, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 16000, 64)    384         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 16000, 64)    0           conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D)    (None, 5333, 64)     0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 5333, 64)     20544       max_pooling1d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 5333, 64)     0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 1777, 64)     0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 1777, 64)     20544       max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 1777, 64)     0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 592, 64)      0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d (GlobalMax (None, 64)           0           max_pooling1d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 64)           0           max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_2 (GlobalM (None, 64)           0           max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 192)          0           global_max_pooling1d[0][0]       \n",
      "                                                                 global_max_pooling1d_1[0][0]     \n",
      "                                                                 global_max_pooling1d_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 192)          0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 16)           3088        dropout[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 44,560\n",
      "Trainable params: 44,560\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 16000, 64)    384         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 16000, 64)    0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 5333, 64)     0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 5333, 64)     20544       max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 5333, 64)     0           conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 1777, 64)     0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 1777, 64)     20544       max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 1777, 64)     0           conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 592, 64)      0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 592, 64)      20544       max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 592, 64)      0           conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1D)  (None, 197, 64)      0           activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_3 (GlobalM (None, 64)           0           max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_4 (GlobalM (None, 64)           0           max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_5 (GlobalM (None, 64)           0           max_pooling1d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 192)          0           global_max_pooling1d_3[0][0]     \n",
      "                                                                 global_max_pooling1d_4[0][0]     \n",
      "                                                                 global_max_pooling1d_5[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 192)          0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 16)           3088        dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 65,104\n",
      "Trainable params: 65,104\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 16000, 64)    384         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 16000, 64)    0           conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1D)  (None, 5333, 64)     0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 5333, 64)     20544       max_pooling1d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 5333, 64)     0           conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1D)  (None, 1777, 64)     0           activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 1777, 64)     20544       max_pooling1d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 1777, 64)     0           conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1D)  (None, 592, 64)      0           activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 592, 64)      0           conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling1D) (None, 197, 64)      0           activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 197, 128)     0           conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling1D) (None, 65, 128)      0           activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_6 (GlobalM (None, 64)           0           max_pooling1d_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_7 (GlobalM (None, 64)           0           max_pooling1d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_8 (GlobalM (None, 128)          0           max_pooling1d_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 256)          0           global_max_pooling1d_6[0][0]     \n",
      "                                                                 global_max_pooling1d_7[0][0]     \n",
      "                                                                 global_max_pooling1d_8[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 256)          0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 16)           4112        dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 107,216\n",
      "Trainable params: 107,216\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 16000, 64)    384         input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 16000, 64)    0           conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling1D) (None, 5333, 64)     0           activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 5333, 64)     0           conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling1D) (None, 1777, 64)     0           activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_13[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 1777, 64)     0           conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling1D) (None, 592, 64)      0           activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_14[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 592, 64)      0           conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling1D) (None, 197, 64)      0           activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_15[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 197, 128)     0           conv1d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling1D) (None, 65, 128)      0           activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_17 (Conv1D)              (None, 65, 128)      82048       max_pooling1d_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 65, 128)      0           conv1d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling1D) (None, 21, 128)      0           activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_9 (GlobalM (None, 64)           0           max_pooling1d_15[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_10 (Global (None, 128)          0           max_pooling1d_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_11 (Global (None, 128)          0           max_pooling1d_17[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 320)          0           global_max_pooling1d_9[0][0]     \n",
      "                                                                 global_max_pooling1d_10[0][0]    \n",
      "                                                                 global_max_pooling1d_11[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 320)          0           concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 16)           5136        dropout_3[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 190,288\n",
      "Trainable params: 190,288\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_18 (Conv1D)              (None, 16000, 64)    384         input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 16000, 64)    0           conv1d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling1D) (None, 5333, 64)     0           activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_19 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_18[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 5333, 64)     0           conv1d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling1D) (None, 1777, 64)     0           activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_20 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_19[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 1777, 64)     0           conv1d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling1D) (None, 592, 64)      0           activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_21 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_20[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 592, 64)      0           conv1d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling1D) (None, 197, 64)      0           activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_22 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_21[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 197, 128)     0           conv1d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling1D) (None, 65, 128)      0           activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_23 (Conv1D)              (None, 65, 128)      82048       max_pooling1d_22[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 65, 128)      0           conv1d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling1D) (None, 21, 128)      0           activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_24 (Conv1D)              (None, 21, 128)      82048       max_pooling1d_23[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 21, 128)      0           conv1d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling1D) (None, 7, 128)       0           activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_12 (Global (None, 128)          0           max_pooling1d_22[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_13 (Global (None, 128)          0           max_pooling1d_23[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_14 (Global (None, 128)          0           max_pooling1d_24[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 384)          0           global_max_pooling1d_12[0][0]    \n",
      "                                                                 global_max_pooling1d_13[0][0]    \n",
      "                                                                 global_max_pooling1d_14[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 384)          0           concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 16)           6160        dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 273,360\n",
      "Trainable params: 273,360\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_25 (Conv1D)              (None, 16000, 64)    384         input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 16000, 64)    0           conv1d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling1D) (None, 5333, 64)     0           activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_26 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_25[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 5333, 64)     0           conv1d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_26 (MaxPooling1D) (None, 1777, 64)     0           activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_27 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_26[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 1777, 64)     0           conv1d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_27 (MaxPooling1D) (None, 592, 64)      0           activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_28 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_27[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 592, 64)      0           conv1d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_28 (MaxPooling1D) (None, 197, 64)      0           activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_29 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_28[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 197, 128)     0           conv1d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_29 (MaxPooling1D) (None, 65, 128)      0           activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_30 (Conv1D)              (None, 65, 128)      82048       max_pooling1d_29[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 65, 128)      0           conv1d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_30 (MaxPooling1D) (None, 21, 128)      0           activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_31 (Conv1D)              (None, 21, 128)      82048       max_pooling1d_30[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 21, 128)      0           conv1d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_31 (MaxPooling1D) (None, 7, 128)       0           activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_32 (Conv1D)              (None, 7, 128)       82048       max_pooling1d_31[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 7, 128)       0           conv1d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_32 (MaxPooling1D) (None, 2, 128)       0           activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_15 (Global (None, 128)          0           max_pooling1d_30[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_16 (Global (None, 128)          0           max_pooling1d_31[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_17 (Global (None, 128)          0           max_pooling1d_32[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 384)          0           global_max_pooling1d_15[0][0]    \n",
      "                                                                 global_max_pooling1d_16[0][0]    \n",
      "                                                                 global_max_pooling1d_17[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 384)          0           concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 16)           6160        dropout_5[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 355,408\n",
      "Trainable params: 355,408\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in range(3, 9):\n",
    "    model = build_cnn(conv_num=i)\n",
    "    model.summary()\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36805 samples, validate on 4293 samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.6390 - acc: 0.1370\n",
      "Epoch 00001: val_loss improved from inf to 2.41191, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_3_conv_checkpoint/001-2.4119.hdf5\n",
      "36805/36805 [==============================] - 34s 913us/sample - loss: 2.6390 - acc: 0.1370 - val_loss: 2.4119 - val_acc: 0.2383\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.3405 - acc: 0.2207\n",
      "Epoch 00002: val_loss improved from 2.41191 to 2.10952, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_3_conv_checkpoint/002-2.1095.hdf5\n",
      "36805/36805 [==============================] - 31s 855us/sample - loss: 2.3404 - acc: 0.2207 - val_loss: 2.1095 - val_acc: 0.3371\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.1652 - acc: 0.2688\n",
      "Epoch 00003: val_loss improved from 2.10952 to 1.93829, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_3_conv_checkpoint/003-1.9383.hdf5\n",
      "36805/36805 [==============================] - 32s 859us/sample - loss: 2.1651 - acc: 0.2688 - val_loss: 1.9383 - val_acc: 0.3871\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.0457 - acc: 0.3072\n",
      "Epoch 00004: val_loss improved from 1.93829 to 1.81187, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_3_conv_checkpoint/004-1.8119.hdf5\n",
      "36805/36805 [==============================] - 32s 856us/sample - loss: 2.0456 - acc: 0.3072 - val_loss: 1.8119 - val_acc: 0.4302\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9469 - acc: 0.3468\n",
      "Epoch 00005: val_loss improved from 1.81187 to 1.69596, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_3_conv_checkpoint/005-1.6960.hdf5\n",
      "36805/36805 [==============================] - 31s 855us/sample - loss: 1.9470 - acc: 0.3468 - val_loss: 1.6960 - val_acc: 0.4796\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.8664 - acc: 0.3729\n",
      "Epoch 00006: val_loss improved from 1.69596 to 1.61274, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_3_conv_checkpoint/006-1.6127.hdf5\n",
      "36805/36805 [==============================] - 32s 858us/sample - loss: 1.8665 - acc: 0.3729 - val_loss: 1.6127 - val_acc: 0.4980\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.7980 - acc: 0.3971\n",
      "Epoch 00007: val_loss improved from 1.61274 to 1.52959, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_3_conv_checkpoint/007-1.5296.hdf5\n",
      "36805/36805 [==============================] - 32s 856us/sample - loss: 1.7981 - acc: 0.3971 - val_loss: 1.5296 - val_acc: 0.5306\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.7336 - acc: 0.4190\n",
      "Epoch 00008: val_loss improved from 1.52959 to 1.46439, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_3_conv_checkpoint/008-1.4644.hdf5\n",
      "36805/36805 [==============================] - 31s 851us/sample - loss: 1.7336 - acc: 0.4190 - val_loss: 1.4644 - val_acc: 0.5458\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6800 - acc: 0.4419\n",
      "Epoch 00009: val_loss improved from 1.46439 to 1.40967, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_3_conv_checkpoint/009-1.4097.hdf5\n",
      "36805/36805 [==============================] - 31s 851us/sample - loss: 1.6800 - acc: 0.4420 - val_loss: 1.4097 - val_acc: 0.5639\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6379 - acc: 0.4530\n",
      "Epoch 00010: val_loss improved from 1.40967 to 1.35392, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_3_conv_checkpoint/010-1.3539.hdf5\n",
      "36805/36805 [==============================] - 31s 851us/sample - loss: 1.6380 - acc: 0.4530 - val_loss: 1.3539 - val_acc: 0.5868\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5846 - acc: 0.4736\n",
      "Epoch 00011: val_loss improved from 1.35392 to 1.30223, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_3_conv_checkpoint/011-1.3022.hdf5\n",
      "36805/36805 [==============================] - 31s 852us/sample - loss: 1.5847 - acc: 0.4735 - val_loss: 1.3022 - val_acc: 0.5982\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5453 - acc: 0.4874\n",
      "Epoch 00012: val_loss improved from 1.30223 to 1.26567, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_3_conv_checkpoint/012-1.2657.hdf5\n",
      "36805/36805 [==============================] - 31s 851us/sample - loss: 1.5455 - acc: 0.4874 - val_loss: 1.2657 - val_acc: 0.6075\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5134 - acc: 0.4966\n",
      "Epoch 00013: val_loss improved from 1.26567 to 1.23220, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_3_conv_checkpoint/013-1.2322.hdf5\n",
      "36805/36805 [==============================] - 31s 850us/sample - loss: 1.5133 - acc: 0.4966 - val_loss: 1.2322 - val_acc: 0.6147\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4837 - acc: 0.5112\n",
      "Epoch 00014: val_loss improved from 1.23220 to 1.19875, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_3_conv_checkpoint/014-1.1987.hdf5\n",
      "36805/36805 [==============================] - 31s 852us/sample - loss: 1.4835 - acc: 0.5113 - val_loss: 1.1987 - val_acc: 0.6247\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4612 - acc: 0.5174\n",
      "Epoch 00015: val_loss improved from 1.19875 to 1.16954, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_3_conv_checkpoint/015-1.1695.hdf5\n",
      "36805/36805 [==============================] - 31s 852us/sample - loss: 1.4612 - acc: 0.5175 - val_loss: 1.1695 - val_acc: 0.6275\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4237 - acc: 0.5299\n",
      "Epoch 00016: val_loss improved from 1.16954 to 1.13860, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_3_conv_checkpoint/016-1.1386.hdf5\n",
      "36805/36805 [==============================] - 31s 847us/sample - loss: 1.4236 - acc: 0.5299 - val_loss: 1.1386 - val_acc: 0.6415\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4025 - acc: 0.5367\n",
      "Epoch 00017: val_loss improved from 1.13860 to 1.11303, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_3_conv_checkpoint/017-1.1130.hdf5\n",
      "36805/36805 [==============================] - 31s 849us/sample - loss: 1.4025 - acc: 0.5367 - val_loss: 1.1130 - val_acc: 0.6511\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3754 - acc: 0.5480\n",
      "Epoch 00018: val_loss improved from 1.11303 to 1.09990, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_3_conv_checkpoint/018-1.0999.hdf5\n",
      "36805/36805 [==============================] - 31s 848us/sample - loss: 1.3754 - acc: 0.5480 - val_loss: 1.0999 - val_acc: 0.6576\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3571 - acc: 0.5523\n",
      "Epoch 00019: val_loss improved from 1.09990 to 1.07063, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_3_conv_checkpoint/019-1.0706.hdf5\n",
      "36805/36805 [==============================] - 31s 847us/sample - loss: 1.3572 - acc: 0.5523 - val_loss: 1.0706 - val_acc: 0.6655\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3365 - acc: 0.5605\n",
      "Epoch 00020: val_loss improved from 1.07063 to 1.05441, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_3_conv_checkpoint/020-1.0544.hdf5\n",
      "36805/36805 [==============================] - 31s 843us/sample - loss: 1.3364 - acc: 0.5605 - val_loss: 1.0544 - val_acc: 0.6704\n",
      "Epoch 21/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3171 - acc: 0.5669\n",
      "Epoch 00021: val_loss improved from 1.05441 to 1.03375, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_3_conv_checkpoint/021-1.0338.hdf5\n",
      "36805/36805 [==============================] - 31s 849us/sample - loss: 1.3172 - acc: 0.5669 - val_loss: 1.0338 - val_acc: 0.6797\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3062 - acc: 0.5743\n",
      "Epoch 00022: val_loss improved from 1.03375 to 1.01566, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_3_conv_checkpoint/022-1.0157.hdf5\n",
      "36805/36805 [==============================] - 31s 843us/sample - loss: 1.3062 - acc: 0.5744 - val_loss: 1.0157 - val_acc: 0.6816\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2806 - acc: 0.5820\n",
      "Epoch 00023: val_loss improved from 1.01566 to 1.00633, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_3_conv_checkpoint/023-1.0063.hdf5\n",
      "36805/36805 [==============================] - 31s 844us/sample - loss: 1.2806 - acc: 0.5820 - val_loss: 1.0063 - val_acc: 0.6876\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2724 - acc: 0.5818\n",
      "Epoch 00024: val_loss improved from 1.00633 to 0.98057, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_3_conv_checkpoint/024-0.9806.hdf5\n",
      "36805/36805 [==============================] - 31s 849us/sample - loss: 1.2724 - acc: 0.5818 - val_loss: 0.9806 - val_acc: 0.6904\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2589 - acc: 0.5904\n",
      "Epoch 00025: val_loss improved from 0.98057 to 0.97101, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_3_conv_checkpoint/025-0.9710.hdf5\n",
      "36805/36805 [==============================] - 31s 844us/sample - loss: 1.2589 - acc: 0.5904 - val_loss: 0.9710 - val_acc: 0.6921\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2457 - acc: 0.5945\n",
      "Epoch 00026: val_loss improved from 0.97101 to 0.95984, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_3_conv_checkpoint/026-0.9598.hdf5\n",
      "36805/36805 [==============================] - 31s 847us/sample - loss: 1.2456 - acc: 0.5945 - val_loss: 0.9598 - val_acc: 0.7023\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2245 - acc: 0.6008\n",
      "Epoch 00027: val_loss improved from 0.95984 to 0.94349, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_3_conv_checkpoint/027-0.9435.hdf5\n",
      "36805/36805 [==============================] - 31s 841us/sample - loss: 1.2244 - acc: 0.6008 - val_loss: 0.9435 - val_acc: 0.7044\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2236 - acc: 0.6012\n",
      "Epoch 00028: val_loss improved from 0.94349 to 0.93739, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_3_conv_checkpoint/028-0.9374.hdf5\n",
      "36805/36805 [==============================] - 31s 842us/sample - loss: 1.2235 - acc: 0.6012 - val_loss: 0.9374 - val_acc: 0.7086\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2071 - acc: 0.6082\n",
      "Epoch 00029: val_loss improved from 0.93739 to 0.92138, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_3_conv_checkpoint/029-0.9214.hdf5\n",
      "36805/36805 [==============================] - 31s 844us/sample - loss: 1.2070 - acc: 0.6082 - val_loss: 0.9214 - val_acc: 0.7156\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1974 - acc: 0.6096\n",
      "Epoch 00030: val_loss improved from 0.92138 to 0.91468, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_3_conv_checkpoint/030-0.9147.hdf5\n",
      "36805/36805 [==============================] - 31s 841us/sample - loss: 1.1974 - acc: 0.6096 - val_loss: 0.9147 - val_acc: 0.7172\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1894 - acc: 0.6151\n",
      "Epoch 00031: val_loss improved from 0.91468 to 0.90807, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_3_conv_checkpoint/031-0.9081.hdf5\n",
      "36805/36805 [==============================] - 31s 842us/sample - loss: 1.1893 - acc: 0.6151 - val_loss: 0.9081 - val_acc: 0.7160\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1796 - acc: 0.6185\n",
      "Epoch 00032: val_loss improved from 0.90807 to 0.89707, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_3_conv_checkpoint/032-0.8971.hdf5\n",
      "36805/36805 [==============================] - 31s 844us/sample - loss: 1.1796 - acc: 0.6185 - val_loss: 0.8971 - val_acc: 0.7233\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1720 - acc: 0.6223\n",
      "Epoch 00033: val_loss improved from 0.89707 to 0.88311, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_3_conv_checkpoint/033-0.8831.hdf5\n",
      "36805/36805 [==============================] - 31s 844us/sample - loss: 1.1720 - acc: 0.6223 - val_loss: 0.8831 - val_acc: 0.7247\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1656 - acc: 0.6224\n",
      "Epoch 00034: val_loss improved from 0.88311 to 0.87516, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_3_conv_checkpoint/034-0.8752.hdf5\n",
      "36805/36805 [==============================] - 31s 844us/sample - loss: 1.1657 - acc: 0.6224 - val_loss: 0.8752 - val_acc: 0.7286\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1564 - acc: 0.6254\n",
      "Epoch 00035: val_loss did not improve from 0.87516\n",
      "36805/36805 [==============================] - 31s 843us/sample - loss: 1.1564 - acc: 0.6254 - val_loss: 0.8755 - val_acc: 0.7265\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1494 - acc: 0.6291\n",
      "Epoch 00036: val_loss improved from 0.87516 to 0.86457, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_3_conv_checkpoint/036-0.8646.hdf5\n",
      "36805/36805 [==============================] - 31s 842us/sample - loss: 1.1494 - acc: 0.6291 - val_loss: 0.8646 - val_acc: 0.7335\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1401 - acc: 0.6310\n",
      "Epoch 00037: val_loss improved from 0.86457 to 0.85632, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_3_conv_checkpoint/037-0.8563.hdf5\n",
      "36805/36805 [==============================] - 31s 844us/sample - loss: 1.1401 - acc: 0.6309 - val_loss: 0.8563 - val_acc: 0.7303\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1313 - acc: 0.6355\n",
      "Epoch 00038: val_loss improved from 0.85632 to 0.85358, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_3_conv_checkpoint/038-0.8536.hdf5\n",
      "36805/36805 [==============================] - 31s 843us/sample - loss: 1.1313 - acc: 0.6356 - val_loss: 0.8536 - val_acc: 0.7352\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1284 - acc: 0.6368\n",
      "Epoch 00039: val_loss improved from 0.85358 to 0.84464, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_3_conv_checkpoint/039-0.8446.hdf5\n",
      "36805/36805 [==============================] - 31s 844us/sample - loss: 1.1284 - acc: 0.6368 - val_loss: 0.8446 - val_acc: 0.7396\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1189 - acc: 0.6379\n",
      "Epoch 00040: val_loss did not improve from 0.84464\n",
      "36805/36805 [==============================] - 31s 844us/sample - loss: 1.1192 - acc: 0.6379 - val_loss: 0.8496 - val_acc: 0.7370\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1223 - acc: 0.6397\n",
      "Epoch 00041: val_loss improved from 0.84464 to 0.83972, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_3_conv_checkpoint/041-0.8397.hdf5\n",
      "36805/36805 [==============================] - 31s 842us/sample - loss: 1.1222 - acc: 0.6397 - val_loss: 0.8397 - val_acc: 0.7431\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1133 - acc: 0.6406\n",
      "Epoch 00042: val_loss improved from 0.83972 to 0.82622, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_3_conv_checkpoint/042-0.8262.hdf5\n",
      "36805/36805 [==============================] - 31s 844us/sample - loss: 1.1132 - acc: 0.6406 - val_loss: 0.8262 - val_acc: 0.7414\n",
      "Epoch 43/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1058 - acc: 0.6455\n",
      "Epoch 00043: val_loss improved from 0.82622 to 0.82346, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_3_conv_checkpoint/043-0.8235.hdf5\n",
      "36805/36805 [==============================] - 31s 844us/sample - loss: 1.1058 - acc: 0.6455 - val_loss: 0.8235 - val_acc: 0.7438\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0956 - acc: 0.6477\n",
      "Epoch 00044: val_loss did not improve from 0.82346\n",
      "36805/36805 [==============================] - 31s 844us/sample - loss: 1.0956 - acc: 0.6477 - val_loss: 0.8263 - val_acc: 0.7452\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0905 - acc: 0.6492\n",
      "Epoch 00045: val_loss improved from 0.82346 to 0.82181, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_3_conv_checkpoint/045-0.8218.hdf5\n",
      "36805/36805 [==============================] - 31s 846us/sample - loss: 1.0906 - acc: 0.6492 - val_loss: 0.8218 - val_acc: 0.7447\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0942 - acc: 0.6501\n",
      "Epoch 00046: val_loss improved from 0.82181 to 0.80160, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_3_conv_checkpoint/046-0.8016.hdf5\n",
      "36805/36805 [==============================] - 31s 845us/sample - loss: 1.0942 - acc: 0.6501 - val_loss: 0.8016 - val_acc: 0.7510\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0854 - acc: 0.6543\n",
      "Epoch 00047: val_loss did not improve from 0.80160\n",
      "36805/36805 [==============================] - 31s 840us/sample - loss: 1.0854 - acc: 0.6543 - val_loss: 0.8050 - val_acc: 0.7573\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0774 - acc: 0.6553\n",
      "Epoch 00048: val_loss improved from 0.80160 to 0.80045, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_3_conv_checkpoint/048-0.8004.hdf5\n",
      "36805/36805 [==============================] - 31s 846us/sample - loss: 1.0773 - acc: 0.6553 - val_loss: 0.8004 - val_acc: 0.7491\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0734 - acc: 0.6532\n",
      "Epoch 00049: val_loss improved from 0.80045 to 0.78630, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_3_conv_checkpoint/049-0.7863.hdf5\n",
      "36805/36805 [==============================] - 31s 843us/sample - loss: 1.0734 - acc: 0.6532 - val_loss: 0.7863 - val_acc: 0.7591\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0751 - acc: 0.6559\n",
      "Epoch 00050: val_loss did not improve from 0.78630\n",
      "36805/36805 [==============================] - 31s 841us/sample - loss: 1.0750 - acc: 0.6559 - val_loss: 0.7912 - val_acc: 0.7545\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0683 - acc: 0.6562\n",
      "Epoch 00051: val_loss improved from 0.78630 to 0.78470, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_3_conv_checkpoint/051-0.7847.hdf5\n",
      "36805/36805 [==============================] - 31s 841us/sample - loss: 1.0683 - acc: 0.6562 - val_loss: 0.7847 - val_acc: 0.7577\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0652 - acc: 0.6611\n",
      "Epoch 00052: val_loss improved from 0.78470 to 0.78461, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_3_conv_checkpoint/052-0.7846.hdf5\n",
      "36805/36805 [==============================] - 31s 836us/sample - loss: 1.0651 - acc: 0.6612 - val_loss: 0.7846 - val_acc: 0.7570\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0603 - acc: 0.6592\n",
      "Epoch 00053: val_loss improved from 0.78461 to 0.77936, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_3_conv_checkpoint/053-0.7794.hdf5\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 1.0604 - acc: 0.6592 - val_loss: 0.7794 - val_acc: 0.7617\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0627 - acc: 0.6594\n",
      "Epoch 00054: val_loss improved from 0.77936 to 0.77865, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_3_conv_checkpoint/054-0.7786.hdf5\n",
      "36805/36805 [==============================] - 31s 836us/sample - loss: 1.0626 - acc: 0.6595 - val_loss: 0.7786 - val_acc: 0.7636\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0556 - acc: 0.6609\n",
      "Epoch 00055: val_loss improved from 0.77865 to 0.77339, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_3_conv_checkpoint/055-0.7734.hdf5\n",
      "36805/36805 [==============================] - 31s 835us/sample - loss: 1.0556 - acc: 0.6609 - val_loss: 0.7734 - val_acc: 0.7647\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0493 - acc: 0.6663\n",
      "Epoch 00056: val_loss did not improve from 0.77339\n",
      "36805/36805 [==============================] - 31s 831us/sample - loss: 1.0494 - acc: 0.6663 - val_loss: 0.7758 - val_acc: 0.7619\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0488 - acc: 0.6639\n",
      "Epoch 00057: val_loss did not improve from 0.77339\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 1.0487 - acc: 0.6640 - val_loss: 0.7785 - val_acc: 0.7598\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0489 - acc: 0.6663\n",
      "Epoch 00058: val_loss improved from 0.77339 to 0.76457, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_3_conv_checkpoint/058-0.7646.hdf5\n",
      "36805/36805 [==============================] - 31s 834us/sample - loss: 1.0489 - acc: 0.6663 - val_loss: 0.7646 - val_acc: 0.7668\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0467 - acc: 0.6648\n",
      "Epoch 00059: val_loss improved from 0.76457 to 0.75674, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_3_conv_checkpoint/059-0.7567.hdf5\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 1.0467 - acc: 0.6648 - val_loss: 0.7567 - val_acc: 0.7675\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0427 - acc: 0.6676\n",
      "Epoch 00060: val_loss did not improve from 0.75674\n",
      "36805/36805 [==============================] - 31s 842us/sample - loss: 1.0426 - acc: 0.6676 - val_loss: 0.7573 - val_acc: 0.7650\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0350 - acc: 0.6687\n",
      "Epoch 00061: val_loss improved from 0.75674 to 0.74690, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_3_conv_checkpoint/061-0.7469.hdf5\n",
      "36805/36805 [==============================] - 31s 835us/sample - loss: 1.0349 - acc: 0.6687 - val_loss: 0.7469 - val_acc: 0.7706\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0360 - acc: 0.6689\n",
      "Epoch 00062: val_loss did not improve from 0.74690\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 1.0361 - acc: 0.6689 - val_loss: 0.7598 - val_acc: 0.7722\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0300 - acc: 0.6720\n",
      "Epoch 00063: val_loss did not improve from 0.74690\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 1.0300 - acc: 0.6721 - val_loss: 0.7527 - val_acc: 0.7671\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0347 - acc: 0.6659\n",
      "Epoch 00064: val_loss did not improve from 0.74690\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 1.0346 - acc: 0.6659 - val_loss: 0.7482 - val_acc: 0.7715\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0250 - acc: 0.6724\n",
      "Epoch 00065: val_loss improved from 0.74690 to 0.74655, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_3_conv_checkpoint/065-0.7466.hdf5\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 1.0249 - acc: 0.6725 - val_loss: 0.7466 - val_acc: 0.7708\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0194 - acc: 0.6741\n",
      "Epoch 00066: val_loss improved from 0.74655 to 0.74141, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_3_conv_checkpoint/066-0.7414.hdf5\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 1.0194 - acc: 0.6741 - val_loss: 0.7414 - val_acc: 0.7727\n",
      "Epoch 67/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0143 - acc: 0.6774\n",
      "Epoch 00067: val_loss did not improve from 0.74141\n",
      "36805/36805 [==============================] - 31s 856us/sample - loss: 1.0142 - acc: 0.6774 - val_loss: 0.7452 - val_acc: 0.7731\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0137 - acc: 0.6745\n",
      "Epoch 00068: val_loss did not improve from 0.74141\n",
      "36805/36805 [==============================] - 31s 836us/sample - loss: 1.0136 - acc: 0.6745 - val_loss: 0.7520 - val_acc: 0.7657\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0171 - acc: 0.6767\n",
      "Epoch 00069: val_loss improved from 0.74141 to 0.73579, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_3_conv_checkpoint/069-0.7358.hdf5\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 1.0171 - acc: 0.6768 - val_loss: 0.7358 - val_acc: 0.7792\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0101 - acc: 0.6733\n",
      "Epoch 00070: val_loss did not improve from 0.73579\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 1.0102 - acc: 0.6733 - val_loss: 0.7363 - val_acc: 0.7759\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0075 - acc: 0.6787\n",
      "Epoch 00071: val_loss did not improve from 0.73579\n",
      "36805/36805 [==============================] - 31s 835us/sample - loss: 1.0075 - acc: 0.6787 - val_loss: 0.7427 - val_acc: 0.7761\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0044 - acc: 0.6788\n",
      "Epoch 00072: val_loss improved from 0.73579 to 0.73385, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_3_conv_checkpoint/072-0.7339.hdf5\n",
      "36805/36805 [==============================] - 31s 840us/sample - loss: 1.0044 - acc: 0.6788 - val_loss: 0.7339 - val_acc: 0.7773\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0016 - acc: 0.6796\n",
      "Epoch 00073: val_loss did not improve from 0.73385\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 1.0015 - acc: 0.6796 - val_loss: 0.7342 - val_acc: 0.7754\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0038 - acc: 0.6790\n",
      "Epoch 00074: val_loss improved from 0.73385 to 0.73353, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_3_conv_checkpoint/074-0.7335.hdf5\n",
      "36805/36805 [==============================] - 31s 840us/sample - loss: 1.0037 - acc: 0.6791 - val_loss: 0.7335 - val_acc: 0.7778\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0025 - acc: 0.6794\n",
      "Epoch 00075: val_loss did not improve from 0.73353\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 1.0025 - acc: 0.6794 - val_loss: 0.7424 - val_acc: 0.7680\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9972 - acc: 0.6824\n",
      "Epoch 00076: val_loss improved from 0.73353 to 0.71835, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_3_conv_checkpoint/076-0.7184.hdf5\n",
      "36805/36805 [==============================] - 31s 836us/sample - loss: 0.9971 - acc: 0.6824 - val_loss: 0.7184 - val_acc: 0.7813\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9931 - acc: 0.6811\n",
      "Epoch 00077: val_loss did not improve from 0.71835\n",
      "36805/36805 [==============================] - 31s 835us/sample - loss: 0.9930 - acc: 0.6811 - val_loss: 0.7205 - val_acc: 0.7792\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9991 - acc: 0.6832\n",
      "Epoch 00078: val_loss improved from 0.71835 to 0.71247, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_3_conv_checkpoint/078-0.7125.hdf5\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.9991 - acc: 0.6831 - val_loss: 0.7125 - val_acc: 0.7838\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9898 - acc: 0.6839\n",
      "Epoch 00079: val_loss did not improve from 0.71247\n",
      "36805/36805 [==============================] - 31s 841us/sample - loss: 0.9897 - acc: 0.6838 - val_loss: 0.7170 - val_acc: 0.7806\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9885 - acc: 0.6842\n",
      "Epoch 00080: val_loss improved from 0.71247 to 0.70733, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_3_conv_checkpoint/080-0.7073.hdf5\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.9884 - acc: 0.6843 - val_loss: 0.7073 - val_acc: 0.7862\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9892 - acc: 0.6838\n",
      "Epoch 00081: val_loss did not improve from 0.70733\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.9892 - acc: 0.6838 - val_loss: 0.7081 - val_acc: 0.7869\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9838 - acc: 0.6840\n",
      "Epoch 00082: val_loss did not improve from 0.70733\n",
      "36805/36805 [==============================] - 31s 833us/sample - loss: 0.9839 - acc: 0.6840 - val_loss: 0.7144 - val_acc: 0.7813\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9896 - acc: 0.6856\n",
      "Epoch 00083: val_loss did not improve from 0.70733\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.9896 - acc: 0.6856 - val_loss: 0.7080 - val_acc: 0.7850\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9841 - acc: 0.6840\n",
      "Epoch 00084: val_loss improved from 0.70733 to 0.70322, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_3_conv_checkpoint/084-0.7032.hdf5\n",
      "36805/36805 [==============================] - 31s 840us/sample - loss: 0.9841 - acc: 0.6840 - val_loss: 0.7032 - val_acc: 0.7876\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9830 - acc: 0.6880\n",
      "Epoch 00085: val_loss did not improve from 0.70322\n",
      "36805/36805 [==============================] - 31s 833us/sample - loss: 0.9830 - acc: 0.6880 - val_loss: 0.7081 - val_acc: 0.7848\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9801 - acc: 0.6868\n",
      "Epoch 00086: val_loss did not improve from 0.70322\n",
      "36805/36805 [==============================] - 31s 840us/sample - loss: 0.9802 - acc: 0.6868 - val_loss: 0.7043 - val_acc: 0.7871\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9727 - acc: 0.6886\n",
      "Epoch 00087: val_loss did not improve from 0.70322\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.9728 - acc: 0.6885 - val_loss: 0.7062 - val_acc: 0.7883\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9756 - acc: 0.6862\n",
      "Epoch 00088: val_loss did not improve from 0.70322\n",
      "36805/36805 [==============================] - 31s 836us/sample - loss: 0.9756 - acc: 0.6863 - val_loss: 0.7059 - val_acc: 0.7906\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9676 - acc: 0.6937\n",
      "Epoch 00089: val_loss improved from 0.70322 to 0.69885, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_3_conv_checkpoint/089-0.6988.hdf5\n",
      "36805/36805 [==============================] - 31s 840us/sample - loss: 0.9675 - acc: 0.6937 - val_loss: 0.6988 - val_acc: 0.7859\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9658 - acc: 0.6938\n",
      "Epoch 00090: val_loss improved from 0.69885 to 0.69599, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_3_conv_checkpoint/090-0.6960.hdf5\n",
      "36805/36805 [==============================] - 31s 840us/sample - loss: 0.9659 - acc: 0.6938 - val_loss: 0.6960 - val_acc: 0.7892\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9721 - acc: 0.6905\n",
      "Epoch 00091: val_loss did not improve from 0.69599\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.9722 - acc: 0.6904 - val_loss: 0.7002 - val_acc: 0.7918\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9739 - acc: 0.6888\n",
      "Epoch 00092: val_loss improved from 0.69599 to 0.69497, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_3_conv_checkpoint/092-0.6950.hdf5\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.9738 - acc: 0.6888 - val_loss: 0.6950 - val_acc: 0.7920\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9668 - acc: 0.6917\n",
      "Epoch 00093: val_loss did not improve from 0.69497\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.9669 - acc: 0.6917 - val_loss: 0.6953 - val_acc: 0.7878\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9677 - acc: 0.6922\n",
      "Epoch 00094: val_loss did not improve from 0.69497\n",
      "36805/36805 [==============================] - 31s 835us/sample - loss: 0.9680 - acc: 0.6921 - val_loss: 0.6963 - val_acc: 0.7920\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9631 - acc: 0.6932\n",
      "Epoch 00095: val_loss improved from 0.69497 to 0.68498, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_3_conv_checkpoint/095-0.6850.hdf5\n",
      "36805/36805 [==============================] - 31s 835us/sample - loss: 0.9631 - acc: 0.6932 - val_loss: 0.6850 - val_acc: 0.7950\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9627 - acc: 0.6947\n",
      "Epoch 00096: val_loss did not improve from 0.68498\n",
      "36805/36805 [==============================] - 31s 836us/sample - loss: 0.9627 - acc: 0.6947 - val_loss: 0.6883 - val_acc: 0.7952\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9580 - acc: 0.6924\n",
      "Epoch 00097: val_loss did not improve from 0.68498\n",
      "36805/36805 [==============================] - 31s 835us/sample - loss: 0.9581 - acc: 0.6924 - val_loss: 0.6892 - val_acc: 0.7939\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9540 - acc: 0.6957\n",
      "Epoch 00098: val_loss did not improve from 0.68498\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.9540 - acc: 0.6957 - val_loss: 0.6880 - val_acc: 0.7925\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9567 - acc: 0.6933\n",
      "Epoch 00099: val_loss did not improve from 0.68498\n",
      "36805/36805 [==============================] - 31s 836us/sample - loss: 0.9566 - acc: 0.6933 - val_loss: 0.6912 - val_acc: 0.7983\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9560 - acc: 0.6939\n",
      "Epoch 00100: val_loss improved from 0.68498 to 0.67582, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_3_conv_checkpoint/100-0.6758.hdf5\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.9560 - acc: 0.6939 - val_loss: 0.6758 - val_acc: 0.7980\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9481 - acc: 0.6953\n",
      "Epoch 00101: val_loss did not improve from 0.67582\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.9482 - acc: 0.6952 - val_loss: 0.6890 - val_acc: 0.7973\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9463 - acc: 0.6979\n",
      "Epoch 00102: val_loss did not improve from 0.67582\n",
      "36805/36805 [==============================] - 31s 836us/sample - loss: 0.9464 - acc: 0.6979 - val_loss: 0.6879 - val_acc: 0.7950\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9520 - acc: 0.6951\n",
      "Epoch 00103: val_loss did not improve from 0.67582\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.9522 - acc: 0.6951 - val_loss: 0.6788 - val_acc: 0.8013\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9541 - acc: 0.6978\n",
      "Epoch 00104: val_loss improved from 0.67582 to 0.67242, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_3_conv_checkpoint/104-0.6724.hdf5\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.9541 - acc: 0.6978 - val_loss: 0.6724 - val_acc: 0.7971\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9492 - acc: 0.6971\n",
      "Epoch 00105: val_loss improved from 0.67242 to 0.67120, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_3_conv_checkpoint/105-0.6712.hdf5\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.9492 - acc: 0.6971 - val_loss: 0.6712 - val_acc: 0.7969\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9416 - acc: 0.6995\n",
      "Epoch 00106: val_loss did not improve from 0.67120\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.9417 - acc: 0.6994 - val_loss: 0.6741 - val_acc: 0.8020\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9470 - acc: 0.6997\n",
      "Epoch 00107: val_loss improved from 0.67120 to 0.66811, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_3_conv_checkpoint/107-0.6681.hdf5\n",
      "36805/36805 [==============================] - 31s 836us/sample - loss: 0.9470 - acc: 0.6997 - val_loss: 0.6681 - val_acc: 0.8015\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9428 - acc: 0.6998\n",
      "Epoch 00108: val_loss improved from 0.66811 to 0.66577, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_3_conv_checkpoint/108-0.6658.hdf5\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.9429 - acc: 0.6997 - val_loss: 0.6658 - val_acc: 0.8020\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9358 - acc: 0.7012\n",
      "Epoch 00109: val_loss did not improve from 0.66577\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.9359 - acc: 0.7011 - val_loss: 0.6696 - val_acc: 0.8022\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9310 - acc: 0.7050\n",
      "Epoch 00110: val_loss did not improve from 0.66577\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.9311 - acc: 0.7050 - val_loss: 0.6781 - val_acc: 0.8029\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9362 - acc: 0.6995\n",
      "Epoch 00111: val_loss improved from 0.66577 to 0.66192, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_3_conv_checkpoint/111-0.6619.hdf5\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.9362 - acc: 0.6995 - val_loss: 0.6619 - val_acc: 0.8036\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9356 - acc: 0.7042\n",
      "Epoch 00112: val_loss did not improve from 0.66192\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.9357 - acc: 0.7042 - val_loss: 0.6673 - val_acc: 0.8050\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9406 - acc: 0.7031\n",
      "Epoch 00113: val_loss improved from 0.66192 to 0.65869, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_3_conv_checkpoint/113-0.6587.hdf5\n",
      "36805/36805 [==============================] - 31s 841us/sample - loss: 0.9406 - acc: 0.7031 - val_loss: 0.6587 - val_acc: 0.8055\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9292 - acc: 0.7040\n",
      "Epoch 00114: val_loss improved from 0.65869 to 0.65233, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_3_conv_checkpoint/114-0.6523.hdf5\n",
      "36805/36805 [==============================] - 31s 842us/sample - loss: 0.9293 - acc: 0.7041 - val_loss: 0.6523 - val_acc: 0.8099\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9231 - acc: 0.7063\n",
      "Epoch 00115: val_loss improved from 0.65233 to 0.65084, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_3_conv_checkpoint/115-0.6508.hdf5\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.9232 - acc: 0.7062 - val_loss: 0.6508 - val_acc: 0.8081\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9295 - acc: 0.7064\n",
      "Epoch 00116: val_loss improved from 0.65084 to 0.64961, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_3_conv_checkpoint/116-0.6496.hdf5\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.9296 - acc: 0.7064 - val_loss: 0.6496 - val_acc: 0.8092\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9266 - acc: 0.7045\n",
      "Epoch 00117: val_loss did not improve from 0.64961\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.9266 - acc: 0.7045 - val_loss: 0.6516 - val_acc: 0.8111\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9204 - acc: 0.7094\n",
      "Epoch 00118: val_loss improved from 0.64961 to 0.64452, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_3_conv_checkpoint/118-0.6445.hdf5\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.9204 - acc: 0.7094 - val_loss: 0.6445 - val_acc: 0.8137\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9220 - acc: 0.7074\n",
      "Epoch 00119: val_loss did not improve from 0.64452\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.9221 - acc: 0.7074 - val_loss: 0.6452 - val_acc: 0.8130\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9198 - acc: 0.7084\n",
      "Epoch 00120: val_loss improved from 0.64452 to 0.64186, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_3_conv_checkpoint/120-0.6419.hdf5\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.9197 - acc: 0.7085 - val_loss: 0.6419 - val_acc: 0.8125\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9182 - acc: 0.7083\n",
      "Epoch 00121: val_loss improved from 0.64186 to 0.63909, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_3_conv_checkpoint/121-0.6391.hdf5\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.9182 - acc: 0.7083 - val_loss: 0.6391 - val_acc: 0.8137\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9197 - acc: 0.7079\n",
      "Epoch 00122: val_loss did not improve from 0.63909\n",
      "36805/36805 [==============================] - 31s 834us/sample - loss: 0.9196 - acc: 0.7079 - val_loss: 0.6486 - val_acc: 0.8109\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9136 - acc: 0.7096\n",
      "Epoch 00123: val_loss did not improve from 0.63909\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.9135 - acc: 0.7096 - val_loss: 0.6443 - val_acc: 0.8125\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9148 - acc: 0.7100\n",
      "Epoch 00124: val_loss improved from 0.63909 to 0.63864, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_3_conv_checkpoint/124-0.6386.hdf5\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.9147 - acc: 0.7100 - val_loss: 0.6386 - val_acc: 0.8120\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9142 - acc: 0.7108\n",
      "Epoch 00125: val_loss did not improve from 0.63864\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.9142 - acc: 0.7107 - val_loss: 0.6456 - val_acc: 0.8148\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9024 - acc: 0.7132\n",
      "Epoch 00126: val_loss improved from 0.63864 to 0.63408, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_3_conv_checkpoint/126-0.6341.hdf5\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.9024 - acc: 0.7132 - val_loss: 0.6341 - val_acc: 0.8109\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9073 - acc: 0.7130\n",
      "Epoch 00127: val_loss improved from 0.63408 to 0.63265, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_3_conv_checkpoint/127-0.6326.hdf5\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.9072 - acc: 0.7130 - val_loss: 0.6326 - val_acc: 0.8141\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9075 - acc: 0.7103\n",
      "Epoch 00128: val_loss improved from 0.63265 to 0.62963, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_3_conv_checkpoint/128-0.6296.hdf5\n",
      "36805/36805 [==============================] - 31s 842us/sample - loss: 0.9076 - acc: 0.7103 - val_loss: 0.6296 - val_acc: 0.8176\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9157 - acc: 0.7117\n",
      "Epoch 00129: val_loss did not improve from 0.62963\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.9157 - acc: 0.7117 - val_loss: 0.6414 - val_acc: 0.8123\n",
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9097 - acc: 0.7107\n",
      "Epoch 00130: val_loss did not improve from 0.62963\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.9096 - acc: 0.7106 - val_loss: 0.6392 - val_acc: 0.8111\n",
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9012 - acc: 0.7150\n",
      "Epoch 00131: val_loss improved from 0.62963 to 0.62282, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_3_conv_checkpoint/131-0.6228.hdf5\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.9011 - acc: 0.7150 - val_loss: 0.6228 - val_acc: 0.8192\n",
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8978 - acc: 0.7162\n",
      "Epoch 00132: val_loss did not improve from 0.62282\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.8979 - acc: 0.7162 - val_loss: 0.6268 - val_acc: 0.8141\n",
      "Epoch 133/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8990 - acc: 0.7133\n",
      "Epoch 00133: val_loss did not improve from 0.62282\n",
      "36805/36805 [==============================] - 31s 841us/sample - loss: 0.8991 - acc: 0.7132 - val_loss: 0.6334 - val_acc: 0.8162\n",
      "Epoch 134/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8978 - acc: 0.7145\n",
      "Epoch 00134: val_loss did not improve from 0.62282\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.8979 - acc: 0.7144 - val_loss: 0.6304 - val_acc: 0.8192\n",
      "Epoch 135/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8951 - acc: 0.7169\n",
      "Epoch 00135: val_loss improved from 0.62282 to 0.62100, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_3_conv_checkpoint/135-0.6210.hdf5\n",
      "36805/36805 [==============================] - 31s 840us/sample - loss: 0.8951 - acc: 0.7169 - val_loss: 0.6210 - val_acc: 0.8160\n",
      "Epoch 136/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9029 - acc: 0.7158\n",
      "Epoch 00136: val_loss did not improve from 0.62100\n",
      "36805/36805 [==============================] - 31s 836us/sample - loss: 0.9029 - acc: 0.7158 - val_loss: 0.6266 - val_acc: 0.8139\n",
      "Epoch 137/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8974 - acc: 0.7163\n",
      "Epoch 00137: val_loss improved from 0.62100 to 0.62089, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_3_conv_checkpoint/137-0.6209.hdf5\n",
      "36805/36805 [==============================] - 31s 845us/sample - loss: 0.8973 - acc: 0.7163 - val_loss: 0.6209 - val_acc: 0.8188\n",
      "Epoch 138/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8940 - acc: 0.7157\n",
      "Epoch 00138: val_loss improved from 0.62089 to 0.62018, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_3_conv_checkpoint/138-0.6202.hdf5\n",
      "36805/36805 [==============================] - 31s 840us/sample - loss: 0.8940 - acc: 0.7156 - val_loss: 0.6202 - val_acc: 0.8178\n",
      "Epoch 139/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8926 - acc: 0.7162\n",
      "Epoch 00139: val_loss did not improve from 0.62018\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.8927 - acc: 0.7162 - val_loss: 0.6278 - val_acc: 0.8227\n",
      "Epoch 140/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8887 - acc: 0.7155\n",
      "Epoch 00140: val_loss improved from 0.62018 to 0.61504, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_3_conv_checkpoint/140-0.6150.hdf5\n",
      "36805/36805 [==============================] - 31s 844us/sample - loss: 0.8887 - acc: 0.7156 - val_loss: 0.6150 - val_acc: 0.8237\n",
      "Epoch 141/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8969 - acc: 0.7166\n",
      "Epoch 00141: val_loss improved from 0.61504 to 0.61497, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_3_conv_checkpoint/141-0.6150.hdf5\n",
      "36805/36805 [==============================] - 31s 840us/sample - loss: 0.8969 - acc: 0.7167 - val_loss: 0.6150 - val_acc: 0.8218\n",
      "Epoch 142/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8874 - acc: 0.7170\n",
      "Epoch 00142: val_loss did not improve from 0.61497\n",
      "36805/36805 [==============================] - 31s 840us/sample - loss: 0.8873 - acc: 0.7170 - val_loss: 0.6196 - val_acc: 0.8195\n",
      "Epoch 143/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8935 - acc: 0.7166\n",
      "Epoch 00143: val_loss did not improve from 0.61497\n",
      "36805/36805 [==============================] - 31s 841us/sample - loss: 0.8935 - acc: 0.7166 - val_loss: 0.6158 - val_acc: 0.8209\n",
      "Epoch 144/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8931 - acc: 0.7179\n",
      "Epoch 00144: val_loss did not improve from 0.61497\n",
      "36805/36805 [==============================] - 31s 841us/sample - loss: 0.8931 - acc: 0.7178 - val_loss: 0.6227 - val_acc: 0.8157\n",
      "Epoch 145/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8844 - acc: 0.7173\n",
      "Epoch 00145: val_loss improved from 0.61497 to 0.61195, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_3_conv_checkpoint/145-0.6119.hdf5\n",
      "36805/36805 [==============================] - 31s 845us/sample - loss: 0.8844 - acc: 0.7173 - val_loss: 0.6119 - val_acc: 0.8199\n",
      "Epoch 146/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8863 - acc: 0.7172\n",
      "Epoch 00146: val_loss did not improve from 0.61195\n",
      "36805/36805 [==============================] - 31s 840us/sample - loss: 0.8863 - acc: 0.7172 - val_loss: 0.6183 - val_acc: 0.8220\n",
      "Epoch 147/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8804 - acc: 0.7211\n",
      "Epoch 00147: val_loss did not improve from 0.61195\n",
      "36805/36805 [==============================] - 31s 840us/sample - loss: 0.8805 - acc: 0.7210 - val_loss: 0.6153 - val_acc: 0.8241\n",
      "Epoch 148/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8835 - acc: 0.7183\n",
      "Epoch 00148: val_loss did not improve from 0.61195\n",
      "36805/36805 [==============================] - 31s 843us/sample - loss: 0.8834 - acc: 0.7184 - val_loss: 0.6167 - val_acc: 0.8218\n",
      "Epoch 149/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8819 - acc: 0.7201\n",
      "Epoch 00149: val_loss did not improve from 0.61195\n",
      "36805/36805 [==============================] - 31s 843us/sample - loss: 0.8819 - acc: 0.7201 - val_loss: 0.6201 - val_acc: 0.8213\n",
      "Epoch 150/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8767 - acc: 0.7193\n",
      "Epoch 00150: val_loss did not improve from 0.61195\n",
      "36805/36805 [==============================] - 31s 844us/sample - loss: 0.8768 - acc: 0.7193 - val_loss: 0.6148 - val_acc: 0.8241\n",
      "Epoch 151/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8814 - acc: 0.7205\n",
      "Epoch 00151: val_loss did not improve from 0.61195\n",
      "36805/36805 [==============================] - 31s 840us/sample - loss: 0.8816 - acc: 0.7205 - val_loss: 0.6139 - val_acc: 0.8253\n",
      "Epoch 152/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8774 - acc: 0.7210\n",
      "Epoch 00152: val_loss improved from 0.61195 to 0.60952, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_3_conv_checkpoint/152-0.6095.hdf5\n",
      "36805/36805 [==============================] - 31s 840us/sample - loss: 0.8773 - acc: 0.7210 - val_loss: 0.6095 - val_acc: 0.8262\n",
      "Epoch 153/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8801 - acc: 0.7223\n",
      "Epoch 00153: val_loss improved from 0.60952 to 0.59498, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_3_conv_checkpoint/153-0.5950.hdf5\n",
      "36805/36805 [==============================] - 31s 843us/sample - loss: 0.8802 - acc: 0.7222 - val_loss: 0.5950 - val_acc: 0.8288\n",
      "Epoch 154/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8814 - acc: 0.7195\n",
      "Epoch 00154: val_loss did not improve from 0.59498\n",
      "36805/36805 [==============================] - 31s 840us/sample - loss: 0.8813 - acc: 0.7195 - val_loss: 0.6077 - val_acc: 0.8244\n",
      "Epoch 155/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8802 - acc: 0.7195\n",
      "Epoch 00155: val_loss did not improve from 0.59498\n",
      "36805/36805 [==============================] - 31s 841us/sample - loss: 0.8802 - acc: 0.7195 - val_loss: 0.6085 - val_acc: 0.8241\n",
      "Epoch 156/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8770 - acc: 0.7212\n",
      "Epoch 00156: val_loss did not improve from 0.59498\n",
      "36805/36805 [==============================] - 31s 841us/sample - loss: 0.8771 - acc: 0.7212 - val_loss: 0.6004 - val_acc: 0.8269\n",
      "Epoch 157/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8684 - acc: 0.7238\n",
      "Epoch 00157: val_loss did not improve from 0.59498\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.8684 - acc: 0.7238 - val_loss: 0.6036 - val_acc: 0.8279\n",
      "Epoch 158/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8745 - acc: 0.7203\n",
      "Epoch 00158: val_loss did not improve from 0.59498\n",
      "36805/36805 [==============================] - 31s 842us/sample - loss: 0.8744 - acc: 0.7203 - val_loss: 0.6115 - val_acc: 0.8281\n",
      "Epoch 159/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8712 - acc: 0.7209\n",
      "Epoch 00159: val_loss did not improve from 0.59498\n",
      "36805/36805 [==============================] - 31s 840us/sample - loss: 0.8712 - acc: 0.7209 - val_loss: 0.5989 - val_acc: 0.8314\n",
      "Epoch 160/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8679 - acc: 0.7227\n",
      "Epoch 00160: val_loss improved from 0.59498 to 0.59158, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_3_conv_checkpoint/160-0.5916.hdf5\n",
      "36805/36805 [==============================] - 31s 843us/sample - loss: 0.8681 - acc: 0.7227 - val_loss: 0.5916 - val_acc: 0.8307\n",
      "Epoch 161/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8734 - acc: 0.7214\n",
      "Epoch 00161: val_loss did not improve from 0.59158\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.8733 - acc: 0.7214 - val_loss: 0.5943 - val_acc: 0.8265\n",
      "Epoch 162/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8732 - acc: 0.7225\n",
      "Epoch 00162: val_loss did not improve from 0.59158\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.8732 - acc: 0.7225 - val_loss: 0.6022 - val_acc: 0.8262\n",
      "Epoch 163/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8603 - acc: 0.7260\n",
      "Epoch 00163: val_loss did not improve from 0.59158\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.8602 - acc: 0.7260 - val_loss: 0.6003 - val_acc: 0.8300\n",
      "Epoch 164/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8640 - acc: 0.7257\n",
      "Epoch 00164: val_loss did not improve from 0.59158\n",
      "36805/36805 [==============================] - 31s 835us/sample - loss: 0.8640 - acc: 0.7257 - val_loss: 0.5957 - val_acc: 0.8276\n",
      "Epoch 165/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8740 - acc: 0.7229\n",
      "Epoch 00165: val_loss did not improve from 0.59158\n",
      "36805/36805 [==============================] - 31s 841us/sample - loss: 0.8740 - acc: 0.7229 - val_loss: 0.5920 - val_acc: 0.8307\n",
      "Epoch 166/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8645 - acc: 0.7261\n",
      "Epoch 00166: val_loss did not improve from 0.59158\n",
      "36805/36805 [==============================] - 31s 834us/sample - loss: 0.8645 - acc: 0.7261 - val_loss: 0.5975 - val_acc: 0.8290\n",
      "Epoch 167/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8710 - acc: 0.7232\n",
      "Epoch 00167: val_loss did not improve from 0.59158\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.8709 - acc: 0.7232 - val_loss: 0.5986 - val_acc: 0.8316\n",
      "Epoch 168/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8657 - acc: 0.7262\n",
      "Epoch 00168: val_loss did not improve from 0.59158\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.8656 - acc: 0.7263 - val_loss: 0.5938 - val_acc: 0.8297\n",
      "Epoch 169/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8625 - acc: 0.7265\n",
      "Epoch 00169: val_loss did not improve from 0.59158\n",
      "36805/36805 [==============================] - 31s 836us/sample - loss: 0.8625 - acc: 0.7265 - val_loss: 0.5928 - val_acc: 0.8283\n",
      "Epoch 170/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8638 - acc: 0.7269\n",
      "Epoch 00170: val_loss did not improve from 0.59158\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.8638 - acc: 0.7269 - val_loss: 0.5948 - val_acc: 0.8302\n",
      "Epoch 171/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8594 - acc: 0.7261\n",
      "Epoch 00171: val_loss improved from 0.59158 to 0.58763, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_3_conv_checkpoint/171-0.5876.hdf5\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.8593 - acc: 0.7261 - val_loss: 0.5876 - val_acc: 0.8316\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 172/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8613 - acc: 0.7267\n",
      "Epoch 00172: val_loss did not improve from 0.58763\n",
      "36805/36805 [==============================] - 31s 840us/sample - loss: 0.8613 - acc: 0.7267 - val_loss: 0.5909 - val_acc: 0.8304\n",
      "Epoch 173/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8604 - acc: 0.7255\n",
      "Epoch 00173: val_loss did not improve from 0.58763\n",
      "36805/36805 [==============================] - 31s 842us/sample - loss: 0.8604 - acc: 0.7255 - val_loss: 0.5907 - val_acc: 0.8344\n",
      "Epoch 174/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8581 - acc: 0.7274\n",
      "Epoch 00174: val_loss did not improve from 0.58763\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.8581 - acc: 0.7274 - val_loss: 0.5882 - val_acc: 0.8314\n",
      "Epoch 175/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8671 - acc: 0.7252\n",
      "Epoch 00175: val_loss improved from 0.58763 to 0.58187, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_3_conv_checkpoint/175-0.5819.hdf5\n",
      "36805/36805 [==============================] - 31s 840us/sample - loss: 0.8671 - acc: 0.7253 - val_loss: 0.5819 - val_acc: 0.8330\n",
      "Epoch 176/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8543 - acc: 0.7286\n",
      "Epoch 00176: val_loss did not improve from 0.58187\n",
      "36805/36805 [==============================] - 31s 836us/sample - loss: 0.8543 - acc: 0.7285 - val_loss: 0.5827 - val_acc: 0.8372\n",
      "Epoch 177/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8591 - acc: 0.7274\n",
      "Epoch 00177: val_loss did not improve from 0.58187\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.8591 - acc: 0.7274 - val_loss: 0.5908 - val_acc: 0.8330\n",
      "Epoch 178/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8503 - acc: 0.7286\n",
      "Epoch 00178: val_loss did not improve from 0.58187\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.8504 - acc: 0.7286 - val_loss: 0.5924 - val_acc: 0.8328\n",
      "Epoch 179/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8553 - acc: 0.7290\n",
      "Epoch 00179: val_loss did not improve from 0.58187\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.8552 - acc: 0.7291 - val_loss: 0.5936 - val_acc: 0.8330\n",
      "Epoch 180/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8541 - acc: 0.7276\n",
      "Epoch 00180: val_loss improved from 0.58187 to 0.57817, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_3_conv_checkpoint/180-0.5782.hdf5\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.8540 - acc: 0.7276 - val_loss: 0.5782 - val_acc: 0.8381\n",
      "Epoch 181/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8524 - acc: 0.7310\n",
      "Epoch 00181: val_loss did not improve from 0.57817\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.8524 - acc: 0.7309 - val_loss: 0.5792 - val_acc: 0.8353\n",
      "Epoch 182/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8528 - acc: 0.7275\n",
      "Epoch 00182: val_loss did not improve from 0.57817\n",
      "36805/36805 [==============================] - 31s 840us/sample - loss: 0.8529 - acc: 0.7274 - val_loss: 0.5905 - val_acc: 0.8318\n",
      "Epoch 183/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8520 - acc: 0.7308\n",
      "Epoch 00183: val_loss did not improve from 0.57817\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.8520 - acc: 0.7308 - val_loss: 0.5956 - val_acc: 0.8307\n",
      "Epoch 184/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8484 - acc: 0.7307\n",
      "Epoch 00184: val_loss did not improve from 0.57817\n",
      "36805/36805 [==============================] - 31s 840us/sample - loss: 0.8484 - acc: 0.7307 - val_loss: 0.5865 - val_acc: 0.8309\n",
      "Epoch 185/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8549 - acc: 0.7265\n",
      "Epoch 00185: val_loss did not improve from 0.57817\n",
      "36805/36805 [==============================] - 31s 836us/sample - loss: 0.8549 - acc: 0.7265 - val_loss: 0.5940 - val_acc: 0.8260\n",
      "Epoch 186/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8552 - acc: 0.7283\n",
      "Epoch 00186: val_loss improved from 0.57817 to 0.57728, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_3_conv_checkpoint/186-0.5773.hdf5\n",
      "36805/36805 [==============================] - 31s 840us/sample - loss: 0.8551 - acc: 0.7284 - val_loss: 0.5773 - val_acc: 0.8355\n",
      "Epoch 187/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8505 - acc: 0.7275\n",
      "Epoch 00187: val_loss did not improve from 0.57728\n",
      "36805/36805 [==============================] - 31s 836us/sample - loss: 0.8506 - acc: 0.7275 - val_loss: 0.5881 - val_acc: 0.8316\n",
      "Epoch 188/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8533 - acc: 0.7290\n",
      "Epoch 00188: val_loss improved from 0.57728 to 0.57665, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_3_conv_checkpoint/188-0.5767.hdf5\n",
      "36805/36805 [==============================] - 31s 841us/sample - loss: 0.8534 - acc: 0.7290 - val_loss: 0.5767 - val_acc: 0.8334\n",
      "Epoch 189/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8454 - acc: 0.7305\n",
      "Epoch 00189: val_loss improved from 0.57665 to 0.57119, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_3_conv_checkpoint/189-0.5712.hdf5\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.8453 - acc: 0.7306 - val_loss: 0.5712 - val_acc: 0.8372\n",
      "Epoch 190/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8527 - acc: 0.7283\n",
      "Epoch 00190: val_loss did not improve from 0.57119\n",
      "36805/36805 [==============================] - 31s 836us/sample - loss: 0.8527 - acc: 0.7283 - val_loss: 0.5845 - val_acc: 0.8355\n",
      "Epoch 191/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8460 - acc: 0.7305\n",
      "Epoch 00191: val_loss did not improve from 0.57119\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.8459 - acc: 0.7305 - val_loss: 0.5797 - val_acc: 0.8353\n",
      "Epoch 192/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8476 - acc: 0.7297\n",
      "Epoch 00192: val_loss did not improve from 0.57119\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.8475 - acc: 0.7297 - val_loss: 0.5834 - val_acc: 0.8325\n",
      "Epoch 193/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8432 - acc: 0.7306\n",
      "Epoch 00193: val_loss did not improve from 0.57119\n",
      "36805/36805 [==============================] - 31s 833us/sample - loss: 0.8434 - acc: 0.7306 - val_loss: 0.5854 - val_acc: 0.8325\n",
      "Epoch 194/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8439 - acc: 0.7312\n",
      "Epoch 00194: val_loss did not improve from 0.57119\n",
      "36805/36805 [==============================] - 31s 840us/sample - loss: 0.8438 - acc: 0.7312 - val_loss: 0.5795 - val_acc: 0.8383\n",
      "Epoch 195/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8447 - acc: 0.7318\n",
      "Epoch 00195: val_loss did not improve from 0.57119\n",
      "36805/36805 [==============================] - 31s 840us/sample - loss: 0.8449 - acc: 0.7317 - val_loss: 0.5756 - val_acc: 0.8374\n",
      "Epoch 196/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8434 - acc: 0.7325\n",
      "Epoch 00196: val_loss did not improve from 0.57119\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.8434 - acc: 0.7325 - val_loss: 0.5844 - val_acc: 0.8307\n",
      "Epoch 197/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8454 - acc: 0.7317\n",
      "Epoch 00197: val_loss did not improve from 0.57119\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.8454 - acc: 0.7317 - val_loss: 0.5786 - val_acc: 0.8337\n",
      "Epoch 198/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8476 - acc: 0.7324\n",
      "Epoch 00198: val_loss did not improve from 0.57119\n",
      "36805/36805 [==============================] - 31s 836us/sample - loss: 0.8475 - acc: 0.7324 - val_loss: 0.5812 - val_acc: 0.8346\n",
      "Epoch 199/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8427 - acc: 0.7312\n",
      "Epoch 00199: val_loss did not improve from 0.57119\n",
      "36805/36805 [==============================] - 31s 840us/sample - loss: 0.8426 - acc: 0.7312 - val_loss: 0.5802 - val_acc: 0.8332\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 200/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8453 - acc: 0.7316\n",
      "Epoch 00200: val_loss did not improve from 0.57119\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.8453 - acc: 0.7316 - val_loss: 0.5730 - val_acc: 0.8376\n",
      "Epoch 201/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8419 - acc: 0.7324\n",
      "Epoch 00201: val_loss did not improve from 0.57119\n",
      "36805/36805 [==============================] - 31s 841us/sample - loss: 0.8419 - acc: 0.7323 - val_loss: 0.5746 - val_acc: 0.8353\n",
      "Epoch 202/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8355 - acc: 0.7338\n",
      "Epoch 00202: val_loss improved from 0.57119 to 0.56709, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_3_conv_checkpoint/202-0.5671.hdf5\n",
      "36805/36805 [==============================] - 31s 840us/sample - loss: 0.8355 - acc: 0.7338 - val_loss: 0.5671 - val_acc: 0.8402\n",
      "Epoch 203/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8444 - acc: 0.7339\n",
      "Epoch 00203: val_loss did not improve from 0.56709\n",
      "36805/36805 [==============================] - 31s 840us/sample - loss: 0.8443 - acc: 0.7339 - val_loss: 0.5806 - val_acc: 0.8358\n",
      "Epoch 204/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8316 - acc: 0.7361\n",
      "Epoch 00204: val_loss did not improve from 0.56709\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.8315 - acc: 0.7361 - val_loss: 0.5840 - val_acc: 0.8344\n",
      "Epoch 205/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8366 - acc: 0.7335\n",
      "Epoch 00205: val_loss did not improve from 0.56709\n",
      "36805/36805 [==============================] - 31s 835us/sample - loss: 0.8367 - acc: 0.7335 - val_loss: 0.5702 - val_acc: 0.8360\n",
      "Epoch 206/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8412 - acc: 0.7330- ETA: 1s - \n",
      "Epoch 00206: val_loss did not improve from 0.56709\n",
      "36805/36805 [==============================] - 31s 840us/sample - loss: 0.8412 - acc: 0.7330 - val_loss: 0.5803 - val_acc: 0.8339\n",
      "Epoch 207/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8323 - acc: 0.7365\n",
      "Epoch 00207: val_loss did not improve from 0.56709\n",
      "36805/36805 [==============================] - 31s 851us/sample - loss: 0.8324 - acc: 0.7364 - val_loss: 0.5828 - val_acc: 0.8365\n",
      "Epoch 208/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8353 - acc: 0.7335\n",
      "Epoch 00208: val_loss did not improve from 0.56709\n",
      "36805/36805 [==============================] - 31s 841us/sample - loss: 0.8352 - acc: 0.7335 - val_loss: 0.5753 - val_acc: 0.8372\n",
      "Epoch 209/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8350 - acc: 0.7332\n",
      "Epoch 00209: val_loss did not improve from 0.56709\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.8349 - acc: 0.7332 - val_loss: 0.5684 - val_acc: 0.8393\n",
      "Epoch 210/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8341 - acc: 0.7348\n",
      "Epoch 00210: val_loss did not improve from 0.56709\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.8341 - acc: 0.7348 - val_loss: 0.5812 - val_acc: 0.8351\n",
      "Epoch 211/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8310 - acc: 0.7352\n",
      "Epoch 00211: val_loss did not improve from 0.56709\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.8310 - acc: 0.7352 - val_loss: 0.5717 - val_acc: 0.8351\n",
      "Epoch 212/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8307 - acc: 0.7370\n",
      "Epoch 00212: val_loss did not improve from 0.56709\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.8307 - acc: 0.7370 - val_loss: 0.5708 - val_acc: 0.8348\n",
      "Epoch 213/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8346 - acc: 0.7371\n",
      "Epoch 00213: val_loss did not improve from 0.56709\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.8345 - acc: 0.7371 - val_loss: 0.5746 - val_acc: 0.8381\n",
      "Epoch 214/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8303 - acc: 0.7379\n",
      "Epoch 00214: val_loss did not improve from 0.56709\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.8303 - acc: 0.7379 - val_loss: 0.5726 - val_acc: 0.8388\n",
      "Epoch 215/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8291 - acc: 0.7349\n",
      "Epoch 00215: val_loss did not improve from 0.56709\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.8291 - acc: 0.7349 - val_loss: 0.5700 - val_acc: 0.8388\n",
      "Epoch 216/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8317 - acc: 0.7367\n",
      "Epoch 00216: val_loss did not improve from 0.56709\n",
      "36805/36805 [==============================] - 31s 841us/sample - loss: 0.8317 - acc: 0.7366 - val_loss: 0.5743 - val_acc: 0.8325\n",
      "Epoch 217/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8310 - acc: 0.7360\n",
      "Epoch 00217: val_loss did not improve from 0.56709\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.8310 - acc: 0.7359 - val_loss: 0.5722 - val_acc: 0.8407\n",
      "Epoch 218/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8326 - acc: 0.7334\n",
      "Epoch 00218: val_loss did not improve from 0.56709\n",
      "36805/36805 [==============================] - 31s 836us/sample - loss: 0.8327 - acc: 0.7333 - val_loss: 0.5736 - val_acc: 0.8348\n",
      "Epoch 219/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8322 - acc: 0.7377\n",
      "Epoch 00219: val_loss did not improve from 0.56709\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.8321 - acc: 0.7377 - val_loss: 0.5692 - val_acc: 0.8383\n",
      "Epoch 220/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8265 - acc: 0.7373\n",
      "Epoch 00220: val_loss did not improve from 0.56709\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.8265 - acc: 0.7373 - val_loss: 0.5742 - val_acc: 0.8358\n",
      "Epoch 221/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8247 - acc: 0.7381\n",
      "Epoch 00221: val_loss did not improve from 0.56709\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.8247 - acc: 0.7381 - val_loss: 0.5716 - val_acc: 0.8390\n",
      "Epoch 222/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8303 - acc: 0.7350\n",
      "Epoch 00222: val_loss did not improve from 0.56709\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.8302 - acc: 0.7350 - val_loss: 0.5690 - val_acc: 0.8365\n",
      "Epoch 223/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8275 - acc: 0.7370\n",
      "Epoch 00223: val_loss improved from 0.56709 to 0.56562, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_3_conv_checkpoint/223-0.5656.hdf5\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.8275 - acc: 0.7370 - val_loss: 0.5656 - val_acc: 0.8416\n",
      "Epoch 224/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8255 - acc: 0.7372\n",
      "Epoch 00224: val_loss did not improve from 0.56562\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.8254 - acc: 0.7372 - val_loss: 0.5732 - val_acc: 0.8388\n",
      "Epoch 225/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8306 - acc: 0.7374\n",
      "Epoch 00225: val_loss did not improve from 0.56562\n",
      "36805/36805 [==============================] - 31s 836us/sample - loss: 0.8307 - acc: 0.7373 - val_loss: 0.5682 - val_acc: 0.8379\n",
      "Epoch 226/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8251 - acc: 0.7384\n",
      "Epoch 00226: val_loss did not improve from 0.56562\n",
      "36805/36805 [==============================] - 31s 840us/sample - loss: 0.8251 - acc: 0.7384 - val_loss: 0.5796 - val_acc: 0.8360\n",
      "Epoch 227/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8166 - acc: 0.7427\n",
      "Epoch 00227: val_loss did not improve from 0.56562\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.8166 - acc: 0.7428 - val_loss: 0.5669 - val_acc: 0.8409\n",
      "Epoch 228/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8181 - acc: 0.7395\n",
      "Epoch 00228: val_loss did not improve from 0.56562\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.8181 - acc: 0.7395 - val_loss: 0.5691 - val_acc: 0.8407\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 229/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8218 - acc: 0.7383\n",
      "Epoch 00229: val_loss did not improve from 0.56562\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.8219 - acc: 0.7383 - val_loss: 0.5681 - val_acc: 0.8379\n",
      "Epoch 230/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8200 - acc: 0.7405\n",
      "Epoch 00230: val_loss improved from 0.56562 to 0.56527, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_3_conv_checkpoint/230-0.5653.hdf5\n",
      "36805/36805 [==============================] - 31s 841us/sample - loss: 0.8199 - acc: 0.7406 - val_loss: 0.5653 - val_acc: 0.8402\n",
      "Epoch 231/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8206 - acc: 0.7377\n",
      "Epoch 00231: val_loss did not improve from 0.56527\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.8206 - acc: 0.7378 - val_loss: 0.5713 - val_acc: 0.8388\n",
      "Epoch 232/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8142 - acc: 0.7410\n",
      "Epoch 00232: val_loss did not improve from 0.56527\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.8142 - acc: 0.7410 - val_loss: 0.5727 - val_acc: 0.8365\n",
      "Epoch 233/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8257 - acc: 0.7390\n",
      "Epoch 00233: val_loss did not improve from 0.56527\n",
      "36805/36805 [==============================] - 31s 836us/sample - loss: 0.8257 - acc: 0.7390 - val_loss: 0.5694 - val_acc: 0.8404\n",
      "Epoch 234/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8178 - acc: 0.7417\n",
      "Epoch 00234: val_loss improved from 0.56527 to 0.56237, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_3_conv_checkpoint/234-0.5624.hdf5\n",
      "36805/36805 [==============================] - 31s 840us/sample - loss: 0.8179 - acc: 0.7416 - val_loss: 0.5624 - val_acc: 0.8435\n",
      "Epoch 235/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8236 - acc: 0.7389\n",
      "Epoch 00235: val_loss improved from 0.56237 to 0.55613, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_3_conv_checkpoint/235-0.5561.hdf5\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.8236 - acc: 0.7389 - val_loss: 0.5561 - val_acc: 0.8402\n",
      "Epoch 236/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8159 - acc: 0.7421\n",
      "Epoch 00236: val_loss did not improve from 0.55613\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.8159 - acc: 0.7421 - val_loss: 0.5778 - val_acc: 0.8367\n",
      "Epoch 237/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8119 - acc: 0.7455\n",
      "Epoch 00237: val_loss did not improve from 0.55613\n",
      "36805/36805 [==============================] - 31s 841us/sample - loss: 0.8119 - acc: 0.7455 - val_loss: 0.5606 - val_acc: 0.8423\n",
      "Epoch 238/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8123 - acc: 0.7410\n",
      "Epoch 00238: val_loss did not improve from 0.55613\n",
      "36805/36805 [==============================] - 31s 836us/sample - loss: 0.8123 - acc: 0.7410 - val_loss: 0.5636 - val_acc: 0.8383\n",
      "Epoch 239/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8220 - acc: 0.7407\n",
      "Epoch 00239: val_loss did not improve from 0.55613\n",
      "36805/36805 [==============================] - 31s 843us/sample - loss: 0.8219 - acc: 0.7407 - val_loss: 0.5801 - val_acc: 0.8330\n",
      "Epoch 240/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8170 - acc: 0.7389\n",
      "Epoch 00240: val_loss did not improve from 0.55613\n",
      "36805/36805 [==============================] - 31s 842us/sample - loss: 0.8170 - acc: 0.7388 - val_loss: 0.5654 - val_acc: 0.8411\n",
      "Epoch 241/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8196 - acc: 0.7396\n",
      "Epoch 00241: val_loss did not improve from 0.55613\n",
      "36805/36805 [==============================] - 31s 840us/sample - loss: 0.8197 - acc: 0.7396 - val_loss: 0.5641 - val_acc: 0.8437\n",
      "Epoch 242/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8115 - acc: 0.7378\n",
      "Epoch 00242: val_loss did not improve from 0.55613\n",
      "36805/36805 [==============================] - 31s 843us/sample - loss: 0.8115 - acc: 0.7378 - val_loss: 0.5681 - val_acc: 0.8358\n",
      "Epoch 243/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8140 - acc: 0.7413\n",
      "Epoch 00243: val_loss did not improve from 0.55613\n",
      "36805/36805 [==============================] - 31s 841us/sample - loss: 0.8139 - acc: 0.7414 - val_loss: 0.5579 - val_acc: 0.8435\n",
      "Epoch 244/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8150 - acc: 0.7416\n",
      "Epoch 00244: val_loss did not improve from 0.55613\n",
      "36805/36805 [==============================] - 31s 840us/sample - loss: 0.8150 - acc: 0.7416 - val_loss: 0.5689 - val_acc: 0.8388\n",
      "Epoch 245/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8132 - acc: 0.7413\n",
      "Epoch 00245: val_loss did not improve from 0.55613\n",
      "36805/36805 [==============================] - 31s 840us/sample - loss: 0.8131 - acc: 0.7413 - val_loss: 0.5623 - val_acc: 0.8407\n",
      "Epoch 246/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8166 - acc: 0.7399\n",
      "Epoch 00246: val_loss did not improve from 0.55613\n",
      "36805/36805 [==============================] - 31s 842us/sample - loss: 0.8166 - acc: 0.7400 - val_loss: 0.5679 - val_acc: 0.8416\n",
      "Epoch 247/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8114 - acc: 0.7427\n",
      "Epoch 00247: val_loss improved from 0.55613 to 0.55251, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_3_conv_checkpoint/247-0.5525.hdf5\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.8113 - acc: 0.7427 - val_loss: 0.5525 - val_acc: 0.8442\n",
      "Epoch 248/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8074 - acc: 0.7432\n",
      "Epoch 00248: val_loss did not improve from 0.55251\n",
      "36805/36805 [==============================] - 31s 840us/sample - loss: 0.8074 - acc: 0.7431 - val_loss: 0.5641 - val_acc: 0.8355\n",
      "Epoch 249/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8145 - acc: 0.7417\n",
      "Epoch 00249: val_loss did not improve from 0.55251\n",
      "36805/36805 [==============================] - 31s 842us/sample - loss: 0.8147 - acc: 0.7417 - val_loss: 0.5648 - val_acc: 0.8390\n",
      "Epoch 250/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8188 - acc: 0.7390\n",
      "Epoch 00250: val_loss did not improve from 0.55251\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.8188 - acc: 0.7391 - val_loss: 0.5597 - val_acc: 0.8437\n",
      "Epoch 251/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8086 - acc: 0.7443\n",
      "Epoch 00251: val_loss did not improve from 0.55251\n",
      "36805/36805 [==============================] - 31s 840us/sample - loss: 0.8086 - acc: 0.7443 - val_loss: 0.5629 - val_acc: 0.8390\n",
      "Epoch 252/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8144 - acc: 0.7438\n",
      "Epoch 00252: val_loss did not improve from 0.55251\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.8144 - acc: 0.7438 - val_loss: 0.5620 - val_acc: 0.8390\n",
      "Epoch 253/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8088 - acc: 0.7433\n",
      "Epoch 00253: val_loss did not improve from 0.55251\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.8088 - acc: 0.7433 - val_loss: 0.5541 - val_acc: 0.8437\n",
      "Epoch 254/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8060 - acc: 0.7463\n",
      "Epoch 00254: val_loss did not improve from 0.55251\n",
      "36805/36805 [==============================] - 31s 843us/sample - loss: 0.8061 - acc: 0.7463 - val_loss: 0.5628 - val_acc: 0.8388\n",
      "Epoch 255/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8072 - acc: 0.7429\n",
      "Epoch 00255: val_loss did not improve from 0.55251\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.8073 - acc: 0.7429 - val_loss: 0.5652 - val_acc: 0.8400\n",
      "Epoch 256/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8094 - acc: 0.7432\n",
      "Epoch 00256: val_loss did not improve from 0.55251\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.8093 - acc: 0.7432 - val_loss: 0.5540 - val_acc: 0.8458\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 257/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8004 - acc: 0.7463\n",
      "Epoch 00257: val_loss improved from 0.55251 to 0.54701, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_3_conv_checkpoint/257-0.5470.hdf5\n",
      "36805/36805 [==============================] - 31s 840us/sample - loss: 0.8004 - acc: 0.7463 - val_loss: 0.5470 - val_acc: 0.8463\n",
      "Epoch 258/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8031 - acc: 0.7445\n",
      "Epoch 00258: val_loss did not improve from 0.54701\n",
      "36805/36805 [==============================] - 31s 840us/sample - loss: 0.8031 - acc: 0.7445 - val_loss: 0.5521 - val_acc: 0.8446\n",
      "Epoch 259/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8072 - acc: 0.7447\n",
      "Epoch 00259: val_loss did not improve from 0.54701\n",
      "36805/36805 [==============================] - 31s 842us/sample - loss: 0.8072 - acc: 0.7447 - val_loss: 0.5574 - val_acc: 0.8444\n",
      "Epoch 260/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8071 - acc: 0.7426\n",
      "Epoch 00260: val_loss did not improve from 0.54701\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.8070 - acc: 0.7427 - val_loss: 0.5673 - val_acc: 0.8369\n",
      "Epoch 261/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8092 - acc: 0.7445\n",
      "Epoch 00261: val_loss did not improve from 0.54701\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.8092 - acc: 0.7445 - val_loss: 0.5585 - val_acc: 0.8402\n",
      "Epoch 262/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7968 - acc: 0.7470\n",
      "Epoch 00262: val_loss did not improve from 0.54701\n",
      "36805/36805 [==============================] - 31s 841us/sample - loss: 0.7969 - acc: 0.7470 - val_loss: 0.5617 - val_acc: 0.8395\n",
      "Epoch 263/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8082 - acc: 0.7454\n",
      "Epoch 00263: val_loss did not improve from 0.54701\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.8084 - acc: 0.7453 - val_loss: 0.5516 - val_acc: 0.8400\n",
      "Epoch 264/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8007 - acc: 0.7439\n",
      "Epoch 00264: val_loss did not improve from 0.54701\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.8006 - acc: 0.7439 - val_loss: 0.5570 - val_acc: 0.8432\n",
      "Epoch 265/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8060 - acc: 0.7444\n",
      "Epoch 00265: val_loss did not improve from 0.54701\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.8060 - acc: 0.7444 - val_loss: 0.5555 - val_acc: 0.8439\n",
      "Epoch 266/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8051 - acc: 0.7448\n",
      "Epoch 00266: val_loss did not improve from 0.54701\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.8054 - acc: 0.7447 - val_loss: 0.5653 - val_acc: 0.8339\n",
      "Epoch 267/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7995 - acc: 0.7472\n",
      "Epoch 00267: val_loss did not improve from 0.54701\n",
      "36805/36805 [==============================] - 31s 840us/sample - loss: 0.7994 - acc: 0.7472 - val_loss: 0.5583 - val_acc: 0.8409\n",
      "Epoch 268/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7981 - acc: 0.7470\n",
      "Epoch 00268: val_loss did not improve from 0.54701\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.7981 - acc: 0.7470 - val_loss: 0.5559 - val_acc: 0.8404\n",
      "Epoch 269/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7991 - acc: 0.7463\n",
      "Epoch 00269: val_loss did not improve from 0.54701\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.7991 - acc: 0.7463 - val_loss: 0.5598 - val_acc: 0.8353\n",
      "Epoch 270/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7996 - acc: 0.7467\n",
      "Epoch 00270: val_loss did not improve from 0.54701\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.7997 - acc: 0.7466 - val_loss: 0.5500 - val_acc: 0.8437\n",
      "Epoch 271/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7979 - acc: 0.7465\n",
      "Epoch 00271: val_loss did not improve from 0.54701\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.7978 - acc: 0.7465 - val_loss: 0.5586 - val_acc: 0.8430\n",
      "Epoch 272/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8036 - acc: 0.7455\n",
      "Epoch 00272: val_loss did not improve from 0.54701\n",
      "36805/36805 [==============================] - 31s 841us/sample - loss: 0.8036 - acc: 0.7455 - val_loss: 0.5472 - val_acc: 0.8451\n",
      "Epoch 273/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7981 - acc: 0.7471\n",
      "Epoch 00273: val_loss did not improve from 0.54701\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.7981 - acc: 0.7471 - val_loss: 0.5574 - val_acc: 0.8393\n",
      "Epoch 274/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8011 - acc: 0.7456\n",
      "Epoch 00274: val_loss did not improve from 0.54701\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.8011 - acc: 0.7456 - val_loss: 0.5529 - val_acc: 0.8430\n",
      "Epoch 275/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8033 - acc: 0.7458\n",
      "Epoch 00275: val_loss did not improve from 0.54701\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.8033 - acc: 0.7458 - val_loss: 0.5531 - val_acc: 0.8372\n",
      "Epoch 276/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8016 - acc: 0.7456\n",
      "Epoch 00276: val_loss did not improve from 0.54701\n",
      "36805/36805 [==============================] - 31s 841us/sample - loss: 0.8017 - acc: 0.7456 - val_loss: 0.5560 - val_acc: 0.8439\n",
      "Epoch 277/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8015 - acc: 0.7457\n",
      "Epoch 00277: val_loss did not improve from 0.54701\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.8016 - acc: 0.7457 - val_loss: 0.5606 - val_acc: 0.8397\n",
      "Epoch 278/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8015 - acc: 0.7480\n",
      "Epoch 00278: val_loss did not improve from 0.54701\n",
      "36805/36805 [==============================] - 31s 835us/sample - loss: 0.8015 - acc: 0.7480 - val_loss: 0.5522 - val_acc: 0.8409\n",
      "Epoch 279/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7948 - acc: 0.7468\n",
      "Epoch 00279: val_loss did not improve from 0.54701\n",
      "36805/36805 [==============================] - 31s 840us/sample - loss: 0.7947 - acc: 0.7469 - val_loss: 0.5481 - val_acc: 0.8435\n",
      "Epoch 280/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8017 - acc: 0.7439\n",
      "Epoch 00280: val_loss did not improve from 0.54701\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.8018 - acc: 0.7439 - val_loss: 0.5584 - val_acc: 0.8423\n",
      "Epoch 281/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7902 - acc: 0.7441\n",
      "Epoch 00281: val_loss did not improve from 0.54701\n",
      "36805/36805 [==============================] - 31s 836us/sample - loss: 0.7902 - acc: 0.7441 - val_loss: 0.5711 - val_acc: 0.8369\n",
      "Epoch 282/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7880 - acc: 0.7496\n",
      "Epoch 00282: val_loss did not improve from 0.54701\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.7880 - acc: 0.7496 - val_loss: 0.5545 - val_acc: 0.8418\n",
      "Epoch 283/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7977 - acc: 0.7478\n",
      "Epoch 00283: val_loss did not improve from 0.54701\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.7977 - acc: 0.7478 - val_loss: 0.5552 - val_acc: 0.8421\n",
      "Epoch 284/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7906 - acc: 0.7460\n",
      "Epoch 00284: val_loss did not improve from 0.54701\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.7907 - acc: 0.7460 - val_loss: 0.5476 - val_acc: 0.8425\n",
      "Epoch 285/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7926 - acc: 0.7471\n",
      "Epoch 00285: val_loss improved from 0.54701 to 0.54451, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_3_conv_checkpoint/285-0.5445.hdf5\n",
      "36805/36805 [==============================] - 31s 842us/sample - loss: 0.7925 - acc: 0.7471 - val_loss: 0.5445 - val_acc: 0.8444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 286/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7922 - acc: 0.7467\n",
      "Epoch 00286: val_loss did not improve from 0.54451\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.7922 - acc: 0.7467 - val_loss: 0.5647 - val_acc: 0.8407\n",
      "Epoch 287/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7969 - acc: 0.7468\n",
      "Epoch 00287: val_loss did not improve from 0.54451\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.7969 - acc: 0.7468 - val_loss: 0.5535 - val_acc: 0.8439\n",
      "Epoch 288/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7924 - acc: 0.7489\n",
      "Epoch 00288: val_loss did not improve from 0.54451\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.7924 - acc: 0.7489 - val_loss: 0.5517 - val_acc: 0.8423\n",
      "Epoch 289/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7932 - acc: 0.7466\n",
      "Epoch 00289: val_loss did not improve from 0.54451\n",
      "36805/36805 [==============================] - 31s 841us/sample - loss: 0.7931 - acc: 0.7467 - val_loss: 0.5513 - val_acc: 0.8421\n",
      "Epoch 290/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7949 - acc: 0.7470\n",
      "Epoch 00290: val_loss did not improve from 0.54451\n",
      "36805/36805 [==============================] - 31s 842us/sample - loss: 0.7949 - acc: 0.7470 - val_loss: 0.5641 - val_acc: 0.8421\n",
      "Epoch 291/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7885 - acc: 0.7511\n",
      "Epoch 00291: val_loss did not improve from 0.54451\n",
      "36805/36805 [==============================] - 31s 840us/sample - loss: 0.7884 - acc: 0.7511 - val_loss: 0.5523 - val_acc: 0.8423\n",
      "Epoch 292/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7896 - acc: 0.7485\n",
      "Epoch 00292: val_loss did not improve from 0.54451\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.7895 - acc: 0.7485 - val_loss: 0.5491 - val_acc: 0.8435\n",
      "Epoch 293/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7877 - acc: 0.7495\n",
      "Epoch 00293: val_loss did not improve from 0.54451\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.7878 - acc: 0.7494 - val_loss: 0.5480 - val_acc: 0.8446\n",
      "Epoch 294/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7919 - acc: 0.7484\n",
      "Epoch 00294: val_loss did not improve from 0.54451\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.7918 - acc: 0.7485 - val_loss: 0.5457 - val_acc: 0.8472\n",
      "Epoch 295/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7915 - acc: 0.7477\n",
      "Epoch 00295: val_loss did not improve from 0.54451\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.7915 - acc: 0.7477 - val_loss: 0.5597 - val_acc: 0.8435\n",
      "Epoch 296/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7906 - acc: 0.7490\n",
      "Epoch 00296: val_loss did not improve from 0.54451\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.7906 - acc: 0.7490 - val_loss: 0.5502 - val_acc: 0.8425\n",
      "Epoch 297/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7887 - acc: 0.7483\n",
      "Epoch 00297: val_loss did not improve from 0.54451\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.7888 - acc: 0.7483 - val_loss: 0.5635 - val_acc: 0.8369\n",
      "Epoch 298/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7944 - acc: 0.7471\n",
      "Epoch 00298: val_loss did not improve from 0.54451\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.7944 - acc: 0.7471 - val_loss: 0.5484 - val_acc: 0.8425\n",
      "Epoch 299/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7894 - acc: 0.7492\n",
      "Epoch 00299: val_loss did not improve from 0.54451\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.7893 - acc: 0.7492 - val_loss: 0.5500 - val_acc: 0.8428\n",
      "Epoch 300/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7861 - acc: 0.7506\n",
      "Epoch 00300: val_loss improved from 0.54451 to 0.54308, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_3_conv_checkpoint/300-0.5431.hdf5\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.7863 - acc: 0.7506 - val_loss: 0.5431 - val_acc: 0.8444\n",
      "Epoch 301/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7893 - acc: 0.7478\n",
      "Epoch 00301: val_loss did not improve from 0.54308\n",
      "36805/36805 [==============================] - 31s 842us/sample - loss: 0.7893 - acc: 0.7478 - val_loss: 0.5539 - val_acc: 0.8435\n",
      "Epoch 302/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7905 - acc: 0.7480\n",
      "Epoch 00302: val_loss did not improve from 0.54308\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.7905 - acc: 0.7480 - val_loss: 0.5497 - val_acc: 0.8451\n",
      "Epoch 303/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7872 - acc: 0.7498\n",
      "Epoch 00303: val_loss did not improve from 0.54308\n",
      "36805/36805 [==============================] - 31s 844us/sample - loss: 0.7871 - acc: 0.7498 - val_loss: 0.5456 - val_acc: 0.8449\n",
      "Epoch 304/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7944 - acc: 0.7473\n",
      "Epoch 00304: val_loss did not improve from 0.54308\n",
      "36805/36805 [==============================] - 31s 836us/sample - loss: 0.7944 - acc: 0.7473 - val_loss: 0.5485 - val_acc: 0.8428\n",
      "Epoch 305/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7903 - acc: 0.7487\n",
      "Epoch 00305: val_loss did not improve from 0.54308\n",
      "36805/36805 [==============================] - 31s 842us/sample - loss: 0.7905 - acc: 0.7486 - val_loss: 0.5473 - val_acc: 0.8421\n",
      "Epoch 306/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7877 - acc: 0.7488\n",
      "Epoch 00306: val_loss did not improve from 0.54308\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.7876 - acc: 0.7488 - val_loss: 0.5508 - val_acc: 0.8439\n",
      "Epoch 307/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7804 - acc: 0.7553\n",
      "Epoch 00307: val_loss did not improve from 0.54308\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.7803 - acc: 0.7553 - val_loss: 0.5451 - val_acc: 0.8404\n",
      "Epoch 308/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7804 - acc: 0.7535\n",
      "Epoch 00308: val_loss did not improve from 0.54308\n",
      "36805/36805 [==============================] - 31s 836us/sample - loss: 0.7803 - acc: 0.7535 - val_loss: 0.5523 - val_acc: 0.8416\n",
      "Epoch 309/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7826 - acc: 0.7507\n",
      "Epoch 00309: val_loss did not improve from 0.54308\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.7826 - acc: 0.7507 - val_loss: 0.5504 - val_acc: 0.8430\n",
      "Epoch 310/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7823 - acc: 0.7519\n",
      "Epoch 00310: val_loss did not improve from 0.54308\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.7823 - acc: 0.7519 - val_loss: 0.5468 - val_acc: 0.8397\n",
      "Epoch 311/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7839 - acc: 0.7510\n",
      "Epoch 00311: val_loss improved from 0.54308 to 0.53796, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_3_conv_checkpoint/311-0.5380.hdf5\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.7839 - acc: 0.7510 - val_loss: 0.5380 - val_acc: 0.8460\n",
      "Epoch 312/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7779 - acc: 0.7527\n",
      "Epoch 00312: val_loss did not improve from 0.53796\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.7778 - acc: 0.7527 - val_loss: 0.5382 - val_acc: 0.8458\n",
      "Epoch 313/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7850 - acc: 0.7499\n",
      "Epoch 00313: val_loss did not improve from 0.53796\n",
      "36805/36805 [==============================] - 31s 840us/sample - loss: 0.7852 - acc: 0.7499 - val_loss: 0.5503 - val_acc: 0.8430\n",
      "Epoch 314/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7846 - acc: 0.7512\n",
      "Epoch 00314: val_loss did not improve from 0.53796\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.7846 - acc: 0.7511 - val_loss: 0.5437 - val_acc: 0.8435\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 315/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7828 - acc: 0.7503\n",
      "Epoch 00315: val_loss did not improve from 0.53796\n",
      "36805/36805 [==============================] - 31s 841us/sample - loss: 0.7830 - acc: 0.7503 - val_loss: 0.5462 - val_acc: 0.8416\n",
      "Epoch 316/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7806 - acc: 0.7533\n",
      "Epoch 00316: val_loss did not improve from 0.53796\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.7806 - acc: 0.7533 - val_loss: 0.5545 - val_acc: 0.8442\n",
      "Epoch 317/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7863 - acc: 0.7493\n",
      "Epoch 00317: val_loss did not improve from 0.53796\n",
      "36805/36805 [==============================] - 31s 835us/sample - loss: 0.7863 - acc: 0.7493 - val_loss: 0.5460 - val_acc: 0.8467\n",
      "Epoch 318/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7814 - acc: 0.7516\n",
      "Epoch 00318: val_loss did not improve from 0.53796\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.7814 - acc: 0.7516 - val_loss: 0.5482 - val_acc: 0.8474\n",
      "Epoch 319/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7836 - acc: 0.7501\n",
      "Epoch 00319: val_loss did not improve from 0.53796\n",
      "36805/36805 [==============================] - 31s 840us/sample - loss: 0.7837 - acc: 0.7501 - val_loss: 0.5396 - val_acc: 0.8467\n",
      "Epoch 320/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7815 - acc: 0.7514\n",
      "Epoch 00320: val_loss did not improve from 0.53796\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.7815 - acc: 0.7514 - val_loss: 0.5450 - val_acc: 0.8437\n",
      "Epoch 321/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7811 - acc: 0.7513\n",
      "Epoch 00321: val_loss did not improve from 0.53796\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.7810 - acc: 0.7514 - val_loss: 0.5507 - val_acc: 0.8409\n",
      "Epoch 322/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7779 - acc: 0.7516\n",
      "Epoch 00322: val_loss did not improve from 0.53796\n",
      "36805/36805 [==============================] - 31s 840us/sample - loss: 0.7779 - acc: 0.7516 - val_loss: 0.5502 - val_acc: 0.8435\n",
      "Epoch 323/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7777 - acc: 0.7527\n",
      "Epoch 00323: val_loss did not improve from 0.53796\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.7776 - acc: 0.7528 - val_loss: 0.5509 - val_acc: 0.8409\n",
      "Epoch 324/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7785 - acc: 0.7531\n",
      "Epoch 00324: val_loss did not improve from 0.53796\n",
      "36805/36805 [==============================] - 31s 841us/sample - loss: 0.7785 - acc: 0.7531 - val_loss: 0.5478 - val_acc: 0.8442\n",
      "Epoch 325/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7736 - acc: 0.7520\n",
      "Epoch 00325: val_loss did not improve from 0.53796\n",
      "36805/36805 [==============================] - 31s 836us/sample - loss: 0.7736 - acc: 0.7520 - val_loss: 0.5419 - val_acc: 0.8484\n",
      "Epoch 326/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7798 - acc: 0.7515\n",
      "Epoch 00326: val_loss did not improve from 0.53796\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.7797 - acc: 0.7516 - val_loss: 0.5480 - val_acc: 0.8425\n",
      "Epoch 327/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7802 - acc: 0.7510\n",
      "Epoch 00327: val_loss did not improve from 0.53796\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.7801 - acc: 0.7510 - val_loss: 0.5465 - val_acc: 0.8432\n",
      "Epoch 328/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7775 - acc: 0.7554\n",
      "Epoch 00328: val_loss did not improve from 0.53796\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.7777 - acc: 0.7554 - val_loss: 0.5426 - val_acc: 0.8446\n",
      "Epoch 329/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7775 - acc: 0.7499\n",
      "Epoch 00329: val_loss did not improve from 0.53796\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.7774 - acc: 0.7499 - val_loss: 0.5508 - val_acc: 0.8397\n",
      "Epoch 330/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7706 - acc: 0.7532\n",
      "Epoch 00330: val_loss did not improve from 0.53796\n",
      "36805/36805 [==============================] - 31s 834us/sample - loss: 0.7705 - acc: 0.7531 - val_loss: 0.5431 - val_acc: 0.8467\n",
      "Epoch 331/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7780 - acc: 0.7530\n",
      "Epoch 00331: val_loss did not improve from 0.53796\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.7779 - acc: 0.7530 - val_loss: 0.5487 - val_acc: 0.8407\n",
      "Epoch 332/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7774 - acc: 0.7529\n",
      "Epoch 00332: val_loss improved from 0.53796 to 0.53477, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_3_conv_checkpoint/332-0.5348.hdf5\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.7774 - acc: 0.7529 - val_loss: 0.5348 - val_acc: 0.8432\n",
      "Epoch 333/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7725 - acc: 0.7546\n",
      "Epoch 00333: val_loss did not improve from 0.53477\n",
      "36805/36805 [==============================] - 31s 841us/sample - loss: 0.7725 - acc: 0.7546 - val_loss: 0.5484 - val_acc: 0.8451\n",
      "Epoch 334/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7746 - acc: 0.7544\n",
      "Epoch 00334: val_loss did not improve from 0.53477\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.7746 - acc: 0.7544 - val_loss: 0.5437 - val_acc: 0.8430\n",
      "Epoch 335/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7784 - acc: 0.7541\n",
      "Epoch 00335: val_loss did not improve from 0.53477\n",
      "36805/36805 [==============================] - 31s 841us/sample - loss: 0.7785 - acc: 0.7540 - val_loss: 0.5571 - val_acc: 0.8458\n",
      "Epoch 336/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7758 - acc: 0.7560\n",
      "Epoch 00336: val_loss did not improve from 0.53477\n",
      "36805/36805 [==============================] - 31s 836us/sample - loss: 0.7758 - acc: 0.7560 - val_loss: 0.5405 - val_acc: 0.8428\n",
      "Epoch 337/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7736 - acc: 0.7542\n",
      "Epoch 00337: val_loss did not improve from 0.53477\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.7737 - acc: 0.7541 - val_loss: 0.5494 - val_acc: 0.8411\n",
      "Epoch 338/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7737 - acc: 0.7521\n",
      "Epoch 00338: val_loss did not improve from 0.53477\n",
      "36805/36805 [==============================] - 31s 842us/sample - loss: 0.7737 - acc: 0.7522 - val_loss: 0.5475 - val_acc: 0.8437\n",
      "Epoch 339/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7715 - acc: 0.7549\n",
      "Epoch 00339: val_loss did not improve from 0.53477\n",
      "36805/36805 [==============================] - 31s 841us/sample - loss: 0.7714 - acc: 0.7550 - val_loss: 0.5363 - val_acc: 0.8474\n",
      "Epoch 340/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7704 - acc: 0.7578\n",
      "Epoch 00340: val_loss did not improve from 0.53477\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.7704 - acc: 0.7578 - val_loss: 0.5420 - val_acc: 0.8451\n",
      "Epoch 341/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7734 - acc: 0.7555\n",
      "Epoch 00341: val_loss did not improve from 0.53477\n",
      "36805/36805 [==============================] - 31s 842us/sample - loss: 0.7734 - acc: 0.7555 - val_loss: 0.5513 - val_acc: 0.8411\n",
      "Epoch 342/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7748 - acc: 0.7542\n",
      "Epoch 00342: val_loss did not improve from 0.53477\n",
      "36805/36805 [==============================] - 31s 840us/sample - loss: 0.7747 - acc: 0.7542 - val_loss: 0.5449 - val_acc: 0.8449\n",
      "Epoch 343/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7706 - acc: 0.7542\n",
      "Epoch 00343: val_loss did not improve from 0.53477\n",
      "36805/36805 [==============================] - 31s 840us/sample - loss: 0.7705 - acc: 0.7542 - val_loss: 0.5363 - val_acc: 0.8460\n",
      "Epoch 344/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7684 - acc: 0.7530\n",
      "Epoch 00344: val_loss did not improve from 0.53477\n",
      "36805/36805 [==============================] - 31s 843us/sample - loss: 0.7683 - acc: 0.7530 - val_loss: 0.5411 - val_acc: 0.8472\n",
      "Epoch 345/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7623 - acc: 0.7569\n",
      "Epoch 00345: val_loss did not improve from 0.53477\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.7622 - acc: 0.7570 - val_loss: 0.5418 - val_acc: 0.8458\n",
      "Epoch 346/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7718 - acc: 0.7537\n",
      "Epoch 00346: val_loss did not improve from 0.53477\n",
      "36805/36805 [==============================] - 31s 841us/sample - loss: 0.7718 - acc: 0.7537 - val_loss: 0.5379 - val_acc: 0.8474\n",
      "Epoch 347/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7657 - acc: 0.7548\n",
      "Epoch 00347: val_loss did not improve from 0.53477\n",
      "36805/36805 [==============================] - 31s 840us/sample - loss: 0.7657 - acc: 0.7548 - val_loss: 0.5388 - val_acc: 0.8437\n",
      "Epoch 348/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7641 - acc: 0.7576\n",
      "Epoch 00348: val_loss did not improve from 0.53477\n",
      "36805/36805 [==============================] - 31s 843us/sample - loss: 0.7641 - acc: 0.7576 - val_loss: 0.5387 - val_acc: 0.8460\n",
      "Epoch 349/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7650 - acc: 0.7584\n",
      "Epoch 00349: val_loss did not improve from 0.53477\n",
      "36805/36805 [==============================] - 31s 843us/sample - loss: 0.7650 - acc: 0.7584 - val_loss: 0.5422 - val_acc: 0.8416\n",
      "Epoch 350/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7676 - acc: 0.7561\n",
      "Epoch 00350: val_loss did not improve from 0.53477\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.7677 - acc: 0.7560 - val_loss: 0.5371 - val_acc: 0.8467\n",
      "Epoch 351/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7628 - acc: 0.7557\n",
      "Epoch 00351: val_loss improved from 0.53477 to 0.53379, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_3_conv_checkpoint/351-0.5338.hdf5\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.7627 - acc: 0.7557 - val_loss: 0.5338 - val_acc: 0.8460\n",
      "Epoch 352/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7609 - acc: 0.7584\n",
      "Epoch 00352: val_loss did not improve from 0.53379\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.7608 - acc: 0.7584 - val_loss: 0.5430 - val_acc: 0.8453\n",
      "Epoch 353/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7612 - acc: 0.7550\n",
      "Epoch 00353: val_loss did not improve from 0.53379\n",
      "36805/36805 [==============================] - 31s 841us/sample - loss: 0.7612 - acc: 0.7550 - val_loss: 0.5412 - val_acc: 0.8458\n",
      "Epoch 354/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7677 - acc: 0.7553\n",
      "Epoch 00354: val_loss did not improve from 0.53379\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.7677 - acc: 0.7553 - val_loss: 0.5659 - val_acc: 0.8404\n",
      "Epoch 355/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7634 - acc: 0.7561\n",
      "Epoch 00355: val_loss did not improve from 0.53379\n",
      "36805/36805 [==============================] - 31s 834us/sample - loss: 0.7634 - acc: 0.7561 - val_loss: 0.5343 - val_acc: 0.8451\n",
      "Epoch 356/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7660 - acc: 0.7556\n",
      "Epoch 00356: val_loss did not improve from 0.53379\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.7660 - acc: 0.7557 - val_loss: 0.5409 - val_acc: 0.8460\n",
      "Epoch 357/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7658 - acc: 0.7551\n",
      "Epoch 00357: val_loss did not improve from 0.53379\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.7659 - acc: 0.7551 - val_loss: 0.5494 - val_acc: 0.8465\n",
      "Epoch 358/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7709 - acc: 0.7532\n",
      "Epoch 00358: val_loss did not improve from 0.53379\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.7709 - acc: 0.7532 - val_loss: 0.5485 - val_acc: 0.8446\n",
      "Epoch 359/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7648 - acc: 0.7556\n",
      "Epoch 00359: val_loss did not improve from 0.53379\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.7648 - acc: 0.7556 - val_loss: 0.5344 - val_acc: 0.8470\n",
      "Epoch 360/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7632 - acc: 0.7568\n",
      "Epoch 00360: val_loss did not improve from 0.53379\n",
      "36805/36805 [==============================] - 31s 841us/sample - loss: 0.7632 - acc: 0.7568 - val_loss: 0.5378 - val_acc: 0.8458\n",
      "Epoch 361/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7596 - acc: 0.7567\n",
      "Epoch 00361: val_loss did not improve from 0.53379\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.7596 - acc: 0.7566 - val_loss: 0.5546 - val_acc: 0.8446\n",
      "Epoch 362/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7574 - acc: 0.7561\n",
      "Epoch 00362: val_loss did not improve from 0.53379\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.7575 - acc: 0.7561 - val_loss: 0.5420 - val_acc: 0.8446\n",
      "Epoch 363/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7631 - acc: 0.7546\n",
      "Epoch 00363: val_loss did not improve from 0.53379\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.7632 - acc: 0.7546 - val_loss: 0.5345 - val_acc: 0.8458\n",
      "Epoch 364/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7638 - acc: 0.7577\n",
      "Epoch 00364: val_loss did not improve from 0.53379\n",
      "36805/36805 [==============================] - 31s 836us/sample - loss: 0.7637 - acc: 0.7577 - val_loss: 0.5448 - val_acc: 0.8437\n",
      "Epoch 365/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7644 - acc: 0.7584\n",
      "Epoch 00365: val_loss did not improve from 0.53379\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.7643 - acc: 0.7584 - val_loss: 0.5479 - val_acc: 0.8423\n",
      "Epoch 366/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7585 - acc: 0.7564\n",
      "Epoch 00366: val_loss did not improve from 0.53379\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.7586 - acc: 0.7564 - val_loss: 0.5386 - val_acc: 0.8442\n",
      "Epoch 367/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7654 - acc: 0.7563\n",
      "Epoch 00367: val_loss did not improve from 0.53379\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.7653 - acc: 0.7563 - val_loss: 0.5422 - val_acc: 0.8423\n",
      "Epoch 368/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7601 - acc: 0.7585\n",
      "Epoch 00368: val_loss improved from 0.53379 to 0.53259, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_3_conv_checkpoint/368-0.5326.hdf5\n",
      "36805/36805 [==============================] - 31s 840us/sample - loss: 0.7601 - acc: 0.7586 - val_loss: 0.5326 - val_acc: 0.8470\n",
      "Epoch 369/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7591 - acc: 0.7597\n",
      "Epoch 00369: val_loss did not improve from 0.53259\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.7592 - acc: 0.7597 - val_loss: 0.5361 - val_acc: 0.8493\n",
      "Epoch 370/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7637 - acc: 0.7541\n",
      "Epoch 00370: val_loss did not improve from 0.53259\n",
      "36805/36805 [==============================] - 31s 841us/sample - loss: 0.7637 - acc: 0.7541 - val_loss: 0.5396 - val_acc: 0.8442\n",
      "Epoch 371/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7578 - acc: 0.7587\n",
      "Epoch 00371: val_loss did not improve from 0.53259\n",
      "36805/36805 [==============================] - 31s 835us/sample - loss: 0.7579 - acc: 0.7587 - val_loss: 0.5379 - val_acc: 0.8423\n",
      "Epoch 372/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7562 - acc: 0.7583\n",
      "Epoch 00372: val_loss did not improve from 0.53259\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.7562 - acc: 0.7583 - val_loss: 0.5493 - val_acc: 0.8463\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 373/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7619 - acc: 0.7584\n",
      "Epoch 00373: val_loss did not improve from 0.53259\n",
      "36805/36805 [==============================] - 31s 836us/sample - loss: 0.7620 - acc: 0.7584 - val_loss: 0.5353 - val_acc: 0.8472\n",
      "Epoch 374/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7615 - acc: 0.7560\n",
      "Epoch 00374: val_loss improved from 0.53259 to 0.52718, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_3_conv_checkpoint/374-0.5272.hdf5\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.7614 - acc: 0.7560 - val_loss: 0.5272 - val_acc: 0.8460\n",
      "Epoch 375/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7573 - acc: 0.7575\n",
      "Epoch 00375: val_loss did not improve from 0.52718\n",
      "36805/36805 [==============================] - 31s 841us/sample - loss: 0.7572 - acc: 0.7576 - val_loss: 0.5385 - val_acc: 0.8456\n",
      "Epoch 376/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7490 - acc: 0.7604\n",
      "Epoch 00376: val_loss did not improve from 0.52718\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.7490 - acc: 0.7604 - val_loss: 0.5447 - val_acc: 0.8435\n",
      "Epoch 377/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7587 - acc: 0.7589\n",
      "Epoch 00377: val_loss did not improve from 0.52718\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.7588 - acc: 0.7589 - val_loss: 0.5406 - val_acc: 0.8421\n",
      "Epoch 378/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7583 - acc: 0.7577\n",
      "Epoch 00378: val_loss did not improve from 0.52718\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.7583 - acc: 0.7577 - val_loss: 0.5378 - val_acc: 0.8416\n",
      "Epoch 379/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7526 - acc: 0.7595\n",
      "Epoch 00379: val_loss did not improve from 0.52718\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.7526 - acc: 0.7595 - val_loss: 0.5335 - val_acc: 0.8474\n",
      "Epoch 380/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7578 - acc: 0.7608\n",
      "Epoch 00380: val_loss did not improve from 0.52718\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.7578 - acc: 0.7608 - val_loss: 0.5350 - val_acc: 0.8460\n",
      "Epoch 381/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7561 - acc: 0.7600\n",
      "Epoch 00381: val_loss did not improve from 0.52718\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.7561 - acc: 0.7600 - val_loss: 0.5387 - val_acc: 0.8432\n",
      "Epoch 382/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7576 - acc: 0.7581\n",
      "Epoch 00382: val_loss improved from 0.52718 to 0.52464, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_3_conv_checkpoint/382-0.5246.hdf5\n",
      "36805/36805 [==============================] - 31s 835us/sample - loss: 0.7576 - acc: 0.7581 - val_loss: 0.5246 - val_acc: 0.8491\n",
      "Epoch 383/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7571 - acc: 0.7611\n",
      "Epoch 00383: val_loss did not improve from 0.52464\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.7572 - acc: 0.7611 - val_loss: 0.5383 - val_acc: 0.8472\n",
      "Epoch 384/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7585 - acc: 0.7584\n",
      "Epoch 00384: val_loss did not improve from 0.52464\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.7585 - acc: 0.7584 - val_loss: 0.5444 - val_acc: 0.8456\n",
      "Epoch 385/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7539 - acc: 0.7598\n",
      "Epoch 00385: val_loss did not improve from 0.52464\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.7539 - acc: 0.7598 - val_loss: 0.5394 - val_acc: 0.8418\n",
      "Epoch 386/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7558 - acc: 0.7598\n",
      "Epoch 00386: val_loss did not improve from 0.52464\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.7558 - acc: 0.7597 - val_loss: 0.5469 - val_acc: 0.8400\n",
      "Epoch 387/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7472 - acc: 0.7618\n",
      "Epoch 00387: val_loss did not improve from 0.52464\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.7472 - acc: 0.7618 - val_loss: 0.5350 - val_acc: 0.8425\n",
      "Epoch 388/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7503 - acc: 0.7606\n",
      "Epoch 00388: val_loss did not improve from 0.52464\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.7503 - acc: 0.7606 - val_loss: 0.5350 - val_acc: 0.8477\n",
      "Epoch 389/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7508 - acc: 0.7628\n",
      "Epoch 00389: val_loss did not improve from 0.52464\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.7508 - acc: 0.7628 - val_loss: 0.5287 - val_acc: 0.8488\n",
      "Epoch 390/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7520 - acc: 0.7601\n",
      "Epoch 00390: val_loss did not improve from 0.52464\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.7520 - acc: 0.7601 - val_loss: 0.5332 - val_acc: 0.8453\n",
      "Epoch 391/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7565 - acc: 0.7586\n",
      "Epoch 00391: val_loss did not improve from 0.52464\n",
      "36805/36805 [==============================] - 31s 836us/sample - loss: 0.7566 - acc: 0.7586 - val_loss: 0.5396 - val_acc: 0.8423\n",
      "Epoch 392/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7530 - acc: 0.7589\n",
      "Epoch 00392: val_loss did not improve from 0.52464\n",
      "36805/36805 [==============================] - 31s 840us/sample - loss: 0.7529 - acc: 0.7589 - val_loss: 0.5355 - val_acc: 0.8488\n",
      "Epoch 393/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7554 - acc: 0.7603\n",
      "Epoch 00393: val_loss did not improve from 0.52464\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.7554 - acc: 0.7603 - val_loss: 0.5448 - val_acc: 0.8442\n",
      "Epoch 394/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7575 - acc: 0.7565\n",
      "Epoch 00394: val_loss did not improve from 0.52464\n",
      "36805/36805 [==============================] - 31s 836us/sample - loss: 0.7575 - acc: 0.7565 - val_loss: 0.5274 - val_acc: 0.8444\n",
      "Epoch 395/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7570 - acc: 0.7586\n",
      "Epoch 00395: val_loss did not improve from 0.52464\n",
      "36805/36805 [==============================] - 31s 841us/sample - loss: 0.7570 - acc: 0.7586 - val_loss: 0.5336 - val_acc: 0.8446\n",
      "Epoch 396/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7535 - acc: 0.7596\n",
      "Epoch 00396: val_loss did not improve from 0.52464\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.7535 - acc: 0.7596 - val_loss: 0.5313 - val_acc: 0.8470\n",
      "Epoch 397/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7517 - acc: 0.7616\n",
      "Epoch 00397: val_loss did not improve from 0.52464\n",
      "36805/36805 [==============================] - 31s 835us/sample - loss: 0.7519 - acc: 0.7616 - val_loss: 0.5260 - val_acc: 0.8505\n",
      "Epoch 398/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7471 - acc: 0.7605\n",
      "Epoch 00398: val_loss did not improve from 0.52464\n",
      "36805/36805 [==============================] - 31s 841us/sample - loss: 0.7473 - acc: 0.7605 - val_loss: 0.5356 - val_acc: 0.8477\n",
      "Epoch 399/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7461 - acc: 0.7635\n",
      "Epoch 00399: val_loss did not improve from 0.52464\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.7460 - acc: 0.7636 - val_loss: 0.5288 - val_acc: 0.8467\n",
      "Epoch 400/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7479 - acc: 0.7643\n",
      "Epoch 00400: val_loss did not improve from 0.52464\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.7479 - acc: 0.7643 - val_loss: 0.5373 - val_acc: 0.8481\n",
      "Epoch 401/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7481 - acc: 0.7593\n",
      "Epoch 00401: val_loss did not improve from 0.52464\n",
      "36805/36805 [==============================] - 31s 834us/sample - loss: 0.7481 - acc: 0.7593 - val_loss: 0.5420 - val_acc: 0.8372\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 402/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7537 - acc: 0.7610\n",
      "Epoch 00402: val_loss did not improve from 0.52464\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.7537 - acc: 0.7610 - val_loss: 0.5344 - val_acc: 0.8428\n",
      "Epoch 403/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7487 - acc: 0.7613\n",
      "Epoch 00403: val_loss did not improve from 0.52464\n",
      "36805/36805 [==============================] - 31s 835us/sample - loss: 0.7487 - acc: 0.7613 - val_loss: 0.5331 - val_acc: 0.8446\n",
      "Epoch 404/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7528 - acc: 0.7599\n",
      "Epoch 00404: val_loss did not improve from 0.52464\n",
      "36805/36805 [==============================] - 31s 836us/sample - loss: 0.7529 - acc: 0.7599 - val_loss: 0.5341 - val_acc: 0.8463\n",
      "Epoch 405/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7469 - acc: 0.7609\n",
      "Epoch 00405: val_loss did not improve from 0.52464\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.7468 - acc: 0.7609 - val_loss: 0.5382 - val_acc: 0.8442\n",
      "Epoch 406/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7518 - acc: 0.7585\n",
      "Epoch 00406: val_loss did not improve from 0.52464\n",
      "36805/36805 [==============================] - 31s 836us/sample - loss: 0.7519 - acc: 0.7585 - val_loss: 0.5317 - val_acc: 0.8474\n",
      "Epoch 407/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7502 - acc: 0.7586\n",
      "Epoch 00407: val_loss did not improve from 0.52464\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.7502 - acc: 0.7586 - val_loss: 0.5289 - val_acc: 0.8484\n",
      "Epoch 408/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7433 - acc: 0.7610\n",
      "Epoch 00408: val_loss did not improve from 0.52464\n",
      "36805/36805 [==============================] - 31s 841us/sample - loss: 0.7433 - acc: 0.7610 - val_loss: 0.5262 - val_acc: 0.8467\n",
      "Epoch 409/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7490 - acc: 0.7616\n",
      "Epoch 00409: val_loss did not improve from 0.52464\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.7491 - acc: 0.7616 - val_loss: 0.5449 - val_acc: 0.8411\n",
      "Epoch 410/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7552 - acc: 0.7599\n",
      "Epoch 00410: val_loss did not improve from 0.52464\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.7552 - acc: 0.7600 - val_loss: 0.5411 - val_acc: 0.8423\n",
      "Epoch 411/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7453 - acc: 0.7646- ETA: 0s - loss: 0.7\n",
      "Epoch 00411: val_loss did not improve from 0.52464\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.7452 - acc: 0.7647 - val_loss: 0.5401 - val_acc: 0.8418\n",
      "Epoch 412/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7449 - acc: 0.7632\n",
      "Epoch 00412: val_loss did not improve from 0.52464\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.7450 - acc: 0.7632 - val_loss: 0.5413 - val_acc: 0.8409\n",
      "Epoch 413/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7448 - acc: 0.7643\n",
      "Epoch 00413: val_loss did not improve from 0.52464\n",
      "36805/36805 [==============================] - 31s 845us/sample - loss: 0.7448 - acc: 0.7643 - val_loss: 0.5318 - val_acc: 0.8444\n",
      "Epoch 414/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7409 - acc: 0.7634\n",
      "Epoch 00414: val_loss did not improve from 0.52464\n",
      "36805/36805 [==============================] - 31s 845us/sample - loss: 0.7408 - acc: 0.7634 - val_loss: 0.5359 - val_acc: 0.8409\n",
      "Epoch 415/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7428 - acc: 0.7629\n",
      "Epoch 00415: val_loss did not improve from 0.52464\n",
      "36805/36805 [==============================] - 31s 836us/sample - loss: 0.7429 - acc: 0.7628 - val_loss: 0.5318 - val_acc: 0.8451\n",
      "Epoch 416/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7435 - acc: 0.7609\n",
      "Epoch 00416: val_loss did not improve from 0.52464\n",
      "36805/36805 [==============================] - 31s 835us/sample - loss: 0.7434 - acc: 0.7609 - val_loss: 0.5343 - val_acc: 0.8477\n",
      "Epoch 417/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7425 - acc: 0.7630\n",
      "Epoch 00417: val_loss did not improve from 0.52464\n",
      "36805/36805 [==============================] - 31s 840us/sample - loss: 0.7425 - acc: 0.7630 - val_loss: 0.5339 - val_acc: 0.8449\n",
      "Epoch 418/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7407 - acc: 0.7639\n",
      "Epoch 00418: val_loss did not improve from 0.52464\n",
      "36805/36805 [==============================] - 31s 835us/sample - loss: 0.7407 - acc: 0.7639 - val_loss: 0.5419 - val_acc: 0.8509\n",
      "Epoch 419/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7442 - acc: 0.7614\n",
      "Epoch 00419: val_loss improved from 0.52464 to 0.52288, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_3_conv_checkpoint/419-0.5229.hdf5\n",
      "36805/36805 [==============================] - 31s 840us/sample - loss: 0.7441 - acc: 0.7614 - val_loss: 0.5229 - val_acc: 0.8488\n",
      "Epoch 420/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7404 - acc: 0.7626\n",
      "Epoch 00420: val_loss did not improve from 0.52288\n",
      "36805/36805 [==============================] - 32s 856us/sample - loss: 0.7404 - acc: 0.7626 - val_loss: 0.5301 - val_acc: 0.8442\n",
      "Epoch 421/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7413 - acc: 0.7636\n",
      "Epoch 00421: val_loss did not improve from 0.52288\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.7413 - acc: 0.7636 - val_loss: 0.5373 - val_acc: 0.8458\n",
      "Epoch 422/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7424 - acc: 0.7653\n",
      "Epoch 00422: val_loss did not improve from 0.52288\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.7425 - acc: 0.7652 - val_loss: 0.5269 - val_acc: 0.8493\n",
      "Epoch 423/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7434 - acc: 0.7643\n",
      "Epoch 00423: val_loss did not improve from 0.52288\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.7433 - acc: 0.7643 - val_loss: 0.5403 - val_acc: 0.8418\n",
      "Epoch 424/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7387 - acc: 0.7636\n",
      "Epoch 00424: val_loss did not improve from 0.52288\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.7387 - acc: 0.7635 - val_loss: 0.5334 - val_acc: 0.8435\n",
      "Epoch 425/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7430 - acc: 0.7621\n",
      "Epoch 00425: val_loss did not improve from 0.52288\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.7430 - acc: 0.7621 - val_loss: 0.5354 - val_acc: 0.8449\n",
      "Epoch 426/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7370 - acc: 0.7612\n",
      "Epoch 00426: val_loss did not improve from 0.52288\n",
      "36805/36805 [==============================] - 31s 840us/sample - loss: 0.7369 - acc: 0.7612 - val_loss: 0.5424 - val_acc: 0.8414\n",
      "Epoch 427/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7457 - acc: 0.7617\n",
      "Epoch 00427: val_loss did not improve from 0.52288\n",
      "36805/36805 [==============================] - 31s 835us/sample - loss: 0.7457 - acc: 0.7617 - val_loss: 0.5455 - val_acc: 0.8397\n",
      "Epoch 428/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7463 - acc: 0.7622\n",
      "Epoch 00428: val_loss did not improve from 0.52288\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.7463 - acc: 0.7622 - val_loss: 0.5275 - val_acc: 0.8479\n",
      "Epoch 429/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7407 - acc: 0.7631\n",
      "Epoch 00429: val_loss did not improve from 0.52288\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.7409 - acc: 0.7631 - val_loss: 0.5303 - val_acc: 0.8463\n",
      "Epoch 430/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7408 - acc: 0.7640\n",
      "Epoch 00430: val_loss did not improve from 0.52288\n",
      "36805/36805 [==============================] - 31s 840us/sample - loss: 0.7407 - acc: 0.7641 - val_loss: 0.5237 - val_acc: 0.8495\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 431/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7426 - acc: 0.7626\n",
      "Epoch 00431: val_loss did not improve from 0.52288\n",
      "36805/36805 [==============================] - 31s 836us/sample - loss: 0.7425 - acc: 0.7626 - val_loss: 0.5278 - val_acc: 0.8500\n",
      "Epoch 432/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7486 - acc: 0.7608\n",
      "Epoch 00432: val_loss did not improve from 0.52288\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.7486 - acc: 0.7608 - val_loss: 0.5369 - val_acc: 0.8458\n",
      "Epoch 433/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7417 - acc: 0.7646\n",
      "Epoch 00433: val_loss did not improve from 0.52288\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.7417 - acc: 0.7646 - val_loss: 0.5306 - val_acc: 0.8437\n",
      "Epoch 434/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7356 - acc: 0.7660\n",
      "Epoch 00434: val_loss did not improve from 0.52288\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.7355 - acc: 0.7660 - val_loss: 0.5337 - val_acc: 0.8458\n",
      "Epoch 435/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7404 - acc: 0.7631\n",
      "Epoch 00435: val_loss did not improve from 0.52288\n",
      "36805/36805 [==============================] - 31s 835us/sample - loss: 0.7403 - acc: 0.7631 - val_loss: 0.5252 - val_acc: 0.8488\n",
      "Epoch 436/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7394 - acc: 0.7620\n",
      "Epoch 00436: val_loss did not improve from 0.52288\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.7393 - acc: 0.7620 - val_loss: 0.5356 - val_acc: 0.8479\n",
      "Epoch 437/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7361 - acc: 0.7666\n",
      "Epoch 00437: val_loss improved from 0.52288 to 0.51789, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_3_conv_checkpoint/437-0.5179.hdf5\n",
      "36805/36805 [==============================] - 31s 841us/sample - loss: 0.7361 - acc: 0.7666 - val_loss: 0.5179 - val_acc: 0.8439\n",
      "Epoch 438/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7372 - acc: 0.7640\n",
      "Epoch 00438: val_loss did not improve from 0.51789\n",
      "36805/36805 [==============================] - 31s 841us/sample - loss: 0.7372 - acc: 0.7640 - val_loss: 0.5322 - val_acc: 0.8439\n",
      "Epoch 439/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7344 - acc: 0.7655\n",
      "Epoch 00439: val_loss did not improve from 0.51789\n",
      "36805/36805 [==============================] - 31s 841us/sample - loss: 0.7344 - acc: 0.7655 - val_loss: 0.5262 - val_acc: 0.8491\n",
      "Epoch 440/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7333 - acc: 0.7663- ETA: 1s - lo\n",
      "Epoch 00440: val_loss did not improve from 0.51789\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.7335 - acc: 0.7663 - val_loss: 0.5322 - val_acc: 0.8453\n",
      "Epoch 441/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7410 - acc: 0.7625\n",
      "Epoch 00441: val_loss did not improve from 0.51789\n",
      "36805/36805 [==============================] - 31s 840us/sample - loss: 0.7409 - acc: 0.7625 - val_loss: 0.5398 - val_acc: 0.8453\n",
      "Epoch 442/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7405 - acc: 0.7632\n",
      "Epoch 00442: val_loss did not improve from 0.51789\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.7404 - acc: 0.7632 - val_loss: 0.5267 - val_acc: 0.8474\n",
      "Epoch 443/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7388 - acc: 0.7619\n",
      "Epoch 00443: val_loss did not improve from 0.51789\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.7387 - acc: 0.7620 - val_loss: 0.5316 - val_acc: 0.8458\n",
      "Epoch 444/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7401 - acc: 0.7648\n",
      "Epoch 00444: val_loss did not improve from 0.51789\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.7401 - acc: 0.7648 - val_loss: 0.5287 - val_acc: 0.8498\n",
      "Epoch 445/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7397 - acc: 0.7636\n",
      "Epoch 00445: val_loss did not improve from 0.51789\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.7396 - acc: 0.7636 - val_loss: 0.5338 - val_acc: 0.8488\n",
      "Epoch 446/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7366 - acc: 0.7647\n",
      "Epoch 00446: val_loss did not improve from 0.51789\n",
      "36805/36805 [==============================] - 31s 841us/sample - loss: 0.7366 - acc: 0.7647 - val_loss: 0.5306 - val_acc: 0.8458\n",
      "Epoch 447/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7332 - acc: 0.7647\n",
      "Epoch 00447: val_loss did not improve from 0.51789\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.7331 - acc: 0.7647 - val_loss: 0.5296 - val_acc: 0.8451\n",
      "Epoch 448/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7355 - acc: 0.7652\n",
      "Epoch 00448: val_loss did not improve from 0.51789\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.7355 - acc: 0.7652 - val_loss: 0.5371 - val_acc: 0.8425\n",
      "Epoch 449/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7376 - acc: 0.7654\n",
      "Epoch 00449: val_loss did not improve from 0.51789\n",
      "36805/36805 [==============================] - 31s 835us/sample - loss: 0.7376 - acc: 0.7654 - val_loss: 0.5343 - val_acc: 0.8458\n",
      "Epoch 450/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7370 - acc: 0.7649\n",
      "Epoch 00450: val_loss did not improve from 0.51789\n",
      "36805/36805 [==============================] - 31s 840us/sample - loss: 0.7371 - acc: 0.7649 - val_loss: 0.5359 - val_acc: 0.8467\n",
      "Epoch 451/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7399 - acc: 0.7634\n",
      "Epoch 00451: val_loss did not improve from 0.51789\n",
      "36805/36805 [==============================] - 31s 836us/sample - loss: 0.7399 - acc: 0.7634 - val_loss: 0.5374 - val_acc: 0.8453\n",
      "Epoch 452/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7405 - acc: 0.7658\n",
      "Epoch 00452: val_loss did not improve from 0.51789\n",
      "36805/36805 [==============================] - 31s 840us/sample - loss: 0.7405 - acc: 0.7658 - val_loss: 0.5283 - val_acc: 0.8456\n",
      "Epoch 453/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7425 - acc: 0.7610\n",
      "Epoch 00453: val_loss did not improve from 0.51789\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.7424 - acc: 0.7610 - val_loss: 0.5269 - val_acc: 0.8470\n",
      "Epoch 454/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7293 - acc: 0.7643\n",
      "Epoch 00454: val_loss did not improve from 0.51789\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.7293 - acc: 0.7643 - val_loss: 0.5299 - val_acc: 0.8467\n",
      "Epoch 455/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7297 - acc: 0.7679\n",
      "Epoch 00455: val_loss did not improve from 0.51789\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.7298 - acc: 0.7679 - val_loss: 0.5335 - val_acc: 0.8446\n",
      "Epoch 456/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7326 - acc: 0.7677\n",
      "Epoch 00456: val_loss did not improve from 0.51789\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.7326 - acc: 0.7677 - val_loss: 0.5349 - val_acc: 0.8400\n",
      "Epoch 457/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7286 - acc: 0.7683\n",
      "Epoch 00457: val_loss did not improve from 0.51789\n",
      "36805/36805 [==============================] - 31s 840us/sample - loss: 0.7286 - acc: 0.7683 - val_loss: 0.5357 - val_acc: 0.8470\n",
      "Epoch 458/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7288 - acc: 0.7650\n",
      "Epoch 00458: val_loss did not improve from 0.51789\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.7288 - acc: 0.7650 - val_loss: 0.5507 - val_acc: 0.8411\n",
      "Epoch 459/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7272 - acc: 0.7686\n",
      "Epoch 00459: val_loss did not improve from 0.51789\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.7272 - acc: 0.7686 - val_loss: 0.5398 - val_acc: 0.8421\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 460/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7381 - acc: 0.7648\n",
      "Epoch 00460: val_loss did not improve from 0.51789\n",
      "36805/36805 [==============================] - 31s 840us/sample - loss: 0.7381 - acc: 0.7648 - val_loss: 0.5339 - val_acc: 0.8453\n",
      "Epoch 461/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7265 - acc: 0.7701\n",
      "Epoch 00461: val_loss did not improve from 0.51789\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.7265 - acc: 0.7701 - val_loss: 0.5240 - val_acc: 0.8481\n",
      "Epoch 462/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7295 - acc: 0.7647\n",
      "Epoch 00462: val_loss did not improve from 0.51789\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.7295 - acc: 0.7647 - val_loss: 0.5340 - val_acc: 0.8477\n",
      "Epoch 463/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7265 - acc: 0.7691\n",
      "Epoch 00463: val_loss did not improve from 0.51789\n",
      "36805/36805 [==============================] - 31s 841us/sample - loss: 0.7265 - acc: 0.7691 - val_loss: 0.5393 - val_acc: 0.8418\n",
      "Epoch 464/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7337 - acc: 0.7651\n",
      "Epoch 00464: val_loss did not improve from 0.51789\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.7336 - acc: 0.7651 - val_loss: 0.5333 - val_acc: 0.8467\n",
      "Epoch 465/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7419 - acc: 0.7604\n",
      "Epoch 00465: val_loss did not improve from 0.51789\n",
      "36805/36805 [==============================] - 31s 835us/sample - loss: 0.7420 - acc: 0.7603 - val_loss: 0.5374 - val_acc: 0.8456\n",
      "Epoch 466/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7370 - acc: 0.7682\n",
      "Epoch 00466: val_loss did not improve from 0.51789\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.7369 - acc: 0.7682 - val_loss: 0.5311 - val_acc: 0.8493\n",
      "Epoch 467/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7332 - acc: 0.7650\n",
      "Epoch 00467: val_loss did not improve from 0.51789\n",
      "36805/36805 [==============================] - 31s 836us/sample - loss: 0.7333 - acc: 0.7650 - val_loss: 0.5218 - val_acc: 0.8493\n",
      "Epoch 468/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7316 - acc: 0.7683\n",
      "Epoch 00468: val_loss did not improve from 0.51789\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.7316 - acc: 0.7684 - val_loss: 0.5303 - val_acc: 0.8465\n",
      "Epoch 469/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7354 - acc: 0.7665\n",
      "Epoch 00469: val_loss did not improve from 0.51789\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.7354 - acc: 0.7665 - val_loss: 0.5388 - val_acc: 0.8465\n",
      "Epoch 470/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7262 - acc: 0.7694\n",
      "Epoch 00470: val_loss did not improve from 0.51789\n",
      "36805/36805 [==============================] - 31s 847us/sample - loss: 0.7262 - acc: 0.7694 - val_loss: 0.5259 - val_acc: 0.8470\n",
      "Epoch 471/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7313 - acc: 0.7672\n",
      "Epoch 00471: val_loss did not improve from 0.51789\n",
      "36805/36805 [==============================] - 31s 845us/sample - loss: 0.7313 - acc: 0.7672 - val_loss: 0.5263 - val_acc: 0.8463\n",
      "Epoch 472/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7291 - acc: 0.7689\n",
      "Epoch 00472: val_loss did not improve from 0.51789\n",
      "36805/36805 [==============================] - 31s 840us/sample - loss: 0.7290 - acc: 0.7689 - val_loss: 0.5299 - val_acc: 0.8474\n",
      "Epoch 473/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7267 - acc: 0.7674\n",
      "Epoch 00473: val_loss did not improve from 0.51789\n",
      "36805/36805 [==============================] - 31s 835us/sample - loss: 0.7266 - acc: 0.7674 - val_loss: 0.5338 - val_acc: 0.8488\n",
      "Epoch 474/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7237 - acc: 0.7698\n",
      "Epoch 00474: val_loss did not improve from 0.51789\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.7237 - acc: 0.7699 - val_loss: 0.5263 - val_acc: 0.8519\n",
      "Epoch 475/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7252 - acc: 0.7676\n",
      "Epoch 00475: val_loss did not improve from 0.51789\n",
      "36805/36805 [==============================] - 31s 840us/sample - loss: 0.7253 - acc: 0.7676 - val_loss: 0.5247 - val_acc: 0.8451\n",
      "Epoch 476/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7329 - acc: 0.7662\n",
      "Epoch 00476: val_loss did not improve from 0.51789\n",
      "36805/36805 [==============================] - 31s 836us/sample - loss: 0.7328 - acc: 0.7662 - val_loss: 0.5256 - val_acc: 0.8523\n",
      "Epoch 477/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7236 - acc: 0.7687\n",
      "Epoch 00477: val_loss did not improve from 0.51789\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.7237 - acc: 0.7687 - val_loss: 0.5278 - val_acc: 0.8474\n",
      "Epoch 478/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7276 - acc: 0.7680\n",
      "Epoch 00478: val_loss did not improve from 0.51789\n",
      "36805/36805 [==============================] - 31s 841us/sample - loss: 0.7278 - acc: 0.7680 - val_loss: 0.5305 - val_acc: 0.8465\n",
      "Epoch 479/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7274 - acc: 0.7676\n",
      "Epoch 00479: val_loss did not improve from 0.51789\n",
      "36805/36805 [==============================] - 31s 835us/sample - loss: 0.7275 - acc: 0.7676 - val_loss: 0.5296 - val_acc: 0.8505\n",
      "Epoch 480/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7260 - acc: 0.7693\n",
      "Epoch 00480: val_loss did not improve from 0.51789\n",
      "36805/36805 [==============================] - 31s 836us/sample - loss: 0.7259 - acc: 0.7694 - val_loss: 0.5392 - val_acc: 0.8467\n",
      "Epoch 481/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7163 - acc: 0.7691\n",
      "Epoch 00481: val_loss did not improve from 0.51789\n",
      "36805/36805 [==============================] - 31s 840us/sample - loss: 0.7163 - acc: 0.7691 - val_loss: 0.5370 - val_acc: 0.8502\n",
      "Epoch 482/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7205 - acc: 0.7701\n",
      "Epoch 00482: val_loss did not improve from 0.51789\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.7205 - acc: 0.7700 - val_loss: 0.5584 - val_acc: 0.8397\n",
      "Epoch 483/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7199 - acc: 0.7685\n",
      "Epoch 00483: val_loss did not improve from 0.51789\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.7199 - acc: 0.7685 - val_loss: 0.5278 - val_acc: 0.8465\n",
      "Epoch 484/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7273 - acc: 0.7676\n",
      "Epoch 00484: val_loss did not improve from 0.51789\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.7274 - acc: 0.7675 - val_loss: 0.5259 - val_acc: 0.8442\n",
      "Epoch 485/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7219 - acc: 0.7699\n",
      "Epoch 00485: val_loss did not improve from 0.51789\n",
      "36805/36805 [==============================] - 31s 842us/sample - loss: 0.7219 - acc: 0.7698 - val_loss: 0.5419 - val_acc: 0.8400\n",
      "Epoch 486/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7237 - acc: 0.7696\n",
      "Epoch 00486: val_loss did not improve from 0.51789\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.7237 - acc: 0.7697 - val_loss: 0.5255 - val_acc: 0.8470\n",
      "Epoch 487/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7221 - acc: 0.7671\n",
      "Epoch 00487: val_loss did not improve from 0.51789\n",
      "36805/36805 [==============================] - 31s 840us/sample - loss: 0.7221 - acc: 0.7671 - val_loss: 0.5256 - val_acc: 0.8484\n",
      "\n",
      "1D_CNN_custom_multi_3_GMP_DO_3_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 394us/sample - loss: 0.6007 - acc: 0.8214\n",
      "Loss: 0.6007182153462126 Accuracy: 0.82139146\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.5566 - acc: 0.1607\n",
      "Epoch 00001: val_loss improved from inf to 2.20721, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_4_conv_checkpoint/001-2.2072.hdf5\n",
      "36805/36805 [==============================] - 31s 856us/sample - loss: 2.5565 - acc: 0.1607 - val_loss: 2.2072 - val_acc: 0.3058\n",
      "Epoch 2/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 2.1920 - acc: 0.2630\n",
      "Epoch 00002: val_loss improved from 2.20721 to 1.93123, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_4_conv_checkpoint/002-1.9312.hdf5\n",
      "36805/36805 [==============================] - 30s 814us/sample - loss: 2.1916 - acc: 0.2631 - val_loss: 1.9312 - val_acc: 0.4067\n",
      "Epoch 3/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 2.0240 - acc: 0.3160\n",
      "Epoch 00003: val_loss improved from 1.93123 to 1.76370, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_4_conv_checkpoint/003-1.7637.hdf5\n",
      "36805/36805 [==============================] - 30s 819us/sample - loss: 2.0240 - acc: 0.3162 - val_loss: 1.7637 - val_acc: 0.4584\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.8937 - acc: 0.3628\n",
      "Epoch 00004: val_loss improved from 1.76370 to 1.60801, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_4_conv_checkpoint/004-1.6080.hdf5\n",
      "36805/36805 [==============================] - 30s 818us/sample - loss: 1.8937 - acc: 0.3628 - val_loss: 1.6080 - val_acc: 0.5006\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.7831 - acc: 0.4014\n",
      "Epoch 00005: val_loss improved from 1.60801 to 1.49144, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_4_conv_checkpoint/005-1.4914.hdf5\n",
      "36805/36805 [==============================] - 30s 815us/sample - loss: 1.7832 - acc: 0.4014 - val_loss: 1.4914 - val_acc: 0.5388\n",
      "Epoch 6/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.6910 - acc: 0.4317\n",
      "Epoch 00006: val_loss improved from 1.49144 to 1.39439, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_4_conv_checkpoint/006-1.3944.hdf5\n",
      "36805/36805 [==============================] - 30s 818us/sample - loss: 1.6909 - acc: 0.4316 - val_loss: 1.3944 - val_acc: 0.5635\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6050 - acc: 0.4629\n",
      "Epoch 00007: val_loss improved from 1.39439 to 1.29738, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_4_conv_checkpoint/007-1.2974.hdf5\n",
      "36805/36805 [==============================] - 30s 815us/sample - loss: 1.6049 - acc: 0.4629 - val_loss: 1.2974 - val_acc: 0.6080\n",
      "Epoch 8/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.5361 - acc: 0.4865\n",
      "Epoch 00008: val_loss improved from 1.29738 to 1.23594, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_4_conv_checkpoint/008-1.2359.hdf5\n",
      "36805/36805 [==============================] - 30s 815us/sample - loss: 1.5359 - acc: 0.4865 - val_loss: 1.2359 - val_acc: 0.6219\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4669 - acc: 0.5107\n",
      "Epoch 00009: val_loss improved from 1.23594 to 1.15721, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_4_conv_checkpoint/009-1.1572.hdf5\n",
      "36805/36805 [==============================] - 30s 818us/sample - loss: 1.4668 - acc: 0.5107 - val_loss: 1.1572 - val_acc: 0.6480\n",
      "Epoch 10/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.4103 - acc: 0.5304\n",
      "Epoch 00010: val_loss improved from 1.15721 to 1.10268, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_4_conv_checkpoint/010-1.1027.hdf5\n",
      "36805/36805 [==============================] - 30s 812us/sample - loss: 1.4103 - acc: 0.5304 - val_loss: 1.1027 - val_acc: 0.6655\n",
      "Epoch 11/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3588 - acc: 0.5503\n",
      "Epoch 00011: val_loss improved from 1.10268 to 1.05741, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_4_conv_checkpoint/011-1.0574.hdf5\n",
      "36805/36805 [==============================] - 30s 819us/sample - loss: 1.3588 - acc: 0.5503 - val_loss: 1.0574 - val_acc: 0.6778\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3119 - acc: 0.5668\n",
      "Epoch 00012: val_loss improved from 1.05741 to 1.01146, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_4_conv_checkpoint/012-1.0115.hdf5\n",
      "36805/36805 [==============================] - 30s 816us/sample - loss: 1.3119 - acc: 0.5668 - val_loss: 1.0115 - val_acc: 0.6888\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2749 - acc: 0.5768\n",
      "Epoch 00013: val_loss improved from 1.01146 to 0.99820, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_4_conv_checkpoint/013-0.9982.hdf5\n",
      "36805/36805 [==============================] - 30s 818us/sample - loss: 1.2750 - acc: 0.5768 - val_loss: 0.9982 - val_acc: 0.6930\n",
      "Epoch 14/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2411 - acc: 0.5936\n",
      "Epoch 00014: val_loss improved from 0.99820 to 0.93793, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_4_conv_checkpoint/014-0.9379.hdf5\n",
      "36805/36805 [==============================] - 30s 815us/sample - loss: 1.2409 - acc: 0.5936 - val_loss: 0.9379 - val_acc: 0.7149\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2056 - acc: 0.6057- ETA: 1s - \n",
      "Epoch 00015: val_loss improved from 0.93793 to 0.92356, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_4_conv_checkpoint/015-0.9236.hdf5\n",
      "36805/36805 [==============================] - 30s 818us/sample - loss: 1.2056 - acc: 0.6058 - val_loss: 0.9236 - val_acc: 0.7228\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1772 - acc: 0.6128\n",
      "Epoch 00016: val_loss improved from 0.92356 to 0.89141, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_4_conv_checkpoint/016-0.8914.hdf5\n",
      "36805/36805 [==============================] - 30s 815us/sample - loss: 1.1772 - acc: 0.6127 - val_loss: 0.8914 - val_acc: 0.7307\n",
      "Epoch 17/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1523 - acc: 0.6230\n",
      "Epoch 00017: val_loss improved from 0.89141 to 0.87526, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_4_conv_checkpoint/017-0.8753.hdf5\n",
      "36805/36805 [==============================] - 30s 817us/sample - loss: 1.1530 - acc: 0.6228 - val_loss: 0.8753 - val_acc: 0.7328\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1286 - acc: 0.6310\n",
      "Epoch 00018: val_loss improved from 0.87526 to 0.84895, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_4_conv_checkpoint/018-0.8490.hdf5\n",
      "36805/36805 [==============================] - 30s 818us/sample - loss: 1.1286 - acc: 0.6311 - val_loss: 0.8490 - val_acc: 0.7389\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1090 - acc: 0.6410\n",
      "Epoch 00019: val_loss improved from 0.84895 to 0.82376, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_4_conv_checkpoint/019-0.8238.hdf5\n",
      "36805/36805 [==============================] - 30s 815us/sample - loss: 1.1091 - acc: 0.6409 - val_loss: 0.8238 - val_acc: 0.7496\n",
      "Epoch 20/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0823 - acc: 0.6505\n",
      "Epoch 00020: val_loss improved from 0.82376 to 0.80699, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_4_conv_checkpoint/020-0.8070.hdf5\n",
      "36805/36805 [==============================] - 30s 813us/sample - loss: 1.0822 - acc: 0.6506 - val_loss: 0.8070 - val_acc: 0.7629\n",
      "Epoch 21/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0624 - acc: 0.6542\n",
      "Epoch 00021: val_loss improved from 0.80699 to 0.79952, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_4_conv_checkpoint/021-0.7995.hdf5\n",
      "36805/36805 [==============================] - 30s 816us/sample - loss: 1.0627 - acc: 0.6541 - val_loss: 0.7995 - val_acc: 0.7603\n",
      "Epoch 22/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0513 - acc: 0.6600\n",
      "Epoch 00022: val_loss improved from 0.79952 to 0.76679, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_4_conv_checkpoint/022-0.7668.hdf5\n",
      "36805/36805 [==============================] - 30s 815us/sample - loss: 1.0509 - acc: 0.6600 - val_loss: 0.7668 - val_acc: 0.7699\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0286 - acc: 0.6701\n",
      "Epoch 00023: val_loss improved from 0.76679 to 0.76299, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_4_conv_checkpoint/023-0.7630.hdf5\n",
      "36805/36805 [==============================] - 30s 815us/sample - loss: 1.0284 - acc: 0.6701 - val_loss: 0.7630 - val_acc: 0.7715\n",
      "Epoch 24/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0159 - acc: 0.6755\n",
      "Epoch 00024: val_loss improved from 0.76299 to 0.75382, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_4_conv_checkpoint/024-0.7538.hdf5\n",
      "36805/36805 [==============================] - 30s 814us/sample - loss: 1.0157 - acc: 0.6755 - val_loss: 0.7538 - val_acc: 0.7780\n",
      "Epoch 25/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0085 - acc: 0.6768\n",
      "Epoch 00025: val_loss improved from 0.75382 to 0.73993, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_4_conv_checkpoint/025-0.7399.hdf5\n",
      "36805/36805 [==============================] - 30s 816us/sample - loss: 1.0088 - acc: 0.6766 - val_loss: 0.7399 - val_acc: 0.7727\n",
      "Epoch 26/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9849 - acc: 0.6822\n",
      "Epoch 00026: val_loss improved from 0.73993 to 0.72217, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_4_conv_checkpoint/026-0.7222.hdf5\n",
      "36805/36805 [==============================] - 30s 813us/sample - loss: 0.9847 - acc: 0.6823 - val_loss: 0.7222 - val_acc: 0.7859\n",
      "Epoch 27/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9707 - acc: 0.6908\n",
      "Epoch 00027: val_loss improved from 0.72217 to 0.70447, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_4_conv_checkpoint/027-0.7045.hdf5\n",
      "36805/36805 [==============================] - 30s 813us/sample - loss: 0.9705 - acc: 0.6907 - val_loss: 0.7045 - val_acc: 0.7945\n",
      "Epoch 28/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9619 - acc: 0.6936\n",
      "Epoch 00028: val_loss did not improve from 0.70447\n",
      "36805/36805 [==============================] - 30s 812us/sample - loss: 0.9618 - acc: 0.6937 - val_loss: 0.7094 - val_acc: 0.7864\n",
      "Epoch 29/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9504 - acc: 0.6965\n",
      "Epoch 00029: val_loss improved from 0.70447 to 0.69143, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_4_conv_checkpoint/029-0.6914.hdf5\n",
      "36805/36805 [==============================] - 30s 815us/sample - loss: 0.9509 - acc: 0.6963 - val_loss: 0.6914 - val_acc: 0.7899\n",
      "Epoch 30/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9374 - acc: 0.6983\n",
      "Epoch 00030: val_loss improved from 0.69143 to 0.68745, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_4_conv_checkpoint/030-0.6874.hdf5\n",
      "36805/36805 [==============================] - 30s 818us/sample - loss: 0.9373 - acc: 0.6982 - val_loss: 0.6874 - val_acc: 0.7978\n",
      "Epoch 31/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9293 - acc: 0.7032\n",
      "Epoch 00031: val_loss improved from 0.68745 to 0.66722, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_4_conv_checkpoint/031-0.6672.hdf5\n",
      "36805/36805 [==============================] - 30s 817us/sample - loss: 0.9291 - acc: 0.7032 - val_loss: 0.6672 - val_acc: 0.7987\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9251 - acc: 0.7077\n",
      "Epoch 00032: val_loss did not improve from 0.66722\n",
      "36805/36805 [==============================] - 30s 817us/sample - loss: 0.9250 - acc: 0.7077 - val_loss: 0.6965 - val_acc: 0.7955\n",
      "Epoch 33/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9085 - acc: 0.7124\n",
      "Epoch 00033: val_loss improved from 0.66722 to 0.66223, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_4_conv_checkpoint/033-0.6622.hdf5\n",
      "36805/36805 [==============================] - 30s 815us/sample - loss: 0.9086 - acc: 0.7123 - val_loss: 0.6622 - val_acc: 0.7990\n",
      "Epoch 34/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9031 - acc: 0.7149- ETA: 0s - loss: 0.9026 - acc: 0.71\n",
      "Epoch 00034: val_loss improved from 0.66223 to 0.64877, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_4_conv_checkpoint/034-0.6488.hdf5\n",
      "36805/36805 [==============================] - 30s 815us/sample - loss: 0.9027 - acc: 0.7150 - val_loss: 0.6488 - val_acc: 0.8050\n",
      "Epoch 35/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8909 - acc: 0.7178\n",
      "Epoch 00035: val_loss improved from 0.64877 to 0.63469, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_4_conv_checkpoint/035-0.6347.hdf5\n",
      "36805/36805 [==============================] - 30s 817us/sample - loss: 0.8906 - acc: 0.7179 - val_loss: 0.6347 - val_acc: 0.8104\n",
      "Epoch 36/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8848 - acc: 0.7202\n",
      "Epoch 00036: val_loss improved from 0.63469 to 0.62207, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_4_conv_checkpoint/036-0.6221.hdf5\n",
      "36805/36805 [==============================] - 30s 815us/sample - loss: 0.8854 - acc: 0.7201 - val_loss: 0.6221 - val_acc: 0.8132\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8779 - acc: 0.7216\n",
      "Epoch 00037: val_loss improved from 0.62207 to 0.61325, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_4_conv_checkpoint/037-0.6132.hdf5\n",
      "36805/36805 [==============================] - 30s 820us/sample - loss: 0.8779 - acc: 0.7216 - val_loss: 0.6132 - val_acc: 0.8160\n",
      "Epoch 38/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8746 - acc: 0.7248\n",
      "Epoch 00038: val_loss did not improve from 0.61325\n",
      "36805/36805 [==============================] - 30s 815us/sample - loss: 0.8745 - acc: 0.7248 - val_loss: 0.6158 - val_acc: 0.8202\n",
      "Epoch 39/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8637 - acc: 0.7285\n",
      "Epoch 00039: val_loss improved from 0.61325 to 0.60306, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_4_conv_checkpoint/039-0.6031.hdf5\n",
      "36805/36805 [==============================] - 30s 813us/sample - loss: 0.8642 - acc: 0.7283 - val_loss: 0.6031 - val_acc: 0.8244\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8494 - acc: 0.7321\n",
      "Epoch 00040: val_loss did not improve from 0.60306\n",
      "36805/36805 [==============================] - 30s 816us/sample - loss: 0.8493 - acc: 0.7321 - val_loss: 0.6064 - val_acc: 0.8211\n",
      "Epoch 41/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8431 - acc: 0.7372\n",
      "Epoch 00041: val_loss improved from 0.60306 to 0.59126, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_4_conv_checkpoint/041-0.5913.hdf5\n",
      "36805/36805 [==============================] - 30s 816us/sample - loss: 0.8429 - acc: 0.7373 - val_loss: 0.5913 - val_acc: 0.8251\n",
      "Epoch 42/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8301 - acc: 0.7376- ETA: 1s - los\n",
      "Epoch 00042: val_loss improved from 0.59126 to 0.59075, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_4_conv_checkpoint/042-0.5907.hdf5\n",
      "36805/36805 [==============================] - 30s 814us/sample - loss: 0.8303 - acc: 0.7377 - val_loss: 0.5907 - val_acc: 0.8244\n",
      "Epoch 43/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8324 - acc: 0.7372\n",
      "Epoch 00043: val_loss improved from 0.59075 to 0.58721, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_4_conv_checkpoint/043-0.5872.hdf5\n",
      "36805/36805 [==============================] - 30s 818us/sample - loss: 0.8321 - acc: 0.7374 - val_loss: 0.5872 - val_acc: 0.8269\n",
      "Epoch 44/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8276 - acc: 0.7382\n",
      "Epoch 00044: val_loss improved from 0.58721 to 0.56717, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_4_conv_checkpoint/044-0.5672.hdf5\n",
      "36805/36805 [==============================] - 30s 814us/sample - loss: 0.8275 - acc: 0.7382 - val_loss: 0.5672 - val_acc: 0.8332\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8174 - acc: 0.7438\n",
      "Epoch 00045: val_loss did not improve from 0.56717\n",
      "36805/36805 [==============================] - 30s 811us/sample - loss: 0.8174 - acc: 0.7438 - val_loss: 0.5780 - val_acc: 0.8283\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8148 - acc: 0.7427\n",
      "Epoch 00046: val_loss improved from 0.56717 to 0.56514, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_4_conv_checkpoint/046-0.5651.hdf5\n",
      "36805/36805 [==============================] - 30s 813us/sample - loss: 0.8148 - acc: 0.7428 - val_loss: 0.5651 - val_acc: 0.8360\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8109 - acc: 0.7445\n",
      "Epoch 00047: val_loss did not improve from 0.56514\n",
      "36805/36805 [==============================] - 30s 812us/sample - loss: 0.8108 - acc: 0.7445 - val_loss: 0.5663 - val_acc: 0.8316\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7957 - acc: 0.7480\n",
      "Epoch 00048: val_loss improved from 0.56514 to 0.55166, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_4_conv_checkpoint/048-0.5517.hdf5\n",
      "36805/36805 [==============================] - 30s 819us/sample - loss: 0.7956 - acc: 0.7481 - val_loss: 0.5517 - val_acc: 0.8376\n",
      "Epoch 49/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7982 - acc: 0.7522\n",
      "Epoch 00049: val_loss improved from 0.55166 to 0.54390, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_4_conv_checkpoint/049-0.5439.hdf5\n",
      "36805/36805 [==============================] - 30s 810us/sample - loss: 0.7981 - acc: 0.7522 - val_loss: 0.5439 - val_acc: 0.8381\n",
      "Epoch 50/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7900 - acc: 0.7569\n",
      "Epoch 00050: val_loss did not improve from 0.54390\n",
      "36805/36805 [==============================] - 30s 811us/sample - loss: 0.7898 - acc: 0.7569 - val_loss: 0.5465 - val_acc: 0.8411\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7812 - acc: 0.7571\n",
      "Epoch 00051: val_loss improved from 0.54390 to 0.53627, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_4_conv_checkpoint/051-0.5363.hdf5\n",
      "36805/36805 [==============================] - 30s 816us/sample - loss: 0.7811 - acc: 0.7572 - val_loss: 0.5363 - val_acc: 0.8439\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7770 - acc: 0.7578\n",
      "Epoch 00052: val_loss improved from 0.53627 to 0.52774, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_4_conv_checkpoint/052-0.5277.hdf5\n",
      "36805/36805 [==============================] - 30s 813us/sample - loss: 0.7770 - acc: 0.7578 - val_loss: 0.5277 - val_acc: 0.8444\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7746 - acc: 0.7588\n",
      "Epoch 00053: val_loss did not improve from 0.52774\n",
      "36805/36805 [==============================] - 30s 815us/sample - loss: 0.7746 - acc: 0.7588 - val_loss: 0.5294 - val_acc: 0.8453\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7643 - acc: 0.7613\n",
      "Epoch 00054: val_loss improved from 0.52774 to 0.51449, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_4_conv_checkpoint/054-0.5145.hdf5\n",
      "36805/36805 [==============================] - 30s 815us/sample - loss: 0.7643 - acc: 0.7614 - val_loss: 0.5145 - val_acc: 0.8479\n",
      "Epoch 55/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7691 - acc: 0.7606\n",
      "Epoch 00055: val_loss did not improve from 0.51449\n",
      "36805/36805 [==============================] - 30s 816us/sample - loss: 0.7687 - acc: 0.7606 - val_loss: 0.5261 - val_acc: 0.8432\n",
      "Epoch 56/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7583 - acc: 0.7631\n",
      "Epoch 00056: val_loss did not improve from 0.51449\n",
      "36805/36805 [==============================] - 30s 810us/sample - loss: 0.7582 - acc: 0.7632 - val_loss: 0.5253 - val_acc: 0.8435\n",
      "Epoch 57/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7513 - acc: 0.7651\n",
      "Epoch 00057: val_loss did not improve from 0.51449\n",
      "36805/36805 [==============================] - 30s 814us/sample - loss: 0.7514 - acc: 0.7651 - val_loss: 0.5180 - val_acc: 0.8505\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7474 - acc: 0.7671\n",
      "Epoch 00058: val_loss did not improve from 0.51449\n",
      "36805/36805 [==============================] - 30s 817us/sample - loss: 0.7474 - acc: 0.7670 - val_loss: 0.5157 - val_acc: 0.8484\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7445 - acc: 0.7690\n",
      "Epoch 00059: val_loss improved from 0.51449 to 0.50664, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_4_conv_checkpoint/059-0.5066.hdf5\n",
      "36805/36805 [==============================] - 30s 814us/sample - loss: 0.7445 - acc: 0.7690 - val_loss: 0.5066 - val_acc: 0.8456\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7459 - acc: 0.7688- ETA\n",
      "Epoch 00060: val_loss did not improve from 0.50664\n",
      "36805/36805 [==============================] - 30s 816us/sample - loss: 0.7458 - acc: 0.7688 - val_loss: 0.5119 - val_acc: 0.8479\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7328 - acc: 0.7729\n",
      "Epoch 00061: val_loss improved from 0.50664 to 0.48761, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_4_conv_checkpoint/061-0.4876.hdf5\n",
      "36805/36805 [==============================] - 30s 813us/sample - loss: 0.7329 - acc: 0.7729 - val_loss: 0.4876 - val_acc: 0.8572\n",
      "Epoch 62/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7307 - acc: 0.7709\n",
      "Epoch 00062: val_loss did not improve from 0.48761\n",
      "36805/36805 [==============================] - 30s 811us/sample - loss: 0.7307 - acc: 0.7708 - val_loss: 0.5096 - val_acc: 0.8446\n",
      "Epoch 63/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7310 - acc: 0.7722\n",
      "Epoch 00063: val_loss did not improve from 0.48761\n",
      "36805/36805 [==============================] - 30s 815us/sample - loss: 0.7306 - acc: 0.7723 - val_loss: 0.5017 - val_acc: 0.8532\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7274 - acc: 0.7718\n",
      "Epoch 00064: val_loss did not improve from 0.48761\n",
      "36805/36805 [==============================] - 30s 815us/sample - loss: 0.7276 - acc: 0.7717 - val_loss: 0.4938 - val_acc: 0.8530\n",
      "Epoch 65/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7201 - acc: 0.7757\n",
      "Epoch 00065: val_loss improved from 0.48761 to 0.48392, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_4_conv_checkpoint/065-0.4839.hdf5\n",
      "36805/36805 [==============================] - 30s 819us/sample - loss: 0.7203 - acc: 0.7757 - val_loss: 0.4839 - val_acc: 0.8567\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7187 - acc: 0.7767\n",
      "Epoch 00066: val_loss improved from 0.48392 to 0.47753, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_4_conv_checkpoint/066-0.4775.hdf5\n",
      "36805/36805 [==============================] - 30s 813us/sample - loss: 0.7188 - acc: 0.7767 - val_loss: 0.4775 - val_acc: 0.8535\n",
      "Epoch 67/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7142 - acc: 0.7771\n",
      "Epoch 00067: val_loss improved from 0.47753 to 0.47394, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_4_conv_checkpoint/067-0.4739.hdf5\n",
      "36805/36805 [==============================] - 30s 817us/sample - loss: 0.7143 - acc: 0.7771 - val_loss: 0.4739 - val_acc: 0.8595\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7100 - acc: 0.7790\n",
      "Epoch 00068: val_loss did not improve from 0.47394\n",
      "36805/36805 [==============================] - 30s 813us/sample - loss: 0.7100 - acc: 0.7791 - val_loss: 0.4745 - val_acc: 0.8581\n",
      "Epoch 69/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7055 - acc: 0.7781\n",
      "Epoch 00069: val_loss improved from 0.47394 to 0.47040, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_4_conv_checkpoint/069-0.4704.hdf5\n",
      "36805/36805 [==============================] - 30s 814us/sample - loss: 0.7059 - acc: 0.7780 - val_loss: 0.4704 - val_acc: 0.8607\n",
      "Epoch 70/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7042 - acc: 0.7826\n",
      "Epoch 00070: val_loss did not improve from 0.47040\n",
      "36805/36805 [==============================] - 30s 815us/sample - loss: 0.7041 - acc: 0.7826 - val_loss: 0.4730 - val_acc: 0.8612\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6963 - acc: 0.7842\n",
      "Epoch 00071: val_loss improved from 0.47040 to 0.46139, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_4_conv_checkpoint/071-0.4614.hdf5\n",
      "36805/36805 [==============================] - 30s 813us/sample - loss: 0.6962 - acc: 0.7842 - val_loss: 0.4614 - val_acc: 0.8616\n",
      "Epoch 72/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7044 - acc: 0.7782\n",
      "Epoch 00072: val_loss did not improve from 0.46139\n",
      "36805/36805 [==============================] - 30s 813us/sample - loss: 0.7047 - acc: 0.7781 - val_loss: 0.4712 - val_acc: 0.8616\n",
      "Epoch 73/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6973 - acc: 0.7816- ETA: 0s - loss: 0.6983 - ac\n",
      "Epoch 00073: val_loss did not improve from 0.46139\n",
      "36805/36805 [==============================] - 30s 818us/sample - loss: 0.6976 - acc: 0.7816 - val_loss: 0.4659 - val_acc: 0.8609\n",
      "Epoch 74/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6916 - acc: 0.7854\n",
      "Epoch 00074: val_loss improved from 0.46139 to 0.45730, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_4_conv_checkpoint/074-0.4573.hdf5\n",
      "36805/36805 [==============================] - 30s 812us/sample - loss: 0.6914 - acc: 0.7854 - val_loss: 0.4573 - val_acc: 0.8658\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6880 - acc: 0.7827\n",
      "Epoch 00075: val_loss did not improve from 0.45730\n",
      "36805/36805 [==============================] - 30s 815us/sample - loss: 0.6879 - acc: 0.7827 - val_loss: 0.4633 - val_acc: 0.8570\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6852 - acc: 0.7865\n",
      "Epoch 00076: val_loss did not improve from 0.45730\n",
      "36805/36805 [==============================] - 30s 816us/sample - loss: 0.6852 - acc: 0.7866 - val_loss: 0.4610 - val_acc: 0.8626\n",
      "Epoch 77/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6884 - acc: 0.7873\n",
      "Epoch 00077: val_loss did not improve from 0.45730\n",
      "36805/36805 [==============================] - 30s 812us/sample - loss: 0.6882 - acc: 0.7873 - val_loss: 0.4606 - val_acc: 0.8654\n",
      "Epoch 78/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6762 - acc: 0.7908\n",
      "Epoch 00078: val_loss did not improve from 0.45730\n",
      "36805/36805 [==============================] - 30s 815us/sample - loss: 0.6761 - acc: 0.7908 - val_loss: 0.4609 - val_acc: 0.8581\n",
      "Epoch 79/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6738 - acc: 0.7891\n",
      "Epoch 00079: val_loss improved from 0.45730 to 0.45719, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_4_conv_checkpoint/079-0.4572.hdf5\n",
      "36805/36805 [==============================] - 30s 817us/sample - loss: 0.6738 - acc: 0.7892 - val_loss: 0.4572 - val_acc: 0.8649\n",
      "Epoch 80/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6756 - acc: 0.7868\n",
      "Epoch 00080: val_loss improved from 0.45719 to 0.45097, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_4_conv_checkpoint/080-0.4510.hdf5\n",
      "36805/36805 [==============================] - 30s 815us/sample - loss: 0.6757 - acc: 0.7868 - val_loss: 0.4510 - val_acc: 0.8654\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6715 - acc: 0.7888\n",
      "Epoch 00081: val_loss improved from 0.45097 to 0.44538, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_4_conv_checkpoint/081-0.4454.hdf5\n",
      "36805/36805 [==============================] - 30s 812us/sample - loss: 0.6714 - acc: 0.7888 - val_loss: 0.4454 - val_acc: 0.8649\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6691 - acc: 0.7928- E\n",
      "Epoch 00082: val_loss improved from 0.44538 to 0.44181, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_4_conv_checkpoint/082-0.4418.hdf5\n",
      "36805/36805 [==============================] - 30s 810us/sample - loss: 0.6692 - acc: 0.7927 - val_loss: 0.4418 - val_acc: 0.8682\n",
      "Epoch 83/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6659 - acc: 0.7941\n",
      "Epoch 00083: val_loss improved from 0.44181 to 0.43894, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_4_conv_checkpoint/083-0.4389.hdf5\n",
      "36805/36805 [==============================] - 30s 813us/sample - loss: 0.6660 - acc: 0.7940 - val_loss: 0.4389 - val_acc: 0.8693\n",
      "Epoch 84/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6600 - acc: 0.7932\n",
      "Epoch 00084: val_loss did not improve from 0.43894\n",
      "36805/36805 [==============================] - 30s 812us/sample - loss: 0.6599 - acc: 0.7933 - val_loss: 0.4414 - val_acc: 0.8693\n",
      "Epoch 85/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6598 - acc: 0.7927\n",
      "Epoch 00085: val_loss did not improve from 0.43894\n",
      "36805/36805 [==============================] - 30s 814us/sample - loss: 0.6599 - acc: 0.7927 - val_loss: 0.4458 - val_acc: 0.8656\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6548 - acc: 0.7962\n",
      "Epoch 00086: val_loss did not improve from 0.43894\n",
      "36805/36805 [==============================] - 30s 816us/sample - loss: 0.6547 - acc: 0.7963 - val_loss: 0.4408 - val_acc: 0.8658\n",
      "Epoch 87/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6537 - acc: 0.7952\n",
      "Epoch 00087: val_loss improved from 0.43894 to 0.43717, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_4_conv_checkpoint/087-0.4372.hdf5\n",
      "36805/36805 [==============================] - 30s 813us/sample - loss: 0.6540 - acc: 0.7951 - val_loss: 0.4372 - val_acc: 0.8700\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6532 - acc: 0.7958- ETA: 1s - l\n",
      "Epoch 00088: val_loss did not improve from 0.43717\n",
      "36805/36805 [==============================] - 30s 814us/sample - loss: 0.6531 - acc: 0.7958 - val_loss: 0.4381 - val_acc: 0.8670\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6501 - acc: 0.7957\n",
      "Epoch 00089: val_loss improved from 0.43717 to 0.43147, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_4_conv_checkpoint/089-0.4315.hdf5\n",
      "36805/36805 [==============================] - 30s 815us/sample - loss: 0.6502 - acc: 0.7957 - val_loss: 0.4315 - val_acc: 0.8686\n",
      "Epoch 90/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6476 - acc: 0.8000\n",
      "Epoch 00090: val_loss did not improve from 0.43147\n",
      "36805/36805 [==============================] - 30s 814us/sample - loss: 0.6473 - acc: 0.8001 - val_loss: 0.4360 - val_acc: 0.8642\n",
      "Epoch 91/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6505 - acc: 0.7967\n",
      "Epoch 00091: val_loss improved from 0.43147 to 0.42982, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_4_conv_checkpoint/091-0.4298.hdf5\n",
      "36805/36805 [==============================] - 30s 816us/sample - loss: 0.6506 - acc: 0.7966 - val_loss: 0.4298 - val_acc: 0.8710\n",
      "Epoch 92/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6469 - acc: 0.7976\n",
      "Epoch 00092: val_loss did not improve from 0.42982\n",
      "36805/36805 [==============================] - 30s 813us/sample - loss: 0.6468 - acc: 0.7976 - val_loss: 0.4319 - val_acc: 0.8728\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6400 - acc: 0.7983\n",
      "Epoch 00093: val_loss improved from 0.42982 to 0.42828, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_4_conv_checkpoint/093-0.4283.hdf5\n",
      "36805/36805 [==============================] - 30s 815us/sample - loss: 0.6399 - acc: 0.7983 - val_loss: 0.4283 - val_acc: 0.8744\n",
      "Epoch 94/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6407 - acc: 0.8001\n",
      "Epoch 00094: val_loss improved from 0.42828 to 0.42763, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_4_conv_checkpoint/094-0.4276.hdf5\n",
      "36805/36805 [==============================] - 30s 815us/sample - loss: 0.6413 - acc: 0.7999 - val_loss: 0.4276 - val_acc: 0.8717\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6402 - acc: 0.7985\n",
      "Epoch 00095: val_loss did not improve from 0.42763\n",
      "36805/36805 [==============================] - 30s 812us/sample - loss: 0.6403 - acc: 0.7985 - val_loss: 0.4444 - val_acc: 0.8663\n",
      "Epoch 96/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6388 - acc: 0.8005\n",
      "Epoch 00096: val_loss improved from 0.42763 to 0.42582, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_4_conv_checkpoint/096-0.4258.hdf5\n",
      "36805/36805 [==============================] - 30s 818us/sample - loss: 0.6391 - acc: 0.8004 - val_loss: 0.4258 - val_acc: 0.8719\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6382 - acc: 0.7990\n",
      "Epoch 00097: val_loss improved from 0.42582 to 0.42040, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_4_conv_checkpoint/097-0.4204.hdf5\n",
      "36805/36805 [==============================] - 30s 815us/sample - loss: 0.6382 - acc: 0.7990 - val_loss: 0.4204 - val_acc: 0.8740\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6283 - acc: 0.8049\n",
      "Epoch 00098: val_loss did not improve from 0.42040\n",
      "36805/36805 [==============================] - 30s 817us/sample - loss: 0.6284 - acc: 0.8049 - val_loss: 0.4320 - val_acc: 0.8689\n",
      "Epoch 99/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6391 - acc: 0.8011\n",
      "Epoch 00099: val_loss improved from 0.42040 to 0.41884, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_4_conv_checkpoint/099-0.4188.hdf5\n",
      "36805/36805 [==============================] - 30s 816us/sample - loss: 0.6386 - acc: 0.8013 - val_loss: 0.4188 - val_acc: 0.8758\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6281 - acc: 0.8043\n",
      "Epoch 00100: val_loss improved from 0.41884 to 0.41826, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_4_conv_checkpoint/100-0.4183.hdf5\n",
      "36805/36805 [==============================] - 30s 812us/sample - loss: 0.6282 - acc: 0.8042 - val_loss: 0.4183 - val_acc: 0.8726\n",
      "Epoch 101/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6319 - acc: 0.8008\n",
      "Epoch 00101: val_loss did not improve from 0.41826\n",
      "36805/36805 [==============================] - 30s 811us/sample - loss: 0.6315 - acc: 0.8010 - val_loss: 0.4262 - val_acc: 0.8737\n",
      "Epoch 102/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6221 - acc: 0.8048\n",
      "Epoch 00102: val_loss improved from 0.41826 to 0.41492, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_4_conv_checkpoint/102-0.4149.hdf5\n",
      "36805/36805 [==============================] - 30s 815us/sample - loss: 0.6219 - acc: 0.8049 - val_loss: 0.4149 - val_acc: 0.8749\n",
      "Epoch 103/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6205 - acc: 0.8070- ETA: 3s - loss: 0 - ETA: 1s - lo\n",
      "Epoch 00103: val_loss did not improve from 0.41492\n",
      "36805/36805 [==============================] - 30s 812us/sample - loss: 0.6204 - acc: 0.8069 - val_loss: 0.4167 - val_acc: 0.8724\n",
      "Epoch 104/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6190 - acc: 0.8056\n",
      "Epoch 00104: val_loss did not improve from 0.41492\n",
      "36805/36805 [==============================] - 30s 815us/sample - loss: 0.6189 - acc: 0.8056 - val_loss: 0.4341 - val_acc: 0.8682\n",
      "Epoch 105/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6221 - acc: 0.8067\n",
      "Epoch 00105: val_loss did not improve from 0.41492\n",
      "36805/36805 [==============================] - 30s 815us/sample - loss: 0.6221 - acc: 0.8067 - val_loss: 0.4164 - val_acc: 0.8726\n",
      "Epoch 106/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6135 - acc: 0.8066- ETA: 0s - loss: 0.6130 - a\n",
      "Epoch 00106: val_loss did not improve from 0.41492\n",
      "36805/36805 [==============================] - 30s 810us/sample - loss: 0.6136 - acc: 0.8065 - val_loss: 0.4208 - val_acc: 0.8719\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6170 - acc: 0.8081\n",
      "Epoch 00107: val_loss improved from 0.41492 to 0.41072, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_4_conv_checkpoint/107-0.4107.hdf5\n",
      "36805/36805 [==============================] - 30s 814us/sample - loss: 0.6170 - acc: 0.8081 - val_loss: 0.4107 - val_acc: 0.8763\n",
      "Epoch 108/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6137 - acc: 0.8070\n",
      "Epoch 00108: val_loss improved from 0.41072 to 0.41012, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_4_conv_checkpoint/108-0.4101.hdf5\n",
      "36805/36805 [==============================] - 30s 818us/sample - loss: 0.6138 - acc: 0.8069 - val_loss: 0.4101 - val_acc: 0.8721\n",
      "Epoch 109/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6137 - acc: 0.8089\n",
      "Epoch 00109: val_loss improved from 0.41012 to 0.40596, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_4_conv_checkpoint/109-0.4060.hdf5\n",
      "36805/36805 [==============================] - 30s 813us/sample - loss: 0.6140 - acc: 0.8089 - val_loss: 0.4060 - val_acc: 0.8756\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6091 - acc: 0.8101\n",
      "Epoch 00110: val_loss improved from 0.40596 to 0.40186, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_4_conv_checkpoint/110-0.4019.hdf5\n",
      "36805/36805 [==============================] - 30s 818us/sample - loss: 0.6091 - acc: 0.8101 - val_loss: 0.4019 - val_acc: 0.8796\n",
      "Epoch 111/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6023 - acc: 0.8106\n",
      "Epoch 00111: val_loss did not improve from 0.40186\n",
      "36805/36805 [==============================] - 30s 814us/sample - loss: 0.6025 - acc: 0.8105 - val_loss: 0.4212 - val_acc: 0.8733\n",
      "Epoch 112/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6093 - acc: 0.8071\n",
      "Epoch 00112: val_loss improved from 0.40186 to 0.40063, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_4_conv_checkpoint/112-0.4006.hdf5\n",
      "36805/36805 [==============================] - 30s 814us/sample - loss: 0.6094 - acc: 0.8071 - val_loss: 0.4006 - val_acc: 0.8786\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6053 - acc: 0.8077\n",
      "Epoch 00113: val_loss did not improve from 0.40063\n",
      "36805/36805 [==============================] - 30s 815us/sample - loss: 0.6052 - acc: 0.8077 - val_loss: 0.4100 - val_acc: 0.8763\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6023 - acc: 0.8116\n",
      "Epoch 00114: val_loss did not improve from 0.40063\n",
      "36805/36805 [==============================] - 30s 816us/sample - loss: 0.6023 - acc: 0.8117 - val_loss: 0.4054 - val_acc: 0.8784\n",
      "Epoch 115/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6063 - acc: 0.8092\n",
      "Epoch 00115: val_loss did not improve from 0.40063\n",
      "36805/36805 [==============================] - 30s 814us/sample - loss: 0.6065 - acc: 0.8090 - val_loss: 0.4078 - val_acc: 0.8754\n",
      "Epoch 116/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5977 - acc: 0.8122\n",
      "Epoch 00116: val_loss did not improve from 0.40063\n",
      "36805/36805 [==============================] - 30s 812us/sample - loss: 0.5978 - acc: 0.8121 - val_loss: 0.4047 - val_acc: 0.8777\n",
      "Epoch 117/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5994 - acc: 0.8122\n",
      "Epoch 00117: val_loss did not improve from 0.40063\n",
      "36805/36805 [==============================] - 30s 819us/sample - loss: 0.5994 - acc: 0.8123 - val_loss: 0.4021 - val_acc: 0.8786\n",
      "Epoch 118/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5944 - acc: 0.8115\n",
      "Epoch 00118: val_loss improved from 0.40063 to 0.39961, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_4_conv_checkpoint/118-0.3996.hdf5\n",
      "36805/36805 [==============================] - 30s 819us/sample - loss: 0.5948 - acc: 0.8114 - val_loss: 0.3996 - val_acc: 0.8793\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5952 - acc: 0.8132\n",
      "Epoch 00119: val_loss did not improve from 0.39961\n",
      "36805/36805 [==============================] - 30s 817us/sample - loss: 0.5952 - acc: 0.8132 - val_loss: 0.4078 - val_acc: 0.8782\n",
      "Epoch 120/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5873 - acc: 0.8146\n",
      "Epoch 00120: val_loss improved from 0.39961 to 0.39822, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_4_conv_checkpoint/120-0.3982.hdf5\n",
      "36805/36805 [==============================] - 30s 821us/sample - loss: 0.5874 - acc: 0.8145 - val_loss: 0.3982 - val_acc: 0.8817\n",
      "Epoch 121/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5898 - acc: 0.8143\n",
      "Epoch 00121: val_loss improved from 0.39822 to 0.39557, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_4_conv_checkpoint/121-0.3956.hdf5\n",
      "36805/36805 [==============================] - 30s 817us/sample - loss: 0.5894 - acc: 0.8143 - val_loss: 0.3956 - val_acc: 0.8805\n",
      "Epoch 122/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5869 - acc: 0.8153\n",
      "Epoch 00122: val_loss did not improve from 0.39557\n",
      "36805/36805 [==============================] - 30s 815us/sample - loss: 0.5869 - acc: 0.8152 - val_loss: 0.4069 - val_acc: 0.8742\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5854 - acc: 0.8150\n",
      "Epoch 00123: val_loss did not improve from 0.39557\n",
      "36805/36805 [==============================] - 30s 817us/sample - loss: 0.5854 - acc: 0.8150 - val_loss: 0.4141 - val_acc: 0.8751\n",
      "Epoch 124/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5815 - acc: 0.8180\n",
      "Epoch 00124: val_loss improved from 0.39557 to 0.39102, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_4_conv_checkpoint/124-0.3910.hdf5\n",
      "36805/36805 [==============================] - 30s 815us/sample - loss: 0.5816 - acc: 0.8180 - val_loss: 0.3910 - val_acc: 0.8810\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5842 - acc: 0.8179\n",
      "Epoch 00125: val_loss did not improve from 0.39102\n",
      "36805/36805 [==============================] - 30s 816us/sample - loss: 0.5842 - acc: 0.8179 - val_loss: 0.3997 - val_acc: 0.8791\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5780 - acc: 0.8166\n",
      "Epoch 00126: val_loss did not improve from 0.39102\n",
      "36805/36805 [==============================] - 30s 816us/sample - loss: 0.5779 - acc: 0.8166 - val_loss: 0.3948 - val_acc: 0.8810\n",
      "Epoch 127/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5816 - acc: 0.8194\n",
      "Epoch 00127: val_loss did not improve from 0.39102\n",
      "36805/36805 [==============================] - 30s 815us/sample - loss: 0.5819 - acc: 0.8193 - val_loss: 0.4076 - val_acc: 0.8772\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5826 - acc: 0.8164\n",
      "Epoch 00128: val_loss did not improve from 0.39102\n",
      "36805/36805 [==============================] - 30s 814us/sample - loss: 0.5826 - acc: 0.8164 - val_loss: 0.4002 - val_acc: 0.8756\n",
      "Epoch 129/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5707 - acc: 0.8198\n",
      "Epoch 00129: val_loss did not improve from 0.39102\n",
      "36805/36805 [==============================] - 30s 814us/sample - loss: 0.5704 - acc: 0.8198 - val_loss: 0.3938 - val_acc: 0.8817\n",
      "Epoch 130/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5759 - acc: 0.8180\n",
      "Epoch 00130: val_loss did not improve from 0.39102\n",
      "36805/36805 [==============================] - 30s 814us/sample - loss: 0.5760 - acc: 0.8180 - val_loss: 0.3955 - val_acc: 0.8807\n",
      "Epoch 131/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5723 - acc: 0.8200\n",
      "Epoch 00131: val_loss improved from 0.39102 to 0.38805, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_4_conv_checkpoint/131-0.3880.hdf5\n",
      "36805/36805 [==============================] - 30s 819us/sample - loss: 0.5722 - acc: 0.8199 - val_loss: 0.3880 - val_acc: 0.8847\n",
      "Epoch 132/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5719 - acc: 0.8213\n",
      "Epoch 00132: val_loss did not improve from 0.38805\n",
      "36805/36805 [==============================] - 30s 815us/sample - loss: 0.5717 - acc: 0.8214 - val_loss: 0.3928 - val_acc: 0.8824\n",
      "Epoch 133/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5651 - acc: 0.8207\n",
      "Epoch 00133: val_loss did not improve from 0.38805\n",
      "36805/36805 [==============================] - 30s 813us/sample - loss: 0.5651 - acc: 0.8207 - val_loss: 0.3957 - val_acc: 0.8791\n",
      "Epoch 134/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5744 - acc: 0.8179\n",
      "Epoch 00134: val_loss did not improve from 0.38805\n",
      "36805/36805 [==============================] - 30s 817us/sample - loss: 0.5744 - acc: 0.8179 - val_loss: 0.3947 - val_acc: 0.8803\n",
      "Epoch 135/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5702 - acc: 0.8205\n",
      "Epoch 00135: val_loss improved from 0.38805 to 0.38471, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_4_conv_checkpoint/135-0.3847.hdf5\n",
      "36805/36805 [==============================] - 30s 814us/sample - loss: 0.5704 - acc: 0.8204 - val_loss: 0.3847 - val_acc: 0.8835\n",
      "Epoch 136/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5678 - acc: 0.8205\n",
      "Epoch 00136: val_loss did not improve from 0.38471\n",
      "36805/36805 [==============================] - 30s 814us/sample - loss: 0.5678 - acc: 0.8205 - val_loss: 0.3880 - val_acc: 0.8826\n",
      "Epoch 137/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5685 - acc: 0.8200\n",
      "Epoch 00137: val_loss did not improve from 0.38471\n",
      "36805/36805 [==============================] - 30s 815us/sample - loss: 0.5685 - acc: 0.8200 - val_loss: 0.3929 - val_acc: 0.8798\n",
      "Epoch 138/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5670 - acc: 0.8216\n",
      "Epoch 00138: val_loss did not improve from 0.38471\n",
      "36805/36805 [==============================] - 30s 814us/sample - loss: 0.5669 - acc: 0.8216 - val_loss: 0.3861 - val_acc: 0.8831\n",
      "Epoch 139/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5658 - acc: 0.8217\n",
      "Epoch 00139: val_loss did not improve from 0.38471\n",
      "36805/36805 [==============================] - 30s 814us/sample - loss: 0.5654 - acc: 0.8219 - val_loss: 0.4005 - val_acc: 0.8803\n",
      "Epoch 140/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5613 - acc: 0.8241\n",
      "Epoch 00140: val_loss did not improve from 0.38471\n",
      "36805/36805 [==============================] - 30s 817us/sample - loss: 0.5612 - acc: 0.8241 - val_loss: 0.3954 - val_acc: 0.8819\n",
      "Epoch 141/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5586 - acc: 0.8252\n",
      "Epoch 00141: val_loss did not improve from 0.38471\n",
      "36805/36805 [==============================] - 30s 814us/sample - loss: 0.5587 - acc: 0.8251 - val_loss: 0.3862 - val_acc: 0.8849\n",
      "Epoch 142/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5521 - acc: 0.8256\n",
      "Epoch 00142: val_loss improved from 0.38471 to 0.38371, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_4_conv_checkpoint/142-0.3837.hdf5\n",
      "36805/36805 [==============================] - 30s 818us/sample - loss: 0.5522 - acc: 0.8257 - val_loss: 0.3837 - val_acc: 0.8826\n",
      "Epoch 143/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5581 - acc: 0.8247\n",
      "Epoch 00143: val_loss improved from 0.38371 to 0.38213, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_4_conv_checkpoint/143-0.3821.hdf5\n",
      "36805/36805 [==============================] - 30s 815us/sample - loss: 0.5577 - acc: 0.8249 - val_loss: 0.3821 - val_acc: 0.8854\n",
      "Epoch 144/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5548 - acc: 0.8248\n",
      "Epoch 00144: val_loss did not improve from 0.38213\n",
      "36805/36805 [==============================] - 30s 815us/sample - loss: 0.5547 - acc: 0.8249 - val_loss: 0.3968 - val_acc: 0.8784\n",
      "Epoch 145/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5578 - acc: 0.8238\n",
      "Epoch 00145: val_loss did not improve from 0.38213\n",
      "36805/36805 [==============================] - 30s 814us/sample - loss: 0.5576 - acc: 0.8240 - val_loss: 0.3878 - val_acc: 0.8854\n",
      "Epoch 146/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5508 - acc: 0.8255\n",
      "Epoch 00146: val_loss improved from 0.38213 to 0.37897, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_4_conv_checkpoint/146-0.3790.hdf5\n",
      "36805/36805 [==============================] - 30s 815us/sample - loss: 0.5512 - acc: 0.8254 - val_loss: 0.3790 - val_acc: 0.8866\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 147/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5547 - acc: 0.8254\n",
      "Epoch 00147: val_loss did not improve from 0.37897\n",
      "36805/36805 [==============================] - 30s 813us/sample - loss: 0.5547 - acc: 0.8254 - val_loss: 0.3833 - val_acc: 0.8838\n",
      "Epoch 148/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5559 - acc: 0.8229\n",
      "Epoch 00148: val_loss did not improve from 0.37897\n",
      "36805/36805 [==============================] - 30s 817us/sample - loss: 0.5559 - acc: 0.8229 - val_loss: 0.3833 - val_acc: 0.8840\n",
      "Epoch 149/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5478 - acc: 0.8268\n",
      "Epoch 00149: val_loss did not improve from 0.37897\n",
      "36805/36805 [==============================] - 30s 811us/sample - loss: 0.5480 - acc: 0.8267 - val_loss: 0.3948 - val_acc: 0.8782\n",
      "Epoch 150/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5464 - acc: 0.8254\n",
      "Epoch 00150: val_loss did not improve from 0.37897\n",
      "36805/36805 [==============================] - 30s 816us/sample - loss: 0.5463 - acc: 0.8254 - val_loss: 0.3814 - val_acc: 0.8875\n",
      "Epoch 151/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5419 - acc: 0.8287\n",
      "Epoch 00151: val_loss improved from 0.37897 to 0.37763, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_4_conv_checkpoint/151-0.3776.hdf5\n",
      "36805/36805 [==============================] - 30s 818us/sample - loss: 0.5418 - acc: 0.8287 - val_loss: 0.3776 - val_acc: 0.8835\n",
      "Epoch 152/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5453 - acc: 0.8281\n",
      "Epoch 00152: val_loss improved from 0.37763 to 0.37162, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_4_conv_checkpoint/152-0.3716.hdf5\n",
      "36805/36805 [==============================] - 30s 815us/sample - loss: 0.5453 - acc: 0.8281 - val_loss: 0.3716 - val_acc: 0.8849\n",
      "Epoch 153/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5460 - acc: 0.8289\n",
      "Epoch 00153: val_loss improved from 0.37162 to 0.37129, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_4_conv_checkpoint/153-0.3713.hdf5\n",
      "36805/36805 [==============================] - 30s 817us/sample - loss: 0.5460 - acc: 0.8289 - val_loss: 0.3713 - val_acc: 0.8887\n",
      "Epoch 154/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5467 - acc: 0.8273\n",
      "Epoch 00154: val_loss did not improve from 0.37129\n",
      "36805/36805 [==============================] - 30s 816us/sample - loss: 0.5466 - acc: 0.8273 - val_loss: 0.3733 - val_acc: 0.8887\n",
      "Epoch 155/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5396 - acc: 0.8285\n",
      "Epoch 00155: val_loss did not improve from 0.37129\n",
      "36805/36805 [==============================] - 30s 818us/sample - loss: 0.5395 - acc: 0.8285 - val_loss: 0.3816 - val_acc: 0.8840\n",
      "Epoch 156/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5440 - acc: 0.8280- ETA: 0s - loss: 0.5436 - acc: 0.82\n",
      "Epoch 00156: val_loss did not improve from 0.37129\n",
      "36805/36805 [==============================] - 30s 820us/sample - loss: 0.5442 - acc: 0.8278 - val_loss: 0.3821 - val_acc: 0.8840\n",
      "Epoch 157/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5424 - acc: 0.8291\n",
      "Epoch 00157: val_loss did not improve from 0.37129\n",
      "36805/36805 [==============================] - 30s 816us/sample - loss: 0.5426 - acc: 0.8290 - val_loss: 0.3732 - val_acc: 0.8875\n",
      "Epoch 158/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5370 - acc: 0.8308\n",
      "Epoch 00158: val_loss did not improve from 0.37129\n",
      "36805/36805 [==============================] - 30s 813us/sample - loss: 0.5370 - acc: 0.8308 - val_loss: 0.3749 - val_acc: 0.8903\n",
      "Epoch 159/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5362 - acc: 0.8319\n",
      "Epoch 00159: val_loss did not improve from 0.37129\n",
      "36805/36805 [==============================] - 30s 813us/sample - loss: 0.5362 - acc: 0.8319 - val_loss: 0.3758 - val_acc: 0.8887\n",
      "Epoch 160/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5365 - acc: 0.8299\n",
      "Epoch 00160: val_loss did not improve from 0.37129\n",
      "36805/36805 [==============================] - 30s 816us/sample - loss: 0.5368 - acc: 0.8298 - val_loss: 0.3942 - val_acc: 0.8838\n",
      "Epoch 161/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5395 - acc: 0.8280\n",
      "Epoch 00161: val_loss did not improve from 0.37129\n",
      "36805/36805 [==============================] - 30s 815us/sample - loss: 0.5396 - acc: 0.8280 - val_loss: 0.3743 - val_acc: 0.8868\n",
      "Epoch 162/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5277 - acc: 0.8340\n",
      "Epoch 00162: val_loss improved from 0.37129 to 0.36991, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_4_conv_checkpoint/162-0.3699.hdf5\n",
      "36805/36805 [==============================] - 30s 814us/sample - loss: 0.5276 - acc: 0.8340 - val_loss: 0.3699 - val_acc: 0.8901\n",
      "Epoch 163/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5352 - acc: 0.8296- ETA: 1s - l\n",
      "Epoch 00163: val_loss did not improve from 0.36991\n",
      "36805/36805 [==============================] - 30s 814us/sample - loss: 0.5350 - acc: 0.8296 - val_loss: 0.3712 - val_acc: 0.8891\n",
      "Epoch 164/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5340 - acc: 0.8340\n",
      "Epoch 00164: val_loss did not improve from 0.36991\n",
      "36805/36805 [==============================] - 30s 819us/sample - loss: 0.5337 - acc: 0.8341 - val_loss: 0.3713 - val_acc: 0.8831\n",
      "Epoch 165/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5289 - acc: 0.8347\n",
      "Epoch 00165: val_loss improved from 0.36991 to 0.36854, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_4_conv_checkpoint/165-0.3685.hdf5\n",
      "36805/36805 [==============================] - 30s 818us/sample - loss: 0.5286 - acc: 0.8347 - val_loss: 0.3685 - val_acc: 0.8877\n",
      "Epoch 166/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5323 - acc: 0.8318\n",
      "Epoch 00166: val_loss did not improve from 0.36854\n",
      "36805/36805 [==============================] - 30s 820us/sample - loss: 0.5322 - acc: 0.8318 - val_loss: 0.3790 - val_acc: 0.8835\n",
      "Epoch 167/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5247 - acc: 0.8353\n",
      "Epoch 00167: val_loss did not improve from 0.36854\n",
      "36805/36805 [==============================] - 30s 818us/sample - loss: 0.5249 - acc: 0.8353 - val_loss: 0.3752 - val_acc: 0.8854\n",
      "Epoch 168/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5296 - acc: 0.8329\n",
      "Epoch 00168: val_loss did not improve from 0.36854\n",
      "36805/36805 [==============================] - 30s 827us/sample - loss: 0.5296 - acc: 0.8329 - val_loss: 0.3686 - val_acc: 0.8915\n",
      "Epoch 169/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5323 - acc: 0.8330- ETA: 1s - loss\n",
      "Epoch 00169: val_loss did not improve from 0.36854\n",
      "36805/36805 [==============================] - 31s 830us/sample - loss: 0.5322 - acc: 0.8330 - val_loss: 0.3722 - val_acc: 0.8854\n",
      "Epoch 170/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5228 - acc: 0.8356\n",
      "Epoch 00170: val_loss did not improve from 0.36854\n",
      "36805/36805 [==============================] - 30s 817us/sample - loss: 0.5228 - acc: 0.8356 - val_loss: 0.3745 - val_acc: 0.8866\n",
      "Epoch 171/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5228 - acc: 0.8336\n",
      "Epoch 00171: val_loss improved from 0.36854 to 0.36652, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_4_conv_checkpoint/171-0.3665.hdf5\n",
      "36805/36805 [==============================] - 30s 818us/sample - loss: 0.5228 - acc: 0.8336 - val_loss: 0.3665 - val_acc: 0.8891\n",
      "Epoch 172/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5235 - acc: 0.8327\n",
      "Epoch 00172: val_loss did not improve from 0.36652\n",
      "36805/36805 [==============================] - 30s 822us/sample - loss: 0.5236 - acc: 0.8327 - val_loss: 0.3681 - val_acc: 0.8894\n",
      "Epoch 173/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5198 - acc: 0.8361\n",
      "Epoch 00173: val_loss did not improve from 0.36652\n",
      "36805/36805 [==============================] - 30s 814us/sample - loss: 0.5198 - acc: 0.8361 - val_loss: 0.3762 - val_acc: 0.8896\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 174/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5189 - acc: 0.8374\n",
      "Epoch 00174: val_loss did not improve from 0.36652\n",
      "36805/36805 [==============================] - 30s 817us/sample - loss: 0.5191 - acc: 0.8373 - val_loss: 0.3756 - val_acc: 0.8868\n",
      "Epoch 175/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5124 - acc: 0.8385\n",
      "Epoch 00175: val_loss did not improve from 0.36652\n",
      "36805/36805 [==============================] - 30s 815us/sample - loss: 0.5124 - acc: 0.8385 - val_loss: 0.3685 - val_acc: 0.8905\n",
      "Epoch 176/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5242 - acc: 0.8358\n",
      "Epoch 00176: val_loss improved from 0.36652 to 0.36268, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_4_conv_checkpoint/176-0.3627.hdf5\n",
      "36805/36805 [==============================] - 30s 816us/sample - loss: 0.5242 - acc: 0.8358 - val_loss: 0.3627 - val_acc: 0.8915\n",
      "Epoch 177/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5172 - acc: 0.8396\n",
      "Epoch 00177: val_loss did not improve from 0.36268\n",
      "36805/36805 [==============================] - 30s 820us/sample - loss: 0.5173 - acc: 0.8396 - val_loss: 0.3770 - val_acc: 0.8866\n",
      "Epoch 178/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5247 - acc: 0.8349\n",
      "Epoch 00178: val_loss improved from 0.36268 to 0.36005, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_4_conv_checkpoint/178-0.3600.hdf5\n",
      "36805/36805 [==============================] - 30s 812us/sample - loss: 0.5247 - acc: 0.8349 - val_loss: 0.3600 - val_acc: 0.8908\n",
      "Epoch 179/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5128 - acc: 0.8365\n",
      "Epoch 00179: val_loss did not improve from 0.36005\n",
      "36805/36805 [==============================] - 30s 811us/sample - loss: 0.5132 - acc: 0.8365 - val_loss: 0.3664 - val_acc: 0.8894\n",
      "Epoch 180/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5103 - acc: 0.8388\n",
      "Epoch 00180: val_loss did not improve from 0.36005\n",
      "36805/36805 [==============================] - 30s 816us/sample - loss: 0.5103 - acc: 0.8389 - val_loss: 0.3802 - val_acc: 0.8856\n",
      "Epoch 181/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5120 - acc: 0.8376\n",
      "Epoch 00181: val_loss did not improve from 0.36005\n",
      "36805/36805 [==============================] - 30s 815us/sample - loss: 0.5119 - acc: 0.8376 - val_loss: 0.3655 - val_acc: 0.8896\n",
      "Epoch 182/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5155 - acc: 0.8388\n",
      "Epoch 00182: val_loss did not improve from 0.36005\n",
      "36805/36805 [==============================] - 30s 813us/sample - loss: 0.5157 - acc: 0.8387 - val_loss: 0.3776 - val_acc: 0.8845\n",
      "Epoch 183/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5121 - acc: 0.8381\n",
      "Epoch 00183: val_loss did not improve from 0.36005\n",
      "36805/36805 [==============================] - 30s 818us/sample - loss: 0.5121 - acc: 0.8381 - val_loss: 0.3754 - val_acc: 0.8840\n",
      "Epoch 184/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5124 - acc: 0.8380\n",
      "Epoch 00184: val_loss did not improve from 0.36005\n",
      "36805/36805 [==============================] - 30s 812us/sample - loss: 0.5126 - acc: 0.8379 - val_loss: 0.3760 - val_acc: 0.8868\n",
      "Epoch 185/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5072 - acc: 0.8403\n",
      "Epoch 00185: val_loss did not improve from 0.36005\n",
      "36805/36805 [==============================] - 30s 815us/sample - loss: 0.5070 - acc: 0.8403 - val_loss: 0.3689 - val_acc: 0.8891\n",
      "Epoch 186/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5081 - acc: 0.8408\n",
      "Epoch 00186: val_loss did not improve from 0.36005\n",
      "36805/36805 [==============================] - 30s 815us/sample - loss: 0.5082 - acc: 0.8408 - val_loss: 0.3635 - val_acc: 0.8901\n",
      "Epoch 187/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5074 - acc: 0.8371\n",
      "Epoch 00187: val_loss did not improve from 0.36005\n",
      "36805/36805 [==============================] - 30s 812us/sample - loss: 0.5074 - acc: 0.8371 - val_loss: 0.3622 - val_acc: 0.8894\n",
      "Epoch 188/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5044 - acc: 0.8401- ETA: 0s - loss: 0.5047 - acc: 0.8\n",
      "Epoch 00188: val_loss improved from 0.36005 to 0.35407, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_4_conv_checkpoint/188-0.3541.hdf5\n",
      "36805/36805 [==============================] - 30s 824us/sample - loss: 0.5044 - acc: 0.8401 - val_loss: 0.3541 - val_acc: 0.8945\n",
      "Epoch 189/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5092 - acc: 0.8398\n",
      "Epoch 00189: val_loss did not improve from 0.35407\n",
      "36805/36805 [==============================] - 30s 818us/sample - loss: 0.5092 - acc: 0.8398 - val_loss: 0.3675 - val_acc: 0.8891\n",
      "Epoch 190/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5008 - acc: 0.8426\n",
      "Epoch 00190: val_loss did not improve from 0.35407\n",
      "36805/36805 [==============================] - 30s 813us/sample - loss: 0.5008 - acc: 0.8426 - val_loss: 0.3571 - val_acc: 0.8912\n",
      "Epoch 191/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5022 - acc: 0.8411\n",
      "Epoch 00191: val_loss did not improve from 0.35407\n",
      "36805/36805 [==============================] - 30s 814us/sample - loss: 0.5022 - acc: 0.8411 - val_loss: 0.3630 - val_acc: 0.8868\n",
      "Epoch 192/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5059 - acc: 0.8382\n",
      "Epoch 00192: val_loss did not improve from 0.35407\n",
      "36805/36805 [==============================] - 30s 814us/sample - loss: 0.5058 - acc: 0.8382 - val_loss: 0.3618 - val_acc: 0.8889\n",
      "Epoch 193/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4999 - acc: 0.8412\n",
      "Epoch 00193: val_loss did not improve from 0.35407\n",
      "36805/36805 [==============================] - 30s 818us/sample - loss: 0.4999 - acc: 0.8412 - val_loss: 0.3615 - val_acc: 0.8887\n",
      "Epoch 194/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5033 - acc: 0.8402\n",
      "Epoch 00194: val_loss did not improve from 0.35407\n",
      "36805/36805 [==============================] - 30s 814us/sample - loss: 0.5033 - acc: 0.8402 - val_loss: 0.3643 - val_acc: 0.8891\n",
      "Epoch 195/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4953 - acc: 0.8426\n",
      "Epoch 00195: val_loss did not improve from 0.35407\n",
      "36805/36805 [==============================] - 30s 817us/sample - loss: 0.4952 - acc: 0.8426 - val_loss: 0.3647 - val_acc: 0.8921\n",
      "Epoch 196/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4932 - acc: 0.8418- ETA: 2s\n",
      "Epoch 00196: val_loss did not improve from 0.35407\n",
      "36805/36805 [==============================] - 30s 819us/sample - loss: 0.4932 - acc: 0.8419 - val_loss: 0.3619 - val_acc: 0.8903\n",
      "Epoch 197/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4962 - acc: 0.8408\n",
      "Epoch 00197: val_loss did not improve from 0.35407\n",
      "36805/36805 [==============================] - 30s 816us/sample - loss: 0.4962 - acc: 0.8408 - val_loss: 0.3628 - val_acc: 0.8924\n",
      "Epoch 198/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4934 - acc: 0.8427\n",
      "Epoch 00198: val_loss improved from 0.35407 to 0.35382, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_4_conv_checkpoint/198-0.3538.hdf5\n",
      "36805/36805 [==============================] - 30s 817us/sample - loss: 0.4934 - acc: 0.8427 - val_loss: 0.3538 - val_acc: 0.8956\n",
      "Epoch 199/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5017 - acc: 0.8413\n",
      "Epoch 00199: val_loss did not improve from 0.35382\n",
      "36805/36805 [==============================] - 30s 815us/sample - loss: 0.5017 - acc: 0.8413 - val_loss: 0.3623 - val_acc: 0.8898\n",
      "Epoch 200/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4915 - acc: 0.8423\n",
      "Epoch 00200: val_loss did not improve from 0.35382\n",
      "36805/36805 [==============================] - 30s 811us/sample - loss: 0.4915 - acc: 0.8423 - val_loss: 0.3573 - val_acc: 0.8928\n",
      "Epoch 201/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4911 - acc: 0.8444\n",
      "Epoch 00201: val_loss did not improve from 0.35382\n",
      "36805/36805 [==============================] - 30s 814us/sample - loss: 0.4911 - acc: 0.8444 - val_loss: 0.3550 - val_acc: 0.8896\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 202/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4934 - acc: 0.8418\n",
      "Epoch 00202: val_loss did not improve from 0.35382\n",
      "36805/36805 [==============================] - 30s 817us/sample - loss: 0.4934 - acc: 0.8418 - val_loss: 0.3630 - val_acc: 0.8924\n",
      "Epoch 203/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4993 - acc: 0.8402\n",
      "Epoch 00203: val_loss did not improve from 0.35382\n",
      "36805/36805 [==============================] - 30s 813us/sample - loss: 0.4992 - acc: 0.8403 - val_loss: 0.3637 - val_acc: 0.8877\n",
      "Epoch 204/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4931 - acc: 0.8440\n",
      "Epoch 00204: val_loss did not improve from 0.35382\n",
      "36805/36805 [==============================] - 30s 815us/sample - loss: 0.4933 - acc: 0.8440 - val_loss: 0.3592 - val_acc: 0.8908\n",
      "Epoch 205/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4856 - acc: 0.8463\n",
      "Epoch 00205: val_loss did not improve from 0.35382\n",
      "36805/36805 [==============================] - 30s 828us/sample - loss: 0.4855 - acc: 0.8463 - val_loss: 0.3684 - val_acc: 0.8884\n",
      "Epoch 206/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4869 - acc: 0.8437\n",
      "Epoch 00206: val_loss did not improve from 0.35382\n",
      "36805/36805 [==============================] - 30s 826us/sample - loss: 0.4868 - acc: 0.8437 - val_loss: 0.3599 - val_acc: 0.8931\n",
      "Epoch 207/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4919 - acc: 0.8428\n",
      "Epoch 00207: val_loss did not improve from 0.35382\n",
      "36805/36805 [==============================] - 30s 813us/sample - loss: 0.4915 - acc: 0.8429 - val_loss: 0.3587 - val_acc: 0.8921\n",
      "Epoch 208/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4845 - acc: 0.8449\n",
      "Epoch 00208: val_loss did not improve from 0.35382\n",
      "36805/36805 [==============================] - 30s 812us/sample - loss: 0.4844 - acc: 0.8449 - val_loss: 0.3590 - val_acc: 0.8919\n",
      "Epoch 209/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4862 - acc: 0.8443\n",
      "Epoch 00209: val_loss did not improve from 0.35382\n",
      "36805/36805 [==============================] - 30s 826us/sample - loss: 0.4864 - acc: 0.8442 - val_loss: 0.3592 - val_acc: 0.8940\n",
      "Epoch 210/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4839 - acc: 0.8468\n",
      "Epoch 00210: val_loss did not improve from 0.35382\n",
      "36805/36805 [==============================] - 30s 812us/sample - loss: 0.4839 - acc: 0.8468 - val_loss: 0.3603 - val_acc: 0.8924\n",
      "Epoch 211/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4835 - acc: 0.8460\n",
      "Epoch 00211: val_loss did not improve from 0.35382\n",
      "36805/36805 [==============================] - 30s 812us/sample - loss: 0.4835 - acc: 0.8460 - val_loss: 0.3598 - val_acc: 0.8905\n",
      "Epoch 212/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4827 - acc: 0.8467\n",
      "Epoch 00212: val_loss did not improve from 0.35382\n",
      "36805/36805 [==============================] - 30s 815us/sample - loss: 0.4829 - acc: 0.8466 - val_loss: 0.3607 - val_acc: 0.8896\n",
      "Epoch 213/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4823 - acc: 0.8467\n",
      "Epoch 00213: val_loss improved from 0.35382 to 0.35323, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_4_conv_checkpoint/213-0.3532.hdf5\n",
      "36805/36805 [==============================] - 30s 814us/sample - loss: 0.4827 - acc: 0.8467 - val_loss: 0.3532 - val_acc: 0.8968\n",
      "Epoch 214/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4839 - acc: 0.8461\n",
      "Epoch 00214: val_loss did not improve from 0.35323\n",
      "36805/36805 [==============================] - 30s 815us/sample - loss: 0.4836 - acc: 0.8463 - val_loss: 0.3736 - val_acc: 0.8903\n",
      "Epoch 215/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4766 - acc: 0.8482\n",
      "Epoch 00215: val_loss improved from 0.35323 to 0.35243, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_4_conv_checkpoint/215-0.3524.hdf5\n",
      "36805/36805 [==============================] - 30s 815us/sample - loss: 0.4766 - acc: 0.8482 - val_loss: 0.3524 - val_acc: 0.8940\n",
      "Epoch 216/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4764 - acc: 0.8463\n",
      "Epoch 00216: val_loss did not improve from 0.35243\n",
      "36805/36805 [==============================] - 30s 814us/sample - loss: 0.4764 - acc: 0.8462 - val_loss: 0.3602 - val_acc: 0.8901\n",
      "Epoch 217/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4833 - acc: 0.8445\n",
      "Epoch 00217: val_loss did not improve from 0.35243\n",
      "36805/36805 [==============================] - 30s 814us/sample - loss: 0.4833 - acc: 0.8445 - val_loss: 0.3628 - val_acc: 0.8938\n",
      "Epoch 218/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4805 - acc: 0.8466\n",
      "Epoch 00218: val_loss did not improve from 0.35243\n",
      "36805/36805 [==============================] - 30s 819us/sample - loss: 0.4805 - acc: 0.8466 - val_loss: 0.3590 - val_acc: 0.8954\n",
      "Epoch 219/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4758 - acc: 0.8501\n",
      "Epoch 00219: val_loss did not improve from 0.35243\n",
      "36805/36805 [==============================] - 30s 813us/sample - loss: 0.4754 - acc: 0.8502 - val_loss: 0.3536 - val_acc: 0.8915\n",
      "Epoch 220/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4781 - acc: 0.8486\n",
      "Epoch 00220: val_loss did not improve from 0.35243\n",
      "36805/36805 [==============================] - 30s 814us/sample - loss: 0.4780 - acc: 0.8487 - val_loss: 0.3547 - val_acc: 0.8921\n",
      "Epoch 221/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4748 - acc: 0.8482\n",
      "Epoch 00221: val_loss did not improve from 0.35243\n",
      "36805/36805 [==============================] - 30s 814us/sample - loss: 0.4746 - acc: 0.8483 - val_loss: 0.3535 - val_acc: 0.8945\n",
      "Epoch 222/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4776 - acc: 0.8468\n",
      "Epoch 00222: val_loss improved from 0.35243 to 0.35024, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_4_conv_checkpoint/222-0.3502.hdf5\n",
      "36805/36805 [==============================] - 30s 817us/sample - loss: 0.4776 - acc: 0.8468 - val_loss: 0.3502 - val_acc: 0.8959\n",
      "Epoch 223/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4778 - acc: 0.8471\n",
      "Epoch 00223: val_loss did not improve from 0.35024\n",
      "36805/36805 [==============================] - 30s 814us/sample - loss: 0.4778 - acc: 0.8470 - val_loss: 0.3604 - val_acc: 0.8912\n",
      "Epoch 224/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4767 - acc: 0.8482\n",
      "Epoch 00224: val_loss did not improve from 0.35024\n",
      "36805/36805 [==============================] - 30s 817us/sample - loss: 0.4766 - acc: 0.8482 - val_loss: 0.3517 - val_acc: 0.8935\n",
      "Epoch 225/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4785 - acc: 0.8488\n",
      "Epoch 00225: val_loss did not improve from 0.35024\n",
      "36805/36805 [==============================] - 30s 816us/sample - loss: 0.4784 - acc: 0.8488 - val_loss: 0.3596 - val_acc: 0.8903\n",
      "Epoch 226/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4727 - acc: 0.8482\n",
      "Epoch 00226: val_loss did not improve from 0.35024\n",
      "36805/36805 [==============================] - 31s 829us/sample - loss: 0.4727 - acc: 0.8483 - val_loss: 0.3526 - val_acc: 0.8952\n",
      "Epoch 227/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4751 - acc: 0.8495\n",
      "Epoch 00227: val_loss did not improve from 0.35024\n",
      "36805/36805 [==============================] - 31s 830us/sample - loss: 0.4751 - acc: 0.8495 - val_loss: 0.3555 - val_acc: 0.8905\n",
      "Epoch 228/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4710 - acc: 0.8493\n",
      "Epoch 00228: val_loss did not improve from 0.35024\n",
      "36805/36805 [==============================] - 30s 819us/sample - loss: 0.4710 - acc: 0.8493 - val_loss: 0.3553 - val_acc: 0.8924\n",
      "Epoch 229/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4707 - acc: 0.8496\n",
      "Epoch 00229: val_loss did not improve from 0.35024\n",
      "36805/36805 [==============================] - 30s 817us/sample - loss: 0.4708 - acc: 0.8496 - val_loss: 0.3520 - val_acc: 0.8945\n",
      "Epoch 230/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4683 - acc: 0.8497\n",
      "Epoch 00230: val_loss did not improve from 0.35024\n",
      "36805/36805 [==============================] - 30s 818us/sample - loss: 0.4682 - acc: 0.8498 - val_loss: 0.3562 - val_acc: 0.8959\n",
      "Epoch 231/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4661 - acc: 0.8512\n",
      "Epoch 00231: val_loss did not improve from 0.35024\n",
      "36805/36805 [==============================] - 30s 816us/sample - loss: 0.4661 - acc: 0.8512 - val_loss: 0.3506 - val_acc: 0.8947\n",
      "Epoch 232/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4668 - acc: 0.8504\n",
      "Epoch 00232: val_loss did not improve from 0.35024\n",
      "36805/36805 [==============================] - 30s 816us/sample - loss: 0.4670 - acc: 0.8502 - val_loss: 0.3567 - val_acc: 0.8940\n",
      "Epoch 233/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4685 - acc: 0.8507\n",
      "Epoch 00233: val_loss did not improve from 0.35024\n",
      "36805/36805 [==============================] - 30s 814us/sample - loss: 0.4686 - acc: 0.8508 - val_loss: 0.3520 - val_acc: 0.8949\n",
      "Epoch 234/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4694 - acc: 0.8501\n",
      "Epoch 00234: val_loss improved from 0.35024 to 0.34845, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_4_conv_checkpoint/234-0.3485.hdf5\n",
      "36805/36805 [==============================] - 30s 817us/sample - loss: 0.4693 - acc: 0.8501 - val_loss: 0.3485 - val_acc: 0.8926\n",
      "Epoch 235/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4708 - acc: 0.8486\n",
      "Epoch 00235: val_loss did not improve from 0.34845\n",
      "36805/36805 [==============================] - 30s 820us/sample - loss: 0.4709 - acc: 0.8486 - val_loss: 0.3510 - val_acc: 0.8931\n",
      "Epoch 236/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4636 - acc: 0.8505\n",
      "Epoch 00236: val_loss did not improve from 0.34845\n",
      "36805/36805 [==============================] - 30s 816us/sample - loss: 0.4636 - acc: 0.8505 - val_loss: 0.3500 - val_acc: 0.8940\n",
      "Epoch 237/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4636 - acc: 0.8524- ETA: 1s - loss: 0.4\n",
      "Epoch 00237: val_loss improved from 0.34845 to 0.34804, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_4_conv_checkpoint/237-0.3480.hdf5\n",
      "36805/36805 [==============================] - 30s 818us/sample - loss: 0.4637 - acc: 0.8524 - val_loss: 0.3480 - val_acc: 0.8942\n",
      "Epoch 238/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4582 - acc: 0.8536\n",
      "Epoch 00238: val_loss did not improve from 0.34804\n",
      "36805/36805 [==============================] - 30s 819us/sample - loss: 0.4585 - acc: 0.8535 - val_loss: 0.3535 - val_acc: 0.8947\n",
      "Epoch 239/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4635 - acc: 0.8519\n",
      "Epoch 00239: val_loss did not improve from 0.34804\n",
      "36805/36805 [==============================] - 30s 816us/sample - loss: 0.4636 - acc: 0.8519 - val_loss: 0.3516 - val_acc: 0.8947\n",
      "Epoch 240/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4609 - acc: 0.8510\n",
      "Epoch 00240: val_loss did not improve from 0.34804\n",
      "36805/36805 [==============================] - 30s 814us/sample - loss: 0.4609 - acc: 0.8510 - val_loss: 0.3537 - val_acc: 0.8921\n",
      "Epoch 241/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4635 - acc: 0.8543\n",
      "Epoch 00241: val_loss did not improve from 0.34804\n",
      "36805/36805 [==============================] - 30s 819us/sample - loss: 0.4635 - acc: 0.8543 - val_loss: 0.3500 - val_acc: 0.8949\n",
      "Epoch 242/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4640 - acc: 0.8525\n",
      "Epoch 00242: val_loss did not improve from 0.34804\n",
      "36805/36805 [==============================] - 30s 814us/sample - loss: 0.4639 - acc: 0.8525 - val_loss: 0.3580 - val_acc: 0.8921\n",
      "Epoch 243/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4595 - acc: 0.8533\n",
      "Epoch 00243: val_loss did not improve from 0.34804\n",
      "36805/36805 [==============================] - 30s 821us/sample - loss: 0.4594 - acc: 0.8533 - val_loss: 0.3505 - val_acc: 0.8947\n",
      "Epoch 244/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4590 - acc: 0.8526\n",
      "Epoch 00244: val_loss did not improve from 0.34804\n",
      "36805/36805 [==============================] - 30s 821us/sample - loss: 0.4590 - acc: 0.8526 - val_loss: 0.3557 - val_acc: 0.8940\n",
      "Epoch 245/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4572 - acc: 0.8542\n",
      "Epoch 00245: val_loss did not improve from 0.34804\n",
      "36805/36805 [==============================] - 30s 819us/sample - loss: 0.4572 - acc: 0.8542 - val_loss: 0.3485 - val_acc: 0.8954\n",
      "Epoch 246/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4614 - acc: 0.8520\n",
      "Epoch 00246: val_loss improved from 0.34804 to 0.34721, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_4_conv_checkpoint/246-0.3472.hdf5\n",
      "36805/36805 [==============================] - 31s 833us/sample - loss: 0.4615 - acc: 0.8520 - val_loss: 0.3472 - val_acc: 0.8977\n",
      "Epoch 247/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4546 - acc: 0.8537\n",
      "Epoch 00247: val_loss did not improve from 0.34721\n",
      "36805/36805 [==============================] - 31s 830us/sample - loss: 0.4545 - acc: 0.8537 - val_loss: 0.3741 - val_acc: 0.8856\n",
      "Epoch 248/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4586 - acc: 0.8530\n",
      "Epoch 00248: val_loss did not improve from 0.34721\n",
      "36805/36805 [==============================] - 30s 822us/sample - loss: 0.4585 - acc: 0.8531 - val_loss: 0.3512 - val_acc: 0.8945\n",
      "Epoch 249/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4552 - acc: 0.8537\n",
      "Epoch 00249: val_loss did not improve from 0.34721\n",
      "36805/36805 [==============================] - 30s 823us/sample - loss: 0.4552 - acc: 0.8536 - val_loss: 0.3542 - val_acc: 0.8910\n",
      "Epoch 250/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4562 - acc: 0.8526\n",
      "Epoch 00250: val_loss did not improve from 0.34721\n",
      "36805/36805 [==============================] - 30s 818us/sample - loss: 0.4562 - acc: 0.8526 - val_loss: 0.3532 - val_acc: 0.8931\n",
      "Epoch 251/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4577 - acc: 0.8537\n",
      "Epoch 00251: val_loss did not improve from 0.34721\n",
      "36805/36805 [==============================] - 30s 822us/sample - loss: 0.4577 - acc: 0.8537 - val_loss: 0.3551 - val_acc: 0.8933\n",
      "Epoch 252/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4474 - acc: 0.8582\n",
      "Epoch 00252: val_loss did not improve from 0.34721\n",
      "36805/36805 [==============================] - 30s 821us/sample - loss: 0.4473 - acc: 0.8582 - val_loss: 0.3577 - val_acc: 0.8921\n",
      "Epoch 253/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4526 - acc: 0.8552\n",
      "Epoch 00253: val_loss did not improve from 0.34721\n",
      "36805/36805 [==============================] - 30s 822us/sample - loss: 0.4526 - acc: 0.8552 - val_loss: 0.3508 - val_acc: 0.8935\n",
      "Epoch 254/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4598 - acc: 0.8546\n",
      "Epoch 00254: val_loss did not improve from 0.34721\n",
      "36805/36805 [==============================] - 30s 818us/sample - loss: 0.4598 - acc: 0.8546 - val_loss: 0.3473 - val_acc: 0.8921\n",
      "Epoch 255/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4511 - acc: 0.8534\n",
      "Epoch 00255: val_loss did not improve from 0.34721\n",
      "36805/36805 [==============================] - 30s 821us/sample - loss: 0.4511 - acc: 0.8534 - val_loss: 0.3573 - val_acc: 0.8898\n",
      "Epoch 256/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4507 - acc: 0.8576\n",
      "Epoch 00256: val_loss did not improve from 0.34721\n",
      "36805/36805 [==============================] - 30s 820us/sample - loss: 0.4507 - acc: 0.8575 - val_loss: 0.3544 - val_acc: 0.8908\n",
      "Epoch 257/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4554 - acc: 0.8538\n",
      "Epoch 00257: val_loss improved from 0.34721 to 0.34468, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_4_conv_checkpoint/257-0.3447.hdf5\n",
      "36805/36805 [==============================] - 30s 819us/sample - loss: 0.4554 - acc: 0.8538 - val_loss: 0.3447 - val_acc: 0.8970\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 258/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4526 - acc: 0.8557\n",
      "Epoch 00258: val_loss did not improve from 0.34468\n",
      "36805/36805 [==============================] - 30s 819us/sample - loss: 0.4525 - acc: 0.8556 - val_loss: 0.3622 - val_acc: 0.8887\n",
      "Epoch 259/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4457 - acc: 0.8556\n",
      "Epoch 00259: val_loss did not improve from 0.34468\n",
      "36805/36805 [==============================] - 30s 822us/sample - loss: 0.4457 - acc: 0.8556 - val_loss: 0.3522 - val_acc: 0.8956\n",
      "Epoch 260/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4494 - acc: 0.8567\n",
      "Epoch 00260: val_loss did not improve from 0.34468\n",
      "36805/36805 [==============================] - 30s 821us/sample - loss: 0.4494 - acc: 0.8568 - val_loss: 0.3492 - val_acc: 0.8915\n",
      "Epoch 261/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4507 - acc: 0.8533\n",
      "Epoch 00261: val_loss did not improve from 0.34468\n",
      "36805/36805 [==============================] - 30s 822us/sample - loss: 0.4507 - acc: 0.8533 - val_loss: 0.3483 - val_acc: 0.8935\n",
      "Epoch 262/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4489 - acc: 0.8569\n",
      "Epoch 00262: val_loss did not improve from 0.34468\n",
      "36805/36805 [==============================] - 30s 818us/sample - loss: 0.4488 - acc: 0.8569 - val_loss: 0.3469 - val_acc: 0.8917\n",
      "Epoch 263/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4431 - acc: 0.8574\n",
      "Epoch 00263: val_loss did not improve from 0.34468\n",
      "36805/36805 [==============================] - 30s 820us/sample - loss: 0.4431 - acc: 0.8574 - val_loss: 0.3501 - val_acc: 0.8910\n",
      "Epoch 264/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4427 - acc: 0.8589\n",
      "Epoch 00264: val_loss did not improve from 0.34468\n",
      "36805/36805 [==============================] - 30s 818us/sample - loss: 0.4427 - acc: 0.8589 - val_loss: 0.3484 - val_acc: 0.8901\n",
      "Epoch 265/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4427 - acc: 0.8554\n",
      "Epoch 00265: val_loss did not improve from 0.34468\n",
      "36805/36805 [==============================] - 30s 822us/sample - loss: 0.4427 - acc: 0.8554 - val_loss: 0.3475 - val_acc: 0.8935\n",
      "Epoch 266/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4460 - acc: 0.8579\n",
      "Epoch 00266: val_loss did not improve from 0.34468\n",
      "36805/36805 [==============================] - 31s 831us/sample - loss: 0.4460 - acc: 0.8579 - val_loss: 0.3559 - val_acc: 0.8952\n",
      "Epoch 267/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4383 - acc: 0.8597\n",
      "Epoch 00267: val_loss did not improve from 0.34468\n",
      "36805/36805 [==============================] - 31s 831us/sample - loss: 0.4383 - acc: 0.8597 - val_loss: 0.3600 - val_acc: 0.8917\n",
      "Epoch 268/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4457 - acc: 0.8572\n",
      "Epoch 00268: val_loss did not improve from 0.34468\n",
      "36805/36805 [==============================] - 30s 818us/sample - loss: 0.4458 - acc: 0.8572 - val_loss: 0.3548 - val_acc: 0.8917\n",
      "Epoch 269/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4378 - acc: 0.8595\n",
      "Epoch 00269: val_loss did not improve from 0.34468\n",
      "36805/36805 [==============================] - 30s 821us/sample - loss: 0.4378 - acc: 0.8595 - val_loss: 0.3602 - val_acc: 0.8931\n",
      "Epoch 270/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4418 - acc: 0.8598\n",
      "Epoch 00270: val_loss did not improve from 0.34468\n",
      "36805/36805 [==============================] - 30s 827us/sample - loss: 0.4419 - acc: 0.8598 - val_loss: 0.3495 - val_acc: 0.8966\n",
      "Epoch 271/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4444 - acc: 0.8558\n",
      "Epoch 00271: val_loss did not improve from 0.34468\n",
      "36805/36805 [==============================] - 30s 818us/sample - loss: 0.4443 - acc: 0.8558 - val_loss: 0.3610 - val_acc: 0.8908\n",
      "Epoch 272/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4411 - acc: 0.8580- ETA: 0s - loss: 0.4412 - \n",
      "Epoch 00272: val_loss did not improve from 0.34468\n",
      "36805/36805 [==============================] - 30s 814us/sample - loss: 0.4410 - acc: 0.8581 - val_loss: 0.3503 - val_acc: 0.8966\n",
      "Epoch 273/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4430 - acc: 0.8589\n",
      "Epoch 00273: val_loss did not improve from 0.34468\n",
      "36805/36805 [==============================] - 30s 816us/sample - loss: 0.4430 - acc: 0.8589 - val_loss: 0.3542 - val_acc: 0.8956\n",
      "Epoch 274/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4393 - acc: 0.8594\n",
      "Epoch 00274: val_loss did not improve from 0.34468\n",
      "36805/36805 [==============================] - 30s 821us/sample - loss: 0.4394 - acc: 0.8594 - val_loss: 0.3489 - val_acc: 0.8926\n",
      "Epoch 275/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4342 - acc: 0.8601\n",
      "Epoch 00275: val_loss did not improve from 0.34468\n",
      "36805/36805 [==============================] - 30s 822us/sample - loss: 0.4341 - acc: 0.8601 - val_loss: 0.3556 - val_acc: 0.8877\n",
      "Epoch 276/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4380 - acc: 0.8582\n",
      "Epoch 00276: val_loss did not improve from 0.34468\n",
      "36805/36805 [==============================] - 30s 819us/sample - loss: 0.4382 - acc: 0.8580 - val_loss: 0.3485 - val_acc: 0.8928\n",
      "Epoch 277/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4449 - acc: 0.8582\n",
      "Epoch 00277: val_loss did not improve from 0.34468\n",
      "36805/36805 [==============================] - 30s 820us/sample - loss: 0.4446 - acc: 0.8583 - val_loss: 0.3450 - val_acc: 0.8940\n",
      "Epoch 278/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4381 - acc: 0.8596\n",
      "Epoch 00278: val_loss did not improve from 0.34468\n",
      "36805/36805 [==============================] - 30s 820us/sample - loss: 0.4384 - acc: 0.8595 - val_loss: 0.3495 - val_acc: 0.8945\n",
      "Epoch 279/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4394 - acc: 0.8579\n",
      "Epoch 00279: val_loss did not improve from 0.34468\n",
      "36805/36805 [==============================] - 30s 816us/sample - loss: 0.4393 - acc: 0.8580 - val_loss: 0.3470 - val_acc: 0.8952\n",
      "Epoch 280/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4346 - acc: 0.8589\n",
      "Epoch 00280: val_loss did not improve from 0.34468\n",
      "36805/36805 [==============================] - 30s 820us/sample - loss: 0.4343 - acc: 0.8589 - val_loss: 0.3515 - val_acc: 0.8931\n",
      "Epoch 281/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4360 - acc: 0.8593\n",
      "Epoch 00281: val_loss did not improve from 0.34468\n",
      "36805/36805 [==============================] - 30s 822us/sample - loss: 0.4361 - acc: 0.8592 - val_loss: 0.3531 - val_acc: 0.8919\n",
      "Epoch 282/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4370 - acc: 0.8605\n",
      "Epoch 00282: val_loss did not improve from 0.34468\n",
      "36805/36805 [==============================] - 30s 820us/sample - loss: 0.4372 - acc: 0.8603 - val_loss: 0.3514 - val_acc: 0.8921\n",
      "Epoch 283/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4368 - acc: 0.8596\n",
      "Epoch 00283: val_loss did not improve from 0.34468\n",
      "36805/36805 [==============================] - 30s 820us/sample - loss: 0.4368 - acc: 0.8596 - val_loss: 0.3573 - val_acc: 0.8931\n",
      "Epoch 284/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4276 - acc: 0.8611\n",
      "Epoch 00284: val_loss did not improve from 0.34468\n",
      "36805/36805 [==============================] - 30s 821us/sample - loss: 0.4276 - acc: 0.8612 - val_loss: 0.3518 - val_acc: 0.8924\n",
      "Epoch 285/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4311 - acc: 0.8612\n",
      "Epoch 00285: val_loss did not improve from 0.34468\n",
      "36805/36805 [==============================] - 30s 816us/sample - loss: 0.4311 - acc: 0.8611 - val_loss: 0.3495 - val_acc: 0.8926\n",
      "Epoch 286/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4359 - acc: 0.8591\n",
      "Epoch 00286: val_loss did not improve from 0.34468\n",
      "36805/36805 [==============================] - 30s 823us/sample - loss: 0.4359 - acc: 0.8591 - val_loss: 0.3484 - val_acc: 0.8940\n",
      "Epoch 287/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4348 - acc: 0.8618\n",
      "Epoch 00287: val_loss did not improve from 0.34468\n",
      "36805/36805 [==============================] - 31s 829us/sample - loss: 0.4348 - acc: 0.8618 - val_loss: 0.3600 - val_acc: 0.8924\n",
      "Epoch 288/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4288 - acc: 0.8614\n",
      "Epoch 00288: val_loss did not improve from 0.34468\n",
      "36805/36805 [==============================] - 31s 830us/sample - loss: 0.4289 - acc: 0.8614 - val_loss: 0.3530 - val_acc: 0.8945\n",
      "Epoch 289/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4341 - acc: 0.8583\n",
      "Epoch 00289: val_loss did not improve from 0.34468\n",
      "36805/36805 [==============================] - 30s 815us/sample - loss: 0.4340 - acc: 0.8584 - val_loss: 0.3485 - val_acc: 0.8959\n",
      "Epoch 290/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4314 - acc: 0.8605\n",
      "Epoch 00290: val_loss did not improve from 0.34468\n",
      "36805/36805 [==============================] - 30s 824us/sample - loss: 0.4314 - acc: 0.8606 - val_loss: 0.3528 - val_acc: 0.8931\n",
      "Epoch 291/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4365 - acc: 0.8586\n",
      "Epoch 00291: val_loss did not improve from 0.34468\n",
      "36805/36805 [==============================] - 31s 833us/sample - loss: 0.4365 - acc: 0.8586 - val_loss: 0.3509 - val_acc: 0.8940\n",
      "Epoch 292/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4263 - acc: 0.8614\n",
      "Epoch 00292: val_loss did not improve from 0.34468\n",
      "36805/36805 [==============================] - 30s 820us/sample - loss: 0.4267 - acc: 0.8613 - val_loss: 0.3497 - val_acc: 0.8928\n",
      "Epoch 293/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4368 - acc: 0.8602\n",
      "Epoch 00293: val_loss did not improve from 0.34468\n",
      "36805/36805 [==============================] - 30s 819us/sample - loss: 0.4367 - acc: 0.8603 - val_loss: 0.3545 - val_acc: 0.8947\n",
      "Epoch 294/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4237 - acc: 0.8620\n",
      "Epoch 00294: val_loss did not improve from 0.34468\n",
      "36805/36805 [==============================] - 30s 819us/sample - loss: 0.4237 - acc: 0.8620 - val_loss: 0.3567 - val_acc: 0.8910\n",
      "Epoch 295/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4283 - acc: 0.8615\n",
      "Epoch 00295: val_loss did not improve from 0.34468\n",
      "36805/36805 [==============================] - 30s 823us/sample - loss: 0.4284 - acc: 0.8615 - val_loss: 0.3510 - val_acc: 0.8940\n",
      "Epoch 296/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4273 - acc: 0.8634\n",
      "Epoch 00296: val_loss did not improve from 0.34468\n",
      "36805/36805 [==============================] - 30s 817us/sample - loss: 0.4272 - acc: 0.8635 - val_loss: 0.3448 - val_acc: 0.8935\n",
      "Epoch 297/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4277 - acc: 0.8614\n",
      "Epoch 00297: val_loss did not improve from 0.34468\n",
      "36805/36805 [==============================] - 30s 825us/sample - loss: 0.4274 - acc: 0.8615 - val_loss: 0.3512 - val_acc: 0.8940\n",
      "Epoch 298/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4237 - acc: 0.8630\n",
      "Epoch 00298: val_loss did not improve from 0.34468\n",
      "36805/36805 [==============================] - 30s 817us/sample - loss: 0.4234 - acc: 0.8631 - val_loss: 0.3787 - val_acc: 0.8873\n",
      "Epoch 299/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4245 - acc: 0.8622\n",
      "Epoch 00299: val_loss did not improve from 0.34468\n",
      "36805/36805 [==============================] - 30s 820us/sample - loss: 0.4246 - acc: 0.8622 - val_loss: 0.3458 - val_acc: 0.8947\n",
      "Epoch 300/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4253 - acc: 0.8625\n",
      "Epoch 00300: val_loss did not improve from 0.34468\n",
      "36805/36805 [==============================] - 30s 822us/sample - loss: 0.4252 - acc: 0.8624 - val_loss: 0.3515 - val_acc: 0.8942\n",
      "Epoch 301/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4235 - acc: 0.8622\n",
      "Epoch 00301: val_loss did not improve from 0.34468\n",
      "36805/36805 [==============================] - 30s 818us/sample - loss: 0.4235 - acc: 0.8622 - val_loss: 0.3468 - val_acc: 0.8952\n",
      "Epoch 302/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4207 - acc: 0.8655\n",
      "Epoch 00302: val_loss did not improve from 0.34468\n",
      "36805/36805 [==============================] - 30s 817us/sample - loss: 0.4207 - acc: 0.8655 - val_loss: 0.3605 - val_acc: 0.8912\n",
      "Epoch 303/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4224 - acc: 0.8638\n",
      "Epoch 00303: val_loss did not improve from 0.34468\n",
      "36805/36805 [==============================] - 30s 822us/sample - loss: 0.4223 - acc: 0.8638 - val_loss: 0.3512 - val_acc: 0.8947\n",
      "Epoch 304/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4224 - acc: 0.8645\n",
      "Epoch 00304: val_loss improved from 0.34468 to 0.33961, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_4_conv_checkpoint/304-0.3396.hdf5\n",
      "36805/36805 [==============================] - 30s 819us/sample - loss: 0.4224 - acc: 0.8645 - val_loss: 0.3396 - val_acc: 0.8970\n",
      "Epoch 305/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4167 - acc: 0.8667\n",
      "Epoch 00305: val_loss did not improve from 0.33961\n",
      "36805/36805 [==============================] - 30s 819us/sample - loss: 0.4167 - acc: 0.8667 - val_loss: 0.3452 - val_acc: 0.8954\n",
      "Epoch 306/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4224 - acc: 0.8622\n",
      "Epoch 00306: val_loss did not improve from 0.33961\n",
      "36805/36805 [==============================] - 30s 822us/sample - loss: 0.4224 - acc: 0.8622 - val_loss: 0.3439 - val_acc: 0.8968\n",
      "Epoch 307/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4224 - acc: 0.8657\n",
      "Epoch 00307: val_loss did not improve from 0.33961\n",
      "36805/36805 [==============================] - 30s 820us/sample - loss: 0.4223 - acc: 0.8657 - val_loss: 0.3567 - val_acc: 0.8926\n",
      "Epoch 308/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4234 - acc: 0.8622\n",
      "Epoch 00308: val_loss did not improve from 0.33961\n",
      "36805/36805 [==============================] - 30s 826us/sample - loss: 0.4234 - acc: 0.8622 - val_loss: 0.3478 - val_acc: 0.8947\n",
      "Epoch 309/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4158 - acc: 0.8659\n",
      "Epoch 00309: val_loss did not improve from 0.33961\n",
      "36805/36805 [==============================] - 31s 830us/sample - loss: 0.4159 - acc: 0.8658 - val_loss: 0.3512 - val_acc: 0.8931\n",
      "Epoch 310/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4183 - acc: 0.8642\n",
      "Epoch 00310: val_loss did not improve from 0.33961\n",
      "36805/36805 [==============================] - 30s 820us/sample - loss: 0.4183 - acc: 0.8642 - val_loss: 0.3444 - val_acc: 0.8952\n",
      "Epoch 311/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4162 - acc: 0.8660\n",
      "Epoch 00311: val_loss did not improve from 0.33961\n",
      "36805/36805 [==============================] - 30s 818us/sample - loss: 0.4162 - acc: 0.8659 - val_loss: 0.3485 - val_acc: 0.8938\n",
      "Epoch 312/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4195 - acc: 0.8649\n",
      "Epoch 00312: val_loss did not improve from 0.33961\n",
      "36805/36805 [==============================] - 31s 833us/sample - loss: 0.4194 - acc: 0.8649 - val_loss: 0.3477 - val_acc: 0.8956\n",
      "Epoch 313/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4152 - acc: 0.8659\n",
      "Epoch 00313: val_loss did not improve from 0.33961\n",
      "36805/36805 [==============================] - 30s 820us/sample - loss: 0.4153 - acc: 0.8660 - val_loss: 0.3488 - val_acc: 0.8968\n",
      "Epoch 314/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4106 - acc: 0.8656\n",
      "Epoch 00314: val_loss did not improve from 0.33961\n",
      "36805/36805 [==============================] - 30s 824us/sample - loss: 0.4103 - acc: 0.8656 - val_loss: 0.3526 - val_acc: 0.8949\n",
      "Epoch 315/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4136 - acc: 0.8677\n",
      "Epoch 00315: val_loss did not improve from 0.33961\n",
      "36805/36805 [==============================] - 30s 819us/sample - loss: 0.4137 - acc: 0.8677 - val_loss: 0.3703 - val_acc: 0.8887\n",
      "Epoch 316/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4148 - acc: 0.8654\n",
      "Epoch 00316: val_loss did not improve from 0.33961\n",
      "36805/36805 [==============================] - 30s 819us/sample - loss: 0.4147 - acc: 0.8654 - val_loss: 0.3450 - val_acc: 0.8945\n",
      "Epoch 317/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4157 - acc: 0.8646\n",
      "Epoch 00317: val_loss did not improve from 0.33961\n",
      "36805/36805 [==============================] - 30s 822us/sample - loss: 0.4155 - acc: 0.8646 - val_loss: 0.3556 - val_acc: 0.8954\n",
      "Epoch 318/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4134 - acc: 0.8662- ETA: 1s\n",
      "Epoch 00318: val_loss did not improve from 0.33961\n",
      "36805/36805 [==============================] - 30s 821us/sample - loss: 0.4134 - acc: 0.8662 - val_loss: 0.3489 - val_acc: 0.8924\n",
      "Epoch 319/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4180 - acc: 0.8656\n",
      "Epoch 00319: val_loss did not improve from 0.33961\n",
      "36805/36805 [==============================] - 30s 823us/sample - loss: 0.4180 - acc: 0.8656 - val_loss: 0.3482 - val_acc: 0.8970\n",
      "Epoch 320/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4089 - acc: 0.8671\n",
      "Epoch 00320: val_loss did not improve from 0.33961\n",
      "36805/36805 [==============================] - 30s 823us/sample - loss: 0.4090 - acc: 0.8671 - val_loss: 0.3417 - val_acc: 0.8977\n",
      "Epoch 321/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4132 - acc: 0.8672\n",
      "Epoch 00321: val_loss did not improve from 0.33961\n",
      "36805/36805 [==============================] - 30s 823us/sample - loss: 0.4132 - acc: 0.8672 - val_loss: 0.3626 - val_acc: 0.8898\n",
      "Epoch 322/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4165 - acc: 0.8633\n",
      "Epoch 00322: val_loss did not improve from 0.33961\n",
      "36805/36805 [==============================] - 30s 820us/sample - loss: 0.4165 - acc: 0.8634 - val_loss: 0.3557 - val_acc: 0.8949\n",
      "Epoch 323/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4088 - acc: 0.8679\n",
      "Epoch 00323: val_loss did not improve from 0.33961\n",
      "36805/36805 [==============================] - 30s 817us/sample - loss: 0.4087 - acc: 0.8680 - val_loss: 0.3486 - val_acc: 0.8940\n",
      "Epoch 324/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4146 - acc: 0.8664\n",
      "Epoch 00324: val_loss did not improve from 0.33961\n",
      "36805/36805 [==============================] - 30s 822us/sample - loss: 0.4145 - acc: 0.8664 - val_loss: 0.3500 - val_acc: 0.8952\n",
      "Epoch 325/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4130 - acc: 0.8678\n",
      "Epoch 00325: val_loss did not improve from 0.33961\n",
      "36805/36805 [==============================] - 30s 818us/sample - loss: 0.4132 - acc: 0.8678 - val_loss: 0.3418 - val_acc: 0.8959\n",
      "Epoch 326/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4100 - acc: 0.8677\n",
      "Epoch 00326: val_loss did not improve from 0.33961\n",
      "36805/36805 [==============================] - 30s 821us/sample - loss: 0.4099 - acc: 0.8676 - val_loss: 0.3507 - val_acc: 0.8966\n",
      "Epoch 327/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4112 - acc: 0.8675\n",
      "Epoch 00327: val_loss did not improve from 0.33961\n",
      "36805/36805 [==============================] - 30s 820us/sample - loss: 0.4112 - acc: 0.8675 - val_loss: 0.3475 - val_acc: 0.8959\n",
      "Epoch 328/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4159 - acc: 0.8666\n",
      "Epoch 00328: val_loss did not improve from 0.33961\n",
      "36805/36805 [==============================] - 30s 818us/sample - loss: 0.4159 - acc: 0.8666 - val_loss: 0.3536 - val_acc: 0.8935\n",
      "Epoch 329/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4116 - acc: 0.8689\n",
      "Epoch 00329: val_loss did not improve from 0.33961\n",
      "36805/36805 [==============================] - 31s 833us/sample - loss: 0.4117 - acc: 0.8688 - val_loss: 0.3568 - val_acc: 0.8933\n",
      "Epoch 330/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4098 - acc: 0.8662\n",
      "Epoch 00330: val_loss did not improve from 0.33961\n",
      "36805/36805 [==============================] - 31s 831us/sample - loss: 0.4098 - acc: 0.8662 - val_loss: 0.3496 - val_acc: 0.8919\n",
      "Epoch 331/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4107 - acc: 0.8667\n",
      "Epoch 00331: val_loss did not improve from 0.33961\n",
      "36805/36805 [==============================] - 30s 821us/sample - loss: 0.4106 - acc: 0.8667 - val_loss: 0.3517 - val_acc: 0.8933\n",
      "Epoch 332/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4076 - acc: 0.8690\n",
      "Epoch 00332: val_loss did not improve from 0.33961\n",
      "36805/36805 [==============================] - 30s 820us/sample - loss: 0.4075 - acc: 0.8690 - val_loss: 0.3534 - val_acc: 0.8919\n",
      "Epoch 333/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4055 - acc: 0.8696\n",
      "Epoch 00333: val_loss did not improve from 0.33961\n",
      "36805/36805 [==============================] - 31s 833us/sample - loss: 0.4054 - acc: 0.8696 - val_loss: 0.3450 - val_acc: 0.8954\n",
      "Epoch 334/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4020 - acc: 0.8696\n",
      "Epoch 00334: val_loss did not improve from 0.33961\n",
      "36805/36805 [==============================] - 30s 823us/sample - loss: 0.4020 - acc: 0.8696 - val_loss: 0.3539 - val_acc: 0.8952\n",
      "Epoch 335/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4105 - acc: 0.8669\n",
      "Epoch 00335: val_loss did not improve from 0.33961\n",
      "36805/36805 [==============================] - 30s 820us/sample - loss: 0.4104 - acc: 0.8669 - val_loss: 0.3509 - val_acc: 0.8935\n",
      "Epoch 336/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4001 - acc: 0.8693\n",
      "Epoch 00336: val_loss did not improve from 0.33961\n",
      "36805/36805 [==============================] - 30s 826us/sample - loss: 0.4001 - acc: 0.8693 - val_loss: 0.3451 - val_acc: 0.8961\n",
      "Epoch 337/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4023 - acc: 0.8712\n",
      "Epoch 00337: val_loss did not improve from 0.33961\n",
      "36805/36805 [==============================] - 30s 818us/sample - loss: 0.4021 - acc: 0.8712 - val_loss: 0.3489 - val_acc: 0.8968\n",
      "Epoch 338/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4081 - acc: 0.8672\n",
      "Epoch 00338: val_loss did not improve from 0.33961\n",
      "36805/36805 [==============================] - 30s 824us/sample - loss: 0.4081 - acc: 0.8672 - val_loss: 0.3564 - val_acc: 0.8924\n",
      "Epoch 339/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4014 - acc: 0.8690\n",
      "Epoch 00339: val_loss did not improve from 0.33961\n",
      "36805/36805 [==============================] - 30s 819us/sample - loss: 0.4015 - acc: 0.8690 - val_loss: 0.3461 - val_acc: 0.8961\n",
      "Epoch 340/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4065 - acc: 0.8665\n",
      "Epoch 00340: val_loss did not improve from 0.33961\n",
      "36805/36805 [==============================] - 30s 821us/sample - loss: 0.4064 - acc: 0.8666 - val_loss: 0.3682 - val_acc: 0.8947\n",
      "Epoch 341/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4000 - acc: 0.8701\n",
      "Epoch 00341: val_loss did not improve from 0.33961\n",
      "36805/36805 [==============================] - 30s 819us/sample - loss: 0.4000 - acc: 0.8701 - val_loss: 0.3495 - val_acc: 0.8959\n",
      "Epoch 342/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4029 - acc: 0.8694\n",
      "Epoch 00342: val_loss did not improve from 0.33961\n",
      "36805/36805 [==============================] - 30s 816us/sample - loss: 0.4027 - acc: 0.8694 - val_loss: 0.3591 - val_acc: 0.8947\n",
      "Epoch 343/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4031 - acc: 0.8710\n",
      "Epoch 00343: val_loss did not improve from 0.33961\n",
      "36805/36805 [==============================] - 30s 821us/sample - loss: 0.4030 - acc: 0.8710 - val_loss: 0.3475 - val_acc: 0.8963\n",
      "Epoch 344/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4022 - acc: 0.8685\n",
      "Epoch 00344: val_loss did not improve from 0.33961\n",
      "36805/36805 [==============================] - 30s 821us/sample - loss: 0.4020 - acc: 0.8686 - val_loss: 0.3516 - val_acc: 0.8940\n",
      "Epoch 345/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4015 - acc: 0.8695\n",
      "Epoch 00345: val_loss did not improve from 0.33961\n",
      "36805/36805 [==============================] - 30s 819us/sample - loss: 0.4016 - acc: 0.8695 - val_loss: 0.3453 - val_acc: 0.8973\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 346/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4056 - acc: 0.8693\n",
      "Epoch 00346: val_loss did not improve from 0.33961\n",
      "36805/36805 [==============================] - 30s 824us/sample - loss: 0.4056 - acc: 0.8693 - val_loss: 0.3544 - val_acc: 0.8940\n",
      "Epoch 347/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3994 - acc: 0.8698\n",
      "Epoch 00347: val_loss did not improve from 0.33961\n",
      "36805/36805 [==============================] - 30s 818us/sample - loss: 0.3994 - acc: 0.8698 - val_loss: 0.3563 - val_acc: 0.8917\n",
      "Epoch 348/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4054 - acc: 0.8704\n",
      "Epoch 00348: val_loss did not improve from 0.33961\n",
      "36805/36805 [==============================] - 30s 822us/sample - loss: 0.4052 - acc: 0.8704 - val_loss: 0.3518 - val_acc: 0.8938\n",
      "Epoch 349/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4011 - acc: 0.8690\n",
      "Epoch 00349: val_loss did not improve from 0.33961\n",
      "36805/36805 [==============================] - 30s 818us/sample - loss: 0.4010 - acc: 0.8691 - val_loss: 0.3498 - val_acc: 0.8940\n",
      "Epoch 350/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4003 - acc: 0.8700\n",
      "Epoch 00350: val_loss did not improve from 0.33961\n",
      "36805/36805 [==============================] - 31s 830us/sample - loss: 0.4003 - acc: 0.8700 - val_loss: 0.3582 - val_acc: 0.8940\n",
      "Epoch 351/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4019 - acc: 0.8698\n",
      "Epoch 00351: val_loss did not improve from 0.33961\n",
      "36805/36805 [==============================] - 31s 829us/sample - loss: 0.4019 - acc: 0.8699 - val_loss: 0.3470 - val_acc: 0.8968\n",
      "Epoch 352/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3922 - acc: 0.8730\n",
      "Epoch 00352: val_loss did not improve from 0.33961\n",
      "36805/36805 [==============================] - 30s 822us/sample - loss: 0.3921 - acc: 0.8731 - val_loss: 0.3457 - val_acc: 0.8954\n",
      "Epoch 353/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4008 - acc: 0.8690\n",
      "Epoch 00353: val_loss did not improve from 0.33961\n",
      "36805/36805 [==============================] - 30s 825us/sample - loss: 0.4008 - acc: 0.8689 - val_loss: 0.3553 - val_acc: 0.8928\n",
      "Epoch 354/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4019 - acc: 0.8681\n",
      "Epoch 00354: val_loss did not improve from 0.33961\n",
      "36805/36805 [==============================] - 30s 829us/sample - loss: 0.4019 - acc: 0.8681 - val_loss: 0.3516 - val_acc: 0.8952\n",
      "\n",
      "1D_CNN_custom_multi_3_GMP_DO_4_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 404us/sample - loss: 0.4084 - acc: 0.8768\n",
      "Loss: 0.40840566355119984 Accuracy: 0.8768432\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.4743 - acc: 0.1772\n",
      "Epoch 00001: val_loss improved from inf to 1.99777, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_5_conv_checkpoint/001-1.9978.hdf5\n",
      "36805/36805 [==============================] - 33s 899us/sample - loss: 2.4743 - acc: 0.1773 - val_loss: 1.9978 - val_acc: 0.3720\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9557 - acc: 0.3457\n",
      "Epoch 00002: val_loss improved from 1.99777 to 1.53245, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_5_conv_checkpoint/002-1.5324.hdf5\n",
      "36805/36805 [==============================] - 31s 833us/sample - loss: 1.9557 - acc: 0.3457 - val_loss: 1.5324 - val_acc: 0.5330\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6320 - acc: 0.4562\n",
      "Epoch 00003: val_loss improved from 1.53245 to 1.21962, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_5_conv_checkpoint/003-1.2196.hdf5\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 1.6319 - acc: 0.4562 - val_loss: 1.2196 - val_acc: 0.6371\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3844 - acc: 0.5486\n",
      "Epoch 00004: val_loss improved from 1.21962 to 1.01688, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_5_conv_checkpoint/004-1.0169.hdf5\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 1.3845 - acc: 0.5486 - val_loss: 1.0169 - val_acc: 0.7130\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2020 - acc: 0.6110\n",
      "Epoch 00005: val_loss improved from 1.01688 to 0.86637, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_5_conv_checkpoint/005-0.8664.hdf5\n",
      "36805/36805 [==============================] - 31s 835us/sample - loss: 1.2021 - acc: 0.6110 - val_loss: 0.8664 - val_acc: 0.7452\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0757 - acc: 0.6547\n",
      "Epoch 00006: val_loss improved from 0.86637 to 0.74443, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_5_conv_checkpoint/006-0.7444.hdf5\n",
      "36805/36805 [==============================] - 31s 836us/sample - loss: 1.0756 - acc: 0.6547 - val_loss: 0.7444 - val_acc: 0.7836\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9817 - acc: 0.6886\n",
      "Epoch 00007: val_loss improved from 0.74443 to 0.66211, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_5_conv_checkpoint/007-0.6621.hdf5\n",
      "36805/36805 [==============================] - 31s 836us/sample - loss: 0.9816 - acc: 0.6887 - val_loss: 0.6621 - val_acc: 0.8078\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9018 - acc: 0.7157\n",
      "Epoch 00008: val_loss improved from 0.66211 to 0.61056, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_5_conv_checkpoint/008-0.6106.hdf5\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.9019 - acc: 0.7157 - val_loss: 0.6106 - val_acc: 0.8234\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8491 - acc: 0.7317\n",
      "Epoch 00009: val_loss improved from 0.61056 to 0.55469, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_5_conv_checkpoint/009-0.5547.hdf5\n",
      "36805/36805 [==============================] - 31s 835us/sample - loss: 0.8491 - acc: 0.7317 - val_loss: 0.5547 - val_acc: 0.8430\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8006 - acc: 0.7516\n",
      "Epoch 00010: val_loss improved from 0.55469 to 0.55152, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_5_conv_checkpoint/010-0.5515.hdf5\n",
      "36805/36805 [==============================] - 31s 840us/sample - loss: 0.8006 - acc: 0.7516 - val_loss: 0.5515 - val_acc: 0.8404\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7595 - acc: 0.7643\n",
      "Epoch 00011: val_loss improved from 0.55152 to 0.49786, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_5_conv_checkpoint/011-0.4979.hdf5\n",
      "36805/36805 [==============================] - 31s 835us/sample - loss: 0.7595 - acc: 0.7643 - val_loss: 0.4979 - val_acc: 0.8549\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7236 - acc: 0.7743\n",
      "Epoch 00012: val_loss improved from 0.49786 to 0.45933, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_5_conv_checkpoint/012-0.4593.hdf5\n",
      "36805/36805 [==============================] - 31s 836us/sample - loss: 0.7236 - acc: 0.7744 - val_loss: 0.4593 - val_acc: 0.8621\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6915 - acc: 0.7854\n",
      "Epoch 00013: val_loss did not improve from 0.45933\n",
      "36805/36805 [==============================] - 31s 832us/sample - loss: 0.6915 - acc: 0.7854 - val_loss: 0.4819 - val_acc: 0.8565\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6644 - acc: 0.7930- ETA: 1s - loss\n",
      "Epoch 00014: val_loss did not improve from 0.45933\n",
      "36805/36805 [==============================] - 31s 836us/sample - loss: 0.6645 - acc: 0.7929 - val_loss: 0.4678 - val_acc: 0.8572\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6433 - acc: 0.8016\n",
      "Epoch 00015: val_loss improved from 0.45933 to 0.40382, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_5_conv_checkpoint/015-0.4038.hdf5\n",
      "36805/36805 [==============================] - 31s 835us/sample - loss: 0.6432 - acc: 0.8016 - val_loss: 0.4038 - val_acc: 0.8803\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6216 - acc: 0.8065\n",
      "Epoch 00016: val_loss did not improve from 0.40382\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.6215 - acc: 0.8065 - val_loss: 0.4055 - val_acc: 0.8784\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5973 - acc: 0.8160\n",
      "Epoch 00017: val_loss improved from 0.40382 to 0.37634, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_5_conv_checkpoint/017-0.3763.hdf5\n",
      "36805/36805 [==============================] - 31s 833us/sample - loss: 0.5973 - acc: 0.8160 - val_loss: 0.3763 - val_acc: 0.8905\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5876 - acc: 0.8195\n",
      "Epoch 00018: val_loss did not improve from 0.37634\n",
      "36805/36805 [==============================] - 30s 829us/sample - loss: 0.5876 - acc: 0.8196 - val_loss: 0.3820 - val_acc: 0.8887\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5683 - acc: 0.8243\n",
      "Epoch 00019: val_loss improved from 0.37634 to 0.35684, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_5_conv_checkpoint/019-0.3568.hdf5\n",
      "36805/36805 [==============================] - 31s 836us/sample - loss: 0.5684 - acc: 0.8243 - val_loss: 0.3568 - val_acc: 0.8945\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5543 - acc: 0.8289\n",
      "Epoch 00020: val_loss did not improve from 0.35684\n",
      "36805/36805 [==============================] - 31s 832us/sample - loss: 0.5544 - acc: 0.8289 - val_loss: 0.3622 - val_acc: 0.8931\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5412 - acc: 0.8330\n",
      "Epoch 00021: val_loss improved from 0.35684 to 0.34812, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_5_conv_checkpoint/021-0.3481.hdf5\n",
      "36805/36805 [==============================] - 31s 834us/sample - loss: 0.5412 - acc: 0.8330 - val_loss: 0.3481 - val_acc: 0.8987\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5326 - acc: 0.8356\n",
      "Epoch 00022: val_loss improved from 0.34812 to 0.33924, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_5_conv_checkpoint/022-0.3392.hdf5\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.5327 - acc: 0.8356 - val_loss: 0.3392 - val_acc: 0.8975\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5199 - acc: 0.8422\n",
      "Epoch 00023: val_loss improved from 0.33924 to 0.33795, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_5_conv_checkpoint/023-0.3379.hdf5\n",
      "36805/36805 [==============================] - 31s 832us/sample - loss: 0.5203 - acc: 0.8421 - val_loss: 0.3379 - val_acc: 0.9031\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5096 - acc: 0.8432\n",
      "Epoch 00024: val_loss improved from 0.33795 to 0.33331, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_5_conv_checkpoint/024-0.3333.hdf5\n",
      "36805/36805 [==============================] - 31s 833us/sample - loss: 0.5097 - acc: 0.8431 - val_loss: 0.3333 - val_acc: 0.9019\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5000 - acc: 0.8468\n",
      "Epoch 00025: val_loss improved from 0.33331 to 0.31245, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_5_conv_checkpoint/025-0.3125.hdf5\n",
      "36805/36805 [==============================] - 31s 835us/sample - loss: 0.5001 - acc: 0.8468 - val_loss: 0.3125 - val_acc: 0.9080\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4899 - acc: 0.8493\n",
      "Epoch 00026: val_loss improved from 0.31245 to 0.30513, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_5_conv_checkpoint/026-0.3051.hdf5\n",
      "36805/36805 [==============================] - 31s 834us/sample - loss: 0.4900 - acc: 0.8493 - val_loss: 0.3051 - val_acc: 0.9106\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4770 - acc: 0.8536\n",
      "Epoch 00027: val_loss improved from 0.30513 to 0.30134, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_5_conv_checkpoint/027-0.3013.hdf5\n",
      "36805/36805 [==============================] - 31s 835us/sample - loss: 0.4771 - acc: 0.8535 - val_loss: 0.3013 - val_acc: 0.9115\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4713 - acc: 0.8537\n",
      "Epoch 00028: val_loss did not improve from 0.30134\n",
      "36805/36805 [==============================] - 31s 833us/sample - loss: 0.4712 - acc: 0.8538 - val_loss: 0.3014 - val_acc: 0.9068\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4617 - acc: 0.8581\n",
      "Epoch 00029: val_loss improved from 0.30134 to 0.29749, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_5_conv_checkpoint/029-0.2975.hdf5\n",
      "36805/36805 [==============================] - 31s 831us/sample - loss: 0.4617 - acc: 0.8581 - val_loss: 0.2975 - val_acc: 0.9106\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4537 - acc: 0.8606\n",
      "Epoch 00030: val_loss improved from 0.29749 to 0.29351, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_5_conv_checkpoint/030-0.2935.hdf5\n",
      "36805/36805 [==============================] - 31s 835us/sample - loss: 0.4537 - acc: 0.8606 - val_loss: 0.2935 - val_acc: 0.9103\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4527 - acc: 0.8623\n",
      "Epoch 00031: val_loss did not improve from 0.29351\n",
      "36805/36805 [==============================] - 31s 830us/sample - loss: 0.4527 - acc: 0.8623 - val_loss: 0.3060 - val_acc: 0.9054\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4446 - acc: 0.8639\n",
      "Epoch 00032: val_loss improved from 0.29351 to 0.27434, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_5_conv_checkpoint/032-0.2743.hdf5\n",
      "36805/36805 [==============================] - 31s 831us/sample - loss: 0.4446 - acc: 0.8638 - val_loss: 0.2743 - val_acc: 0.9217\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4380 - acc: 0.8672\n",
      "Epoch 00033: val_loss improved from 0.27434 to 0.26908, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_5_conv_checkpoint/033-0.2691.hdf5\n",
      "36805/36805 [==============================] - 31s 832us/sample - loss: 0.4380 - acc: 0.8672 - val_loss: 0.2691 - val_acc: 0.9210\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4336 - acc: 0.8655\n",
      "Epoch 00034: val_loss did not improve from 0.26908\n",
      "36805/36805 [==============================] - 31s 833us/sample - loss: 0.4336 - acc: 0.8655 - val_loss: 0.2845 - val_acc: 0.9117\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4227 - acc: 0.8679\n",
      "Epoch 00035: val_loss improved from 0.26908 to 0.26826, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_5_conv_checkpoint/035-0.2683.hdf5\n",
      "36805/36805 [==============================] - 31s 832us/sample - loss: 0.4227 - acc: 0.8679 - val_loss: 0.2683 - val_acc: 0.9203\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4238 - acc: 0.8680\n",
      "Epoch 00036: val_loss did not improve from 0.26826\n",
      "36805/36805 [==============================] - 31s 832us/sample - loss: 0.4241 - acc: 0.8680 - val_loss: 0.2746 - val_acc: 0.9175\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4134 - acc: 0.8715\n",
      "Epoch 00037: val_loss improved from 0.26826 to 0.26714, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_5_conv_checkpoint/037-0.2671.hdf5\n",
      "36805/36805 [==============================] - 31s 832us/sample - loss: 0.4135 - acc: 0.8715 - val_loss: 0.2671 - val_acc: 0.9217\n",
      "Epoch 38/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4097 - acc: 0.8734\n",
      "Epoch 00038: val_loss improved from 0.26714 to 0.26151, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_5_conv_checkpoint/038-0.2615.hdf5\n",
      "36805/36805 [==============================] - 31s 835us/sample - loss: 0.4097 - acc: 0.8733 - val_loss: 0.2615 - val_acc: 0.9234\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4026 - acc: 0.8735\n",
      "Epoch 00039: val_loss did not improve from 0.26151\n",
      "36805/36805 [==============================] - 31s 833us/sample - loss: 0.4026 - acc: 0.8735 - val_loss: 0.2663 - val_acc: 0.9227\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3951 - acc: 0.8787\n",
      "Epoch 00040: val_loss did not improve from 0.26151\n",
      "36805/36805 [==============================] - 31s 833us/sample - loss: 0.3951 - acc: 0.8787 - val_loss: 0.2652 - val_acc: 0.9264\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3920 - acc: 0.8773\n",
      "Epoch 00041: val_loss improved from 0.26151 to 0.24882, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_5_conv_checkpoint/041-0.2488.hdf5\n",
      "36805/36805 [==============================] - 31s 835us/sample - loss: 0.3920 - acc: 0.8772 - val_loss: 0.2488 - val_acc: 0.9311\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3895 - acc: 0.8795\n",
      "Epoch 00042: val_loss improved from 0.24882 to 0.24693, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_5_conv_checkpoint/042-0.2469.hdf5\n",
      "36805/36805 [==============================] - 31s 836us/sample - loss: 0.3895 - acc: 0.8794 - val_loss: 0.2469 - val_acc: 0.9304\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3830 - acc: 0.8820\n",
      "Epoch 00043: val_loss did not improve from 0.24693\n",
      "36805/36805 [==============================] - 31s 835us/sample - loss: 0.3830 - acc: 0.8820 - val_loss: 0.2621 - val_acc: 0.9196\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3790 - acc: 0.8810\n",
      "Epoch 00044: val_loss improved from 0.24693 to 0.24181, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_5_conv_checkpoint/044-0.2418.hdf5\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.3790 - acc: 0.8810 - val_loss: 0.2418 - val_acc: 0.9306\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3798 - acc: 0.8819\n",
      "Epoch 00045: val_loss did not improve from 0.24181\n",
      "36805/36805 [==============================] - 31s 834us/sample - loss: 0.3799 - acc: 0.8819 - val_loss: 0.2445 - val_acc: 0.9294\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3702 - acc: 0.8850\n",
      "Epoch 00046: val_loss improved from 0.24181 to 0.23895, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_5_conv_checkpoint/046-0.2389.hdf5\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.3702 - acc: 0.8850 - val_loss: 0.2389 - val_acc: 0.9320\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3702 - acc: 0.8846\n",
      "Epoch 00047: val_loss did not improve from 0.23895\n",
      "36805/36805 [==============================] - 31s 835us/sample - loss: 0.3702 - acc: 0.8846 - val_loss: 0.2460 - val_acc: 0.9250\n",
      "Epoch 48/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3619 - acc: 0.8882- ETA: 0s - loss: 0.3619 - acc: 0.888\n",
      "Epoch 00048: val_loss did not improve from 0.23895\n",
      "36805/36805 [==============================] - 31s 834us/sample - loss: 0.3619 - acc: 0.8882 - val_loss: 0.2407 - val_acc: 0.9294\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3612 - acc: 0.8885\n",
      "Epoch 00049: val_loss did not improve from 0.23895\n",
      "36805/36805 [==============================] - 31s 834us/sample - loss: 0.3611 - acc: 0.8885 - val_loss: 0.2407 - val_acc: 0.9320\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3598 - acc: 0.8899\n",
      "Epoch 00050: val_loss improved from 0.23895 to 0.23300, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_5_conv_checkpoint/050-0.2330.hdf5\n",
      "36805/36805 [==============================] - 31s 833us/sample - loss: 0.3598 - acc: 0.8899 - val_loss: 0.2330 - val_acc: 0.9334\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3519 - acc: 0.8907\n",
      "Epoch 00051: val_loss did not improve from 0.23300\n",
      "36805/36805 [==============================] - 31s 833us/sample - loss: 0.3520 - acc: 0.8907 - val_loss: 0.2338 - val_acc: 0.9320\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3524 - acc: 0.8917\n",
      "Epoch 00052: val_loss improved from 0.23300 to 0.23002, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_5_conv_checkpoint/052-0.2300.hdf5\n",
      "36805/36805 [==============================] - 31s 832us/sample - loss: 0.3523 - acc: 0.8918 - val_loss: 0.2300 - val_acc: 0.9357\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3481 - acc: 0.8932\n",
      "Epoch 00053: val_loss did not improve from 0.23002\n",
      "36805/36805 [==============================] - 31s 829us/sample - loss: 0.3481 - acc: 0.8932 - val_loss: 0.2332 - val_acc: 0.9366\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3446 - acc: 0.8936\n",
      "Epoch 00054: val_loss improved from 0.23002 to 0.22504, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_5_conv_checkpoint/054-0.2250.hdf5\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.3446 - acc: 0.8936 - val_loss: 0.2250 - val_acc: 0.9348\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3456 - acc: 0.8940\n",
      "Epoch 00055: val_loss did not improve from 0.22504\n",
      "36805/36805 [==============================] - 31s 829us/sample - loss: 0.3456 - acc: 0.8940 - val_loss: 0.2616 - val_acc: 0.9166\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3374 - acc: 0.8967\n",
      "Epoch 00056: val_loss did not improve from 0.22504\n",
      "36805/36805 [==============================] - 31s 831us/sample - loss: 0.3374 - acc: 0.8967 - val_loss: 0.2260 - val_acc: 0.9345\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3377 - acc: 0.8946\n",
      "Epoch 00057: val_loss did not improve from 0.22504\n",
      "36805/36805 [==============================] - 31s 830us/sample - loss: 0.3377 - acc: 0.8946 - val_loss: 0.2398 - val_acc: 0.9287\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3336 - acc: 0.8968\n",
      "Epoch 00058: val_loss did not improve from 0.22504\n",
      "36805/36805 [==============================] - 31s 830us/sample - loss: 0.3335 - acc: 0.8969 - val_loss: 0.2266 - val_acc: 0.9348\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3314 - acc: 0.8965\n",
      "Epoch 00059: val_loss did not improve from 0.22504\n",
      "36805/36805 [==============================] - 31s 833us/sample - loss: 0.3314 - acc: 0.8965 - val_loss: 0.2379 - val_acc: 0.9301\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3307 - acc: 0.8985\n",
      "Epoch 00060: val_loss improved from 0.22504 to 0.22250, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_5_conv_checkpoint/060-0.2225.hdf5\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.3307 - acc: 0.8985 - val_loss: 0.2225 - val_acc: 0.9348\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3224 - acc: 0.8980\n",
      "Epoch 00061: val_loss did not improve from 0.22250\n",
      "36805/36805 [==============================] - 31s 834us/sample - loss: 0.3224 - acc: 0.8980 - val_loss: 0.2274 - val_acc: 0.9306\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3193 - acc: 0.9007\n",
      "Epoch 00062: val_loss did not improve from 0.22250\n",
      "36805/36805 [==============================] - 31s 835us/sample - loss: 0.3193 - acc: 0.9007 - val_loss: 0.2260 - val_acc: 0.9338\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3217 - acc: 0.8999\n",
      "Epoch 00063: val_loss improved from 0.22250 to 0.21877, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_5_conv_checkpoint/063-0.2188.hdf5\n",
      "36805/36805 [==============================] - 31s 835us/sample - loss: 0.3217 - acc: 0.8999 - val_loss: 0.2188 - val_acc: 0.9392\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3142 - acc: 0.9008\n",
      "Epoch 00064: val_loss did not improve from 0.21877\n",
      "36805/36805 [==============================] - 31s 831us/sample - loss: 0.3142 - acc: 0.9008 - val_loss: 0.2197 - val_acc: 0.9352\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3121 - acc: 0.9028\n",
      "Epoch 00065: val_loss improved from 0.21877 to 0.21770, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_5_conv_checkpoint/065-0.2177.hdf5\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.3121 - acc: 0.9028 - val_loss: 0.2177 - val_acc: 0.9404\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3138 - acc: 0.9011\n",
      "Epoch 00066: val_loss did not improve from 0.21770\n",
      "36805/36805 [==============================] - 31s 833us/sample - loss: 0.3138 - acc: 0.9012 - val_loss: 0.2190 - val_acc: 0.9373\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3131 - acc: 0.9021- ETA: 0s - loss: 0.3113\n",
      "Epoch 00067: val_loss improved from 0.21770 to 0.21313, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_5_conv_checkpoint/067-0.2131.hdf5\n",
      "36805/36805 [==============================] - 31s 833us/sample - loss: 0.3131 - acc: 0.9021 - val_loss: 0.2131 - val_acc: 0.9362\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3073 - acc: 0.9033\n",
      "Epoch 00068: val_loss did not improve from 0.21313\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.3074 - acc: 0.9033 - val_loss: 0.2195 - val_acc: 0.9359\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3031 - acc: 0.9042\n",
      "Epoch 00069: val_loss did not improve from 0.21313\n",
      "36805/36805 [==============================] - 31s 832us/sample - loss: 0.3031 - acc: 0.9042 - val_loss: 0.2204 - val_acc: 0.9364\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3031 - acc: 0.9064\n",
      "Epoch 00070: val_loss did not improve from 0.21313\n",
      "36805/36805 [==============================] - 31s 835us/sample - loss: 0.3033 - acc: 0.9064 - val_loss: 0.2217 - val_acc: 0.9352\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3001 - acc: 0.9087\n",
      "Epoch 00071: val_loss improved from 0.21313 to 0.21133, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_5_conv_checkpoint/071-0.2113.hdf5\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.3000 - acc: 0.9087 - val_loss: 0.2113 - val_acc: 0.9366\n",
      "Epoch 72/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2954 - acc: 0.9072\n",
      "Epoch 00072: val_loss improved from 0.21133 to 0.20876, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_5_conv_checkpoint/072-0.2088.hdf5\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.2954 - acc: 0.9072 - val_loss: 0.2088 - val_acc: 0.9390\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2933 - acc: 0.9076\n",
      "Epoch 00073: val_loss improved from 0.20876 to 0.20783, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_5_conv_checkpoint/073-0.2078.hdf5\n",
      "36805/36805 [==============================] - 31s 834us/sample - loss: 0.2933 - acc: 0.9076 - val_loss: 0.2078 - val_acc: 0.9392\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2940 - acc: 0.9086\n",
      "Epoch 00074: val_loss did not improve from 0.20783\n",
      "36805/36805 [==============================] - 31s 836us/sample - loss: 0.2940 - acc: 0.9086 - val_loss: 0.2096 - val_acc: 0.9378\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2917 - acc: 0.9091\n",
      "Epoch 00075: val_loss did not improve from 0.20783\n",
      "36805/36805 [==============================] - 31s 832us/sample - loss: 0.2916 - acc: 0.9091 - val_loss: 0.2092 - val_acc: 0.9401\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2942 - acc: 0.9087\n",
      "Epoch 00076: val_loss improved from 0.20783 to 0.20400, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_5_conv_checkpoint/076-0.2040.hdf5\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.2943 - acc: 0.9087 - val_loss: 0.2040 - val_acc: 0.9408\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2907 - acc: 0.9087\n",
      "Epoch 00077: val_loss did not improve from 0.20400\n",
      "36805/36805 [==============================] - 31s 833us/sample - loss: 0.2908 - acc: 0.9087 - val_loss: 0.2061 - val_acc: 0.9387\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2854 - acc: 0.9094\n",
      "Epoch 00078: val_loss did not improve from 0.20400\n",
      "36805/36805 [==============================] - 31s 835us/sample - loss: 0.2854 - acc: 0.9094 - val_loss: 0.2115 - val_acc: 0.9376\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2816 - acc: 0.9117\n",
      "Epoch 00079: val_loss did not improve from 0.20400\n",
      "36805/36805 [==============================] - 31s 834us/sample - loss: 0.2816 - acc: 0.9117 - val_loss: 0.2135 - val_acc: 0.9387\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2839 - acc: 0.9092\n",
      "Epoch 00080: val_loss did not improve from 0.20400\n",
      "36805/36805 [==============================] - 31s 834us/sample - loss: 0.2839 - acc: 0.9092 - val_loss: 0.2077 - val_acc: 0.9378\n",
      "Epoch 81/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2795 - acc: 0.9109\n",
      "Epoch 00081: val_loss did not improve from 0.20400\n",
      "36805/36805 [==============================] - 31s 832us/sample - loss: 0.2794 - acc: 0.9109 - val_loss: 0.2183 - val_acc: 0.9329\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2789 - acc: 0.9140\n",
      "Epoch 00082: val_loss did not improve from 0.20400\n",
      "36805/36805 [==============================] - 31s 833us/sample - loss: 0.2789 - acc: 0.9140 - val_loss: 0.2096 - val_acc: 0.9394\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2788 - acc: 0.9141\n",
      "Epoch 00083: val_loss did not improve from 0.20400\n",
      "36805/36805 [==============================] - 31s 833us/sample - loss: 0.2788 - acc: 0.9141 - val_loss: 0.2229 - val_acc: 0.9341\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2726 - acc: 0.9147\n",
      "Epoch 00084: val_loss did not improve from 0.20400\n",
      "36805/36805 [==============================] - 31s 833us/sample - loss: 0.2726 - acc: 0.9147 - val_loss: 0.2096 - val_acc: 0.9352\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2707 - acc: 0.9139\n",
      "Epoch 00085: val_loss did not improve from 0.20400\n",
      "36805/36805 [==============================] - 31s 834us/sample - loss: 0.2707 - acc: 0.9139 - val_loss: 0.2046 - val_acc: 0.9429\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2679 - acc: 0.9149\n",
      "Epoch 00086: val_loss did not improve from 0.20400\n",
      "36805/36805 [==============================] - 31s 833us/sample - loss: 0.2680 - acc: 0.9149 - val_loss: 0.2045 - val_acc: 0.9408\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2684 - acc: 0.9155\n",
      "Epoch 00087: val_loss improved from 0.20400 to 0.20201, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_5_conv_checkpoint/087-0.2020.hdf5\n",
      "36805/36805 [==============================] - 31s 835us/sample - loss: 0.2684 - acc: 0.9154 - val_loss: 0.2020 - val_acc: 0.9397\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2651 - acc: 0.9165\n",
      "Epoch 00088: val_loss did not improve from 0.20201\n",
      "36805/36805 [==============================] - 31s 832us/sample - loss: 0.2651 - acc: 0.9165 - val_loss: 0.2028 - val_acc: 0.9415\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2616 - acc: 0.9176\n",
      "Epoch 00089: val_loss did not improve from 0.20201\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.2615 - acc: 0.9176 - val_loss: 0.2021 - val_acc: 0.9404\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2660 - acc: 0.9155\n",
      "Epoch 00090: val_loss did not improve from 0.20201\n",
      "36805/36805 [==============================] - 31s 830us/sample - loss: 0.2659 - acc: 0.9155 - val_loss: 0.2031 - val_acc: 0.9413\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2642 - acc: 0.9164\n",
      "Epoch 00091: val_loss did not improve from 0.20201\n",
      "36805/36805 [==============================] - 31s 836us/sample - loss: 0.2642 - acc: 0.9164 - val_loss: 0.2072 - val_acc: 0.9385\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2583 - acc: 0.9188\n",
      "Epoch 00092: val_loss did not improve from 0.20201\n",
      "36805/36805 [==============================] - 31s 835us/sample - loss: 0.2583 - acc: 0.9188 - val_loss: 0.2079 - val_acc: 0.9385\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2597 - acc: 0.9177\n",
      "Epoch 00093: val_loss did not improve from 0.20201\n",
      "36805/36805 [==============================] - 31s 831us/sample - loss: 0.2597 - acc: 0.9176 - val_loss: 0.2109 - val_acc: 0.9383\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2572 - acc: 0.9210\n",
      "Epoch 00094: val_loss did not improve from 0.20201\n",
      "36805/36805 [==============================] - 31s 835us/sample - loss: 0.2572 - acc: 0.9210 - val_loss: 0.2030 - val_acc: 0.9378\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2532 - acc: 0.9193\n",
      "Epoch 00095: val_loss improved from 0.20201 to 0.19916, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_5_conv_checkpoint/095-0.1992.hdf5\n",
      "36805/36805 [==============================] - 31s 835us/sample - loss: 0.2532 - acc: 0.9194 - val_loss: 0.1992 - val_acc: 0.9422\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2551 - acc: 0.9177\n",
      "Epoch 00096: val_loss did not improve from 0.19916\n",
      "36805/36805 [==============================] - 31s 835us/sample - loss: 0.2551 - acc: 0.9178 - val_loss: 0.2022 - val_acc: 0.9397\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2562 - acc: 0.9173\n",
      "Epoch 00097: val_loss improved from 0.19916 to 0.19450, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_5_conv_checkpoint/097-0.1945.hdf5\n",
      "36805/36805 [==============================] - 31s 834us/sample - loss: 0.2562 - acc: 0.9173 - val_loss: 0.1945 - val_acc: 0.9434\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2542 - acc: 0.9179\n",
      "Epoch 00098: val_loss did not improve from 0.19450\n",
      "36805/36805 [==============================] - 31s 833us/sample - loss: 0.2542 - acc: 0.9179 - val_loss: 0.1986 - val_acc: 0.9418\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2490 - acc: 0.9208\n",
      "Epoch 00099: val_loss did not improve from 0.19450\n",
      "36805/36805 [==============================] - 30s 828us/sample - loss: 0.2489 - acc: 0.9208 - val_loss: 0.2072 - val_acc: 0.9392\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2462 - acc: 0.9224\n",
      "Epoch 00100: val_loss did not improve from 0.19450\n",
      "36805/36805 [==============================] - 31s 835us/sample - loss: 0.2461 - acc: 0.9224 - val_loss: 0.1983 - val_acc: 0.9422\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2462 - acc: 0.9211\n",
      "Epoch 00101: val_loss did not improve from 0.19450\n",
      "36805/36805 [==============================] - 31s 832us/sample - loss: 0.2462 - acc: 0.9211 - val_loss: 0.2043 - val_acc: 0.9404\n",
      "Epoch 102/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2422 - acc: 0.9219\n",
      "Epoch 00102: val_loss did not improve from 0.19450\n",
      "36805/36805 [==============================] - 31s 834us/sample - loss: 0.2422 - acc: 0.9219 - val_loss: 0.2031 - val_acc: 0.9418\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2404 - acc: 0.9243\n",
      "Epoch 00103: val_loss did not improve from 0.19450\n",
      "36805/36805 [==============================] - 31s 832us/sample - loss: 0.2403 - acc: 0.9243 - val_loss: 0.1960 - val_acc: 0.9467\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2406 - acc: 0.9230\n",
      "Epoch 00104: val_loss did not improve from 0.19450\n",
      "36805/36805 [==============================] - 31s 834us/sample - loss: 0.2406 - acc: 0.9230 - val_loss: 0.1981 - val_acc: 0.9415\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2431 - acc: 0.9227\n",
      "Epoch 00105: val_loss did not improve from 0.19450\n",
      "36805/36805 [==============================] - 31s 834us/sample - loss: 0.2430 - acc: 0.9227 - val_loss: 0.2048 - val_acc: 0.9436\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2363 - acc: 0.9237\n",
      "Epoch 00106: val_loss did not improve from 0.19450\n",
      "36805/36805 [==============================] - 30s 829us/sample - loss: 0.2363 - acc: 0.9237 - val_loss: 0.1973 - val_acc: 0.9439\n",
      "Epoch 107/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2345 - acc: 0.9249\n",
      "Epoch 00107: val_loss did not improve from 0.19450\n",
      "36805/36805 [==============================] - 31s 833us/sample - loss: 0.2344 - acc: 0.9250 - val_loss: 0.2124 - val_acc: 0.9392\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2367 - acc: 0.9242\n",
      "Epoch 00108: val_loss did not improve from 0.19450\n",
      "36805/36805 [==============================] - 31s 833us/sample - loss: 0.2367 - acc: 0.9242 - val_loss: 0.2026 - val_acc: 0.9415\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2324 - acc: 0.9254\n",
      "Epoch 00109: val_loss did not improve from 0.19450\n",
      "36805/36805 [==============================] - 31s 832us/sample - loss: 0.2325 - acc: 0.9254 - val_loss: 0.2058 - val_acc: 0.9408\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2328 - acc: 0.9258\n",
      "Epoch 00110: val_loss did not improve from 0.19450\n",
      "36805/36805 [==============================] - 31s 834us/sample - loss: 0.2329 - acc: 0.9257 - val_loss: 0.2103 - val_acc: 0.9371\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2311 - acc: 0.9270\n",
      "Epoch 00111: val_loss did not improve from 0.19450\n",
      "36805/36805 [==============================] - 31s 830us/sample - loss: 0.2311 - acc: 0.9270 - val_loss: 0.1964 - val_acc: 0.9436\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2303 - acc: 0.9265\n",
      "Epoch 00112: val_loss did not improve from 0.19450\n",
      "36805/36805 [==============================] - 31s 829us/sample - loss: 0.2303 - acc: 0.9265 - val_loss: 0.2006 - val_acc: 0.9415\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2311 - acc: 0.9252\n",
      "Epoch 00113: val_loss did not improve from 0.19450\n",
      "36805/36805 [==============================] - 31s 834us/sample - loss: 0.2311 - acc: 0.9252 - val_loss: 0.1953 - val_acc: 0.9453\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2317 - acc: 0.9261\n",
      "Epoch 00114: val_loss did not improve from 0.19450\n",
      "36805/36805 [==============================] - 31s 831us/sample - loss: 0.2318 - acc: 0.9261 - val_loss: 0.2192 - val_acc: 0.9364\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2275 - acc: 0.9275\n",
      "Epoch 00115: val_loss did not improve from 0.19450\n",
      "36805/36805 [==============================] - 31s 835us/sample - loss: 0.2275 - acc: 0.9275 - val_loss: 0.1974 - val_acc: 0.9418\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2302 - acc: 0.9273- ETA: 1s - loss: 0.2299\n",
      "Epoch 00116: val_loss did not improve from 0.19450\n",
      "36805/36805 [==============================] - 31s 832us/sample - loss: 0.2302 - acc: 0.9273 - val_loss: 0.2063 - val_acc: 0.9387\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2269 - acc: 0.9271\n",
      "Epoch 00117: val_loss did not improve from 0.19450\n",
      "36805/36805 [==============================] - 31s 836us/sample - loss: 0.2269 - acc: 0.9272 - val_loss: 0.1998 - val_acc: 0.9422\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2254 - acc: 0.9295\n",
      "Epoch 00118: val_loss did not improve from 0.19450\n",
      "36805/36805 [==============================] - 31s 830us/sample - loss: 0.2253 - acc: 0.9295 - val_loss: 0.1968 - val_acc: 0.9425\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2173 - acc: 0.9312\n",
      "Epoch 00119: val_loss did not improve from 0.19450\n",
      "36805/36805 [==============================] - 31s 832us/sample - loss: 0.2173 - acc: 0.9313 - val_loss: 0.2009 - val_acc: 0.9418\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2204 - acc: 0.9295\n",
      "Epoch 00120: val_loss improved from 0.19450 to 0.19136, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_5_conv_checkpoint/120-0.1914.hdf5\n",
      "36805/36805 [==============================] - 31s 833us/sample - loss: 0.2205 - acc: 0.9294 - val_loss: 0.1914 - val_acc: 0.9446\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2196 - acc: 0.9308\n",
      "Epoch 00121: val_loss did not improve from 0.19136\n",
      "36805/36805 [==============================] - 31s 833us/sample - loss: 0.2196 - acc: 0.9308 - val_loss: 0.1978 - val_acc: 0.9439\n",
      "Epoch 122/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2178 - acc: 0.9295\n",
      "Epoch 00122: val_loss did not improve from 0.19136\n",
      "36805/36805 [==============================] - 31s 833us/sample - loss: 0.2178 - acc: 0.9295 - val_loss: 0.1949 - val_acc: 0.9450\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2195 - acc: 0.9288\n",
      "Epoch 00123: val_loss did not improve from 0.19136\n",
      "36805/36805 [==============================] - 31s 832us/sample - loss: 0.2196 - acc: 0.9288 - val_loss: 0.2035 - val_acc: 0.9390\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2174 - acc: 0.9308\n",
      "Epoch 00124: val_loss did not improve from 0.19136\n",
      "36805/36805 [==============================] - 31s 829us/sample - loss: 0.2174 - acc: 0.9308 - val_loss: 0.2013 - val_acc: 0.9406\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2199 - acc: 0.9286\n",
      "Epoch 00125: val_loss did not improve from 0.19136\n",
      "36805/36805 [==============================] - 31s 835us/sample - loss: 0.2200 - acc: 0.9286 - val_loss: 0.1984 - val_acc: 0.9441\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2129 - acc: 0.9310\n",
      "Epoch 00126: val_loss did not improve from 0.19136\n",
      "36805/36805 [==============================] - 31s 831us/sample - loss: 0.2129 - acc: 0.9310 - val_loss: 0.1923 - val_acc: 0.9457\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2149 - acc: 0.9302\n",
      "Epoch 00127: val_loss did not improve from 0.19136\n",
      "36805/36805 [==============================] - 31s 833us/sample - loss: 0.2149 - acc: 0.9302 - val_loss: 0.2010 - val_acc: 0.9420\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2118 - acc: 0.9318\n",
      "Epoch 00128: val_loss did not improve from 0.19136\n",
      "36805/36805 [==============================] - 31s 834us/sample - loss: 0.2118 - acc: 0.9318 - val_loss: 0.1936 - val_acc: 0.9469\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2150 - acc: 0.9295\n",
      "Epoch 00129: val_loss did not improve from 0.19136\n",
      "36805/36805 [==============================] - 31s 831us/sample - loss: 0.2149 - acc: 0.9295 - val_loss: 0.1954 - val_acc: 0.9439\n",
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2101 - acc: 0.9322\n",
      "Epoch 00130: val_loss did not improve from 0.19136\n",
      "36805/36805 [==============================] - 31s 833us/sample - loss: 0.2101 - acc: 0.9322 - val_loss: 0.2018 - val_acc: 0.9418\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2117 - acc: 0.9313\n",
      "Epoch 00131: val_loss did not improve from 0.19136\n",
      "36805/36805 [==============================] - 31s 833us/sample - loss: 0.2117 - acc: 0.9313 - val_loss: 0.1996 - val_acc: 0.9439\n",
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2061 - acc: 0.9333\n",
      "Epoch 00132: val_loss did not improve from 0.19136\n",
      "36805/36805 [==============================] - 31s 834us/sample - loss: 0.2061 - acc: 0.9334 - val_loss: 0.1999 - val_acc: 0.9439\n",
      "Epoch 133/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2040 - acc: 0.9339\n",
      "Epoch 00133: val_loss did not improve from 0.19136\n",
      "36805/36805 [==============================] - 31s 834us/sample - loss: 0.2040 - acc: 0.9339 - val_loss: 0.1922 - val_acc: 0.9457\n",
      "Epoch 134/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2044 - acc: 0.9335\n",
      "Epoch 00134: val_loss did not improve from 0.19136\n",
      "36805/36805 [==============================] - 31s 835us/sample - loss: 0.2043 - acc: 0.9335 - val_loss: 0.1915 - val_acc: 0.9457\n",
      "Epoch 135/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2017 - acc: 0.9340\n",
      "Epoch 00135: val_loss did not improve from 0.19136\n",
      "36805/36805 [==============================] - 31s 832us/sample - loss: 0.2018 - acc: 0.9340 - val_loss: 0.1975 - val_acc: 0.9422\n",
      "Epoch 136/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2047 - acc: 0.9330- ETA: 0s - loss: 0.2\n",
      "Epoch 00136: val_loss improved from 0.19136 to 0.18903, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_5_conv_checkpoint/136-0.1890.hdf5\n",
      "36805/36805 [==============================] - 31s 833us/sample - loss: 0.2047 - acc: 0.9331 - val_loss: 0.1890 - val_acc: 0.9469\n",
      "Epoch 137/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2027 - acc: 0.9358\n",
      "Epoch 00137: val_loss did not improve from 0.18903\n",
      "36805/36805 [==============================] - 31s 833us/sample - loss: 0.2026 - acc: 0.9358 - val_loss: 0.1958 - val_acc: 0.9460\n",
      "Epoch 138/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1980 - acc: 0.9345\n",
      "Epoch 00138: val_loss did not improve from 0.18903\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.1980 - acc: 0.9345 - val_loss: 0.2007 - val_acc: 0.9420\n",
      "Epoch 139/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1968 - acc: 0.9364\n",
      "Epoch 00139: val_loss did not improve from 0.18903\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.1969 - acc: 0.9364 - val_loss: 0.2026 - val_acc: 0.9404\n",
      "Epoch 140/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2001 - acc: 0.9337\n",
      "Epoch 00140: val_loss did not improve from 0.18903\n",
      "36805/36805 [==============================] - 31s 834us/sample - loss: 0.2001 - acc: 0.9338 - val_loss: 0.1963 - val_acc: 0.9439\n",
      "Epoch 141/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1961 - acc: 0.9366\n",
      "Epoch 00141: val_loss did not improve from 0.18903\n",
      "36805/36805 [==============================] - 31s 835us/sample - loss: 0.1961 - acc: 0.9366 - val_loss: 0.1975 - val_acc: 0.9432\n",
      "Epoch 142/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1978 - acc: 0.9362\n",
      "Epoch 00142: val_loss did not improve from 0.18903\n",
      "36805/36805 [==============================] - 31s 834us/sample - loss: 0.1978 - acc: 0.9361 - val_loss: 0.1929 - val_acc: 0.9450\n",
      "Epoch 143/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1952 - acc: 0.9373\n",
      "Epoch 00143: val_loss did not improve from 0.18903\n",
      "36805/36805 [==============================] - 31s 834us/sample - loss: 0.1951 - acc: 0.9373 - val_loss: 0.2033 - val_acc: 0.9397\n",
      "Epoch 144/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1928 - acc: 0.9371\n",
      "Epoch 00144: val_loss did not improve from 0.18903\n",
      "36805/36805 [==============================] - 31s 833us/sample - loss: 0.1927 - acc: 0.9371 - val_loss: 0.1956 - val_acc: 0.9448\n",
      "Epoch 145/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1938 - acc: 0.9384\n",
      "Epoch 00145: val_loss did not improve from 0.18903\n",
      "36805/36805 [==============================] - 31s 830us/sample - loss: 0.1938 - acc: 0.9384 - val_loss: 0.1991 - val_acc: 0.9450\n",
      "Epoch 146/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1922 - acc: 0.9367\n",
      "Epoch 00146: val_loss did not improve from 0.18903\n",
      "36805/36805 [==============================] - 31s 833us/sample - loss: 0.1922 - acc: 0.9367 - val_loss: 0.2170 - val_acc: 0.9387\n",
      "Epoch 147/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1925 - acc: 0.9382\n",
      "Epoch 00147: val_loss did not improve from 0.18903\n",
      "36805/36805 [==============================] - 31s 834us/sample - loss: 0.1925 - acc: 0.9382 - val_loss: 0.2153 - val_acc: 0.9390\n",
      "Epoch 148/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1901 - acc: 0.9388\n",
      "Epoch 00148: val_loss did not improve from 0.18903\n",
      "36805/36805 [==============================] - 31s 834us/sample - loss: 0.1901 - acc: 0.9388 - val_loss: 0.1986 - val_acc: 0.9427\n",
      "Epoch 149/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1924 - acc: 0.9363\n",
      "Epoch 00149: val_loss did not improve from 0.18903\n",
      "36805/36805 [==============================] - 31s 832us/sample - loss: 0.1924 - acc: 0.9363 - val_loss: 0.2075 - val_acc: 0.9418\n",
      "Epoch 150/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1880 - acc: 0.9389\n",
      "Epoch 00150: val_loss did not improve from 0.18903\n",
      "36805/36805 [==============================] - 31s 835us/sample - loss: 0.1881 - acc: 0.9389 - val_loss: 0.2007 - val_acc: 0.9425\n",
      "Epoch 151/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1862 - acc: 0.9397\n",
      "Epoch 00151: val_loss did not improve from 0.18903\n",
      "36805/36805 [==============================] - 31s 830us/sample - loss: 0.1863 - acc: 0.9397 - val_loss: 0.2009 - val_acc: 0.9429\n",
      "Epoch 152/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1858 - acc: 0.9394\n",
      "Epoch 00152: val_loss did not improve from 0.18903\n",
      "36805/36805 [==============================] - 31s 831us/sample - loss: 0.1858 - acc: 0.9394 - val_loss: 0.2015 - val_acc: 0.9404\n",
      "Epoch 153/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1859 - acc: 0.9388\n",
      "Epoch 00153: val_loss did not improve from 0.18903\n",
      "36805/36805 [==============================] - 31s 832us/sample - loss: 0.1860 - acc: 0.9387 - val_loss: 0.1969 - val_acc: 0.9455\n",
      "Epoch 154/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1869 - acc: 0.9399\n",
      "Epoch 00154: val_loss did not improve from 0.18903\n",
      "36805/36805 [==============================] - 31s 830us/sample - loss: 0.1869 - acc: 0.9399 - val_loss: 0.1968 - val_acc: 0.9418\n",
      "Epoch 155/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1876 - acc: 0.9394\n",
      "Epoch 00155: val_loss did not improve from 0.18903\n",
      "36805/36805 [==============================] - 30s 827us/sample - loss: 0.1876 - acc: 0.9394 - val_loss: 0.2034 - val_acc: 0.9427\n",
      "Epoch 156/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1842 - acc: 0.9397\n",
      "Epoch 00156: val_loss did not improve from 0.18903\n",
      "36805/36805 [==============================] - 31s 831us/sample - loss: 0.1843 - acc: 0.9397 - val_loss: 0.2014 - val_acc: 0.9441\n",
      "Epoch 157/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1831 - acc: 0.9399\n",
      "Epoch 00157: val_loss did not improve from 0.18903\n",
      "36805/36805 [==============================] - 31s 833us/sample - loss: 0.1831 - acc: 0.9400 - val_loss: 0.2080 - val_acc: 0.9427\n",
      "Epoch 158/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1811 - acc: 0.9408\n",
      "Epoch 00158: val_loss did not improve from 0.18903\n",
      "36805/36805 [==============================] - 31s 831us/sample - loss: 0.1811 - acc: 0.9409 - val_loss: 0.1984 - val_acc: 0.9450\n",
      "Epoch 159/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1819 - acc: 0.9402\n",
      "Epoch 00159: val_loss did not improve from 0.18903\n",
      "36805/36805 [==============================] - 31s 831us/sample - loss: 0.1819 - acc: 0.9402 - val_loss: 0.1979 - val_acc: 0.9434\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 160/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1791 - acc: 0.9418\n",
      "Epoch 00160: val_loss did not improve from 0.18903\n",
      "36805/36805 [==============================] - 30s 828us/sample - loss: 0.1791 - acc: 0.9418 - val_loss: 0.2026 - val_acc: 0.9427\n",
      "Epoch 161/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1800 - acc: 0.9399\n",
      "Epoch 00161: val_loss did not improve from 0.18903\n",
      "36805/36805 [==============================] - 31s 830us/sample - loss: 0.1800 - acc: 0.9399 - val_loss: 0.2052 - val_acc: 0.9429\n",
      "Epoch 162/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1787 - acc: 0.9406\n",
      "Epoch 00162: val_loss did not improve from 0.18903\n",
      "36805/36805 [==============================] - 31s 831us/sample - loss: 0.1786 - acc: 0.9406 - val_loss: 0.2028 - val_acc: 0.9411\n",
      "Epoch 163/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1736 - acc: 0.9432\n",
      "Epoch 00163: val_loss did not improve from 0.18903\n",
      "36805/36805 [==============================] - 31s 830us/sample - loss: 0.1736 - acc: 0.9432 - val_loss: 0.1895 - val_acc: 0.9460\n",
      "Epoch 164/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1777 - acc: 0.9424\n",
      "Epoch 00164: val_loss did not improve from 0.18903\n",
      "36805/36805 [==============================] - 30s 828us/sample - loss: 0.1777 - acc: 0.9425 - val_loss: 0.2062 - val_acc: 0.9406\n",
      "Epoch 165/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1762 - acc: 0.9429\n",
      "Epoch 00165: val_loss did not improve from 0.18903\n",
      "36805/36805 [==============================] - 31s 833us/sample - loss: 0.1761 - acc: 0.9429 - val_loss: 0.1948 - val_acc: 0.9436\n",
      "Epoch 166/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1805 - acc: 0.9404\n",
      "Epoch 00166: val_loss did not improve from 0.18903\n",
      "36805/36805 [==============================] - 31s 832us/sample - loss: 0.1805 - acc: 0.9404 - val_loss: 0.1998 - val_acc: 0.9436\n",
      "Epoch 167/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1734 - acc: 0.9427\n",
      "Epoch 00167: val_loss did not improve from 0.18903\n",
      "36805/36805 [==============================] - 30s 828us/sample - loss: 0.1735 - acc: 0.9426 - val_loss: 0.2055 - val_acc: 0.9427\n",
      "Epoch 168/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1725 - acc: 0.9430\n",
      "Epoch 00168: val_loss did not improve from 0.18903\n",
      "36805/36805 [==============================] - 31s 831us/sample - loss: 0.1725 - acc: 0.9429 - val_loss: 0.1973 - val_acc: 0.9467\n",
      "Epoch 169/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1682 - acc: 0.9447\n",
      "Epoch 00169: val_loss did not improve from 0.18903\n",
      "36805/36805 [==============================] - 31s 832us/sample - loss: 0.1682 - acc: 0.9447 - val_loss: 0.2024 - val_acc: 0.9427\n",
      "Epoch 170/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1696 - acc: 0.9441\n",
      "Epoch 00170: val_loss did not improve from 0.18903\n",
      "36805/36805 [==============================] - 31s 829us/sample - loss: 0.1696 - acc: 0.9441 - val_loss: 0.2009 - val_acc: 0.9439\n",
      "Epoch 171/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1703 - acc: 0.9448\n",
      "Epoch 00171: val_loss did not improve from 0.18903\n",
      "36805/36805 [==============================] - 31s 834us/sample - loss: 0.1702 - acc: 0.9448 - val_loss: 0.2019 - val_acc: 0.9441\n",
      "Epoch 172/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1673 - acc: 0.9452\n",
      "Epoch 00172: val_loss did not improve from 0.18903\n",
      "36805/36805 [==============================] - 31s 831us/sample - loss: 0.1673 - acc: 0.9452 - val_loss: 0.2009 - val_acc: 0.9432\n",
      "Epoch 173/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1652 - acc: 0.9449\n",
      "Epoch 00173: val_loss did not improve from 0.18903\n",
      "36805/36805 [==============================] - 31s 830us/sample - loss: 0.1652 - acc: 0.9450 - val_loss: 0.2091 - val_acc: 0.9420\n",
      "Epoch 174/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1690 - acc: 0.9446\n",
      "Epoch 00174: val_loss did not improve from 0.18903\n",
      "36805/36805 [==============================] - 31s 833us/sample - loss: 0.1690 - acc: 0.9446 - val_loss: 0.2054 - val_acc: 0.9418\n",
      "Epoch 175/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1687 - acc: 0.9446\n",
      "Epoch 00175: val_loss did not improve from 0.18903\n",
      "36805/36805 [==============================] - 30s 828us/sample - loss: 0.1688 - acc: 0.9445 - val_loss: 0.2016 - val_acc: 0.9401\n",
      "Epoch 176/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1682 - acc: 0.9437\n",
      "Epoch 00176: val_loss did not improve from 0.18903\n",
      "36805/36805 [==============================] - 31s 832us/sample - loss: 0.1682 - acc: 0.9437 - val_loss: 0.2010 - val_acc: 0.9436\n",
      "Epoch 177/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1654 - acc: 0.9471\n",
      "Epoch 00177: val_loss did not improve from 0.18903\n",
      "36805/36805 [==============================] - 31s 830us/sample - loss: 0.1654 - acc: 0.9471 - val_loss: 0.2023 - val_acc: 0.9432\n",
      "Epoch 178/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1688 - acc: 0.9442\n",
      "Epoch 00178: val_loss did not improve from 0.18903\n",
      "36805/36805 [==============================] - 31s 834us/sample - loss: 0.1688 - acc: 0.9442 - val_loss: 0.2085 - val_acc: 0.9427\n",
      "Epoch 179/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1611 - acc: 0.9478\n",
      "Epoch 00179: val_loss did not improve from 0.18903\n",
      "36805/36805 [==============================] - 31s 836us/sample - loss: 0.1610 - acc: 0.9478 - val_loss: 0.2082 - val_acc: 0.9408\n",
      "Epoch 180/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1667 - acc: 0.9448\n",
      "Epoch 00180: val_loss did not improve from 0.18903\n",
      "36805/36805 [==============================] - 31s 831us/sample - loss: 0.1667 - acc: 0.9448 - val_loss: 0.2038 - val_acc: 0.9446\n",
      "Epoch 181/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1613 - acc: 0.9458\n",
      "Epoch 00181: val_loss did not improve from 0.18903\n",
      "36805/36805 [==============================] - 31s 835us/sample - loss: 0.1613 - acc: 0.9457 - val_loss: 0.2042 - val_acc: 0.9408\n",
      "Epoch 182/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1628 - acc: 0.9465\n",
      "Epoch 00182: val_loss did not improve from 0.18903\n",
      "36805/36805 [==============================] - 31s 831us/sample - loss: 0.1629 - acc: 0.9465 - val_loss: 0.1979 - val_acc: 0.9420\n",
      "Epoch 183/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1589 - acc: 0.9483\n",
      "Epoch 00183: val_loss did not improve from 0.18903\n",
      "36805/36805 [==============================] - 31s 835us/sample - loss: 0.1589 - acc: 0.9483 - val_loss: 0.2084 - val_acc: 0.9450\n",
      "Epoch 184/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1605 - acc: 0.9468\n",
      "Epoch 00184: val_loss did not improve from 0.18903\n",
      "36805/36805 [==============================] - 31s 831us/sample - loss: 0.1605 - acc: 0.9469 - val_loss: 0.2034 - val_acc: 0.9418\n",
      "Epoch 185/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1581 - acc: 0.9473\n",
      "Epoch 00185: val_loss did not improve from 0.18903\n",
      "36805/36805 [==============================] - 31s 834us/sample - loss: 0.1581 - acc: 0.9473 - val_loss: 0.2035 - val_acc: 0.9418\n",
      "Epoch 186/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1558 - acc: 0.9483\n",
      "Epoch 00186: val_loss did not improve from 0.18903\n",
      "36805/36805 [==============================] - 31s 833us/sample - loss: 0.1558 - acc: 0.9483 - val_loss: 0.2282 - val_acc: 0.9380\n",
      "\n",
      "1D_CNN_custom_multi_3_GMP_DO_5_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzs3Xl4lOW5+PHvM3uSyb6QkIQEEFnCviuupaJii1qLaLUuPdrdau3xlNrT1u5a7c8erdZqj3Xf6u4pihuYUhEEZAdlJwnZl0kmM5n1+f3xJCFgEgJkCDD357rmmszMu9zvJHnv91lfpbVGCCGEALAMdABCCCGOH5IUhBBCdJKkIIQQopMkBSGEEJ0kKQghhOgkSUEIIUQnSQpCCCE6SVIQQgjRSZKCEEKITraBDuBwZWVl6eLi4oEOQwghTiirV6+u01pnH2q5Ey4pFBcXs2rVqoEOQwghTihKqT19WU6qj4QQQnSSpCCEEKJTzJKCUqpQKbVEKbVZKbVJKXVzN8uco5TyKKXWtj9+Hqt4hBBCHFos2xTCwI+01muUUsnAaqXUO1rrzQct9y+t9ZeOZkehUIjy8nLa2tqOZjNxzeVyUVBQgN1uH+hQhBADKGZJQWtdCVS2/9yilNoC5AMHJ4WjVl5eTnJyMsXFxSil+nvzJz2tNfX19ZSXlzN06NCBDkcIMYCOSZuCUqoYmASs6Obj05RS65RSbyqlSo5k+21tbWRmZkpCOEJKKTIzM6WkJYSIfZdUpZQbeAm4RWvdfNDHa4AirbVXKTUXeBUY0c02vgl8E2DIkCE97ac/w4478v0JISDGJQWllB2TEJ7WWr988Oda62attbf950WAXSmV1c1yD2utp2qtp2ZnH3LsRbciET+BQAXRaOiI1hdCiHgQy95HCvhfYIvW+v/1sExu+3Iopaa3x1Mfi3ii0TaCwUq07v+k0NTUxIMPPnhE686dO5empqY+L3/HHXdwzz33HNG+hBDiUGJZUpgFfB34Qpcup3OVUt9WSn27fZmvAhuVUuuA+4ArtNY6FsEo1XGo0X7fdm9JIRwO97ruokWLSEtL6/eYhBDiSMQsKWitl2mtldZ6vNZ6Yvtjkdb6Ia31Q+3L/FlrXaK1nqC1nqm1/jBW8XQcqtb9nxQWLlzIjh07mDhxIrfddhtLly7lzDPPZN68eYwZMwaASy65hClTplBSUsLDDz/cuW5xcTF1dXXs3r2b0aNHc+ONN1JSUsKcOXPw+/297nft2rXMnDmT8ePHc+mll9LY2AjAfffdx5gxYxg/fjxXXHEFAB988AETJ05k4sSJTJo0iZaWln7/HoQQJ74Tbu6jQ9m27Ra83rXdfBIhEvFhsSSg1OEdtts9kREj/tTj53feeScbN25k7Vqz36VLl7JmzRo2btzY2cXz0UcfJSMjA7/fz7Rp07jsssvIzMw8KPZtPPvsszzyyCNcfvnlvPTSS1x99dU97veaa67h/vvv5+yzz+bnP/85v/zlL/nTn/7EnXfeya5du3A6nZ1VU/fccw8PPPAAs2bNwuv14nK5Dus7EELEhzia5uLY9q6ZPn36AX3+77vvPiZMmMDMmTMpKytj27Ztn1tn6NChTJw4EYApU6awe/fuHrfv8Xhoamri7LPPBuDaa6+ltLQUgPHjx3PVVVfx1FNPYbOZBDhr1ixuvfVW7rvvPpqamjrfF0KIrk66M0NPV/SRSBs+30ZcrqHY7ZndLtOfkpKSOn9eunQp7777LsuXLycxMZFzzjmn2zEBTqez82er1XrI6qOe/POf/6S0tJQ33niD3/72t2zYsIGFCxdy0UUXsWjRImbNmsXixYsZNWrUEW1fCHHyipuSQkdDcyzaFJKTk3uto/d4PKSnp5OYmMjWrVv56KOPjnqfqamppKen869//QuAJ598krPPPptoNEpZWRnnnnsud911Fx6PB6/Xy44dOxg3bhw//vGPmTZtGlu3bj3qGIQQJ5+TrqTQs9j1PsrMzGTWrFmMHTuWCy+8kIsuuuiAzy+44AIeeughRo8ezciRI5k5c2a/7Pfxxx/n29/+Nj6fj2HDhvH3v/+dSCTC1VdfjcfjQWvND37wA9LS0vjZz37GkiVLsFgslJSUcOGFF/ZLDEKIk4uKUQ/QmJk6dao++CY7W7ZsYfTo0b2up3UUr3cNDkc+TmdeLEM8YfXlexRCnJiUUqu11lMPtVzcVB/tb2ju/5KCEEKcLOImKZiB05aYtCkIIcTJIm6SAnQ0NktSEEKInsRVUpCSghBC9C7ukoKUFIQQomdxlRSUkpKCEEL0Jq6SwvFUUnC73Yf1vhBCHAtxlRSkpCCEEL2Lq6Rgxir0/2C9hQsX8sADD3S+7rgRjtfrZfbs2UyePJlx48bx2muv9XmbWmtuu+02xo4dy7hx43j++ecBqKys5KyzzmLixImMHTuWf/3rX0QiEa677rrOZe+9995+P0YhRHw4+aa5uOUWWNvd1NngjPpBR8Ga1O3nPZo4Ef7U89TZCxYs4JZbbuF73/seAC+88AKLFy/G5XLxyiuvkJKSQl1dHTNnzmTevHl9uh/yyy+/zNq1a1m3bh11dXVMmzaNs846i2eeeYbzzz+fn/70p0QiEXw+H2vXrqWiooKNGzcCHNad3IQQoquTLyn0KjbTZ0+aNImamhr27dtHbW0t6enpFBYWEgqFuP322yktLcVisVBRUUF1dTW5ubmH3OayZcu48sorsVqtDBo0iLPPPpuPP/6YadOm8Y1vfINQKMQll1zCxIkTGTZsGDt37uSmm27ioosuYs6cOTE5TiHEye/kSwq9XNGH2vYQCjWSnDyx33c7f/58XnzxRaqqqliwYAEATz/9NLW1taxevRq73U5xcXG3U2YfjrPOOovS0lL++c9/ct1113HrrbdyzTXXsG7dOhYvXsxDDz3ECy+8wKOPPtofhyWEiDNx1qYQu95HCxYs4LnnnuPFF19k/vz5gJkyOycnB7vdzpIlS9izZ0+ft3fmmWfy/PPPE4lEqK2tpbS0lOnTp7Nnzx4GDRrEjTfeyA033MCaNWuoq6sjGo1y2WWX8Zvf/IY1a9bE5BiFECe/k6+k0IuOaS601n2q1z8cJSUltLS0kJ+fT16emYX1qquu4stf/jLjxo1j6tSph3VTm0svvZTly5czYcIElFL84Q9/IDc3l8cff5y7774bu92O2+3miSeeoKKiguuvv55o1CS83//+9/16bEKI+BE3U2cDBAKVBIMVuN2TO2+6I/aTqbOFOHnJ1NndiOXd14QQ4mQQV0khlndfE0KIk0FcJQUpKQghRO/iKilISUEIIXoXV0lBSgpCCNG7uEoKUlIQQojexVVS2N8NtX+TQlNTEw8++OARrTt37lyZq0gIcdyIq6TQcbj9XX3UW1IIh8O9rrto0SLS0tL6NR4hhDhScZUU9o9i7t+ksHDhQnbs2MHEiRO57bbbWLp0KWeeeSbz5s1jzJgxAFxyySVMmTKFkpISHn744c51i4uLqaurY/fu3YwePZobb7yRkpIS5syZg9/v/9y+3njjDWbMmMGkSZP44he/SHV1NQBer5frr7+ecePGMX78eF566SUA3nrrLSZPnsyECROYPXt2vx63EOLkc9JNc9HLzNmAg0hkJBaLk8OZ5eIQM2dz5513snHjRta273jp0qWsWbOGjRs3MnToUAAeffRRMjIy8Pv9TJs2jcsuu4zMzMwDtrNt2zaeffZZHnnkES6//HJeeuklrr766gOWOeOMM/joo49QSvG3v/2NP/zhD/zxj3/k17/+NampqWzYsAGAxsZGamtrufHGGyktLWXo0KE0NDT0/aCFEHHppEsKvYvN1NndmT59emdCALjvvvt45ZVXACgrK2Pbtm2fSwpDhw5l4kQzg+uUKVPYvXv357ZbXl7OggULqKysJBgMdu7j3Xff5bnnnutcLj09nTfeeIOzzjqrc5mMjIx+PUYhxMnnpEsKvV3Ra63xej/F4cjH6cyLaRxJSftv5LN06VLeffddli9fTmJiIuecc063U2g7nc7On61Wa7fVRzfddBO33nor8+bNY+nSpdxxxx0xiV8IEZ/iqk1hf0mhf9sUkpOTaWlp6fFzj8dDeno6iYmJbN26lY8++uiI9+XxeMjPzwfg8ccf73z/vPPOO+CWoI2NjcycOZPS0lJ27doFINVHQohDillSUEoVKqWWKKU2K6U2KaVu7mYZpZS6Tym1XSm1Xik1OVbxtO8PsPR776PMzExmzZrF2LFjue222z73+QUXXEA4HGb06NEsXLiQmTNnHvG+7rjjDubPn8+UKVPIysrqfP+///u/aWxsZOzYsUyYMIElS5aQnZ3Nww8/zFe+8hUmTJjQefMfIYToScymzlZK5QF5Wus1SqlkYDVwidZ6c5dl5gI3AXOBGcD/aK1n9Lbdo5k6G8DrXYvNlo7LVXRYxxMPZOpsIU5eAz51tta6Umu9pv3nFmALkH/QYhcDT2jjIyCtPZnEUP+XFIQQ4mRxTNoUlFLFwCRgxUEf5QNlXV6X8/nE0T+ammD9eiwhhUxzIYQQ3Yt5UlBKuYGXgFu01s1HuI1vKqVWKaVW1dbWHlkgWkMwiIoqKSkIIUQPYpoUlFJ2TEJ4Wmv9cjeLVACFXV4XtL93AK31w1rrqVrrqdnZ2UcWjNVqYopKSUEIIXoSy95HCvhfYIvW+v/1sNjrwDXtvZBmAh6tdWVMArKYQ1VaSgpCCNGTWA5emwV8HdiglOqYeOJ2YAiA1vohYBGm59F2wAdcH7No2pMCWkoKQgjRk5glBa31Mg4xr4Q2/WG/F6sYDtBRUogqtI4ck132xu124/V6BzoMIYQ4QPyMaD6g+mjgk4IQQhyP4jIpQIT+HLS3cOHCA6aYuOOOO7jnnnvwer3Mnj2byZMnM27cOF577bVDbqunKba7mwK7p+myhRDiSJ10E+Ld8tYtrK3qYe7slha0w0bUFsZqddPXWVMn5k7kTxf0PNPeggULuOWWW/je90xN2AsvvMDixYtxuVy88sorpKSkUFdXx8yZM5k3b16X+zp8XndTbEej0W6nwO5uumwhhDgaJ11SGAiTJk2ipqaGffv2UVtbS3p6OoWFhYRCIW6//XZKS0uxWCxUVFRQXV1Nbm5uj9vqbort2trabqfA7m66bCGEOBonXVLo7YqeTz4hku7Gl+khMXEMVmtiv+13/vz5vPjii1RVVXVOPPf0009TW1vL6tWrsdvtFBcXdztldoe+TrEthBCxEj9tCgAWCypq2hL6u7F5wYIFPPfcc7z44ovMnz8fMNNc5+TkYLfbWbJkCXv27Ol1Gz1Nsd3TFNjdTZcthBBHI+6SAp3ty/2bFEpKSmhpaSE/P5+8PDOn31VXXcWqVasYN24cTzzxBKNGjep1Gz1Nsd3TFNjdTZcthBBHI2ZTZ8fKUU2dvWkT2mHDm9uCyzUUuz3z0OvEEZk6W4iT14BPnX1cslggRtVHQghxMoivpGC1SlIQQohenDRJoU/VYBYLRKOAjGo+2IlWjSiEiI2TIim4XC7q6+sPfWKzWFDRKEpZ6e+G5hOZ1pr6+npcLtdAhyKEGGAnxTiFgoICysvLOeQNeOrrwe8nEFFYLM3Y7b5jE+AJwOVyUVBQMNBhCCEG2EmRFOx2e+do317dfDM89hir3huBw5HD6NGLYh+cEEKcQE6K6qM+S0oCnw+bLY1w2DPQ0QghxHEn/pJCOIxdJ0tSEEKIbpwU1Ud9lmjmOrIHEwlHmgY4GCGEOP7EV1JISgLAHkokoqWkIIQQB4u/6iPAHnQRiXhlrIIQQhwkvpJCe/WRLeAEIBxuHshohBDiuBNfSaG9pGALOgAIh6VdQQghuoqvpNBZUrADSA8kIYQ4SHwlhc6SghWASESSghBCdBWXScHaZg5bSgpCCHGg+EoK7dVHlvbbHkubghBCHCi+kkJnScG8lJKCEEIcKL6SQmdJwUyxHQ43DGQ0Qghx3ImvpOBwgNWKxd+GzZZGKFQ/0BEJIcRxJb6SglKmCqm1Fbs9i1CobqAjEkKI40p8JQUwVUiSFIQQolvxlxTa76kgSUEIIT4v/pKClBSEEKJH8ZcUpKQghBA9is+k0F5SiEb9RCK+gY5ICCGOGzFLCkqpR5VSNUqpjT18fo5SyqOUWtv++HmsYjlAUhJ4vdjtWQBSWhBCiC5iWVJ4DLjgEMv8S2s9sf3xqxjGsl9qKjQ3S1IQQohuxCwpaK1LgeNvyHBaGjQ1SVIQQohuDHSbwmlKqXVKqTeVUiU9LaSU+qZSapVSalVtbe3R7TEtDTwebJZ0QJKCEEJ0NZBJYQ1QpLWeANwPvNrTglrrh7XWU7XWU7Ozs49ur6mpoDX2NhcgSUEIIboasKSgtW7WWnvbf14E2JVSWTHfcVoaAHafApQkBSGE6GLAkoJSKlcppdp/nt4eS+xnqGtPCsrTgs2WIUlBCCG6sMVqw0qpZ4FzgCylVDnwC8AOoLV+CPgq8B2lVBjwA1dorXWs4unUnhRoasLulAFsQgjRVcySgtb6ykN8/mfgz7Haf4+6JoUCSQpCCNHVQPc+OvZSU81ze7dUSQpCCLFf/CWFjpKCxyNJQQghDhJ/SaGbksKxaMoQQogTQZ+SglLqZqVUijL+Vym1Rik1J9bBxYTdbuY/ak8KWoeIRFoGOiohhDgu9LWk8A2tdTMwB0gHvg7cGbOoYq19qguHIweAYLB6gAMSQojjQ1+Tgmp/ngs8qbXe1OW9E09qKjQ14XTmAxAIVAxwQEIIcXzoa1JYrZR6G5MUFiulkoFo7MKKsfb5jxwOkxSCQUkKQggBfR+n8B/ARGCn1tqnlMoAro9dWDGWlgbV1VJSEEKIg/S1pHAa8KnWukkpdTXw34AndmHFWHubgs2WjNWaLElBCCHa9TUp/AXwKaUmAD8CdgBPxCyqWGtPCgBOZ74kBSGEaNfXpBBun5foYuDPWusHgOTYhRVjHUlBaxyOfGlTEEKIdn1NCi1KqZ9guqL+UylloX1yuxNSaipEIuDzSUlBCCG66GtSWAAEMOMVqoAC4O6YRRVrXSbFM0lhH1pHBjYmIYQ4DvQpKbQngqeBVKXUl4A2rfWJ3aYAXcYqRAgGawY0JCGEOB70dZqLy4GVwHzgcmCFUuqrsQwsprokhY6xClKFJIQQfR+n8FNgmta6BkAplQ28C7wYq8Bi6nMlhY4BbFMHLiYhhDgO9LVNwdKRENrVH8a6x5+OmVI9HpzOAkBKCkIIAX0vKbyllFoMPNv+egGwKDYhHQMHVB/lAFZJCkIIQR+Tgtb6NqXUZcCs9rce1lq/EruwYiw93TzX1aGUFaczT5KCEEJwGPdo1lq/BLwUw1iOHYcDMjKgurr9ZT6BQPkAByWEEAOv16SglGoBurstmQK01jolJlEdC3l5UFkJQELCcDyeZQMckBBCDLxek4LW+sSdyuJQcnOhqgqAxMSR1NQ8QyTiw2pNHODAhBBi4Jy4PYiO1kFJAcDv3zaQEQkhxICL36TQUX2kNYmJowDw+T4d4KCEEGJgxW9SyM2FtjZobiYhYQQgSUEIIeI7KQBUVWG1JuJ0DpGkIISIe/GbFPLyzHN7D6TExJH4/ZIUhBDxLX6TQpeSApik4PN9irmXkBBCxKf4TQodJYX2pJCQMJJIpIVgsGoAgxJCiIEVv0khLc2MbO5SfQTS2CyEiG/xmxSUOmisQke31M0DGZUQQgyo+E0KYKqQ2pOC01mA3Z5FS8vqAQ5KCCEGTnwnhdzczuojpRTJydNoafl4gIMSQoiBE7OkoJR6VClVo5Ta2MPnSil1n1Jqu1JqvVJqcqxi6VGX6iOA5OSptLZuIhLxHfNQhBDieBDLksJjwAW9fH4hMKL98U3gLzGMpXt5eVBXB6EQAMnJ04AoXu8nxzwUIYQ4HsQsKWitS4GGXha5GHhCGx8BaUqpvFjF0628PNC6s7SQnGzu0dzSsuqYhiGEEMeLgWxTyAfKurwub3/v2CkqMs979gDgdObhcOTT3CztCkKI+HRCNDQrpb6plFqllFpVW1vbfxseOtQ879rV+VZKyjQpKQgh4tZAJoUKoLDL64L29z5Ha/2w1nqq1npqdnZ2/0VQVGTGK3RJCsnJ0/D7PyUUauy//QghxAliIJPC68A17b2QZgIerXXlMY3A6YTBgw9ICqmpZwDg8fz7mIYihBDHg15vx3k0lFLPAucAWUqpcuAXgB1Aa/0QsAiYC2wHfMD1sYqlV8XFB5UUpqOUA4+nlKysLw1ISCL+7GvZR7W3mnGDxmGz2IjqKJFoBKvFikUd+tpNa01Naw3ZSdl9Wj4cDdMcaMZlcxEIBwhEAuS6cw9YpiXQQk1rDQ6rg4KUAqpbq/m07lMKUgoYkjoEu9UOQDASRGuNJ+ChprWGSDRCkiOJoWlDaQ4080nVJ9gtdga5B3FKximd8WmtieooVouV1mAr/rCfrMQsAJrammgJtJBgT+h8ry+iOkqZp4z8lHxsFlvnfgKRAE6rE42m2luN3Won1ZnaeQxdlTeX47Q6yU7K7lxfKdX58/aG7TS1eci3j0VFXESjmlA0hNZgUw465tS0281MOh3Pfr9myxaF1ua0s3s3VFdD4dA2khKttDbb8XjA44GKhnqSXYnkZCQQDILPB34/jBsHM2b0+es4IjFLClrrKw/xuQa+F6v999nQoVBa2vnSanWRkjKDpqbSXlY6uXX9J+gQ1VHqffWEoiFSnClsb9jOZ/Wf0RJowWF1UJhayJlDzsRqsdLU1kQwEiQnKYfa1loqvZWUZJfgC/nYWreVUzNPJdmZzI6GHby/633WV69ndPZoItEIy8qWkeJIYXjGcNwONwm2BBLsCUSiETSaFGcKkWiExrZGGvwNNPrNs91qx+1wU+Wtoqa1Bn/YT5I9iZykHLITs3HanHjaPNitdjITMjm98HT8YT+Lti1iaNpQZhbMpDXUypvb3uQfm/+BzWIz6yZlE4wEqfPVkWBLwGlz0hpsJT0hnWFpw3DZXFgtVqzKitVixRfysbJiJU1tTQxJHcJez172ePYwKGkQBSkFDHIP6kwAOUk5KKWobKlkV5O5MElxppBkT6LKW4VGk2RP4rTC07AqK1XeKuxWO7nuXMZkjWHlvpV8WPYh2YnZ+EI+GtsaGZU1iktHXcqKihX4Q36Gpg9lr2cvez17sSgLVmUlqqOUN5cTioYO+B2PzBzJpLxJ1Pnq2FK7hYqW/bW5Kc4UmgPNna8tykKeO4/mQDMtwZZu/45c1gQCkTY0+2ceTrKmkOMYik0nURXeQku4ESs2IoQByLacilW7qNLrO9cZYpnBYOsE/KEAVZFNtKi9uKxJoBWBSBthAiisJIWK8Np3ErDWY4k6cYZyCVubCVk9oKJYok40UbRl/3FbIglYtHnfHsokqjXBxN0A2BpK0An1RBKqUFE7KuogqjXY28cxRWwQcYLND5Zo+3t2CKRAIBmCyeY5nADpOyFtN0RtEHFA2Am+bNAWyPwUonaoGwUqAsmVkFgPWsHqIrOPtlSomsj8HV/hhRnn9+G/+MipE22q6KlTp+pVq/qxIfjnP4ff/tbchc1urhp27vxvysruYtasRmw2d//tqx9VtlSi0WQlZuGwOoD9J3OtNRUtFVQ0V9ASbMFlc7G7aTcflX/EiooV7GrcRYI9gdlDZ/Pdad/lL6v+wge7PyDZmUy9r556fz0XnnIhIzJGsKJiBbubdrOvZd/nTiIHu3TUpXx32ne57IXLaAm0cGrmqWxr2EZUR3E73PhDfiI6gkLhtDlpC7cBkGRPojXUCkBxWjH+kJ/q1uo+fQ9Oq5P0hHRCkRAtwRZy3bnkunNJsCXQGmqlprWGam91ZzILR8N4g97O9RNsCfjD/s7XdoudL536JdwONzWtNdT6arFb7GQnZdMWbiMQDpBoT6TWV8uepj0EI0EiOkIkGiGiI9iUjcl5U8hxZ7PXs5f85HyGpw+nxldDeXM5VS1VZDoHk2rNpSViOk2k2LIosEzHGcxlW7AUZQ0zKDEPHXZS5a1ii/ffWJWNdHsewUiIulAZNdEtpEdOJT8wh5DFg105SVfD+NT2HNWWtWSExmOPpOO17cIdKSQpOAyfD1r9EQJtkBgqwhUZhC/YhiXqxJUQxZ//Jm2JO9HebFy+EaSHSmitGkxb1IsatJFwXTGhsgmkFVYSTNpJs9oDgTSULxsdtZgTYOsgc2JMaIScjeDPgPIZgILUvZC3GlLLwNEC9adCy2CwBcxJT1tg6BKwhGD3udA6CGtqFZFTXjXrROxYmkZiaRpGGH/779+FTTlRtgCR5N2o1sFYK2dgzdoJ7mqswTQsoTQIJ6AdHhQWnG2FYIkQtXuIOhrBav6uI44GlC3IMPvpWJyt7OXf2AKDsPkKCATD2FxBktxRclQJKfY0vMmfgLUNu0rAYUkEIKhbaNMttOlmfJEW/JEWAtFWUigiyzKczExNhCD1njYiCTVYHQEyw+MJhNuo0ZtJdLrIScpmZNapNLe1mP8fFcYTqmFL0yfcNPWH/Oa8n/fpf+NgSqnVWuuph1wu7pPC3/8O3/gGbN8Ow4cD0NDwNuvXn8/48e+QkfHF/tvXITQHmtnr2UtTWxMum4sJgyYQ0RF2NOwgqqPkJOWQkZDBTW/exF9X/xWA7MRsvjP1O3y872Pe2/UehSmFeIPebk+qboebaYOnMTJzJM3BZl7a/BKBSACH1cG8kfMIRoJkJGTgsrp4ZesrNPgbmJQ3iZGZIxmcPJjByYOxW+x4Ah6K04opyS4h1ZVKIBzgla2v8ON3fwyYK86vjfsaH5V/xLTB0zgl4xRWVKwg3ZXOhNwJbK7djKfNw5jsMZxeeDqjskZR3lyORjMkdQgA/pAfX8iHP+zHH/Jjs9jQaJoDzViVlTRnBllJGSTYEz53nFqb8YjhsHkOBjWNjVBRoUhLg8QMD+9+9i9qqqwkVs+mprWafZENWCNu0sOjcYSzCYfpfPh80NxsHtGoaYayWk0xf+9eaGmB5GQoL4eG9pE5yclmIt60NHC7IRg04yQrKsw2j5olTILLSmKCIhrtcrxhDfZWMpPduFwm3mjUXO8MHgz5+ZAaadQoAAAgAElEQVSTY2JvbTXxaW2OraX9gj8vD7xecyyDB0NKivkOUlIgMdEcg8UCw4aZdcNhs1xqqnnfat3/CATM9VZhIWRmQlOT2UZaGpSVmbhHjDDNe5GI2YfTaWJOSjLba2kx+0hJMdvUGurrTZVMSko/fJcnCK01wUgQp815ROtLUuirpUvh3HPh3Xdh9mwAwuEWli1Lo6jopwwd+qt+21UwEmRX4y7cDjdb67bywZ4PsCgLDf4G3vjsDXY37T5g+UR7IsFIkHB0/1kkKzGLOl8d35/2fUpySnjt09d4a/tb5CTlMH/MfOp8dThtTqYPnk5xWjEpzhTawm2myiF7DFaLtXNbe5r28MrWV7h45MUMTR96wL4j0QjhaPiw/gCfXv80r376Kg/OfbCzPhbMP7hS5sRTU2P+sVtbTVNOJGJOWI2N5h+9sdGcDFJTzTIdJ6vmZnOSKiszJ+LqanNycbv3n/w7HpHIYf5i2tlsn39YrWY/qanmBKQ17NtnnpOTYcgQ81lzs5k1ZfhwU/fb1LT/4fWaE116ulm+sBAyMsw6YI4hPx+yssy6ra3m2e02205NNd9hW5v5bpKTzbPV2vvxCNGVJIW+2rPHtPo88gjccEPn22vWnEY0GmDq1DWHtblQJMQzG57BF/KhlOKt7W9R6a0kyZ7Eqn2rDqh/tSgLUR3FaXUyZ/gcZhXOojitmPSEdJramviw7EOS7EmMzRmL3WpnV+MuVu5bySUjL+Gq8Vd1bqfMU0Z2UjYum+uovw4wJzylzAl6wwZzhdvaaq4WW1t7fvh85sS+Y4c5YeXkmCTQ3Gy2d6R/aklJ5oSclmZOqEOGmKtZn8+ccG02k1i6Pg5+LyUFCgrMFXJ1tblqHTIERo40n1ksJkYhTlZ9TQoxa2g+YeTnmzNYlx5IADk5V7J9+814vRtxu8f2uokqbxX3Lr8Xh9XB65+9zvrq/Y1khSmFjMoahSfg4cqxV3J64em0hdsYnDyYLwz9Ai6bi6iOdtsL4vKSy/t0CIWphT1+FomYq+cNG2D5cnPFarWaq+3du80ksU6nOYl6veZr2L3bnCB7u+K2283J+uDHkCFwzjlmmepqkxgyM83VfGoqDBpkkoPTaaof7HZTtZKebpZLSzMne4/HXCm73XJFLMSxJEnBZjNnss8lhSvYvv1WqqufxO2+64DPojra2a3OH/Iz79l5rK5cjdaa/JR8Xr78ZWYUzMAf8jMsfdjnevIczErfz3per7n63rcPNm0yVSuZmaZqparKnMgrKmDjRvPs8XS/HYvF5MP8fLNuKGRO6lOnwhVXmKSQnAwTJphqka4n/sTEzjb5mOhIBkKIY0+SAphL1u3bD3jL4cghM/NCqqufZtiw36GUlTc+fYP7V97P+7veZ0LuBKbkTWFT7SZW7VvFq1e8ytwRc7EoS5/6ifektdWc7MvKzBX05s2werWp5Sov318P3R2Xy+S47GzTn3n2bFNPbbOZnrdnnWWWC4VMMojliV0IcWKSpAAwahQ8+eT+yvR2WdlX89LW/2PHql/zr6p9PLLmEYpSi/jO1O/wSdUnvLr1VZIcSfx57p+ZN3LeYe2yuRneew9qa00VzoYNsH69yU1d696tVhg71tR9z55tTua5uaZaZswYU+1SV2caLtPS+usLEULEK0kKAKNHm7N0ZaXpW4fpffOf/36VZzYDm38JwE/O+Am/OvdXnSMl+yoSMdU5H34Ia9aYE//y5aa7Hpg8dMopMH48XH21ucofNsx0uSssPHRVSjx1yxNCxJYkBTAlBYCtW2HwYNrCbfzH6//BMxuf4+bxpzHatpLpk99jUv7Zfdpcba1JACtWmMfKlaYtAEz9/4gR8K1vwWWXmS6MGRmQ8Pnu9kIIccxJUoD9SWHLFqqmj+Hi5y5mZcVKfj/799w06WI+/ngMmZGVQM9JIRqFdevgL3+Bxx4z9fY2m7n6v+YaOP108ygulq6PQojjlyQFMFVGyckEtm7i4ueeYGPNRl6+/GUuHX0pAKmpZ1JZ+TCFhT9CHdSI/Nln8OCD8NRT+0dZ3ngjfO1rMHmylACEECcWSQpgLt1HjeL74ddYWbHvgIQAkJ//PTZvvoK6utfIzjbvL1kCd90FixebXjyXXgpf+hLMmWP64gshxIlIkkK7JZPS+Vvux/zkjJ8ckBAAsrIuw+Uazt69d1JZeQkLFyrefNMUMH71K1MyyM3tYcNCCHECkaSAmWjqF/mfkdcMP5/8w899brHYSEn5Kf/1X2EWLTIjc+++G77/fTM2QAghThaSFIAlu5fwL72b+/4Fru27YdqBt/wsLYUrrriOmpoIX/vaP/if/5lPZqa0FgshTj4DeTvO40JUR7n9vdvJTxjEjWswQ4i7WLYMLrwQUlIUb7zxCjfcsAB4c0BiFUKIWIv7pPD42sdZUbGC33zxd7gSU+Cjjzo/e+stmDvXDCD74AM4//xLcLmGs3PnQrQ+wvmZhRDiOBbXSaHR38h/vftfnF54OtdMus4MJFi2DICHHoKLLjIji997z/QosljsDBv2W1pbN1BV9diAxi6EELEQ10nh72v/Tp2vjj9f+Gczid0ZZ8DGjSx7s5nvfc9UGy1bZuYb6pCdfTmpqWewc+dCQqGGgQteCCFiIK6Twvu73mdExggm5U0yb5xxBk2kcvV1NoqL4dlnPz/vkFKKESMeIBRqZNeunx7zmIUQIpbiNimEo2FK95RybvG5+9+cNo3bLXdSXuvkmWfM/QS643aPp6DgJvbte4iGhrePTcBCCHEMxG1SWFO5hpZgC+cO3Z8UNu5M5K/RG/lO7ivMmNH7+kOH/pbExNFs3XotwWBdjKMVQohjI26TwpJdSwA4p/gcwNzD4Ec/ghRngDvqbto/rWkPrNZExox5llCogc2b5xONBmIdshBCxFz8JoXdSxiTPYZct5mf4t//hrffhp/dUEVmqApefvmQ23C7JzBq1KM0NS1ly5Zr0Doa67CFECKm4jIphKNhlu1ddkB7wr33mruYfevOoeYmB4891qdtDRp0FcOH30Nt7Qts3/5DdNfbpgkhxAkmLpPC1rqttIZamVkwE4Bdu+DVV82Nb5LcCq67zkyDunt3n7ZXWPgjCgpupaLiPsrK7o5d4EIIEWNxmRTWVa0DYGLuRADuvx8sFjPBHWDuiqMUPP54n7c5fPjd5ORcwc6dC6mv/2d/hyyEEMdEXCaFtVVrcVqdjMwcSTQKzzwDF1/cZZDakCFw7rnw9NOmBboPlLIwcuSjuN0T2bz5azQ3r4zdAQghRIzEZVJYV72OkpwS7FY7K1ZAdTV85SsHLXTllbBtG3zySZ+3a7UmMHbsK1itiaxZM4ONG79KKNTUv8ELIUQMxV1S0FqztmotEweZqqPXXjP3Up4796AFv/IV88Gzzx7W9l2uIqZN20JR0S+or3+d9evnSGIQQpww4i4pVHmrqPXVMiF3AmCSwtlnQ1raQQtmZMD558Pzz0P08Lqa2u1pDB16ByUlL+H1ruWTT87A693QT0cghBCxE3dJYW3VWsA0Mn/2GWzdatoTunXllVBWBhdcAI88ctj7ysr6MuPGLSIUqmP16mmUlf1JxjIIIY5rcZcU1lWbnkfjB41n8WLz3pe/3MPCl10G3/kO7NwJ3/wmbN9+2PvLyPgi06atJyNjDjt2/JB1686jqalUxjMIIY5LMU0KSqkLlFKfKqW2K6UWdvP5dUqpWqXU2vbHDbGMB0xSKEotIs2VxsqVMHgwFBf3sLDLBQ8+CEuXmi6qTz99RPt0OHIYO/Y1Tj31IbzetaxdezZr155Na+vWIz0MIYSIiZglBaWUFXgAuBAYA1yplBrTzaLPa60ntj/+Fqt4Ouxs3MmIzBEArFwJ06b1YaWCAtNF9amn+txF9WBKKQYP/hannVbGiBF/prV1I6tWTWDnztsJh5uPaJtCCNHfYllSmA5s11rv1FoHgeeAnmrvj5k9TXsoSi2iqQk++wymT+/jildfbaqPVh7d+AOrNZH8/O8xffoWcnIuZ+/e37NixQgqKh4iGg0f1baFEOJoxTIp5ANlXV6Xt793sMuUUuuVUi8qpQpjGA9t4TaqW6spSi1i1SrzXp9KCmC6qLpc8Oij/RKLwzGI0aOfZPLkj0lMHMm2bd9h1aoJ1Ne/Ke0NQogBM9ANzW8AxVrr8cA7QLfzSiilvqmUWqWUWlVbW3vEOyvzmBw1JHUIH39s3ps6tY8rp6aa6S8ee8z0SHrvPfjNb6Ct7YjjAUhJmcrEiR9QUvIyWgfZsGEu69efT0vLakkOQohjLpZJoQLoeuVf0P5eJ611vda640YEfwOmdLchrfXDWuupWuup2dnZRxzQHs8eAIrSili5EkaMMDOj9tntt5sxCzfcYLos/exnMHNmnyfO64lSiuzsS5k2bROnnPInWlpWsXr1VFasOIXdu39DOOw5qu0LIURfxTIpfAyMUEoNVUo5gCuA17suoJTK6/JyHrAlhvGw17MXgKLUIj7++DDaEzoUFcE3vmFuvDBokJkwb8cO+Gn/3KvZYnFQUHAzM2bs4NRT/0pCwnB27/4Zy5cXsX37rXg8/6a1dTPRaKhf9ieEEAezxWrDWuuwUur7wGLACjyqtd6klPoVsEpr/TrwA6XUPCAMNADXxSoeMI3MCoXNn09FBUzptlxyCD//OTQ3m1LCmDHw/vvw+usQDptpMfqB3Z7O4MHfZPDgb9LSsoa9e/9ARcX9lJff2/55DoMGfZ28vOtJSirpl30KIQSAOtHqradOnapXdbQSH6brX7ued3a8w2PjyznvPHj3XZg9+ygDevllM8ht6VIzX0aMBAJVeL1rCIXqqat7hfr6N9A6TFLSOFJTzyAv70aSkyfFbP9CiBObUmq11vqQragxKykcj/Y07aEorYgN7dMQjRvXDxudMwccDnjjjZgmBaczF6fTzNqXm/t1gsEaqqufoqHhLaqrn2TfvofIzPwyStlITBxJYeFt2O2H02AihBAD3/vomNrj2cOQ1CFs3Ag5OeZx1NxuM7Dt9dehvh5Cx6a+3+HIobDwViZMeJuZM8soLPwRXu9afL7N7N17JytWDGPTpgWUl/8PgUDVMYlJCHHii5ukENVRyjxlFKWakkK/lBI6fPnL5t4LWVkwYQJ4vf248UOz29MYPvxuTjttD9Onb2Hq1LVkZFxIc/MKtm+/heXLC9iw4cvU1LxIW9temZRPCNGjuKk+qvJWEYqGKEwpYtMmuPHGftz4tdeaxudAAH71K/jJT+C++6ClBVJS+nFHfeN2j2fMmGcA8Pk+parqMaqqnqC+/v8AsFhcuFzDSU09g5yc+aSknI7VmnDM4xRCHH/iJinsaTJjFJz+Ifh8/VxScLtNIgBobDQJ4eWXobYW3nknpm0Nh5KYOJJhw35PcfGvaW7+EJ9vCz7fNvz+T6mufpLKyr8CVpzOPCIRH2lpZzFkyEIcjsHY7ZlYrYkDFrsQ4tiLm6TQMUahdV8R0M9Joavf/Q62bDElhHXrYMECcyefnTtN20Nubox23DuLxUZa2lmkpZ3V+V4k4qOx8V2am1cQCFSglI3a2hepq3u1fZ0kcnOvJSVlBlZrCjZbComJo3A6Bw/IMQghYi9uuqQ2+htZX72e956Ywa9/4cLrhaSkGATY1caNMGMG+HzmdXY2/P3vcNFFMd7xkQuFmqivf4NoNIDHs4yammcx8xl2sJCZORenswiLxYHbPYHk5KkkJo6io4lKKTUgsQshetbXLqlxkxQ6XHEFfPyxGYh8TCxZAps3w6hR8MMfwoYNcPrpZoK9rCwzgm7MGLAcn23+4XALoVAN4XAz4bCHxsZ3qK5+mkjESzTqIxr1A6CUE4hisSSSl/cNUlJOByApqYTExJEodXwenxDxQpJCD2bMMHPbvf12PwbVV21t8PDDcO+9B86XNGyYGUk3dOgABHXktI7g831GS8sqWls3oJQNv38ndXUvofX+acAtlgSczgJstnRstnRycq4gJ+cKrFbXAEYvRHyRpNCDwYPhwgvhf/+3H4M6XFqDxwPV1fDhh/CjH0FmJpSWQl7eodc/zgWDtQSDVWgdxutdS2vrRgKBMsLhZtraduH3f4ZSdhISTiEa9RMON5OVdQlZWRfjdBYQifgIh5uwWJwkJAwnIWHYQB+SECc8GdHcjWAQqqqgMKZ3begDpSAtzTxGjjRVS1/8IgwfDpdeCuecA6ecYpYbMsSUIJSCpib44x/h4osPY87vY8/hyMbhMLPZHjz1htaapqYlNDS8jd//KRZLEkopamqep6qq+3tVJCSMJD39XJKSxgEKuz2LpKRxJCaeKtVSQvSzuEoKFRXmIn3IkIGO5CCnnQYrVsADD8ALL8Azzxz4eUaGSQIbNkBlJTz7rPk54cQbW6CUIj39C6Snf+GA90eM8OLzbSIQ2IfVmoTNlkY0GsDrXUN9/ZtUVz9DJHLgbUtttnSSk6fhchWhlAOtgzgceVgsTtradmOzpeN2TyI9/VwcjkHH8jCFOGHFVfVRaakZMvD223Deef0cWH/RGnbtgj17zM/bt5uW8Y8/huRkmD8fbr4ZfvELuOMOs+zNN5tJ+a69dqCjjxmtIwSD1YAiGKzE612Hx/NvvN61BAJlaB1BKRuhUA2gsduzCYeb0NpMO5KUNBa3exLRaJBIxIPbPaW9pGFHKSs2WxqpqWfKID5x0pI2hW48/bS51fKWLabG5oR15ZXw0kuma2tpqRkwpzV89avm4Gw2kyC++lUoKDDveTymt1NHtdRJKhoNoXUQqzWJaDSE17uOxsbFeDwf4vWuw2pNwGJJpLV1ExA5YF2LJYmUlBm4XEW4XEXY7dloHcZuz8HtHkdCwqkoZcXn+wyItA/wk0kHxYlB2hS6sdeMXxv4NoWjdf/9ZnDcW2+ZdognnjCjqP/2N5g1y4yLuPVW80hJMVNwdLj+enjoITOz66uvmmk57rkHvvCFnvfXV4sWmXEYTz0FTufRb+8IWCx2wN75c0rKVFJSPv9/EA57CQargAhaRwgEyqirexWvdy0NDW8RDFZ+bh2lHFgsCUQi+++E53ZPwu2eSDjswWZLJyFhOMnJk3E48giF6gmF6ohGfdjtg0hKGoPLdbzVXQpxoLgqKXz3u/D882Yy05NSNLp/vMPWrfB//2cm6ps50/Rqev99uPtuKCmB4mL45z9NqcLpNN2xwmHz/rhxZt26OpMs8vLA5ep9tF9bG5x6qrl/9b33wi23HIsjjploNEAo1IjFYicQqMDrXU9r63oikRaSk2dgtSbg9++gvn4RbW27sNnS2pNAda/bTUgYgds9EadzCEpZcDjySE6eistVhM2WhtYRQqEagsFakpOnSHWW6DdSfdSNL30Jysth7dp+DupE8txzpkG7shIuuAD+8z9NA8v27b2vZ7WaL/Css0w3rmHDTJFr2zbTz3fDBlMyGT7c9JJauBDuugu+8x2YNw9++UszUO9nPzPbOkmFwx5aWlYRCjVit2dht2dhtSYQDFbR3PwxTU3v4/NtIRCoADTRaFuP27Jak0lPn43DkU802kooVE9S0jgSEoajdRiLJRG7PR2bLQOl7ESjPiIRH0pZSU6ejMWSQCBQjsMxGJvNfey+BBEb+/aZC7QjrP6VpNCNCRNMz6M33ujnoE50DQ2wahXk55vR1+vWmZsHFRWZO8o1N5u6tyeegJqanrcze7YpiUyebF6PHAmffmp+Tkw01Vqnnw5+v8nOGRkmycyfb0osVqv5ow8GTamnpMQ8f/yx6Uvs8ZhYJk2CM888umNua4NIxJR+NmyAxYvh2982kxseQ+aOep8QCFQQDjehlBW7PRur1U19/es0Ny8nGKzCanVjtabi9396wMDAz4mAsxYCB02x5XQWYLfnkJJyGnl51xOJtBIK1WKzpWGzZWC1ugmF6tE6jNM5mHDYQzjc2D6D7gAPMtTalGLt9oGNYyBVVZmLqquugj/84Yg2IUmhGxkZpo32gQf6Oah4EQ6bk7LDYUoI5eUwYgSsXg0vvgi//jWMHWvaPFwuuOEGk4FXrTJTfLzwgmnDOPVU86ipMV3BOuaGOlhurjlx19Z+/rM5c8zYjsJCcyL3+011l1IwaJAZ65GQYOJ85x2T6KqrTQ+D3FzTjuL3w9y55gZJwaAp5dx1l0luaWm9fxetrSaBtrSY7e7YYa465s7tvSS0Z48pWfX1BOf3w29+Y6r1rr2WSGsD4VUfYNmyg/A50wgVpxIKNaB1CKslkaTv3oP9hcXUPXAlrReOIqEtE7+jDn9gJ8FgFU1NS9E60Ld9AzZbGklJ4wgGa3A4cnA4cvH7t2OxuEhNPQMAraOkp88mJWUmNlta/859VVlpSqhr1pi7Yv3ud/Af/2GqKR0O87vuq127TIIZ1s1gyGDQ/E66xu7xmDa5ru+FQof+3VVVwbJl5m989OjPL+/1mg4ibW2Qnm66mycnH7jM+vVmv2PHmv+BL34RVq6Ejz6C8eP7fsxdSFI4iNdrvvff/97UbIjjREsLLF9u2jU6Rhc6neYf5s03TRvJpZea5JOSYkocTz1lTupVvdxRTilzAuhQUGBOKps2mftezJ5t3nv+eXMiv/Za07W3Y/oRpUxSKSoyCa6tzcRlsZiTxe7d5p/1YPn5prHf7TYz406bZm7CtHkzPPaY+acuLITzzzdtPPX1ZjT7+eeb9puqKtOBoLTUvN6zZ399Z2qq2XeHxEQzZXtDw/62pD/+0Rynx2NKYe+8Y/Z38cVQUkK4tozgplIiF5+HOueLqLffJRxpIeKM4n5mOdatewkXpBItyid6SgFVE2sJN5WR9c9m/AWa2llREnUBCR9XkvTBbqJORTAdiEbxFUPNeQ4iKRasATvZmzJpywzTOKQalzeNnE/SyNyYiLO8DeUL0TpjEIGx2SSEBxMdlkvIGSHt/vextkRQX/8GRCOoO++C6lrUD34A//63+V5OO818j0qZEmNamjlZd7SJzZhhSoBbtsArr5jfk9W6/0R72WX7J6pctsyUFKuqzDoTJpiLmtJSUx06Zw5861tm5oG33jJ/P+edZ9rpli0z+01LM9uKREyJ94MP9t+B0ek0bXRZWSaGhgb45BPz99TBYjF/L0qZPvPRqGnTA7Ntm81c8Dz5pOk+eYQkKRxkyxYz79xTT5kSmDgJNDWZetaWFnOCzMoy7+/caSYi7BgRfs45+7uctbaaEs6pp+5PHB1XgsGgOdl8+KH5J/d6zck/GDSJIRg0//jJyWb9yZNN8TMjwySPxYtNklm1yqxfVGSucDuSx/DhpvfX4sWmSmz2bHPVWl5uEmDHiaIj5sWLzT6ffNKcOF54wWxjyhSzzC23mDmzEhLMiSQQMAn04YdNQqirM/vbvBnee8+UOmB/jzSLxazXITPTnJTKysxVdV3d/s86EmMXesQIs43aWrQOYWlsQdstRFJdWFoCWALmuIMjMrHvakCFNSE3+IpAWyF1I6iDbgIYToJQKiTsM68DGbDpt1ZaSxLQoQinPmAn671Wqr7sBEcCmR9pLFE7yu4AqwXbzhoszeY4tYK2aYXo9DTsfiuR887C2hzG9tcnUc0tZgfjxpkr9aIi07X7H/8wCSIaNQln3br9peMzzjDL/+MfpgQzcaJJ1I2NJqFYrebv64wzzJT5u3eb3//69eZvNRIxfyslJaadLSPDlDKXLzef+/3m78DjgR//2FSlrlxp1ps+3ZS8j4IkhYO8/ba5GCstPfrqaCH6bN8+M1p93DhzQu9IQF2TEZgTwfbt5kTQ0ZgYbm87sPXQczwaNSeewkKz7OrV5gTncpkEoZQ5mXUsW1FhrkjdbjNoZ8sWM9YlOdmcCDuq3DpUVpqqNYCvfc2UWkpLTfIYP95UjXT1yScmcTU0mH2cf745Ib72mmlLmj+fyPjRhKKNRKNtuHxuKCvHb6nEunUP1op6fJdMxe+qJbp6BTrNTaQwm3C0kWjUTN8eDFYSDjXidBUSDFbR0rKqvWtx+3ksAglVoEImuYQz7J0DGDuoCFgCCqs9DUtyBkrZiEZbsViSSGzLofgvrUSy3VTcmI292YJ9Wy21xbuxpxWQk3MlLlsBjnAaSXmnYbHY0VoTjfpRyorFcpRdsbU2jxjMmixJ4SCLFpkLq3feMRcFQoiTQzQaJhJpbr/3uAY0FksiVmsCSlkJBmvw+T4jHG4gFGogHG5s/9k8ax3Bak0kEmnF79+J17sGgISE4USjIUCTnDy1/a6FWzr3a7G4sFgSCIeb6RgI6XDkk5AwFJstndbWjQSDNVgsDpSyY7UmtW8zgN+/g7S0s0lNPYOmpg+wWBJISzunfTsKt3syFouLSMSDw5GLw5GPxXJ0w8okKQghxBEIhZpQyoLNduD91bXWtLXtJBSqp61tF83NK9tHz6dgtSajdQC/fxdtbbsIhepISirB6cxH6zDRaIhIpBm/fztK2XE6C2loeKv9pJ9PNNpGONzbACorTmc+BQU3U1h46xEdl4xoFkKII2C3d9/zTCnVPpX7cFJSppOTs+Co9hOJ+AgEKkhIOAWI4vNtw2pNIBoN0NKyBtDYbCkEApW0te0mENiDwxH72/lKUhBCiAFgtSaSmDii4xVJSfsnZEtMPHVggqLjprpCCCEEkhSEEEJ0IUlBCCFEJ0kKQgghOklSEEII0UmSghBCiE6SFIQQQnSSpCCEEKLTCTfNhVKqFthzhKtnAXWHXGrgnQhxSoz9Q2LsHxLjoRVprbMPtdAJlxSOhlJqVV/m/hhoJ0KcEmP/kBj7h8TYf6T6SAghRCdJCkIIITrFW1J4eKAD6KMTIU6JsX9IjP1DYuwncdWmIIQQonfxVlIQQgjRi7hJCkqpC5RSnyqltiulFg50PABKqUKl1BKl1Gal1Cal1M3t79+hlKpQSq1tf8wd4Dh3K6U2tMeyqv29DKXUO0qpbe3P6Xnpo6YAAAXESURBVAMY38gu39VapVSzUuqW4+F7VEo9qpSqUUpt7PJet9+dMu5r/xtdr5SaPIAx3q2U2toexytKqf/f3r2GSFWHcRz//tKSSku6iVi5ar2ooMwiolKColIqu2cXswtEUC8iogy70TuD6pWkRNFadqEbSRBIvjB64aU2tyzzmpCxrmDR/aY+vTj/OZ2ddXSxdv+H9veBYc88c3Z45jkz88z5z5z/GZnibZJ+q9R0fsYcW25fSQ+lOq6TdHHGHF+v5LdF0uoUz1LHPomI//0FGAJsAsYDBwGdwMk1yGs0MCktjwDWAycDjwP3586vkucW4Kim2JPA7LQ8G5ibO8/Ktt4GjK1DHYEpwCRgzb5qB0wD3gcEnA2syJjjRcDQtDy3kmNbdb3Mddzj9k2voU5gGDAuvfaH5Mix6fangEdz1rEvl8Gyp3AWsDEiNkfEn8BrwPTMORERXRHRkZZ/AtYCY/Jm1WfTgfa03A5ckTGXqguATRGxvwc4/qci4kPgu6Zwq9pNBxZGYTkwUtLoHDlGxJKI2JmuLgeO7e889qZFHVuZDrwWEX9ExNfARor3gH61txwlCbgOeLW/8/i3BktTGAN8U7m+lZq9+UpqA04HVqTQPWnX/YWcQzNJAEskfSLpzhQbFRFdaXkbMCpPar3MoOcLr051bGhVu7o+T2+n2INpGCfpU0nLJE3OlVSyp+1bxzpOBrojYkMlVqc6lgZLU6g1ScOBt4B7I+JH4FlgAjAR6KLY7czpvIiYBEwF7pY0pXpjFPvD2X/GJukg4HLgjRSqWx17qUvtWpE0B9gJLEqhLuD4iDgduA94RdJhmdKr/fatuIGeH1bqVMceBktT+BY4rnL92BTLTtKBFA1hUUS8DRAR3RGxKyJ2A88xALu+exMR36a/24F3Uj7djaGN9Hd7vgxLU4GOiOiG+tWxolXtavU8lXQrcClwU2pepCGZHWn5E4rx+ixnmd/L9q1bHYcCVwGvN2J1qmOzwdIUVgEnShqXPk3OABZnzqkxzvg8sDYinq7Eq+PIVwJrmv93oEg6VNKIxjLFF5BrKOo3K602C3g3T4Y99Pg0Vqc6NmlVu8XALelXSGcDP1SGmQaUpEuAB4DLI+LXSvxoSUPS8njgRGBzphxbbd/FwAxJwySNo8hx5UDnV3Eh8FVEbG0E6lTHXnJ/0z1QF4pfdqyn6MhzcueTcjqPYujgM2B1ukwDXgI+T/HFwOiMOY6n+CVHJ/BFo3bAkcBSYAPwAXBE5loeCuwADq/EsteRokl1AX9RjG3f0ap2FL86mpeeo58DZ2bMcSPFuHzjeTk/rXt1eh6sBjqAyzLm2HL7AnNSHdcBU3PlmOIvAnc1rZuljn25+IhmMzMrDZbhIzMz6wM3BTMzK7kpmJlZyU3BzMxKbgpmZlZyUzAbQJLOl/Re7jzMWnFTMDOzkpuC2R5IulnSyjTX/QJJQyT9LOkZFee+WCrp6LTuREnLK+ceaJwf4QRJH0jqlNQhaUK6++GS3kznK1iUjmw3qwU3BbMmkk4CrgfOjYiJwC7gJoqjpj+OiFOAZcBj6V8WAg9GxKkUR9g24ouAeRFxGnAOxdGuUMyGey/FvP/jgXP7/UGZ9dHQ3AmY1dAFwBnAqvQh/mCKSet288+kZi8Db0s6HBgZEctSvB14I80XNSYi3gGIiN8B0v2tjDQPTjoTVxvwUf8/LLN9c1Mw601Ae0Q81CMoPdK03v7OEfNHZXkXfh1ajXj4yKy3pcA1ko6B8pzKYyleL9ekdW4EPoqIH4DvKydJmQksi+JMelslXZHuY5ikQwb0UZjtB39CMWsSEV9KepjibHMHUMx6eTfwC3BWum07xfcOUEx/PT+96W8GbkvxmcACSU+k+7h2AB+G2X7xLKlmfSTp54gYnjsPs/7k4SMzMyt5T8HMzEreUzAzs5KbgpmZldwUzMys5KZgZmYlNwUzMyu5KZiZWelvmol5mlMf4RAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 436us/sample - loss: 0.2305 - acc: 0.9294\n",
      "Loss: 0.2305163688377428 Accuracy: 0.92938733\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.2657 - acc: 0.2533\n",
      "Epoch 00001: val_loss improved from inf to 1.51939, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_6_conv_checkpoint/001-1.5194.hdf5\n",
      "36805/36805 [==============================] - 34s 926us/sample - loss: 2.2657 - acc: 0.2533 - val_loss: 1.5194 - val_acc: 0.5549\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5222 - acc: 0.5046\n",
      "Epoch 00002: val_loss improved from 1.51939 to 1.00714, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_6_conv_checkpoint/002-1.0071.hdf5\n",
      "36805/36805 [==============================] - 31s 853us/sample - loss: 1.5223 - acc: 0.5046 - val_loss: 1.0071 - val_acc: 0.7195\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1593 - acc: 0.6274\n",
      "Epoch 00003: val_loss improved from 1.00714 to 0.74656, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_6_conv_checkpoint/003-0.7466.hdf5\n",
      "36805/36805 [==============================] - 31s 849us/sample - loss: 1.1592 - acc: 0.6274 - val_loss: 0.7466 - val_acc: 0.7952\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9349 - acc: 0.7028\n",
      "Epoch 00004: val_loss improved from 0.74656 to 0.57525, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_6_conv_checkpoint/004-0.5752.hdf5\n",
      "36805/36805 [==============================] - 31s 853us/sample - loss: 0.9350 - acc: 0.7028 - val_loss: 0.5752 - val_acc: 0.8470\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8006 - acc: 0.7474\n",
      "Epoch 00005: val_loss improved from 0.57525 to 0.49557, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_6_conv_checkpoint/005-0.4956.hdf5\n",
      "36805/36805 [==============================] - 31s 842us/sample - loss: 0.8005 - acc: 0.7474 - val_loss: 0.4956 - val_acc: 0.8586\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7046 - acc: 0.7797\n",
      "Epoch 00006: val_loss improved from 0.49557 to 0.42465, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_6_conv_checkpoint/006-0.4247.hdf5\n",
      "36805/36805 [==============================] - 31s 846us/sample - loss: 0.7046 - acc: 0.7797 - val_loss: 0.4247 - val_acc: 0.8782\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6370 - acc: 0.8004\n",
      "Epoch 00007: val_loss improved from 0.42465 to 0.39900, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_6_conv_checkpoint/007-0.3990.hdf5\n",
      "36805/36805 [==============================] - 31s 848us/sample - loss: 0.6371 - acc: 0.8004 - val_loss: 0.3990 - val_acc: 0.8847\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5868 - acc: 0.8165\n",
      "Epoch 00008: val_loss improved from 0.39900 to 0.34559, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_6_conv_checkpoint/008-0.3456.hdf5\n",
      "36805/36805 [==============================] - 31s 849us/sample - loss: 0.5868 - acc: 0.8165 - val_loss: 0.3456 - val_acc: 0.8961\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5354 - acc: 0.8343\n",
      "Epoch 00009: val_loss improved from 0.34559 to 0.32877, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_6_conv_checkpoint/009-0.3288.hdf5\n",
      "36805/36805 [==============================] - 31s 845us/sample - loss: 0.5354 - acc: 0.8344 - val_loss: 0.3288 - val_acc: 0.9059\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5067 - acc: 0.8424\n",
      "Epoch 00010: val_loss improved from 0.32877 to 0.29925, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_6_conv_checkpoint/010-0.2993.hdf5\n",
      "36805/36805 [==============================] - 31s 846us/sample - loss: 0.5066 - acc: 0.8425 - val_loss: 0.2993 - val_acc: 0.9124\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4742 - acc: 0.8529\n",
      "Epoch 00011: val_loss did not improve from 0.29925\n",
      "36805/36805 [==============================] - 31s 840us/sample - loss: 0.4742 - acc: 0.8528 - val_loss: 0.2998 - val_acc: 0.9152\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4479 - acc: 0.8605\n",
      "Epoch 00012: val_loss improved from 0.29925 to 0.26441, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_6_conv_checkpoint/012-0.2644.hdf5\n",
      "36805/36805 [==============================] - 31s 849us/sample - loss: 0.4478 - acc: 0.8605 - val_loss: 0.2644 - val_acc: 0.9262\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4304 - acc: 0.8656\n",
      "Epoch 00013: val_loss did not improve from 0.26441\n",
      "36805/36805 [==============================] - 31s 846us/sample - loss: 0.4303 - acc: 0.8656 - val_loss: 0.2721 - val_acc: 0.9192\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4090 - acc: 0.8731\n",
      "Epoch 00014: val_loss improved from 0.26441 to 0.23725, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_6_conv_checkpoint/014-0.2372.hdf5\n",
      "36805/36805 [==============================] - 31s 844us/sample - loss: 0.4089 - acc: 0.8731 - val_loss: 0.2372 - val_acc: 0.9299\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3903 - acc: 0.8807\n",
      "Epoch 00015: val_loss improved from 0.23725 to 0.23008, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_6_conv_checkpoint/015-0.2301.hdf5\n",
      "36805/36805 [==============================] - 31s 849us/sample - loss: 0.3903 - acc: 0.8807 - val_loss: 0.2301 - val_acc: 0.9399\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3769 - acc: 0.8826\n",
      "Epoch 00016: val_loss improved from 0.23008 to 0.21468, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_6_conv_checkpoint/016-0.2147.hdf5\n",
      "36805/36805 [==============================] - 31s 848us/sample - loss: 0.3770 - acc: 0.8826 - val_loss: 0.2147 - val_acc: 0.9378\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3614 - acc: 0.8871\n",
      "Epoch 00017: val_loss improved from 0.21468 to 0.20766, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_6_conv_checkpoint/017-0.2077.hdf5\n",
      "36805/36805 [==============================] - 31s 847us/sample - loss: 0.3614 - acc: 0.8871 - val_loss: 0.2077 - val_acc: 0.9385\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3491 - acc: 0.8924\n",
      "Epoch 00018: val_loss improved from 0.20766 to 0.20279, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_6_conv_checkpoint/018-0.2028.hdf5\n",
      "36805/36805 [==============================] - 31s 843us/sample - loss: 0.3490 - acc: 0.8924 - val_loss: 0.2028 - val_acc: 0.9406\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3389 - acc: 0.8943\n",
      "Epoch 00019: val_loss improved from 0.20279 to 0.19803, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_6_conv_checkpoint/019-0.1980.hdf5\n",
      "36805/36805 [==============================] - 31s 847us/sample - loss: 0.3389 - acc: 0.8943 - val_loss: 0.1980 - val_acc: 0.9429\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3288 - acc: 0.8973\n",
      "Epoch 00020: val_loss did not improve from 0.19803\n",
      "36805/36805 [==============================] - 31s 845us/sample - loss: 0.3287 - acc: 0.8974 - val_loss: 0.2038 - val_acc: 0.9441\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3134 - acc: 0.9027\n",
      "Epoch 00021: val_loss improved from 0.19803 to 0.18291, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_6_conv_checkpoint/021-0.1829.hdf5\n",
      "36805/36805 [==============================] - 31s 848us/sample - loss: 0.3133 - acc: 0.9027 - val_loss: 0.1829 - val_acc: 0.9471\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3035 - acc: 0.9054\n",
      "Epoch 00022: val_loss did not improve from 0.18291\n",
      "36805/36805 [==============================] - 31s 841us/sample - loss: 0.3035 - acc: 0.9054 - val_loss: 0.1923 - val_acc: 0.9455\n",
      "Epoch 23/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3011 - acc: 0.9057\n",
      "Epoch 00023: val_loss improved from 0.18291 to 0.18086, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_6_conv_checkpoint/023-0.1809.hdf5\n",
      "36805/36805 [==============================] - 31s 848us/sample - loss: 0.3010 - acc: 0.9057 - val_loss: 0.1809 - val_acc: 0.9495\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2929 - acc: 0.9087\n",
      "Epoch 00024: val_loss did not improve from 0.18086\n",
      "36805/36805 [==============================] - 31s 842us/sample - loss: 0.2930 - acc: 0.9087 - val_loss: 0.2079 - val_acc: 0.9348\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2862 - acc: 0.9092\n",
      "Epoch 00025: val_loss did not improve from 0.18086\n",
      "36805/36805 [==============================] - 31s 846us/sample - loss: 0.2862 - acc: 0.9092 - val_loss: 0.1952 - val_acc: 0.9420\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2800 - acc: 0.9122\n",
      "Epoch 00026: val_loss did not improve from 0.18086\n",
      "36805/36805 [==============================] - 31s 847us/sample - loss: 0.2800 - acc: 0.9122 - val_loss: 0.1874 - val_acc: 0.9450\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2721 - acc: 0.9140\n",
      "Epoch 00027: val_loss did not improve from 0.18086\n",
      "36805/36805 [==============================] - 31s 846us/sample - loss: 0.2722 - acc: 0.9140 - val_loss: 0.2261 - val_acc: 0.9252\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2706 - acc: 0.9159\n",
      "Epoch 00028: val_loss did not improve from 0.18086\n",
      "36805/36805 [==============================] - 31s 848us/sample - loss: 0.2706 - acc: 0.9159 - val_loss: 0.1821 - val_acc: 0.9446\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2595 - acc: 0.9194\n",
      "Epoch 00029: val_loss improved from 0.18086 to 0.16203, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_6_conv_checkpoint/029-0.1620.hdf5\n",
      "36805/36805 [==============================] - 31s 846us/sample - loss: 0.2595 - acc: 0.9194 - val_loss: 0.1620 - val_acc: 0.9534\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2501 - acc: 0.9216\n",
      "Epoch 00030: val_loss improved from 0.16203 to 0.15969, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_6_conv_checkpoint/030-0.1597.hdf5\n",
      "36805/36805 [==============================] - 31s 848us/sample - loss: 0.2501 - acc: 0.9216 - val_loss: 0.1597 - val_acc: 0.9539\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2463 - acc: 0.9218\n",
      "Epoch 00031: val_loss did not improve from 0.15969\n",
      "36805/36805 [==============================] - 31s 845us/sample - loss: 0.2464 - acc: 0.9217 - val_loss: 0.1986 - val_acc: 0.9420\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2479 - acc: 0.9234\n",
      "Epoch 00032: val_loss improved from 0.15969 to 0.15836, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_6_conv_checkpoint/032-0.1584.hdf5\n",
      "36805/36805 [==============================] - 31s 847us/sample - loss: 0.2478 - acc: 0.9234 - val_loss: 0.1584 - val_acc: 0.9534\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2418 - acc: 0.9234\n",
      "Epoch 00033: val_loss improved from 0.15836 to 0.15030, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_6_conv_checkpoint/033-0.1503.hdf5\n",
      "36805/36805 [==============================] - 31s 847us/sample - loss: 0.2418 - acc: 0.9234 - val_loss: 0.1503 - val_acc: 0.9569\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2390 - acc: 0.9248\n",
      "Epoch 00034: val_loss improved from 0.15030 to 0.14855, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_6_conv_checkpoint/034-0.1485.hdf5\n",
      "36805/36805 [==============================] - 31s 849us/sample - loss: 0.2390 - acc: 0.9248 - val_loss: 0.1485 - val_acc: 0.9534\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2317 - acc: 0.9277\n",
      "Epoch 00035: val_loss did not improve from 0.14855\n",
      "36805/36805 [==============================] - 31s 846us/sample - loss: 0.2317 - acc: 0.9277 - val_loss: 0.1529 - val_acc: 0.9525\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2257 - acc: 0.9288\n",
      "Epoch 00036: val_loss improved from 0.14855 to 0.14515, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_6_conv_checkpoint/036-0.1451.hdf5\n",
      "36805/36805 [==============================] - 31s 848us/sample - loss: 0.2258 - acc: 0.9288 - val_loss: 0.1451 - val_acc: 0.9578\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2216 - acc: 0.9319\n",
      "Epoch 00037: val_loss did not improve from 0.14515\n",
      "36805/36805 [==============================] - 31s 849us/sample - loss: 0.2216 - acc: 0.9319 - val_loss: 0.1559 - val_acc: 0.9529\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2199 - acc: 0.9294\n",
      "Epoch 00038: val_loss did not improve from 0.14515\n",
      "36805/36805 [==============================] - 31s 844us/sample - loss: 0.2199 - acc: 0.9294 - val_loss: 0.1521 - val_acc: 0.9553\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2178 - acc: 0.9305\n",
      "Epoch 00039: val_loss did not improve from 0.14515\n",
      "36805/36805 [==============================] - 31s 847us/sample - loss: 0.2178 - acc: 0.9305 - val_loss: 0.1462 - val_acc: 0.9529\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2137 - acc: 0.9320\n",
      "Epoch 00040: val_loss did not improve from 0.14515\n",
      "36805/36805 [==============================] - 31s 848us/sample - loss: 0.2137 - acc: 0.9320 - val_loss: 0.1568 - val_acc: 0.9509\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2057 - acc: 0.9351\n",
      "Epoch 00041: val_loss improved from 0.14515 to 0.13664, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_6_conv_checkpoint/041-0.1366.hdf5\n",
      "36805/36805 [==============================] - 31s 845us/sample - loss: 0.2057 - acc: 0.9351 - val_loss: 0.1366 - val_acc: 0.9571\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2049 - acc: 0.9350\n",
      "Epoch 00042: val_loss did not improve from 0.13664\n",
      "36805/36805 [==============================] - 31s 848us/sample - loss: 0.2049 - acc: 0.9350 - val_loss: 0.1369 - val_acc: 0.9592\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2017 - acc: 0.9354\n",
      "Epoch 00043: val_loss did not improve from 0.13664\n",
      "36805/36805 [==============================] - 31s 842us/sample - loss: 0.2017 - acc: 0.9354 - val_loss: 0.1419 - val_acc: 0.9553\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2011 - acc: 0.9361\n",
      "Epoch 00044: val_loss improved from 0.13664 to 0.13607, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_6_conv_checkpoint/044-0.1361.hdf5\n",
      "36805/36805 [==============================] - 31s 846us/sample - loss: 0.2011 - acc: 0.9361 - val_loss: 0.1361 - val_acc: 0.9592\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1966 - acc: 0.9369\n",
      "Epoch 00045: val_loss did not improve from 0.13607\n",
      "36805/36805 [==============================] - 31s 848us/sample - loss: 0.1966 - acc: 0.9369 - val_loss: 0.1490 - val_acc: 0.9522\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1932 - acc: 0.9381\n",
      "Epoch 00046: val_loss improved from 0.13607 to 0.13243, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_6_conv_checkpoint/046-0.1324.hdf5\n",
      "36805/36805 [==============================] - 31s 847us/sample - loss: 0.1932 - acc: 0.9381 - val_loss: 0.1324 - val_acc: 0.9581\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1888 - acc: 0.9389\n",
      "Epoch 00047: val_loss did not improve from 0.13243\n",
      "36805/36805 [==============================] - 31s 845us/sample - loss: 0.1888 - acc: 0.9389 - val_loss: 0.1510 - val_acc: 0.9532\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1822 - acc: 0.9402\n",
      "Epoch 00048: val_loss did not improve from 0.13243\n",
      "36805/36805 [==============================] - 31s 849us/sample - loss: 0.1822 - acc: 0.9403 - val_loss: 0.1367 - val_acc: 0.9574\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1857 - acc: 0.9401\n",
      "Epoch 00049: val_loss did not improve from 0.13243\n",
      "36805/36805 [==============================] - 31s 850us/sample - loss: 0.1857 - acc: 0.9401 - val_loss: 0.1370 - val_acc: 0.9588\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1811 - acc: 0.9422\n",
      "Epoch 00050: val_loss did not improve from 0.13243\n",
      "36805/36805 [==============================] - 31s 846us/sample - loss: 0.1810 - acc: 0.9422 - val_loss: 0.1505 - val_acc: 0.9569\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1821 - acc: 0.9428\n",
      "Epoch 00051: val_loss improved from 0.13243 to 0.13053, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_6_conv_checkpoint/051-0.1305.hdf5\n",
      "36805/36805 [==============================] - 31s 850us/sample - loss: 0.1820 - acc: 0.9428 - val_loss: 0.1305 - val_acc: 0.9576\n",
      "Epoch 52/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1763 - acc: 0.9439\n",
      "Epoch 00052: val_loss improved from 0.13053 to 0.12662, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_6_conv_checkpoint/052-0.1266.hdf5\n",
      "36805/36805 [==============================] - 31s 848us/sample - loss: 0.1761 - acc: 0.9440 - val_loss: 0.1266 - val_acc: 0.9606\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1732 - acc: 0.9436\n",
      "Epoch 00053: val_loss did not improve from 0.12662\n",
      "36805/36805 [==============================] - 31s 845us/sample - loss: 0.1732 - acc: 0.9436 - val_loss: 0.1364 - val_acc: 0.9578\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1768 - acc: 0.9429\n",
      "Epoch 00054: val_loss did not improve from 0.12662\n",
      "36805/36805 [==============================] - 31s 845us/sample - loss: 0.1768 - acc: 0.9429 - val_loss: 0.1439 - val_acc: 0.9574\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1734 - acc: 0.9452\n",
      "Epoch 00055: val_loss did not improve from 0.12662\n",
      "36805/36805 [==============================] - 31s 845us/sample - loss: 0.1734 - acc: 0.9452 - val_loss: 0.1286 - val_acc: 0.9595\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1711 - acc: 0.9453\n",
      "Epoch 00056: val_loss did not improve from 0.12662\n",
      "36805/36805 [==============================] - 31s 842us/sample - loss: 0.1711 - acc: 0.9453 - val_loss: 0.1285 - val_acc: 0.9595\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1656 - acc: 0.9463\n",
      "Epoch 00057: val_loss did not improve from 0.12662\n",
      "36805/36805 [==============================] - 31s 845us/sample - loss: 0.1656 - acc: 0.9463 - val_loss: 0.1287 - val_acc: 0.9590\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1624 - acc: 0.9479\n",
      "Epoch 00058: val_loss did not improve from 0.12662\n",
      "36805/36805 [==============================] - 31s 846us/sample - loss: 0.1624 - acc: 0.9479 - val_loss: 0.1394 - val_acc: 0.9562\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1665 - acc: 0.9468\n",
      "Epoch 00059: val_loss did not improve from 0.12662\n",
      "36805/36805 [==============================] - 31s 845us/sample - loss: 0.1665 - acc: 0.9468 - val_loss: 0.1307 - val_acc: 0.9599\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1595 - acc: 0.9483\n",
      "Epoch 00060: val_loss improved from 0.12662 to 0.12399, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_6_conv_checkpoint/060-0.1240.hdf5\n",
      "36805/36805 [==============================] - 31s 845us/sample - loss: 0.1595 - acc: 0.9483 - val_loss: 0.1240 - val_acc: 0.9616\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1567 - acc: 0.9498\n",
      "Epoch 00061: val_loss did not improve from 0.12399\n",
      "36805/36805 [==============================] - 31s 841us/sample - loss: 0.1566 - acc: 0.9498 - val_loss: 0.1400 - val_acc: 0.9564\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1595 - acc: 0.9496\n",
      "Epoch 00062: val_loss did not improve from 0.12399\n",
      "36805/36805 [==============================] - 31s 847us/sample - loss: 0.1595 - acc: 0.9496 - val_loss: 0.1360 - val_acc: 0.9597\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1538 - acc: 0.9505\n",
      "Epoch 00063: val_loss did not improve from 0.12399\n",
      "36805/36805 [==============================] - 31s 842us/sample - loss: 0.1537 - acc: 0.9506 - val_loss: 0.1329 - val_acc: 0.9609\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1510 - acc: 0.9507\n",
      "Epoch 00064: val_loss did not improve from 0.12399\n",
      "36805/36805 [==============================] - 31s 844us/sample - loss: 0.1510 - acc: 0.9507 - val_loss: 0.1307 - val_acc: 0.9567\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1530 - acc: 0.9509\n",
      "Epoch 00065: val_loss did not improve from 0.12399\n",
      "36805/36805 [==============================] - 31s 849us/sample - loss: 0.1530 - acc: 0.9509 - val_loss: 0.1297 - val_acc: 0.9609\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1456 - acc: 0.9538\n",
      "Epoch 00066: val_loss did not improve from 0.12399\n",
      "36805/36805 [==============================] - 31s 842us/sample - loss: 0.1456 - acc: 0.9538 - val_loss: 0.1256 - val_acc: 0.9630\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1432 - acc: 0.9536\n",
      "Epoch 00067: val_loss did not improve from 0.12399\n",
      "36805/36805 [==============================] - 31s 847us/sample - loss: 0.1432 - acc: 0.9536 - val_loss: 0.1272 - val_acc: 0.9604\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1431 - acc: 0.9540\n",
      "Epoch 00068: val_loss did not improve from 0.12399\n",
      "36805/36805 [==============================] - 31s 845us/sample - loss: 0.1431 - acc: 0.9541 - val_loss: 0.1380 - val_acc: 0.9585\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1415 - acc: 0.9534\n",
      "Epoch 00069: val_loss did not improve from 0.12399\n",
      "36805/36805 [==============================] - 31s 848us/sample - loss: 0.1415 - acc: 0.9534 - val_loss: 0.1361 - val_acc: 0.9597\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1376 - acc: 0.9555\n",
      "Epoch 00070: val_loss improved from 0.12399 to 0.12267, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_6_conv_checkpoint/070-0.1227.hdf5\n",
      "36805/36805 [==============================] - 31s 849us/sample - loss: 0.1376 - acc: 0.9555 - val_loss: 0.1227 - val_acc: 0.9641\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1380 - acc: 0.9548\n",
      "Epoch 00071: val_loss improved from 0.12267 to 0.12009, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_6_conv_checkpoint/071-0.1201.hdf5\n",
      "36805/36805 [==============================] - 31s 845us/sample - loss: 0.1380 - acc: 0.9548 - val_loss: 0.1201 - val_acc: 0.9646\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1352 - acc: 0.9559\n",
      "Epoch 00072: val_loss did not improve from 0.12009\n",
      "36805/36805 [==============================] - 31s 846us/sample - loss: 0.1352 - acc: 0.9559 - val_loss: 0.1331 - val_acc: 0.9599\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1341 - acc: 0.9556\n",
      "Epoch 00073: val_loss did not improve from 0.12009\n",
      "36805/36805 [==============================] - 31s 846us/sample - loss: 0.1341 - acc: 0.9556 - val_loss: 0.1310 - val_acc: 0.9583\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1347 - acc: 0.9554\n",
      "Epoch 00074: val_loss did not improve from 0.12009\n",
      "36805/36805 [==============================] - 31s 849us/sample - loss: 0.1346 - acc: 0.9554 - val_loss: 0.1363 - val_acc: 0.9604\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1307 - acc: 0.9582\n",
      "Epoch 00075: val_loss did not improve from 0.12009\n",
      "36805/36805 [==============================] - 31s 843us/sample - loss: 0.1309 - acc: 0.9582 - val_loss: 0.1259 - val_acc: 0.9627\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1296 - acc: 0.9577\n",
      "Epoch 00076: val_loss did not improve from 0.12009\n",
      "36805/36805 [==============================] - 31s 847us/sample - loss: 0.1296 - acc: 0.9577 - val_loss: 0.1333 - val_acc: 0.9599\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1256 - acc: 0.9590\n",
      "Epoch 00077: val_loss did not improve from 0.12009\n",
      "36805/36805 [==============================] - 31s 848us/sample - loss: 0.1256 - acc: 0.9590 - val_loss: 0.1206 - val_acc: 0.9625\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1292 - acc: 0.9568\n",
      "Epoch 00078: val_loss improved from 0.12009 to 0.11614, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_6_conv_checkpoint/078-0.1161.hdf5\n",
      "36805/36805 [==============================] - 31s 847us/sample - loss: 0.1291 - acc: 0.9568 - val_loss: 0.1161 - val_acc: 0.9641\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1233 - acc: 0.9601\n",
      "Epoch 00079: val_loss did not improve from 0.11614\n",
      "36805/36805 [==============================] - 31s 848us/sample - loss: 0.1234 - acc: 0.9601 - val_loss: 0.1206 - val_acc: 0.9651\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1278 - acc: 0.9586\n",
      "Epoch 00080: val_loss did not improve from 0.11614\n",
      "36805/36805 [==============================] - 31s 848us/sample - loss: 0.1278 - acc: 0.9586 - val_loss: 0.1262 - val_acc: 0.9616\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1189 - acc: 0.9604\n",
      "Epoch 00081: val_loss did not improve from 0.11614\n",
      "36805/36805 [==============================] - 31s 845us/sample - loss: 0.1189 - acc: 0.9604 - val_loss: 0.1260 - val_acc: 0.9613\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1234 - acc: 0.9598\n",
      "Epoch 00082: val_loss did not improve from 0.11614\n",
      "36805/36805 [==============================] - 31s 845us/sample - loss: 0.1235 - acc: 0.9597 - val_loss: 0.1482 - val_acc: 0.9571\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1261 - acc: 0.9577\n",
      "Epoch 00083: val_loss did not improve from 0.11614\n",
      "36805/36805 [==============================] - 31s 850us/sample - loss: 0.1260 - acc: 0.9577 - val_loss: 0.1238 - val_acc: 0.9634\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1161 - acc: 0.9619\n",
      "Epoch 00084: val_loss did not improve from 0.11614\n",
      "36805/36805 [==============================] - 31s 843us/sample - loss: 0.1161 - acc: 0.9619 - val_loss: 0.1227 - val_acc: 0.9646\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1163 - acc: 0.9620\n",
      "Epoch 00085: val_loss did not improve from 0.11614\n",
      "36805/36805 [==============================] - 31s 846us/sample - loss: 0.1163 - acc: 0.9620 - val_loss: 0.1241 - val_acc: 0.9634\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1153 - acc: 0.9622\n",
      "Epoch 00086: val_loss did not improve from 0.11614\n",
      "36805/36805 [==============================] - 31s 846us/sample - loss: 0.1153 - acc: 0.9622 - val_loss: 0.1254 - val_acc: 0.9644\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1114 - acc: 0.9623\n",
      "Epoch 00087: val_loss improved from 0.11614 to 0.11555, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_6_conv_checkpoint/087-0.1155.hdf5\n",
      "36805/36805 [==============================] - 31s 848us/sample - loss: 0.1114 - acc: 0.9623 - val_loss: 0.1155 - val_acc: 0.9653\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1115 - acc: 0.9631\n",
      "Epoch 00088: val_loss did not improve from 0.11555\n",
      "36805/36805 [==============================] - 31s 848us/sample - loss: 0.1115 - acc: 0.9631 - val_loss: 0.1310 - val_acc: 0.9604\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1120 - acc: 0.9625\n",
      "Epoch 00089: val_loss did not improve from 0.11555\n",
      "36805/36805 [==============================] - 31s 842us/sample - loss: 0.1120 - acc: 0.9625 - val_loss: 0.1212 - val_acc: 0.9644\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1133 - acc: 0.9636\n",
      "Epoch 00090: val_loss did not improve from 0.11555\n",
      "36805/36805 [==============================] - 31s 846us/sample - loss: 0.1132 - acc: 0.9636 - val_loss: 0.1212 - val_acc: 0.9634\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1113 - acc: 0.9633\n",
      "Epoch 00091: val_loss did not improve from 0.11555\n",
      "36805/36805 [==============================] - 31s 842us/sample - loss: 0.1113 - acc: 0.9633 - val_loss: 0.1198 - val_acc: 0.9651\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1064 - acc: 0.9647\n",
      "Epoch 00092: val_loss did not improve from 0.11555\n",
      "36805/36805 [==============================] - 31s 849us/sample - loss: 0.1064 - acc: 0.9647 - val_loss: 0.1210 - val_acc: 0.9646\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1059 - acc: 0.9646\n",
      "Epoch 00093: val_loss did not improve from 0.11555\n",
      "36805/36805 [==============================] - 31s 848us/sample - loss: 0.1059 - acc: 0.9646 - val_loss: 0.1291 - val_acc: 0.9625\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1072 - acc: 0.9646\n",
      "Epoch 00094: val_loss did not improve from 0.11555\n",
      "36805/36805 [==============================] - 31s 849us/sample - loss: 0.1072 - acc: 0.9646 - val_loss: 0.1330 - val_acc: 0.9637\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1036 - acc: 0.9654\n",
      "Epoch 00095: val_loss did not improve from 0.11555\n",
      "36805/36805 [==============================] - 31s 845us/sample - loss: 0.1036 - acc: 0.9653 - val_loss: 0.1168 - val_acc: 0.9672\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1039 - acc: 0.9662\n",
      "Epoch 00096: val_loss did not improve from 0.11555\n",
      "36805/36805 [==============================] - 31s 852us/sample - loss: 0.1039 - acc: 0.9662 - val_loss: 0.1239 - val_acc: 0.9632\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1034 - acc: 0.9655\n",
      "Epoch 00097: val_loss did not improve from 0.11555\n",
      "36805/36805 [==============================] - 31s 846us/sample - loss: 0.1034 - acc: 0.9655 - val_loss: 0.1349 - val_acc: 0.9606\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1020 - acc: 0.9653\n",
      "Epoch 00098: val_loss did not improve from 0.11555\n",
      "36805/36805 [==============================] - 31s 849us/sample - loss: 0.1020 - acc: 0.9653 - val_loss: 0.1331 - val_acc: 0.9606\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1011 - acc: 0.9669\n",
      "Epoch 00099: val_loss did not improve from 0.11555\n",
      "36805/36805 [==============================] - 31s 848us/sample - loss: 0.1011 - acc: 0.9669 - val_loss: 0.1205 - val_acc: 0.9646\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0978 - acc: 0.9674\n",
      "Epoch 00100: val_loss did not improve from 0.11555\n",
      "36805/36805 [==============================] - 31s 849us/sample - loss: 0.0978 - acc: 0.9674 - val_loss: 0.1231 - val_acc: 0.9620\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0956 - acc: 0.9681\n",
      "Epoch 00101: val_loss did not improve from 0.11555\n",
      "36805/36805 [==============================] - 31s 848us/sample - loss: 0.0955 - acc: 0.9681 - val_loss: 0.1209 - val_acc: 0.9641\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0975 - acc: 0.9670\n",
      "Epoch 00102: val_loss did not improve from 0.11555\n",
      "36805/36805 [==============================] - 31s 844us/sample - loss: 0.0975 - acc: 0.9670 - val_loss: 0.1169 - val_acc: 0.9655\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0941 - acc: 0.9686\n",
      "Epoch 00103: val_loss did not improve from 0.11555\n",
      "36805/36805 [==============================] - 31s 846us/sample - loss: 0.0941 - acc: 0.9686 - val_loss: 0.1195 - val_acc: 0.9655\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0978 - acc: 0.9667\n",
      "Epoch 00104: val_loss did not improve from 0.11555\n",
      "36805/36805 [==============================] - 31s 846us/sample - loss: 0.0978 - acc: 0.9667 - val_loss: 0.1275 - val_acc: 0.9646\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0936 - acc: 0.9678\n",
      "Epoch 00105: val_loss did not improve from 0.11555\n",
      "36805/36805 [==============================] - 31s 848us/sample - loss: 0.0935 - acc: 0.9678 - val_loss: 0.1239 - val_acc: 0.9639\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0913 - acc: 0.9688\n",
      "Epoch 00106: val_loss did not improve from 0.11555\n",
      "36805/36805 [==============================] - 31s 846us/sample - loss: 0.0913 - acc: 0.9688 - val_loss: 0.1216 - val_acc: 0.9639\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0924 - acc: 0.9683\n",
      "Epoch 00107: val_loss did not improve from 0.11555\n",
      "36805/36805 [==============================] - 31s 843us/sample - loss: 0.0924 - acc: 0.9683 - val_loss: 0.1272 - val_acc: 0.9644\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0926 - acc: 0.9697\n",
      "Epoch 00108: val_loss did not improve from 0.11555\n",
      "36805/36805 [==============================] - 31s 847us/sample - loss: 0.0926 - acc: 0.9697 - val_loss: 0.1308 - val_acc: 0.9667\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0923 - acc: 0.9688\n",
      "Epoch 00109: val_loss did not improve from 0.11555\n",
      "36805/36805 [==============================] - 31s 851us/sample - loss: 0.0922 - acc: 0.9688 - val_loss: 0.1345 - val_acc: 0.9634\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0888 - acc: 0.9697\n",
      "Epoch 00110: val_loss did not improve from 0.11555\n",
      "36805/36805 [==============================] - 31s 843us/sample - loss: 0.0888 - acc: 0.9697 - val_loss: 0.1299 - val_acc: 0.9627\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0893 - acc: 0.9700\n",
      "Epoch 00111: val_loss did not improve from 0.11555\n",
      "36805/36805 [==============================] - 31s 850us/sample - loss: 0.0893 - acc: 0.9700 - val_loss: 0.1297 - val_acc: 0.9632\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0900 - acc: 0.9705\n",
      "Epoch 00112: val_loss did not improve from 0.11555\n",
      "36805/36805 [==============================] - 31s 845us/sample - loss: 0.0900 - acc: 0.9705 - val_loss: 0.1221 - val_acc: 0.9669\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0896 - acc: 0.9699\n",
      "Epoch 00113: val_loss did not improve from 0.11555\n",
      "36805/36805 [==============================] - 31s 846us/sample - loss: 0.0896 - acc: 0.9699 - val_loss: 0.1201 - val_acc: 0.9646\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0866 - acc: 0.9715\n",
      "Epoch 00114: val_loss did not improve from 0.11555\n",
      "36805/36805 [==============================] - 31s 847us/sample - loss: 0.0866 - acc: 0.9715 - val_loss: 0.1230 - val_acc: 0.9648\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0835 - acc: 0.9713\n",
      "Epoch 00115: val_loss did not improve from 0.11555\n",
      "36805/36805 [==============================] - 31s 847us/sample - loss: 0.0835 - acc: 0.9713 - val_loss: 0.1337 - val_acc: 0.9620\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0827 - acc: 0.9726\n",
      "Epoch 00116: val_loss did not improve from 0.11555\n",
      "36805/36805 [==============================] - 31s 846us/sample - loss: 0.0827 - acc: 0.9726 - val_loss: 0.1238 - val_acc: 0.9665\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0842 - acc: 0.9711\n",
      "Epoch 00117: val_loss did not improve from 0.11555\n",
      "36805/36805 [==============================] - 31s 844us/sample - loss: 0.0842 - acc: 0.9711 - val_loss: 0.1326 - val_acc: 0.9660\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0826 - acc: 0.9723\n",
      "Epoch 00118: val_loss did not improve from 0.11555\n",
      "36805/36805 [==============================] - 31s 849us/sample - loss: 0.0826 - acc: 0.9723 - val_loss: 0.1221 - val_acc: 0.9653\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0804 - acc: 0.9732\n",
      "Epoch 00119: val_loss did not improve from 0.11555\n",
      "36805/36805 [==============================] - 31s 847us/sample - loss: 0.0804 - acc: 0.9732 - val_loss: 0.1240 - val_acc: 0.9653\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0803 - acc: 0.9738\n",
      "Epoch 00120: val_loss did not improve from 0.11555\n",
      "36805/36805 [==============================] - 31s 851us/sample - loss: 0.0803 - acc: 0.9738 - val_loss: 0.1324 - val_acc: 0.9662\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0829 - acc: 0.9724\n",
      "Epoch 00121: val_loss did not improve from 0.11555\n",
      "36805/36805 [==============================] - 31s 848us/sample - loss: 0.0829 - acc: 0.9724 - val_loss: 0.1332 - val_acc: 0.9630\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0835 - acc: 0.9717\n",
      "Epoch 00122: val_loss did not improve from 0.11555\n",
      "36805/36805 [==============================] - 31s 842us/sample - loss: 0.0835 - acc: 0.9717 - val_loss: 0.1247 - val_acc: 0.9662\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0780 - acc: 0.9736\n",
      "Epoch 00123: val_loss did not improve from 0.11555\n",
      "36805/36805 [==============================] - 31s 846us/sample - loss: 0.0780 - acc: 0.9736 - val_loss: 0.1215 - val_acc: 0.9688\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0782 - acc: 0.9736\n",
      "Epoch 00124: val_loss did not improve from 0.11555\n",
      "36805/36805 [==============================] - 31s 845us/sample - loss: 0.0782 - acc: 0.9736 - val_loss: 0.1303 - val_acc: 0.9627\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0738 - acc: 0.9749\n",
      "Epoch 00125: val_loss did not improve from 0.11555\n",
      "36805/36805 [==============================] - 31s 846us/sample - loss: 0.0738 - acc: 0.9749 - val_loss: 0.1227 - val_acc: 0.9660\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0799 - acc: 0.9734\n",
      "Epoch 00126: val_loss did not improve from 0.11555\n",
      "36805/36805 [==============================] - 31s 850us/sample - loss: 0.0799 - acc: 0.9734 - val_loss: 0.1395 - val_acc: 0.9613\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0767 - acc: 0.9738\n",
      "Epoch 00127: val_loss did not improve from 0.11555\n",
      "36805/36805 [==============================] - 31s 847us/sample - loss: 0.0767 - acc: 0.9738 - val_loss: 0.1240 - val_acc: 0.9669\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0768 - acc: 0.9743\n",
      "Epoch 00128: val_loss did not improve from 0.11555\n",
      "36805/36805 [==============================] - 31s 848us/sample - loss: 0.0768 - acc: 0.9743 - val_loss: 0.1346 - val_acc: 0.9686\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0759 - acc: 0.9740\n",
      "Epoch 00129: val_loss did not improve from 0.11555\n",
      "36805/36805 [==============================] - 31s 848us/sample - loss: 0.0759 - acc: 0.9740 - val_loss: 0.1269 - val_acc: 0.9655\n",
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0721 - acc: 0.9754\n",
      "Epoch 00130: val_loss did not improve from 0.11555\n",
      "36805/36805 [==============================] - 31s 849us/sample - loss: 0.0721 - acc: 0.9754 - val_loss: 0.1299 - val_acc: 0.9653\n",
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0713 - acc: 0.9757\n",
      "Epoch 00131: val_loss did not improve from 0.11555\n",
      "36805/36805 [==============================] - 31s 843us/sample - loss: 0.0713 - acc: 0.9757 - val_loss: 0.1345 - val_acc: 0.9660\n",
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0773 - acc: 0.9734\n",
      "Epoch 00132: val_loss did not improve from 0.11555\n",
      "36805/36805 [==============================] - 31s 847us/sample - loss: 0.0773 - acc: 0.9734 - val_loss: 0.1447 - val_acc: 0.9625\n",
      "Epoch 133/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0755 - acc: 0.9751\n",
      "Epoch 00133: val_loss did not improve from 0.11555\n",
      "36805/36805 [==============================] - 31s 842us/sample - loss: 0.0755 - acc: 0.9751 - val_loss: 0.1301 - val_acc: 0.9660\n",
      "Epoch 134/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0678 - acc: 0.9767\n",
      "Epoch 00134: val_loss did not improve from 0.11555\n",
      "36805/36805 [==============================] - 31s 848us/sample - loss: 0.0679 - acc: 0.9766 - val_loss: 0.1428 - val_acc: 0.9604\n",
      "Epoch 135/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0708 - acc: 0.9752\n",
      "Epoch 00135: val_loss did not improve from 0.11555\n",
      "36805/36805 [==============================] - 31s 844us/sample - loss: 0.0709 - acc: 0.9752 - val_loss: 0.1341 - val_acc: 0.9653\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 136/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0707 - acc: 0.9765\n",
      "Epoch 00136: val_loss did not improve from 0.11555\n",
      "36805/36805 [==============================] - 31s 847us/sample - loss: 0.0707 - acc: 0.9765 - val_loss: 0.1424 - val_acc: 0.9632\n",
      "Epoch 137/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0684 - acc: 0.9769\n",
      "Epoch 00137: val_loss did not improve from 0.11555\n",
      "36805/36805 [==============================] - 31s 841us/sample - loss: 0.0684 - acc: 0.9769 - val_loss: 0.1311 - val_acc: 0.9653\n",
      "\n",
      "1D_CNN_custom_multi_3_GMP_DO_6_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 469us/sample - loss: 0.1525 - acc: 0.9537\n",
      "Loss: 0.15245757239289498 Accuracy: 0.9536864\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.1134 - acc: 0.3034\n",
      "Epoch 00001: val_loss improved from inf to 1.29027, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_7_conv_checkpoint/001-1.2903.hdf5\n",
      "36805/36805 [==============================] - 36s 971us/sample - loss: 2.1134 - acc: 0.3034 - val_loss: 1.2903 - val_acc: 0.6154\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2678 - acc: 0.5864\n",
      "Epoch 00002: val_loss improved from 1.29027 to 0.75220, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_7_conv_checkpoint/002-0.7522.hdf5\n",
      "36805/36805 [==============================] - 32s 868us/sample - loss: 1.2678 - acc: 0.5864 - val_loss: 0.7522 - val_acc: 0.7911\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9097 - acc: 0.7099\n",
      "Epoch 00003: val_loss improved from 0.75220 to 0.55085, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_7_conv_checkpoint/003-0.5509.hdf5\n",
      "36805/36805 [==============================] - 32s 867us/sample - loss: 0.9096 - acc: 0.7099 - val_loss: 0.5509 - val_acc: 0.8425\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7240 - acc: 0.7693\n",
      "Epoch 00004: val_loss improved from 0.55085 to 0.43546, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_7_conv_checkpoint/004-0.4355.hdf5\n",
      "36805/36805 [==============================] - 32s 873us/sample - loss: 0.7239 - acc: 0.7693 - val_loss: 0.4355 - val_acc: 0.8761\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6024 - acc: 0.8086\n",
      "Epoch 00005: val_loss improved from 0.43546 to 0.34281, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_7_conv_checkpoint/005-0.3428.hdf5\n",
      "36805/36805 [==============================] - 32s 869us/sample - loss: 0.6024 - acc: 0.8086 - val_loss: 0.3428 - val_acc: 0.9017\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5276 - acc: 0.8343\n",
      "Epoch 00006: val_loss improved from 0.34281 to 0.31177, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_7_conv_checkpoint/006-0.3118.hdf5\n",
      "36805/36805 [==============================] - 33s 892us/sample - loss: 0.5276 - acc: 0.8343 - val_loss: 0.3118 - val_acc: 0.9080\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4668 - acc: 0.8541\n",
      "Epoch 00007: val_loss improved from 0.31177 to 0.27346, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_7_conv_checkpoint/007-0.2735.hdf5\n",
      "36805/36805 [==============================] - 32s 869us/sample - loss: 0.4668 - acc: 0.8542 - val_loss: 0.2735 - val_acc: 0.9189\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4193 - acc: 0.8683\n",
      "Epoch 00008: val_loss improved from 0.27346 to 0.23685, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_7_conv_checkpoint/008-0.2369.hdf5\n",
      "36805/36805 [==============================] - 32s 870us/sample - loss: 0.4193 - acc: 0.8683 - val_loss: 0.2369 - val_acc: 0.9331\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3844 - acc: 0.8797\n",
      "Epoch 00009: val_loss improved from 0.23685 to 0.21438, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_7_conv_checkpoint/009-0.2144.hdf5\n",
      "36805/36805 [==============================] - 32s 872us/sample - loss: 0.3843 - acc: 0.8797 - val_loss: 0.2144 - val_acc: 0.9390\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3571 - acc: 0.8892\n",
      "Epoch 00010: val_loss improved from 0.21438 to 0.20634, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_7_conv_checkpoint/010-0.2063.hdf5\n",
      "36805/36805 [==============================] - 32s 872us/sample - loss: 0.3570 - acc: 0.8892 - val_loss: 0.2063 - val_acc: 0.9413\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3306 - acc: 0.8955\n",
      "Epoch 00011: val_loss improved from 0.20634 to 0.18949, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_7_conv_checkpoint/011-0.1895.hdf5\n",
      "36805/36805 [==============================] - 32s 869us/sample - loss: 0.3308 - acc: 0.8955 - val_loss: 0.1895 - val_acc: 0.9441\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3125 - acc: 0.9016\n",
      "Epoch 00012: val_loss improved from 0.18949 to 0.17733, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_7_conv_checkpoint/012-0.1773.hdf5\n",
      "36805/36805 [==============================] - 32s 868us/sample - loss: 0.3125 - acc: 0.9016 - val_loss: 0.1773 - val_acc: 0.9492\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2966 - acc: 0.9074\n",
      "Epoch 00013: val_loss improved from 0.17733 to 0.16321, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_7_conv_checkpoint/013-0.1632.hdf5\n",
      "36805/36805 [==============================] - 32s 869us/sample - loss: 0.2966 - acc: 0.9074 - val_loss: 0.1632 - val_acc: 0.9541\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2814 - acc: 0.9111\n",
      "Epoch 00014: val_loss did not improve from 0.16321\n",
      "36805/36805 [==============================] - 32s 868us/sample - loss: 0.2814 - acc: 0.9111 - val_loss: 0.1686 - val_acc: 0.9462\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2671 - acc: 0.9157\n",
      "Epoch 00015: val_loss improved from 0.16321 to 0.15532, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_7_conv_checkpoint/015-0.1553.hdf5\n",
      "36805/36805 [==============================] - 32s 869us/sample - loss: 0.2670 - acc: 0.9157 - val_loss: 0.1553 - val_acc: 0.9532\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2525 - acc: 0.9204\n",
      "Epoch 00016: val_loss improved from 0.15532 to 0.14258, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_7_conv_checkpoint/016-0.1426.hdf5\n",
      "36805/36805 [==============================] - 32s 862us/sample - loss: 0.2524 - acc: 0.9204 - val_loss: 0.1426 - val_acc: 0.9581\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2480 - acc: 0.9226\n",
      "Epoch 00017: val_loss did not improve from 0.14258\n",
      "36805/36805 [==============================] - 32s 867us/sample - loss: 0.2480 - acc: 0.9226 - val_loss: 0.1433 - val_acc: 0.9543\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2384 - acc: 0.9247\n",
      "Epoch 00018: val_loss did not improve from 0.14258\n",
      "36805/36805 [==============================] - 32s 867us/sample - loss: 0.2383 - acc: 0.9247 - val_loss: 0.1444 - val_acc: 0.9588\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2280 - acc: 0.9276\n",
      "Epoch 00019: val_loss did not improve from 0.14258\n",
      "36805/36805 [==============================] - 32s 865us/sample - loss: 0.2280 - acc: 0.9276 - val_loss: 0.1464 - val_acc: 0.9560\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2136 - acc: 0.9337\n",
      "Epoch 00020: val_loss improved from 0.14258 to 0.13716, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_7_conv_checkpoint/020-0.1372.hdf5\n",
      "36805/36805 [==============================] - 32s 870us/sample - loss: 0.2135 - acc: 0.9337 - val_loss: 0.1372 - val_acc: 0.9585\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2085 - acc: 0.9328\n",
      "Epoch 00021: val_loss improved from 0.13716 to 0.13000, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_7_conv_checkpoint/021-0.1300.hdf5\n",
      "36805/36805 [==============================] - 32s 864us/sample - loss: 0.2085 - acc: 0.9328 - val_loss: 0.1300 - val_acc: 0.9623\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2028 - acc: 0.9349\n",
      "Epoch 00022: val_loss did not improve from 0.13000\n",
      "36805/36805 [==============================] - 32s 863us/sample - loss: 0.2028 - acc: 0.9349 - val_loss: 0.1438 - val_acc: 0.9571\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1957 - acc: 0.9381\n",
      "Epoch 00023: val_loss did not improve from 0.13000\n",
      "36805/36805 [==============================] - 32s 871us/sample - loss: 0.1957 - acc: 0.9381 - val_loss: 0.1409 - val_acc: 0.9553\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1895 - acc: 0.9387\n",
      "Epoch 00024: val_loss did not improve from 0.13000\n",
      "36805/36805 [==============================] - 32s 861us/sample - loss: 0.1895 - acc: 0.9387 - val_loss: 0.1309 - val_acc: 0.9609\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1836 - acc: 0.9413\n",
      "Epoch 00025: val_loss improved from 0.13000 to 0.12876, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_7_conv_checkpoint/025-0.1288.hdf5\n",
      "36805/36805 [==============================] - 32s 868us/sample - loss: 0.1836 - acc: 0.9413 - val_loss: 0.1288 - val_acc: 0.9625\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1783 - acc: 0.9438\n",
      "Epoch 00026: val_loss improved from 0.12876 to 0.12314, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_7_conv_checkpoint/026-0.1231.hdf5\n",
      "36805/36805 [==============================] - 32s 869us/sample - loss: 0.1785 - acc: 0.9438 - val_loss: 0.1231 - val_acc: 0.9585\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1761 - acc: 0.9432\n",
      "Epoch 00027: val_loss improved from 0.12314 to 0.12133, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_7_conv_checkpoint/027-0.1213.hdf5\n",
      "36805/36805 [==============================] - 32s 873us/sample - loss: 0.1761 - acc: 0.9432 - val_loss: 0.1213 - val_acc: 0.9606\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1700 - acc: 0.9447\n",
      "Epoch 00028: val_loss did not improve from 0.12133\n",
      "36805/36805 [==============================] - 32s 865us/sample - loss: 0.1700 - acc: 0.9447 - val_loss: 0.1356 - val_acc: 0.9569\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1618 - acc: 0.9487\n",
      "Epoch 00029: val_loss did not improve from 0.12133\n",
      "36805/36805 [==============================] - 32s 865us/sample - loss: 0.1618 - acc: 0.9487 - val_loss: 0.1236 - val_acc: 0.9646\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1557 - acc: 0.9498\n",
      "Epoch 00030: val_loss did not improve from 0.12133\n",
      "36805/36805 [==============================] - 32s 868us/sample - loss: 0.1556 - acc: 0.9498 - val_loss: 0.1245 - val_acc: 0.9604\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1514 - acc: 0.9519\n",
      "Epoch 00031: val_loss improved from 0.12133 to 0.11130, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_7_conv_checkpoint/031-0.1113.hdf5\n",
      "36805/36805 [==============================] - 32s 868us/sample - loss: 0.1514 - acc: 0.9519 - val_loss: 0.1113 - val_acc: 0.9651\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1531 - acc: 0.9508\n",
      "Epoch 00032: val_loss did not improve from 0.11130\n",
      "36805/36805 [==============================] - 32s 864us/sample - loss: 0.1531 - acc: 0.9508 - val_loss: 0.1162 - val_acc: 0.9653\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1450 - acc: 0.9531\n",
      "Epoch 00033: val_loss did not improve from 0.11130\n",
      "36805/36805 [==============================] - 32s 867us/sample - loss: 0.1450 - acc: 0.9531 - val_loss: 0.1124 - val_acc: 0.9667\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1461 - acc: 0.9526\n",
      "Epoch 00034: val_loss did not improve from 0.11130\n",
      "36805/36805 [==============================] - 32s 867us/sample - loss: 0.1462 - acc: 0.9526 - val_loss: 0.1159 - val_acc: 0.9630\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1338 - acc: 0.9571\n",
      "Epoch 00035: val_loss improved from 0.11130 to 0.10676, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_7_conv_checkpoint/035-0.1068.hdf5\n",
      "36805/36805 [==============================] - 32s 867us/sample - loss: 0.1338 - acc: 0.9572 - val_loss: 0.1068 - val_acc: 0.9665\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1336 - acc: 0.9557\n",
      "Epoch 00036: val_loss did not improve from 0.10676\n",
      "36805/36805 [==============================] - 32s 868us/sample - loss: 0.1336 - acc: 0.9557 - val_loss: 0.1101 - val_acc: 0.9627\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1337 - acc: 0.9568\n",
      "Epoch 00037: val_loss did not improve from 0.10676\n",
      "36805/36805 [==============================] - 32s 866us/sample - loss: 0.1337 - acc: 0.9568 - val_loss: 0.1070 - val_acc: 0.9618\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1262 - acc: 0.9592\n",
      "Epoch 00038: val_loss did not improve from 0.10676\n",
      "36805/36805 [==============================] - 32s 860us/sample - loss: 0.1262 - acc: 0.9592 - val_loss: 0.1248 - val_acc: 0.9595\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1276 - acc: 0.9582\n",
      "Epoch 00039: val_loss did not improve from 0.10676\n",
      "36805/36805 [==============================] - 32s 866us/sample - loss: 0.1276 - acc: 0.9582 - val_loss: 0.1178 - val_acc: 0.9618\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1252 - acc: 0.9593\n",
      "Epoch 00040: val_loss did not improve from 0.10676\n",
      "36805/36805 [==============================] - 32s 865us/sample - loss: 0.1252 - acc: 0.9593 - val_loss: 0.1082 - val_acc: 0.9630\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1212 - acc: 0.9607\n",
      "Epoch 00041: val_loss did not improve from 0.10676\n",
      "36805/36805 [==============================] - 32s 863us/sample - loss: 0.1212 - acc: 0.9607 - val_loss: 0.1128 - val_acc: 0.9655\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1142 - acc: 0.9624\n",
      "Epoch 00042: val_loss improved from 0.10676 to 0.10043, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_7_conv_checkpoint/042-0.1004.hdf5\n",
      "36805/36805 [==============================] - 32s 869us/sample - loss: 0.1142 - acc: 0.9625 - val_loss: 0.1004 - val_acc: 0.9681\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1121 - acc: 0.9640\n",
      "Epoch 00043: val_loss did not improve from 0.10043\n",
      "36805/36805 [==============================] - 32s 865us/sample - loss: 0.1120 - acc: 0.9640 - val_loss: 0.1140 - val_acc: 0.9637\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1115 - acc: 0.9632\n",
      "Epoch 00044: val_loss did not improve from 0.10043\n",
      "36805/36805 [==============================] - 32s 866us/sample - loss: 0.1115 - acc: 0.9632 - val_loss: 0.1068 - val_acc: 0.9634\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1065 - acc: 0.9659\n",
      "Epoch 00045: val_loss did not improve from 0.10043\n",
      "36805/36805 [==============================] - 32s 862us/sample - loss: 0.1065 - acc: 0.9659 - val_loss: 0.1203 - val_acc: 0.9653\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1060 - acc: 0.9649\n",
      "Epoch 00046: val_loss did not improve from 0.10043\n",
      "36805/36805 [==============================] - 32s 862us/sample - loss: 0.1060 - acc: 0.9649 - val_loss: 0.1107 - val_acc: 0.9655\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1036 - acc: 0.9660\n",
      "Epoch 00047: val_loss did not improve from 0.10043\n",
      "36805/36805 [==============================] - 32s 861us/sample - loss: 0.1035 - acc: 0.9660 - val_loss: 0.1101 - val_acc: 0.9653\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0994 - acc: 0.9669\n",
      "Epoch 00048: val_loss did not improve from 0.10043\n",
      "36805/36805 [==============================] - 32s 863us/sample - loss: 0.0994 - acc: 0.9669 - val_loss: 0.1204 - val_acc: 0.9618\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0990 - acc: 0.9676\n",
      "Epoch 00049: val_loss did not improve from 0.10043\n",
      "36805/36805 [==============================] - 32s 871us/sample - loss: 0.0990 - acc: 0.9676 - val_loss: 0.1103 - val_acc: 0.9667\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0972 - acc: 0.9673\n",
      "Epoch 00050: val_loss did not improve from 0.10043\n",
      "36805/36805 [==============================] - 32s 861us/sample - loss: 0.0972 - acc: 0.9673 - val_loss: 0.1136 - val_acc: 0.9644\n",
      "Epoch 51/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0968 - acc: 0.9680\n",
      "Epoch 00051: val_loss did not improve from 0.10043\n",
      "36805/36805 [==============================] - 32s 860us/sample - loss: 0.0968 - acc: 0.9680 - val_loss: 0.1105 - val_acc: 0.9669\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0944 - acc: 0.9687\n",
      "Epoch 00052: val_loss did not improve from 0.10043\n",
      "36805/36805 [==============================] - 32s 864us/sample - loss: 0.0944 - acc: 0.9687 - val_loss: 0.1130 - val_acc: 0.9641\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0948 - acc: 0.9685\n",
      "Epoch 00053: val_loss did not improve from 0.10043\n",
      "36805/36805 [==============================] - 32s 863us/sample - loss: 0.0948 - acc: 0.9685 - val_loss: 0.1069 - val_acc: 0.9667\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0891 - acc: 0.9708\n",
      "Epoch 00054: val_loss did not improve from 0.10043\n",
      "36805/36805 [==============================] - 32s 865us/sample - loss: 0.0892 - acc: 0.9707 - val_loss: 0.1106 - val_acc: 0.9651\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0871 - acc: 0.9710\n",
      "Epoch 00055: val_loss did not improve from 0.10043\n",
      "36805/36805 [==============================] - 32s 861us/sample - loss: 0.0871 - acc: 0.9710 - val_loss: 0.1015 - val_acc: 0.9681\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0825 - acc: 0.9728\n",
      "Epoch 00056: val_loss did not improve from 0.10043\n",
      "36805/36805 [==============================] - 32s 867us/sample - loss: 0.0825 - acc: 0.9728 - val_loss: 0.1189 - val_acc: 0.9686\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0818 - acc: 0.9726\n",
      "Epoch 00057: val_loss did not improve from 0.10043\n",
      "36805/36805 [==============================] - 32s 866us/sample - loss: 0.0818 - acc: 0.9726 - val_loss: 0.1016 - val_acc: 0.9697\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0789 - acc: 0.9732\n",
      "Epoch 00058: val_loss did not improve from 0.10043\n",
      "36805/36805 [==============================] - 32s 867us/sample - loss: 0.0789 - acc: 0.9732 - val_loss: 0.1093 - val_acc: 0.9658\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0811 - acc: 0.9729\n",
      "Epoch 00059: val_loss did not improve from 0.10043\n",
      "36805/36805 [==============================] - 32s 865us/sample - loss: 0.0811 - acc: 0.9729 - val_loss: 0.1114 - val_acc: 0.9686\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0743 - acc: 0.9755\n",
      "Epoch 00060: val_loss did not improve from 0.10043\n",
      "36805/36805 [==============================] - 32s 872us/sample - loss: 0.0743 - acc: 0.9755 - val_loss: 0.1128 - val_acc: 0.9667\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0777 - acc: 0.9728\n",
      "Epoch 00061: val_loss did not improve from 0.10043\n",
      "36805/36805 [==============================] - 32s 867us/sample - loss: 0.0777 - acc: 0.9728 - val_loss: 0.1271 - val_acc: 0.9651\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0753 - acc: 0.9749\n",
      "Epoch 00062: val_loss did not improve from 0.10043\n",
      "36805/36805 [==============================] - 32s 870us/sample - loss: 0.0753 - acc: 0.9749 - val_loss: 0.1090 - val_acc: 0.9686\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0743 - acc: 0.9756\n",
      "Epoch 00063: val_loss did not improve from 0.10043\n",
      "36805/36805 [==============================] - 32s 865us/sample - loss: 0.0743 - acc: 0.9756 - val_loss: 0.1115 - val_acc: 0.9672\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0720 - acc: 0.9759\n",
      "Epoch 00064: val_loss did not improve from 0.10043\n",
      "36805/36805 [==============================] - 32s 866us/sample - loss: 0.0721 - acc: 0.9759 - val_loss: 0.1102 - val_acc: 0.9667\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0751 - acc: 0.9746\n",
      "Epoch 00065: val_loss did not improve from 0.10043\n",
      "36805/36805 [==============================] - 32s 866us/sample - loss: 0.0751 - acc: 0.9746 - val_loss: 0.1289 - val_acc: 0.9618\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0672 - acc: 0.9782\n",
      "Epoch 00066: val_loss did not improve from 0.10043\n",
      "36805/36805 [==============================] - 32s 867us/sample - loss: 0.0672 - acc: 0.9782 - val_loss: 0.1140 - val_acc: 0.9676\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0697 - acc: 0.9766\n",
      "Epoch 00067: val_loss did not improve from 0.10043\n",
      "36805/36805 [==============================] - 32s 864us/sample - loss: 0.0697 - acc: 0.9766 - val_loss: 0.1137 - val_acc: 0.9686\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0610 - acc: 0.9797\n",
      "Epoch 00068: val_loss did not improve from 0.10043\n",
      "36805/36805 [==============================] - 32s 864us/sample - loss: 0.0610 - acc: 0.9797 - val_loss: 0.1088 - val_acc: 0.9674\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0676 - acc: 0.9778\n",
      "Epoch 00069: val_loss did not improve from 0.10043\n",
      "36805/36805 [==============================] - 32s 862us/sample - loss: 0.0676 - acc: 0.9778 - val_loss: 0.1213 - val_acc: 0.9634\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0640 - acc: 0.9785\n",
      "Epoch 00070: val_loss did not improve from 0.10043\n",
      "36805/36805 [==============================] - 32s 862us/sample - loss: 0.0640 - acc: 0.9785 - val_loss: 0.1110 - val_acc: 0.9688\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0639 - acc: 0.9787\n",
      "Epoch 00071: val_loss did not improve from 0.10043\n",
      "36805/36805 [==============================] - 32s 864us/sample - loss: 0.0639 - acc: 0.9787 - val_loss: 0.1228 - val_acc: 0.9679\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0589 - acc: 0.9800\n",
      "Epoch 00072: val_loss did not improve from 0.10043\n",
      "36805/36805 [==============================] - 32s 863us/sample - loss: 0.0589 - acc: 0.9800 - val_loss: 0.1116 - val_acc: 0.9690\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0587 - acc: 0.9796\n",
      "Epoch 00073: val_loss did not improve from 0.10043\n",
      "36805/36805 [==============================] - 32s 869us/sample - loss: 0.0587 - acc: 0.9796 - val_loss: 0.1143 - val_acc: 0.9697\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0577 - acc: 0.9813\n",
      "Epoch 00074: val_loss did not improve from 0.10043\n",
      "36805/36805 [==============================] - 32s 868us/sample - loss: 0.0577 - acc: 0.9813 - val_loss: 0.1431 - val_acc: 0.9618\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0575 - acc: 0.9812\n",
      "Epoch 00075: val_loss did not improve from 0.10043\n",
      "36805/36805 [==============================] - 32s 862us/sample - loss: 0.0575 - acc: 0.9812 - val_loss: 0.1199 - val_acc: 0.9690\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0587 - acc: 0.9795\n",
      "Epoch 00076: val_loss did not improve from 0.10043\n",
      "36805/36805 [==============================] - 32s 865us/sample - loss: 0.0587 - acc: 0.9795 - val_loss: 0.1195 - val_acc: 0.9686\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0574 - acc: 0.9810\n",
      "Epoch 00077: val_loss did not improve from 0.10043\n",
      "36805/36805 [==============================] - 32s 868us/sample - loss: 0.0575 - acc: 0.9810 - val_loss: 0.1168 - val_acc: 0.9674\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0542 - acc: 0.9811\n",
      "Epoch 00078: val_loss did not improve from 0.10043\n",
      "36805/36805 [==============================] - 32s 866us/sample - loss: 0.0542 - acc: 0.9811 - val_loss: 0.1128 - val_acc: 0.9665\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0511 - acc: 0.9829\n",
      "Epoch 00079: val_loss did not improve from 0.10043\n",
      "36805/36805 [==============================] - 32s 864us/sample - loss: 0.0511 - acc: 0.9829 - val_loss: 0.1148 - val_acc: 0.9693\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0546 - acc: 0.9817\n",
      "Epoch 00080: val_loss did not improve from 0.10043\n",
      "36805/36805 [==============================] - 32s 867us/sample - loss: 0.0546 - acc: 0.9817 - val_loss: 0.1207 - val_acc: 0.9665\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0536 - acc: 0.9815\n",
      "Epoch 00081: val_loss did not improve from 0.10043\n",
      "36805/36805 [==============================] - 32s 864us/sample - loss: 0.0536 - acc: 0.9815 - val_loss: 0.1182 - val_acc: 0.9683\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0513 - acc: 0.9825\n",
      "Epoch 00082: val_loss did not improve from 0.10043\n",
      "36805/36805 [==============================] - 32s 868us/sample - loss: 0.0513 - acc: 0.9825 - val_loss: 0.1199 - val_acc: 0.9681\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0498 - acc: 0.9834\n",
      "Epoch 00083: val_loss did not improve from 0.10043\n",
      "36805/36805 [==============================] - 32s 866us/sample - loss: 0.0498 - acc: 0.9834 - val_loss: 0.1191 - val_acc: 0.9681\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0489 - acc: 0.9835\n",
      "Epoch 00084: val_loss did not improve from 0.10043\n",
      "36805/36805 [==============================] - 32s 864us/sample - loss: 0.0489 - acc: 0.9835 - val_loss: 0.1209 - val_acc: 0.9697\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0488 - acc: 0.9834\n",
      "Epoch 00085: val_loss did not improve from 0.10043\n",
      "36805/36805 [==============================] - 32s 863us/sample - loss: 0.0488 - acc: 0.9834 - val_loss: 0.1272 - val_acc: 0.9683\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0484 - acc: 0.9839\n",
      "Epoch 00086: val_loss did not improve from 0.10043\n",
      "36805/36805 [==============================] - 32s 866us/sample - loss: 0.0484 - acc: 0.9839 - val_loss: 0.1249 - val_acc: 0.9681\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0464 - acc: 0.9836\n",
      "Epoch 00087: val_loss did not improve from 0.10043\n",
      "36805/36805 [==============================] - 32s 861us/sample - loss: 0.0464 - acc: 0.9836 - val_loss: 0.1205 - val_acc: 0.9681\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0443 - acc: 0.9853\n",
      "Epoch 00088: val_loss did not improve from 0.10043\n",
      "36805/36805 [==============================] - 32s 866us/sample - loss: 0.0443 - acc: 0.9853 - val_loss: 0.1292 - val_acc: 0.9686\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0430 - acc: 0.9858\n",
      "Epoch 00089: val_loss did not improve from 0.10043\n",
      "36805/36805 [==============================] - 32s 864us/sample - loss: 0.0430 - acc: 0.9858 - val_loss: 0.1228 - val_acc: 0.9674\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0429 - acc: 0.9850\n",
      "Epoch 00090: val_loss did not improve from 0.10043\n",
      "36805/36805 [==============================] - 32s 862us/sample - loss: 0.0429 - acc: 0.9850 - val_loss: 0.1426 - val_acc: 0.9662\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0454 - acc: 0.9853\n",
      "Epoch 00091: val_loss did not improve from 0.10043\n",
      "36805/36805 [==============================] - 32s 863us/sample - loss: 0.0454 - acc: 0.9853 - val_loss: 0.1260 - val_acc: 0.9674\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0406 - acc: 0.9862\n",
      "Epoch 00092: val_loss did not improve from 0.10043\n",
      "36805/36805 [==============================] - 32s 864us/sample - loss: 0.0406 - acc: 0.9862 - val_loss: 0.1242 - val_acc: 0.9688\n",
      "\n",
      "1D_CNN_custom_multi_3_GMP_DO_7_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 482us/sample - loss: 0.1510 - acc: 0.9514\n",
      "Loss: 0.1509859473689199 Accuracy: 0.9514019\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9804 - acc: 0.3544\n",
      "Epoch 00001: val_loss improved from inf to 1.05770, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_8_conv_checkpoint/001-1.0577.hdf5\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 1.9803 - acc: 0.3544 - val_loss: 1.0577 - val_acc: 0.6897\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0832 - acc: 0.6508\n",
      "Epoch 00002: val_loss improved from 1.05770 to 0.67895, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_8_conv_checkpoint/002-0.6790.hdf5\n",
      "36805/36805 [==============================] - 33s 886us/sample - loss: 1.0833 - acc: 0.6507 - val_loss: 0.6790 - val_acc: 0.8057\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7715 - acc: 0.7511\n",
      "Epoch 00003: val_loss improved from 0.67895 to 0.49647, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_8_conv_checkpoint/003-0.4965.hdf5\n",
      "36805/36805 [==============================] - 33s 884us/sample - loss: 0.7714 - acc: 0.7511 - val_loss: 0.4965 - val_acc: 0.8467\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6046 - acc: 0.8073\n",
      "Epoch 00004: val_loss improved from 0.49647 to 0.39772, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_8_conv_checkpoint/004-0.3977.hdf5\n",
      "36805/36805 [==============================] - 33s 886us/sample - loss: 0.6046 - acc: 0.8073 - val_loss: 0.3977 - val_acc: 0.8789\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5084 - acc: 0.8360\n",
      "Epoch 00005: val_loss improved from 0.39772 to 0.31483, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_8_conv_checkpoint/005-0.3148.hdf5\n",
      "36805/36805 [==============================] - 33s 885us/sample - loss: 0.5084 - acc: 0.8359 - val_loss: 0.3148 - val_acc: 0.9122\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4306 - acc: 0.8621\n",
      "Epoch 00006: val_loss improved from 0.31483 to 0.27687, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_8_conv_checkpoint/006-0.2769.hdf5\n",
      "36805/36805 [==============================] - 33s 885us/sample - loss: 0.4306 - acc: 0.8622 - val_loss: 0.2769 - val_acc: 0.9175\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3894 - acc: 0.8760\n",
      "Epoch 00007: val_loss improved from 0.27687 to 0.22703, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_8_conv_checkpoint/007-0.2270.hdf5\n",
      "36805/36805 [==============================] - 33s 886us/sample - loss: 0.3893 - acc: 0.8760 - val_loss: 0.2270 - val_acc: 0.9322\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3511 - acc: 0.8912\n",
      "Epoch 00008: val_loss improved from 0.22703 to 0.22361, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_8_conv_checkpoint/008-0.2236.hdf5\n",
      "36805/36805 [==============================] - 33s 888us/sample - loss: 0.3510 - acc: 0.8912 - val_loss: 0.2236 - val_acc: 0.9336\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3137 - acc: 0.9013\n",
      "Epoch 00009: val_loss improved from 0.22361 to 0.20068, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_8_conv_checkpoint/009-0.2007.hdf5\n",
      "36805/36805 [==============================] - 33s 885us/sample - loss: 0.3137 - acc: 0.9013 - val_loss: 0.2007 - val_acc: 0.9380\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2885 - acc: 0.9086\n",
      "Epoch 00010: val_loss improved from 0.20068 to 0.17564, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_8_conv_checkpoint/010-0.1756.hdf5\n",
      "36805/36805 [==============================] - 33s 888us/sample - loss: 0.2884 - acc: 0.9086 - val_loss: 0.1756 - val_acc: 0.9457\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2676 - acc: 0.9155\n",
      "Epoch 00011: val_loss did not improve from 0.17564\n",
      "36805/36805 [==============================] - 32s 883us/sample - loss: 0.2676 - acc: 0.9155 - val_loss: 0.1977 - val_acc: 0.9394\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2518 - acc: 0.9197\n",
      "Epoch 00012: val_loss improved from 0.17564 to 0.17050, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_8_conv_checkpoint/012-0.1705.hdf5\n",
      "36805/36805 [==============================] - 32s 882us/sample - loss: 0.2518 - acc: 0.9197 - val_loss: 0.1705 - val_acc: 0.9469\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2331 - acc: 0.9253\n",
      "Epoch 00013: val_loss improved from 0.17050 to 0.15871, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_8_conv_checkpoint/013-0.1587.hdf5\n",
      "36805/36805 [==============================] - 33s 886us/sample - loss: 0.2331 - acc: 0.9253 - val_loss: 0.1587 - val_acc: 0.9511\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2213 - acc: 0.9294\n",
      "Epoch 00014: val_loss improved from 0.15871 to 0.15618, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_8_conv_checkpoint/014-0.1562.hdf5\n",
      "36805/36805 [==============================] - 33s 885us/sample - loss: 0.2213 - acc: 0.9294 - val_loss: 0.1562 - val_acc: 0.9495\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2062 - acc: 0.9331\n",
      "Epoch 00015: val_loss improved from 0.15618 to 0.14008, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_8_conv_checkpoint/015-0.1401.hdf5\n",
      "36805/36805 [==============================] - 33s 886us/sample - loss: 0.2062 - acc: 0.9331 - val_loss: 0.1401 - val_acc: 0.9581\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1941 - acc: 0.9379\n",
      "Epoch 00016: val_loss did not improve from 0.14008\n",
      "36805/36805 [==============================] - 32s 879us/sample - loss: 0.1941 - acc: 0.9379 - val_loss: 0.1405 - val_acc: 0.9569\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1840 - acc: 0.9411\n",
      "Epoch 00017: val_loss did not improve from 0.14008\n",
      "36805/36805 [==============================] - 32s 881us/sample - loss: 0.1840 - acc: 0.9411 - val_loss: 0.1419 - val_acc: 0.9571\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1755 - acc: 0.9443\n",
      "Epoch 00018: val_loss improved from 0.14008 to 0.12819, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_8_conv_checkpoint/018-0.1282.hdf5\n",
      "36805/36805 [==============================] - 32s 882us/sample - loss: 0.1755 - acc: 0.9443 - val_loss: 0.1282 - val_acc: 0.9599\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1709 - acc: 0.9451\n",
      "Epoch 00019: val_loss did not improve from 0.12819\n",
      "36805/36805 [==============================] - 32s 883us/sample - loss: 0.1709 - acc: 0.9451 - val_loss: 0.1465 - val_acc: 0.9525\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1661 - acc: 0.9469\n",
      "Epoch 00020: val_loss improved from 0.12819 to 0.12035, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_8_conv_checkpoint/020-0.1204.hdf5\n",
      "36805/36805 [==============================] - 33s 887us/sample - loss: 0.1660 - acc: 0.9469 - val_loss: 0.1204 - val_acc: 0.9609\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1518 - acc: 0.9497\n",
      "Epoch 00021: val_loss did not improve from 0.12035\n",
      "36805/36805 [==============================] - 32s 880us/sample - loss: 0.1518 - acc: 0.9497 - val_loss: 0.1259 - val_acc: 0.9606\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1462 - acc: 0.9529\n",
      "Epoch 00022: val_loss did not improve from 0.12035\n",
      "36805/36805 [==============================] - 33s 884us/sample - loss: 0.1462 - acc: 0.9529 - val_loss: 0.1249 - val_acc: 0.9627\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1389 - acc: 0.9552\n",
      "Epoch 00023: val_loss improved from 0.12035 to 0.11975, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_8_conv_checkpoint/023-0.1197.hdf5\n",
      "36805/36805 [==============================] - 32s 882us/sample - loss: 0.1390 - acc: 0.9552 - val_loss: 0.1197 - val_acc: 0.9646\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1371 - acc: 0.9548\n",
      "Epoch 00024: val_loss did not improve from 0.11975\n",
      "36805/36805 [==============================] - 32s 881us/sample - loss: 0.1371 - acc: 0.9548 - val_loss: 0.1205 - val_acc: 0.9618\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1304 - acc: 0.9575\n",
      "Epoch 00025: val_loss did not improve from 0.11975\n",
      "36805/36805 [==============================] - 32s 882us/sample - loss: 0.1304 - acc: 0.9575 - val_loss: 0.1218 - val_acc: 0.9625\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1245 - acc: 0.9588\n",
      "Epoch 00026: val_loss improved from 0.11975 to 0.11825, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_8_conv_checkpoint/026-0.1183.hdf5\n",
      "36805/36805 [==============================] - 32s 882us/sample - loss: 0.1245 - acc: 0.9588 - val_loss: 0.1183 - val_acc: 0.9627\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1173 - acc: 0.9617\n",
      "Epoch 00027: val_loss did not improve from 0.11825\n",
      "36805/36805 [==============================] - 32s 883us/sample - loss: 0.1174 - acc: 0.9617 - val_loss: 0.1201 - val_acc: 0.9620\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1128 - acc: 0.9639\n",
      "Epoch 00028: val_loss improved from 0.11825 to 0.11165, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_8_conv_checkpoint/028-0.1117.hdf5\n",
      "36805/36805 [==============================] - 32s 881us/sample - loss: 0.1128 - acc: 0.9639 - val_loss: 0.1117 - val_acc: 0.9644\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1106 - acc: 0.9640\n",
      "Epoch 00029: val_loss did not improve from 0.11165\n",
      "36805/36805 [==============================] - 33s 884us/sample - loss: 0.1106 - acc: 0.9640 - val_loss: 0.1168 - val_acc: 0.9641\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1069 - acc: 0.9649\n",
      "Epoch 00030: val_loss did not improve from 0.11165\n",
      "36805/36805 [==============================] - 32s 883us/sample - loss: 0.1068 - acc: 0.9649 - val_loss: 0.1170 - val_acc: 0.9655\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1033 - acc: 0.9660\n",
      "Epoch 00031: val_loss improved from 0.11165 to 0.11132, saving model to model/checkpoint/1D_CNN_custom_multi_3_GMP_DO_8_conv_checkpoint/031-0.1113.hdf5\n",
      "36805/36805 [==============================] - 33s 885us/sample - loss: 0.1033 - acc: 0.9660 - val_loss: 0.1113 - val_acc: 0.9653\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0955 - acc: 0.9683\n",
      "Epoch 00032: val_loss did not improve from 0.11132\n",
      "36805/36805 [==============================] - 32s 881us/sample - loss: 0.0955 - acc: 0.9683 - val_loss: 0.1161 - val_acc: 0.9662\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0952 - acc: 0.9682\n",
      "Epoch 00033: val_loss did not improve from 0.11132\n",
      "36805/36805 [==============================] - 32s 880us/sample - loss: 0.0952 - acc: 0.9682 - val_loss: 0.1298 - val_acc: 0.9609\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0881 - acc: 0.9703\n",
      "Epoch 00034: val_loss did not improve from 0.11132\n",
      "36805/36805 [==============================] - 32s 882us/sample - loss: 0.0881 - acc: 0.9703 - val_loss: 0.1359 - val_acc: 0.9583\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0849 - acc: 0.9705\n",
      "Epoch 00035: val_loss did not improve from 0.11132\n",
      "36805/36805 [==============================] - 32s 881us/sample - loss: 0.0849 - acc: 0.9705 - val_loss: 0.1151 - val_acc: 0.9653\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0834 - acc: 0.9713\n",
      "Epoch 00036: val_loss did not improve from 0.11132\n",
      "36805/36805 [==============================] - 32s 881us/sample - loss: 0.0834 - acc: 0.9713 - val_loss: 0.1211 - val_acc: 0.9627\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0837 - acc: 0.9720\n",
      "Epoch 00037: val_loss did not improve from 0.11132\n",
      "36805/36805 [==============================] - 32s 880us/sample - loss: 0.0837 - acc: 0.9720 - val_loss: 0.1161 - val_acc: 0.9669\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0755 - acc: 0.9743\n",
      "Epoch 00038: val_loss did not improve from 0.11132\n",
      "36805/36805 [==============================] - 32s 877us/sample - loss: 0.0754 - acc: 0.9743 - val_loss: 0.1170 - val_acc: 0.9655\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0755 - acc: 0.9743\n",
      "Epoch 00039: val_loss did not improve from 0.11132\n",
      "36805/36805 [==============================] - 32s 879us/sample - loss: 0.0755 - acc: 0.9743 - val_loss: 0.1465 - val_acc: 0.9567\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0747 - acc: 0.9742\n",
      "Epoch 00040: val_loss did not improve from 0.11132\n",
      "36805/36805 [==============================] - 32s 881us/sample - loss: 0.0747 - acc: 0.9742 - val_loss: 0.1293 - val_acc: 0.9613\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0694 - acc: 0.9771\n",
      "Epoch 00041: val_loss did not improve from 0.11132\n",
      "36805/36805 [==============================] - 32s 880us/sample - loss: 0.0694 - acc: 0.9771 - val_loss: 0.1195 - val_acc: 0.9639\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0690 - acc: 0.9776\n",
      "Epoch 00042: val_loss did not improve from 0.11132\n",
      "36805/36805 [==============================] - 32s 883us/sample - loss: 0.0690 - acc: 0.9776 - val_loss: 0.1274 - val_acc: 0.9658\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0631 - acc: 0.9788\n",
      "Epoch 00043: val_loss did not improve from 0.11132\n",
      "36805/36805 [==============================] - 32s 879us/sample - loss: 0.0631 - acc: 0.9788 - val_loss: 0.1337 - val_acc: 0.9632\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0656 - acc: 0.9773\n",
      "Epoch 00044: val_loss did not improve from 0.11132\n",
      "36805/36805 [==============================] - 32s 880us/sample - loss: 0.0656 - acc: 0.9773 - val_loss: 0.1147 - val_acc: 0.9693\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0608 - acc: 0.9796\n",
      "Epoch 00045: val_loss did not improve from 0.11132\n",
      "36805/36805 [==============================] - 32s 881us/sample - loss: 0.0608 - acc: 0.9796 - val_loss: 0.1267 - val_acc: 0.9648\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0600 - acc: 0.9796\n",
      "Epoch 00046: val_loss did not improve from 0.11132\n",
      "36805/36805 [==============================] - 32s 881us/sample - loss: 0.0600 - acc: 0.9796 - val_loss: 0.1168 - val_acc: 0.9711\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0532 - acc: 0.9815\n",
      "Epoch 00047: val_loss did not improve from 0.11132\n",
      "36805/36805 [==============================] - 32s 881us/sample - loss: 0.0532 - acc: 0.9815 - val_loss: 0.1374 - val_acc: 0.9625\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0551 - acc: 0.9814\n",
      "Epoch 00048: val_loss did not improve from 0.11132\n",
      "36805/36805 [==============================] - 32s 878us/sample - loss: 0.0551 - acc: 0.9814 - val_loss: 0.1215 - val_acc: 0.9655\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0522 - acc: 0.9825\n",
      "Epoch 00049: val_loss did not improve from 0.11132\n",
      "36805/36805 [==============================] - 32s 881us/sample - loss: 0.0523 - acc: 0.9825 - val_loss: 0.1309 - val_acc: 0.9641\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0500 - acc: 0.9832\n",
      "Epoch 00050: val_loss did not improve from 0.11132\n",
      "36805/36805 [==============================] - 32s 882us/sample - loss: 0.0499 - acc: 0.9832 - val_loss: 0.1336 - val_acc: 0.9651\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0515 - acc: 0.9828\n",
      "Epoch 00051: val_loss did not improve from 0.11132\n",
      "36805/36805 [==============================] - 33s 884us/sample - loss: 0.0515 - acc: 0.9828 - val_loss: 0.1280 - val_acc: 0.9641\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0483 - acc: 0.9833\n",
      "Epoch 00052: val_loss did not improve from 0.11132\n",
      "36805/36805 [==============================] - 32s 880us/sample - loss: 0.0482 - acc: 0.9833 - val_loss: 0.1391 - val_acc: 0.9669\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0455 - acc: 0.9845\n",
      "Epoch 00053: val_loss did not improve from 0.11132\n",
      "36805/36805 [==============================] - 32s 882us/sample - loss: 0.0455 - acc: 0.9845 - val_loss: 0.1314 - val_acc: 0.9655\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0455 - acc: 0.9848\n",
      "Epoch 00054: val_loss did not improve from 0.11132\n",
      "36805/36805 [==============================] - 32s 880us/sample - loss: 0.0454 - acc: 0.9848 - val_loss: 0.1318 - val_acc: 0.9658\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0446 - acc: 0.9850\n",
      "Epoch 00055: val_loss did not improve from 0.11132\n",
      "36805/36805 [==============================] - 32s 883us/sample - loss: 0.0446 - acc: 0.9850 - val_loss: 0.1371 - val_acc: 0.9665\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0430 - acc: 0.9851\n",
      "Epoch 00056: val_loss did not improve from 0.11132\n",
      "36805/36805 [==============================] - 32s 881us/sample - loss: 0.0430 - acc: 0.9851 - val_loss: 0.1289 - val_acc: 0.9697\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0409 - acc: 0.9856\n",
      "Epoch 00057: val_loss did not improve from 0.11132\n",
      "36805/36805 [==============================] - 32s 880us/sample - loss: 0.0410 - acc: 0.9855 - val_loss: 0.1579 - val_acc: 0.9625\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0482 - acc: 0.9848\n",
      "Epoch 00058: val_loss did not improve from 0.11132\n",
      "36805/36805 [==============================] - 32s 880us/sample - loss: 0.0482 - acc: 0.9848 - val_loss: 0.1317 - val_acc: 0.9674\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0366 - acc: 0.9878\n",
      "Epoch 00059: val_loss did not improve from 0.11132\n",
      "36805/36805 [==============================] - 32s 879us/sample - loss: 0.0366 - acc: 0.9878 - val_loss: 0.1406 - val_acc: 0.9662\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0356 - acc: 0.9885\n",
      "Epoch 00060: val_loss did not improve from 0.11132\n",
      "36805/36805 [==============================] - 32s 880us/sample - loss: 0.0356 - acc: 0.9885 - val_loss: 0.1391 - val_acc: 0.9686\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0393 - acc: 0.9866\n",
      "Epoch 00061: val_loss did not improve from 0.11132\n",
      "36805/36805 [==============================] - 32s 879us/sample - loss: 0.0393 - acc: 0.9866 - val_loss: 0.1467 - val_acc: 0.9655\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0367 - acc: 0.9878\n",
      "Epoch 00062: val_loss did not improve from 0.11132\n",
      "36805/36805 [==============================] - 32s 877us/sample - loss: 0.0367 - acc: 0.9878 - val_loss: 0.1474 - val_acc: 0.9695\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0347 - acc: 0.9882\n",
      "Epoch 00063: val_loss did not improve from 0.11132\n",
      "36805/36805 [==============================] - 32s 881us/sample - loss: 0.0348 - acc: 0.9882 - val_loss: 0.1345 - val_acc: 0.9711\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0369 - acc: 0.9878\n",
      "Epoch 00064: val_loss did not improve from 0.11132\n",
      "36805/36805 [==============================] - 32s 878us/sample - loss: 0.0369 - acc: 0.9878 - val_loss: 0.1530 - val_acc: 0.9669\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0323 - acc: 0.9890\n",
      "Epoch 00065: val_loss did not improve from 0.11132\n",
      "36805/36805 [==============================] - 32s 878us/sample - loss: 0.0323 - acc: 0.9890 - val_loss: 0.1290 - val_acc: 0.9674\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0326 - acc: 0.9891\n",
      "Epoch 00066: val_loss did not improve from 0.11132\n",
      "36805/36805 [==============================] - 32s 881us/sample - loss: 0.0326 - acc: 0.9891 - val_loss: 0.1507 - val_acc: 0.9658\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0312 - acc: 0.9896\n",
      "Epoch 00067: val_loss did not improve from 0.11132\n",
      "36805/36805 [==============================] - 32s 877us/sample - loss: 0.0312 - acc: 0.9896 - val_loss: 0.1561 - val_acc: 0.9655\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0312 - acc: 0.9899\n",
      "Epoch 00068: val_loss did not improve from 0.11132\n",
      "36805/36805 [==============================] - 32s 877us/sample - loss: 0.0312 - acc: 0.9899 - val_loss: 0.1544 - val_acc: 0.9651\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0288 - acc: 0.9902\n",
      "Epoch 00069: val_loss did not improve from 0.11132\n",
      "36805/36805 [==============================] - 32s 875us/sample - loss: 0.0288 - acc: 0.9902 - val_loss: 0.1469 - val_acc: 0.9672\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0291 - acc: 0.9902\n",
      "Epoch 00070: val_loss did not improve from 0.11132\n",
      "36805/36805 [==============================] - 32s 876us/sample - loss: 0.0291 - acc: 0.9902 - val_loss: 0.1433 - val_acc: 0.9667\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0282 - acc: 0.9908\n",
      "Epoch 00071: val_loss did not improve from 0.11132\n",
      "36805/36805 [==============================] - 32s 880us/sample - loss: 0.0282 - acc: 0.9908 - val_loss: 0.1471 - val_acc: 0.9674\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0268 - acc: 0.9911\n",
      "Epoch 00072: val_loss did not improve from 0.11132\n",
      "36805/36805 [==============================] - 32s 877us/sample - loss: 0.0268 - acc: 0.9911 - val_loss: 0.1542 - val_acc: 0.9644\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0303 - acc: 0.9905\n",
      "Epoch 00073: val_loss did not improve from 0.11132\n",
      "36805/36805 [==============================] - 32s 881us/sample - loss: 0.0303 - acc: 0.9905 - val_loss: 0.1525 - val_acc: 0.9655\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0265 - acc: 0.9913\n",
      "Epoch 00074: val_loss did not improve from 0.11132\n",
      "36805/36805 [==============================] - 32s 878us/sample - loss: 0.0265 - acc: 0.9913 - val_loss: 0.1627 - val_acc: 0.9651\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0240 - acc: 0.9925\n",
      "Epoch 00075: val_loss did not improve from 0.11132\n",
      "36805/36805 [==============================] - 32s 879us/sample - loss: 0.0240 - acc: 0.9925 - val_loss: 0.1727 - val_acc: 0.9669\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0282 - acc: 0.9906\n",
      "Epoch 00076: val_loss did not improve from 0.11132\n",
      "36805/36805 [==============================] - 32s 875us/sample - loss: 0.0282 - acc: 0.9906 - val_loss: 0.1666 - val_acc: 0.9662\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0242 - acc: 0.9922\n",
      "Epoch 00077: val_loss did not improve from 0.11132\n",
      "36805/36805 [==============================] - 32s 881us/sample - loss: 0.0242 - acc: 0.9922 - val_loss: 0.1819 - val_acc: 0.9662\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0220 - acc: 0.9931\n",
      "Epoch 00078: val_loss did not improve from 0.11132\n",
      "36805/36805 [==============================] - 32s 879us/sample - loss: 0.0220 - acc: 0.9931 - val_loss: 0.1685 - val_acc: 0.9681\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0250 - acc: 0.9914\n",
      "Epoch 00079: val_loss did not improve from 0.11132\n",
      "36805/36805 [==============================] - 32s 881us/sample - loss: 0.0250 - acc: 0.9914 - val_loss: 0.1638 - val_acc: 0.9686\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0235 - acc: 0.9929\n",
      "Epoch 00080: val_loss did not improve from 0.11132\n",
      "36805/36805 [==============================] - 32s 875us/sample - loss: 0.0234 - acc: 0.9929 - val_loss: 0.1699 - val_acc: 0.9651\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0255 - acc: 0.9918\n",
      "Epoch 00081: val_loss did not improve from 0.11132\n",
      "36805/36805 [==============================] - 32s 876us/sample - loss: 0.0255 - acc: 0.9918 - val_loss: 0.1743 - val_acc: 0.9679\n",
      "\n",
      "1D_CNN_custom_multi_3_GMP_DO_8_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 472us/sample - loss: 0.1654 - acc: 0.9491\n",
      "Loss: 0.16542160726101104 Accuracy: 0.94911736\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base = '1D_CNN_custom_multi_3_GMP_DO'\n",
    "\n",
    "for i in range(3, 9):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    model = build_cnn(conv_num=i)\n",
    "#         model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4),\n",
    "          metrics=['accuracy'])\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model_filename = model_path+'{epoch:03d}-{val_loss:.4f}.hdf5'\n",
    "    checkpointer = ModelCheckpoint(filepath = model_filename, monitor = \"val_loss\", \n",
    "                                   verbose=1, save_best_only=True)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=50)\n",
    "    hist = model.fit(x_train_abs, y_train_onehot, batch_size=64, epochs=500, \n",
    "                     validation_data=[x_val_abs, y_val_onehot], shuffle=True, \n",
    "                     callbacks = [checkpointer, early_stopping])\n",
    "\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "    ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "    ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "    ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    png_path = 'visualization/learning_curve/'\n",
    "    filename = model_name+'.png'\n",
    "    os.makedirs(png_path, exist_ok=True)\n",
    "    fig.savefig(png_path+filename, transparent=True)\n",
    "\n",
    "    model.save(model_path+'000_last.hdf5')\n",
    "    del(model)\n",
    "    \n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "    model = load_model(model_filename)\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "    print()\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_multi_3_GMP_DO_3_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_33 (Conv1D)              (None, 16000, 64)    384         input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 16000, 64)    0           conv1d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling1D) (None, 5333, 64)     0           activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_34 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_33[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 5333, 64)     0           conv1d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling1D) (None, 1777, 64)     0           activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_35 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 1777, 64)     0           conv1d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling1D) (None, 592, 64)      0           activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_18 (Global (None, 64)           0           max_pooling1d_33[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_19 (Global (None, 64)           0           max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_20 (Global (None, 64)           0           max_pooling1d_35[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 192)          0           global_max_pooling1d_18[0][0]    \n",
      "                                                                 global_max_pooling1d_19[0][0]    \n",
      "                                                                 global_max_pooling1d_20[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 192)          0           concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 16)           3088        dropout_6[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 44,560\n",
      "Trainable params: 44,560\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 2s 421us/sample - loss: 0.6007 - acc: 0.8214\n",
      "Loss: 0.6007182153462126 Accuracy: 0.82139146\n",
      "\n",
      "1D_CNN_custom_multi_3_GMP_DO_4_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_8 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_36 (Conv1D)              (None, 16000, 64)    384         input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 16000, 64)    0           conv1d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling1D) (None, 5333, 64)     0           activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_37 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_36[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 5333, 64)     0           conv1d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling1D) (None, 1777, 64)     0           activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_38 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_37[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 1777, 64)     0           conv1d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling1D) (None, 592, 64)      0           activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_39 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 592, 64)      0           conv1d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling1D) (None, 197, 64)      0           activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_21 (Global (None, 64)           0           max_pooling1d_37[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_22 (Global (None, 64)           0           max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_23 (Global (None, 64)           0           max_pooling1d_39[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 192)          0           global_max_pooling1d_21[0][0]    \n",
      "                                                                 global_max_pooling1d_22[0][0]    \n",
      "                                                                 global_max_pooling1d_23[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 192)          0           concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 16)           3088        dropout_7[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 65,104\n",
      "Trainable params: 65,104\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 454us/sample - loss: 0.4084 - acc: 0.8768\n",
      "Loss: 0.40840566355119984 Accuracy: 0.8768432\n",
      "\n",
      "1D_CNN_custom_multi_3_GMP_DO_5_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_40 (Conv1D)              (None, 16000, 64)    384         input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 16000, 64)    0           conv1d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling1D) (None, 5333, 64)     0           activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_41 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_40[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 5333, 64)     0           conv1d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling1D) (None, 1777, 64)     0           activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_42 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_41[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 1777, 64)     0           conv1d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling1D) (None, 592, 64)      0           activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_43 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_42[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 592, 64)      0           conv1d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling1D) (None, 197, 64)      0           activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_44 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 197, 128)     0           conv1d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling1D) (None, 65, 128)      0           activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_24 (Global (None, 64)           0           max_pooling1d_42[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_25 (Global (None, 64)           0           max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_26 (Global (None, 128)          0           max_pooling1d_44[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 256)          0           global_max_pooling1d_24[0][0]    \n",
      "                                                                 global_max_pooling1d_25[0][0]    \n",
      "                                                                 global_max_pooling1d_26[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 256)          0           concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 16)           4112        dropout_8[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 107,216\n",
      "Trainable params: 107,216\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 2s 491us/sample - loss: 0.2305 - acc: 0.9294\n",
      "Loss: 0.2305163688377428 Accuracy: 0.92938733\n",
      "\n",
      "1D_CNN_custom_multi_3_GMP_DO_6_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_10 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_45 (Conv1D)              (None, 16000, 64)    384         input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 16000, 64)    0           conv1d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling1D) (None, 5333, 64)     0           activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_46 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_45[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 5333, 64)     0           conv1d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling1D) (None, 1777, 64)     0           activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_47 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_46[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 1777, 64)     0           conv1d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling1D) (None, 592, 64)      0           activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_48 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_47[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 592, 64)      0           conv1d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling1D) (None, 197, 64)      0           activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_49 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_48[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 197, 128)     0           conv1d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling1D) (None, 65, 128)      0           activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_50 (Conv1D)              (None, 65, 128)      82048       max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 65, 128)      0           conv1d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling1D) (None, 21, 128)      0           activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_27 (Global (None, 64)           0           max_pooling1d_48[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_28 (Global (None, 128)          0           max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_29 (Global (None, 128)          0           max_pooling1d_50[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 320)          0           global_max_pooling1d_27[0][0]    \n",
      "                                                                 global_max_pooling1d_28[0][0]    \n",
      "                                                                 global_max_pooling1d_29[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 320)          0           concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 16)           5136        dropout_9[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 190,288\n",
      "Trainable params: 190,288\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 487us/sample - loss: 0.1525 - acc: 0.9537\n",
      "Loss: 0.15245757239289498 Accuracy: 0.9536864\n",
      "\n",
      "1D_CNN_custom_multi_3_GMP_DO_7_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_51 (Conv1D)              (None, 16000, 64)    384         input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 16000, 64)    0           conv1d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling1D) (None, 5333, 64)     0           activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_52 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_51[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 5333, 64)     0           conv1d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling1D) (None, 1777, 64)     0           activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_53 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_52[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 1777, 64)     0           conv1d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling1D) (None, 592, 64)      0           activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_54 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_53[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 592, 64)      0           conv1d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling1D) (None, 197, 64)      0           activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_55 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_54[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 197, 128)     0           conv1d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling1D) (None, 65, 128)      0           activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_56 (Conv1D)              (None, 65, 128)      82048       max_pooling1d_55[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 65, 128)      0           conv1d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling1D) (None, 21, 128)      0           activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_57 (Conv1D)              (None, 21, 128)      82048       max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 21, 128)      0           conv1d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling1D) (None, 7, 128)       0           activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_30 (Global (None, 128)          0           max_pooling1d_55[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_31 (Global (None, 128)          0           max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_32 (Global (None, 128)          0           max_pooling1d_57[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 384)          0           global_max_pooling1d_30[0][0]    \n",
      "                                                                 global_max_pooling1d_31[0][0]    \n",
      "                                                                 global_max_pooling1d_32[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 384)          0           concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 16)           6160        dropout_10[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 273,360\n",
      "Trainable params: 273,360\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 2s 498us/sample - loss: 0.1510 - acc: 0.9514\n",
      "Loss: 0.1509859473689199 Accuracy: 0.9514019\n",
      "\n",
      "1D_CNN_custom_multi_3_GMP_DO_8_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_12 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_58 (Conv1D)              (None, 16000, 64)    384         input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 16000, 64)    0           conv1d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling1D) (None, 5333, 64)     0           activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_59 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_58[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 5333, 64)     0           conv1d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling1D) (None, 1777, 64)     0           activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_60 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_59[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 1777, 64)     0           conv1d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling1D) (None, 592, 64)      0           activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_61 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_60[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 592, 64)      0           conv1d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling1D) (None, 197, 64)      0           activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_62 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_61[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 197, 128)     0           conv1d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling1D) (None, 65, 128)      0           activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_63 (Conv1D)              (None, 65, 128)      82048       max_pooling1d_62[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 65, 128)      0           conv1d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling1D) (None, 21, 128)      0           activation_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_64 (Conv1D)              (None, 21, 128)      82048       max_pooling1d_63[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 21, 128)      0           conv1d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling1D) (None, 7, 128)       0           activation_64[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_65 (Conv1D)              (None, 7, 128)       82048       max_pooling1d_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 7, 128)       0           conv1d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling1D) (None, 2, 128)       0           activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_33 (Global (None, 128)          0           max_pooling1d_63[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_34 (Global (None, 128)          0           max_pooling1d_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_35 (Global (None, 128)          0           max_pooling1d_65[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 384)          0           global_max_pooling1d_33[0][0]    \n",
      "                                                                 global_max_pooling1d_34[0][0]    \n",
      "                                                                 global_max_pooling1d_35[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 384)          0           concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 16)           6160        dropout_11[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 355,408\n",
      "Trainable params: 355,408\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 517us/sample - loss: 0.1654 - acc: 0.9491\n",
      "Loss: 0.16542160726101104 Accuracy: 0.94911736\n"
     ]
    }
   ],
   "source": [
    "log_dir = 'log'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "base = '1D_CNN_custom_multi_3_GMP_DO'\n",
    "\n",
    "with open(path.join(log_dir, base), 'w') as log_file:\n",
    "    for i in range(3, 9):\n",
    "        model_name = base+'_{}_conv'.format(i)\n",
    "        print()\n",
    "        print(model_name, 'Model')\n",
    "        model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "        model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "\n",
    "        model = load_model(model_filename)\n",
    "        model.summary()\n",
    "\n",
    "        [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "        print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "        del(model)\n",
    "\n",
    "        log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_multi_3_GMP_DO_3_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_33 (Conv1D)              (None, 16000, 64)    384         input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 16000, 64)    0           conv1d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling1D) (None, 5333, 64)     0           activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_34 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_33[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 5333, 64)     0           conv1d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling1D) (None, 1777, 64)     0           activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_35 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 1777, 64)     0           conv1d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling1D) (None, 592, 64)      0           activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_18 (Global (None, 64)           0           max_pooling1d_33[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_19 (Global (None, 64)           0           max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_20 (Global (None, 64)           0           max_pooling1d_35[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 192)          0           global_max_pooling1d_18[0][0]    \n",
      "                                                                 global_max_pooling1d_19[0][0]    \n",
      "                                                                 global_max_pooling1d_20[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 192)          0           concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 16)           3088        dropout_6[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 44,560\n",
      "Trainable params: 44,560\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 2s 498us/sample - loss: 0.6006 - acc: 0.8247\n",
      "Loss: 0.6006029593486528 Accuracy: 0.8247144\n",
      "\n",
      "1D_CNN_custom_multi_3_GMP_DO_4_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_8 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_36 (Conv1D)              (None, 16000, 64)    384         input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 16000, 64)    0           conv1d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling1D) (None, 5333, 64)     0           activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_37 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_36[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 5333, 64)     0           conv1d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling1D) (None, 1777, 64)     0           activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_38 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_37[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 1777, 64)     0           conv1d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling1D) (None, 592, 64)      0           activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_39 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 592, 64)      0           conv1d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling1D) (None, 197, 64)      0           activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_21 (Global (None, 64)           0           max_pooling1d_37[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_22 (Global (None, 64)           0           max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_23 (Global (None, 64)           0           max_pooling1d_39[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 192)          0           global_max_pooling1d_21[0][0]    \n",
      "                                                                 global_max_pooling1d_22[0][0]    \n",
      "                                                                 global_max_pooling1d_23[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 192)          0           concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 16)           3088        dropout_7[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 65,104\n",
      "Trainable params: 65,104\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 500us/sample - loss: 0.4232 - acc: 0.8739\n",
      "Loss: 0.4232453409385087 Accuracy: 0.87393564\n",
      "\n",
      "1D_CNN_custom_multi_3_GMP_DO_5_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_40 (Conv1D)              (None, 16000, 64)    384         input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 16000, 64)    0           conv1d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling1D) (None, 5333, 64)     0           activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_41 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_40[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 5333, 64)     0           conv1d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling1D) (None, 1777, 64)     0           activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_42 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_41[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 1777, 64)     0           conv1d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling1D) (None, 592, 64)      0           activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_43 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_42[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 592, 64)      0           conv1d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling1D) (None, 197, 64)      0           activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_44 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 197, 128)     0           conv1d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling1D) (None, 65, 128)      0           activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_24 (Global (None, 64)           0           max_pooling1d_42[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_25 (Global (None, 64)           0           max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_26 (Global (None, 128)          0           max_pooling1d_44[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 256)          0           global_max_pooling1d_24[0][0]    \n",
      "                                                                 global_max_pooling1d_25[0][0]    \n",
      "                                                                 global_max_pooling1d_26[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 256)          0           concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 16)           4112        dropout_8[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 107,216\n",
      "Trainable params: 107,216\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 526us/sample - loss: 0.2735 - acc: 0.9211\n",
      "Loss: 0.2734724458256738 Accuracy: 0.92107993\n",
      "\n",
      "1D_CNN_custom_multi_3_GMP_DO_6_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_10 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_45 (Conv1D)              (None, 16000, 64)    384         input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 16000, 64)    0           conv1d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling1D) (None, 5333, 64)     0           activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_46 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_45[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 5333, 64)     0           conv1d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling1D) (None, 1777, 64)     0           activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_47 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_46[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 1777, 64)     0           conv1d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling1D) (None, 592, 64)      0           activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_48 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_47[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 592, 64)      0           conv1d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling1D) (None, 197, 64)      0           activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_49 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_48[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 197, 128)     0           conv1d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling1D) (None, 65, 128)      0           activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_50 (Conv1D)              (None, 65, 128)      82048       max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 65, 128)      0           conv1d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling1D) (None, 21, 128)      0           activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_27 (Global (None, 64)           0           max_pooling1d_48[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_28 (Global (None, 128)          0           max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_29 (Global (None, 128)          0           max_pooling1d_50[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 320)          0           global_max_pooling1d_27[0][0]    \n",
      "                                                                 global_max_pooling1d_28[0][0]    \n",
      "                                                                 global_max_pooling1d_29[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 320)          0           concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 16)           5136        dropout_9[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 190,288\n",
      "Trainable params: 190,288\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 538us/sample - loss: 0.1706 - acc: 0.9541\n",
      "Loss: 0.1706164086533484 Accuracy: 0.95410174\n",
      "\n",
      "1D_CNN_custom_multi_3_GMP_DO_7_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_51 (Conv1D)              (None, 16000, 64)    384         input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 16000, 64)    0           conv1d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling1D) (None, 5333, 64)     0           activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_52 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_51[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 5333, 64)     0           conv1d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling1D) (None, 1777, 64)     0           activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_53 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_52[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 1777, 64)     0           conv1d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling1D) (None, 592, 64)      0           activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_54 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_53[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 592, 64)      0           conv1d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling1D) (None, 197, 64)      0           activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_55 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_54[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 197, 128)     0           conv1d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling1D) (None, 65, 128)      0           activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_56 (Conv1D)              (None, 65, 128)      82048       max_pooling1d_55[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 65, 128)      0           conv1d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling1D) (None, 21, 128)      0           activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_57 (Conv1D)              (None, 21, 128)      82048       max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 21, 128)      0           conv1d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling1D) (None, 7, 128)       0           activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_30 (Global (None, 128)          0           max_pooling1d_55[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_31 (Global (None, 128)          0           max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_32 (Global (None, 128)          0           max_pooling1d_57[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 384)          0           global_max_pooling1d_30[0][0]    \n",
      "                                                                 global_max_pooling1d_31[0][0]    \n",
      "                                                                 global_max_pooling1d_32[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 384)          0           concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 16)           6160        dropout_10[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 273,360\n",
      "Trainable params: 273,360\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 552us/sample - loss: 0.1814 - acc: 0.9535\n",
      "Loss: 0.18142219788045744 Accuracy: 0.9534787\n",
      "\n",
      "1D_CNN_custom_multi_3_GMP_DO_8_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_12 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_58 (Conv1D)              (None, 16000, 64)    384         input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 16000, 64)    0           conv1d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling1D) (None, 5333, 64)     0           activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_59 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_58[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 5333, 64)     0           conv1d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling1D) (None, 1777, 64)     0           activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_60 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_59[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 1777, 64)     0           conv1d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling1D) (None, 592, 64)      0           activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_61 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_60[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 592, 64)      0           conv1d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling1D) (None, 197, 64)      0           activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_62 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_61[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 197, 128)     0           conv1d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling1D) (None, 65, 128)      0           activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_63 (Conv1D)              (None, 65, 128)      82048       max_pooling1d_62[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 65, 128)      0           conv1d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling1D) (None, 21, 128)      0           activation_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_64 (Conv1D)              (None, 21, 128)      82048       max_pooling1d_63[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 21, 128)      0           conv1d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling1D) (None, 7, 128)       0           activation_64[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_65 (Conv1D)              (None, 7, 128)       82048       max_pooling1d_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 7, 128)       0           conv1d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling1D) (None, 2, 128)       0           activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_33 (Global (None, 128)          0           max_pooling1d_63[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_34 (Global (None, 128)          0           max_pooling1d_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_35 (Global (None, 128)          0           max_pooling1d_65[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 384)          0           global_max_pooling1d_33[0][0]    \n",
      "                                                                 global_max_pooling1d_34[0][0]    \n",
      "                                                                 global_max_pooling1d_35[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 384)          0           concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 16)           6160        dropout_11[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 355,408\n",
      "Trainable params: 355,408\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 574us/sample - loss: 0.2334 - acc: 0.9522\n",
      "Loss: 0.23342520691474236 Accuracy: 0.9522326\n"
     ]
    }
   ],
   "source": [
    "# log_dir = 'log'\n",
    "# os.makedirs(log_dir, exist_ok=True)\n",
    "# base = '1D_CNN_custom_DO'\n",
    "\n",
    "# with open(path.join(log_dir, base), 'w') as log_file:\n",
    "for i in range(3, 9):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + '000_last.hdf5'\n",
    "\n",
    "    model = load_model(model_filename)\n",
    "    model.summary()\n",
    "\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "    del(model)\n",
    "\n",
    "#         log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
