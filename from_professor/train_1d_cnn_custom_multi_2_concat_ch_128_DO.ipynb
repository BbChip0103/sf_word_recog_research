{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path as path\n",
    "import itertools\n",
    "from sklearn.preprocessing import maxabs_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Input, Dense, Activation, BatchNormalization, \\\n",
    "                                    Flatten, Conv1D, MaxPooling1D, Dropout, \\\n",
    "                                    Concatenate, GlobalMaxPool1D, GlobalAvgPool1D\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler, \\\n",
    "                                        EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = path.join('..', 'data')\n",
    "data_dir = path.join(base_dir, 'data_speech_commands_v0.02')\n",
    " \n",
    "train_txt = path.join(data_dir, 'wav_train_16words.txt')\n",
    "val_txt = path.join(data_dir, 'wav_validation_16words.txt')\n",
    "test_txt = path.join(data_dir, 'wav_test_16words.txt')\n",
    "\n",
    "train_data = np.load(path.join(data_dir, 'wav_train_data.npz'))\n",
    "val_data = np.load(path.join(data_dir, 'wav_validation_data.npz'))\n",
    "test_data = np.load(path.join(data_dir, 'wav_test_data.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36805, 16000, 1),\n",
       " (36805,),\n",
       " (4293, 16000, 1),\n",
       " (4293,),\n",
       " (4815, 16000, 1),\n",
       " (4815,),\n",
       " (16, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = train_data['x_train']\n",
    "y_train = train_data['y_train']\n",
    "x_val = val_data['x_val']\n",
    "y_val = val_data['y_val']\n",
    "x_test = test_data['x_test']\n",
    "y_test = test_data['y_test']\n",
    "y_table = test_data['table']\n",
    "\n",
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape, y_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_test[0].shape\n",
    "output_size = y_table.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_abs = np.asarray([maxabs_scale(wav) for wav in x_train])\n",
    "y_train_onehot = np.asarray([to_categorical(label, output_size) for label in y_train])\n",
    "del x_train, y_train\n",
    "\n",
    "x_val_abs = np.asarray([maxabs_scale(wav) for wav in x_val])\n",
    "y_val_onehot = np.asarray([to_categorical(label, output_size) for label in y_val])\n",
    "del x_val, y_val\n",
    "\n",
    "x_test_abs = np.asarray([maxabs_scale(wav) for wav in x_test])\n",
    "y_test_onehot = np.asarray([to_categorical(label, output_size) for label in y_test])\n",
    "del x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn(conv_num=1):\n",
    "    filter_size = 128\n",
    "\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    x = input_layer\n",
    "\n",
    "    layer_outputs = []\n",
    "    for i in range(conv_num):\n",
    "        x = Conv1D (kernel_size=5, filters=filter_size*(2**(i//4)), \n",
    "                          strides=1, padding='same')(x)\n",
    "        x = Activation('relu')(x)\n",
    "        x = MaxPooling1D(pool_size=3, strides=3)(x)\n",
    "        layer_outputs.append(x)    \n",
    "    \n",
    "    x = Concatenate()([Flatten()(output) for output in layer_outputs[-2:]])\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(output_size, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 16000, 128)   768         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 16000, 128)   0           conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D)    (None, 5333, 128)    0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 5333, 128)    82048       max_pooling1d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 5333, 128)    0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 1777, 128)    0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 1777, 128)    82048       max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 1777, 128)    0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 592, 128)     0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 227456)       0           max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 75776)        0           max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 303232)       0           flatten[0][0]                    \n",
      "                                                                 flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 303232)       0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 16)           4851728     dropout[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 5,016,592\n",
      "Trainable params: 5,016,592\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 16000, 128)   768         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 16000, 128)   0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 5333, 128)    0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 5333, 128)    82048       max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 5333, 128)    0           conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 1777, 128)    0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 1777, 128)    82048       max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 1777, 128)    0           conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 592, 128)     0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 592, 128)     82048       max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 592, 128)     0           conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1D)  (None, 197, 128)     0           activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 75776)        0           max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 25216)        0           max_pooling1d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 100992)       0           flatten_2[0][0]                  \n",
      "                                                                 flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 100992)       0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 16)           1615888     dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,862,800\n",
      "Trainable params: 1,862,800\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 16000, 128)   768         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 16000, 128)   0           conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1D)  (None, 5333, 128)    0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 5333, 128)    82048       max_pooling1d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 5333, 128)    0           conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1D)  (None, 1777, 128)    0           activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 1777, 128)    82048       max_pooling1d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 1777, 128)    0           conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1D)  (None, 592, 128)     0           activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 592, 128)     82048       max_pooling1d_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 592, 128)     0           conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling1D) (None, 197, 128)     0           activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 197, 256)     164096      max_pooling1d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 197, 256)     0           conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling1D) (None, 65, 256)      0           activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 25216)        0           max_pooling1d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, 16640)        0           max_pooling1d_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 41856)        0           flatten_4[0][0]                  \n",
      "                                                                 flatten_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 41856)        0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 16)           669712      dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,080,720\n",
      "Trainable params: 1,080,720\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 16000, 128)   768         input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 16000, 128)   0           conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling1D) (None, 5333, 128)    0           activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 5333, 128)    82048       max_pooling1d_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 5333, 128)    0           conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling1D) (None, 1777, 128)    0           activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, 1777, 128)    82048       max_pooling1d_13[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 1777, 128)    0           conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling1D) (None, 592, 128)     0           activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, 592, 128)     82048       max_pooling1d_14[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 592, 128)     0           conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling1D) (None, 197, 128)     0           activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)              (None, 197, 256)     164096      max_pooling1d_15[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 197, 256)     0           conv1d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling1D) (None, 65, 256)      0           activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_17 (Conv1D)              (None, 65, 256)      327936      max_pooling1d_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 65, 256)      0           conv1d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling1D) (None, 21, 256)      0           activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_6 (Flatten)             (None, 16640)        0           max_pooling1d_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_7 (Flatten)             (None, 5376)         0           max_pooling1d_17[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 22016)        0           flatten_6[0][0]                  \n",
      "                                                                 flatten_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 22016)        0           concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 16)           352272      dropout_3[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,091,216\n",
      "Trainable params: 1,091,216\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_18 (Conv1D)              (None, 16000, 128)   768         input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 16000, 128)   0           conv1d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling1D) (None, 5333, 128)    0           activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_19 (Conv1D)              (None, 5333, 128)    82048       max_pooling1d_18[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 5333, 128)    0           conv1d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling1D) (None, 1777, 128)    0           activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_20 (Conv1D)              (None, 1777, 128)    82048       max_pooling1d_19[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 1777, 128)    0           conv1d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling1D) (None, 592, 128)     0           activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_21 (Conv1D)              (None, 592, 128)     82048       max_pooling1d_20[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 592, 128)     0           conv1d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling1D) (None, 197, 128)     0           activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_22 (Conv1D)              (None, 197, 256)     164096      max_pooling1d_21[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 197, 256)     0           conv1d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling1D) (None, 65, 256)      0           activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_23 (Conv1D)              (None, 65, 256)      327936      max_pooling1d_22[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 65, 256)      0           conv1d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling1D) (None, 21, 256)      0           activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_24 (Conv1D)              (None, 21, 256)      327936      max_pooling1d_23[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 21, 256)      0           conv1d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling1D) (None, 7, 256)       0           activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_8 (Flatten)             (None, 5376)         0           max_pooling1d_23[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_9 (Flatten)             (None, 1792)         0           max_pooling1d_24[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 7168)         0           flatten_8[0][0]                  \n",
      "                                                                 flatten_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 7168)         0           concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 16)           114704      dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,181,584\n",
      "Trainable params: 1,181,584\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_25 (Conv1D)              (None, 16000, 128)   768         input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 16000, 128)   0           conv1d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling1D) (None, 5333, 128)    0           activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_26 (Conv1D)              (None, 5333, 128)    82048       max_pooling1d_25[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 5333, 128)    0           conv1d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_26 (MaxPooling1D) (None, 1777, 128)    0           activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_27 (Conv1D)              (None, 1777, 128)    82048       max_pooling1d_26[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 1777, 128)    0           conv1d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_27 (MaxPooling1D) (None, 592, 128)     0           activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_28 (Conv1D)              (None, 592, 128)     82048       max_pooling1d_27[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 592, 128)     0           conv1d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_28 (MaxPooling1D) (None, 197, 128)     0           activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_29 (Conv1D)              (None, 197, 256)     164096      max_pooling1d_28[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 197, 256)     0           conv1d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_29 (MaxPooling1D) (None, 65, 256)      0           activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_30 (Conv1D)              (None, 65, 256)      327936      max_pooling1d_29[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 65, 256)      0           conv1d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_30 (MaxPooling1D) (None, 21, 256)      0           activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_31 (Conv1D)              (None, 21, 256)      327936      max_pooling1d_30[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 21, 256)      0           conv1d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_31 (MaxPooling1D) (None, 7, 256)       0           activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_32 (Conv1D)              (None, 7, 256)       327936      max_pooling1d_31[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 7, 256)       0           conv1d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_32 (MaxPooling1D) (None, 2, 256)       0           activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_10 (Flatten)            (None, 1792)         0           max_pooling1d_31[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_11 (Flatten)            (None, 512)          0           max_pooling1d_32[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 2304)         0           flatten_10[0][0]                 \n",
      "                                                                 flatten_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 2304)         0           concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 16)           36880       dropout_5[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,431,696\n",
      "Trainable params: 1,431,696\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in range(3, 9):\n",
    "    model = build_cnn(conv_num=i)\n",
    "    model.summary()\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36805 samples, validate on 4293 samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.8505 - acc: 0.4119\n",
      "Epoch 00001: val_loss improved from inf to 1.45276, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_DO_4_conv_checkpoint/001-1.4528.hdf5\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 1.8506 - acc: 0.4119 - val_loss: 1.4528 - val_acc: 0.5611\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2953 - acc: 0.5995\n",
      "Epoch 00002: val_loss improved from 1.45276 to 1.17405, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_DO_4_conv_checkpoint/002-1.1740.hdf5\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 1.2952 - acc: 0.5995 - val_loss: 1.1740 - val_acc: 0.6396\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0903 - acc: 0.6684\n",
      "Epoch 00003: val_loss improved from 1.17405 to 1.05345, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_DO_4_conv_checkpoint/003-1.0534.hdf5\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 1.0902 - acc: 0.6684 - val_loss: 1.0534 - val_acc: 0.6811\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9409 - acc: 0.7173\n",
      "Epoch 00004: val_loss improved from 1.05345 to 1.01259, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_DO_4_conv_checkpoint/004-1.0126.hdf5\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.9413 - acc: 0.7173 - val_loss: 1.0126 - val_acc: 0.7053\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8229 - acc: 0.7523\n",
      "Epoch 00005: val_loss improved from 1.01259 to 0.92325, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_DO_4_conv_checkpoint/005-0.9233.hdf5\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.8229 - acc: 0.7523 - val_loss: 0.9233 - val_acc: 0.7235\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7159 - acc: 0.7850\n",
      "Epoch 00006: val_loss did not improve from 0.92325\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.7159 - acc: 0.7851 - val_loss: 0.9274 - val_acc: 0.7186\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6267 - acc: 0.8118\n",
      "Epoch 00007: val_loss improved from 0.92325 to 0.90257, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_DO_4_conv_checkpoint/007-0.9026.hdf5\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.6267 - acc: 0.8118 - val_loss: 0.9026 - val_acc: 0.7338\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5421 - acc: 0.8385\n",
      "Epoch 00008: val_loss improved from 0.90257 to 0.88965, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_DO_4_conv_checkpoint/008-0.8896.hdf5\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.5422 - acc: 0.8384 - val_loss: 0.8896 - val_acc: 0.7396\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4712 - acc: 0.8576\n",
      "Epoch 00009: val_loss improved from 0.88965 to 0.88482, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_DO_4_conv_checkpoint/009-0.8848.hdf5\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.4714 - acc: 0.8576 - val_loss: 0.8848 - val_acc: 0.7389\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4041 - acc: 0.8779\n",
      "Epoch 00010: val_loss did not improve from 0.88482\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.4041 - acc: 0.8778 - val_loss: 0.9054 - val_acc: 0.7400\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3527 - acc: 0.8925\n",
      "Epoch 00011: val_loss did not improve from 0.88482\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.3527 - acc: 0.8925 - val_loss: 0.9826 - val_acc: 0.7298\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3067 - acc: 0.9073\n",
      "Epoch 00012: val_loss did not improve from 0.88482\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.3067 - acc: 0.9073 - val_loss: 0.9224 - val_acc: 0.7463\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2647 - acc: 0.9208\n",
      "Epoch 00013: val_loss did not improve from 0.88482\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.2647 - acc: 0.9208 - val_loss: 0.9901 - val_acc: 0.7347\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2364 - acc: 0.9279\n",
      "Epoch 00014: val_loss did not improve from 0.88482\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.2364 - acc: 0.9278 - val_loss: 1.0346 - val_acc: 0.7363\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2005 - acc: 0.9405\n",
      "Epoch 00015: val_loss did not improve from 0.88482\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.2005 - acc: 0.9406 - val_loss: 1.0153 - val_acc: 0.7470\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1861 - acc: 0.9434\n",
      "Epoch 00016: val_loss did not improve from 0.88482\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.1860 - acc: 0.9434 - val_loss: 0.9983 - val_acc: 0.7549\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1624 - acc: 0.9507\n",
      "Epoch 00017: val_loss did not improve from 0.88482\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.1624 - acc: 0.9507 - val_loss: 1.0413 - val_acc: 0.7466\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1518 - acc: 0.9541\n",
      "Epoch 00018: val_loss did not improve from 0.88482\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.1518 - acc: 0.9541 - val_loss: 1.1034 - val_acc: 0.7505\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1301 - acc: 0.9618\n",
      "Epoch 00019: val_loss did not improve from 0.88482\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.1301 - acc: 0.9618 - val_loss: 1.1332 - val_acc: 0.7426\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1238 - acc: 0.9618\n",
      "Epoch 00020: val_loss did not improve from 0.88482\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.1238 - acc: 0.9619 - val_loss: 1.1575 - val_acc: 0.7379\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1175 - acc: 0.9644\n",
      "Epoch 00021: val_loss did not improve from 0.88482\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.1175 - acc: 0.9644 - val_loss: 1.1125 - val_acc: 0.7622\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1053 - acc: 0.9682\n",
      "Epoch 00022: val_loss did not improve from 0.88482\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.1053 - acc: 0.9682 - val_loss: 1.1638 - val_acc: 0.7477\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0973 - acc: 0.9718\n",
      "Epoch 00023: val_loss did not improve from 0.88482\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.0973 - acc: 0.9718 - val_loss: 1.1420 - val_acc: 0.7638\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0938 - acc: 0.9724\n",
      "Epoch 00024: val_loss did not improve from 0.88482\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.0939 - acc: 0.9723 - val_loss: 1.4093 - val_acc: 0.7307\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0930 - acc: 0.9731\n",
      "Epoch 00025: val_loss did not improve from 0.88482\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.0930 - acc: 0.9731 - val_loss: 1.1821 - val_acc: 0.7622\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0805 - acc: 0.9771\n",
      "Epoch 00026: val_loss did not improve from 0.88482\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.0805 - acc: 0.9771 - val_loss: 1.1970 - val_acc: 0.7636\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0781 - acc: 0.9765\n",
      "Epoch 00027: val_loss did not improve from 0.88482\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.0781 - acc: 0.9766 - val_loss: 1.2004 - val_acc: 0.7643\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0732 - acc: 0.9785\n",
      "Epoch 00028: val_loss did not improve from 0.88482\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.0732 - acc: 0.9785 - val_loss: 1.2471 - val_acc: 0.7636\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0679 - acc: 0.9808\n",
      "Epoch 00029: val_loss did not improve from 0.88482\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.0679 - acc: 0.9808 - val_loss: 1.2934 - val_acc: 0.7603\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0695 - acc: 0.9796\n",
      "Epoch 00030: val_loss did not improve from 0.88482\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.0696 - acc: 0.9796 - val_loss: 1.2641 - val_acc: 0.7668\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0667 - acc: 0.9807\n",
      "Epoch 00031: val_loss did not improve from 0.88482\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.0667 - acc: 0.9807 - val_loss: 1.2839 - val_acc: 0.7594\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0669 - acc: 0.9811\n",
      "Epoch 00032: val_loss did not improve from 0.88482\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.0669 - acc: 0.9811 - val_loss: 1.2467 - val_acc: 0.7720\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0645 - acc: 0.9810\n",
      "Epoch 00033: val_loss did not improve from 0.88482\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.0645 - acc: 0.9810 - val_loss: 1.3396 - val_acc: 0.7622\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0630 - acc: 0.9816\n",
      "Epoch 00034: val_loss did not improve from 0.88482\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.0631 - acc: 0.9816 - val_loss: 1.3247 - val_acc: 0.7603\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0577 - acc: 0.9844\n",
      "Epoch 00035: val_loss did not improve from 0.88482\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.0577 - acc: 0.9844 - val_loss: 1.2603 - val_acc: 0.7657\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0492 - acc: 0.9861\n",
      "Epoch 00036: val_loss did not improve from 0.88482\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.0492 - acc: 0.9861 - val_loss: 1.3368 - val_acc: 0.7687\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0562 - acc: 0.9844\n",
      "Epoch 00037: val_loss did not improve from 0.88482\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.0562 - acc: 0.9844 - val_loss: 1.2920 - val_acc: 0.7736\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0499 - acc: 0.9867\n",
      "Epoch 00038: val_loss did not improve from 0.88482\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.0499 - acc: 0.9867 - val_loss: 1.3311 - val_acc: 0.7671\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0550 - acc: 0.9849\n",
      "Epoch 00039: val_loss did not improve from 0.88482\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.0550 - acc: 0.9849 - val_loss: 1.3325 - val_acc: 0.7654\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0480 - acc: 0.9870\n",
      "Epoch 00040: val_loss did not improve from 0.88482\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.0480 - acc: 0.9870 - val_loss: 1.3023 - val_acc: 0.7694\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0464 - acc: 0.9872\n",
      "Epoch 00041: val_loss did not improve from 0.88482\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.0464 - acc: 0.9871 - val_loss: 1.3390 - val_acc: 0.7664\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0465 - acc: 0.9877\n",
      "Epoch 00042: val_loss did not improve from 0.88482\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.0465 - acc: 0.9877 - val_loss: 1.3419 - val_acc: 0.7796\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0476 - acc: 0.9862\n",
      "Epoch 00043: val_loss did not improve from 0.88482\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.0476 - acc: 0.9862 - val_loss: 1.3287 - val_acc: 0.7778\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0536 - acc: 0.9845\n",
      "Epoch 00044: val_loss did not improve from 0.88482\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.0536 - acc: 0.9845 - val_loss: 1.3145 - val_acc: 0.7743\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0413 - acc: 0.9891\n",
      "Epoch 00045: val_loss did not improve from 0.88482\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.0413 - acc: 0.9891 - val_loss: 1.3641 - val_acc: 0.7696\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0403 - acc: 0.9895\n",
      "Epoch 00046: val_loss did not improve from 0.88482\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.0403 - acc: 0.9895 - val_loss: 1.4035 - val_acc: 0.7661\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0433 - acc: 0.9878\n",
      "Epoch 00047: val_loss did not improve from 0.88482\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.0433 - acc: 0.9878 - val_loss: 1.3580 - val_acc: 0.7799\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0406 - acc: 0.9893\n",
      "Epoch 00048: val_loss did not improve from 0.88482\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.0406 - acc: 0.9893 - val_loss: 1.4791 - val_acc: 0.7659\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0403 - acc: 0.9895\n",
      "Epoch 00049: val_loss did not improve from 0.88482\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.0403 - acc: 0.9895 - val_loss: 1.4003 - val_acc: 0.7743\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0407 - acc: 0.9887\n",
      "Epoch 00050: val_loss did not improve from 0.88482\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.0407 - acc: 0.9888 - val_loss: 1.3823 - val_acc: 0.7738\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0356 - acc: 0.9911\n",
      "Epoch 00051: val_loss did not improve from 0.88482\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.0356 - acc: 0.9911 - val_loss: 1.3636 - val_acc: 0.7817\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0452 - acc: 0.9877\n",
      "Epoch 00052: val_loss did not improve from 0.88482\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.0452 - acc: 0.9877 - val_loss: 1.5766 - val_acc: 0.7489\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0402 - acc: 0.9886\n",
      "Epoch 00053: val_loss did not improve from 0.88482\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.0402 - acc: 0.9886 - val_loss: 1.4018 - val_acc: 0.7768\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0345 - acc: 0.9914\n",
      "Epoch 00054: val_loss did not improve from 0.88482\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.0345 - acc: 0.9914 - val_loss: 1.3763 - val_acc: 0.7824\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0349 - acc: 0.9908\n",
      "Epoch 00055: val_loss did not improve from 0.88482\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.0349 - acc: 0.9908 - val_loss: 1.4507 - val_acc: 0.7750\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0364 - acc: 0.9904\n",
      "Epoch 00056: val_loss did not improve from 0.88482\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.0364 - acc: 0.9904 - val_loss: 1.3945 - val_acc: 0.7747\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0368 - acc: 0.9906\n",
      "Epoch 00057: val_loss did not improve from 0.88482\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.0368 - acc: 0.9906 - val_loss: 1.4565 - val_acc: 0.7720\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0335 - acc: 0.9914\n",
      "Epoch 00058: val_loss did not improve from 0.88482\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.0335 - acc: 0.9914 - val_loss: 1.3831 - val_acc: 0.7834\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0364 - acc: 0.9906\n",
      "Epoch 00059: val_loss did not improve from 0.88482\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 0.0364 - acc: 0.9906 - val_loss: 1.3629 - val_acc: 0.7873\n",
      "\n",
      "1D_CNN_custom_multi_2_concat_ch_128_DO_4_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 711us/sample - loss: 0.9640 - acc: 0.7130\n",
      "Loss: 0.9640262068493963 Accuracy: 0.7129803\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9111 - acc: 0.3802\n",
      "Epoch 00001: val_loss improved from inf to 1.40263, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_DO_5_conv_checkpoint/001-1.4026.hdf5\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 1.9112 - acc: 0.3802 - val_loss: 1.4026 - val_acc: 0.5751\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3244 - acc: 0.5904\n",
      "Epoch 00002: val_loss improved from 1.40263 to 1.12327, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_DO_5_conv_checkpoint/002-1.1233.hdf5\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 1.3243 - acc: 0.5904 - val_loss: 1.1233 - val_acc: 0.6629\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1119 - acc: 0.6622\n",
      "Epoch 00003: val_loss improved from 1.12327 to 1.04352, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_DO_5_conv_checkpoint/003-1.0435.hdf5\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 1.1120 - acc: 0.6622 - val_loss: 1.0435 - val_acc: 0.6778\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9662 - acc: 0.7105\n",
      "Epoch 00004: val_loss improved from 1.04352 to 0.86617, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_DO_5_conv_checkpoint/004-0.8662.hdf5\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.9662 - acc: 0.7106 - val_loss: 0.8662 - val_acc: 0.7324\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8507 - acc: 0.7457\n",
      "Epoch 00005: val_loss improved from 0.86617 to 0.80672, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_DO_5_conv_checkpoint/005-0.8067.hdf5\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.8507 - acc: 0.7457 - val_loss: 0.8067 - val_acc: 0.7536\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7454 - acc: 0.7782\n",
      "Epoch 00006: val_loss improved from 0.80672 to 0.73048, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_DO_5_conv_checkpoint/006-0.7305.hdf5\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.7454 - acc: 0.7782 - val_loss: 0.7305 - val_acc: 0.7815\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6618 - acc: 0.8019\n",
      "Epoch 00007: val_loss did not improve from 0.73048\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.6617 - acc: 0.8019 - val_loss: 0.7515 - val_acc: 0.7922\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5966 - acc: 0.8237\n",
      "Epoch 00008: val_loss improved from 0.73048 to 0.60486, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_DO_5_conv_checkpoint/008-0.6049.hdf5\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.5966 - acc: 0.8237 - val_loss: 0.6049 - val_acc: 0.8276\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5291 - acc: 0.8430\n",
      "Epoch 00009: val_loss did not improve from 0.60486\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.5291 - acc: 0.8430 - val_loss: 0.6635 - val_acc: 0.8048\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4801 - acc: 0.8579\n",
      "Epoch 00010: val_loss improved from 0.60486 to 0.56346, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_DO_5_conv_checkpoint/010-0.5635.hdf5\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.4801 - acc: 0.8579 - val_loss: 0.5635 - val_acc: 0.8411\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4271 - acc: 0.8733\n",
      "Epoch 00011: val_loss improved from 0.56346 to 0.54333, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_DO_5_conv_checkpoint/011-0.5433.hdf5\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.4271 - acc: 0.8733 - val_loss: 0.5433 - val_acc: 0.8486\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3885 - acc: 0.8852\n",
      "Epoch 00012: val_loss improved from 0.54333 to 0.53229, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_DO_5_conv_checkpoint/012-0.5323.hdf5\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.3884 - acc: 0.8852 - val_loss: 0.5323 - val_acc: 0.8458\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3516 - acc: 0.8963\n",
      "Epoch 00013: val_loss did not improve from 0.53229\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.3516 - acc: 0.8963 - val_loss: 0.5935 - val_acc: 0.8311\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3211 - acc: 0.9025\n",
      "Epoch 00014: val_loss improved from 0.53229 to 0.51033, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_DO_5_conv_checkpoint/014-0.5103.hdf5\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.3210 - acc: 0.9025 - val_loss: 0.5103 - val_acc: 0.8546\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2858 - acc: 0.9148\n",
      "Epoch 00015: val_loss did not improve from 0.51033\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.2858 - acc: 0.9148 - val_loss: 0.5408 - val_acc: 0.8484\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2604 - acc: 0.9203\n",
      "Epoch 00016: val_loss did not improve from 0.51033\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.2603 - acc: 0.9203 - val_loss: 0.5169 - val_acc: 0.8658\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2366 - acc: 0.9285\n",
      "Epoch 00017: val_loss did not improve from 0.51033\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.2366 - acc: 0.9285 - val_loss: 0.5850 - val_acc: 0.8421\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2202 - acc: 0.9317\n",
      "Epoch 00018: val_loss improved from 0.51033 to 0.49827, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_DO_5_conv_checkpoint/018-0.4983.hdf5\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.2202 - acc: 0.9317 - val_loss: 0.4983 - val_acc: 0.8733\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1948 - acc: 0.9395\n",
      "Epoch 00019: val_loss did not improve from 0.49827\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.1948 - acc: 0.9395 - val_loss: 0.4985 - val_acc: 0.8642\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1780 - acc: 0.9450\n",
      "Epoch 00020: val_loss did not improve from 0.49827\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.1780 - acc: 0.9450 - val_loss: 0.5080 - val_acc: 0.8689\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1707 - acc: 0.9459\n",
      "Epoch 00021: val_loss did not improve from 0.49827\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.1707 - acc: 0.9459 - val_loss: 0.5340 - val_acc: 0.8593\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1474 - acc: 0.9537\n",
      "Epoch 00022: val_loss improved from 0.49827 to 0.49546, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_DO_5_conv_checkpoint/022-0.4955.hdf5\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.1474 - acc: 0.9537 - val_loss: 0.4955 - val_acc: 0.8810\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1366 - acc: 0.9573\n",
      "Epoch 00023: val_loss did not improve from 0.49546\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.1366 - acc: 0.9573 - val_loss: 0.5216 - val_acc: 0.8707\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1252 - acc: 0.9605\n",
      "Epoch 00024: val_loss did not improve from 0.49546\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.1252 - acc: 0.9605 - val_loss: 0.5108 - val_acc: 0.8742\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1158 - acc: 0.9638\n",
      "Epoch 00025: val_loss did not improve from 0.49546\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.1159 - acc: 0.9638 - val_loss: 0.6123 - val_acc: 0.8584\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1096 - acc: 0.9658\n",
      "Epoch 00026: val_loss did not improve from 0.49546\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.1096 - acc: 0.9658 - val_loss: 0.5584 - val_acc: 0.8693\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1004 - acc: 0.9687\n",
      "Epoch 00027: val_loss did not improve from 0.49546\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.1004 - acc: 0.9687 - val_loss: 0.5559 - val_acc: 0.8789\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0978 - acc: 0.9689\n",
      "Epoch 00028: val_loss did not improve from 0.49546\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0978 - acc: 0.9689 - val_loss: 0.5518 - val_acc: 0.8775\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0919 - acc: 0.9711\n",
      "Epoch 00029: val_loss did not improve from 0.49546\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0919 - acc: 0.9711 - val_loss: 0.6027 - val_acc: 0.8686\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0800 - acc: 0.9745\n",
      "Epoch 00030: val_loss did not improve from 0.49546\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0800 - acc: 0.9745 - val_loss: 0.7343 - val_acc: 0.8383\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0899 - acc: 0.9730\n",
      "Epoch 00031: val_loss did not improve from 0.49546\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0899 - acc: 0.9730 - val_loss: 0.5605 - val_acc: 0.8763\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0751 - acc: 0.9765\n",
      "Epoch 00032: val_loss did not improve from 0.49546\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0751 - acc: 0.9765 - val_loss: 0.5846 - val_acc: 0.8758\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0779 - acc: 0.9760\n",
      "Epoch 00033: val_loss did not improve from 0.49546\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0779 - acc: 0.9760 - val_loss: 0.6826 - val_acc: 0.8586\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0716 - acc: 0.9781\n",
      "Epoch 00034: val_loss did not improve from 0.49546\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0716 - acc: 0.9781 - val_loss: 0.5778 - val_acc: 0.8724\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0648 - acc: 0.9804\n",
      "Epoch 00035: val_loss did not improve from 0.49546\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0648 - acc: 0.9804 - val_loss: 0.5734 - val_acc: 0.8772\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0667 - acc: 0.9800\n",
      "Epoch 00036: val_loss did not improve from 0.49546\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0667 - acc: 0.9800 - val_loss: 0.5754 - val_acc: 0.8784\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0635 - acc: 0.9808\n",
      "Epoch 00037: val_loss did not improve from 0.49546\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0635 - acc: 0.9808 - val_loss: 0.6097 - val_acc: 0.8765\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0633 - acc: 0.9803\n",
      "Epoch 00038: val_loss did not improve from 0.49546\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0632 - acc: 0.9803 - val_loss: 0.6320 - val_acc: 0.8728\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0598 - acc: 0.9822\n",
      "Epoch 00039: val_loss did not improve from 0.49546\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0598 - acc: 0.9822 - val_loss: 0.5573 - val_acc: 0.8856\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0534 - acc: 0.9836\n",
      "Epoch 00040: val_loss did not improve from 0.49546\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0534 - acc: 0.9836 - val_loss: 0.6956 - val_acc: 0.8698\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0547 - acc: 0.9845\n",
      "Epoch 00041: val_loss did not improve from 0.49546\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0547 - acc: 0.9845 - val_loss: 0.6204 - val_acc: 0.8791\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0499 - acc: 0.9852\n",
      "Epoch 00042: val_loss did not improve from 0.49546\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0500 - acc: 0.9851 - val_loss: 0.6577 - val_acc: 0.8777\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0575 - acc: 0.9825\n",
      "Epoch 00043: val_loss did not improve from 0.49546\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0575 - acc: 0.9825 - val_loss: 0.6044 - val_acc: 0.8840\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0454 - acc: 0.9864\n",
      "Epoch 00044: val_loss did not improve from 0.49546\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0454 - acc: 0.9864 - val_loss: 0.6590 - val_acc: 0.8826\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0512 - acc: 0.9838\n",
      "Epoch 00045: val_loss did not improve from 0.49546\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0512 - acc: 0.9838 - val_loss: 0.5949 - val_acc: 0.8854\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0460 - acc: 0.9873\n",
      "Epoch 00046: val_loss did not improve from 0.49546\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0460 - acc: 0.9873 - val_loss: 0.6014 - val_acc: 0.8800\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0477 - acc: 0.9858\n",
      "Epoch 00047: val_loss did not improve from 0.49546\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0477 - acc: 0.9858 - val_loss: 0.5944 - val_acc: 0.8856\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0471 - acc: 0.9858\n",
      "Epoch 00048: val_loss did not improve from 0.49546\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0471 - acc: 0.9858 - val_loss: 0.6070 - val_acc: 0.8824\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0407 - acc: 0.9886\n",
      "Epoch 00049: val_loss did not improve from 0.49546\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0407 - acc: 0.9886 - val_loss: 0.6493 - val_acc: 0.8849\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0438 - acc: 0.9870\n",
      "Epoch 00050: val_loss did not improve from 0.49546\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0438 - acc: 0.9870 - val_loss: 0.6360 - val_acc: 0.8849\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0431 - acc: 0.9872\n",
      "Epoch 00051: val_loss did not improve from 0.49546\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0430 - acc: 0.9872 - val_loss: 0.6356 - val_acc: 0.8812\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0405 - acc: 0.9877\n",
      "Epoch 00052: val_loss did not improve from 0.49546\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0405 - acc: 0.9877 - val_loss: 0.6749 - val_acc: 0.8786\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0450 - acc: 0.9870\n",
      "Epoch 00053: val_loss did not improve from 0.49546\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0450 - acc: 0.9870 - val_loss: 0.6657 - val_acc: 0.8819\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0383 - acc: 0.9891\n",
      "Epoch 00054: val_loss did not improve from 0.49546\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0383 - acc: 0.9891 - val_loss: 0.6544 - val_acc: 0.8889\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0389 - acc: 0.9890\n",
      "Epoch 00055: val_loss did not improve from 0.49546\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0389 - acc: 0.9890 - val_loss: 0.6447 - val_acc: 0.8786\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0426 - acc: 0.9878\n",
      "Epoch 00056: val_loss did not improve from 0.49546\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0426 - acc: 0.9878 - val_loss: 0.6006 - val_acc: 0.8861\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0362 - acc: 0.9902\n",
      "Epoch 00057: val_loss did not improve from 0.49546\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0363 - acc: 0.9901 - val_loss: 0.6631 - val_acc: 0.8810\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0389 - acc: 0.9890\n",
      "Epoch 00058: val_loss did not improve from 0.49546\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0389 - acc: 0.9890 - val_loss: 0.6113 - val_acc: 0.8842\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0412 - acc: 0.9884\n",
      "Epoch 00059: val_loss did not improve from 0.49546\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0412 - acc: 0.9884 - val_loss: 0.6507 - val_acc: 0.8854\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0352 - acc: 0.9902\n",
      "Epoch 00060: val_loss did not improve from 0.49546\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0352 - acc: 0.9902 - val_loss: 0.6027 - val_acc: 0.8928\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0354 - acc: 0.9898\n",
      "Epoch 00061: val_loss did not improve from 0.49546\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0354 - acc: 0.9898 - val_loss: 0.6325 - val_acc: 0.8896\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0350 - acc: 0.9905\n",
      "Epoch 00062: val_loss did not improve from 0.49546\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0350 - acc: 0.9905 - val_loss: 0.6941 - val_acc: 0.8749\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0353 - acc: 0.9901\n",
      "Epoch 00063: val_loss did not improve from 0.49546\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0353 - acc: 0.9901 - val_loss: 0.6653 - val_acc: 0.8835\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0323 - acc: 0.9907\n",
      "Epoch 00064: val_loss did not improve from 0.49546\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0323 - acc: 0.9907 - val_loss: 0.6374 - val_acc: 0.8861\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0326 - acc: 0.9906\n",
      "Epoch 00065: val_loss did not improve from 0.49546\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0326 - acc: 0.9906 - val_loss: 0.6632 - val_acc: 0.8826\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0376 - acc: 0.9891\n",
      "Epoch 00066: val_loss did not improve from 0.49546\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0376 - acc: 0.9891 - val_loss: 0.6205 - val_acc: 0.8854\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0278 - acc: 0.9929\n",
      "Epoch 00067: val_loss did not improve from 0.49546\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0278 - acc: 0.9929 - val_loss: 0.6489 - val_acc: 0.8894\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0291 - acc: 0.9921\n",
      "Epoch 00068: val_loss did not improve from 0.49546\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0291 - acc: 0.9921 - val_loss: 0.6401 - val_acc: 0.8873\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0322 - acc: 0.9911\n",
      "Epoch 00069: val_loss did not improve from 0.49546\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0322 - acc: 0.9910 - val_loss: 0.7033 - val_acc: 0.8847\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0335 - acc: 0.9910\n",
      "Epoch 00070: val_loss did not improve from 0.49546\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0335 - acc: 0.9910 - val_loss: 0.6224 - val_acc: 0.8956\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0310 - acc: 0.9916\n",
      "Epoch 00071: val_loss did not improve from 0.49546\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0310 - acc: 0.9916 - val_loss: 0.6344 - val_acc: 0.8945\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0279 - acc: 0.9929\n",
      "Epoch 00072: val_loss did not improve from 0.49546\n",
      "36805/36805 [==============================] - 64s 2ms/sample - loss: 0.0279 - acc: 0.9929 - val_loss: 0.6689 - val_acc: 0.8859\n",
      "\n",
      "1D_CNN_custom_multi_2_concat_ch_128_DO_5_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 717us/sample - loss: 0.6069 - acc: 0.8474\n",
      "Loss: 0.6069400324380781 Accuracy: 0.847352\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9476 - acc: 0.3682\n",
      "Epoch 00001: val_loss improved from inf to 1.33270, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_DO_6_conv_checkpoint/001-1.3327.hdf5\n",
      "36805/36805 [==============================] - 67s 2ms/sample - loss: 1.9477 - acc: 0.3682 - val_loss: 1.3327 - val_acc: 0.5756\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2604 - acc: 0.6050\n",
      "Epoch 00002: val_loss improved from 1.33270 to 0.98248, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_DO_6_conv_checkpoint/002-0.9825.hdf5\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 1.2602 - acc: 0.6051 - val_loss: 0.9825 - val_acc: 0.7093\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9651 - acc: 0.7061\n",
      "Epoch 00003: val_loss improved from 0.98248 to 0.87725, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_DO_6_conv_checkpoint/003-0.8772.hdf5\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.9651 - acc: 0.7061 - val_loss: 0.8772 - val_acc: 0.7338\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7881 - acc: 0.7585\n",
      "Epoch 00004: val_loss improved from 0.87725 to 0.62500, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_DO_6_conv_checkpoint/004-0.6250.hdf5\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.7882 - acc: 0.7585 - val_loss: 0.6250 - val_acc: 0.8169\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6503 - acc: 0.8007\n",
      "Epoch 00005: val_loss improved from 0.62500 to 0.54671, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_DO_6_conv_checkpoint/005-0.5467.hdf5\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.6504 - acc: 0.8007 - val_loss: 0.5467 - val_acc: 0.8474\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5596 - acc: 0.8301\n",
      "Epoch 00006: val_loss improved from 0.54671 to 0.46277, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_DO_6_conv_checkpoint/006-0.4628.hdf5\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.5595 - acc: 0.8302 - val_loss: 0.4628 - val_acc: 0.8654\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4839 - acc: 0.8522\n",
      "Epoch 00007: val_loss improved from 0.46277 to 0.42886, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_DO_6_conv_checkpoint/007-0.4289.hdf5\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.4839 - acc: 0.8522 - val_loss: 0.4289 - val_acc: 0.8672\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4287 - acc: 0.8694\n",
      "Epoch 00008: val_loss improved from 0.42886 to 0.35678, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_DO_6_conv_checkpoint/008-0.3568.hdf5\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.4287 - acc: 0.8694 - val_loss: 0.3568 - val_acc: 0.8998\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3803 - acc: 0.8841\n",
      "Epoch 00009: val_loss improved from 0.35678 to 0.35027, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_DO_6_conv_checkpoint/009-0.3503.hdf5\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.3803 - acc: 0.8841 - val_loss: 0.3503 - val_acc: 0.9008\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3482 - acc: 0.8917\n",
      "Epoch 00010: val_loss improved from 0.35027 to 0.33558, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_DO_6_conv_checkpoint/010-0.3356.hdf5\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.3482 - acc: 0.8917 - val_loss: 0.3356 - val_acc: 0.9092\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3100 - acc: 0.9031\n",
      "Epoch 00011: val_loss improved from 0.33558 to 0.31236, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_DO_6_conv_checkpoint/011-0.3124.hdf5\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.3100 - acc: 0.9031 - val_loss: 0.3124 - val_acc: 0.9150\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2914 - acc: 0.9097\n",
      "Epoch 00012: val_loss improved from 0.31236 to 0.29558, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_DO_6_conv_checkpoint/012-0.2956.hdf5\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.2914 - acc: 0.9097 - val_loss: 0.2956 - val_acc: 0.9166\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2715 - acc: 0.9139\n",
      "Epoch 00013: val_loss did not improve from 0.29558\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.2716 - acc: 0.9139 - val_loss: 0.2961 - val_acc: 0.9189\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2416 - acc: 0.9230\n",
      "Epoch 00014: val_loss did not improve from 0.29558\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.2415 - acc: 0.9231 - val_loss: 0.3012 - val_acc: 0.9124\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2269 - acc: 0.9279\n",
      "Epoch 00015: val_loss improved from 0.29558 to 0.29494, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_DO_6_conv_checkpoint/015-0.2949.hdf5\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.2269 - acc: 0.9279 - val_loss: 0.2949 - val_acc: 0.9196\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2096 - acc: 0.9336\n",
      "Epoch 00016: val_loss improved from 0.29494 to 0.27692, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_DO_6_conv_checkpoint/016-0.2769.hdf5\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.2096 - acc: 0.9337 - val_loss: 0.2769 - val_acc: 0.9273\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2003 - acc: 0.9358\n",
      "Epoch 00017: val_loss improved from 0.27692 to 0.26592, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_DO_6_conv_checkpoint/017-0.2659.hdf5\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.2003 - acc: 0.9358 - val_loss: 0.2659 - val_acc: 0.9213\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1855 - acc: 0.9415\n",
      "Epoch 00018: val_loss improved from 0.26592 to 0.26127, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_DO_6_conv_checkpoint/018-0.2613.hdf5\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.1855 - acc: 0.9415 - val_loss: 0.2613 - val_acc: 0.9290\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1739 - acc: 0.9446\n",
      "Epoch 00019: val_loss did not improve from 0.26127\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.1740 - acc: 0.9445 - val_loss: 0.2614 - val_acc: 0.9315\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1609 - acc: 0.9471\n",
      "Epoch 00020: val_loss did not improve from 0.26127\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.1609 - acc: 0.9471 - val_loss: 0.2649 - val_acc: 0.9266\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1507 - acc: 0.9514\n",
      "Epoch 00021: val_loss did not improve from 0.26127\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.1507 - acc: 0.9514 - val_loss: 0.2770 - val_acc: 0.9255\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1431 - acc: 0.9535\n",
      "Epoch 00022: val_loss did not improve from 0.26127\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.1431 - acc: 0.9535 - val_loss: 0.2894 - val_acc: 0.9257\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1294 - acc: 0.9576\n",
      "Epoch 00023: val_loss did not improve from 0.26127\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.1294 - acc: 0.9576 - val_loss: 0.2819 - val_acc: 0.9273\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1255 - acc: 0.9579\n",
      "Epoch 00024: val_loss improved from 0.26127 to 0.25547, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_DO_6_conv_checkpoint/024-0.2555.hdf5\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.1254 - acc: 0.9579 - val_loss: 0.2555 - val_acc: 0.9317\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1146 - acc: 0.9636\n",
      "Epoch 00025: val_loss did not improve from 0.25547\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.1146 - acc: 0.9636 - val_loss: 0.2577 - val_acc: 0.9366\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1076 - acc: 0.9646\n",
      "Epoch 00026: val_loss did not improve from 0.25547\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.1076 - acc: 0.9646 - val_loss: 0.2631 - val_acc: 0.9364\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1066 - acc: 0.9645\n",
      "Epoch 00027: val_loss did not improve from 0.25547\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.1066 - acc: 0.9645 - val_loss: 0.2648 - val_acc: 0.9331\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0934 - acc: 0.9691\n",
      "Epoch 00028: val_loss did not improve from 0.25547\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0934 - acc: 0.9691 - val_loss: 0.2715 - val_acc: 0.9352\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0912 - acc: 0.9705\n",
      "Epoch 00029: val_loss did not improve from 0.25547\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0911 - acc: 0.9705 - val_loss: 0.2884 - val_acc: 0.9280\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0856 - acc: 0.9718\n",
      "Epoch 00030: val_loss did not improve from 0.25547\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0856 - acc: 0.9718 - val_loss: 0.2612 - val_acc: 0.9338\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0804 - acc: 0.9739\n",
      "Epoch 00031: val_loss did not improve from 0.25547\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0804 - acc: 0.9739 - val_loss: 0.2802 - val_acc: 0.9371\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0766 - acc: 0.9743\n",
      "Epoch 00032: val_loss did not improve from 0.25547\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0766 - acc: 0.9744 - val_loss: 0.2679 - val_acc: 0.9397\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0774 - acc: 0.9738\n",
      "Epoch 00033: val_loss did not improve from 0.25547\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0774 - acc: 0.9738 - val_loss: 0.2960 - val_acc: 0.9336\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0683 - acc: 0.9775\n",
      "Epoch 00034: val_loss did not improve from 0.25547\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0684 - acc: 0.9775 - val_loss: 0.2725 - val_acc: 0.9380\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0674 - acc: 0.9776\n",
      "Epoch 00035: val_loss did not improve from 0.25547\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0674 - acc: 0.9776 - val_loss: 0.2770 - val_acc: 0.9415\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0610 - acc: 0.9802\n",
      "Epoch 00036: val_loss did not improve from 0.25547\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0610 - acc: 0.9802 - val_loss: 0.2933 - val_acc: 0.9320\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0604 - acc: 0.9796\n",
      "Epoch 00037: val_loss did not improve from 0.25547\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0604 - acc: 0.9796 - val_loss: 0.2832 - val_acc: 0.9385\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0585 - acc: 0.9803\n",
      "Epoch 00038: val_loss did not improve from 0.25547\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0585 - acc: 0.9803 - val_loss: 0.2855 - val_acc: 0.9406\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0524 - acc: 0.9836\n",
      "Epoch 00039: val_loss did not improve from 0.25547\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0523 - acc: 0.9836 - val_loss: 0.2965 - val_acc: 0.9408\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0551 - acc: 0.9817\n",
      "Epoch 00040: val_loss did not improve from 0.25547\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0551 - acc: 0.9817 - val_loss: 0.2772 - val_acc: 0.9369\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0544 - acc: 0.9824\n",
      "Epoch 00041: val_loss did not improve from 0.25547\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0544 - acc: 0.9824 - val_loss: 0.3012 - val_acc: 0.9399\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0545 - acc: 0.9822\n",
      "Epoch 00042: val_loss did not improve from 0.25547\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0544 - acc: 0.9822 - val_loss: 0.2904 - val_acc: 0.9385\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0471 - acc: 0.9844\n",
      "Epoch 00043: val_loss did not improve from 0.25547\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0471 - acc: 0.9844 - val_loss: 0.2879 - val_acc: 0.9376\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0453 - acc: 0.9856\n",
      "Epoch 00044: val_loss did not improve from 0.25547\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0453 - acc: 0.9856 - val_loss: 0.3056 - val_acc: 0.9331\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0440 - acc: 0.9860\n",
      "Epoch 00045: val_loss did not improve from 0.25547\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0439 - acc: 0.9860 - val_loss: 0.2979 - val_acc: 0.9373\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0429 - acc: 0.9861\n",
      "Epoch 00046: val_loss did not improve from 0.25547\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0429 - acc: 0.9861 - val_loss: 0.3282 - val_acc: 0.9243\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0400 - acc: 0.9872\n",
      "Epoch 00047: val_loss did not improve from 0.25547\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0400 - acc: 0.9872 - val_loss: 0.2932 - val_acc: 0.9383\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0418 - acc: 0.9865\n",
      "Epoch 00048: val_loss did not improve from 0.25547\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0418 - acc: 0.9865 - val_loss: 0.3198 - val_acc: 0.9390\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0388 - acc: 0.9873\n",
      "Epoch 00049: val_loss did not improve from 0.25547\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0388 - acc: 0.9873 - val_loss: 0.3143 - val_acc: 0.9378\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0410 - acc: 0.9866\n",
      "Epoch 00050: val_loss did not improve from 0.25547\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0410 - acc: 0.9866 - val_loss: 0.2977 - val_acc: 0.9383\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0318 - acc: 0.9900\n",
      "Epoch 00051: val_loss did not improve from 0.25547\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0318 - acc: 0.9900 - val_loss: 0.3200 - val_acc: 0.9394\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0349 - acc: 0.9892\n",
      "Epoch 00052: val_loss did not improve from 0.25547\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0349 - acc: 0.9892 - val_loss: 0.3095 - val_acc: 0.9418\n",
      "Epoch 53/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0323 - acc: 0.9897\n",
      "Epoch 00053: val_loss did not improve from 0.25547\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0323 - acc: 0.9897 - val_loss: 0.3507 - val_acc: 0.9338\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0326 - acc: 0.9902\n",
      "Epoch 00054: val_loss did not improve from 0.25547\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0326 - acc: 0.9902 - val_loss: 0.3142 - val_acc: 0.9369\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0317 - acc: 0.9900\n",
      "Epoch 00055: val_loss did not improve from 0.25547\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0317 - acc: 0.9900 - val_loss: 0.3020 - val_acc: 0.9362\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0344 - acc: 0.9889\n",
      "Epoch 00056: val_loss did not improve from 0.25547\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0344 - acc: 0.9889 - val_loss: 0.3507 - val_acc: 0.9355\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0268 - acc: 0.9909\n",
      "Epoch 00057: val_loss did not improve from 0.25547\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0268 - acc: 0.9909 - val_loss: 0.3275 - val_acc: 0.9394\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0300 - acc: 0.9905\n",
      "Epoch 00058: val_loss did not improve from 0.25547\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0300 - acc: 0.9905 - val_loss: 0.3117 - val_acc: 0.9397\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0331 - acc: 0.9896\n",
      "Epoch 00059: val_loss did not improve from 0.25547\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0331 - acc: 0.9896 - val_loss: 0.3284 - val_acc: 0.9397\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0252 - acc: 0.9921\n",
      "Epoch 00060: val_loss did not improve from 0.25547\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0252 - acc: 0.9921 - val_loss: 0.3779 - val_acc: 0.9306\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0300 - acc: 0.9906\n",
      "Epoch 00061: val_loss did not improve from 0.25547\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0300 - acc: 0.9906 - val_loss: 0.3202 - val_acc: 0.9413\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0283 - acc: 0.9906\n",
      "Epoch 00062: val_loss did not improve from 0.25547\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0283 - acc: 0.9906 - val_loss: 0.3284 - val_acc: 0.9392\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0247 - acc: 0.9924\n",
      "Epoch 00063: val_loss did not improve from 0.25547\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0247 - acc: 0.9924 - val_loss: 0.3396 - val_acc: 0.9320\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0269 - acc: 0.9919\n",
      "Epoch 00064: val_loss did not improve from 0.25547\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0269 - acc: 0.9919 - val_loss: 0.4349 - val_acc: 0.9227\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0265 - acc: 0.9914\n",
      "Epoch 00065: val_loss did not improve from 0.25547\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0265 - acc: 0.9914 - val_loss: 0.3493 - val_acc: 0.9432\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0232 - acc: 0.9932\n",
      "Epoch 00066: val_loss did not improve from 0.25547\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0232 - acc: 0.9932 - val_loss: 0.3695 - val_acc: 0.9401\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0242 - acc: 0.9924\n",
      "Epoch 00067: val_loss did not improve from 0.25547\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0242 - acc: 0.9924 - val_loss: 0.3662 - val_acc: 0.9383\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0248 - acc: 0.9923\n",
      "Epoch 00068: val_loss did not improve from 0.25547\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0248 - acc: 0.9923 - val_loss: 0.3429 - val_acc: 0.9411\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0224 - acc: 0.9927\n",
      "Epoch 00069: val_loss did not improve from 0.25547\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0224 - acc: 0.9927 - val_loss: 0.3352 - val_acc: 0.9380\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0211 - acc: 0.9935\n",
      "Epoch 00070: val_loss did not improve from 0.25547\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0211 - acc: 0.9935 - val_loss: 0.3592 - val_acc: 0.9387\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0241 - acc: 0.9926\n",
      "Epoch 00071: val_loss did not improve from 0.25547\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0241 - acc: 0.9926 - val_loss: 0.3308 - val_acc: 0.9390\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0241 - acc: 0.9925\n",
      "Epoch 00072: val_loss did not improve from 0.25547\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0241 - acc: 0.9925 - val_loss: 0.3213 - val_acc: 0.9415\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0235 - acc: 0.9923\n",
      "Epoch 00073: val_loss did not improve from 0.25547\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0235 - acc: 0.9923 - val_loss: 0.3511 - val_acc: 0.9343\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0223 - acc: 0.9930\n",
      "Epoch 00074: val_loss did not improve from 0.25547\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0223 - acc: 0.9930 - val_loss: 0.3574 - val_acc: 0.9383\n",
      "\n",
      "1D_CNN_custom_multi_2_concat_ch_128_DO_6_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 725us/sample - loss: 0.3025 - acc: 0.9155\n",
      "Loss: 0.302481050196714 Accuracy: 0.9154725\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9474 - acc: 0.3665\n",
      "Epoch 00001: val_loss improved from inf to 1.10666, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_DO_7_conv_checkpoint/001-1.1067.hdf5\n",
      "36805/36805 [==============================] - 68s 2ms/sample - loss: 1.9472 - acc: 0.3666 - val_loss: 1.1067 - val_acc: 0.6483\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0383 - acc: 0.6690\n",
      "Epoch 00002: val_loss improved from 1.10666 to 0.70650, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_DO_7_conv_checkpoint/002-0.7065.hdf5\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 1.0383 - acc: 0.6690 - val_loss: 0.7065 - val_acc: 0.7855\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7338 - acc: 0.7690\n",
      "Epoch 00003: val_loss improved from 0.70650 to 0.50539, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_DO_7_conv_checkpoint/003-0.5054.hdf5\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.7338 - acc: 0.7690 - val_loss: 0.5054 - val_acc: 0.8530\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5722 - acc: 0.8213\n",
      "Epoch 00004: val_loss improved from 0.50539 to 0.39620, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_DO_7_conv_checkpoint/004-0.3962.hdf5\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.5723 - acc: 0.8212 - val_loss: 0.3962 - val_acc: 0.8863\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4695 - acc: 0.8529\n",
      "Epoch 00005: val_loss improved from 0.39620 to 0.33009, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_DO_7_conv_checkpoint/005-0.3301.hdf5\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.4694 - acc: 0.8529 - val_loss: 0.3301 - val_acc: 0.9080\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3896 - acc: 0.8766\n",
      "Epoch 00006: val_loss improved from 0.33009 to 0.28457, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_DO_7_conv_checkpoint/006-0.2846.hdf5\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.3896 - acc: 0.8766 - val_loss: 0.2846 - val_acc: 0.9124\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3374 - acc: 0.8940\n",
      "Epoch 00007: val_loss improved from 0.28457 to 0.27225, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_DO_7_conv_checkpoint/007-0.2722.hdf5\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.3373 - acc: 0.8940 - val_loss: 0.2722 - val_acc: 0.9122\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3005 - acc: 0.9038\n",
      "Epoch 00008: val_loss improved from 0.27225 to 0.23740, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_DO_7_conv_checkpoint/008-0.2374.hdf5\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.3005 - acc: 0.9038 - val_loss: 0.2374 - val_acc: 0.9294\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2663 - acc: 0.9151\n",
      "Epoch 00009: val_loss did not improve from 0.23740\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.2664 - acc: 0.9150 - val_loss: 0.2452 - val_acc: 0.9278\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2411 - acc: 0.9236\n",
      "Epoch 00010: val_loss improved from 0.23740 to 0.19734, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_DO_7_conv_checkpoint/010-0.1973.hdf5\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.2411 - acc: 0.9236 - val_loss: 0.1973 - val_acc: 0.9392\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2236 - acc: 0.9287\n",
      "Epoch 00011: val_loss did not improve from 0.19734\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.2236 - acc: 0.9287 - val_loss: 0.2317 - val_acc: 0.9297\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2064 - acc: 0.9337\n",
      "Epoch 00012: val_loss improved from 0.19734 to 0.18281, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_DO_7_conv_checkpoint/012-0.1828.hdf5\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.2064 - acc: 0.9337 - val_loss: 0.1828 - val_acc: 0.9446\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1942 - acc: 0.9381\n",
      "Epoch 00013: val_loss did not improve from 0.18281\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.1942 - acc: 0.9381 - val_loss: 0.1921 - val_acc: 0.9432\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1751 - acc: 0.9445\n",
      "Epoch 00014: val_loss improved from 0.18281 to 0.17579, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_DO_7_conv_checkpoint/014-0.1758.hdf5\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.1751 - acc: 0.9445 - val_loss: 0.1758 - val_acc: 0.9474\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1647 - acc: 0.9462\n",
      "Epoch 00015: val_loss did not improve from 0.17579\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.1647 - acc: 0.9462 - val_loss: 0.1813 - val_acc: 0.9420\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1551 - acc: 0.9494\n",
      "Epoch 00016: val_loss did not improve from 0.17579\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.1552 - acc: 0.9494 - val_loss: 0.1800 - val_acc: 0.9462\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1431 - acc: 0.9540\n",
      "Epoch 00017: val_loss improved from 0.17579 to 0.17560, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_DO_7_conv_checkpoint/017-0.1756.hdf5\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.1432 - acc: 0.9539 - val_loss: 0.1756 - val_acc: 0.9476\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1345 - acc: 0.9557\n",
      "Epoch 00018: val_loss improved from 0.17560 to 0.15187, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_DO_7_conv_checkpoint/018-0.1519.hdf5\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.1345 - acc: 0.9557 - val_loss: 0.1519 - val_acc: 0.9518\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1219 - acc: 0.9585\n",
      "Epoch 00019: val_loss did not improve from 0.15187\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.1220 - acc: 0.9585 - val_loss: 0.1583 - val_acc: 0.9515\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1196 - acc: 0.9609\n",
      "Epoch 00020: val_loss did not improve from 0.15187\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.1196 - acc: 0.9609 - val_loss: 0.1914 - val_acc: 0.9467\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1094 - acc: 0.9638\n",
      "Epoch 00021: val_loss improved from 0.15187 to 0.14368, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_DO_7_conv_checkpoint/021-0.1437.hdf5\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.1094 - acc: 0.9638 - val_loss: 0.1437 - val_acc: 0.9590\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1017 - acc: 0.9660\n",
      "Epoch 00022: val_loss did not improve from 0.14368\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.1017 - acc: 0.9660 - val_loss: 0.1494 - val_acc: 0.9515\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0980 - acc: 0.9670\n",
      "Epoch 00023: val_loss did not improve from 0.14368\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0980 - acc: 0.9670 - val_loss: 0.1545 - val_acc: 0.9569\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0901 - acc: 0.9701\n",
      "Epoch 00024: val_loss did not improve from 0.14368\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0901 - acc: 0.9701 - val_loss: 0.1673 - val_acc: 0.9520\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0837 - acc: 0.9721\n",
      "Epoch 00025: val_loss did not improve from 0.14368\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0837 - acc: 0.9721 - val_loss: 0.1730 - val_acc: 0.9509\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0810 - acc: 0.9721\n",
      "Epoch 00026: val_loss did not improve from 0.14368\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0810 - acc: 0.9720 - val_loss: 0.1677 - val_acc: 0.9564\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0765 - acc: 0.9734\n",
      "Epoch 00027: val_loss did not improve from 0.14368\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0765 - acc: 0.9734 - val_loss: 0.1580 - val_acc: 0.9536\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0749 - acc: 0.9742\n",
      "Epoch 00028: val_loss did not improve from 0.14368\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0749 - acc: 0.9742 - val_loss: 0.1596 - val_acc: 0.9564\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0681 - acc: 0.9768\n",
      "Epoch 00029: val_loss did not improve from 0.14368\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0681 - acc: 0.9769 - val_loss: 0.1517 - val_acc: 0.9574\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0639 - acc: 0.9775\n",
      "Epoch 00030: val_loss did not improve from 0.14368\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0639 - acc: 0.9775 - val_loss: 0.1592 - val_acc: 0.9616\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0639 - acc: 0.9785\n",
      "Epoch 00031: val_loss did not improve from 0.14368\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0639 - acc: 0.9785 - val_loss: 0.1550 - val_acc: 0.9604\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0549 - acc: 0.9812\n",
      "Epoch 00032: val_loss did not improve from 0.14368\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0549 - acc: 0.9813 - val_loss: 0.1614 - val_acc: 0.9595\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0545 - acc: 0.9816\n",
      "Epoch 00033: val_loss did not improve from 0.14368\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0545 - acc: 0.9816 - val_loss: 0.1763 - val_acc: 0.9574\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0525 - acc: 0.9817\n",
      "Epoch 00034: val_loss did not improve from 0.14368\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0525 - acc: 0.9817 - val_loss: 0.1577 - val_acc: 0.9592\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0489 - acc: 0.9833\n",
      "Epoch 00035: val_loss did not improve from 0.14368\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0489 - acc: 0.9833 - val_loss: 0.1705 - val_acc: 0.9564\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0483 - acc: 0.9838\n",
      "Epoch 00036: val_loss did not improve from 0.14368\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0483 - acc: 0.9838 - val_loss: 0.1714 - val_acc: 0.9562\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0475 - acc: 0.9831\n",
      "Epoch 00037: val_loss did not improve from 0.14368\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0475 - acc: 0.9831 - val_loss: 0.1522 - val_acc: 0.9625\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0457 - acc: 0.9846\n",
      "Epoch 00038: val_loss did not improve from 0.14368\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0457 - acc: 0.9846 - val_loss: 0.1476 - val_acc: 0.9618\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0397 - acc: 0.9865\n",
      "Epoch 00039: val_loss did not improve from 0.14368\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0397 - acc: 0.9865 - val_loss: 0.1680 - val_acc: 0.9630\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0406 - acc: 0.9858\n",
      "Epoch 00040: val_loss did not improve from 0.14368\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0406 - acc: 0.9858 - val_loss: 0.1668 - val_acc: 0.9606\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0421 - acc: 0.9854\n",
      "Epoch 00041: val_loss did not improve from 0.14368\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0421 - acc: 0.9854 - val_loss: 0.2006 - val_acc: 0.9574\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0407 - acc: 0.9866\n",
      "Epoch 00042: val_loss did not improve from 0.14368\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0407 - acc: 0.9866 - val_loss: 0.1768 - val_acc: 0.9609\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0344 - acc: 0.9879\n",
      "Epoch 00043: val_loss did not improve from 0.14368\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0344 - acc: 0.9879 - val_loss: 0.1796 - val_acc: 0.9602\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0330 - acc: 0.9878\n",
      "Epoch 00044: val_loss did not improve from 0.14368\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0330 - acc: 0.9878 - val_loss: 0.1872 - val_acc: 0.9606\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0335 - acc: 0.9883\n",
      "Epoch 00045: val_loss did not improve from 0.14368\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0335 - acc: 0.9883 - val_loss: 0.1720 - val_acc: 0.9634\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0315 - acc: 0.9898\n",
      "Epoch 00046: val_loss did not improve from 0.14368\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0315 - acc: 0.9898 - val_loss: 0.1728 - val_acc: 0.9602\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0308 - acc: 0.9895\n",
      "Epoch 00047: val_loss did not improve from 0.14368\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0308 - acc: 0.9895 - val_loss: 0.1826 - val_acc: 0.9581\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0312 - acc: 0.9895\n",
      "Epoch 00048: val_loss did not improve from 0.14368\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0312 - acc: 0.9895 - val_loss: 0.1723 - val_acc: 0.9602\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0298 - acc: 0.9906\n",
      "Epoch 00049: val_loss did not improve from 0.14368\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0298 - acc: 0.9906 - val_loss: 0.1890 - val_acc: 0.9604\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0320 - acc: 0.9889\n",
      "Epoch 00050: val_loss did not improve from 0.14368\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0320 - acc: 0.9889 - val_loss: 0.1890 - val_acc: 0.9576\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0279 - acc: 0.9908\n",
      "Epoch 00051: val_loss did not improve from 0.14368\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0279 - acc: 0.9908 - val_loss: 0.1812 - val_acc: 0.9611\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0273 - acc: 0.9911\n",
      "Epoch 00052: val_loss did not improve from 0.14368\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0273 - acc: 0.9911 - val_loss: 0.1811 - val_acc: 0.9609\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0247 - acc: 0.9920\n",
      "Epoch 00053: val_loss did not improve from 0.14368\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0247 - acc: 0.9920 - val_loss: 0.1609 - val_acc: 0.9602\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0226 - acc: 0.9927\n",
      "Epoch 00054: val_loss did not improve from 0.14368\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0226 - acc: 0.9927 - val_loss: 0.2199 - val_acc: 0.9567\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0255 - acc: 0.9912\n",
      "Epoch 00055: val_loss did not improve from 0.14368\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0255 - acc: 0.9912 - val_loss: 0.1716 - val_acc: 0.9644\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0217 - acc: 0.9928\n",
      "Epoch 00056: val_loss did not improve from 0.14368\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0217 - acc: 0.9928 - val_loss: 0.2190 - val_acc: 0.9590\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0234 - acc: 0.9921\n",
      "Epoch 00057: val_loss did not improve from 0.14368\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0234 - acc: 0.9921 - val_loss: 0.1758 - val_acc: 0.9632\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0236 - acc: 0.9923\n",
      "Epoch 00058: val_loss did not improve from 0.14368\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0236 - acc: 0.9923 - val_loss: 0.1604 - val_acc: 0.9676\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0255 - acc: 0.9917\n",
      "Epoch 00059: val_loss did not improve from 0.14368\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0255 - acc: 0.9917 - val_loss: 0.1792 - val_acc: 0.9634\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0156 - acc: 0.9950\n",
      "Epoch 00060: val_loss did not improve from 0.14368\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0156 - acc: 0.9950 - val_loss: 0.1949 - val_acc: 0.9616\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0207 - acc: 0.9926\n",
      "Epoch 00061: val_loss did not improve from 0.14368\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0207 - acc: 0.9926 - val_loss: 0.2039 - val_acc: 0.9639\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0247 - acc: 0.9921\n",
      "Epoch 00062: val_loss did not improve from 0.14368\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0247 - acc: 0.9921 - val_loss: 0.2115 - val_acc: 0.9560\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0205 - acc: 0.9931\n",
      "Epoch 00063: val_loss did not improve from 0.14368\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0205 - acc: 0.9931 - val_loss: 0.1791 - val_acc: 0.9632\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0183 - acc: 0.9939\n",
      "Epoch 00064: val_loss did not improve from 0.14368\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0183 - acc: 0.9939 - val_loss: 0.1725 - val_acc: 0.9632\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0189 - acc: 0.9945\n",
      "Epoch 00065: val_loss did not improve from 0.14368\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0189 - acc: 0.9945 - val_loss: 0.1766 - val_acc: 0.9623\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0197 - acc: 0.9933\n",
      "Epoch 00066: val_loss did not improve from 0.14368\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0197 - acc: 0.9933 - val_loss: 0.1976 - val_acc: 0.9634\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0152 - acc: 0.9952\n",
      "Epoch 00067: val_loss did not improve from 0.14368\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0152 - acc: 0.9952 - val_loss: 0.1906 - val_acc: 0.9620\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0178 - acc: 0.9937\n",
      "Epoch 00068: val_loss did not improve from 0.14368\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0178 - acc: 0.9937 - val_loss: 0.2054 - val_acc: 0.9620\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0176 - acc: 0.9940\n",
      "Epoch 00069: val_loss did not improve from 0.14368\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0176 - acc: 0.9940 - val_loss: 0.1968 - val_acc: 0.9644\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0153 - acc: 0.9943\n",
      "Epoch 00070: val_loss did not improve from 0.14368\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0153 - acc: 0.9943 - val_loss: 0.1891 - val_acc: 0.9644\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0178 - acc: 0.9939\n",
      "Epoch 00071: val_loss did not improve from 0.14368\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 0.0178 - acc: 0.9939 - val_loss: 0.1971 - val_acc: 0.9620\n",
      "\n",
      "1D_CNN_custom_multi_2_concat_ch_128_DO_7_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 753us/sample - loss: 0.1918 - acc: 0.9456\n",
      "Loss: 0.19176398669509748 Accuracy: 0.9455867\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.7234 - acc: 0.4395\n",
      "Epoch 00001: val_loss improved from inf to 0.89131, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_DO_8_conv_checkpoint/001-0.8913.hdf5\n",
      "36805/36805 [==============================] - 70s 2ms/sample - loss: 1.7232 - acc: 0.4396 - val_loss: 0.8913 - val_acc: 0.7154\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8379 - acc: 0.7289\n",
      "Epoch 00002: val_loss improved from 0.89131 to 0.56734, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_DO_8_conv_checkpoint/002-0.5673.hdf5\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.8378 - acc: 0.7289 - val_loss: 0.5673 - val_acc: 0.8262\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5855 - acc: 0.8112\n",
      "Epoch 00003: val_loss improved from 0.56734 to 0.42477, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_DO_8_conv_checkpoint/003-0.4248.hdf5\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.5855 - acc: 0.8112 - val_loss: 0.4248 - val_acc: 0.8726\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4480 - acc: 0.8570\n",
      "Epoch 00004: val_loss improved from 0.42477 to 0.29366, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_DO_8_conv_checkpoint/004-0.2937.hdf5\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.4479 - acc: 0.8570 - val_loss: 0.2937 - val_acc: 0.9089\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3684 - acc: 0.8822\n",
      "Epoch 00005: val_loss improved from 0.29366 to 0.24642, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_DO_8_conv_checkpoint/005-0.2464.hdf5\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.3684 - acc: 0.8822 - val_loss: 0.2464 - val_acc: 0.9248\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3081 - acc: 0.9006\n",
      "Epoch 00006: val_loss improved from 0.24642 to 0.21393, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_DO_8_conv_checkpoint/006-0.2139.hdf5\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.3081 - acc: 0.9006 - val_loss: 0.2139 - val_acc: 0.9334\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2732 - acc: 0.9133\n",
      "Epoch 00007: val_loss improved from 0.21393 to 0.18841, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_DO_8_conv_checkpoint/007-0.1884.hdf5\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.2732 - acc: 0.9133 - val_loss: 0.1884 - val_acc: 0.9394\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2436 - acc: 0.9214\n",
      "Epoch 00008: val_loss did not improve from 0.18841\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.2437 - acc: 0.9214 - val_loss: 0.1890 - val_acc: 0.9408\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2176 - acc: 0.9293\n",
      "Epoch 00009: val_loss improved from 0.18841 to 0.16673, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_DO_8_conv_checkpoint/009-0.1667.hdf5\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.2176 - acc: 0.9293 - val_loss: 0.1667 - val_acc: 0.9495\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2007 - acc: 0.9348\n",
      "Epoch 00010: val_loss did not improve from 0.16673\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.2007 - acc: 0.9347 - val_loss: 0.1747 - val_acc: 0.9448\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1785 - acc: 0.9423\n",
      "Epoch 00011: val_loss did not improve from 0.16673\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.1785 - acc: 0.9423 - val_loss: 0.1714 - val_acc: 0.9471\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1678 - acc: 0.9442\n",
      "Epoch 00012: val_loss improved from 0.16673 to 0.15195, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_DO_8_conv_checkpoint/012-0.1520.hdf5\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.1678 - acc: 0.9442 - val_loss: 0.1520 - val_acc: 0.9522\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1527 - acc: 0.9499\n",
      "Epoch 00013: val_loss improved from 0.15195 to 0.14410, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_DO_8_conv_checkpoint/013-0.1441.hdf5\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.1527 - acc: 0.9499 - val_loss: 0.1441 - val_acc: 0.9557\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1389 - acc: 0.9539\n",
      "Epoch 00014: val_loss improved from 0.14410 to 0.14009, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_DO_8_conv_checkpoint/014-0.1401.hdf5\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.1389 - acc: 0.9539 - val_loss: 0.1401 - val_acc: 0.9553\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1272 - acc: 0.9585\n",
      "Epoch 00015: val_loss did not improve from 0.14009\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.1272 - acc: 0.9585 - val_loss: 0.1476 - val_acc: 0.9550\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1196 - acc: 0.9601\n",
      "Epoch 00016: val_loss did not improve from 0.14009\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.1196 - acc: 0.9601 - val_loss: 0.1685 - val_acc: 0.9483\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1131 - acc: 0.9621\n",
      "Epoch 00017: val_loss did not improve from 0.14009\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.1131 - acc: 0.9621 - val_loss: 0.1499 - val_acc: 0.9550\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1033 - acc: 0.9657\n",
      "Epoch 00018: val_loss did not improve from 0.14009\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.1033 - acc: 0.9657 - val_loss: 0.1436 - val_acc: 0.9553\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0940 - acc: 0.9693\n",
      "Epoch 00019: val_loss did not improve from 0.14009\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0940 - acc: 0.9693 - val_loss: 0.1512 - val_acc: 0.9571\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0889 - acc: 0.9691\n",
      "Epoch 00020: val_loss did not improve from 0.14009\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0889 - acc: 0.9691 - val_loss: 0.1561 - val_acc: 0.9555\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0858 - acc: 0.9708\n",
      "Epoch 00021: val_loss did not improve from 0.14009\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0858 - acc: 0.9708 - val_loss: 0.1435 - val_acc: 0.9592\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0767 - acc: 0.9735\n",
      "Epoch 00022: val_loss improved from 0.14009 to 0.13934, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_DO_8_conv_checkpoint/022-0.1393.hdf5\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0769 - acc: 0.9735 - val_loss: 0.1393 - val_acc: 0.9585\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0768 - acc: 0.9743\n",
      "Epoch 00023: val_loss did not improve from 0.13934\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0768 - acc: 0.9743 - val_loss: 0.1408 - val_acc: 0.9581\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0661 - acc: 0.9780\n",
      "Epoch 00024: val_loss did not improve from 0.13934\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0661 - acc: 0.9780 - val_loss: 0.1497 - val_acc: 0.9583\n",
      "Epoch 25/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0683 - acc: 0.9779\n",
      "Epoch 00025: val_loss did not improve from 0.13934\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0683 - acc: 0.9779 - val_loss: 0.1468 - val_acc: 0.9604\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0563 - acc: 0.9807\n",
      "Epoch 00026: val_loss did not improve from 0.13934\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0563 - acc: 0.9807 - val_loss: 0.1764 - val_acc: 0.9515\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0571 - acc: 0.9806\n",
      "Epoch 00027: val_loss did not improve from 0.13934\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0571 - acc: 0.9806 - val_loss: 0.1805 - val_acc: 0.9478\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0530 - acc: 0.9823\n",
      "Epoch 00028: val_loss improved from 0.13934 to 0.13818, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_ch_128_DO_8_conv_checkpoint/028-0.1382.hdf5\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0530 - acc: 0.9823 - val_loss: 0.1382 - val_acc: 0.9623\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0512 - acc: 0.9822\n",
      "Epoch 00029: val_loss did not improve from 0.13818\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0512 - acc: 0.9822 - val_loss: 0.1789 - val_acc: 0.9555\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0466 - acc: 0.9843\n",
      "Epoch 00030: val_loss did not improve from 0.13818\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0466 - acc: 0.9843 - val_loss: 0.1634 - val_acc: 0.9578\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0457 - acc: 0.9842\n",
      "Epoch 00031: val_loss did not improve from 0.13818\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0457 - acc: 0.9842 - val_loss: 0.1530 - val_acc: 0.9609\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0416 - acc: 0.9857\n",
      "Epoch 00032: val_loss did not improve from 0.13818\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0416 - acc: 0.9857 - val_loss: 0.1612 - val_acc: 0.9599\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0392 - acc: 0.9865\n",
      "Epoch 00033: val_loss did not improve from 0.13818\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0392 - acc: 0.9865 - val_loss: 0.1565 - val_acc: 0.9625\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0380 - acc: 0.9871\n",
      "Epoch 00034: val_loss did not improve from 0.13818\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0380 - acc: 0.9871 - val_loss: 0.1720 - val_acc: 0.9578\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0402 - acc: 0.9865\n",
      "Epoch 00035: val_loss did not improve from 0.13818\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0402 - acc: 0.9865 - val_loss: 0.1778 - val_acc: 0.9588\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0356 - acc: 0.9887\n",
      "Epoch 00036: val_loss did not improve from 0.13818\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0356 - acc: 0.9887 - val_loss: 0.1824 - val_acc: 0.9588\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0356 - acc: 0.9874\n",
      "Epoch 00037: val_loss did not improve from 0.13818\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0356 - acc: 0.9874 - val_loss: 0.1539 - val_acc: 0.9630\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0308 - acc: 0.9899\n",
      "Epoch 00038: val_loss did not improve from 0.13818\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0308 - acc: 0.9899 - val_loss: 0.1505 - val_acc: 0.9660\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0341 - acc: 0.9883\n",
      "Epoch 00039: val_loss did not improve from 0.13818\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0341 - acc: 0.9883 - val_loss: 0.1732 - val_acc: 0.9623\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0302 - acc: 0.9910\n",
      "Epoch 00040: val_loss did not improve from 0.13818\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0301 - acc: 0.9910 - val_loss: 0.1725 - val_acc: 0.9627\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0275 - acc: 0.9906\n",
      "Epoch 00041: val_loss did not improve from 0.13818\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0275 - acc: 0.9906 - val_loss: 0.1660 - val_acc: 0.9627\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0305 - acc: 0.9893\n",
      "Epoch 00042: val_loss did not improve from 0.13818\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0305 - acc: 0.9893 - val_loss: 0.1662 - val_acc: 0.9611\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0252 - acc: 0.9918\n",
      "Epoch 00043: val_loss did not improve from 0.13818\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0252 - acc: 0.9918 - val_loss: 0.1882 - val_acc: 0.9595\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0286 - acc: 0.9905\n",
      "Epoch 00044: val_loss did not improve from 0.13818\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0286 - acc: 0.9905 - val_loss: 0.2017 - val_acc: 0.9571\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0256 - acc: 0.9914\n",
      "Epoch 00045: val_loss did not improve from 0.13818\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0256 - acc: 0.9914 - val_loss: 0.1792 - val_acc: 0.9681\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0231 - acc: 0.9925\n",
      "Epoch 00046: val_loss did not improve from 0.13818\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0231 - acc: 0.9925 - val_loss: 0.1750 - val_acc: 0.9613\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0262 - acc: 0.9912\n",
      "Epoch 00047: val_loss did not improve from 0.13818\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0262 - acc: 0.9912 - val_loss: 0.1786 - val_acc: 0.9613\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0222 - acc: 0.9932\n",
      "Epoch 00048: val_loss did not improve from 0.13818\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0222 - acc: 0.9932 - val_loss: 0.1766 - val_acc: 0.9618\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0230 - acc: 0.9927\n",
      "Epoch 00049: val_loss did not improve from 0.13818\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0230 - acc: 0.9927 - val_loss: 0.1867 - val_acc: 0.9611\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0199 - acc: 0.9939\n",
      "Epoch 00050: val_loss did not improve from 0.13818\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0199 - acc: 0.9939 - val_loss: 0.2164 - val_acc: 0.9581\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0214 - acc: 0.9929\n",
      "Epoch 00051: val_loss did not improve from 0.13818\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0213 - acc: 0.9929 - val_loss: 0.1886 - val_acc: 0.9623\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0198 - acc: 0.9933\n",
      "Epoch 00052: val_loss did not improve from 0.13818\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0198 - acc: 0.9933 - val_loss: 0.1655 - val_acc: 0.9641\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0209 - acc: 0.9933\n",
      "Epoch 00053: val_loss did not improve from 0.13818\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0209 - acc: 0.9933 - val_loss: 0.2220 - val_acc: 0.9585\n",
      "Epoch 54/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0202 - acc: 0.9930\n",
      "Epoch 00054: val_loss did not improve from 0.13818\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0202 - acc: 0.9930 - val_loss: 0.2242 - val_acc: 0.9606\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0188 - acc: 0.9940\n",
      "Epoch 00055: val_loss did not improve from 0.13818\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0187 - acc: 0.9940 - val_loss: 0.2295 - val_acc: 0.9588\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0175 - acc: 0.9943\n",
      "Epoch 00056: val_loss did not improve from 0.13818\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0175 - acc: 0.9943 - val_loss: 0.2296 - val_acc: 0.9595\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0205 - acc: 0.9935\n",
      "Epoch 00057: val_loss did not improve from 0.13818\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0205 - acc: 0.9935 - val_loss: 0.2246 - val_acc: 0.9578\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0146 - acc: 0.9951\n",
      "Epoch 00058: val_loss did not improve from 0.13818\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0146 - acc: 0.9951 - val_loss: 0.1968 - val_acc: 0.9625\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0179 - acc: 0.9942\n",
      "Epoch 00059: val_loss did not improve from 0.13818\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0179 - acc: 0.9942 - val_loss: 0.1950 - val_acc: 0.9625\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0140 - acc: 0.9955\n",
      "Epoch 00060: val_loss did not improve from 0.13818\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0140 - acc: 0.9955 - val_loss: 0.2005 - val_acc: 0.9648\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0166 - acc: 0.9947\n",
      "Epoch 00061: val_loss did not improve from 0.13818\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0166 - acc: 0.9947 - val_loss: 0.1897 - val_acc: 0.9658\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0155 - acc: 0.9952\n",
      "Epoch 00062: val_loss did not improve from 0.13818\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0155 - acc: 0.9952 - val_loss: 0.2035 - val_acc: 0.9639\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0167 - acc: 0.9943\n",
      "Epoch 00063: val_loss did not improve from 0.13818\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0167 - acc: 0.9943 - val_loss: 0.2528 - val_acc: 0.9592\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0134 - acc: 0.9959\n",
      "Epoch 00064: val_loss did not improve from 0.13818\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0134 - acc: 0.9959 - val_loss: 0.1975 - val_acc: 0.9646\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0131 - acc: 0.9960\n",
      "Epoch 00065: val_loss did not improve from 0.13818\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0131 - acc: 0.9960 - val_loss: 0.2250 - val_acc: 0.9616\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0190 - acc: 0.9938\n",
      "Epoch 00066: val_loss did not improve from 0.13818\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0189 - acc: 0.9938 - val_loss: 0.1979 - val_acc: 0.9618\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0121 - acc: 0.9963\n",
      "Epoch 00067: val_loss did not improve from 0.13818\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0121 - acc: 0.9963 - val_loss: 0.2351 - val_acc: 0.9630\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0163 - acc: 0.9949\n",
      "Epoch 00068: val_loss did not improve from 0.13818\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0163 - acc: 0.9949 - val_loss: 0.2026 - val_acc: 0.9627\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0161 - acc: 0.9947\n",
      "Epoch 00069: val_loss did not improve from 0.13818\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0161 - acc: 0.9947 - val_loss: 0.1954 - val_acc: 0.9625\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0165 - acc: 0.9948\n",
      "Epoch 00070: val_loss did not improve from 0.13818\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0165 - acc: 0.9948 - val_loss: 0.2267 - val_acc: 0.9618\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0121 - acc: 0.9960\n",
      "Epoch 00071: val_loss did not improve from 0.13818\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0121 - acc: 0.9960 - val_loss: 0.1918 - val_acc: 0.9625\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0104 - acc: 0.9966\n",
      "Epoch 00072: val_loss did not improve from 0.13818\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0104 - acc: 0.9966 - val_loss: 0.2083 - val_acc: 0.9651\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0117 - acc: 0.9965\n",
      "Epoch 00073: val_loss did not improve from 0.13818\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0117 - acc: 0.9965 - val_loss: 0.2018 - val_acc: 0.9639\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0132 - acc: 0.9959\n",
      "Epoch 00074: val_loss did not improve from 0.13818\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0132 - acc: 0.9959 - val_loss: 0.1778 - val_acc: 0.9662\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0116 - acc: 0.9964\n",
      "Epoch 00075: val_loss did not improve from 0.13818\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0116 - acc: 0.9964 - val_loss: 0.2079 - val_acc: 0.9644\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0162 - acc: 0.9951\n",
      "Epoch 00076: val_loss did not improve from 0.13818\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0162 - acc: 0.9951 - val_loss: 0.2045 - val_acc: 0.9627\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0104 - acc: 0.9968\n",
      "Epoch 00077: val_loss did not improve from 0.13818\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0104 - acc: 0.9968 - val_loss: 0.2552 - val_acc: 0.9597\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0190 - acc: 0.9942\n",
      "Epoch 00078: val_loss did not improve from 0.13818\n",
      "36805/36805 [==============================] - 66s 2ms/sample - loss: 0.0190 - acc: 0.9942 - val_loss: 0.2135 - val_acc: 0.9632\n",
      "\n",
      "1D_CNN_custom_multi_2_concat_ch_128_DO_8_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 762us/sample - loss: 0.1959 - acc: 0.9479\n",
      "Loss: 0.19594085773903788 Accuracy: 0.9478712\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base = '1D_CNN_custom_multi_2_concat_ch_128_DO'\n",
    "\n",
    "for i in range(4, 9):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    model = build_cnn(conv_num=i)\n",
    "#         model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4),\n",
    "          metrics=['accuracy'])\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model_filename = model_path+'{epoch:03d}-{val_loss:.4f}.hdf5'\n",
    "    checkpointer = ModelCheckpoint(filepath = model_filename, monitor = \"val_loss\", \n",
    "                                   verbose=1, save_best_only=True)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=50)\n",
    "    hist = model.fit(x_train_abs, y_train_onehot, batch_size=64, epochs=500, \n",
    "                     validation_data=[x_val_abs, y_val_onehot], shuffle=True, \n",
    "                     callbacks = [checkpointer, early_stopping])\n",
    "\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "    ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "    ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "    ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    png_path = 'visualization/learning_curve/'\n",
    "    filename = model_name+'.png'\n",
    "    os.makedirs(png_path, exist_ok=True)\n",
    "    fig.savefig(png_path+filename, transparent=True)\n",
    "\n",
    "    model.save(model_path+'000_last.hdf5')\n",
    "    del(model)\n",
    "\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "    model = load_model(model_filename)\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "    print()\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_multi_2_concat_ch_128_DO_4_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_33 (Conv1D)              (None, 16000, 128)   768         input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 16000, 128)   0           conv1d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling1D) (None, 5333, 128)    0           activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_34 (Conv1D)              (None, 5333, 128)    82048       max_pooling1d_33[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 5333, 128)    0           conv1d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling1D) (None, 1777, 128)    0           activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_35 (Conv1D)              (None, 1777, 128)    82048       max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 1777, 128)    0           conv1d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling1D) (None, 592, 128)     0           activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_36 (Conv1D)              (None, 592, 128)     82048       max_pooling1d_35[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 592, 128)     0           conv1d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling1D) (None, 197, 128)     0           activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_12 (Flatten)            (None, 75776)        0           max_pooling1d_35[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_13 (Flatten)            (None, 25216)        0           max_pooling1d_36[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 100992)       0           flatten_12[0][0]                 \n",
      "                                                                 flatten_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 100992)       0           concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 16)           1615888     dropout_6[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,862,800\n",
      "Trainable params: 1,862,800\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 726us/sample - loss: 0.9640 - acc: 0.7130\n",
      "Loss: 0.9640262068493963 Accuracy: 0.7129803\n",
      "\n",
      "1D_CNN_custom_multi_2_concat_ch_128_DO_5_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_8 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_37 (Conv1D)              (None, 16000, 128)   768         input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 16000, 128)   0           conv1d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling1D) (None, 5333, 128)    0           activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_38 (Conv1D)              (None, 5333, 128)    82048       max_pooling1d_37[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 5333, 128)    0           conv1d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling1D) (None, 1777, 128)    0           activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_39 (Conv1D)              (None, 1777, 128)    82048       max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 1777, 128)    0           conv1d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling1D) (None, 592, 128)     0           activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_40 (Conv1D)              (None, 592, 128)     82048       max_pooling1d_39[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 592, 128)     0           conv1d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling1D) (None, 197, 128)     0           activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_41 (Conv1D)              (None, 197, 256)     164096      max_pooling1d_40[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 197, 256)     0           conv1d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling1D) (None, 65, 256)      0           activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_14 (Flatten)            (None, 25216)        0           max_pooling1d_40[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_15 (Flatten)            (None, 16640)        0           max_pooling1d_41[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 41856)        0           flatten_14[0][0]                 \n",
      "                                                                 flatten_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 41856)        0           concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 16)           669712      dropout_7[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,080,720\n",
      "Trainable params: 1,080,720\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 750us/sample - loss: 0.6069 - acc: 0.8474\n",
      "Loss: 0.6069400324380781 Accuracy: 0.847352\n",
      "\n",
      "1D_CNN_custom_multi_2_concat_ch_128_DO_6_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_42 (Conv1D)              (None, 16000, 128)   768         input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 16000, 128)   0           conv1d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling1D) (None, 5333, 128)    0           activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_43 (Conv1D)              (None, 5333, 128)    82048       max_pooling1d_42[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 5333, 128)    0           conv1d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling1D) (None, 1777, 128)    0           activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_44 (Conv1D)              (None, 1777, 128)    82048       max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 1777, 128)    0           conv1d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling1D) (None, 592, 128)     0           activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_45 (Conv1D)              (None, 592, 128)     82048       max_pooling1d_44[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 592, 128)     0           conv1d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling1D) (None, 197, 128)     0           activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_46 (Conv1D)              (None, 197, 256)     164096      max_pooling1d_45[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 197, 256)     0           conv1d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling1D) (None, 65, 256)      0           activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_47 (Conv1D)              (None, 65, 256)      327936      max_pooling1d_46[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 65, 256)      0           conv1d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling1D) (None, 21, 256)      0           activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_16 (Flatten)            (None, 16640)        0           max_pooling1d_46[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_17 (Flatten)            (None, 5376)         0           max_pooling1d_47[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 22016)        0           flatten_16[0][0]                 \n",
      "                                                                 flatten_17[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 22016)        0           concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 16)           352272      dropout_8[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,091,216\n",
      "Trainable params: 1,091,216\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 764us/sample - loss: 0.3025 - acc: 0.9155\n",
      "Loss: 0.302481050196714 Accuracy: 0.9154725\n",
      "\n",
      "1D_CNN_custom_multi_2_concat_ch_128_DO_7_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_10 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_48 (Conv1D)              (None, 16000, 128)   768         input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 16000, 128)   0           conv1d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling1D) (None, 5333, 128)    0           activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_49 (Conv1D)              (None, 5333, 128)    82048       max_pooling1d_48[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 5333, 128)    0           conv1d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling1D) (None, 1777, 128)    0           activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_50 (Conv1D)              (None, 1777, 128)    82048       max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 1777, 128)    0           conv1d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling1D) (None, 592, 128)     0           activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_51 (Conv1D)              (None, 592, 128)     82048       max_pooling1d_50[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 592, 128)     0           conv1d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling1D) (None, 197, 128)     0           activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_52 (Conv1D)              (None, 197, 256)     164096      max_pooling1d_51[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 197, 256)     0           conv1d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling1D) (None, 65, 256)      0           activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_53 (Conv1D)              (None, 65, 256)      327936      max_pooling1d_52[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 65, 256)      0           conv1d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling1D) (None, 21, 256)      0           activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_54 (Conv1D)              (None, 21, 256)      327936      max_pooling1d_53[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 21, 256)      0           conv1d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling1D) (None, 7, 256)       0           activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_18 (Flatten)            (None, 5376)         0           max_pooling1d_53[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_19 (Flatten)            (None, 1792)         0           max_pooling1d_54[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 7168)         0           flatten_18[0][0]                 \n",
      "                                                                 flatten_19[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 7168)         0           concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 16)           114704      dropout_9[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,181,584\n",
      "Trainable params: 1,181,584\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 779us/sample - loss: 0.1918 - acc: 0.9456\n",
      "Loss: 0.19176398669509748 Accuracy: 0.9455867\n",
      "\n",
      "1D_CNN_custom_multi_2_concat_ch_128_DO_8_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_55 (Conv1D)              (None, 16000, 128)   768         input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 16000, 128)   0           conv1d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling1D) (None, 5333, 128)    0           activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_56 (Conv1D)              (None, 5333, 128)    82048       max_pooling1d_55[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 5333, 128)    0           conv1d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling1D) (None, 1777, 128)    0           activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_57 (Conv1D)              (None, 1777, 128)    82048       max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 1777, 128)    0           conv1d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling1D) (None, 592, 128)     0           activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_58 (Conv1D)              (None, 592, 128)     82048       max_pooling1d_57[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 592, 128)     0           conv1d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling1D) (None, 197, 128)     0           activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_59 (Conv1D)              (None, 197, 256)     164096      max_pooling1d_58[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 197, 256)     0           conv1d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling1D) (None, 65, 256)      0           activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_60 (Conv1D)              (None, 65, 256)      327936      max_pooling1d_59[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 65, 256)      0           conv1d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling1D) (None, 21, 256)      0           activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_61 (Conv1D)              (None, 21, 256)      327936      max_pooling1d_60[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 21, 256)      0           conv1d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling1D) (None, 7, 256)       0           activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_62 (Conv1D)              (None, 7, 256)       327936      max_pooling1d_61[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 7, 256)       0           conv1d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling1D) (None, 2, 256)       0           activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_20 (Flatten)            (None, 1792)         0           max_pooling1d_61[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_21 (Flatten)            (None, 512)          0           max_pooling1d_62[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 2304)         0           flatten_20[0][0]                 \n",
      "                                                                 flatten_21[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 2304)         0           concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 16)           36880       dropout_10[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 1,431,696\n",
      "Trainable params: 1,431,696\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 797us/sample - loss: 0.1959 - acc: 0.9479\n",
      "Loss: 0.19594085773903788 Accuracy: 0.9478712\n"
     ]
    }
   ],
   "source": [
    "log_dir = 'log'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "base = '1D_CNN_custom_multi_2_concat_ch_128_DO'\n",
    "\n",
    "with open(path.join(log_dir, base), 'w') as log_file:\n",
    "    for i in range(4, 9):\n",
    "        model_name = base+'_{}_conv'.format(i)\n",
    "        print()\n",
    "        print(model_name, 'Model')\n",
    "        model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "        model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "\n",
    "        model = load_model(model_filename)\n",
    "        model.summary()\n",
    "\n",
    "        [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "        print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "        del(model)\n",
    "\n",
    "        log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_multi_2_concat_ch_128_DO_4_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_33 (Conv1D)              (None, 16000, 128)   768         input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 16000, 128)   0           conv1d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling1D) (None, 5333, 128)    0           activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_34 (Conv1D)              (None, 5333, 128)    82048       max_pooling1d_33[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 5333, 128)    0           conv1d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling1D) (None, 1777, 128)    0           activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_35 (Conv1D)              (None, 1777, 128)    82048       max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 1777, 128)    0           conv1d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling1D) (None, 592, 128)     0           activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_36 (Conv1D)              (None, 592, 128)     82048       max_pooling1d_35[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 592, 128)     0           conv1d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling1D) (None, 197, 128)     0           activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_12 (Flatten)            (None, 75776)        0           max_pooling1d_35[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_13 (Flatten)            (None, 25216)        0           max_pooling1d_36[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 100992)       0           flatten_12[0][0]                 \n",
      "                                                                 flatten_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 100992)       0           concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 16)           1615888     dropout_6[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,862,800\n",
      "Trainable params: 1,862,800\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 744us/sample - loss: 1.5699 - acc: 0.7520\n",
      "Loss: 1.5698519917663385 Accuracy: 0.75202495\n",
      "\n",
      "1D_CNN_custom_multi_2_concat_ch_128_DO_5_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_8 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_37 (Conv1D)              (None, 16000, 128)   768         input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 16000, 128)   0           conv1d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling1D) (None, 5333, 128)    0           activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_38 (Conv1D)              (None, 5333, 128)    82048       max_pooling1d_37[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 5333, 128)    0           conv1d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling1D) (None, 1777, 128)    0           activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_39 (Conv1D)              (None, 1777, 128)    82048       max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 1777, 128)    0           conv1d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling1D) (None, 592, 128)     0           activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_40 (Conv1D)              (None, 592, 128)     82048       max_pooling1d_39[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 592, 128)     0           conv1d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling1D) (None, 197, 128)     0           activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_41 (Conv1D)              (None, 197, 256)     164096      max_pooling1d_40[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 197, 256)     0           conv1d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling1D) (None, 65, 256)      0           activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_14 (Flatten)            (None, 25216)        0           max_pooling1d_40[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_15 (Flatten)            (None, 16640)        0           max_pooling1d_41[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 41856)        0           flatten_14[0][0]                 \n",
      "                                                                 flatten_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 41856)        0           concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 16)           669712      dropout_7[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,080,720\n",
      "Trainable params: 1,080,720\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 776us/sample - loss: 0.8642 - acc: 0.8596\n",
      "Loss: 0.8641717162087699 Accuracy: 0.8596054\n",
      "\n",
      "1D_CNN_custom_multi_2_concat_ch_128_DO_6_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_42 (Conv1D)              (None, 16000, 128)   768         input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 16000, 128)   0           conv1d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling1D) (None, 5333, 128)    0           activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_43 (Conv1D)              (None, 5333, 128)    82048       max_pooling1d_42[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 5333, 128)    0           conv1d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling1D) (None, 1777, 128)    0           activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_44 (Conv1D)              (None, 1777, 128)    82048       max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 1777, 128)    0           conv1d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling1D) (None, 592, 128)     0           activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_45 (Conv1D)              (None, 592, 128)     82048       max_pooling1d_44[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 592, 128)     0           conv1d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling1D) (None, 197, 128)     0           activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_46 (Conv1D)              (None, 197, 256)     164096      max_pooling1d_45[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 197, 256)     0           conv1d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling1D) (None, 65, 256)      0           activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_47 (Conv1D)              (None, 65, 256)      327936      max_pooling1d_46[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 65, 256)      0           conv1d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling1D) (None, 21, 256)      0           activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_16 (Flatten)            (None, 16640)        0           max_pooling1d_46[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_17 (Flatten)            (None, 5376)         0           max_pooling1d_47[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 22016)        0           flatten_16[0][0]                 \n",
      "                                                                 flatten_17[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 22016)        0           concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 16)           352272      dropout_8[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,091,216\n",
      "Trainable params: 1,091,216\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 801us/sample - loss: 0.4151 - acc: 0.9240\n",
      "Loss: 0.4150888350120091 Accuracy: 0.92398757\n",
      "\n",
      "1D_CNN_custom_multi_2_concat_ch_128_DO_7_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_10 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_48 (Conv1D)              (None, 16000, 128)   768         input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 16000, 128)   0           conv1d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling1D) (None, 5333, 128)    0           activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_49 (Conv1D)              (None, 5333, 128)    82048       max_pooling1d_48[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 5333, 128)    0           conv1d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling1D) (None, 1777, 128)    0           activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_50 (Conv1D)              (None, 1777, 128)    82048       max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 1777, 128)    0           conv1d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling1D) (None, 592, 128)     0           activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_51 (Conv1D)              (None, 592, 128)     82048       max_pooling1d_50[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 592, 128)     0           conv1d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling1D) (None, 197, 128)     0           activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_52 (Conv1D)              (None, 197, 256)     164096      max_pooling1d_51[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 197, 256)     0           conv1d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling1D) (None, 65, 256)      0           activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_53 (Conv1D)              (None, 65, 256)      327936      max_pooling1d_52[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 65, 256)      0           conv1d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling1D) (None, 21, 256)      0           activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_54 (Conv1D)              (None, 21, 256)      327936      max_pooling1d_53[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 21, 256)      0           conv1d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling1D) (None, 7, 256)       0           activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_18 (Flatten)            (None, 5376)         0           max_pooling1d_53[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_19 (Flatten)            (None, 1792)         0           max_pooling1d_54[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 7168)         0           flatten_18[0][0]                 \n",
      "                                                                 flatten_19[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 7168)         0           concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 16)           114704      dropout_9[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,181,584\n",
      "Trainable params: 1,181,584\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 834us/sample - loss: 0.2516 - acc: 0.9518\n",
      "Loss: 0.25164901005074686 Accuracy: 0.9518172\n",
      "\n",
      "1D_CNN_custom_multi_2_concat_ch_128_DO_8_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_55 (Conv1D)              (None, 16000, 128)   768         input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 16000, 128)   0           conv1d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling1D) (None, 5333, 128)    0           activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_56 (Conv1D)              (None, 5333, 128)    82048       max_pooling1d_55[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 5333, 128)    0           conv1d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling1D) (None, 1777, 128)    0           activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_57 (Conv1D)              (None, 1777, 128)    82048       max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 1777, 128)    0           conv1d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling1D) (None, 592, 128)     0           activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_58 (Conv1D)              (None, 592, 128)     82048       max_pooling1d_57[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 592, 128)     0           conv1d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling1D) (None, 197, 128)     0           activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_59 (Conv1D)              (None, 197, 256)     164096      max_pooling1d_58[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 197, 256)     0           conv1d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling1D) (None, 65, 256)      0           activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_60 (Conv1D)              (None, 65, 256)      327936      max_pooling1d_59[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 65, 256)      0           conv1d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling1D) (None, 21, 256)      0           activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_61 (Conv1D)              (None, 21, 256)      327936      max_pooling1d_60[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 21, 256)      0           conv1d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling1D) (None, 7, 256)       0           activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_62 (Conv1D)              (None, 7, 256)       327936      max_pooling1d_61[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 7, 256)       0           conv1d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling1D) (None, 2, 256)       0           activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_20 (Flatten)            (None, 1792)         0           max_pooling1d_61[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_21 (Flatten)            (None, 512)          0           max_pooling1d_62[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 2304)         0           flatten_20[0][0]                 \n",
      "                                                                 flatten_21[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 2304)         0           concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 16)           36880       dropout_10[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 1,431,696\n",
      "Trainable params: 1,431,696\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 814us/sample - loss: 0.2726 - acc: 0.9512\n",
      "Loss: 0.27263984684628206 Accuracy: 0.95119417\n"
     ]
    }
   ],
   "source": [
    "for i in range(4, 9):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + '000_last.hdf5'\n",
    "\n",
    "    model = load_model(model_filename)\n",
    "    model.summary()\n",
    "\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
