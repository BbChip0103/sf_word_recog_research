{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path as path\n",
    "import itertools\n",
    "from sklearn.preprocessing import maxabs_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Input, Dense, Activation, BatchNormalization, \\\n",
    "                                    Flatten, Conv1D, MaxPooling1D, Dropout, \\\n",
    "                                    Concatenate, GlobalMaxPool1D, GlobalAvgPool1D\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler, \\\n",
    "                                        EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = path.join('..', 'data')\n",
    "data_dir = path.join(base_dir, 'data_speech_commands_v0.02')\n",
    " \n",
    "train_txt = path.join(data_dir, 'wav_train_16words.txt')\n",
    "val_txt = path.join(data_dir, 'wav_validation_16words.txt')\n",
    "test_txt = path.join(data_dir, 'wav_test_16words.txt')\n",
    "\n",
    "train_data = np.load(path.join(data_dir, 'wav_train_data.npz'))\n",
    "val_data = np.load(path.join(data_dir, 'wav_validation_data.npz'))\n",
    "test_data = np.load(path.join(data_dir, 'wav_test_data.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36805, 16000, 1),\n",
       " (36805,),\n",
       " (4293, 16000, 1),\n",
       " (4293,),\n",
       " (4815, 16000, 1),\n",
       " (4815,),\n",
       " (16, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = train_data['x_train']\n",
    "y_train = train_data['y_train']\n",
    "x_val = val_data['x_val']\n",
    "y_val = val_data['y_val']\n",
    "x_test = test_data['x_test']\n",
    "y_test = test_data['y_test']\n",
    "y_table = test_data['table']\n",
    "\n",
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape, y_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_test[0].shape\n",
    "output_size = y_table.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_abs = np.asarray([maxabs_scale(wav) for wav in x_train])\n",
    "y_train_onehot = np.asarray([to_categorical(label, output_size) for label in y_train])\n",
    "del x_train, y_train\n",
    "\n",
    "x_val_abs = np.asarray([maxabs_scale(wav) for wav in x_val])\n",
    "y_val_onehot = np.asarray([to_categorical(label, output_size) for label in y_val])\n",
    "del x_val, y_val\n",
    "\n",
    "x_test_abs = np.asarray([maxabs_scale(wav) for wav in x_test])\n",
    "y_test_onehot = np.asarray([to_categorical(label, output_size) for label in y_test])\n",
    "del x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn(conv_num=1):\n",
    "    filter_size = 64\n",
    "\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    x = input_layer\n",
    "\n",
    "    layer_outputs = []\n",
    "    for i in range(conv_num):\n",
    "        x = Conv1D (kernel_size=5, filters=filter_size*(2**(i//4)), \n",
    "                          strides=1, padding='same')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('relu')(x)\n",
    "        x = MaxPooling1D(pool_size=3, strides=3)(x)\n",
    "        layer_outputs.append(x)    \n",
    "    \n",
    "    x = Concatenate()([GlobalAvgPool1D()(output) for output in layer_outputs[-3:]])\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(output_size, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 16000, 64)    384         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1 (BatchNo (None, 16000, 64)    256         conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 16000, 64)    0           batch_normalization_v1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D)    (None, 5333, 64)     0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 5333, 64)     20544       max_pooling1d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_1 (Batch (None, 5333, 64)     256         conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 5333, 64)     0           batch_normalization_v1_1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 1777, 64)     0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 1777, 64)     20544       max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_2 (Batch (None, 1777, 64)     256         conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 1777, 64)     0           batch_normalization_v1_2[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 592, 64)      0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d (Globa (None, 64)           0           max_pooling1d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glo (None, 64)           0           max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_2 (Glo (None, 64)           0           max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 192)          0           global_average_pooling1d[0][0]   \n",
      "                                                                 global_average_pooling1d_1[0][0] \n",
      "                                                                 global_average_pooling1d_2[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_3 (Batch (None, 192)          768         concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 16)           3088        batch_normalization_v1_3[0][0]   \n",
      "==================================================================================================\n",
      "Total params: 46,096\n",
      "Trainable params: 45,328\n",
      "Non-trainable params: 768\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 16000, 64)    384         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_4 (Batch (None, 16000, 64)    256         conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 16000, 64)    0           batch_normalization_v1_4[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 5333, 64)     0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 5333, 64)     20544       max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_5 (Batch (None, 5333, 64)     256         conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 5333, 64)     0           batch_normalization_v1_5[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 1777, 64)     0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 1777, 64)     20544       max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_6 (Batch (None, 1777, 64)     256         conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 1777, 64)     0           batch_normalization_v1_6[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 592, 64)      0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 592, 64)      20544       max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_7 (Batch (None, 592, 64)      256         conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 592, 64)      0           batch_normalization_v1_7[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1D)  (None, 197, 64)      0           activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_3 (Glo (None, 64)           0           max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_4 (Glo (None, 64)           0           max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_5 (Glo (None, 64)           0           max_pooling1d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 192)          0           global_average_pooling1d_3[0][0] \n",
      "                                                                 global_average_pooling1d_4[0][0] \n",
      "                                                                 global_average_pooling1d_5[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_8 (Batch (None, 192)          768         concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 16)           3088        batch_normalization_v1_8[0][0]   \n",
      "==================================================================================================\n",
      "Total params: 66,896\n",
      "Trainable params: 66,000\n",
      "Non-trainable params: 896\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 16000, 64)    384         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_9 (Batch (None, 16000, 64)    256         conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 16000, 64)    0           batch_normalization_v1_9[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1D)  (None, 5333, 64)     0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 5333, 64)     20544       max_pooling1d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_10 (Batc (None, 5333, 64)     256         conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 5333, 64)     0           batch_normalization_v1_10[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1D)  (None, 1777, 64)     0           activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 1777, 64)     20544       max_pooling1d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_11 (Batc (None, 1777, 64)     256         conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 1777, 64)     0           batch_normalization_v1_11[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1D)  (None, 592, 64)      0           activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_12 (Batc (None, 592, 64)      256         conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 592, 64)      0           batch_normalization_v1_12[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling1D) (None, 197, 64)      0           activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_13 (Batc (None, 197, 128)     512         conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 197, 128)     0           batch_normalization_v1_13[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling1D) (None, 65, 128)      0           activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_6 (Glo (None, 64)           0           max_pooling1d_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_7 (Glo (None, 64)           0           max_pooling1d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_8 (Glo (None, 128)          0           max_pooling1d_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 256)          0           global_average_pooling1d_6[0][0] \n",
      "                                                                 global_average_pooling1d_7[0][0] \n",
      "                                                                 global_average_pooling1d_8[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_14 (Batc (None, 256)          1024        concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 16)           4112        batch_normalization_v1_14[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 109,776\n",
      "Trainable params: 108,496\n",
      "Non-trainable params: 1,280\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 16000, 64)    384         input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_15 (Batc (None, 16000, 64)    256         conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_15[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling1D) (None, 5333, 64)     0           activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_16 (Batc (None, 5333, 64)     256         conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_16[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling1D) (None, 1777, 64)     0           activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_13[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_17 (Batc (None, 1777, 64)     256         conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_17[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling1D) (None, 592, 64)      0           activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_14[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_18 (Batc (None, 592, 64)      256         conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 592, 64)      0           batch_normalization_v1_18[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling1D) (None, 197, 64)      0           activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_15[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_19 (Batc (None, 197, 128)     512         conv1d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 197, 128)     0           batch_normalization_v1_19[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling1D) (None, 65, 128)      0           activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_17 (Conv1D)              (None, 65, 128)      82048       max_pooling1d_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_20 (Batc (None, 65, 128)      512         conv1d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 65, 128)      0           batch_normalization_v1_20[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling1D) (None, 21, 128)      0           activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_9 (Glo (None, 64)           0           max_pooling1d_15[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_10 (Gl (None, 128)          0           max_pooling1d_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_11 (Gl (None, 128)          0           max_pooling1d_17[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 320)          0           global_average_pooling1d_9[0][0] \n",
      "                                                                 global_average_pooling1d_10[0][0]\n",
      "                                                                 global_average_pooling1d_11[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_21 (Batc (None, 320)          1280        concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 16)           5136        batch_normalization_v1_21[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 193,616\n",
      "Trainable params: 191,952\n",
      "Non-trainable params: 1,664\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_18 (Conv1D)              (None, 16000, 64)    384         input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_22 (Batc (None, 16000, 64)    256         conv1d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_22[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling1D) (None, 5333, 64)     0           activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_19 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_18[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_23 (Batc (None, 5333, 64)     256         conv1d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_23[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling1D) (None, 1777, 64)     0           activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_20 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_19[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_24 (Batc (None, 1777, 64)     256         conv1d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_24[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling1D) (None, 592, 64)      0           activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_21 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_20[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_25 (Batc (None, 592, 64)      256         conv1d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 592, 64)      0           batch_normalization_v1_25[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling1D) (None, 197, 64)      0           activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_22 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_21[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_26 (Batc (None, 197, 128)     512         conv1d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 197, 128)     0           batch_normalization_v1_26[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling1D) (None, 65, 128)      0           activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_23 (Conv1D)              (None, 65, 128)      82048       max_pooling1d_22[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_27 (Batc (None, 65, 128)      512         conv1d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 65, 128)      0           batch_normalization_v1_27[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling1D) (None, 21, 128)      0           activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_24 (Conv1D)              (None, 21, 128)      82048       max_pooling1d_23[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_28 (Batc (None, 21, 128)      512         conv1d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 21, 128)      0           batch_normalization_v1_28[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling1D) (None, 7, 128)       0           activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_12 (Gl (None, 128)          0           max_pooling1d_22[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_13 (Gl (None, 128)          0           max_pooling1d_23[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_14 (Gl (None, 128)          0           max_pooling1d_24[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 384)          0           global_average_pooling1d_12[0][0]\n",
      "                                                                 global_average_pooling1d_13[0][0]\n",
      "                                                                 global_average_pooling1d_14[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_29 (Batc (None, 384)          1536        concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 16)           6160        batch_normalization_v1_29[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 277,456\n",
      "Trainable params: 275,408\n",
      "Non-trainable params: 2,048\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_25 (Conv1D)              (None, 16000, 64)    384         input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_30 (Batc (None, 16000, 64)    256         conv1d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_30[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling1D) (None, 5333, 64)     0           activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_26 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_25[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_31 (Batc (None, 5333, 64)     256         conv1d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_31[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_26 (MaxPooling1D) (None, 1777, 64)     0           activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_27 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_26[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_32 (Batc (None, 1777, 64)     256         conv1d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_32[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_27 (MaxPooling1D) (None, 592, 64)      0           activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_28 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_27[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_33 (Batc (None, 592, 64)      256         conv1d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 592, 64)      0           batch_normalization_v1_33[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_28 (MaxPooling1D) (None, 197, 64)      0           activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_29 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_28[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_34 (Batc (None, 197, 128)     512         conv1d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 197, 128)     0           batch_normalization_v1_34[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_29 (MaxPooling1D) (None, 65, 128)      0           activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_30 (Conv1D)              (None, 65, 128)      82048       max_pooling1d_29[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_35 (Batc (None, 65, 128)      512         conv1d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 65, 128)      0           batch_normalization_v1_35[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_30 (MaxPooling1D) (None, 21, 128)      0           activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_31 (Conv1D)              (None, 21, 128)      82048       max_pooling1d_30[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_36 (Batc (None, 21, 128)      512         conv1d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 21, 128)      0           batch_normalization_v1_36[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_31 (MaxPooling1D) (None, 7, 128)       0           activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_32 (Conv1D)              (None, 7, 128)       82048       max_pooling1d_31[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_37 (Batc (None, 7, 128)       512         conv1d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 7, 128)       0           batch_normalization_v1_37[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_32 (MaxPooling1D) (None, 2, 128)       0           activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_15 (Gl (None, 128)          0           max_pooling1d_30[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_16 (Gl (None, 128)          0           max_pooling1d_31[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_17 (Gl (None, 128)          0           max_pooling1d_32[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 384)          0           global_average_pooling1d_15[0][0]\n",
      "                                                                 global_average_pooling1d_16[0][0]\n",
      "                                                                 global_average_pooling1d_17[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_38 (Batc (None, 384)          1536        concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 16)           6160        batch_normalization_v1_38[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 360,016\n",
      "Trainable params: 357,712\n",
      "Non-trainable params: 2,304\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in range(3, 9):\n",
    "    model = build_cnn(conv_num=i)\n",
    "    model.summary()\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36805 samples, validate on 4293 samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9124 - acc: 0.4212\n",
      "Epoch 00001: val_loss improved from inf to 2.02754, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_BN_3_conv_checkpoint/001-2.0275.hdf5\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 1.9123 - acc: 0.4212 - val_loss: 2.0275 - val_acc: 0.3611\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4597 - acc: 0.5644\n",
      "Epoch 00002: val_loss improved from 2.02754 to 1.60478, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_BN_3_conv_checkpoint/002-1.6048.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.4597 - acc: 0.5643 - val_loss: 1.6048 - val_acc: 0.4764\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3139 - acc: 0.6132\n",
      "Epoch 00003: val_loss improved from 1.60478 to 1.43099, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_BN_3_conv_checkpoint/003-1.4310.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.3140 - acc: 0.6132 - val_loss: 1.4310 - val_acc: 0.5276\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2178 - acc: 0.6425\n",
      "Epoch 00004: val_loss improved from 1.43099 to 1.32916, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_BN_3_conv_checkpoint/004-1.3292.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.2178 - acc: 0.6425 - val_loss: 1.3292 - val_acc: 0.5938\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1506 - acc: 0.6639\n",
      "Epoch 00005: val_loss improved from 1.32916 to 1.18313, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_BN_3_conv_checkpoint/005-1.1831.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.1506 - acc: 0.6638 - val_loss: 1.1831 - val_acc: 0.6385\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0988 - acc: 0.6773\n",
      "Epoch 00006: val_loss did not improve from 1.18313\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.0987 - acc: 0.6773 - val_loss: 1.5263 - val_acc: 0.5241\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0591 - acc: 0.6919\n",
      "Epoch 00007: val_loss improved from 1.18313 to 1.17552, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_BN_3_conv_checkpoint/007-1.1755.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.0591 - acc: 0.6919 - val_loss: 1.1755 - val_acc: 0.6385\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0226 - acc: 0.7002\n",
      "Epoch 00008: val_loss improved from 1.17552 to 1.11599, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_BN_3_conv_checkpoint/008-1.1160.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 1.0228 - acc: 0.7001 - val_loss: 1.1160 - val_acc: 0.6462\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9975 - acc: 0.7095\n",
      "Epoch 00009: val_loss did not improve from 1.11599\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9975 - acc: 0.7095 - val_loss: 1.3451 - val_acc: 0.5344\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9760 - acc: 0.7152\n",
      "Epoch 00010: val_loss did not improve from 1.11599\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9759 - acc: 0.7152 - val_loss: 1.3547 - val_acc: 0.5390\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9549 - acc: 0.7212\n",
      "Epoch 00011: val_loss did not improve from 1.11599\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9549 - acc: 0.7212 - val_loss: 1.1637 - val_acc: 0.6266\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9362 - acc: 0.7271\n",
      "Epoch 00012: val_loss did not improve from 1.11599\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9363 - acc: 0.7271 - val_loss: 1.6165 - val_acc: 0.5001\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9179 - acc: 0.7321\n",
      "Epoch 00013: val_loss did not improve from 1.11599\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9179 - acc: 0.7321 - val_loss: 1.1721 - val_acc: 0.6219\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9033 - acc: 0.7371\n",
      "Epoch 00014: val_loss did not improve from 1.11599\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.9033 - acc: 0.7372 - val_loss: 1.2312 - val_acc: 0.6322\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8907 - acc: 0.7415\n",
      "Epoch 00015: val_loss did not improve from 1.11599\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.8907 - acc: 0.7414 - val_loss: 1.6687 - val_acc: 0.4764\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8781 - acc: 0.7472\n",
      "Epoch 00016: val_loss improved from 1.11599 to 0.92873, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_BN_3_conv_checkpoint/016-0.9287.hdf5\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.8781 - acc: 0.7473 - val_loss: 0.9287 - val_acc: 0.7165\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8623 - acc: 0.7496\n",
      "Epoch 00017: val_loss did not improve from 0.92873\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.8623 - acc: 0.7496 - val_loss: 1.1005 - val_acc: 0.6443\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8520 - acc: 0.7516\n",
      "Epoch 00018: val_loss did not improve from 0.92873\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.8521 - acc: 0.7515 - val_loss: 1.4381 - val_acc: 0.5586\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8448 - acc: 0.7536\n",
      "Epoch 00019: val_loss did not improve from 0.92873\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.8449 - acc: 0.7535 - val_loss: 0.9993 - val_acc: 0.6939\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8384 - acc: 0.7568\n",
      "Epoch 00020: val_loss did not improve from 0.92873\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.8385 - acc: 0.7568 - val_loss: 0.9727 - val_acc: 0.6972\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8256 - acc: 0.7594\n",
      "Epoch 00021: val_loss did not improve from 0.92873\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.8256 - acc: 0.7594 - val_loss: 1.0492 - val_acc: 0.6709\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8188 - acc: 0.7619\n",
      "Epoch 00022: val_loss did not improve from 0.92873\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.8188 - acc: 0.7619 - val_loss: 1.2767 - val_acc: 0.6359\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8091 - acc: 0.7636\n",
      "Epoch 00023: val_loss did not improve from 0.92873\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.8092 - acc: 0.7635 - val_loss: 1.1033 - val_acc: 0.6580\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8109 - acc: 0.7656\n",
      "Epoch 00024: val_loss did not improve from 0.92873\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.8109 - acc: 0.7656 - val_loss: 1.4608 - val_acc: 0.5567\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7966 - acc: 0.7651\n",
      "Epoch 00025: val_loss did not improve from 0.92873\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.7967 - acc: 0.7650 - val_loss: 2.1723 - val_acc: 0.3671\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7858 - acc: 0.7711\n",
      "Epoch 00026: val_loss did not improve from 0.92873\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.7857 - acc: 0.7711 - val_loss: 1.3777 - val_acc: 0.5558\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7830 - acc: 0.7724\n",
      "Epoch 00027: val_loss did not improve from 0.92873\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.7830 - acc: 0.7724 - val_loss: 1.3725 - val_acc: 0.5714\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7774 - acc: 0.7720\n",
      "Epoch 00028: val_loss did not improve from 0.92873\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.7775 - acc: 0.7720 - val_loss: 1.0588 - val_acc: 0.6578\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7661 - acc: 0.7775\n",
      "Epoch 00029: val_loss did not improve from 0.92873\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.7662 - acc: 0.7775 - val_loss: 1.7490 - val_acc: 0.5120\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7601 - acc: 0.7803\n",
      "Epoch 00030: val_loss did not improve from 0.92873\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.7602 - acc: 0.7803 - val_loss: 1.1487 - val_acc: 0.6604\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7573 - acc: 0.7788\n",
      "Epoch 00031: val_loss did not improve from 0.92873\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.7575 - acc: 0.7788 - val_loss: 1.4732 - val_acc: 0.5593\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7514 - acc: 0.7806\n",
      "Epoch 00032: val_loss did not improve from 0.92873\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.7514 - acc: 0.7806 - val_loss: 1.0618 - val_acc: 0.6422\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7432 - acc: 0.7820\n",
      "Epoch 00033: val_loss did not improve from 0.92873\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.7432 - acc: 0.7820 - val_loss: 1.0559 - val_acc: 0.6597\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7420 - acc: 0.7842\n",
      "Epoch 00034: val_loss did not improve from 0.92873\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.7422 - acc: 0.7842 - val_loss: 1.3009 - val_acc: 0.5828\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7341 - acc: 0.7877\n",
      "Epoch 00035: val_loss did not improve from 0.92873\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.7341 - acc: 0.7877 - val_loss: 1.7822 - val_acc: 0.5593\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7265 - acc: 0.7875\n",
      "Epoch 00036: val_loss did not improve from 0.92873\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.7266 - acc: 0.7874 - val_loss: 0.9289 - val_acc: 0.7070\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7269 - acc: 0.7892\n",
      "Epoch 00037: val_loss did not improve from 0.92873\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.7270 - acc: 0.7892 - val_loss: 1.5455 - val_acc: 0.5469\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7209 - acc: 0.7915\n",
      "Epoch 00038: val_loss did not improve from 0.92873\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.7211 - acc: 0.7914 - val_loss: 1.3103 - val_acc: 0.6103\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7177 - acc: 0.7932\n",
      "Epoch 00039: val_loss did not improve from 0.92873\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.7177 - acc: 0.7932 - val_loss: 1.6012 - val_acc: 0.5644\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7114 - acc: 0.7920\n",
      "Epoch 00040: val_loss did not improve from 0.92873\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.7114 - acc: 0.7920 - val_loss: 1.1872 - val_acc: 0.6336\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7086 - acc: 0.7937\n",
      "Epoch 00041: val_loss did not improve from 0.92873\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.7086 - acc: 0.7937 - val_loss: 1.3670 - val_acc: 0.5842\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7033 - acc: 0.7945\n",
      "Epoch 00042: val_loss did not improve from 0.92873\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.7034 - acc: 0.7945 - val_loss: 1.7860 - val_acc: 0.5174\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6965 - acc: 0.7958\n",
      "Epoch 00043: val_loss did not improve from 0.92873\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6969 - acc: 0.7957 - val_loss: 1.4705 - val_acc: 0.5966\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6979 - acc: 0.7976\n",
      "Epoch 00044: val_loss did not improve from 0.92873\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6982 - acc: 0.7976 - val_loss: 1.0334 - val_acc: 0.6860\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6906 - acc: 0.7987\n",
      "Epoch 00045: val_loss did not improve from 0.92873\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6909 - acc: 0.7986 - val_loss: 3.2550 - val_acc: 0.3543\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6834 - acc: 0.8011\n",
      "Epoch 00046: val_loss did not improve from 0.92873\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6833 - acc: 0.8011 - val_loss: 1.0172 - val_acc: 0.7039\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6828 - acc: 0.8024\n",
      "Epoch 00047: val_loss did not improve from 0.92873\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6829 - acc: 0.8023 - val_loss: 1.2751 - val_acc: 0.6247\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6842 - acc: 0.8009\n",
      "Epoch 00048: val_loss did not improve from 0.92873\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6843 - acc: 0.8009 - val_loss: 1.0662 - val_acc: 0.6636\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6782 - acc: 0.8038\n",
      "Epoch 00049: val_loss did not improve from 0.92873\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6781 - acc: 0.8038 - val_loss: 1.2487 - val_acc: 0.6301\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6676 - acc: 0.8051\n",
      "Epoch 00050: val_loss did not improve from 0.92873\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6677 - acc: 0.8051 - val_loss: 1.0035 - val_acc: 0.6872\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6683 - acc: 0.8064\n",
      "Epoch 00051: val_loss did not improve from 0.92873\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6683 - acc: 0.8065 - val_loss: 1.3080 - val_acc: 0.6033\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6648 - acc: 0.8067\n",
      "Epoch 00052: val_loss did not improve from 0.92873\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6648 - acc: 0.8067 - val_loss: 1.0332 - val_acc: 0.6935\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6627 - acc: 0.8073\n",
      "Epoch 00053: val_loss did not improve from 0.92873\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6629 - acc: 0.8073 - val_loss: 0.9678 - val_acc: 0.7023\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6632 - acc: 0.8061\n",
      "Epoch 00054: val_loss did not improve from 0.92873\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6633 - acc: 0.8061 - val_loss: 2.3009 - val_acc: 0.4440\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6571 - acc: 0.8090\n",
      "Epoch 00055: val_loss did not improve from 0.92873\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6572 - acc: 0.8089 - val_loss: 1.0517 - val_acc: 0.7088\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6559 - acc: 0.8092\n",
      "Epoch 00056: val_loss did not improve from 0.92873\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6559 - acc: 0.8092 - val_loss: 1.3568 - val_acc: 0.5877\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6467 - acc: 0.8106\n",
      "Epoch 00057: val_loss did not improve from 0.92873\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6468 - acc: 0.8106 - val_loss: 1.6058 - val_acc: 0.5416\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6500 - acc: 0.8104\n",
      "Epoch 00058: val_loss did not improve from 0.92873\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6501 - acc: 0.8104 - val_loss: 1.4574 - val_acc: 0.6382\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6497 - acc: 0.8092\n",
      "Epoch 00059: val_loss did not improve from 0.92873\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6500 - acc: 0.8091 - val_loss: 1.8601 - val_acc: 0.4910\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6477 - acc: 0.8110\n",
      "Epoch 00060: val_loss did not improve from 0.92873\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6478 - acc: 0.8109 - val_loss: 1.2582 - val_acc: 0.6087\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6377 - acc: 0.8133\n",
      "Epoch 00061: val_loss did not improve from 0.92873\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6378 - acc: 0.8133 - val_loss: 2.6388 - val_acc: 0.4465\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6354 - acc: 0.8138\n",
      "Epoch 00062: val_loss did not improve from 0.92873\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6355 - acc: 0.8137 - val_loss: 1.0937 - val_acc: 0.6695\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6316 - acc: 0.8137\n",
      "Epoch 00063: val_loss did not improve from 0.92873\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6315 - acc: 0.8137 - val_loss: 1.1111 - val_acc: 0.6597\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6313 - acc: 0.8153\n",
      "Epoch 00064: val_loss did not improve from 0.92873\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6313 - acc: 0.8154 - val_loss: 1.0067 - val_acc: 0.6944\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6285 - acc: 0.8186\n",
      "Epoch 00065: val_loss did not improve from 0.92873\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6285 - acc: 0.8186 - val_loss: 0.9363 - val_acc: 0.7233\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6248 - acc: 0.8168\n",
      "Epoch 00066: val_loss did not improve from 0.92873\n",
      "36805/36805 [==============================] - 52s 1ms/sample - loss: 0.6249 - acc: 0.8168 - val_loss: 1.2084 - val_acc: 0.6394\n",
      "\n",
      "1D_CNN_custom_multi_3_GAP_BN_3_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 573us/sample - loss: 1.0163 - acc: 0.6735\n",
      "Loss: 1.016281790054971 Accuracy: 0.67352027\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.7143 - acc: 0.4824\n",
      "Epoch 00001: val_loss improved from inf to 1.90658, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_BN_4_conv_checkpoint/001-1.9066.hdf5\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 1.7143 - acc: 0.4824 - val_loss: 1.9066 - val_acc: 0.4095\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2996 - acc: 0.6175\n",
      "Epoch 00002: val_loss improved from 1.90658 to 1.33820, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_BN_4_conv_checkpoint/002-1.3382.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 1.2996 - acc: 0.6175 - val_loss: 1.3382 - val_acc: 0.5998\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1547 - acc: 0.6639\n",
      "Epoch 00003: val_loss did not improve from 1.33820\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 1.1547 - acc: 0.6639 - val_loss: 1.5521 - val_acc: 0.5104\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0543 - acc: 0.6940\n",
      "Epoch 00004: val_loss improved from 1.33820 to 1.12409, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_BN_4_conv_checkpoint/004-1.1241.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 1.0543 - acc: 0.6940 - val_loss: 1.1241 - val_acc: 0.6676\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9884 - acc: 0.7143\n",
      "Epoch 00005: val_loss did not improve from 1.12409\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.9884 - acc: 0.7143 - val_loss: 1.1613 - val_acc: 0.6327\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9360 - acc: 0.7314\n",
      "Epoch 00006: val_loss improved from 1.12409 to 1.07952, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_BN_4_conv_checkpoint/006-1.0795.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.9360 - acc: 0.7314 - val_loss: 1.0795 - val_acc: 0.6592\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8924 - acc: 0.7439\n",
      "Epoch 00007: val_loss improved from 1.07952 to 0.97151, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_BN_4_conv_checkpoint/007-0.9715.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.8924 - acc: 0.7439 - val_loss: 0.9715 - val_acc: 0.7074\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8596 - acc: 0.7551\n",
      "Epoch 00008: val_loss did not improve from 0.97151\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.8596 - acc: 0.7551 - val_loss: 1.4615 - val_acc: 0.5486\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8237 - acc: 0.7642\n",
      "Epoch 00009: val_loss did not improve from 0.97151\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.8237 - acc: 0.7642 - val_loss: 1.1448 - val_acc: 0.6555\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8020 - acc: 0.7707\n",
      "Epoch 00010: val_loss did not improve from 0.97151\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.8019 - acc: 0.7707 - val_loss: 1.1985 - val_acc: 0.6606\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7809 - acc: 0.7746\n",
      "Epoch 00011: val_loss did not improve from 0.97151\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.7808 - acc: 0.7746 - val_loss: 1.1348 - val_acc: 0.6443\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7570 - acc: 0.7822\n",
      "Epoch 00012: val_loss improved from 0.97151 to 0.93669, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_BN_4_conv_checkpoint/012-0.9367.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.7569 - acc: 0.7822 - val_loss: 0.9367 - val_acc: 0.7156\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7414 - acc: 0.7863\n",
      "Epoch 00013: val_loss did not improve from 0.93669\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.7415 - acc: 0.7863 - val_loss: 1.7471 - val_acc: 0.4999\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7283 - acc: 0.7904\n",
      "Epoch 00014: val_loss did not improve from 0.93669\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.7283 - acc: 0.7904 - val_loss: 1.0180 - val_acc: 0.6723\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7126 - acc: 0.7956\n",
      "Epoch 00015: val_loss did not improve from 0.93669\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.7126 - acc: 0.7956 - val_loss: 1.0959 - val_acc: 0.6546\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6974 - acc: 0.7998\n",
      "Epoch 00016: val_loss improved from 0.93669 to 0.86793, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_BN_4_conv_checkpoint/016-0.8679.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.6974 - acc: 0.7999 - val_loss: 0.8679 - val_acc: 0.7447\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6849 - acc: 0.8061\n",
      "Epoch 00017: val_loss did not improve from 0.86793\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.6850 - acc: 0.8061 - val_loss: 1.0315 - val_acc: 0.6949\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6727 - acc: 0.8057\n",
      "Epoch 00018: val_loss did not improve from 0.86793\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.6727 - acc: 0.8057 - val_loss: 1.3503 - val_acc: 0.5926\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6595 - acc: 0.8099\n",
      "Epoch 00019: val_loss did not improve from 0.86793\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.6595 - acc: 0.8099 - val_loss: 1.3654 - val_acc: 0.5698\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6489 - acc: 0.8149\n",
      "Epoch 00020: val_loss did not improve from 0.86793\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.6488 - acc: 0.8149 - val_loss: 1.4371 - val_acc: 0.5639\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6363 - acc: 0.8164\n",
      "Epoch 00021: val_loss did not improve from 0.86793\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.6362 - acc: 0.8164 - val_loss: 0.8987 - val_acc: 0.7263\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6302 - acc: 0.8194\n",
      "Epoch 00022: val_loss did not improve from 0.86793\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.6304 - acc: 0.8193 - val_loss: 1.0239 - val_acc: 0.6958\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6300 - acc: 0.8198\n",
      "Epoch 00023: val_loss improved from 0.86793 to 0.81797, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_BN_4_conv_checkpoint/023-0.8180.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.6302 - acc: 0.8197 - val_loss: 0.8180 - val_acc: 0.7473\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6089 - acc: 0.8244\n",
      "Epoch 00024: val_loss did not improve from 0.81797\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.6089 - acc: 0.8244 - val_loss: 0.8824 - val_acc: 0.7431\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5999 - acc: 0.8279\n",
      "Epoch 00025: val_loss did not improve from 0.81797\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.5999 - acc: 0.8279 - val_loss: 0.9487 - val_acc: 0.7049\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5924 - acc: 0.8306\n",
      "Epoch 00026: val_loss did not improve from 0.81797\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.5925 - acc: 0.8306 - val_loss: 0.8310 - val_acc: 0.7449\n",
      "Epoch 27/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5860 - acc: 0.8315\n",
      "Epoch 00027: val_loss did not improve from 0.81797\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.5861 - acc: 0.8315 - val_loss: 0.9292 - val_acc: 0.7105\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5780 - acc: 0.8347\n",
      "Epoch 00028: val_loss did not improve from 0.81797\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.5781 - acc: 0.8346 - val_loss: 0.9241 - val_acc: 0.7258\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5718 - acc: 0.8380\n",
      "Epoch 00029: val_loss improved from 0.81797 to 0.77708, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_BN_4_conv_checkpoint/029-0.7771.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.5717 - acc: 0.8380 - val_loss: 0.7771 - val_acc: 0.7668\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5622 - acc: 0.8406\n",
      "Epoch 00030: val_loss did not improve from 0.77708\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.5624 - acc: 0.8405 - val_loss: 0.9767 - val_acc: 0.7060\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5555 - acc: 0.8405\n",
      "Epoch 00031: val_loss did not improve from 0.77708\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.5556 - acc: 0.8405 - val_loss: 0.9332 - val_acc: 0.7163\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5506 - acc: 0.8437\n",
      "Epoch 00032: val_loss did not improve from 0.77708\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.5507 - acc: 0.8437 - val_loss: 1.0691 - val_acc: 0.6944\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5416 - acc: 0.8438\n",
      "Epoch 00033: val_loss did not improve from 0.77708\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.5416 - acc: 0.8438 - val_loss: 1.0499 - val_acc: 0.6594\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5308 - acc: 0.8488\n",
      "Epoch 00034: val_loss improved from 0.77708 to 0.77122, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_BN_4_conv_checkpoint/034-0.7712.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.5309 - acc: 0.8488 - val_loss: 0.7712 - val_acc: 0.7640\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5268 - acc: 0.8488\n",
      "Epoch 00035: val_loss improved from 0.77122 to 0.68852, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_BN_4_conv_checkpoint/035-0.6885.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.5268 - acc: 0.8487 - val_loss: 0.6885 - val_acc: 0.8153\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5246 - acc: 0.8503\n",
      "Epoch 00036: val_loss did not improve from 0.68852\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.5246 - acc: 0.8503 - val_loss: 1.4656 - val_acc: 0.5802\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5172 - acc: 0.8525\n",
      "Epoch 00037: val_loss did not improve from 0.68852\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.5174 - acc: 0.8524 - val_loss: 1.5682 - val_acc: 0.5493\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5124 - acc: 0.8533\n",
      "Epoch 00038: val_loss did not improve from 0.68852\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.5127 - acc: 0.8532 - val_loss: 0.8988 - val_acc: 0.7382\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5085 - acc: 0.8539\n",
      "Epoch 00039: val_loss did not improve from 0.68852\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.5086 - acc: 0.8538 - val_loss: 1.0113 - val_acc: 0.6983\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5024 - acc: 0.8567\n",
      "Epoch 00040: val_loss did not improve from 0.68852\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.5025 - acc: 0.8567 - val_loss: 1.0104 - val_acc: 0.6825\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4919 - acc: 0.8575\n",
      "Epoch 00041: val_loss did not improve from 0.68852\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.4918 - acc: 0.8575 - val_loss: 0.7921 - val_acc: 0.7659\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4893 - acc: 0.8588\n",
      "Epoch 00042: val_loss did not improve from 0.68852\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.4893 - acc: 0.8588 - val_loss: 0.7872 - val_acc: 0.7633\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4805 - acc: 0.8637\n",
      "Epoch 00043: val_loss did not improve from 0.68852\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.4807 - acc: 0.8637 - val_loss: 1.2418 - val_acc: 0.6608\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4785 - acc: 0.8632\n",
      "Epoch 00044: val_loss did not improve from 0.68852\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.4785 - acc: 0.8631 - val_loss: 0.9474 - val_acc: 0.7261\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4738 - acc: 0.8639\n",
      "Epoch 00045: val_loss did not improve from 0.68852\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.4737 - acc: 0.8639 - val_loss: 0.9976 - val_acc: 0.7072\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4686 - acc: 0.8666\n",
      "Epoch 00046: val_loss did not improve from 0.68852\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.4687 - acc: 0.8666 - val_loss: 0.6962 - val_acc: 0.7952\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4631 - acc: 0.8652\n",
      "Epoch 00047: val_loss did not improve from 0.68852\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.4631 - acc: 0.8653 - val_loss: 0.7837 - val_acc: 0.7594\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4571 - acc: 0.8687\n",
      "Epoch 00048: val_loss did not improve from 0.68852\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.4571 - acc: 0.8688 - val_loss: 0.7474 - val_acc: 0.7782\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4527 - acc: 0.8707\n",
      "Epoch 00049: val_loss did not improve from 0.68852\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.4529 - acc: 0.8706 - val_loss: 0.7590 - val_acc: 0.7855\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4463 - acc: 0.8722\n",
      "Epoch 00050: val_loss did not improve from 0.68852\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.4462 - acc: 0.8722 - val_loss: 0.7965 - val_acc: 0.7647\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4436 - acc: 0.8740\n",
      "Epoch 00051: val_loss did not improve from 0.68852\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.4437 - acc: 0.8740 - val_loss: 1.4111 - val_acc: 0.6077\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4383 - acc: 0.8745\n",
      "Epoch 00052: val_loss did not improve from 0.68852\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.4383 - acc: 0.8744 - val_loss: 0.8139 - val_acc: 0.7610\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4342 - acc: 0.8769\n",
      "Epoch 00053: val_loss did not improve from 0.68852\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.4341 - acc: 0.8769 - val_loss: 0.8761 - val_acc: 0.7298\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4266 - acc: 0.8774\n",
      "Epoch 00054: val_loss did not improve from 0.68852\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.4265 - acc: 0.8775 - val_loss: 0.7456 - val_acc: 0.7834\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4258 - acc: 0.8785\n",
      "Epoch 00055: val_loss did not improve from 0.68852\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.4259 - acc: 0.8785 - val_loss: 0.8509 - val_acc: 0.7596\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4240 - acc: 0.8790\n",
      "Epoch 00056: val_loss did not improve from 0.68852\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.4241 - acc: 0.8790 - val_loss: 1.0035 - val_acc: 0.6990\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4201 - acc: 0.8810\n",
      "Epoch 00057: val_loss did not improve from 0.68852\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.4201 - acc: 0.8810 - val_loss: 0.8787 - val_acc: 0.7503\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4123 - acc: 0.8832\n",
      "Epoch 00058: val_loss did not improve from 0.68852\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.4123 - acc: 0.8831 - val_loss: 0.7493 - val_acc: 0.7845\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4086 - acc: 0.8826\n",
      "Epoch 00059: val_loss did not improve from 0.68852\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.4089 - acc: 0.8825 - val_loss: 0.7092 - val_acc: 0.7918\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4101 - acc: 0.8814\n",
      "Epoch 00060: val_loss did not improve from 0.68852\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.4100 - acc: 0.8815 - val_loss: 0.9675 - val_acc: 0.7174\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4025 - acc: 0.8844\n",
      "Epoch 00061: val_loss did not improve from 0.68852\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.4025 - acc: 0.8844 - val_loss: 1.2231 - val_acc: 0.6716\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3951 - acc: 0.8872\n",
      "Epoch 00062: val_loss did not improve from 0.68852\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3951 - acc: 0.8872 - val_loss: 0.7060 - val_acc: 0.7864\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3921 - acc: 0.8867\n",
      "Epoch 00063: val_loss did not improve from 0.68852\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3921 - acc: 0.8867 - val_loss: 1.1542 - val_acc: 0.6515\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3872 - acc: 0.8895\n",
      "Epoch 00064: val_loss did not improve from 0.68852\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3875 - acc: 0.8894 - val_loss: 0.7758 - val_acc: 0.7722\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3900 - acc: 0.8886\n",
      "Epoch 00065: val_loss did not improve from 0.68852\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3901 - acc: 0.8886 - val_loss: 1.0117 - val_acc: 0.7072\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3848 - acc: 0.8904\n",
      "Epoch 00066: val_loss did not improve from 0.68852\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3848 - acc: 0.8904 - val_loss: 0.7097 - val_acc: 0.7952\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3780 - acc: 0.8923\n",
      "Epoch 00067: val_loss did not improve from 0.68852\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3780 - acc: 0.8924 - val_loss: 0.8211 - val_acc: 0.7596\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3755 - acc: 0.8921\n",
      "Epoch 00068: val_loss did not improve from 0.68852\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3755 - acc: 0.8921 - val_loss: 0.8583 - val_acc: 0.7568\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3730 - acc: 0.8936\n",
      "Epoch 00069: val_loss improved from 0.68852 to 0.60053, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_BN_4_conv_checkpoint/069-0.6005.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3731 - acc: 0.8936 - val_loss: 0.6005 - val_acc: 0.8283\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3685 - acc: 0.8940\n",
      "Epoch 00070: val_loss did not improve from 0.60053\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3685 - acc: 0.8940 - val_loss: 1.2930 - val_acc: 0.6205\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3655 - acc: 0.8934\n",
      "Epoch 00071: val_loss did not improve from 0.60053\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3655 - acc: 0.8934 - val_loss: 0.7411 - val_acc: 0.7959\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3611 - acc: 0.8964\n",
      "Epoch 00072: val_loss did not improve from 0.60053\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3611 - acc: 0.8965 - val_loss: 0.9593 - val_acc: 0.7326\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3579 - acc: 0.8992\n",
      "Epoch 00073: val_loss did not improve from 0.60053\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3579 - acc: 0.8991 - val_loss: 0.9016 - val_acc: 0.7470\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3526 - acc: 0.8992\n",
      "Epoch 00074: val_loss did not improve from 0.60053\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3527 - acc: 0.8992 - val_loss: 1.7321 - val_acc: 0.5733\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3516 - acc: 0.8978\n",
      "Epoch 00075: val_loss did not improve from 0.60053\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3517 - acc: 0.8978 - val_loss: 1.2130 - val_acc: 0.6562\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3479 - acc: 0.9004\n",
      "Epoch 00076: val_loss did not improve from 0.60053\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3481 - acc: 0.9004 - val_loss: 0.6681 - val_acc: 0.8141\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3455 - acc: 0.9010\n",
      "Epoch 00077: val_loss did not improve from 0.60053\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3455 - acc: 0.9010 - val_loss: 0.6806 - val_acc: 0.8055\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3398 - acc: 0.9025\n",
      "Epoch 00078: val_loss did not improve from 0.60053\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3398 - acc: 0.9025 - val_loss: 0.9993 - val_acc: 0.7279\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3399 - acc: 0.9028\n",
      "Epoch 00079: val_loss did not improve from 0.60053\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3400 - acc: 0.9028 - val_loss: 0.8755 - val_acc: 0.7563\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3364 - acc: 0.9046\n",
      "Epoch 00080: val_loss did not improve from 0.60053\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3364 - acc: 0.9046 - val_loss: 0.8183 - val_acc: 0.7724\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3310 - acc: 0.9050\n",
      "Epoch 00081: val_loss did not improve from 0.60053\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3310 - acc: 0.9050 - val_loss: 1.6993 - val_acc: 0.5812\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3327 - acc: 0.9044\n",
      "Epoch 00082: val_loss did not improve from 0.60053\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3329 - acc: 0.9043 - val_loss: 0.6799 - val_acc: 0.8076\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3255 - acc: 0.9072\n",
      "Epoch 00083: val_loss did not improve from 0.60053\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3255 - acc: 0.9072 - val_loss: 0.6552 - val_acc: 0.8139\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3218 - acc: 0.9084\n",
      "Epoch 00084: val_loss did not improve from 0.60053\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3217 - acc: 0.9084 - val_loss: 1.2522 - val_acc: 0.6569\n",
      "Epoch 85/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3195 - acc: 0.9092\n",
      "Epoch 00085: val_loss did not improve from 0.60053\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3198 - acc: 0.9092 - val_loss: 0.6473 - val_acc: 0.8181\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3187 - acc: 0.9087\n",
      "Epoch 00086: val_loss did not improve from 0.60053\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3187 - acc: 0.9087 - val_loss: 0.8328 - val_acc: 0.7696\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3163 - acc: 0.9108\n",
      "Epoch 00087: val_loss did not improve from 0.60053\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3163 - acc: 0.9108 - val_loss: 0.6249 - val_acc: 0.8155\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3122 - acc: 0.9122\n",
      "Epoch 00088: val_loss did not improve from 0.60053\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3123 - acc: 0.9122 - val_loss: 0.9832 - val_acc: 0.7445\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3094 - acc: 0.9108\n",
      "Epoch 00089: val_loss did not improve from 0.60053\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3095 - acc: 0.9108 - val_loss: 1.0089 - val_acc: 0.7214\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3033 - acc: 0.9125\n",
      "Epoch 00090: val_loss did not improve from 0.60053\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3033 - acc: 0.9125 - val_loss: 0.6692 - val_acc: 0.8104\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3063 - acc: 0.9105\n",
      "Epoch 00091: val_loss did not improve from 0.60053\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3065 - acc: 0.9105 - val_loss: 0.7648 - val_acc: 0.7901\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3021 - acc: 0.9154\n",
      "Epoch 00092: val_loss did not improve from 0.60053\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3022 - acc: 0.9154 - val_loss: 0.6120 - val_acc: 0.8290\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2972 - acc: 0.9146\n",
      "Epoch 00093: val_loss did not improve from 0.60053\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2972 - acc: 0.9146 - val_loss: 0.8845 - val_acc: 0.7491\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2944 - acc: 0.9161\n",
      "Epoch 00094: val_loss did not improve from 0.60053\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2945 - acc: 0.9160 - val_loss: 0.9841 - val_acc: 0.7424\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2924 - acc: 0.9167\n",
      "Epoch 00095: val_loss did not improve from 0.60053\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2924 - acc: 0.9167 - val_loss: 1.0215 - val_acc: 0.7121\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2893 - acc: 0.9178\n",
      "Epoch 00096: val_loss did not improve from 0.60053\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2893 - acc: 0.9178 - val_loss: 1.3359 - val_acc: 0.6592\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2899 - acc: 0.9182\n",
      "Epoch 00097: val_loss did not improve from 0.60053\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2899 - acc: 0.9182 - val_loss: 0.8179 - val_acc: 0.7743\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2794 - acc: 0.9210\n",
      "Epoch 00098: val_loss did not improve from 0.60053\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2794 - acc: 0.9210 - val_loss: 0.9226 - val_acc: 0.7372\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2810 - acc: 0.9199\n",
      "Epoch 00099: val_loss did not improve from 0.60053\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2810 - acc: 0.9199 - val_loss: 0.6347 - val_acc: 0.8216\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2748 - acc: 0.9224\n",
      "Epoch 00100: val_loss did not improve from 0.60053\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2748 - acc: 0.9224 - val_loss: 0.8923 - val_acc: 0.7643\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2783 - acc: 0.9210\n",
      "Epoch 00101: val_loss did not improve from 0.60053\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2783 - acc: 0.9210 - val_loss: 1.0201 - val_acc: 0.7354\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2730 - acc: 0.9208\n",
      "Epoch 00102: val_loss did not improve from 0.60053\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2731 - acc: 0.9208 - val_loss: 0.8814 - val_acc: 0.7591\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2730 - acc: 0.9224\n",
      "Epoch 00103: val_loss did not improve from 0.60053\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2729 - acc: 0.9224 - val_loss: 0.9001 - val_acc: 0.7554\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2663 - acc: 0.9256\n",
      "Epoch 00104: val_loss did not improve from 0.60053\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2665 - acc: 0.9256 - val_loss: 1.4433 - val_acc: 0.6443\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2691 - acc: 0.9227\n",
      "Epoch 00105: val_loss did not improve from 0.60053\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2691 - acc: 0.9227 - val_loss: 0.6859 - val_acc: 0.8074\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2622 - acc: 0.9252\n",
      "Epoch 00106: val_loss did not improve from 0.60053\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2623 - acc: 0.9251 - val_loss: 0.9115 - val_acc: 0.7508\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2624 - acc: 0.9260\n",
      "Epoch 00107: val_loss did not improve from 0.60053\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2624 - acc: 0.9259 - val_loss: 0.9468 - val_acc: 0.7433\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2574 - acc: 0.9268\n",
      "Epoch 00108: val_loss did not improve from 0.60053\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2574 - acc: 0.9268 - val_loss: 0.8577 - val_acc: 0.7549\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2569 - acc: 0.9272\n",
      "Epoch 00109: val_loss did not improve from 0.60053\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2569 - acc: 0.9272 - val_loss: 1.0108 - val_acc: 0.7428\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2549 - acc: 0.9271\n",
      "Epoch 00110: val_loss did not improve from 0.60053\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2550 - acc: 0.9270 - val_loss: 0.9396 - val_acc: 0.7531\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2523 - acc: 0.9277\n",
      "Epoch 00111: val_loss did not improve from 0.60053\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2522 - acc: 0.9278 - val_loss: 0.9119 - val_acc: 0.7449\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2455 - acc: 0.9312\n",
      "Epoch 00112: val_loss did not improve from 0.60053\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2455 - acc: 0.9312 - val_loss: 0.8988 - val_acc: 0.7664\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2486 - acc: 0.9296\n",
      "Epoch 00113: val_loss did not improve from 0.60053\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2486 - acc: 0.9297 - val_loss: 0.9163 - val_acc: 0.7552\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2460 - acc: 0.9304\n",
      "Epoch 00114: val_loss did not improve from 0.60053\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2460 - acc: 0.9304 - val_loss: 0.8166 - val_acc: 0.7741\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2424 - acc: 0.9305\n",
      "Epoch 00115: val_loss did not improve from 0.60053\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2425 - acc: 0.9305 - val_loss: 0.6942 - val_acc: 0.8148\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2428 - acc: 0.9321\n",
      "Epoch 00116: val_loss did not improve from 0.60053\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2428 - acc: 0.9321 - val_loss: 1.3791 - val_acc: 0.6688\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2372 - acc: 0.9341\n",
      "Epoch 00117: val_loss did not improve from 0.60053\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2372 - acc: 0.9341 - val_loss: 0.9591 - val_acc: 0.7375\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2362 - acc: 0.9339\n",
      "Epoch 00118: val_loss did not improve from 0.60053\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2362 - acc: 0.9339 - val_loss: 1.2536 - val_acc: 0.7226\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2343 - acc: 0.9352\n",
      "Epoch 00119: val_loss did not improve from 0.60053\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2343 - acc: 0.9351 - val_loss: 0.7066 - val_acc: 0.8118\n",
      "\n",
      "1D_CNN_custom_multi_3_GAP_BN_4_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 628us/sample - loss: 0.7190 - acc: 0.7807\n",
      "Loss: 0.7190013960266906 Accuracy: 0.78068537\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5065 - acc: 0.5438\n",
      "Epoch 00001: val_loss improved from inf to 1.55653, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_BN_5_conv_checkpoint/001-1.5565.hdf5\n",
      "36805/36805 [==============================] - 60s 2ms/sample - loss: 1.5065 - acc: 0.5438 - val_loss: 1.5565 - val_acc: 0.5281\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0430 - acc: 0.6977\n",
      "Epoch 00002: val_loss improved from 1.55653 to 1.07221, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_BN_5_conv_checkpoint/002-1.0722.hdf5\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 1.0430 - acc: 0.6976 - val_loss: 1.0722 - val_acc: 0.6653\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8696 - acc: 0.7553\n",
      "Epoch 00003: val_loss improved from 1.07221 to 0.89546, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_BN_5_conv_checkpoint/003-0.8955.hdf5\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.8696 - acc: 0.7553 - val_loss: 0.8955 - val_acc: 0.7526\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7635 - acc: 0.7888\n",
      "Epoch 00004: val_loss did not improve from 0.89546\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.7635 - acc: 0.7888 - val_loss: 0.9248 - val_acc: 0.7140\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6815 - acc: 0.8136\n",
      "Epoch 00005: val_loss improved from 0.89546 to 0.65703, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_BN_5_conv_checkpoint/005-0.6570.hdf5\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.6816 - acc: 0.8135 - val_loss: 0.6570 - val_acc: 0.8234\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6227 - acc: 0.8302\n",
      "Epoch 00006: val_loss did not improve from 0.65703\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.6226 - acc: 0.8302 - val_loss: 0.7056 - val_acc: 0.8008\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5728 - acc: 0.8443\n",
      "Epoch 00007: val_loss did not improve from 0.65703\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.5731 - acc: 0.8443 - val_loss: 0.7496 - val_acc: 0.7792\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5362 - acc: 0.8533\n",
      "Epoch 00008: val_loss improved from 0.65703 to 0.62342, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_BN_5_conv_checkpoint/008-0.6234.hdf5\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.5362 - acc: 0.8533 - val_loss: 0.6234 - val_acc: 0.8239\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5029 - acc: 0.8639\n",
      "Epoch 00009: val_loss did not improve from 0.62342\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.5029 - acc: 0.8639 - val_loss: 0.6536 - val_acc: 0.8132\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4747 - acc: 0.8693\n",
      "Epoch 00010: val_loss improved from 0.62342 to 0.54025, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_BN_5_conv_checkpoint/010-0.5402.hdf5\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.4747 - acc: 0.8693 - val_loss: 0.5402 - val_acc: 0.8516\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4515 - acc: 0.8770\n",
      "Epoch 00011: val_loss did not improve from 0.54025\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.4515 - acc: 0.8771 - val_loss: 0.5959 - val_acc: 0.8062\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4299 - acc: 0.8827\n",
      "Epoch 00012: val_loss did not improve from 0.54025\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.4299 - acc: 0.8827 - val_loss: 0.5747 - val_acc: 0.8337\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4117 - acc: 0.8873\n",
      "Epoch 00013: val_loss improved from 0.54025 to 0.50571, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_BN_5_conv_checkpoint/013-0.5057.hdf5\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.4118 - acc: 0.8873 - val_loss: 0.5057 - val_acc: 0.8593\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3942 - acc: 0.8922\n",
      "Epoch 00014: val_loss improved from 0.50571 to 0.49484, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_BN_5_conv_checkpoint/014-0.4948.hdf5\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.3942 - acc: 0.8922 - val_loss: 0.4948 - val_acc: 0.8640\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3771 - acc: 0.8965\n",
      "Epoch 00015: val_loss did not improve from 0.49484\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.3771 - acc: 0.8965 - val_loss: 0.5790 - val_acc: 0.8188\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3643 - acc: 0.9005\n",
      "Epoch 00016: val_loss did not improve from 0.49484\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.3643 - acc: 0.9005 - val_loss: 0.5517 - val_acc: 0.8290\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3527 - acc: 0.9032\n",
      "Epoch 00017: val_loss did not improve from 0.49484\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.3527 - acc: 0.9032 - val_loss: 0.5145 - val_acc: 0.8430\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3398 - acc: 0.9074\n",
      "Epoch 00018: val_loss improved from 0.49484 to 0.47543, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_BN_5_conv_checkpoint/018-0.4754.hdf5\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.3398 - acc: 0.9074 - val_loss: 0.4754 - val_acc: 0.8693\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3266 - acc: 0.9109\n",
      "Epoch 00019: val_loss improved from 0.47543 to 0.46285, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_BN_5_conv_checkpoint/019-0.4629.hdf5\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.3266 - acc: 0.9109 - val_loss: 0.4629 - val_acc: 0.8705\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3174 - acc: 0.9132\n",
      "Epoch 00020: val_loss did not improve from 0.46285\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.3176 - acc: 0.9131 - val_loss: 0.4634 - val_acc: 0.8658\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3106 - acc: 0.9145\n",
      "Epoch 00021: val_loss did not improve from 0.46285\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.3106 - acc: 0.9145 - val_loss: 0.4828 - val_acc: 0.8577\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2998 - acc: 0.9174\n",
      "Epoch 00022: val_loss improved from 0.46285 to 0.44457, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_BN_5_conv_checkpoint/022-0.4446.hdf5\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.2998 - acc: 0.9174 - val_loss: 0.4446 - val_acc: 0.8705\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2914 - acc: 0.9191\n",
      "Epoch 00023: val_loss improved from 0.44457 to 0.39917, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_BN_5_conv_checkpoint/023-0.3992.hdf5\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.2915 - acc: 0.9191 - val_loss: 0.3992 - val_acc: 0.8940\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2812 - acc: 0.9216\n",
      "Epoch 00024: val_loss did not improve from 0.39917\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.2812 - acc: 0.9216 - val_loss: 0.4235 - val_acc: 0.8798\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2768 - acc: 0.9230\n",
      "Epoch 00025: val_loss did not improve from 0.39917\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.2768 - acc: 0.9229 - val_loss: 0.4620 - val_acc: 0.8607\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2658 - acc: 0.9261\n",
      "Epoch 00026: val_loss improved from 0.39917 to 0.38632, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_BN_5_conv_checkpoint/026-0.3863.hdf5\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.2661 - acc: 0.9260 - val_loss: 0.3863 - val_acc: 0.9017\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2608 - acc: 0.9276\n",
      "Epoch 00027: val_loss did not improve from 0.38632\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.2608 - acc: 0.9275 - val_loss: 0.4315 - val_acc: 0.8721\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2545 - acc: 0.9304\n",
      "Epoch 00028: val_loss did not improve from 0.38632\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.2545 - acc: 0.9303 - val_loss: 0.5039 - val_acc: 0.8505\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2465 - acc: 0.9312\n",
      "Epoch 00029: val_loss did not improve from 0.38632\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.2465 - acc: 0.9312 - val_loss: 0.5422 - val_acc: 0.8477\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2398 - acc: 0.9329\n",
      "Epoch 00030: val_loss did not improve from 0.38632\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.2398 - acc: 0.9329 - val_loss: 0.3890 - val_acc: 0.8921\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2370 - acc: 0.9339\n",
      "Epoch 00031: val_loss did not improve from 0.38632\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.2370 - acc: 0.9339 - val_loss: 0.4627 - val_acc: 0.8777\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2257 - acc: 0.9370\n",
      "Epoch 00032: val_loss did not improve from 0.38632\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.2257 - acc: 0.9370 - val_loss: 0.4909 - val_acc: 0.8465\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2232 - acc: 0.9380\n",
      "Epoch 00033: val_loss improved from 0.38632 to 0.32463, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_BN_5_conv_checkpoint/033-0.3246.hdf5\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.2235 - acc: 0.9380 - val_loss: 0.3246 - val_acc: 0.9115\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2229 - acc: 0.9377\n",
      "Epoch 00034: val_loss did not improve from 0.32463\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.2230 - acc: 0.9376 - val_loss: 0.4202 - val_acc: 0.8852\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2106 - acc: 0.9417\n",
      "Epoch 00035: val_loss did not improve from 0.32463\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.2108 - acc: 0.9416 - val_loss: 0.3647 - val_acc: 0.8984\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2094 - acc: 0.9427\n",
      "Epoch 00036: val_loss did not improve from 0.32463\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.2093 - acc: 0.9427 - val_loss: 0.3345 - val_acc: 0.9033\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2007 - acc: 0.9450\n",
      "Epoch 00037: val_loss did not improve from 0.32463\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.2007 - acc: 0.9450 - val_loss: 0.4352 - val_acc: 0.8849\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1967 - acc: 0.9448\n",
      "Epoch 00038: val_loss did not improve from 0.32463\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1967 - acc: 0.9448 - val_loss: 0.4074 - val_acc: 0.8845\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1900 - acc: 0.9470\n",
      "Epoch 00039: val_loss did not improve from 0.32463\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1901 - acc: 0.9470 - val_loss: 0.3825 - val_acc: 0.8903\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1905 - acc: 0.9477\n",
      "Epoch 00040: val_loss did not improve from 0.32463\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1905 - acc: 0.9477 - val_loss: 0.4203 - val_acc: 0.8896\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1848 - acc: 0.9479\n",
      "Epoch 00041: val_loss did not improve from 0.32463\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1848 - acc: 0.9479 - val_loss: 0.3922 - val_acc: 0.8889\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1798 - acc: 0.9497\n",
      "Epoch 00042: val_loss did not improve from 0.32463\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1798 - acc: 0.9497 - val_loss: 0.3704 - val_acc: 0.8980\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1738 - acc: 0.9520\n",
      "Epoch 00043: val_loss did not improve from 0.32463\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1739 - acc: 0.9519 - val_loss: 0.3617 - val_acc: 0.8998\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1767 - acc: 0.9510\n",
      "Epoch 00044: val_loss did not improve from 0.32463\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1767 - acc: 0.9510 - val_loss: 0.4004 - val_acc: 0.8877\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1645 - acc: 0.9552\n",
      "Epoch 00045: val_loss did not improve from 0.32463\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1645 - acc: 0.9553 - val_loss: 0.5038 - val_acc: 0.8633\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1598 - acc: 0.9567\n",
      "Epoch 00046: val_loss did not improve from 0.32463\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1598 - acc: 0.9567 - val_loss: 0.4181 - val_acc: 0.8882\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1572 - acc: 0.9563\n",
      "Epoch 00047: val_loss did not improve from 0.32463\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1572 - acc: 0.9563 - val_loss: 0.3581 - val_acc: 0.9113\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1560 - acc: 0.9565\n",
      "Epoch 00048: val_loss did not improve from 0.32463\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1560 - acc: 0.9565 - val_loss: 0.4826 - val_acc: 0.8651\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1558 - acc: 0.9583\n",
      "Epoch 00049: val_loss did not improve from 0.32463\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1559 - acc: 0.9583 - val_loss: 0.4439 - val_acc: 0.8777\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1516 - acc: 0.9580\n",
      "Epoch 00050: val_loss did not improve from 0.32463\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1516 - acc: 0.9580 - val_loss: 0.3661 - val_acc: 0.9005\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1442 - acc: 0.9605\n",
      "Epoch 00051: val_loss did not improve from 0.32463\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1443 - acc: 0.9605 - val_loss: 0.3950 - val_acc: 0.8873\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1417 - acc: 0.9621\n",
      "Epoch 00052: val_loss did not improve from 0.32463\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1417 - acc: 0.9621 - val_loss: 0.3414 - val_acc: 0.9045\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1343 - acc: 0.9629\n",
      "Epoch 00053: val_loss did not improve from 0.32463\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1343 - acc: 0.9628 - val_loss: 0.5014 - val_acc: 0.8570\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1356 - acc: 0.9641\n",
      "Epoch 00054: val_loss did not improve from 0.32463\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1356 - acc: 0.9641 - val_loss: 0.4024 - val_acc: 0.8961\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1342 - acc: 0.9651\n",
      "Epoch 00055: val_loss did not improve from 0.32463\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1342 - acc: 0.9651 - val_loss: 0.3638 - val_acc: 0.8940\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1268 - acc: 0.9656\n",
      "Epoch 00056: val_loss did not improve from 0.32463\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1268 - acc: 0.9655 - val_loss: 0.3845 - val_acc: 0.8970\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1245 - acc: 0.9659\n",
      "Epoch 00057: val_loss did not improve from 0.32463\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1245 - acc: 0.9658 - val_loss: 0.3601 - val_acc: 0.9057\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1193 - acc: 0.9677\n",
      "Epoch 00058: val_loss did not improve from 0.32463\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1193 - acc: 0.9677 - val_loss: 0.3866 - val_acc: 0.8956\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1202 - acc: 0.9674\n",
      "Epoch 00059: val_loss did not improve from 0.32463\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1203 - acc: 0.9674 - val_loss: 0.5218 - val_acc: 0.8691\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1154 - acc: 0.9682\n",
      "Epoch 00060: val_loss did not improve from 0.32463\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1154 - acc: 0.9682 - val_loss: 0.4033 - val_acc: 0.8949\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1114 - acc: 0.9713\n",
      "Epoch 00061: val_loss did not improve from 0.32463\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1114 - acc: 0.9713 - val_loss: 0.3832 - val_acc: 0.9001\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1115 - acc: 0.9700\n",
      "Epoch 00062: val_loss did not improve from 0.32463\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1116 - acc: 0.9699 - val_loss: 0.3871 - val_acc: 0.8987\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1086 - acc: 0.9698\n",
      "Epoch 00063: val_loss did not improve from 0.32463\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1086 - acc: 0.9698 - val_loss: 0.4316 - val_acc: 0.8868\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1046 - acc: 0.9723\n",
      "Epoch 00064: val_loss did not improve from 0.32463\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1047 - acc: 0.9723 - val_loss: 0.3350 - val_acc: 0.9136\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1021 - acc: 0.9720\n",
      "Epoch 00065: val_loss did not improve from 0.32463\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1021 - acc: 0.9720 - val_loss: 0.3410 - val_acc: 0.9087\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1002 - acc: 0.9738\n",
      "Epoch 00066: val_loss did not improve from 0.32463\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1002 - acc: 0.9738 - val_loss: 0.3823 - val_acc: 0.9015\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0981 - acc: 0.9732\n",
      "Epoch 00067: val_loss did not improve from 0.32463\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0981 - acc: 0.9732 - val_loss: 0.5188 - val_acc: 0.8635\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0922 - acc: 0.9757\n",
      "Epoch 00068: val_loss did not improve from 0.32463\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0924 - acc: 0.9756 - val_loss: 0.3791 - val_acc: 0.9040\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1006 - acc: 0.9726\n",
      "Epoch 00069: val_loss did not improve from 0.32463\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1006 - acc: 0.9726 - val_loss: 0.4168 - val_acc: 0.8963\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0905 - acc: 0.9757\n",
      "Epoch 00070: val_loss did not improve from 0.32463\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0905 - acc: 0.9757 - val_loss: 0.4582 - val_acc: 0.8896\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0879 - acc: 0.9764\n",
      "Epoch 00071: val_loss did not improve from 0.32463\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0879 - acc: 0.9764 - val_loss: 0.3758 - val_acc: 0.9024\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0860 - acc: 0.9770\n",
      "Epoch 00072: val_loss did not improve from 0.32463\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0861 - acc: 0.9769 - val_loss: 0.3973 - val_acc: 0.9026\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0886 - acc: 0.9765\n",
      "Epoch 00073: val_loss did not improve from 0.32463\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0887 - acc: 0.9765 - val_loss: 0.4723 - val_acc: 0.8770\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0876 - acc: 0.9777\n",
      "Epoch 00074: val_loss did not improve from 0.32463\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0878 - acc: 0.9777 - val_loss: 0.5044 - val_acc: 0.8721\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0894 - acc: 0.9767\n",
      "Epoch 00075: val_loss did not improve from 0.32463\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0894 - acc: 0.9767 - val_loss: 0.3889 - val_acc: 0.8989\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0770 - acc: 0.9803\n",
      "Epoch 00076: val_loss did not improve from 0.32463\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0770 - acc: 0.9803 - val_loss: 0.3726 - val_acc: 0.9064\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0759 - acc: 0.9803\n",
      "Epoch 00077: val_loss did not improve from 0.32463\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0759 - acc: 0.9803 - val_loss: 0.4307 - val_acc: 0.8856\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0751 - acc: 0.9808\n",
      "Epoch 00078: val_loss did not improve from 0.32463\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0752 - acc: 0.9808 - val_loss: 0.3783 - val_acc: 0.8982\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0814 - acc: 0.9797\n",
      "Epoch 00079: val_loss did not improve from 0.32463\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0815 - acc: 0.9797 - val_loss: 0.3986 - val_acc: 0.8996\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0728 - acc: 0.9811\n",
      "Epoch 00080: val_loss did not improve from 0.32463\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0729 - acc: 0.9811 - val_loss: 0.3422 - val_acc: 0.9189\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0706 - acc: 0.9823\n",
      "Epoch 00081: val_loss did not improve from 0.32463\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0706 - acc: 0.9823 - val_loss: 0.3632 - val_acc: 0.9108\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0656 - acc: 0.9839\n",
      "Epoch 00082: val_loss did not improve from 0.32463\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0660 - acc: 0.9838 - val_loss: 0.4449 - val_acc: 0.8875\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0772 - acc: 0.9789\n",
      "Epoch 00083: val_loss did not improve from 0.32463\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0772 - acc: 0.9789 - val_loss: 0.3929 - val_acc: 0.9061\n",
      "\n",
      "1D_CNN_custom_multi_3_GAP_BN_5_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 611us/sample - loss: 0.3889 - acc: 0.8883\n",
      "Loss: 0.388888766405367 Accuracy: 0.88826585\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2913 - acc: 0.6227\n",
      "Epoch 00001: val_loss improved from inf to 1.39321, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_BN_6_conv_checkpoint/001-1.3932.hdf5\n",
      "36805/36805 [==============================] - 63s 2ms/sample - loss: 1.2912 - acc: 0.6227 - val_loss: 1.3932 - val_acc: 0.5847\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7067 - acc: 0.8113\n",
      "Epoch 00002: val_loss improved from 1.39321 to 0.66946, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_BN_6_conv_checkpoint/002-0.6695.hdf5\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.7068 - acc: 0.8113 - val_loss: 0.6695 - val_acc: 0.8164\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5326 - acc: 0.8574\n",
      "Epoch 00003: val_loss improved from 0.66946 to 0.46805, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_BN_6_conv_checkpoint/003-0.4680.hdf5\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.5325 - acc: 0.8574 - val_loss: 0.4680 - val_acc: 0.8779\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4376 - acc: 0.8835\n",
      "Epoch 00004: val_loss did not improve from 0.46805\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.4376 - acc: 0.8835 - val_loss: 0.5034 - val_acc: 0.8546\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3770 - acc: 0.9005\n",
      "Epoch 00005: val_loss improved from 0.46805 to 0.44356, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_BN_6_conv_checkpoint/005-0.4436.hdf5\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.3770 - acc: 0.9004 - val_loss: 0.4436 - val_acc: 0.8796\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3319 - acc: 0.9114\n",
      "Epoch 00006: val_loss improved from 0.44356 to 0.43017, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_BN_6_conv_checkpoint/006-0.4302.hdf5\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.3319 - acc: 0.9115 - val_loss: 0.4302 - val_acc: 0.8749\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2988 - acc: 0.9188\n",
      "Epoch 00007: val_loss improved from 0.43017 to 0.36456, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_BN_6_conv_checkpoint/007-0.3646.hdf5\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.2988 - acc: 0.9188 - val_loss: 0.3646 - val_acc: 0.8973\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2727 - acc: 0.9261\n",
      "Epoch 00008: val_loss did not improve from 0.36456\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.2728 - acc: 0.9260 - val_loss: 0.4438 - val_acc: 0.8724\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2519 - acc: 0.9321\n",
      "Epoch 00009: val_loss improved from 0.36456 to 0.32401, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_BN_6_conv_checkpoint/009-0.3240.hdf5\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.2519 - acc: 0.9321 - val_loss: 0.3240 - val_acc: 0.9040\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2303 - acc: 0.9377\n",
      "Epoch 00010: val_loss improved from 0.32401 to 0.28056, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_BN_6_conv_checkpoint/010-0.2806.hdf5\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.2303 - acc: 0.9378 - val_loss: 0.2806 - val_acc: 0.9208\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2151 - acc: 0.9416\n",
      "Epoch 00011: val_loss improved from 0.28056 to 0.27612, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_BN_6_conv_checkpoint/011-0.2761.hdf5\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.2151 - acc: 0.9416 - val_loss: 0.2761 - val_acc: 0.9224\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2017 - acc: 0.9452\n",
      "Epoch 00012: val_loss did not improve from 0.27612\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.2018 - acc: 0.9452 - val_loss: 0.3283 - val_acc: 0.9043\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1875 - acc: 0.9491\n",
      "Epoch 00013: val_loss improved from 0.27612 to 0.26588, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_BN_6_conv_checkpoint/013-0.2659.hdf5\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1875 - acc: 0.9491 - val_loss: 0.2659 - val_acc: 0.9248\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1782 - acc: 0.9507\n",
      "Epoch 00014: val_loss did not improve from 0.26588\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1782 - acc: 0.9507 - val_loss: 0.2949 - val_acc: 0.9140\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1671 - acc: 0.9551\n",
      "Epoch 00015: val_loss did not improve from 0.26588\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1672 - acc: 0.9551 - val_loss: 0.2764 - val_acc: 0.9182\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1596 - acc: 0.9556\n",
      "Epoch 00016: val_loss improved from 0.26588 to 0.25228, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_BN_6_conv_checkpoint/016-0.2523.hdf5\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1596 - acc: 0.9556 - val_loss: 0.2523 - val_acc: 0.9329\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1467 - acc: 0.9603\n",
      "Epoch 00017: val_loss improved from 0.25228 to 0.23068, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_BN_6_conv_checkpoint/017-0.2307.hdf5\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1467 - acc: 0.9603 - val_loss: 0.2307 - val_acc: 0.9359\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1397 - acc: 0.9607\n",
      "Epoch 00018: val_loss improved from 0.23068 to 0.22724, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_BN_6_conv_checkpoint/018-0.2272.hdf5\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1399 - acc: 0.9607 - val_loss: 0.2272 - val_acc: 0.9355\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1359 - acc: 0.9623\n",
      "Epoch 00019: val_loss did not improve from 0.22724\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1360 - acc: 0.9622 - val_loss: 0.2614 - val_acc: 0.9283\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1271 - acc: 0.9659\n",
      "Epoch 00020: val_loss did not improve from 0.22724\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1272 - acc: 0.9658 - val_loss: 0.2795 - val_acc: 0.9171\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1217 - acc: 0.9668\n",
      "Epoch 00021: val_loss did not improve from 0.22724\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1217 - acc: 0.9668 - val_loss: 0.2542 - val_acc: 0.9273\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1132 - acc: 0.9703\n",
      "Epoch 00022: val_loss improved from 0.22724 to 0.22031, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_BN_6_conv_checkpoint/022-0.2203.hdf5\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1133 - acc: 0.9703 - val_loss: 0.2203 - val_acc: 0.9385\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1077 - acc: 0.9710\n",
      "Epoch 00023: val_loss did not improve from 0.22031\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1077 - acc: 0.9710 - val_loss: 0.2515 - val_acc: 0.9259\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1018 - acc: 0.9724\n",
      "Epoch 00024: val_loss did not improve from 0.22031\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1019 - acc: 0.9723 - val_loss: 0.2283 - val_acc: 0.9399\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1029 - acc: 0.9710\n",
      "Epoch 00025: val_loss did not improve from 0.22031\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1029 - acc: 0.9710 - val_loss: 0.2280 - val_acc: 0.9376\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0908 - acc: 0.9758\n",
      "Epoch 00026: val_loss did not improve from 0.22031\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0908 - acc: 0.9758 - val_loss: 0.2424 - val_acc: 0.9283\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0861 - acc: 0.9772\n",
      "Epoch 00027: val_loss did not improve from 0.22031\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0861 - acc: 0.9772 - val_loss: 0.3659 - val_acc: 0.9003\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0854 - acc: 0.9768\n",
      "Epoch 00028: val_loss improved from 0.22031 to 0.21902, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_BN_6_conv_checkpoint/028-0.2190.hdf5\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0854 - acc: 0.9768 - val_loss: 0.2190 - val_acc: 0.9380\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0777 - acc: 0.9797\n",
      "Epoch 00029: val_loss did not improve from 0.21902\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0778 - acc: 0.9797 - val_loss: 0.2475 - val_acc: 0.9324\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0757 - acc: 0.9814\n",
      "Epoch 00030: val_loss did not improve from 0.21902\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0757 - acc: 0.9814 - val_loss: 0.2225 - val_acc: 0.9385\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0800 - acc: 0.9785\n",
      "Epoch 00031: val_loss did not improve from 0.21902\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0801 - acc: 0.9785 - val_loss: 0.2303 - val_acc: 0.9324\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0686 - acc: 0.9832\n",
      "Epoch 00032: val_loss improved from 0.21902 to 0.21807, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_BN_6_conv_checkpoint/032-0.2181.hdf5\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0687 - acc: 0.9832 - val_loss: 0.2181 - val_acc: 0.9376\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0655 - acc: 0.9832\n",
      "Epoch 00033: val_loss did not improve from 0.21807\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0656 - acc: 0.9832 - val_loss: 0.2960 - val_acc: 0.9210\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0604 - acc: 0.9852\n",
      "Epoch 00034: val_loss improved from 0.21807 to 0.20295, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_BN_6_conv_checkpoint/034-0.2029.hdf5\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0605 - acc: 0.9852 - val_loss: 0.2029 - val_acc: 0.9425\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0605 - acc: 0.9848\n",
      "Epoch 00035: val_loss did not improve from 0.20295\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0605 - acc: 0.9848 - val_loss: 0.2322 - val_acc: 0.9376\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0529 - acc: 0.9869\n",
      "Epoch 00036: val_loss did not improve from 0.20295\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0529 - acc: 0.9869 - val_loss: 0.2395 - val_acc: 0.9359\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0632 - acc: 0.9840\n",
      "Epoch 00037: val_loss did not improve from 0.20295\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0632 - acc: 0.9841 - val_loss: 0.2263 - val_acc: 0.9399\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0478 - acc: 0.9892\n",
      "Epoch 00038: val_loss did not improve from 0.20295\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0478 - acc: 0.9892 - val_loss: 0.2694 - val_acc: 0.9285\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0474 - acc: 0.9894\n",
      "Epoch 00039: val_loss did not improve from 0.20295\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0474 - acc: 0.9893 - val_loss: 0.2503 - val_acc: 0.9329\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0436 - acc: 0.9897\n",
      "Epoch 00040: val_loss did not improve from 0.20295\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0435 - acc: 0.9897 - val_loss: 0.2283 - val_acc: 0.9357\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0458 - acc: 0.9888\n",
      "Epoch 00041: val_loss did not improve from 0.20295\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0459 - acc: 0.9888 - val_loss: 0.2769 - val_acc: 0.9301\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0454 - acc: 0.9891\n",
      "Epoch 00042: val_loss did not improve from 0.20295\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0454 - acc: 0.9891 - val_loss: 0.2682 - val_acc: 0.9348\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0406 - acc: 0.9910\n",
      "Epoch 00043: val_loss did not improve from 0.20295\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0407 - acc: 0.9910 - val_loss: 0.2744 - val_acc: 0.9264\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0394 - acc: 0.9906\n",
      "Epoch 00044: val_loss did not improve from 0.20295\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0394 - acc: 0.9906 - val_loss: 0.2395 - val_acc: 0.9352\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0336 - acc: 0.9930\n",
      "Epoch 00045: val_loss did not improve from 0.20295\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0336 - acc: 0.9930 - val_loss: 0.2555 - val_acc: 0.9341\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0349 - acc: 0.9926\n",
      "Epoch 00046: val_loss did not improve from 0.20295\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0350 - acc: 0.9926 - val_loss: 0.2239 - val_acc: 0.9376\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0392 - acc: 0.9908\n",
      "Epoch 00047: val_loss did not improve from 0.20295\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0392 - acc: 0.9908 - val_loss: 0.2367 - val_acc: 0.9399\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0286 - acc: 0.9943\n",
      "Epoch 00048: val_loss did not improve from 0.20295\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0288 - acc: 0.9942 - val_loss: 0.2217 - val_acc: 0.9408\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0373 - acc: 0.9910\n",
      "Epoch 00049: val_loss did not improve from 0.20295\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0373 - acc: 0.9910 - val_loss: 0.2683 - val_acc: 0.9348\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0272 - acc: 0.9948\n",
      "Epoch 00050: val_loss did not improve from 0.20295\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0273 - acc: 0.9948 - val_loss: 0.2415 - val_acc: 0.9394\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0314 - acc: 0.9933\n",
      "Epoch 00051: val_loss did not improve from 0.20295\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0314 - acc: 0.9933 - val_loss: 0.2199 - val_acc: 0.9427\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0248 - acc: 0.9954\n",
      "Epoch 00052: val_loss did not improve from 0.20295\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0249 - acc: 0.9954 - val_loss: 0.3210 - val_acc: 0.9224\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0355 - acc: 0.9912\n",
      "Epoch 00053: val_loss did not improve from 0.20295\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0356 - acc: 0.9911 - val_loss: 0.3042 - val_acc: 0.9208\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0328 - acc: 0.9917\n",
      "Epoch 00054: val_loss did not improve from 0.20295\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0328 - acc: 0.9917 - val_loss: 0.2607 - val_acc: 0.9352\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0229 - acc: 0.9960\n",
      "Epoch 00055: val_loss did not improve from 0.20295\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0229 - acc: 0.9960 - val_loss: 0.2808 - val_acc: 0.9234\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0275 - acc: 0.9937\n",
      "Epoch 00056: val_loss did not improve from 0.20295\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0277 - acc: 0.9937 - val_loss: 0.3417 - val_acc: 0.9178\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0313 - acc: 0.9925\n",
      "Epoch 00057: val_loss did not improve from 0.20295\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0313 - acc: 0.9925 - val_loss: 0.2084 - val_acc: 0.9460\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0180 - acc: 0.9969\n",
      "Epoch 00058: val_loss did not improve from 0.20295\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0179 - acc: 0.9969 - val_loss: 0.2473 - val_acc: 0.9432\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0224 - acc: 0.9952\n",
      "Epoch 00059: val_loss did not improve from 0.20295\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0224 - acc: 0.9952 - val_loss: 0.3110 - val_acc: 0.9257\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0193 - acc: 0.9965\n",
      "Epoch 00060: val_loss did not improve from 0.20295\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0193 - acc: 0.9965 - val_loss: 0.2639 - val_acc: 0.9341\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0255 - acc: 0.9944\n",
      "Epoch 00061: val_loss did not improve from 0.20295\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0255 - acc: 0.9944 - val_loss: 0.2977 - val_acc: 0.9285\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0177 - acc: 0.9966\n",
      "Epoch 00062: val_loss did not improve from 0.20295\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0177 - acc: 0.9966 - val_loss: 0.2877 - val_acc: 0.9243\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0230 - acc: 0.9950\n",
      "Epoch 00063: val_loss did not improve from 0.20295\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0230 - acc: 0.9950 - val_loss: 0.2789 - val_acc: 0.9355\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0171 - acc: 0.9968\n",
      "Epoch 00064: val_loss did not improve from 0.20295\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0171 - acc: 0.9968 - val_loss: 0.4235 - val_acc: 0.8991\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0215 - acc: 0.9953\n",
      "Epoch 00065: val_loss did not improve from 0.20295\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0215 - acc: 0.9953 - val_loss: 0.2945 - val_acc: 0.9292\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0164 - acc: 0.9969\n",
      "Epoch 00066: val_loss did not improve from 0.20295\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0165 - acc: 0.9969 - val_loss: 0.3005 - val_acc: 0.9269\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0247 - acc: 0.9942\n",
      "Epoch 00067: val_loss did not improve from 0.20295\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0247 - acc: 0.9942 - val_loss: 0.2109 - val_acc: 0.9460\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0167 - acc: 0.9964\n",
      "Epoch 00068: val_loss did not improve from 0.20295\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0167 - acc: 0.9964 - val_loss: 0.2905 - val_acc: 0.9269\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0191 - acc: 0.9960\n",
      "Epoch 00069: val_loss did not improve from 0.20295\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0192 - acc: 0.9959 - val_loss: 0.2629 - val_acc: 0.9387\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0223 - acc: 0.9952\n",
      "Epoch 00070: val_loss did not improve from 0.20295\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0223 - acc: 0.9952 - val_loss: 0.2479 - val_acc: 0.9380\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0149 - acc: 0.9971\n",
      "Epoch 00071: val_loss did not improve from 0.20295\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0149 - acc: 0.9971 - val_loss: 0.2715 - val_acc: 0.9392\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0211 - acc: 0.9950\n",
      "Epoch 00072: val_loss did not improve from 0.20295\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0211 - acc: 0.9950 - val_loss: 0.2505 - val_acc: 0.9420\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0162 - acc: 0.9967\n",
      "Epoch 00073: val_loss did not improve from 0.20295\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0162 - acc: 0.9967 - val_loss: 0.2754 - val_acc: 0.9364\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0128 - acc: 0.9980\n",
      "Epoch 00074: val_loss did not improve from 0.20295\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0128 - acc: 0.9979 - val_loss: 0.3051 - val_acc: 0.9264\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0274 - acc: 0.9931\n",
      "Epoch 00075: val_loss did not improve from 0.20295\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0274 - acc: 0.9931 - val_loss: 0.2187 - val_acc: 0.9450\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0103 - acc: 0.9984\n",
      "Epoch 00076: val_loss did not improve from 0.20295\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0104 - acc: 0.9983 - val_loss: 0.2444 - val_acc: 0.9401\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0232 - acc: 0.9943\n",
      "Epoch 00077: val_loss did not improve from 0.20295\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0232 - acc: 0.9943 - val_loss: 0.2219 - val_acc: 0.9464\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0114 - acc: 0.9981\n",
      "Epoch 00078: val_loss did not improve from 0.20295\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0114 - acc: 0.9981 - val_loss: 0.2593 - val_acc: 0.9331\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0211 - acc: 0.9953\n",
      "Epoch 00079: val_loss did not improve from 0.20295\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0211 - acc: 0.9953 - val_loss: 0.2261 - val_acc: 0.9413\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0138 - acc: 0.9974\n",
      "Epoch 00080: val_loss did not improve from 0.20295\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0140 - acc: 0.9974 - val_loss: 0.2188 - val_acc: 0.9464\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0214 - acc: 0.9948\n",
      "Epoch 00081: val_loss did not improve from 0.20295\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0214 - acc: 0.9948 - val_loss: 0.2747 - val_acc: 0.9366\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0097 - acc: 0.9985\n",
      "Epoch 00082: val_loss did not improve from 0.20295\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0099 - acc: 0.9985 - val_loss: 0.2176 - val_acc: 0.9471\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0228 - acc: 0.9950\n",
      "Epoch 00083: val_loss did not improve from 0.20295\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0229 - acc: 0.9949 - val_loss: 0.2254 - val_acc: 0.9469\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0166 - acc: 0.9962\n",
      "Epoch 00084: val_loss did not improve from 0.20295\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0166 - acc: 0.9962 - val_loss: 0.2444 - val_acc: 0.9446\n",
      "\n",
      "1D_CNN_custom_multi_3_GAP_BN_6_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzsnXd4VUX6xz9z03snlAQTejG00BQEXBXBgigioujqWnbXtuiKoru6/Cwrrl1XrItiRVZkxYUVdQURFxAIvQkBAgnppOem3Hvf3x+TmwJpQG5uIPN5nvMk95w5M+8599z5zrzzzhwlIhgMBoPBAGBxtwEGg8FgaDsYUTAYDAZDNUYUDAaDwVCNEQWDwWAwVGNEwWAwGAzVGFEwGAwGQzVGFAwGg8FQjREFg8FgMFRjRMFgMBgM1Xi624CTJTIyUuLi4txthsFgMJxRbNq0KUdEoppKd8aJQlxcHBs3bnS3GQaDwXBGoZRKaU464z4yGAwGQzVGFAwGg8FQjREFg8FgMFRzxo0p1EdlZSWpqamUlZW525QzFl9fX2JiYvDy8nK3KQaDwY24TBSUUvOBK4AsETm3kXTDgLXA9SLy+amUlZqaSlBQEHFxcSilTs3gdoyIkJubS2pqKvHx8e42x2AwuBFXuo/eByY0lkAp5QE8C3xzOgWVlZURERFhBOEUUUoRERFheloGg8F1oiAiq4FjTSS7F1gMZJ1ueUYQTg9z/wwGA7hxoFkp1QW4GnijGWnvVEptVEptzM7OPrUCrVZIS4PKylM732AwGNoB7ow+ehl4WEQcTSUUkbdFZKiIDI2KanJCXv1YrZCe7hJRyM/PZ968ead07mWXXUZ+fn6z08+ZM4fnn3/+lMoyGAyGpnCnKAwFFiqlDgHXAvOUUpNdVpql6lJFWjzrxkTBZrM1eu7y5csJDQ1tcZsMBoPhVHCbKIhIvIjEiUgc8Dlwl4j8y2UFOkXB0WTH5KSZPXs2ycnJDBo0iFmzZrFq1SouuOACJk2aRL9+/QCYPHkyiYmJ9O/fn7fffrv63Li4OHJycjh06BB9+/bljjvuoH///owfPx6r1dpouVu2bGHkyJEMGDCAq6++mry8PABeffVV+vXrx4ABA7j++usB+OGHHxg0aBCDBg1i8ODBFBUVtfh9MBgMZz6uDEn9FBgHRCqlUoG/AF4AIvKmq8rdt28mxcVbTjxgt0NpKez1A4+Tu+zAwEH07Plyg8fnzp3Ljh072LJFl7tq1SqSkpLYsWNHdYjn/PnzCQ8Px2q1MmzYMKZMmUJERMRxtu/j008/5Z133uG6665j8eLFzJgxo8Fyb775Zl577TXGjh3L448/zv/93//x8ssvM3fuXA4ePIiPj0+1a+r555/n9ddfZ9SoURQXF+Pr63tS98BgMLQPXCYKIjL9JNLe4io7qnFG17S896hehg8fXifm/9VXX2XJkiUAHDlyhH379p0gCvHx8QwaNAiAxMREDh061GD+BQUF5OfnM3bsWAB+/etfM3XqVAAGDBjAjTfeyOTJk5k8WXvkRo0axQMPPMCNN97INddcQ0xMTItdq8FgOHs4K2Y016bBFn1ZGezYAfHxcFxl7AoCAgKq/1+1ahXfffcda9euxd/fn3HjxtU7J8DHx6f6fw8PjybdRw2xbNkyVq9ezVdffcXTTz/N9u3bmT17NpdffjnLly9n1KhRrFixgj59+pxS/gaD4eyl/ax95OwpuGBMISgoqFEffUFBAWFhYfj7+7Nnzx7WrVt32mWGhIQQFhbGjz/+CMCHH37I2LFjcTgcHDlyhAsvvJBnn32WgoICiouLSU5OJiEhgYcffphhw4axZ8+e07bBYDCcfZx1PYUGceFAc0REBKNGjeLcc89l4sSJXH755XWOT5gwgTfffJO+ffvSu3dvRo4c2SLlLliwgN/97neUlpbSrVs33nvvPex2OzNmzKCgoAAR4b777iM0NJTHHnuMlStXYrFY6N+/PxMnTmwRGwwGw9mFEheEaLqSoUOHyvEv2dm9ezd9+/Zt/ES7HTZvhpgY6NjRhRaeuTTrPhoMhjMSpdQmERnaVLr24z5yYU/BYDAYzhbajygopTcjCgaDwdAg7UcUQPcWjCgYDAZDgxhRMBgMBkM1RhQMBoPBUE27EYXKyjzsUo44Gl+gzmAwGNoz7UYUQOmrbSM9hcDAwJPabzAYDK1BuxEFpSyIAmkjomAwGAxtkXYjCmABhcuWzn799derPztfhFNcXMxFF13EkCFDSEhI4Msvv2x2niLCrFmzOPfcc0lISOCzzz4DID09nTFjxjBo0CDOPfdcfvzxR+x2O7fcckt12pdeeqnFr9FgMLQPzr5lLmbOhC0nLp3tIQ7EWoISC9RarK5ZDBoELze8dPa0adOYOXMmd999NwCLFi1ixYoV+Pr6smTJEoKDg8nJyWHkyJFMmjSpWe9D/uKLL9iyZQtbt24lJyeHYcOGMWbMGD755BMuvfRS/vSnP2G32yktLWXLli2kpaWxY8cOgJN6k5vBYDDU5uwThYaorodbflmPwYMHk5WVxdGjR8nOziYsLIzY2FgqKyt59NFHWb16NRaLhbS0NDIzM+nYjGU21qxZw/Tp0/Hw8CA6OpqxY8eyYcMGhg0bxm9+8xsqKyuZPHkygwYNolu3bhw4cIB7772Xyy+/nPHjx7f4NRoMhvbB2ScKDbToxVGJbf9WvKweMHBwixc7depUPv/8czIyMpg2bRoAH3/8MdnZ2WzatAkvLy/i4uLqXTL7ZBgzZgyrV69m2bJl3HLLLTzwwAPcfPPNbN26lRUrVvDmm2+yaNEi5s+f3xKXZTAY2hntZkxBKUtV9JFrFgCcNm0aCxcu5PPPP69+2U1BQQEdOnTAy8uLlStXkpKS0uz8LrjgAj777DPsdjvZ2dmsXr2a4cOHk5KSQnR0NHfccQe33347SUlJ5OTk4HA4mDJlCk899RRJSUkuuUaDwXD2c/b1FBqkaqDZRavC9u/fn6KiIrp06UKnTp0AuPHGG7nyyitJSEhg6NChJ/VSm6uvvpq1a9cycOBAlFL87W9/o2PHjixYsIDnnnsOLy8vAgMD+eCDD0hLS+PWW2/FUTWI/swzz7jkGg0Gw9lP+1k6Gyg/uAnvXEElJta8dMdQjVk622A4e3H70tlKqflKqSyl1I4Gjt+olNqmlNqulPqfUmqgq2ypxqL0ePMZJoQGg8HQWrhyTOF9YEIjxw8CY0UkAXgSeNuFtmiUeaeCwWAwNIbLxhREZLVSKq6R4/+r9XEdEOMqW6qxuO49zQaDwXA20Faij24D/uPyUpxvXzPuI4PBYKgXt0cfKaUuRIvC6EbS3AncCdC1a9dTL8y8ktNgMBgaxa09BaXUAOBd4CoRyW0onYi8LSJDRWRoVFTUaRRoRMFgMBgaw22ioJTqCnwB3CQiv7RKoS7qKeTn5zNv3rxTOveyyy4zaxUZDIY2gytDUj8F1gK9lVKpSqnblFK/U0r9rirJ40AEME8ptUUptbHBzFoKN4iCzdb4S32WL19OaGhoi9pjMBgMp4rLREFEpotIJxHxEpEYEfmHiLwpIm9WHb9dRMJEZFDV1uSkitPGw0P/bWFRmD17NsnJyQwaNIhZs2axatUqLrjgAiZNmkS/fv0AmDx5MomJifTv35+3366Jvo2LiyMnJ4dDhw7Rt29f7rjjDvr378/48eOxWq0nlPXVV18xYsQIBg8ezMUXX0xmZiYAxcXF3HrrrSQkJDBgwAAWL14MwNdff82QIUMYOHAgF110UYtet8FgOPtw+0BzS9PAytkAiL0zqjQKfH3Bq/l5NrFyNnPnzmXHjh1sqSp41apVJCUlsWPHDuLj4wGYP38+4eHhWK1Whg0bxpQpU4iIiKiTz759+/j000955513uO6661i8eDEzZsyok2b06NGsW7cOpRTvvvsuf/vb33jhhRd48sknCQkJYfv27QDk5eWRnZ3NHXfcwerVq4mPj+fYsWPNv2iDwdAuOetEoXGcS1u4PiR1+PDh1YIA8Oqrr7JkyRIAjhw5wr59+04Qhfj4eAYNGgRAYmIihw4dOiHf1NRUpk2bRnp6OhUVFdVlfPfddyxcuLA6XVhYGF999RVjxoypThMeHt6i12gwGM4+zjpRaKxFX1F6DO9daUhsDCq66XcanA4BtV7ks2rVKr777jvWrl2Lv78/48aNq3cJbR8fn+r/PTw86nUf3XvvvTzwwANMmjSJVatWMWfOHJfYbzAY2idtZfJaq6AsekxBHPYWzTcoKIiioqIGjxcUFBAWFoa/vz979uxh3bp1p1xWQUEBXbp0AWDBggXV+y+55JI6rwTNy8tj5MiRrF69moMHDwIY95HBYGiSdiUKWJwDzS0rChEREYwaNYpzzz2XWbNmnXB8woQJ2Gw2+vbty+zZsxk5cuQplzVnzhymTp1KYmIikZGR1fv//Oc/k5eXx7nnnsvAgQNZuXIlUVFRvP3221xzzTUMHDiw+uU/BoPB0BDtaunsysp8PLfuR6IjsMTGN5m+vWGWzjYYzl7cvnR2W0Qpj6q3r5kZzQaDwVAf7UwULIiixd1HBoPBcLbQrkQBLKanYDAYDI3QrkShpqdgRMFgMBjqo12JAnjo+WuOM2tw3WAwGFqLdiUKSlkQCyCmp2AwGAz10a5EAVRVT8H9ohAYGOhuEwwGg+EE2pUoKKWqBpqN+8hgMBjqo12JAoAo1eLvaJ49e3adJSbmzJnD888/T3FxMRdddBFDhgwhISGBL7/8ssm8Glpiu74lsBtaLttgMBhOlbNuQbyZX89kS0YDa2cDYi1G2YFNzXffDOo4iJcnNLzS3rRp05g5cyZ33303AIsWLWLFihX4+vqyZMkSgoODycnJYeTIkUyaNEn3WBqgviW2HQ5HvUtg17dctsFgMJwOZ50oNI+W7SkMHjyYrKwsjh49SnZ2NmFhYcTGxlJZWcmjjz7K6tWrsVgspKWlkZmZSceODa/QWt8S29nZ2fUugV3fctkGg8FwOpx1otBYix6g8sBWPPMrUUNa9kVvU6dO5fPPPycjI6N64bmPP/6Y7OxsNm3ahJeXF3FxcfUume2kuUtsGwwGg6tod2MKWCzgguCjadOmsXDhQj7//HOmTp0K6GWuO3TogJeXFytXriQlJaXRPBpaYruhJbDrWy7bYDAYTod2KApKv3+thcNS+/fvT1FREV26dKFTp04A3HjjjWzcuJGEhAQ++OAD+vTp02geDS2x3dAS2PUtl20wGAyng8uWzlZKzQeuALJE5Nx6jivgFeAyoBS4RUSSmsr3dJbOBqhM3YVXRikMHgweHs06p71gls42GM5e2sLS2e8DExo5PhHoWbXdCbzhQltqUFWX3AYmsBkMBkNbw2WiICKrgcbe/3gV8IFo1gGhSqlOrrKnGosRBYPB0DgiDVcRIlBc3PB0p8rKU5sK5XBAbq77qyZ3Rh91AY7U+pxatS/9+IRKqTvRvQm6du1ab2Yi0mj8fzVVoiAOB81I3W44097A15ax2UAp/agpBRUVkJkJGRl6s9vB0xO8vMDbG8LCIDxcbzYbJCfDgQNw8CAEBUFcnN4iIyElBX75Bfbtg4ICCA6GkBCdzmqF7GzIyYGSEujeHfr1g/79oVMnXVlVVEB5uc5/1y69paRoGzp0gOhoiIiAwEC9+fnBkSOwYwds315jU3i4PsfbG4qKoLBQV5QBARAVpbfgYF3JZWXp66+ogNBQvYWEQFkZHDumt6IiXZ7zuIcH5OfrraBAH+vSRW+RkZCXp/PNztbX6ryfnp7aJh8fvTnzyc3V5dhs+vyoKH2dNpvOy1lWUZG+juJi8PeHAQNg0CBISNDlbdwIGzbo6wkPhz599Obnp7+Tffv0/fTy0rbGxury8vNrvhu7HTp31lvHjtq2ffv0915WpvPq1k1/f+Hh+vqcNk2fDr//vWuf3zMiJFVE3gbeBj2mcPxxX19fcnNziYiIaFoYqt/TbGtxO89URITc3Fx8fX3dbcopIaJ/zLm5+sfn4aE3T09dAWZn6y03V1doMTF6CwrSFd2mTfrHnpZW00J0/rXZ9OZwgK+vrij8/PSP3m7Xx+x2XXEdO6bLsFprbPPw0MdbGqW0LSUlJ+4PD9e2fvhh0y3W8HCIj9eVUlaWvo/14eenxWXoUCgt1RXp7t26og8K0lvHjtqePXvgxx+1UEREaLHp0EFX0gUFWljy83We4eFasHr10pVefr62xWbTohMVpSvHwkItTuvW6XscHq6Pdeig/zq/p8pKbUN5ud6c+YSHQ8+eWqhzc/XzsGdPjSiHhen74LyWwEBt69at8Mkn+n+loG9fuPRSbe+RI7B3Lyxbpr/znj1h5Ei48UZddmqq3nbu1PnHxen7Z7FAejocPaqfvbAwfe6ECVooUlO1QCQn6+NOewID9XPnatwpCmlAbK3PMVX7TpqYmBhSU1PJzs5uMq295BgeOUXILwrl638qxZ2V+Pr6EhMT45K8HY6aysvZonM49I/iyBE4fFi3vPLyalptISG6lRUTo1uOu3frH+i2bbq17URE/2ArK0/Pxh49dOvM2bpXqkZYPD3157IyXSFarboCcx7z9NQVytChuvIJDtZ5OisqHx9d8XXsqFviXl41FVh5ub5eZ4vZYtF2dOum8ywuhkOH9JadDV276gqpe3dd8dvtNS11f39dwTjjJ0pLdaW1c6c+19u7pnfStauu5Dt00NfmpLRU21FSUrN17qxtsbSRWEWRuja3RnlHjuh7GxTUeuW6C5dFHwEopeKAfzcQfXQ5cA86+mgE8KqIDG8qz/qij06GnH89SuTVz1D+1QJ8rrj5lPNpD4joStpq1S3CigpdaTgroaIiXcE4W1cWi3YzbNmit5QUna6wsHk+VqVqXAsFBbrs2sTE6O581651K4XgYN1Fj4zU54rUVMheXjUtyogIbYuzBZeXpyvGIUP0D95gOJtpbvSRy3oKSqlPgXFApFIqFfgL4AUgIm8Cy9GCsB8dknqrq2ypjSUgBABHSUFrFNcmEdG+zeRk2L9ft7xLS/VWUqJbRU6/dmnpyecfGAgDB8KvfqUraafP22LRrWNnq75zZ90biI3VreiQkLqt0eLimsq7Vy9dqZ8u0dG6q24wGOrHZaIgItObOC7A3a4qvyEsAaG6/NKzSxTKy/WgodPFcvRojb/b6WIoKNCuipyc+n3H3t7aBdG5s3ZPXHwxnHOO3ufjo4/7+emWudPPWVmp8yoq0v/37VvjhjldAgP1IJ7BYGg9zoiB5pakRhQK3WxJ05SUaP+u089++LB2z+zcqX3sTl96RYX2dztdNH5+2tXi5VUz6OqM3ujfX7tKunXTfvQePbQI+Ptr37jBYGjftLtqoEYUGgizaGWKi2vcOPv368iLvXt12GFW1onpvbygd29ITNTuFOfgob+/Dj8cOFC7R9rrZO0yWxm+nmdmFNWp4hAHuaW5RPpHNi8s+yTyfW39a2SWZBITHENMcAw9wnvQL6pfi5VxsiQfS+b9Le9zz/B7iA6MPq28HOLAopru0ooIi3cvxu6wM6jjIHpG9MSiLBSUFbDm8BpWp6zGy8OLe4ffW69NDnHwS+4vbDq6iU3pm1Aobhp4E4M6DqpOk2fN44OtH7D0l6XYHDYsyoKH8sDPy4/ogGiiA6LpGNiRkTEjGdZl2Gldd1O0Q1HQjunWFoXS0prQxz17air+9ONmZURHa//5pEnahRMZqQdfw8J0BEvPno2HpTnEQWZxJvll+Xh5eOFp8cTbw5uOgR2b9QOozaajm1idspop/abQNaTu/JBd2bt49qdnKa0srX5ou4Z05fJelxPpH9lovsUVxXy07SP6RPZhaOehBHrrd1uUVpayPnU969PW0zGwI6O7jqZ7WPfqii69KJ1N6ZsI9wvn/Njz6+Rpd9h56NuHeGndS3QI6MDgToMZFD2I3pG9CfMNI9Q3lGCfYNKL09mbs5c9OXvIteYy/dzpTO4zGQ9LwypaWllKQVkBHQM7nnSl6xAHpZWl+Hv5V9//SnslKQUpJB9L5mD+QTKKM8gsziSzJJO8sjyslVZKK0sps5XRPbw7F3S9gNFdR5PQIYHkvGS2ZmxlW+Y29ubu5WD+QVLyUyi3l3Nh3IV8du1nRAVEnZSN9SEi/HHFH3l5/ct4KA/sUhNXe9OAm3h14quE+oaecv5ltjJ8PHxO6n4u2rmI25feTlFFEe8kvcPH13zMRd0uqj6eVZLFP5L+Qa41Fx8PH3w8ffC0eFJaWUpxRTElFSXkWnNJLUwltTCVzJJMOgV2YmTMSEbGjOT82PMZGTOyzu8kqySLW7+8leX7llfvC/AKICY4hn3H9uEQB14WL+xi56V1L/GHEX/gwfMfxEN5sGzfMr7Y/QXfJH9DUYWub3w9fXGIgxfXvcigjoOYkTCDHdk7WLhjIWW2MgZGDyTUNxS7w06lVJJTmsPGoxvJKsnCIQ4eGf2Iy0XBpdFHruB0o4/KCg/gG9KdwoeuIvjZf7WgZXVJS4M1a/S2dq329duqpkZEROiKv3dvXcn37KndON2714QzNhcRYcmeJczbMI+D+Qc5UnCESseJ8ZlR/lFc2uNSJnSfwPju4xusOGwOG//a8y9eWf8Kaw6vAcDL4sWtg25l9ujZ+Hj68JeVf2H+lvkEeAXQJbgLGcUZ5JflA+Bp8WRij4ncNOAmrux9Zb2t9lv+dQsLti4AwKIsJHRIwMfTh6T0JGzHzR/pGNiRflH92JOzh6NFR6v3zxgwg5cufYlI/0gKywuZvng6y/ct58aEG/Hy8GJz+mZ2Zu88IT8n4X7h+Hj4kF6cTnxoPPeNuI+x54xld85udmTtYEfWDlIKUkgtTOWYVU/Mj/SP1BVIl5GE+IawPXM7WzO3sit7Fz0jenJt32u5tt+19Ajvwc9pP7Nwx0IW7VpUbbe/lz/+Xv4csx7DITXTVhWKCP8IogOiCfcLx9/LHz8vP7w9vNmRtYNd2btOsD/AK4C+UX2JD40nLjQOX09fnvvfc0T5R7H4usX1VhxltjJeXf8qc9fMJT4snofOf4gp/abgaTmxbTh3zVwe+e8j/GHEH3hh/AtklWSRWpjKl3u/ZO6auXQM7Mj8q+ZzSbdLSEpPYvHuxfz7l38T6hvKiC4jGBkzkuFdhhMTHFOn4t+WuY1X1r3Cx9s/5reJv+WVia+cUHalvZL9x/YTFRBFuF845bZy7l9xP29teovzYs7j8bGPc/+K+9mbs5fHxjzGDQk38PK6l3l/6/uU2crw9/Kn3FZeLWQKRaB3IIHegYT6hhIbEktMUAwdAztyqOAQ61LXcSDvAABxoXHcNvg2bhl0C3ty9nDTkpvIs+bxwvgXOD/2fDZnbGZz+mZSClIY0mkIY88Zy8iYkRwpPMJfVv2FhTsWEugdSLmtnEpHJR0DO3Jlrys5P/Z8Ejsl0jeqL4XlhXyy/RPe2/IeSelJBHoHMiNhBr8d+ts6vYfa2B12cq25eCgPIvxPLeKiudFH7U4UKiuO4ekXQeHdlxDy6jctZld+Pnz7LXz9NaxcqSfogJ7hOWKEntRy3nmQONSOT0gB+WX55Fnz6BLchY6BJ750JyU/hUU7FzG++3gGdhxYb5lbMrYw8+uZ/JDyAz3DezKsyzC6Bnela0hXQn1DsTlsVDoqsVZaWZu6lhXJK8gpzcFDeXB136v5w4g/MCp2FEop0grT+Mfmf/Bu0rscKTxCfGg89w6/l0u6X8IbG97g3c3vYnfY8fbwxuawcfewu/nTmD9V9wrKbeXsyt7FJ9s/4ePtH5NenE7viN6svnU1HQI6VNv85Z4vmfzZZB4870EujL+QdanrWJe6jnJ7OaNiRzG662jOizmP9OJ0fkz5kTVH1rAnZw99IvuQ2CmRxE6JfHvgW55Z8wxhvmH837j/Y97GeezO3s3fL/s7vxv6u+qyym3lpBWlUVBWQF5ZHvll+UQHRNM7sjeR/pHYHXaW7l3Ki+terBZA0MLWO6I33cO7ExOk3SYB3gFsztjM+tT17M7ZDWhhGRg9kD6RfdicsZl1qeuq9x+zHsPbw5vLel7GyC4jsdqslFSUUFxRTKR/JN3Du9M9rDvdwroRHRhdb8XsJLc0l5+O/MTu7N30jOjJgOgBdAvrdkLPb3P6Zq5ZdA1Hi47y4vgXGRc3jlDfUEJ9Q1m2bxkPf/cwh/IPcWn3SzmUf4i9uXuJD41n5siZjO8+nl4RvbAoC/9I+ge3f3U7NyTcwIdXf3hCORuPbuTmJTezO2c3nYM6c7ToKB7KgzHnjMFqs5KUnkSFvQKAEJ8Q+kX1o19UPw7mH+T7g9/j5+lHQnQCP6f9zL+n/5vLe11enXdJRQnjFoxj49GN1d+Fn6cfRRVFPHT+Qzz1q6fw8vCipKKEu5ffXd248Pbw5uYBN/Pg+Q/SO7I3oCvSSkdls3okWSVZfJv8LfO3zOf7g99jURZEhL5RfVk4ZSEJ0QmNnu9kW+Y2Xl73MuF+4VzT95oTeh7Hk3wsmQ4BHQjycf0ECCMKDeBwVCABPpTMGEXwO2uaPqHBfHQs/n/+o7d163SUT2ioDsW84AK9DRyoB3BLK0t57PvHeO3n1+q05P08/Xjx0hf5beJvqx/c/x74L9M+n0auNReAUbGjuHvY3ZwXex57c/ayK3sX69PWs2jnIsL9wnnywie5I/GORisW0K6MpPQkFu1cxDtJ75Bflk9ip0Q6B3Vm2b5lOMTBxd0u5q6hdzGp96Q6LpW0wjReXPsixRXFPDz6YbqFdWuwHLvDzle/fMUNi2+gT2QfVt2yimCfYHJKc+g/rz+dgzqz/vb1eHt4n/L93565nduW3saGoxsI8w3jn1P/WceVcLJsPLqRA3kH6B/Vn54RPRu1Lb8sn5KKEjoHda5T2RwpOMIXu79gw9ENXNztYib3mXxaLpZTIbc0lxu+uIFvkk9s8AyIHsAL41/g4m4X4xAHX+75kmd/epb1aesBCPMNY0inIaw8tJJLul3C0ulLG7wP1korT65+kl3Zu5jUexJX9b6qugVbbitnS8YWNhzdwK7sXdVbgHcAvx/6e24fcjsBXgEMf3c4GcUZbP/9djoEdMCfb2jVAAAgAElEQVTusHPtP69l6d6lzL1oLt4e3mSWZJJbmss1fa/h0h6XnmDHP3f+k19yf+E3g39Dp6CWWTot+Vgy7215D4uyMHv0bPy9zo5JrkYUGqEiVGG9YjAhHzW5UvcJJCXBG2/AV1/pWbigB30nTtTT1EeMODGK54dDP3Db0ttIzkvmpgE3kdgpkTC/MIJ9gnlj4xt8k/wNV/a6kncnvctH2z5i1rez6BPZh/evep8fD//IvA3zSM5LrpNnpH8kNw24icfGPEaY38nPvCqpKOGDrR/w2s+vkVeWxy0Db+H2IbfTPbz7SefVEP/Z9x8mLZzE6K6j+c+N/+GmJTfx5Z4v2XTnpma3vBrD7rDz2c7PGNFlRIvafaZjd9j58fCPZJVkVfdIY0NimdZ/2gljJyLCnpw9rE1dW91riwuN45Mpn1SP9biKnVk7SXw7kUu6X8LS65cy69tZvLD2BV6Z8Ar3jbjPpWW3R5orCojIGbUlJibK6WKNVlJwTd9mp6+oEFm4UGTUKBEQCQgQuf56kQULRDIyGj7P7rDLfcvvE+Yg3V7pJt8f+L7eNC+tfUm8n/QWv6f8hDnINZ9dI4VlhXXSfL3va3lzw5vyw6EfJKs466Su1518tPUjYQ6SMC9BmIM88+Mz7jbJ0IZ4Zd0rwhzkik+uEOYgdy+7WxwOh7vNOisBNkoz6li3V/Inu7WEKJR09ZSCCfHNSrt+vUj//vpOdesm8tJLIvn5Ncd3Z++WWd/MktzS3BPO/cvKvwhzkHuW3SPF5cWNlrM1Y6uMe3+cPPPjM2fdj8L5wx/57kiptFe62xxDG8LusMulH14qzEEmfjTRPB8uxIhCIxT38pGCcZ0bTWO1ijz0kIjFIhITI7J4sYjNVjfNqoOrJHRuqDAHGfDGgDot+C92fSHMQW751y1nXSV/KqzYv0IyizPdbYahDZJZnCl/Xf3XOr1jQ8vTXFFoI+seti7i44kqq2jw+J49eg31v/0NfvMb2L5dmHBlaZ0JYZ9s/4TxH40nOiCa+ZPmsy93H+MWjCOjOIMdWTu4aclNDO8ynDcuf6NFJxSdqYzvPr5OFJLB4KRDQAceueCRVonAMTRNu5u8BiC+dUWh0l7J1H9OZew5Y5kYNpMLL1Q4HPDNNzB0dB6XfXI5a1PX0i2sGwOiBxDqG8r7W95nzDljWDJtCeF+4cSHxXPFJ1cw9v2x2Bw2gnyCWDJtSbubXWswGM5s2qkoeGMpqnk7ydK9S/ly75d8ufdLHtuWip/9OX5YZSE6LpeLPriEndk7efC8BzlceJitGVvZf2w/MwbM4N0r38XH0weAcXHj+Oamb5j48UTKbGWs+vUqOgd1dtclGgwGwynRfkWhrGaV1Nc3vE6XgHMo+HkSxQNeZNzlmQTHzOXCBZfxS+4vfHn9l0zoMaE6vd1hr3dZhPNjz+fn238mvyyfETEjWuVaDAaDoSVpn6Lg54Mq11Pgd2XvYuWhlYQnPYPXmoe5d2onXtv1KD1e/RyLsrDshmUnTIpqbJ0c52xKg8FgOBNpl6KAny+Wcr32zLwN8/DAm2Pf3caq5YqxYx9hYFIHnv3pWd6d9C5jzhnjZmMNBoOh9WinouCHpVwoKi9iwZYPYOd1TJkYxdix+vBtQ27jtiG3uddGg8FgcAPtWBTgw20fUFxZhMfPdzP3O3cbZTAYDO6nXc5TwM8fZYMXf3wdjg7hnqtH0KOHu40yGAwG9+NSUVBKTVBK7VVK7VdKza7neFel1Eql1Gal1Dal1GWutKe6XP9AfjwHkot247/zbh5/zEwuMxgMBnChKCilPIDXgYlAP2C6Uur49/j9GVgkIoOB64F5rrKnjm3+gTw9qDNYQ3n8musJD2+NUg0Gg6Ht48qewnBgv4gcEJEKYCFw1XFpBHC+aywEOEoroPwD2RgRjPexAcy8++xYK91gMBhaAleKQhfgSK3PqVX7ajMHmKGUSgWWA/e60J5qlH8QRcGFRHpE4uPTGiUaDAbDmYG7B5qnA++LSAxwGfChUie+u04pdadSaqNSamN2dvZpF+rwDaIyKItoj9Z9K5bBYDC0dVwpCmlAbK3PMVX7anMbsAhARNYCvkDk8RmJyNsiMlREhkZF1f/C+ZNhpzUAPGyc4+132nkZDAbD2YQrRWED0FMpFa+U8kYPJC89Ls1h4CIApVRftCicflegCTYX6mUqevm7u6NkMBgMbQuX1YoiYgPuAVYAu9FRRjuVUk8opSZVJfsjcIdSaivwKXBL1csgXMquAhsA/QLKXV2UwWAwnFG4dEaziCxHDyDX3vd4rf93AaNcaUN97CspghAY4l/U2kUbDAZDm6Zd+k+OlOWCzZvoSqu7TTEYDIY2Rbtc+yirMhPfwgiUT6m7TTEYDIY2RbN6CkqpPyilgpXmH0qpJKXUeFcb5yoKLGmEFQYhpUYUDAaDoTbNdR/9RkQKgfFAGHATMNdlVrmQoiKw+aUSXegJZUYUDAaDoTbNFQXninGXAR+KyM5a+84o9ic7IDiNuAI7WM2YgsFgMNSmuaKwSSn1DVoUViilggCH68xyHUl7s8Cjkp6lpShrmbvNMRgMhjZFcweabwMGAQdEpFQpFQ7c6jqzXMfWg6kAJJTlgTW4idQGg8HQvmhuT+E8YK+I5CulZqCXvC5wnVmuY2+6XqOvV0UFymomrxkMBkNtmisKbwClSqmB6FnIycAHLrPKhRzK0z2FzjYvM6ZgMBgMx9FcUbBVLT9xFfB3EXkdCHKdWa4jszQVi8ObSBUA1lIcDtNbMBgMBifNFYUipdQj6FDUZVXLW3u5zizXUFkJheoIwSoGD/8gLOVgtR5wt1kGg8HQZmiuKEwDytHzFTLQy2A/5zKrXMThwyBBqXTwjcESEIqlAqzWX9xtlsFgMLQZmiUKVULwMRCilLoCKBORM25MITkZCE6la2gMloAwLOVQWmpEwWAwGJw0d5mL64CfganAdcB6pdS1rjTMFezb74DgVHpFx2IJCMGz3AOrdZ+7zTIYDIY2Q3PnKfwJGCYiWQBKqSjgO+BzVxnmCnYczIbASvp0joHOpfh8DaUle91tlsFgMLQZmjumYHEKQhW5J3Fum2FPmg5HjQ2Jgfh4PErsVGYaUTAYDAYnze0pfK2UWoF+OxrogefljaRvkxw4pieuxQbHQrzeZzmcic1WhKfnGRlhazAYDC1Ks0RBRGYppaZQ85a0t0VkievManlEIKNE9xRigmMgTr+n2S8DrNb9BAUNdqd5BoPB0CZo9kt2RGQxsNiFtriUrCyo8E3FAy+iAqIgzgcA33QdlmpEwWAwGJoQBaVUESD1HQJERM6YFeV0OOoRIn1isCgLhIYioaH4ZuSbsFSDwWCootHBYhEJEpHgerag5giCUmqCUmqvUmq/Ump2A2muU0rtUkrtVEp9cqoX0hTOOQqxwTE1ZcfH45/la8JSDQaDoQqXRRAppTyA14GJQD9gulKq33FpegKPAKNEpD8w01X23HgjdE1IpUeHGlEgPh6/DIvpKRgMBkMVrgwrHQ7sF5EDIlIBLEQvqFebO4DXRSQP4Liw15ZFOcgoTaVrSGzNvrg4vNMrsJaasFSDwWAA14pCF+BIrc+pVftq0wvopZT6SSm1Tik1wVXG5JTmUGGv0JFHTuLjsZTZsGTnU1mZ66qiDQaD4YzB3RPQPIGewDhgOvCOUir0+ERKqTuVUhuVUhuzs7NPqaAjBVVzFGr3FOL1ZAXfDLMGksFgMIBrRSENqFUDE1O1rzapwFIRqRSRg8AvaJGog4i8LSJDRWRoVFTUKRmTWlhrjoKTuDigJizVYDAY2juuFIUNQE+lVLxSyhu4Hlh6XJp/oXsJKKUi0e4kl7zgIC40jgfPe5BuYd1q7YwDwDddmZ6CwWAwcBKT104WEbEppe4BVgAewHwR2amUegLYKCJLq46NV0rtAuzALBFxiXN/YMeBDOw4sO7OgACIiiIwp5xsE5ZqMBgMrhMFABFZznFrJInI47X+F+CBqs09xMfjl3nA9BQMBoMB9w80u5/4eHyOVmK17kNrlMFgMLRfjCjExeGVXoyjspSKiqPutsZgMBjcihGF+HhUpR2fXBOWajAYDEYUas1VMGGpBoOhvWNEoSos1T/Tl6KiTe61xWAwGNyMEYVzzgEgNO8c8vK+NYPNBoOhXWNEwccHOncmMDeUsrJDWK3J7rbIYDAY3IYRBYD4eHzTdQ8hL+9bqKyEjRvdbJTBYDC0PkYUQK+WmpKBj492ITFzJgwbBitWuNsyg8FgaFWMKADExaFSUwkPugjL0q9h3jy9/9VX3WuXwWAwtDJGFECHpTocdEiOpedcK7ZBfeDRR2H5cthn1kQyGAztByMKUD1XIfS3b2CphIwXx8O994KXF/z97242zmAwGFoPIwpQPVdBZWZxeHYc2aFJ0LEjXHcdvPceFBW51z6DwWBoJYwoAMTGQkgIzJiBzLiewsJ12GxFcN99WhAWLHC3hQaDwdAqGFEA8PSEvXvh/fcJCx+PiI38/FUwfDiMGAGvvQYOh7utNBgMBpdjRMFJdDR4eBAScj4Wiz95ed/o/ffdB7/8At984177DAaDoRUwonAcFosPoaFjOHbsW73j2mv1+MKjj8L27U1n8MYb8PDDrjXSYDAYXIQRhXoICxuP1boXq/UgeHvDyy9DcjIMHAgzZsD+/fWf6HDA00/r+Q2Vla1rtMFgMLQARhTqISrqWsBCevrbese0aXDgADz0EHzxBfTpA//854kn/vwzpKVBWVnzehUGg8HQxjCiUA++vrFERk7m6NF3sNutemdEBMydq3sMffroHsHxK6ouXgxK6f/Xr29dow2GM5Vdu2DQIMjMdLclBlwsCkqpCUqpvUqp/Uqp2Y2km6KUEqXUUFfaczJ06XIPNlsuWVmf1T3QqRPcdRds3QpJSTX7RbQoXHopREXpXoPBYGiar7/Wv6dVq9xtiQEXioJSygN4HZgI9AOmK6X61ZMuCPgD0Kaa1qGh4/D3709a2msnvmPhhhvA1xfefbdm35YtcPAgTJmiw1hNT8FgaB5OV+sm85KrtoArewrDgf0ickBEKoCFwFX1pHsSeBYoc6EtJ41Sii5d7qG4OInCwrV1D4aGwtSp8MknUFKi933xBVgscNVVWhT27IGCgtY33GA409i2Tf81otAmcKUodAGO1PqcWrWvGqXUECBWRJY1lpFS6k6l1Eal1Mbs7OyWt7QBoqNn4OERQlpaPesf3XEHFBbC55/rz4sXw9ix2nU0YoR2J23Y0Gq2GgxnJDabHlMA7Y41bz50O24baFZKWYAXgT82lVZE3haRoSIyNCoqyvXGVeHpGUinTreSnf1PysvT6x4cPRp69YJ33oHdu/U2ZYo+NmyY/mtcSAZD4+zfr6P1Ro2C/Hwd5WdwK64UhTQgttbnmKp9ToKAc4FVSqlDwEhgaVsabAbo3PkuRGw14alOlILbb4effoKnntL7rr5a/w0Nhd69jSgYDE3hHE+49Vb917iQ3I4rRWED0FMpFa+U8gauB5Y6D4pIgYhEikiciMQB64BJItKm3oPp79+T8PDLSEv7O5WV+XUP3nyzXjfpk0/g/POhc+eaYyNG6Agk0x02GBpm2zY9Fjd1qp4oakTB7bhMFETEBtwDrAB2A4tEZKdS6gml1CRXlesK4uOfpLIyl8OHn657IDoaJlVditN15GTECB13ffhw6xjZ1vnzn02YruFEtm/XbtjgYEhIMKLQBnDpmIKILBeRXiLSXUSertr3uIgsrSftuLbWS3ASFDSEjh1vITX1FUpLj1vi4v77oWtX/e6F2owYof8aF5Ke5f3003VDeA0G0KIwYID+f8gQM9jcBjAzmptJfPzTKOXNgQMP1T0wejSkpEBMTN39CQng42NEAWqisMzSH4baFBXpgeWEBP05MRHy8vR8H4PbMKLQTHx8OnHOOY+Qk7OEvLyVTZ/g7a1bPkYUakRhxw7zXoq2TlERZGW1Tlk7d+q/zp5CYqL+a1xIbsWIwkkQE/MAPj5dSU5+ABF70yeMGKG7w+19xVSnKBQXmzGWts6MGXDhha1TlnPSmrOnkJCg34tee/kYQ6tjROEk8PDwo1u3Zyku3sLRo281fcKIEWC16hZyc9iyRU+IO5sQgY0ba374xoXUdtm/H776Sk8my89vOv3psn07BAXBOefozz4+cO65pqfgZowonCQdOkwjLGw8+/c/QFFREw/v8OH67//+13TG+/fD0KEwa9bpG9mWSE7WfuJbbtGfjSi4HhHdCj/ZAds33qg5Z8uWlrfreLZt0yJgqVUNJSZqUTCDzW7DiMJJopSib9+P8fbuwM6d11JZeazhxPHxuoX8/PO6x9AYf/0r2O3w0Udn15pJzjDUX/1KtwiNKLieL77QL4S6+OLmzxAuKYH582tcR5s3u84+0JV+7cgjJ4mJcOyYDt4wuAUjCqeAt3ck/ft/Tnn5UXbvnoFIA4OnSsErr8ChQ/Dccw1nePAgfPihrjhLS/X/ZwsbNoCfH/TvrwWyua40w6mzYgX4++t7n5Cgn0F7E2Ngn3yiXUZPPKEnYTbHr//aa/Cb35xaqz4tTfcgnW5FJ2aw2e0YUThFgoOH06PHKxw79h9SUp5qOOGFF+o5DM8803DrZ+5c3YX+4AO9btK8eWdP93nDBhg8WA8gJiTo1WMrKtxt1dnNypVw0UU6umfcOJg5U0+ybOiZEoG//133LkaN0lFzTfUUyspgzhx47z1YvvzkbXT2GI/vKSQk6FUCnKKQmQnLluneQ2vhcGh352WXtV6ZbQkROaO2xMREaSs4HA7ZtetmWblSSXr6hw0nPHxYxN9fZMqUE4+lpIh4eYncdZf+PH++CIisWlU33apVIu+9J2K3t5j9LqeyUsTPT+QPf9CfP/5YX9u2bS2Tv8MhMmuWyE8/tUx+ZwNHjuh7/OKL+rPDIfLss3rfxx/Xf87q1fr4O+/oz489JmKxiJSUNFzORx/pc4KDRfr10991bf7zH5HYWJE77hBZv17bUZu5c/X5x46dmPegQSKdOon07KnTQM3vozX44x9ryj14sPXKdTHARmlGHev2Sv5kt7YkCiIiNlupbN78K1m50iIZGZ80nPCpp/Tt/vbbuvvvukuLQkqK/lxSIhIaKjJtWk2an34S8fXV50+YIJKR0fIX4gq2btU2f/SR/rxtm/78SSP36WTYskXnd+21LZPf2cAHH+h7snlzzT67XWToUJHOnUWKik4857rr9DPnFIEvvtB5rFvXcDkXXCDSo4fIokU67bvv1hxLSREJD9fl+fvr4+eeK/LCCyKpqTrNjTdq0aiPOXNEoqJEJk0See45kTFjRM4550RhEdF5Xn65SGFho7el2cybp+2dPFn//fvfWybfNoARhVbEZiuWzZvHycqVFsnM/Kz+RFarSPfuIr16iSxcKLJzp26FeHuL3Hln3bT33y/i6SmSni6yZ4/+gfXoIfL881ocoqNFvv66ZYw/cEBX3q7g3Xf1I7Z3r/5cXq6v65FHWib/P/9Z5x8aemJLtb1y6636eTm+R7lunb5XDz9cd39ysv5O/vjHmn0HD+q0b7xRfxk7dujjf/ubrqhHjNACUFKiv+MRI0SCgkR++UWkoEDkrbdEhg3T5yilBaVTJ5HLLmveNb35pj531666++12nQ+IXHih/o2dDsuX6x7SFVeI2Gy6pzJhwunl2YYwotDKVFYWSVLSBbJypYdkZi6qP9E339S0+J2bh4eumGuzd68+dt99IvHxutW0f78+tn27bnWByJNP1t96ag4Oh/7R+/lpYfrqq1PLpzHuvFNX2LUrqP799Y/udHE4RHr31vaDyP/+d/p5Npf779ct1/vvF1m7Vttis4kkJYm88oreX1DQevbUJi5O5Oqr6z92yy26V+oU6e+/189WUFDdZ9DhEAkLO7Gx4uS++/Qzk5WlP//wg/4Onn5auwpB5J//PPG8vXtFnnhCu5tA/98cUlJ0+hdeqLv/p5/0/ilTpLp131jjoLi44V72zz+LBAaKDB5c05u6/359nfX1rloCh0P3ilrp2TWi4AYqKwtl06ZRsnKlkkOHnhZHfRW21aq79h9+KPLQQyL/+Ef9mV10kf56/P31A1ub0lKRm27Sxx944OSFISNDd7lB5JJLtGvBy0vkX/86uXyaYvBgkYsvrrvv+ut1hXq6bN+u7X/qKd36nDPn9PKbM0ffi6Z8yLt369Zkz566wgCRLl20b7222N9ww6kLdn3s26fzbMw+Zwv/1VfrP56ergVg4kTtlrFYRPr0ObEFLiLyq1/p5+J4Sku10F9/fd39V14p4uNT05hpioMHRcrKmk7npF+/E5+lP/5RP7f5+VqMQQtffeNuhw7p7yww8MTnfM0afV/i42vcWyIi//2vzrOlfxdOnGI6dGjLPisNYETBTdhsJbJz53RZuRLZsWOa2GyNDNY1xtdf6we1oRa83S5y7736K7ztNt1SbQ5btujWoY+P/iHZ7SJ5ebrL7+kpsnjxqdl7PFZr/a6ip5/WNp9uS/ovf9FikJEhMny4yHnnnXpemzfrCtLpilqypOG0116rK5asLF0ZffCBbqn+9rd67CQlRffgQAcNtBSXXabz7NZNJC2t/jTOIIXt2xvO54UXaoRrypSGffF//KN+Rioq6u5//3197sqVdffv3Knv4YgR2oXU0jz4YN1Wu8OhK/HaLqg5c7RtkybV7fns3KmFOyREN1Rq97K//14kIEC7dWsLgoi+juBgkdtvPzlbly6tGSNsDGfDDLRAuBgjCm7E4XBISspcWblSyYYNg8VqbcYDUh9N+ckdDh0pAiJTpzbd8ioo0GMTnTufWHEUFIicf752Zy1dWv/5qanNjxxau1bb9cUXdfd/+aW0iLunf3+RsWP1/3/+s66Q6otkaQqHQ2TUKC2UmzbpVpuztXv8/dywQR97/PHG87TZtI/b379uK/ybb3Sl9PLLJ2ejs8X6619rQerbt8Z1U5ubbtLX0Virs6JCZPp0LQ6NpWsoUuy887Tbrr5zk5K0ULoC5z348suaso4f4HY49LhbQIAWtD/9SZ8XHi7SsaMeOystFZkxQ6p7yb6++llKT6+/3KlT9bhFc6P+/v1vnXfPno03fJzjMg8/LBIRoYXMxRhRaAPk5PxbVq8OljVroqWgoJFIjtPl+ef1VzlqVP2VhYj+wVx3na70f/yx/jSFhbpSDAmpGcNwkpGho0U8PRsObazNq69qm44cqbv/wAG9/623ava9/LLI3Xc3v4W5a5fO47XX9Ocff9SfP/+8eefXxhmt43TjlZWJzJxZcz+zs2vSXnKJ/gE3p5eTliYSGSkyYIB2lUyfrvP089M9nH//u3n22e1aSLp21b2vH37QeQwcWFcEHQ6RmBhdibUEznv8/vs1+5wV8fG+/dagvFwL4u9+pz87GwK1vx8nqak6usnZCu/WTQ+oO3E49CC5Uvre1peHkwULdB4bN9a1ZdWqE4UiJ0eLT1yc/p1NmdKw8N5yi/4es7N1w06pmrGehli8uHk9kAYwotBGKC7eKWvXxssPP/hKZuZC1xX02We61RMXp1shx+MMtZs7t/F8Dh7ULpQhQ2qiOcrKdC/Cz09k5EipEwdfHytW1PRIjv9R2O36x33vvfrzG2/U/HivvLJ5fuYnntA/IqcbpaJCd/PvuKP+9AUFusfyhz9oF4/T1VZQoCO5Row48Qf+2We6tdm9u44A+/77k68Qly3T51gs2vUxZ46uyAcP1vbu3l03fW7uiS1W53yAD2vNg1mxQueXmChy9Kjet2+fTjdvXvPtawybTfd0nHNMKit1BFFEhLbTHUyeXBOa2rev7o01xpo1+jlrqBewZ48efG6MrKy6Y1bl5bpVD7qRVTvi6brr9BjH5s16zKah5yU1Vae75x79OSNDP2tOwTseq1WHroPI73/fuL2NYEShDVFeniVJSaNl5Urk4ME54nA00/9/sqxfr1sqQUEib7+t3R3HjukWnre39r82pxvsdPHcdZf+Ad56q/68aJF+QK+9Vn+eNasmDNFm02I0YUJN6+z4ORlORowQGTdOV9QWi/atOnsWEyboLr6T9HSR776r24sYMEBk9Oi6edauMJwsX65dTJ6eOm/n37599bXMnKl/8Bs21G/n2rXaHRMWpgdkY2JOPuzx6ad1JbJnT82+lBSdb69eejwnK0sHHfj7axsfeEC7YaxW3UMYMuTE723ZMp0+NlaPE739tr6244XmdBg5Us8RENEt65acY3IqvPWWtmHxYqnTU3Q1552ne9C1BcH5Gxg9Wovkp59KdQSWiH4Or7lG9xhWr66b36xZ+rmvPe5x2226UXd8r2XvXj2ZzxlUchrjNUYU2hh2e1nV7Gdkw4Yhkp/vojC0w4drBtOcm8WiK7TGusnHM2uWPvfqq/Xfxx6rOWazaXdP7TKcW0iIbh011uK//XZdofn46IrHOWnqnXd0JX3RRSJ//asWD2e+MTF6YNw5Ye14v7yzx+GsfH/4QQth9+4is2fr7n5ZmQ6V7Nu3Jt+Gwi6dHDhQk762//p0Wb1aC8C55+p7YbFol8dtt+l70KFDTajlf/9bfx5JSfq+BARooezYsWWjWO66Szcwdu/W39VVV7VKlEyDHD6s70dsrP57/MCwq3AGR1x8sf7rnNC2cKF+xnr31g2HkSPrjgPm5+sec3S0nlW+bZveFxxcd3KqSM0YwxNP6Hu8Y4eOrAsM1GMiLRAy3iZEAZgA7AX2A7PrOf4AsAvYBvwXOKepPM9UURDRA9AZGZ/KTz91kZUrkV27fi1lZQ10bU+Hyko9qLZkia6gH3ig8YiU+qio0C4jpzAc31J1OHRre+5c/aN54gndZW6O8DjDB3v31n7Y2ixYUBMJNGyYjhL57DPdInPO6wBdQdQmOVnvf+UV7Q8PDdWt+z/Xq5QAABPYSURBVPpcHTabdsvccMOJ5ddHXp62obkRXs3lrbf09dx4Y90W/saNunUKTU/wOnq0ZnB8+vSWte+dd3S+vXrpSs/pqnInzjk6I0e2XpnOmfm1BcHJ6tX63vj51T8msH17TUsftMgeP0bhZMIE/dx261aT/pJLTnzWTxG3iwLgASQD3QBvYCvQ77g0FwL+Vf//HvisqXzPZFFwUllZJMnJs2XVKi/54YcASU5+VCoqTiFyxtUcPapbKy09eWf/fu3uOXSo4eP1tQJXr9aV5M03139ejx66Mo2L062z4ycFtkUa6lE5HDosuTkiW1Kie3JbtrSsbRs31lROCxa0bN6nirMH+9xzrVemw6F7cA31FFNSmr73R47o86dMqRlPO541a/SYzeWX6wZDQ6HHp0hzRUHptC2PUuo8YI6IXFr1+ZGqBfieaSD9YODvIjKqsXyHDh0qGzdubGlz3UJp6T4OHnyM7OzP8PAIITb2QWJiZuLpGehu085M7r5brzDr7w8//KBfWmQ4dcrLITxcr/T71Vd6KXh3s3UrTJ8O33wDMTHutuaMQim1SUSa/FG4cunsLsCRWp9Tq/Y1xG3Af1xoT5vD378n/fsvZOjQLYSGjuHQocf4+eeeHD36Ls16B7ShLtdfD6GhsGiREYSWwMdHL33+2WdtQxBAL++9a5cRBBfSJt6noJSaAQwF6n0TjVLqTqXURqXUxuzs7NY1rhUIDBxIQsJSBg/+H76+8fzyyx1s3DiY3NyvcVVP7qzkggsgJwcuv9zdlpw99OsHAQHutsLQirhSFNKA2FqfY6r21UEpdTHwJ2CSiJTXl5GIvC0iQ0VkaFRUlEuMbQuEhJzH4ME/0a/fIuz2YrZvn8j69T04ePAvlJb+4m7zzgw8PNxtgcFwRuNKUdgA9FRKxSulvIHrgaW1E1SNI7yFFoQsF9pyxqCUokOHqQwfvps+fT7Az687KSlP8fPPvUlKOp/MzE9xOMybywwGg2tw2UAzgFLqMuBldCTSfBF5Win1BHoUfKlS6jsgAUivOuWwiExqLM+zaaC5uZSXHyUr61OOHn0Lq3Uf3t6d6Nz5d0RH34yfX5y7zTMYDGcAzR1odqkouIL2KApORBwcO7aCtLRXOXbsawD8/fsTEXEFkZFXERw8EtVWBgQNBkObwojCWY7VmkxOzlJyc/9NQcFqRGwEB59PXNwcwsIuNuJgMBjqYEShHWGzFZCZ+TGHDz9DeXkqwcGjiI19kLCwi/D0DHK3eQaDoQ1gRKEd4nCUk57+D1JS/kpFRRpKeRIUNJywsIsICkrEz683fn7dsVi83G2qwWBoZZorCp6tYYyhdbBYfOjS5S46dbqNgoKfyMv7jry8/5KS8jTgqErlgb9/byIjJ9Ghw/UEBAwwriaDwVCN6Sm0A2y2QkpLd1NaupfS0r0UFW0gL+97wI6/fx8iIvQgdXDwCHx8OrnbXIPB4AJMT8FQjadnMMHBIwgOHlG9r6Iim5ycL8jKWkhq6guI2ADw8YklPPxSoqKuJTT0V8bVZDC0M0xPwYDdbqW4eDOFhespLPwfx459jd1ejKdnGBERVxIScgHBwSMJCOiLUmbGsMFwJmJ6CoZm4+HhR0jI+YSEnA/cj91eRl7eN2Rnf05u7r/JzPygKl0QQUHDCQk5j+Dg8wgOHomHRzAORykOhxXwwNs70q3XYjAYTg8jCoYT8PDwJTJyEpGRkxARrNZ9FBauq9rWkpLyV2oGrusSEXElXbs+QkjIea1rtMFgaBGMKBgaRSmFv38v/P170bHjzQDYbMUUFW2kqOhnHI5yLBY/PDz8qahIJy1tHps3n09IyFg6d/5tVShsd+N2MhjOEMyYgqFFsdtLOPr/7d17cFzVfcDx72+fd1craaW1ZMmyLBnjUOyEdwngwBBSXgnDo+VhkjApTZrJQKZJSpuEhjQp00zaTiaUZjIBQh6kIc2DksZlSAmv8miCedlgYzAP21iyLPSwHvt+/vrHvV5ky1iCxN619/eZ8Vj33rN3zx6d1W/vuXt+Z+i7DAx8g0LBTYrr80VpalpJMNiBzxfG5wvj97fS1nYWbW3nEAzGa1xrYw5/NnnN1FSlUiSd3kAq9Tzp9POk0xsolSapVHJUKnmKxVFKpUnATzx+Oi0tqwiFOgkGOwgG28nnh0inN5JOb6RYHKGz80q6u/+SYLCt1i/NmEOSBQVT11TLTE+vZXz8HsbH7yGd3gjs2Rd9PododAU+n8P09G/x+Zro7v4LFi68ikhkGYFAm028M2aeLCiYQ4pqmWJxgmJxlGJxjFCoi0jkiOq9iFTqOQYGbmJk5CeoFgHw+2OEw300Na2kuflEYrETiMXeg7t8h1bL+HyhWc+XybzC9PRaFiy4yPJDmYZgQcEclvL5Yaanf0su97r3byup1HPk86/vs7zPF6Wt7WwSiQtobz+b6eknGBq6jcnJhwAIhXo48shv0tFxmV11mMOazVMwh6VwuIuOjj+dtb9YHCeZfJZM5iXe/LqskMls9oaoflUt6zj9LF36NWKxE9i69Xo2bbqCtrbv0tv7Bfz+KCIBRAKEQosIhRZasDANxa4UzGFPVUmnNzIx8QDR6NG0t5+DiM87VmZo6Ba2bPkS5fLUrMf6/S1Eo+8iHO6lVJqgUBimUBimXE4jEkQkiM8XwnH6icWOIxY7lmh0JcFgO35/M35/M4FAHJ9v7s9fu9+LFoTMgWDDR8a8De6VxjqgjGqZSqVAPj9AJrOZbPZl8vlBgsEEoVAXoVAXfn+MSqWIapFKJU82+wqp1HpKpV2zzu3zRWhu/uMZs8CbKJeTlErTlErjZDIvkU5vIpN5EZ8vQnf3x+nu/gSOswSATGYzIyO/IJl8mkTiQ3R2rp73fZBKpcT4+H+Tzb5MKNRDONxDOLzYmztyIJdoN/XGgoIxB5mqks/vIJN5iXJ5ilIpSbmcJJt9lenp35FKrasmHpwpGOwgGj2apqYV5HLb2bXr14DQ3n6O99Xc5wH3/kehsAOfr4nOztW0tJzsZb59kUzmZRxnCfH4mcTj78dx+hkevoOdO28lnx+c9ZzhcB/d3VfT1fXnOE4fqkqhMEwqtZ5MZhPZ7Gtks6+Rz2+ntfV0+vpuqAap+apUCvu8yX+oUVWmp58gEll+SKdxsaBgTJ1xEw+uR7VMINCM399CIBCfNfcil3udnTtv5403fkwotIjOzsvp6LiUUGgR09Nr2bnzdkZGfkqlksbnc4hEjiIaXU42+yqp1HPM/GpvW9vZ9PRcSzz+fgqFYfL5QbLZLYyO/pyJiQcAiMVOIJ8foFgcqT4uEIjjOMsIhbqYmLgfgEWLPkVv73WUShOkUhtIpzdQLqdwnCWEw32Ew4vJZl9haupRJicfJZfbSnPzySxYcCGJxIU4Ti/J5DMkk0+RSq0nGFzo5dxaRTjcQ7E4SSbzAun0C1QqhepwXCDQTLG4i/HxexgdvZtU6hlaW99HInERicT5BAKtXvtmKBR2ksttJ5fbRi73OqXSBK2tq/aYJOmmbnEDdSDQSix2POFw7z6H7VKpjbz22nVMTPyGQCDB8uXforNzdbVsoTDGjh03UyiM0tNzDbHYMXs8Ppt9jcnJx2huPommppV7PEc2u4WJiYdoalo5Ky1MqZRiaOgWstmXaW1dRWvrGThO/+81tFgXQUFEzgNuBvzA7ar6T3sdDwM/Ak4ExoErVHXb/s5pQcEY949GsTiG4/TukUKkWNzF5OSjZLObWbDgYqLRo97yHLnc6wwP/5CJiYeIRJZ5f4SP82afJ2aU2862bTcyPPxDoFzdLxLE72/yJiG+KRBIEI+fQSSynMnJh0kmn5r13OHwEorFUS+RohuE9j6P9yw4Tj/5/ACqJcLhxTQ3v5epqccoFkcQCeI4fRQKb1AuJ2c91udzqskaW1tPIxjsZGrqcYrFN/aqczux2LFEIstwnCNwnKVMTT3C0NBtBAIt9Pb+LWNja0gm15JIXMTSpf/IyMhP2LHjW5TL6erztLWdS2/v58jnhxge/gFTU49VnyMU6qG9/VwCgRbGx39NNru5eqyl5RQWL/5r2tvPZWjoVgYG/oVicQy/v4Vyedprs156ez/P4sWffsvf6f7UPCiI21NfBs4GBoGngCtVddOMMtcAx6jqp0RkNXCJql6xv/NaUDCmNjKZVxgbuxvH6aep6T1EIsvx+YKUSkny+e3kcgM4zhKi0aP3+ESbzw8xPn4PxeKYN5/kREKhBVQqRVKp9UxN/R+ZzEtEIkuJRld6n6iDpFLrSaWeJZ3egOMso6Pjz2huPgkRqU5+HBv7L3K57dV7PaFQF47Ti+P0Ew73Aj6SyScZH7+XXbvupVSapKXlNOLxM2hpOY1yOek9zzrS6Q1ks1tmXDH56em5hv7+rxAMJlAtMzBwE1u33oBqHhA6Oi6nv//LhELdDA3dwuDgv1UDTiTyLrq6riaROJ9k8ml27bqPiYn7KZezxONnkkh8kHj8LCYn/5fBwZvI5bYAPqBCW9u59Pd/lZaWk0mnNzE19QiTk4+SSFxAV9dV7+j3Vw9B4VTgq6p6rrd9PYCqfn1Gmfu8Mr8TkQAwDHTofiplQcEYcyCVSilyuW0EAq04Tu+s45nMZkZH72LBgktoalqxx7FyOcf4+BrC4cW0tJw6a7hHtYxqCZ8vPGv/2NgaJiYeZOHCD3tp7P+w6mGeQg8wMGN7EHjvW5VR1ZKITAEJYGxmIRH5JPBJgCVL3t7NLmOMeTsCgRix2Lvf8ng0ehR9fV/a5zG/36Gz8/K3fKyIf58Zg0X8dHRcQkfHJW+/wn9gh8R30lT1NlU9SVVP6ujoqHV1jDHmsHUgg8IOYOa112Jv3z7LeMNHrbg3nI0xxtTAgQwKTwHLRWSpuBnKVgNr9iqzBviY9/OlwEP7u59gjDHmwDpg9xS8ewSfBu7D/Urq91X1BRG5EXhaVdcA3wP+XUReBXbhBg5jjDE1ckAT4qnqvcC9e+37+xk/54DLDmQdjDHGzN8hcaPZGGPMwWFBwRhjTJUFBWOMMVWHXEI8ERkF9r3M1twWsNfEOLNP1k5zszaam7XR/BysdupT1Tkneh1yQeH3ISJPz2ead6OzdpqbtdHcrI3mp97ayYaPjDHGVFlQMMYYU9VoQeG2WlfgEGHtNDdro7lZG81PXbVTQ91TMMYYs3+NdqVgjDFmPxomKIjIeSKyWUReFZEv1ro+9UBEekXkYRHZJCIviMhnvP3tInK/iLzi/d8217kOdyLiF5F1InKPt71URNZ6/elnXtLHhiYicRG5S0ReEpEXReRU60t7EpHPee+1jSLyHyLi1Ftfaoig4C0N+m3gfGAFcKWIrNj/oxpCCbhOVVcApwDXeu3yReBBVV0OPOhtN7rPAC/O2P5n4CZVPRKYAD5ek1rVl5uB/1HVPwKOxW0v60seEekB/go4SVXfjZsodDV11pcaIigAJwOvquoWVS0APwUuqnGdak5Vd6rqs97PSdw3cQ9u29zhFbsDuLg2NawPIrIY+BBwu7ctwFnAXV4RayORVuAM3MzHqGpBVSexvrS3ABDx1o+JAjups77UKEFhX0uD9tSoLnVJRPqB44G1wEJV3ekdGgYW1qha9eJfgc8DFW87AUyqasnbtv4ES4FR4AfeMNvtItKE9aUqVd0BfAPYjhsMpoBnqLO+1ChBweyHiMSA/wQ+q6rTM495ix417FfUROQCYERVn6l1XepcADgB+I6qHg+k2WuoyPqStOFeOS0FFgFNwHk1rdQ+NEpQmM/SoA1JRIK4AeFOVb3b2/2GiHR7x7uBkVrVrw6sAi4UkW24w45n4Y6dx70hALD+BO4n3EFVXett34UbJKwvvelPgK2qOqqqReBu3P5VV32pUYLCfJYGbTje2Pj3gBdV9ZszDs1cJvVjwK8Odt3qhaper6qLVbUft988pKofAR7GXUIWGryNAFR1GBgQkaO8XR8ANmF9aabtwCkiEvXee7vbqK76UsNMXhORD+KODe9eGvRrNa5SzYnI+4DHgA28OV7+d7j3FX4OLMHNSHu5qu6qSSXriIicCfyNql4gIkfgXjm0A+uAj6pqvpb1qzUROQ73ZnwI2AJcjfvB0/qSR0T+AbgC95t/64BP4N5DqJu+1DBBwRhjzNwaZfjIGGPMPFhQMMYYU2VBwRhjTJUFBWOMMVUWFIwxxlRZUDDmIBKRM3dnWjWmHllQMMYYU2VBwZh9EJGPisiTIrJeRG711lNIichNXj78B0Wkwyt7nIg8ISLPi8gvd68ZICJHisgDIvKciDwrIsu808dmrDtwpze71Zi6YEHBmL2IyNG4s05XqepxQBn4CG4Cs6dVdSXwCPAV7yE/Ar6gqsfgzg7fvf9O4NuqeixwGm5mTHCz0X4Wd22PI3Dz3xhTFwJzFzGm4XwAOBF4yvsQH8FN5FYBfuaV+TFwt7eOQFxVH/H23wH8QkSagR5V/SWAquYAvPM9qaqD3vZ6oB94/MC/LGPmZkHBmNkEuENVr99jp8iX9yr3TnPEzMxrU8beh6aO2PCRMbM9CFwqIp1QXbO6D/f9sjub5YeBx1V1CpgQkdO9/VcBj3gr2Q2KyMXeOcIiEj2or8KYd8A+oRizF1XdJCI3AL8RER9QBK7FXTjmZO/YCO59B3DTHd/i/dHfnR0U3ABxq4jc6J3jsoP4Mox5RyxLqjHzJCIpVY3Vuh7GHEg2fGSMMabKrhSMMcZU2ZWCMcaYKgsKxhhjqiwoGGOMqbKgYIwxpsqCgjHGmCoLCsYYY6r+H5SqB3SBYwdOAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 665us/sample - loss: 0.2738 - acc: 0.9246\n",
      "Loss: 0.2737547862319312 Accuracy: 0.9246106\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0800 - acc: 0.6823\n",
      "Epoch 00001: val_loss improved from inf to 1.18024, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_BN_7_conv_checkpoint/001-1.1802.hdf5\n",
      "36805/36805 [==============================] - 68s 2ms/sample - loss: 1.0800 - acc: 0.6823 - val_loss: 1.1802 - val_acc: 0.6375\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4940 - acc: 0.8646\n",
      "Epoch 00002: val_loss improved from 1.18024 to 0.44419, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_BN_7_conv_checkpoint/002-0.4442.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.4940 - acc: 0.8647 - val_loss: 0.4442 - val_acc: 0.8765\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3517 - acc: 0.9038\n",
      "Epoch 00003: val_loss improved from 0.44419 to 0.33765, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_BN_7_conv_checkpoint/003-0.3377.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.3518 - acc: 0.9038 - val_loss: 0.3377 - val_acc: 0.9096\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2830 - acc: 0.9216\n",
      "Epoch 00004: val_loss improved from 0.33765 to 0.31078, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_BN_7_conv_checkpoint/004-0.3108.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.2831 - acc: 0.9215 - val_loss: 0.3108 - val_acc: 0.9154\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2358 - acc: 0.9351\n",
      "Epoch 00005: val_loss improved from 0.31078 to 0.25452, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_BN_7_conv_checkpoint/005-0.2545.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.2358 - acc: 0.9351 - val_loss: 0.2545 - val_acc: 0.9301\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2046 - acc: 0.9438\n",
      "Epoch 00006: val_loss did not improve from 0.25452\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.2046 - acc: 0.9438 - val_loss: 0.2838 - val_acc: 0.9145\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1806 - acc: 0.9496\n",
      "Epoch 00007: val_loss improved from 0.25452 to 0.23659, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_BN_7_conv_checkpoint/007-0.2366.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1805 - acc: 0.9496 - val_loss: 0.2366 - val_acc: 0.9334\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1585 - acc: 0.9570\n",
      "Epoch 00008: val_loss improved from 0.23659 to 0.21409, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_BN_7_conv_checkpoint/008-0.2141.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1585 - acc: 0.9570 - val_loss: 0.2141 - val_acc: 0.9378\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1459 - acc: 0.9591\n",
      "Epoch 00009: val_loss improved from 0.21409 to 0.20924, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_BN_7_conv_checkpoint/009-0.2092.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1460 - acc: 0.9591 - val_loss: 0.2092 - val_acc: 0.9345\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1298 - acc: 0.9641\n",
      "Epoch 00010: val_loss did not improve from 0.20924\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1299 - acc: 0.9641 - val_loss: 0.2331 - val_acc: 0.9336\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1228 - acc: 0.9659\n",
      "Epoch 00011: val_loss improved from 0.20924 to 0.19499, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_BN_7_conv_checkpoint/011-0.1950.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1228 - acc: 0.9659 - val_loss: 0.1950 - val_acc: 0.9399\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1056 - acc: 0.9713\n",
      "Epoch 00012: val_loss improved from 0.19499 to 0.19046, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_BN_7_conv_checkpoint/012-0.1905.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1056 - acc: 0.9713 - val_loss: 0.1905 - val_acc: 0.9427\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0932 - acc: 0.9751\n",
      "Epoch 00013: val_loss improved from 0.19046 to 0.18701, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_BN_7_conv_checkpoint/013-0.1870.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0932 - acc: 0.9751 - val_loss: 0.1870 - val_acc: 0.9478\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0863 - acc: 0.9778\n",
      "Epoch 00014: val_loss did not improve from 0.18701\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0862 - acc: 0.9778 - val_loss: 0.1948 - val_acc: 0.9446\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0775 - acc: 0.9800\n",
      "Epoch 00015: val_loss did not improve from 0.18701\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0775 - acc: 0.9800 - val_loss: 0.1906 - val_acc: 0.9450\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0694 - acc: 0.9827\n",
      "Epoch 00016: val_loss improved from 0.18701 to 0.17205, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_BN_7_conv_checkpoint/016-0.1720.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0694 - acc: 0.9827 - val_loss: 0.1720 - val_acc: 0.9495\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0672 - acc: 0.9827\n",
      "Epoch 00017: val_loss did not improve from 0.17205\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0672 - acc: 0.9827 - val_loss: 0.1810 - val_acc: 0.9499\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0574 - acc: 0.9864\n",
      "Epoch 00018: val_loss did not improve from 0.17205\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0574 - acc: 0.9864 - val_loss: 0.2276 - val_acc: 0.9350\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0521 - acc: 0.9870\n",
      "Epoch 00019: val_loss did not improve from 0.17205\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0521 - acc: 0.9870 - val_loss: 0.1774 - val_acc: 0.9511\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0474 - acc: 0.9886\n",
      "Epoch 00020: val_loss did not improve from 0.17205\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0474 - acc: 0.9886 - val_loss: 0.2238 - val_acc: 0.9369\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0456 - acc: 0.9892\n",
      "Epoch 00021: val_loss did not improve from 0.17205\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0456 - acc: 0.9892 - val_loss: 0.2492 - val_acc: 0.9311\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0396 - acc: 0.9909\n",
      "Epoch 00022: val_loss did not improve from 0.17205\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0396 - acc: 0.9909 - val_loss: 0.2159 - val_acc: 0.9376\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0356 - acc: 0.9917\n",
      "Epoch 00023: val_loss did not improve from 0.17205\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0357 - acc: 0.9917 - val_loss: 0.2147 - val_acc: 0.9439\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0393 - acc: 0.9911\n",
      "Epoch 00024: val_loss did not improve from 0.17205\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0393 - acc: 0.9910 - val_loss: 0.1915 - val_acc: 0.9476\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0323 - acc: 0.9931\n",
      "Epoch 00025: val_loss did not improve from 0.17205\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0323 - acc: 0.9931 - val_loss: 0.2638 - val_acc: 0.9276\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0335 - acc: 0.9927\n",
      "Epoch 00026: val_loss did not improve from 0.17205\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0335 - acc: 0.9927 - val_loss: 0.1867 - val_acc: 0.9527\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0246 - acc: 0.9954\n",
      "Epoch 00027: val_loss did not improve from 0.17205\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0246 - acc: 0.9954 - val_loss: 0.1768 - val_acc: 0.9534\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0250 - acc: 0.9954\n",
      "Epoch 00028: val_loss did not improve from 0.17205\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0250 - acc: 0.9954 - val_loss: 0.1859 - val_acc: 0.9481\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0305 - acc: 0.9927\n",
      "Epoch 00029: val_loss did not improve from 0.17205\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0305 - acc: 0.9927 - val_loss: 0.1943 - val_acc: 0.9492\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0199 - acc: 0.9965\n",
      "Epoch 00030: val_loss did not improve from 0.17205\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0199 - acc: 0.9965 - val_loss: 0.2479 - val_acc: 0.9364\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0208 - acc: 0.9962\n",
      "Epoch 00031: val_loss did not improve from 0.17205\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0208 - acc: 0.9962 - val_loss: 0.2088 - val_acc: 0.9429\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0213 - acc: 0.9957\n",
      "Epoch 00032: val_loss did not improve from 0.17205\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0213 - acc: 0.9957 - val_loss: 0.2044 - val_acc: 0.9497\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0223 - acc: 0.9948\n",
      "Epoch 00033: val_loss did not improve from 0.17205\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0224 - acc: 0.9948 - val_loss: 0.2561 - val_acc: 0.9345\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0309 - acc: 0.9928\n",
      "Epoch 00034: val_loss did not improve from 0.17205\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0309 - acc: 0.9928 - val_loss: 0.1929 - val_acc: 0.9518\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0129 - acc: 0.9979\n",
      "Epoch 00035: val_loss did not improve from 0.17205\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0130 - acc: 0.9978 - val_loss: 0.1796 - val_acc: 0.9499\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0270 - acc: 0.9932\n",
      "Epoch 00036: val_loss did not improve from 0.17205\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0271 - acc: 0.9932 - val_loss: 0.3136 - val_acc: 0.9196\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0223 - acc: 0.9945\n",
      "Epoch 00037: val_loss did not improve from 0.17205\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0223 - acc: 0.9945 - val_loss: 0.1787 - val_acc: 0.9529\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0112 - acc: 0.9985\n",
      "Epoch 00038: val_loss did not improve from 0.17205\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0112 - acc: 0.9985 - val_loss: 0.1971 - val_acc: 0.9497\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0116 - acc: 0.9982\n",
      "Epoch 00039: val_loss did not improve from 0.17205\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0116 - acc: 0.9982 - val_loss: 0.2382 - val_acc: 0.9427\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0171 - acc: 0.9964\n",
      "Epoch 00040: val_loss did not improve from 0.17205\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0171 - acc: 0.9964 - val_loss: 0.1825 - val_acc: 0.9525\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0148 - acc: 0.9974\n",
      "Epoch 00041: val_loss did not improve from 0.17205\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0148 - acc: 0.9974 - val_loss: 0.2134 - val_acc: 0.9464\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0172 - acc: 0.9965\n",
      "Epoch 00042: val_loss did not improve from 0.17205\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0172 - acc: 0.9965 - val_loss: 0.2173 - val_acc: 0.9448\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0127 - acc: 0.9972\n",
      "Epoch 00043: val_loss did not improve from 0.17205\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0127 - acc: 0.9972 - val_loss: 0.1843 - val_acc: 0.9569\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0102 - acc: 0.9983\n",
      "Epoch 00044: val_loss did not improve from 0.17205\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0103 - acc: 0.9983 - val_loss: 0.1960 - val_acc: 0.9509\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0250 - acc: 0.9935\n",
      "Epoch 00045: val_loss did not improve from 0.17205\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0250 - acc: 0.9935 - val_loss: 0.1734 - val_acc: 0.9569\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0078 - acc: 0.9989\n",
      "Epoch 00046: val_loss did not improve from 0.17205\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0078 - acc: 0.9989 - val_loss: 0.1882 - val_acc: 0.9536\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0097 - acc: 0.9982\n",
      "Epoch 00047: val_loss did not improve from 0.17205\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0098 - acc: 0.9981 - val_loss: 0.2229 - val_acc: 0.9471\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0307 - acc: 0.9916\n",
      "Epoch 00048: val_loss did not improve from 0.17205\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0308 - acc: 0.9916 - val_loss: 0.1841 - val_acc: 0.9541\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0166 - acc: 0.9965\n",
      "Epoch 00049: val_loss did not improve from 0.17205\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0169 - acc: 0.9964 - val_loss: 0.2115 - val_acc: 0.9490\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0271 - acc: 0.9926\n",
      "Epoch 00050: val_loss did not improve from 0.17205\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0271 - acc: 0.9926 - val_loss: 0.1765 - val_acc: 0.9560\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0082 - acc: 0.9989\n",
      "Epoch 00051: val_loss improved from 0.17205 to 0.17165, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_BN_7_conv_checkpoint/051-0.1717.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0082 - acc: 0.9989 - val_loss: 0.1717 - val_acc: 0.9571\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0064 - acc: 0.9992\n",
      "Epoch 00052: val_loss did not improve from 0.17165\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0064 - acc: 0.9992 - val_loss: 0.1939 - val_acc: 0.9509\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0085 - acc: 0.9985\n",
      "Epoch 00053: val_loss did not improve from 0.17165\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0085 - acc: 0.9985 - val_loss: 0.2450 - val_acc: 0.9441\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0102 - acc: 0.9980\n",
      "Epoch 00054: val_loss did not improve from 0.17165\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0102 - acc: 0.9979 - val_loss: 0.1997 - val_acc: 0.9511\n",
      "Epoch 55/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0166 - acc: 0.9961\n",
      "Epoch 00055: val_loss did not improve from 0.17165\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0166 - acc: 0.9961 - val_loss: 0.2020 - val_acc: 0.9513\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0068 - acc: 0.9992\n",
      "Epoch 00056: val_loss did not improve from 0.17165\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0069 - acc: 0.9991 - val_loss: 0.2206 - val_acc: 0.9483\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0158 - acc: 0.9961\n",
      "Epoch 00057: val_loss did not improve from 0.17165\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0159 - acc: 0.9960 - val_loss: 0.1898 - val_acc: 0.9546\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0211 - acc: 0.9944\n",
      "Epoch 00058: val_loss did not improve from 0.17165\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0211 - acc: 0.9943 - val_loss: 0.1935 - val_acc: 0.9525\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0097 - acc: 0.9982\n",
      "Epoch 00059: val_loss improved from 0.17165 to 0.16811, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_BN_7_conv_checkpoint/059-0.1681.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0097 - acc: 0.9982 - val_loss: 0.1681 - val_acc: 0.9576\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0049 - acc: 0.9993\n",
      "Epoch 00060: val_loss did not improve from 0.16811\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0050 - acc: 0.9993 - val_loss: 0.1946 - val_acc: 0.9541\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0149 - acc: 0.9964\n",
      "Epoch 00061: val_loss did not improve from 0.16811\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0149 - acc: 0.9964 - val_loss: 0.1915 - val_acc: 0.9546\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0064 - acc: 0.9991\n",
      "Epoch 00062: val_loss did not improve from 0.16811\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0064 - acc: 0.9991 - val_loss: 0.1955 - val_acc: 0.9520\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0069 - acc: 0.9987\n",
      "Epoch 00063: val_loss did not improve from 0.16811\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0069 - acc: 0.9987 - val_loss: 0.1895 - val_acc: 0.9560\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0123 - acc: 0.9970\n",
      "Epoch 00064: val_loss did not improve from 0.16811\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0123 - acc: 0.9970 - val_loss: 0.2764 - val_acc: 0.9317\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0061 - acc: 0.9989\n",
      "Epoch 00065: val_loss did not improve from 0.16811\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0061 - acc: 0.9989 - val_loss: 0.1812 - val_acc: 0.9548\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0130 - acc: 0.9964\n",
      "Epoch 00066: val_loss did not improve from 0.16811\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0130 - acc: 0.9964 - val_loss: 0.2058 - val_acc: 0.9499\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0090 - acc: 0.9978\n",
      "Epoch 00067: val_loss did not improve from 0.16811\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0090 - acc: 0.9978 - val_loss: 0.1858 - val_acc: 0.9564\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0067 - acc: 0.9986\n",
      "Epoch 00068: val_loss did not improve from 0.16811\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0067 - acc: 0.9986 - val_loss: 0.2263 - val_acc: 0.9460\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0084 - acc: 0.9982\n",
      "Epoch 00069: val_loss did not improve from 0.16811\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0086 - acc: 0.9981 - val_loss: 0.2452 - val_acc: 0.9422\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0194 - acc: 0.9947\n",
      "Epoch 00070: val_loss did not improve from 0.16811\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0194 - acc: 0.9947 - val_loss: 0.1810 - val_acc: 0.9599\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0037 - acc: 0.9995\n",
      "Epoch 00071: val_loss did not improve from 0.16811\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0037 - acc: 0.9995 - val_loss: 0.1743 - val_acc: 0.9583\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0044 - acc: 0.9993\n",
      "Epoch 00072: val_loss improved from 0.16811 to 0.16313, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_BN_7_conv_checkpoint/072-0.1631.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0044 - acc: 0.9993 - val_loss: 0.1631 - val_acc: 0.9606\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0049 - acc: 0.9992\n",
      "Epoch 00073: val_loss did not improve from 0.16313\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0049 - acc: 0.9992 - val_loss: 0.2071 - val_acc: 0.9536\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0158 - acc: 0.9961\n",
      "Epoch 00074: val_loss did not improve from 0.16313\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0158 - acc: 0.9961 - val_loss: 0.2079 - val_acc: 0.9513\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0065 - acc: 0.9989\n",
      "Epoch 00075: val_loss did not improve from 0.16313\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0065 - acc: 0.9989 - val_loss: 0.2608 - val_acc: 0.9404\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0060 - acc: 0.9990\n",
      "Epoch 00076: val_loss did not improve from 0.16313\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0060 - acc: 0.9990 - val_loss: 0.2522 - val_acc: 0.9455\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0083 - acc: 0.9981\n",
      "Epoch 00077: val_loss did not improve from 0.16313\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0083 - acc: 0.9981 - val_loss: 0.1849 - val_acc: 0.9555\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0056 - acc: 0.9990\n",
      "Epoch 00078: val_loss did not improve from 0.16313\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0056 - acc: 0.9990 - val_loss: 0.2103 - val_acc: 0.9546\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0073 - acc: 0.9984\n",
      "Epoch 00079: val_loss did not improve from 0.16313\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0073 - acc: 0.9984 - val_loss: 0.1963 - val_acc: 0.9522\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0075 - acc: 0.9985\n",
      "Epoch 00080: val_loss did not improve from 0.16313\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0076 - acc: 0.9984 - val_loss: 0.5047 - val_acc: 0.9005\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0161 - acc: 0.9954\n",
      "Epoch 00081: val_loss did not improve from 0.16313\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0161 - acc: 0.9954 - val_loss: 0.1979 - val_acc: 0.9539\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0039 - acc: 0.9995\n",
      "Epoch 00082: val_loss did not improve from 0.16313\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0039 - acc: 0.9995 - val_loss: 0.1693 - val_acc: 0.9604\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0031 - acc: 0.9997\n",
      "Epoch 00083: val_loss did not improve from 0.16313\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0031 - acc: 0.9997 - val_loss: 0.1913 - val_acc: 0.9562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0047 - acc: 0.9992\n",
      "Epoch 00084: val_loss did not improve from 0.16313\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0047 - acc: 0.9992 - val_loss: 0.2299 - val_acc: 0.9483\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0076 - acc: 0.9983\n",
      "Epoch 00085: val_loss did not improve from 0.16313\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0077 - acc: 0.9983 - val_loss: 0.1886 - val_acc: 0.9571\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0141 - acc: 0.9960\n",
      "Epoch 00086: val_loss did not improve from 0.16313\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0140 - acc: 0.9960 - val_loss: 0.1857 - val_acc: 0.9529\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0037 - acc: 0.9993\n",
      "Epoch 00087: val_loss did not improve from 0.16313\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0038 - acc: 0.9993 - val_loss: 0.1929 - val_acc: 0.9555\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0132 - acc: 0.9964\n",
      "Epoch 00088: val_loss improved from 0.16313 to 0.16259, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_BN_7_conv_checkpoint/088-0.1626.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0132 - acc: 0.9964 - val_loss: 0.1626 - val_acc: 0.9613\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0028 - acc: 0.9996\n",
      "Epoch 00089: val_loss did not improve from 0.16259\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0028 - acc: 0.9996 - val_loss: 0.1738 - val_acc: 0.9590\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0045 - acc: 0.9991\n",
      "Epoch 00090: val_loss did not improve from 0.16259\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0045 - acc: 0.9991 - val_loss: 0.2205 - val_acc: 0.9495\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0060 - acc: 0.9986\n",
      "Epoch 00091: val_loss did not improve from 0.16259\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0060 - acc: 0.9986 - val_loss: 0.1908 - val_acc: 0.9569\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0048 - acc: 0.9990\n",
      "Epoch 00092: val_loss did not improve from 0.16259\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0048 - acc: 0.9990 - val_loss: 0.2861 - val_acc: 0.9450\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0068 - acc: 0.9985\n",
      "Epoch 00093: val_loss did not improve from 0.16259\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0068 - acc: 0.9985 - val_loss: 0.2457 - val_acc: 0.9467\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0096 - acc: 0.9977\n",
      "Epoch 00094: val_loss did not improve from 0.16259\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0096 - acc: 0.9977 - val_loss: 0.1890 - val_acc: 0.9522\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0029 - acc: 0.9997\n",
      "Epoch 00095: val_loss did not improve from 0.16259\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0029 - acc: 0.9997 - val_loss: 0.1684 - val_acc: 0.9571\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0098 - acc: 0.9972\n",
      "Epoch 00096: val_loss did not improve from 0.16259\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0098 - acc: 0.9972 - val_loss: 0.1718 - val_acc: 0.9590\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0057 - acc: 0.9987\n",
      "Epoch 00097: val_loss did not improve from 0.16259\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0057 - acc: 0.9987 - val_loss: 0.2087 - val_acc: 0.9541\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0054 - acc: 0.9989\n",
      "Epoch 00098: val_loss did not improve from 0.16259\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0054 - acc: 0.9989 - val_loss: 0.1728 - val_acc: 0.9592\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0037 - acc: 0.9993\n",
      "Epoch 00099: val_loss did not improve from 0.16259\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0037 - acc: 0.9993 - val_loss: 0.1891 - val_acc: 0.9595\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0067 - acc: 0.9986\n",
      "Epoch 00100: val_loss did not improve from 0.16259\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0067 - acc: 0.9986 - val_loss: 0.2404 - val_acc: 0.9481\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0049 - acc: 0.9991\n",
      "Epoch 00101: val_loss did not improve from 0.16259\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0049 - acc: 0.9991 - val_loss: 0.2195 - val_acc: 0.9485\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0103 - acc: 0.9970\n",
      "Epoch 00102: val_loss did not improve from 0.16259\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0103 - acc: 0.9970 - val_loss: 0.1928 - val_acc: 0.9611\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0066 - acc: 0.9985\n",
      "Epoch 00103: val_loss did not improve from 0.16259\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0067 - acc: 0.9985 - val_loss: 0.1900 - val_acc: 0.9606\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0104 - acc: 0.9972\n",
      "Epoch 00104: val_loss did not improve from 0.16259\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0105 - acc: 0.9972 - val_loss: 0.1949 - val_acc: 0.9522\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0091 - acc: 0.9976\n",
      "Epoch 00105: val_loss did not improve from 0.16259\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0091 - acc: 0.9976 - val_loss: 0.1857 - val_acc: 0.9606\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0032 - acc: 0.9994\n",
      "Epoch 00106: val_loss did not improve from 0.16259\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0032 - acc: 0.9994 - val_loss: 0.1764 - val_acc: 0.9592\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0017 - acc: 0.9997\n",
      "Epoch 00107: val_loss did not improve from 0.16259\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0017 - acc: 0.9997 - val_loss: 0.2179 - val_acc: 0.9560\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0085 - acc: 0.9977\n",
      "Epoch 00108: val_loss did not improve from 0.16259\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0085 - acc: 0.9977 - val_loss: 0.2108 - val_acc: 0.9534\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0025 - acc: 0.9998\n",
      "Epoch 00109: val_loss did not improve from 0.16259\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0025 - acc: 0.9998 - val_loss: 0.2019 - val_acc: 0.9557\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0039 - acc: 0.9990\n",
      "Epoch 00110: val_loss did not improve from 0.16259\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0039 - acc: 0.9990 - val_loss: 0.2595 - val_acc: 0.9469\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0052 - acc: 0.9987\n",
      "Epoch 00111: val_loss did not improve from 0.16259\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0052 - acc: 0.9987 - val_loss: 0.2023 - val_acc: 0.9560\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0047 - acc: 0.9989\n",
      "Epoch 00112: val_loss did not improve from 0.16259\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0047 - acc: 0.9989 - val_loss: 0.1983 - val_acc: 0.9567\n",
      "Epoch 113/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0113 - acc: 0.9971\n",
      "Epoch 00113: val_loss did not improve from 0.16259\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0113 - acc: 0.9971 - val_loss: 0.1920 - val_acc: 0.9576\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0023 - acc: 0.9996\n",
      "Epoch 00114: val_loss did not improve from 0.16259\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0024 - acc: 0.9996 - val_loss: 0.1787 - val_acc: 0.9618\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0029 - acc: 0.9995\n",
      "Epoch 00115: val_loss did not improve from 0.16259\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0029 - acc: 0.9995 - val_loss: 0.1664 - val_acc: 0.9630\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0084 - acc: 0.9977\n",
      "Epoch 00116: val_loss did not improve from 0.16259\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0084 - acc: 0.9977 - val_loss: 0.2141 - val_acc: 0.9506\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0044 - acc: 0.9991\n",
      "Epoch 00117: val_loss did not improve from 0.16259\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0044 - acc: 0.9991 - val_loss: 0.1854 - val_acc: 0.9595\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0027 - acc: 0.9994\n",
      "Epoch 00118: val_loss did not improve from 0.16259\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0027 - acc: 0.9994 - val_loss: 0.1942 - val_acc: 0.9571\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0087 - acc: 0.9979\n",
      "Epoch 00119: val_loss did not improve from 0.16259\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0087 - acc: 0.9979 - val_loss: 0.2267 - val_acc: 0.9478\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0067 - acc: 0.9983\n",
      "Epoch 00120: val_loss did not improve from 0.16259\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0067 - acc: 0.9982 - val_loss: 0.2199 - val_acc: 0.9539\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0082 - acc: 0.9976\n",
      "Epoch 00121: val_loss did not improve from 0.16259\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0082 - acc: 0.9976 - val_loss: 0.1740 - val_acc: 0.9592\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0017 - acc: 0.9999\n",
      "Epoch 00122: val_loss did not improve from 0.16259\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0017 - acc: 0.9999 - val_loss: 0.1708 - val_acc: 0.9627\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0031 - acc: 0.9994\n",
      "Epoch 00123: val_loss did not improve from 0.16259\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0031 - acc: 0.9994 - val_loss: 0.2008 - val_acc: 0.9588\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0051 - acc: 0.9989\n",
      "Epoch 00124: val_loss did not improve from 0.16259\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0051 - acc: 0.9989 - val_loss: 0.2845 - val_acc: 0.9411\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0067 - acc: 0.9985\n",
      "Epoch 00125: val_loss did not improve from 0.16259\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0067 - acc: 0.9985 - val_loss: 0.2499 - val_acc: 0.9448\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0045 - acc: 0.9990\n",
      "Epoch 00126: val_loss did not improve from 0.16259\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0045 - acc: 0.9990 - val_loss: 0.1945 - val_acc: 0.9569\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0028 - acc: 0.9996\n",
      "Epoch 00127: val_loss did not improve from 0.16259\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0028 - acc: 0.9996 - val_loss: 0.2147 - val_acc: 0.9546\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0046 - acc: 0.9989\n",
      "Epoch 00128: val_loss did not improve from 0.16259\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0046 - acc: 0.9989 - val_loss: 0.2717 - val_acc: 0.9411\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0056 - acc: 0.9988\n",
      "Epoch 00129: val_loss did not improve from 0.16259\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0056 - acc: 0.9988 - val_loss: 0.2213 - val_acc: 0.9567\n",
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0023 - acc: 0.9996\n",
      "Epoch 00130: val_loss did not improve from 0.16259\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0023 - acc: 0.9996 - val_loss: 0.1827 - val_acc: 0.9620\n",
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0020 - acc: 0.9997\n",
      "Epoch 00131: val_loss did not improve from 0.16259\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0020 - acc: 0.9997 - val_loss: 0.1998 - val_acc: 0.9581\n",
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0086 - acc: 0.9974\n",
      "Epoch 00132: val_loss did not improve from 0.16259\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0087 - acc: 0.9974 - val_loss: 0.2848 - val_acc: 0.9371\n",
      "Epoch 133/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0125 - acc: 0.9961\n",
      "Epoch 00133: val_loss did not improve from 0.16259\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0125 - acc: 0.9961 - val_loss: 0.1854 - val_acc: 0.9581\n",
      "Epoch 134/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0030 - acc: 0.9993\n",
      "Epoch 00134: val_loss did not improve from 0.16259\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0030 - acc: 0.9993 - val_loss: 0.1960 - val_acc: 0.9569\n",
      "Epoch 135/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0012 - acc: 0.9999\n",
      "Epoch 00135: val_loss did not improve from 0.16259\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0012 - acc: 0.9999 - val_loss: 0.1814 - val_acc: 0.9595\n",
      "Epoch 136/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0013 - acc: 0.9998\n",
      "Epoch 00136: val_loss did not improve from 0.16259\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0013 - acc: 0.9998 - val_loss: 0.1812 - val_acc: 0.9613\n",
      "Epoch 137/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0019 - acc: 0.9998\n",
      "Epoch 00137: val_loss did not improve from 0.16259\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0019 - acc: 0.9998 - val_loss: 0.2258 - val_acc: 0.9525\n",
      "Epoch 138/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0099 - acc: 0.9971\n",
      "Epoch 00138: val_loss did not improve from 0.16259\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0099 - acc: 0.9971 - val_loss: 0.2798 - val_acc: 0.9432\n",
      "\n",
      "1D_CNN_custom_multi_3_GAP_BN_7_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 683us/sample - loss: 0.1975 - acc: 0.9551\n",
      "Loss: 0.19750091735963146 Accuracy: 0.9551402\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0358 - acc: 0.6884\n",
      "Epoch 00001: val_loss improved from inf to 0.94102, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_BN_8_conv_checkpoint/001-0.9410.hdf5\n",
      "36805/36805 [==============================] - 74s 2ms/sample - loss: 1.0357 - acc: 0.6884 - val_loss: 0.9410 - val_acc: 0.7209\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4337 - acc: 0.8742\n",
      "Epoch 00002: val_loss improved from 0.94102 to 0.36819, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_BN_8_conv_checkpoint/002-0.3682.hdf5\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.4337 - acc: 0.8742 - val_loss: 0.3682 - val_acc: 0.8903\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3031 - acc: 0.9112\n",
      "Epoch 00003: val_loss improved from 0.36819 to 0.34817, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_BN_8_conv_checkpoint/003-0.3482.hdf5\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.3032 - acc: 0.9112 - val_loss: 0.3482 - val_acc: 0.8910\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2417 - acc: 0.9283\n",
      "Epoch 00004: val_loss improved from 0.34817 to 0.32301, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_BN_8_conv_checkpoint/004-0.3230.hdf5\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.2417 - acc: 0.9283 - val_loss: 0.3230 - val_acc: 0.9019\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1991 - acc: 0.9414\n",
      "Epoch 00005: val_loss improved from 0.32301 to 0.20769, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_BN_8_conv_checkpoint/005-0.2077.hdf5\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.1991 - acc: 0.9413 - val_loss: 0.2077 - val_acc: 0.9364\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1725 - acc: 0.9492\n",
      "Epoch 00006: val_loss did not improve from 0.20769\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.1725 - acc: 0.9492 - val_loss: 0.2309 - val_acc: 0.9269\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1471 - acc: 0.9579\n",
      "Epoch 00007: val_loss did not improve from 0.20769\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.1473 - acc: 0.9579 - val_loss: 0.2792 - val_acc: 0.9108\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1323 - acc: 0.9624\n",
      "Epoch 00008: val_loss improved from 0.20769 to 0.18734, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_BN_8_conv_checkpoint/008-0.1873.hdf5\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.1323 - acc: 0.9624 - val_loss: 0.1873 - val_acc: 0.9427\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1125 - acc: 0.9674\n",
      "Epoch 00009: val_loss did not improve from 0.18734\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.1125 - acc: 0.9674 - val_loss: 0.2043 - val_acc: 0.9355\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0993 - acc: 0.9723\n",
      "Epoch 00010: val_loss did not improve from 0.18734\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0994 - acc: 0.9723 - val_loss: 0.1932 - val_acc: 0.9401\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0984 - acc: 0.9711\n",
      "Epoch 00011: val_loss improved from 0.18734 to 0.16011, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_BN_8_conv_checkpoint/011-0.1601.hdf5\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0985 - acc: 0.9711 - val_loss: 0.1601 - val_acc: 0.9509\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0811 - acc: 0.9776\n",
      "Epoch 00012: val_loss did not improve from 0.16011\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0811 - acc: 0.9776 - val_loss: 0.1880 - val_acc: 0.9406\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0723 - acc: 0.9804\n",
      "Epoch 00013: val_loss did not improve from 0.16011\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0723 - acc: 0.9804 - val_loss: 0.2028 - val_acc: 0.9385\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0624 - acc: 0.9835\n",
      "Epoch 00014: val_loss did not improve from 0.16011\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0625 - acc: 0.9835 - val_loss: 0.1652 - val_acc: 0.9481\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0571 - acc: 0.9848\n",
      "Epoch 00015: val_loss did not improve from 0.16011\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0572 - acc: 0.9848 - val_loss: 0.1697 - val_acc: 0.9485\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0514 - acc: 0.9869\n",
      "Epoch 00016: val_loss did not improve from 0.16011\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0514 - acc: 0.9869 - val_loss: 0.1738 - val_acc: 0.9443\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0418 - acc: 0.9902\n",
      "Epoch 00017: val_loss did not improve from 0.16011\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0419 - acc: 0.9901 - val_loss: 0.1691 - val_acc: 0.9483\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0465 - acc: 0.9876\n",
      "Epoch 00018: val_loss did not improve from 0.16011\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0466 - acc: 0.9876 - val_loss: 0.1800 - val_acc: 0.9476\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0408 - acc: 0.9898\n",
      "Epoch 00019: val_loss improved from 0.16011 to 0.15433, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_BN_8_conv_checkpoint/019-0.1543.hdf5\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0409 - acc: 0.9897 - val_loss: 0.1543 - val_acc: 0.9546\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0406 - acc: 0.9894\n",
      "Epoch 00020: val_loss did not improve from 0.15433\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0407 - acc: 0.9893 - val_loss: 0.1689 - val_acc: 0.9499\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0292 - acc: 0.9933\n",
      "Epoch 00021: val_loss did not improve from 0.15433\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0292 - acc: 0.9933 - val_loss: 0.1778 - val_acc: 0.9522\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0239 - acc: 0.9946\n",
      "Epoch 00022: val_loss did not improve from 0.15433\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0240 - acc: 0.9945 - val_loss: 0.2310 - val_acc: 0.9380\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0350 - acc: 0.9910\n",
      "Epoch 00023: val_loss did not improve from 0.15433\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0350 - acc: 0.9910 - val_loss: 0.1704 - val_acc: 0.9515\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0309 - acc: 0.9920\n",
      "Epoch 00024: val_loss did not improve from 0.15433\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0309 - acc: 0.9920 - val_loss: 0.1562 - val_acc: 0.9536\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0172 - acc: 0.9968\n",
      "Epoch 00025: val_loss did not improve from 0.15433\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0172 - acc: 0.9968 - val_loss: 0.2097 - val_acc: 0.9476\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0199 - acc: 0.9955\n",
      "Epoch 00026: val_loss did not improve from 0.15433\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0199 - acc: 0.9955 - val_loss: 0.2448 - val_acc: 0.9378\n",
      "Epoch 27/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0210 - acc: 0.9951\n",
      "Epoch 00027: val_loss did not improve from 0.15433\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0210 - acc: 0.9951 - val_loss: 0.2527 - val_acc: 0.9324\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0256 - acc: 0.9935\n",
      "Epoch 00028: val_loss did not improve from 0.15433\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0256 - acc: 0.9935 - val_loss: 0.2264 - val_acc: 0.9411\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0146 - acc: 0.9972\n",
      "Epoch 00029: val_loss did not improve from 0.15433\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0146 - acc: 0.9972 - val_loss: 0.1983 - val_acc: 0.9483\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0159 - acc: 0.9967\n",
      "Epoch 00030: val_loss did not improve from 0.15433\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0159 - acc: 0.9967 - val_loss: 0.2138 - val_acc: 0.9443\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0161 - acc: 0.9963\n",
      "Epoch 00031: val_loss did not improve from 0.15433\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0161 - acc: 0.9963 - val_loss: 0.2084 - val_acc: 0.9492\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0197 - acc: 0.9949\n",
      "Epoch 00032: val_loss did not improve from 0.15433\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0197 - acc: 0.9949 - val_loss: 0.1600 - val_acc: 0.9585\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0123 - acc: 0.9974\n",
      "Epoch 00033: val_loss did not improve from 0.15433\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0124 - acc: 0.9974 - val_loss: 0.2043 - val_acc: 0.9485\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0314 - acc: 0.9909\n",
      "Epoch 00034: val_loss did not improve from 0.15433\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0316 - acc: 0.9908 - val_loss: 0.2084 - val_acc: 0.9462\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0328 - acc: 0.9906\n",
      "Epoch 00035: val_loss did not improve from 0.15433\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0328 - acc: 0.9906 - val_loss: 0.1615 - val_acc: 0.9588\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0080 - acc: 0.9987\n",
      "Epoch 00036: val_loss did not improve from 0.15433\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0080 - acc: 0.9987 - val_loss: 0.1668 - val_acc: 0.9567\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0074 - acc: 0.9989\n",
      "Epoch 00037: val_loss did not improve from 0.15433\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0074 - acc: 0.9989 - val_loss: 0.1849 - val_acc: 0.9525\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0102 - acc: 0.9977\n",
      "Epoch 00038: val_loss did not improve from 0.15433\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0103 - acc: 0.9977 - val_loss: 0.1902 - val_acc: 0.9541\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0216 - acc: 0.9938\n",
      "Epoch 00039: val_loss did not improve from 0.15433\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0216 - acc: 0.9938 - val_loss: 0.1816 - val_acc: 0.9560\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0073 - acc: 0.9987\n",
      "Epoch 00040: val_loss did not improve from 0.15433\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0073 - acc: 0.9987 - val_loss: 0.1709 - val_acc: 0.9585\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0099 - acc: 0.9979\n",
      "Epoch 00041: val_loss did not improve from 0.15433\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0099 - acc: 0.9979 - val_loss: 0.2257 - val_acc: 0.9464\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0153 - acc: 0.9961\n",
      "Epoch 00042: val_loss did not improve from 0.15433\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0153 - acc: 0.9961 - val_loss: 0.2565 - val_acc: 0.9317\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0079 - acc: 0.9983\n",
      "Epoch 00043: val_loss did not improve from 0.15433\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0080 - acc: 0.9982 - val_loss: 0.2310 - val_acc: 0.9434\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0236 - acc: 0.9930\n",
      "Epoch 00044: val_loss did not improve from 0.15433\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0236 - acc: 0.9930 - val_loss: 0.1946 - val_acc: 0.9495\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0047 - acc: 0.9993\n",
      "Epoch 00045: val_loss did not improve from 0.15433\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0047 - acc: 0.9993 - val_loss: 0.1646 - val_acc: 0.9571\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0074 - acc: 0.9985\n",
      "Epoch 00046: val_loss did not improve from 0.15433\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0075 - acc: 0.9985 - val_loss: 0.1590 - val_acc: 0.9567\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0240 - acc: 0.9930\n",
      "Epoch 00047: val_loss did not improve from 0.15433\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0242 - acc: 0.9929 - val_loss: 0.2017 - val_acc: 0.9502\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0242 - acc: 0.9930\n",
      "Epoch 00048: val_loss did not improve from 0.15433\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0242 - acc: 0.9930 - val_loss: 0.1844 - val_acc: 0.9555\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0048 - acc: 0.9993\n",
      "Epoch 00049: val_loss did not improve from 0.15433\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0048 - acc: 0.9993 - val_loss: 0.1707 - val_acc: 0.9616\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0039 - acc: 0.9996\n",
      "Epoch 00050: val_loss did not improve from 0.15433\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0039 - acc: 0.9996 - val_loss: 0.1574 - val_acc: 0.9590\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0065 - acc: 0.9987\n",
      "Epoch 00051: val_loss did not improve from 0.15433\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0065 - acc: 0.9987 - val_loss: 0.1975 - val_acc: 0.9562\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0110 - acc: 0.9974\n",
      "Epoch 00052: val_loss did not improve from 0.15433\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0112 - acc: 0.9973 - val_loss: 0.2371 - val_acc: 0.9427\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0233 - acc: 0.9932\n",
      "Epoch 00053: val_loss improved from 0.15433 to 0.15067, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_BN_8_conv_checkpoint/053-0.1507.hdf5\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0233 - acc: 0.9932 - val_loss: 0.1507 - val_acc: 0.9625\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0037 - acc: 0.9995\n",
      "Epoch 00054: val_loss did not improve from 0.15067\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0037 - acc: 0.9995 - val_loss: 0.1527 - val_acc: 0.9651\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0042 - acc: 0.9994\n",
      "Epoch 00055: val_loss did not improve from 0.15067\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0042 - acc: 0.9994 - val_loss: 0.1662 - val_acc: 0.9620\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0091 - acc: 0.9979\n",
      "Epoch 00056: val_loss did not improve from 0.15067\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0091 - acc: 0.9979 - val_loss: 0.1798 - val_acc: 0.9581\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0098 - acc: 0.9975\n",
      "Epoch 00057: val_loss did not improve from 0.15067\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0098 - acc: 0.9975 - val_loss: 0.2789 - val_acc: 0.9392\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0105 - acc: 0.9974\n",
      "Epoch 00058: val_loss did not improve from 0.15067\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0105 - acc: 0.9974 - val_loss: 0.1906 - val_acc: 0.9553\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0088 - acc: 0.9981\n",
      "Epoch 00059: val_loss did not improve from 0.15067\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0088 - acc: 0.9981 - val_loss: 0.2523 - val_acc: 0.9441\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0055 - acc: 0.9987\n",
      "Epoch 00060: val_loss did not improve from 0.15067\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0055 - acc: 0.9987 - val_loss: 0.1700 - val_acc: 0.9602\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0047 - acc: 0.9990\n",
      "Epoch 00061: val_loss did not improve from 0.15067\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0047 - acc: 0.9990 - val_loss: 0.2335 - val_acc: 0.9488\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0128 - acc: 0.9965\n",
      "Epoch 00062: val_loss did not improve from 0.15067\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0128 - acc: 0.9965 - val_loss: 0.2445 - val_acc: 0.9446\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0065 - acc: 0.9982\n",
      "Epoch 00063: val_loss did not improve from 0.15067\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0066 - acc: 0.9982 - val_loss: 0.2382 - val_acc: 0.9481\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0225 - acc: 0.9932\n",
      "Epoch 00064: val_loss did not improve from 0.15067\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0225 - acc: 0.9932 - val_loss: 0.1652 - val_acc: 0.9606\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0054 - acc: 0.9987\n",
      "Epoch 00065: val_loss did not improve from 0.15067\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0054 - acc: 0.9988 - val_loss: 0.2120 - val_acc: 0.9497\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0030 - acc: 0.9995\n",
      "Epoch 00066: val_loss did not improve from 0.15067\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0030 - acc: 0.9995 - val_loss: 0.1624 - val_acc: 0.9627\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0031 - acc: 0.9993\n",
      "Epoch 00067: val_loss did not improve from 0.15067\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0031 - acc: 0.9993 - val_loss: 0.4662 - val_acc: 0.9082\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0061 - acc: 0.9986\n",
      "Epoch 00068: val_loss did not improve from 0.15067\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0061 - acc: 0.9986 - val_loss: 0.2381 - val_acc: 0.9446\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0136 - acc: 0.9955\n",
      "Epoch 00069: val_loss did not improve from 0.15067\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0136 - acc: 0.9955 - val_loss: 0.2430 - val_acc: 0.9450\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0057 - acc: 0.9986\n",
      "Epoch 00070: val_loss did not improve from 0.15067\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0057 - acc: 0.9986 - val_loss: 0.1806 - val_acc: 0.9585\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0036 - acc: 0.9992\n",
      "Epoch 00071: val_loss did not improve from 0.15067\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0036 - acc: 0.9992 - val_loss: 0.1558 - val_acc: 0.9623\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0150 - acc: 0.9957\n",
      "Epoch 00072: val_loss did not improve from 0.15067\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0150 - acc: 0.9957 - val_loss: 0.1944 - val_acc: 0.9536\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0030 - acc: 0.9994\n",
      "Epoch 00073: val_loss did not improve from 0.15067\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0030 - acc: 0.9994 - val_loss: 0.1739 - val_acc: 0.9578\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0031 - acc: 0.9995\n",
      "Epoch 00074: val_loss did not improve from 0.15067\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0031 - acc: 0.9995 - val_loss: 0.2462 - val_acc: 0.9474\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0082 - acc: 0.9979\n",
      "Epoch 00075: val_loss did not improve from 0.15067\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0082 - acc: 0.9979 - val_loss: 0.2525 - val_acc: 0.9427\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0097 - acc: 0.9973\n",
      "Epoch 00076: val_loss did not improve from 0.15067\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0097 - acc: 0.9973 - val_loss: 0.1929 - val_acc: 0.9562\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0031 - acc: 0.9994\n",
      "Epoch 00077: val_loss did not improve from 0.15067\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0032 - acc: 0.9994 - val_loss: 0.2132 - val_acc: 0.9532\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0174 - acc: 0.9948\n",
      "Epoch 00078: val_loss did not improve from 0.15067\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0175 - acc: 0.9947 - val_loss: 0.1855 - val_acc: 0.9569\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0123 - acc: 0.9964\n",
      "Epoch 00079: val_loss did not improve from 0.15067\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0123 - acc: 0.9964 - val_loss: 0.1583 - val_acc: 0.9630\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0038 - acc: 0.9992\n",
      "Epoch 00080: val_loss did not improve from 0.15067\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0038 - acc: 0.9992 - val_loss: 0.2060 - val_acc: 0.9546\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0098 - acc: 0.9972\n",
      "Epoch 00081: val_loss did not improve from 0.15067\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0099 - acc: 0.9971 - val_loss: 0.1575 - val_acc: 0.9606\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0091 - acc: 0.9973\n",
      "Epoch 00082: val_loss did not improve from 0.15067\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0091 - acc: 0.9973 - val_loss: 0.1606 - val_acc: 0.9609\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0025 - acc: 0.9996\n",
      "Epoch 00083: val_loss improved from 0.15067 to 0.15014, saving model to model/checkpoint/1D_CNN_custom_multi_3_GAP_BN_8_conv_checkpoint/083-0.1501.hdf5\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0025 - acc: 0.9996 - val_loss: 0.1501 - val_acc: 0.9637\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0064 - acc: 0.9985\n",
      "Epoch 00084: val_loss did not improve from 0.15014\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0065 - acc: 0.9985 - val_loss: 0.1939 - val_acc: 0.9553\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0053 - acc: 0.9986\n",
      "Epoch 00085: val_loss did not improve from 0.15014\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0053 - acc: 0.9986 - val_loss: 0.2135 - val_acc: 0.9532\n",
      "Epoch 86/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0052 - acc: 0.9988\n",
      "Epoch 00086: val_loss did not improve from 0.15014\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0053 - acc: 0.9988 - val_loss: 0.1957 - val_acc: 0.9532\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0098 - acc: 0.9973\n",
      "Epoch 00087: val_loss did not improve from 0.15014\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0098 - acc: 0.9973 - val_loss: 0.1680 - val_acc: 0.9618\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0052 - acc: 0.9989\n",
      "Epoch 00088: val_loss did not improve from 0.15014\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0052 - acc: 0.9989 - val_loss: 0.1666 - val_acc: 0.9620\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0026 - acc: 0.9995\n",
      "Epoch 00089: val_loss did not improve from 0.15014\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0026 - acc: 0.9995 - val_loss: 0.1692 - val_acc: 0.9627\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0026 - acc: 0.9995\n",
      "Epoch 00090: val_loss did not improve from 0.15014\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0026 - acc: 0.9995 - val_loss: 0.2057 - val_acc: 0.9548\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0026 - acc: 0.9996\n",
      "Epoch 00091: val_loss did not improve from 0.15014\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0026 - acc: 0.9996 - val_loss: 0.1932 - val_acc: 0.9576\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0098 - acc: 0.9973\n",
      "Epoch 00092: val_loss did not improve from 0.15014\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0098 - acc: 0.9973 - val_loss: 0.2184 - val_acc: 0.9518\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0063 - acc: 0.9982\n",
      "Epoch 00093: val_loss did not improve from 0.15014\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0063 - acc: 0.9982 - val_loss: 0.2051 - val_acc: 0.9564\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0139 - acc: 0.9960\n",
      "Epoch 00094: val_loss did not improve from 0.15014\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0139 - acc: 0.9960 - val_loss: 0.1727 - val_acc: 0.9634\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0019 - acc: 0.9997\n",
      "Epoch 00095: val_loss did not improve from 0.15014\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0019 - acc: 0.9997 - val_loss: 0.1632 - val_acc: 0.9639\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0018 - acc: 0.9998\n",
      "Epoch 00096: val_loss did not improve from 0.15014\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0018 - acc: 0.9998 - val_loss: 0.1686 - val_acc: 0.9627\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0061 - acc: 0.9983\n",
      "Epoch 00097: val_loss did not improve from 0.15014\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0061 - acc: 0.9983 - val_loss: 0.2787 - val_acc: 0.9399\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0042 - acc: 0.9991\n",
      "Epoch 00098: val_loss did not improve from 0.15014\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0042 - acc: 0.9991 - val_loss: 0.1936 - val_acc: 0.9597\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9998\n",
      "Epoch 00099: val_loss did not improve from 0.15014\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0022 - acc: 0.9998 - val_loss: 0.1704 - val_acc: 0.9616\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0091 - acc: 0.9972\n",
      "Epoch 00100: val_loss did not improve from 0.15014\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0096 - acc: 0.9972 - val_loss: 0.2098 - val_acc: 0.9527\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0121 - acc: 0.9963\n",
      "Epoch 00101: val_loss did not improve from 0.15014\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0121 - acc: 0.9963 - val_loss: 0.1894 - val_acc: 0.9592\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0021 - acc: 0.9996\n",
      "Epoch 00102: val_loss did not improve from 0.15014\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0021 - acc: 0.9996 - val_loss: 0.1852 - val_acc: 0.9599\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0013 - acc: 0.9998\n",
      "Epoch 00103: val_loss did not improve from 0.15014\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0013 - acc: 0.9998 - val_loss: 0.1768 - val_acc: 0.9604\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0060 - acc: 0.9985\n",
      "Epoch 00104: val_loss did not improve from 0.15014\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0061 - acc: 0.9985 - val_loss: 0.3312 - val_acc: 0.9308\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0147 - acc: 0.9954\n",
      "Epoch 00105: val_loss did not improve from 0.15014\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0147 - acc: 0.9954 - val_loss: 0.1991 - val_acc: 0.9557\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0024 - acc: 0.9995\n",
      "Epoch 00106: val_loss did not improve from 0.15014\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0024 - acc: 0.9995 - val_loss: 0.1717 - val_acc: 0.9604\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0014 - acc: 0.9998\n",
      "Epoch 00107: val_loss did not improve from 0.15014\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0014 - acc: 0.9998 - val_loss: 0.2123 - val_acc: 0.9567\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0061 - acc: 0.9980\n",
      "Epoch 00108: val_loss did not improve from 0.15014\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0062 - acc: 0.9980 - val_loss: 0.2105 - val_acc: 0.9539\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0096 - acc: 0.9974\n",
      "Epoch 00109: val_loss did not improve from 0.15014\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0096 - acc: 0.9973 - val_loss: 0.1659 - val_acc: 0.9644\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0057 - acc: 0.9984\n",
      "Epoch 00110: val_loss did not improve from 0.15014\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0057 - acc: 0.9984 - val_loss: 0.1574 - val_acc: 0.9658\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0016 - acc: 0.9997\n",
      "Epoch 00111: val_loss did not improve from 0.15014\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0016 - acc: 0.9997 - val_loss: 0.1603 - val_acc: 0.9644\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0053 - acc: 0.9986\n",
      "Epoch 00112: val_loss did not improve from 0.15014\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0055 - acc: 0.9985 - val_loss: 0.1920 - val_acc: 0.9569\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0101 - acc: 0.9971\n",
      "Epoch 00113: val_loss did not improve from 0.15014\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0101 - acc: 0.9971 - val_loss: 0.1653 - val_acc: 0.9625\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0017 - acc: 0.9997\n",
      "Epoch 00114: val_loss did not improve from 0.15014\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0017 - acc: 0.9997 - val_loss: 0.1632 - val_acc: 0.9613\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0017 - acc: 0.9998\n",
      "Epoch 00115: val_loss did not improve from 0.15014\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0017 - acc: 0.9998 - val_loss: 0.1680 - val_acc: 0.9620\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0026 - acc: 0.9994\n",
      "Epoch 00116: val_loss did not improve from 0.15014\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0026 - acc: 0.9994 - val_loss: 0.1907 - val_acc: 0.9597\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0066 - acc: 0.9982\n",
      "Epoch 00117: val_loss did not improve from 0.15014\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0066 - acc: 0.9982 - val_loss: 0.2156 - val_acc: 0.9562\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0058 - acc: 0.9984\n",
      "Epoch 00118: val_loss did not improve from 0.15014\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0058 - acc: 0.9984 - val_loss: 0.1756 - val_acc: 0.9592\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0018 - acc: 0.9996\n",
      "Epoch 00119: val_loss did not improve from 0.15014\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0018 - acc: 0.9996 - val_loss: 0.1858 - val_acc: 0.9599\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0015 - acc: 0.9998\n",
      "Epoch 00120: val_loss did not improve from 0.15014\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0015 - acc: 0.9998 - val_loss: 0.2213 - val_acc: 0.9553\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0061 - acc: 0.9984\n",
      "Epoch 00121: val_loss did not improve from 0.15014\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0061 - acc: 0.9984 - val_loss: 0.2321 - val_acc: 0.9515\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0078 - acc: 0.9973\n",
      "Epoch 00122: val_loss did not improve from 0.15014\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0078 - acc: 0.9973 - val_loss: 0.1914 - val_acc: 0.9609\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0019 - acc: 0.9995\n",
      "Epoch 00123: val_loss did not improve from 0.15014\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0020 - acc: 0.9995 - val_loss: 0.2271 - val_acc: 0.9550\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0115 - acc: 0.9965\n",
      "Epoch 00124: val_loss did not improve from 0.15014\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0118 - acc: 0.9965 - val_loss: 0.2052 - val_acc: 0.9560\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0117 - acc: 0.9968\n",
      "Epoch 00125: val_loss did not improve from 0.15014\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0117 - acc: 0.9968 - val_loss: 0.1678 - val_acc: 0.9660\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0013 - acc: 0.9998\n",
      "Epoch 00126: val_loss did not improve from 0.15014\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0014 - acc: 0.9998 - val_loss: 0.1667 - val_acc: 0.9662\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0013 - acc: 0.9998\n",
      "Epoch 00127: val_loss did not improve from 0.15014\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0013 - acc: 0.9998 - val_loss: 0.1594 - val_acc: 0.9679\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 9.2070e-04 - acc: 0.9999\n",
      "Epoch 00128: val_loss did not improve from 0.15014\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 9.9148e-04 - acc: 0.9998 - val_loss: 0.2060 - val_acc: 0.9562\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0100 - acc: 0.9971\n",
      "Epoch 00129: val_loss did not improve from 0.15014\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0100 - acc: 0.9971 - val_loss: 0.1812 - val_acc: 0.9613\n",
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0032 - acc: 0.9992\n",
      "Epoch 00130: val_loss did not improve from 0.15014\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0032 - acc: 0.9992 - val_loss: 0.1644 - val_acc: 0.9667\n",
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0015 - acc: 0.9997\n",
      "Epoch 00131: val_loss did not improve from 0.15014\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0016 - acc: 0.9997 - val_loss: 0.1579 - val_acc: 0.9634\n",
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0084 - acc: 0.9974\n",
      "Epoch 00132: val_loss did not improve from 0.15014\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0084 - acc: 0.9974 - val_loss: 0.2044 - val_acc: 0.9574\n",
      "Epoch 133/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0018 - acc: 0.9996\n",
      "Epoch 00133: val_loss did not improve from 0.15014\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0018 - acc: 0.9996 - val_loss: 0.1718 - val_acc: 0.9646\n",
      "\n",
      "1D_CNN_custom_multi_3_GAP_BN_8_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 754us/sample - loss: 0.2388 - acc: 0.9466\n",
      "Loss: 0.23881585360267746 Accuracy: 0.9466251\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base = '1D_CNN_custom_multi_3_GAP_BN'\n",
    "\n",
    "for i in range(3, 9):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    model = build_cnn(conv_num=i)\n",
    "#         model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4),\n",
    "          metrics=['accuracy'])\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model_filename = model_path+'{epoch:03d}-{val_loss:.4f}.hdf5'\n",
    "    checkpointer = ModelCheckpoint(filepath = model_filename, monitor = \"val_loss\", \n",
    "                                   verbose=1, save_best_only=True)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=50)\n",
    "    hist = model.fit(x_train_abs, y_train_onehot, batch_size=64, epochs=500, \n",
    "                     validation_data=[x_val_abs, y_val_onehot], shuffle=True, \n",
    "                     callbacks = [checkpointer, early_stopping])\n",
    "\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "    ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "    ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "    ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    png_path = 'visualization/learning_curve/'\n",
    "    filename = model_name+'.png'\n",
    "    os.makedirs(png_path, exist_ok=True)\n",
    "    fig.savefig(png_path+filename, transparent=True)\n",
    "\n",
    "    model.save(model_path+'000_last.hdf5')\n",
    "    del(model)\n",
    "    \n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "    model = load_model(model_filename)\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "    print()\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_multi_3_GAP_BN_3_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_33 (Conv1D)              (None, 16000, 64)    384         input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_39 (Batc (None, 16000, 64)    256         conv1d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_39[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling1D) (None, 5333, 64)     0           activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_34 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_33[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_40 (Batc (None, 5333, 64)     256         conv1d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_40[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling1D) (None, 1777, 64)     0           activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_35 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_41 (Batc (None, 1777, 64)     256         conv1d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_41[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling1D) (None, 592, 64)      0           activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_18 (Gl (None, 64)           0           max_pooling1d_33[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_19 (Gl (None, 64)           0           max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_20 (Gl (None, 64)           0           max_pooling1d_35[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 192)          0           global_average_pooling1d_18[0][0]\n",
      "                                                                 global_average_pooling1d_19[0][0]\n",
      "                                                                 global_average_pooling1d_20[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_42 (Batc (None, 192)          768         concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 16)           3088        batch_normalization_v1_42[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 46,096\n",
      "Trainable params: 45,328\n",
      "Non-trainable params: 768\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 723us/sample - loss: 1.0163 - acc: 0.6735\n",
      "Loss: 1.016281790054971 Accuracy: 0.67352027\n",
      "\n",
      "1D_CNN_custom_multi_3_GAP_BN_4_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_8 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_36 (Conv1D)              (None, 16000, 64)    384         input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_43 (Batc (None, 16000, 64)    256         conv1d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_43[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling1D) (None, 5333, 64)     0           activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_37 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_36[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_44 (Batc (None, 5333, 64)     256         conv1d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_44[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling1D) (None, 1777, 64)     0           activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_38 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_37[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_45 (Batc (None, 1777, 64)     256         conv1d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_45[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling1D) (None, 592, 64)      0           activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_39 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_46 (Batc (None, 592, 64)      256         conv1d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 592, 64)      0           batch_normalization_v1_46[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling1D) (None, 197, 64)      0           activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_21 (Gl (None, 64)           0           max_pooling1d_37[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_22 (Gl (None, 64)           0           max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_23 (Gl (None, 64)           0           max_pooling1d_39[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 192)          0           global_average_pooling1d_21[0][0]\n",
      "                                                                 global_average_pooling1d_22[0][0]\n",
      "                                                                 global_average_pooling1d_23[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_47 (Batc (None, 192)          768         concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 16)           3088        batch_normalization_v1_47[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 66,896\n",
      "Trainable params: 66,000\n",
      "Non-trainable params: 896\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 737us/sample - loss: 0.7190 - acc: 0.7807\n",
      "Loss: 0.7190013960266906 Accuracy: 0.78068537\n",
      "\n",
      "1D_CNN_custom_multi_3_GAP_BN_5_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_40 (Conv1D)              (None, 16000, 64)    384         input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_48 (Batc (None, 16000, 64)    256         conv1d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_48[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling1D) (None, 5333, 64)     0           activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_41 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_40[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_49 (Batc (None, 5333, 64)     256         conv1d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_49[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling1D) (None, 1777, 64)     0           activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_42 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_41[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_50 (Batc (None, 1777, 64)     256         conv1d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_50[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling1D) (None, 592, 64)      0           activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_43 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_42[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_51 (Batc (None, 592, 64)      256         conv1d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 592, 64)      0           batch_normalization_v1_51[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling1D) (None, 197, 64)      0           activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_44 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_52 (Batc (None, 197, 128)     512         conv1d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 197, 128)     0           batch_normalization_v1_52[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling1D) (None, 65, 128)      0           activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_24 (Gl (None, 64)           0           max_pooling1d_42[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_25 (Gl (None, 64)           0           max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_26 (Gl (None, 128)          0           max_pooling1d_44[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 256)          0           global_average_pooling1d_24[0][0]\n",
      "                                                                 global_average_pooling1d_25[0][0]\n",
      "                                                                 global_average_pooling1d_26[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_53 (Batc (None, 256)          1024        concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 16)           4112        batch_normalization_v1_53[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 109,776\n",
      "Trainable params: 108,496\n",
      "Non-trainable params: 1,280\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 761us/sample - loss: 0.3889 - acc: 0.8883\n",
      "Loss: 0.388888766405367 Accuracy: 0.88826585\n",
      "\n",
      "1D_CNN_custom_multi_3_GAP_BN_6_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_10 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_45 (Conv1D)              (None, 16000, 64)    384         input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_54 (Batc (None, 16000, 64)    256         conv1d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_54[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling1D) (None, 5333, 64)     0           activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_46 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_45[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_55 (Batc (None, 5333, 64)     256         conv1d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_55[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling1D) (None, 1777, 64)     0           activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_47 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_46[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_56 (Batc (None, 1777, 64)     256         conv1d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_56[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling1D) (None, 592, 64)      0           activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_48 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_47[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_57 (Batc (None, 592, 64)      256         conv1d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 592, 64)      0           batch_normalization_v1_57[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling1D) (None, 197, 64)      0           activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_49 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_48[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_58 (Batc (None, 197, 128)     512         conv1d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 197, 128)     0           batch_normalization_v1_58[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling1D) (None, 65, 128)      0           activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_50 (Conv1D)              (None, 65, 128)      82048       max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_59 (Batc (None, 65, 128)      512         conv1d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 65, 128)      0           batch_normalization_v1_59[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling1D) (None, 21, 128)      0           activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_27 (Gl (None, 64)           0           max_pooling1d_48[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_28 (Gl (None, 128)          0           max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_29 (Gl (None, 128)          0           max_pooling1d_50[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 320)          0           global_average_pooling1d_27[0][0]\n",
      "                                                                 global_average_pooling1d_28[0][0]\n",
      "                                                                 global_average_pooling1d_29[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_60 (Batc (None, 320)          1280        concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 16)           5136        batch_normalization_v1_60[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 193,616\n",
      "Trainable params: 191,952\n",
      "Non-trainable params: 1,664\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 793us/sample - loss: 0.2738 - acc: 0.9246\n",
      "Loss: 0.2737547862319312 Accuracy: 0.9246106\n",
      "\n",
      "1D_CNN_custom_multi_3_GAP_BN_7_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_51 (Conv1D)              (None, 16000, 64)    384         input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_61 (Batc (None, 16000, 64)    256         conv1d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_61[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling1D) (None, 5333, 64)     0           activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_52 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_51[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_62 (Batc (None, 5333, 64)     256         conv1d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_62[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling1D) (None, 1777, 64)     0           activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_53 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_52[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_63 (Batc (None, 1777, 64)     256         conv1d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_63[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling1D) (None, 592, 64)      0           activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_54 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_53[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_64 (Batc (None, 592, 64)      256         conv1d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 592, 64)      0           batch_normalization_v1_64[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling1D) (None, 197, 64)      0           activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_55 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_54[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_65 (Batc (None, 197, 128)     512         conv1d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 197, 128)     0           batch_normalization_v1_65[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling1D) (None, 65, 128)      0           activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_56 (Conv1D)              (None, 65, 128)      82048       max_pooling1d_55[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_66 (Batc (None, 65, 128)      512         conv1d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 65, 128)      0           batch_normalization_v1_66[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling1D) (None, 21, 128)      0           activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_57 (Conv1D)              (None, 21, 128)      82048       max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_67 (Batc (None, 21, 128)      512         conv1d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 21, 128)      0           batch_normalization_v1_67[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling1D) (None, 7, 128)       0           activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_30 (Gl (None, 128)          0           max_pooling1d_55[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_31 (Gl (None, 128)          0           max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_32 (Gl (None, 128)          0           max_pooling1d_57[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 384)          0           global_average_pooling1d_30[0][0]\n",
      "                                                                 global_average_pooling1d_31[0][0]\n",
      "                                                                 global_average_pooling1d_32[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_68 (Batc (None, 384)          1536        concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 16)           6160        batch_normalization_v1_68[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 277,456\n",
      "Trainable params: 275,408\n",
      "Non-trainable params: 2,048\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 848us/sample - loss: 0.1975 - acc: 0.9551\n",
      "Loss: 0.19750091735963146 Accuracy: 0.9551402\n",
      "\n",
      "1D_CNN_custom_multi_3_GAP_BN_8_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_12 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_58 (Conv1D)              (None, 16000, 64)    384         input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_69 (Batc (None, 16000, 64)    256         conv1d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_69[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling1D) (None, 5333, 64)     0           activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_59 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_58[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_70 (Batc (None, 5333, 64)     256         conv1d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_70[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling1D) (None, 1777, 64)     0           activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_60 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_59[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_71 (Batc (None, 1777, 64)     256         conv1d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_71[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling1D) (None, 592, 64)      0           activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_61 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_60[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_72 (Batc (None, 592, 64)      256         conv1d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 592, 64)      0           batch_normalization_v1_72[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling1D) (None, 197, 64)      0           activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_62 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_61[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_73 (Batc (None, 197, 128)     512         conv1d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 197, 128)     0           batch_normalization_v1_73[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling1D) (None, 65, 128)      0           activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_63 (Conv1D)              (None, 65, 128)      82048       max_pooling1d_62[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_74 (Batc (None, 65, 128)      512         conv1d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 65, 128)      0           batch_normalization_v1_74[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling1D) (None, 21, 128)      0           activation_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_64 (Conv1D)              (None, 21, 128)      82048       max_pooling1d_63[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_75 (Batc (None, 21, 128)      512         conv1d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 21, 128)      0           batch_normalization_v1_75[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling1D) (None, 7, 128)       0           activation_64[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_65 (Conv1D)              (None, 7, 128)       82048       max_pooling1d_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_76 (Batc (None, 7, 128)       512         conv1d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 7, 128)       0           batch_normalization_v1_76[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling1D) (None, 2, 128)       0           activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_33 (Gl (None, 128)          0           max_pooling1d_63[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_34 (Gl (None, 128)          0           max_pooling1d_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_35 (Gl (None, 128)          0           max_pooling1d_65[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 384)          0           global_average_pooling1d_33[0][0]\n",
      "                                                                 global_average_pooling1d_34[0][0]\n",
      "                                                                 global_average_pooling1d_35[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_77 (Batc (None, 384)          1536        concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 16)           6160        batch_normalization_v1_77[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 360,016\n",
      "Trainable params: 357,712\n",
      "Non-trainable params: 2,304\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 904us/sample - loss: 0.2388 - acc: 0.9466\n",
      "Loss: 0.23881585360267746 Accuracy: 0.9466251\n"
     ]
    }
   ],
   "source": [
    "log_dir = 'log'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "base = '1D_CNN_custom_multi_3_GAP_BN'\n",
    "\n",
    "with open(path.join(log_dir, base), 'w') as log_file:\n",
    "    for i in range(3, 9):\n",
    "        model_name = base+'_{}_conv'.format(i)\n",
    "        print()\n",
    "        print(model_name, 'Model')\n",
    "        model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "        model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "\n",
    "        model = load_model(model_filename)\n",
    "        model.summary()\n",
    "\n",
    "        [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "        print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "        del(model)\n",
    "\n",
    "        log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_multi_3_GAP_BN_3_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_33 (Conv1D)              (None, 16000, 64)    384         input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_39 (Batc (None, 16000, 64)    256         conv1d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_39[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling1D) (None, 5333, 64)     0           activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_34 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_33[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_40 (Batc (None, 5333, 64)     256         conv1d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_40[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling1D) (None, 1777, 64)     0           activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_35 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_41 (Batc (None, 1777, 64)     256         conv1d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_41[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling1D) (None, 592, 64)      0           activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_18 (Gl (None, 64)           0           max_pooling1d_33[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_19 (Gl (None, 64)           0           max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_20 (Gl (None, 64)           0           max_pooling1d_35[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 192)          0           global_average_pooling1d_18[0][0]\n",
      "                                                                 global_average_pooling1d_19[0][0]\n",
      "                                                                 global_average_pooling1d_20[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_42 (Batc (None, 192)          768         concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 16)           3088        batch_normalization_v1_42[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 46,096\n",
      "Trainable params: 45,328\n",
      "Non-trainable params: 768\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 844us/sample - loss: 1.3308 - acc: 0.5938\n",
      "Loss: 1.3308433296276896 Accuracy: 0.5937695\n",
      "\n",
      "1D_CNN_custom_multi_3_GAP_BN_4_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_8 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_36 (Conv1D)              (None, 16000, 64)    384         input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_43 (Batc (None, 16000, 64)    256         conv1d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_43[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling1D) (None, 5333, 64)     0           activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_37 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_36[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_44 (Batc (None, 5333, 64)     256         conv1d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_44[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling1D) (None, 1777, 64)     0           activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_38 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_37[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_45 (Batc (None, 1777, 64)     256         conv1d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_45[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling1D) (None, 592, 64)      0           activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_39 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_46 (Batc (None, 592, 64)      256         conv1d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 592, 64)      0           batch_normalization_v1_46[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling1D) (None, 197, 64)      0           activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_21 (Gl (None, 64)           0           max_pooling1d_37[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_22 (Gl (None, 64)           0           max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_23 (Gl (None, 64)           0           max_pooling1d_39[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 192)          0           global_average_pooling1d_21[0][0]\n",
      "                                                                 global_average_pooling1d_22[0][0]\n",
      "                                                                 global_average_pooling1d_23[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_47 (Batc (None, 192)          768         concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 16)           3088        batch_normalization_v1_47[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 66,896\n",
      "Trainable params: 66,000\n",
      "Non-trainable params: 896\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 858us/sample - loss: 0.8136 - acc: 0.7784\n",
      "Loss: 0.8135995472579235 Accuracy: 0.77840084\n",
      "\n",
      "1D_CNN_custom_multi_3_GAP_BN_5_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_40 (Conv1D)              (None, 16000, 64)    384         input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_48 (Batc (None, 16000, 64)    256         conv1d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_48[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling1D) (None, 5333, 64)     0           activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_41 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_40[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_49 (Batc (None, 5333, 64)     256         conv1d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_49[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling1D) (None, 1777, 64)     0           activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_42 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_41[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_50 (Batc (None, 1777, 64)     256         conv1d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_50[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling1D) (None, 592, 64)      0           activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_43 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_42[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_51 (Batc (None, 592, 64)      256         conv1d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 592, 64)      0           batch_normalization_v1_51[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling1D) (None, 197, 64)      0           activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_44 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_52 (Batc (None, 197, 128)     512         conv1d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 197, 128)     0           batch_normalization_v1_52[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling1D) (None, 65, 128)      0           activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_24 (Gl (None, 64)           0           max_pooling1d_42[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_25 (Gl (None, 64)           0           max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_26 (Gl (None, 128)          0           max_pooling1d_44[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 256)          0           global_average_pooling1d_24[0][0]\n",
      "                                                                 global_average_pooling1d_25[0][0]\n",
      "                                                                 global_average_pooling1d_26[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_53 (Batc (None, 256)          1024        concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 16)           4112        batch_normalization_v1_53[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 109,776\n",
      "Trainable params: 108,496\n",
      "Non-trainable params: 1,280\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 929us/sample - loss: 0.4430 - acc: 0.8812\n",
      "Loss: 0.4430429780285183 Accuracy: 0.88120455\n",
      "\n",
      "1D_CNN_custom_multi_3_GAP_BN_6_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_10 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_45 (Conv1D)              (None, 16000, 64)    384         input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_54 (Batc (None, 16000, 64)    256         conv1d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_54[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling1D) (None, 5333, 64)     0           activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_46 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_45[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_55 (Batc (None, 5333, 64)     256         conv1d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_55[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling1D) (None, 1777, 64)     0           activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_47 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_46[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_56 (Batc (None, 1777, 64)     256         conv1d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_56[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling1D) (None, 592, 64)      0           activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_48 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_47[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_57 (Batc (None, 592, 64)      256         conv1d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 592, 64)      0           batch_normalization_v1_57[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling1D) (None, 197, 64)      0           activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_49 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_48[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_58 (Batc (None, 197, 128)     512         conv1d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 197, 128)     0           batch_normalization_v1_58[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling1D) (None, 65, 128)      0           activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_50 (Conv1D)              (None, 65, 128)      82048       max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_59 (Batc (None, 65, 128)      512         conv1d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 65, 128)      0           batch_normalization_v1_59[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling1D) (None, 21, 128)      0           activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_27 (Gl (None, 64)           0           max_pooling1d_48[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_28 (Gl (None, 128)          0           max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_29 (Gl (None, 128)          0           max_pooling1d_50[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 320)          0           global_average_pooling1d_27[0][0]\n",
      "                                                                 global_average_pooling1d_28[0][0]\n",
      "                                                                 global_average_pooling1d_29[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_60 (Batc (None, 320)          1280        concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 16)           5136        batch_normalization_v1_60[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 193,616\n",
      "Trainable params: 191,952\n",
      "Non-trainable params: 1,664\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 934us/sample - loss: 0.2757 - acc: 0.9321\n",
      "Loss: 0.27571433610819585 Accuracy: 0.93208724\n",
      "\n",
      "1D_CNN_custom_multi_3_GAP_BN_7_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_51 (Conv1D)              (None, 16000, 64)    384         input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_61 (Batc (None, 16000, 64)    256         conv1d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_61[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling1D) (None, 5333, 64)     0           activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_52 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_51[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_62 (Batc (None, 5333, 64)     256         conv1d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_62[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling1D) (None, 1777, 64)     0           activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_53 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_52[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_63 (Batc (None, 1777, 64)     256         conv1d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_63[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling1D) (None, 592, 64)      0           activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_54 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_53[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_64 (Batc (None, 592, 64)      256         conv1d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 592, 64)      0           batch_normalization_v1_64[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling1D) (None, 197, 64)      0           activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_55 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_54[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_65 (Batc (None, 197, 128)     512         conv1d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 197, 128)     0           batch_normalization_v1_65[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling1D) (None, 65, 128)      0           activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_56 (Conv1D)              (None, 65, 128)      82048       max_pooling1d_55[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_66 (Batc (None, 65, 128)      512         conv1d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 65, 128)      0           batch_normalization_v1_66[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling1D) (None, 21, 128)      0           activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_57 (Conv1D)              (None, 21, 128)      82048       max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_67 (Batc (None, 21, 128)      512         conv1d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 21, 128)      0           batch_normalization_v1_67[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling1D) (None, 7, 128)       0           activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_30 (Gl (None, 128)          0           max_pooling1d_55[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_31 (Gl (None, 128)          0           max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_32 (Gl (None, 128)          0           max_pooling1d_57[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 384)          0           global_average_pooling1d_30[0][0]\n",
      "                                                                 global_average_pooling1d_31[0][0]\n",
      "                                                                 global_average_pooling1d_32[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_68 (Batc (None, 384)          1536        concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 16)           6160        batch_normalization_v1_68[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 277,456\n",
      "Trainable params: 275,408\n",
      "Non-trainable params: 2,048\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 5s 970us/sample - loss: 0.3473 - acc: 0.9279\n",
      "Loss: 0.34734601928839925 Accuracy: 0.9279335\n",
      "\n",
      "1D_CNN_custom_multi_3_GAP_BN_8_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_12 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_58 (Conv1D)              (None, 16000, 64)    384         input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_69 (Batc (None, 16000, 64)    256         conv1d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_69[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling1D) (None, 5333, 64)     0           activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_59 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_58[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_70 (Batc (None, 5333, 64)     256         conv1d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_70[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling1D) (None, 1777, 64)     0           activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_60 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_59[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_71 (Batc (None, 1777, 64)     256         conv1d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_71[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling1D) (None, 592, 64)      0           activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_61 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_60[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_72 (Batc (None, 592, 64)      256         conv1d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 592, 64)      0           batch_normalization_v1_72[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling1D) (None, 197, 64)      0           activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_62 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_61[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_73 (Batc (None, 197, 128)     512         conv1d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 197, 128)     0           batch_normalization_v1_73[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling1D) (None, 65, 128)      0           activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_63 (Conv1D)              (None, 65, 128)      82048       max_pooling1d_62[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_74 (Batc (None, 65, 128)      512         conv1d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 65, 128)      0           batch_normalization_v1_74[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling1D) (None, 21, 128)      0           activation_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_64 (Conv1D)              (None, 21, 128)      82048       max_pooling1d_63[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_75 (Batc (None, 21, 128)      512         conv1d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 21, 128)      0           batch_normalization_v1_75[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling1D) (None, 7, 128)       0           activation_64[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_65 (Conv1D)              (None, 7, 128)       82048       max_pooling1d_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_76 (Batc (None, 7, 128)       512         conv1d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 7, 128)       0           batch_normalization_v1_76[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling1D) (None, 2, 128)       0           activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_33 (Gl (None, 128)          0           max_pooling1d_63[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_34 (Gl (None, 128)          0           max_pooling1d_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_35 (Gl (None, 128)          0           max_pooling1d_65[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 384)          0           global_average_pooling1d_33[0][0]\n",
      "                                                                 global_average_pooling1d_34[0][0]\n",
      "                                                                 global_average_pooling1d_35[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_77 (Batc (None, 384)          1536        concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 16)           6160        batch_normalization_v1_77[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 360,016\n",
      "Trainable params: 357,712\n",
      "Non-trainable params: 2,304\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 970us/sample - loss: 0.2594 - acc: 0.9477\n",
      "Loss: 0.25942051445967146 Accuracy: 0.94766355\n"
     ]
    }
   ],
   "source": [
    "# log_dir = 'log'\n",
    "# os.makedirs(log_dir, exist_ok=True)\n",
    "# base = '1D_CNN_custom_BN_2'\n",
    "\n",
    "# with open(path.join(log_dir, base), 'w') as log_file:\n",
    "for i in range(3, 9):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + '000_last.hdf5'\n",
    "\n",
    "    model = load_model(model_filename)\n",
    "    model.summary()\n",
    "\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "    del(model)\n",
    "\n",
    "#         log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
