{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path as path\n",
    "import itertools\n",
    "from sklearn.preprocessing import maxabs_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Input,InputLayer, Dense, Activation, BatchNormalization, Flatten, Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D, GlobalMaxPool1D, Dropout\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler, \\\n",
    "                                        EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.utils import multi_gpu_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0,1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = path.join('..', 'data')\n",
    "data_dir = path.join(base_dir, 'data_speech_commands_v0.02')\n",
    " \n",
    "train_txt = path.join(data_dir, 'wav_train_16words.txt')\n",
    "val_txt = path.join(data_dir, 'wav_validation_16words.txt')\n",
    "test_txt = path.join(data_dir, 'wav_test_16words.txt')\n",
    "\n",
    "train_data = np.load(path.join(data_dir, 'wav_train_data.npz'))\n",
    "val_data = np.load(path.join(data_dir, 'wav_validation_data.npz'))\n",
    "test_data = np.load(path.join(data_dir, 'wav_test_data.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36805, 16000, 1),\n",
       " (36805,),\n",
       " (4293, 16000, 1),\n",
       " (4293,),\n",
       " (4815, 16000, 1),\n",
       " (4815,),\n",
       " (16, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = train_data['x_train']\n",
    "y_train = train_data['y_train']\n",
    "x_val = val_data['x_val']\n",
    "y_val = val_data['y_val']\n",
    "x_test = test_data['x_test']\n",
    "y_test = test_data['y_test']\n",
    "y_table = test_data['table']\n",
    "\n",
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape, y_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_test[0].shape\n",
    "output_size = y_table.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_abs = np.asarray([maxabs_scale(wav) for wav in x_train])\n",
    "y_train_onehot = np.asarray([to_categorical(label, output_size) for label in y_train])\n",
    "del x_train, y_train\n",
    "\n",
    "x_val_abs = np.asarray([maxabs_scale(wav) for wav in x_val])\n",
    "y_val_onehot = np.asarray([to_categorical(label, output_size) for label in y_val])\n",
    "del x_val, y_val\n",
    "\n",
    "x_test_abs = np.asarray([maxabs_scale(wav) for wav in x_test])\n",
    "y_test_onehot = np.asarray([to_categorical(label, output_size) for label in y_test])\n",
    "del x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_1d_cnn_kaggle_origin(conv_num=1):\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    x = BatchNormalization()(input_layer)\n",
    "    for i in range(conv_num):\n",
    "        for j in range(2):\n",
    "            x = Conv1D(8*(2 ** i), (3),padding = 'same')(x)\n",
    "            x = BatchNormalization()(x)\n",
    "            x = Activation('relu')(x)\n",
    "            x = MaxPooling1D((2), padding='same')(x)\n",
    "\n",
    "    x = Conv1D(1024, (1))(x)\n",
    "    x = GlobalMaxPool1D()(x)\n",
    "    x = Dense(1024, activation = 'relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    output_layer = Dense(output_size, activation = 'softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 16000, 1)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1 (Batc (None, 16000, 1)          4         \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 16000, 8)          32        \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_1 (Ba (None, 16000, 8)          32        \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 16000, 8)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 8000, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 8000, 8)           200       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_2 (Ba (None, 8000, 8)           32        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 8000, 8)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 4000, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 4000, 16)          400       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_3 (Ba (None, 4000, 16)          64        \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 4000, 16)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 2000, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 2000, 16)          784       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_4 (Ba (None, 2000, 16)          64        \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 2000, 16)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 1000, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 1000, 32)          1568      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_5 (Ba (None, 1000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 1000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 500, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 500, 32)           3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_6 (Ba (None, 500, 32)           128       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 500, 32)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 250, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 250, 64)           6208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_7 (Ba (None, 250, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 125, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 125, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_8 (Ba (None, 125, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 125, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 63, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 63, 128)           24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_9 (Ba (None, 63, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 63, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 32, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 32, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_10 (B (None, 32, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 32, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1 (None, 16, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, 16, 1024)          132096    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                16400     \n",
      "=================================================================\n",
      "Total params: 1,298,716\n",
      "Trainable params: 1,297,722\n",
      "Non-trainable params: 994\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 16000, 1)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_11 (B (None, 16000, 1)          4         \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 16000, 8)          32        \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_12 (B (None, 16000, 8)          32        \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 16000, 8)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling (None, 8000, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_12 (Conv1D)           (None, 8000, 8)           200       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_13 (B (None, 8000, 8)           32        \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 8000, 8)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling (None, 4000, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 4000, 16)          400       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_14 (B (None, 4000, 16)          64        \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 4000, 16)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling (None, 2000, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 2000, 16)          784       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_15 (B (None, 2000, 16)          64        \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 2000, 16)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling (None, 1000, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_15 (Conv1D)           (None, 1000, 32)          1568      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_16 (B (None, 1000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 1000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling (None, 500, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_16 (Conv1D)           (None, 500, 32)           3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_17 (B (None, 500, 32)           128       \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 500, 32)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling (None, 250, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_17 (Conv1D)           (None, 250, 64)           6208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_18 (B (None, 250, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling (None, 125, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_18 (Conv1D)           (None, 125, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_19 (B (None, 125, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 125, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling (None, 63, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_19 (Conv1D)           (None, 63, 128)           24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_20 (B (None, 63, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 63, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling (None, 32, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_20 (Conv1D)           (None, 32, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_21 (B (None, 32, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 32, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling (None, 16, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_21 (Conv1D)           (None, 16, 256)           98560     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_22 (B (None, 16, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 16, 256)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling (None, 8, 256)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_22 (Conv1D)           (None, 8, 256)            196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_23 (B (None, 8, 256)            1024      \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 8, 256)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling (None, 4, 256)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_23 (Conv1D)           (None, 4, 1024)           263168    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                16400     \n",
      "=================================================================\n",
      "Total params: 1,727,260\n",
      "Trainable params: 1,725,242\n",
      "Non-trainable params: 2,018\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 16000, 1)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_24 (B (None, 16000, 1)          4         \n",
      "_________________________________________________________________\n",
      "conv1d_24 (Conv1D)           (None, 16000, 8)          32        \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_25 (B (None, 16000, 8)          32        \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 16000, 8)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling (None, 8000, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_25 (Conv1D)           (None, 8000, 8)           200       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_26 (B (None, 8000, 8)           32        \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 8000, 8)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling (None, 4000, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_26 (Conv1D)           (None, 4000, 16)          400       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_27 (B (None, 4000, 16)          64        \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 4000, 16)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling (None, 2000, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_27 (Conv1D)           (None, 2000, 16)          784       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_28 (B (None, 2000, 16)          64        \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 2000, 16)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling (None, 1000, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_28 (Conv1D)           (None, 1000, 32)          1568      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_29 (B (None, 1000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 1000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_26 (MaxPooling (None, 500, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_29 (Conv1D)           (None, 500, 32)           3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_30 (B (None, 500, 32)           128       \n",
      "_________________________________________________________________\n",
      "activation_27 (Activation)   (None, 500, 32)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_27 (MaxPooling (None, 250, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_30 (Conv1D)           (None, 250, 64)           6208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_31 (B (None, 250, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_28 (Activation)   (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_28 (MaxPooling (None, 125, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_31 (Conv1D)           (None, 125, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_32 (B (None, 125, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_29 (Activation)   (None, 125, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_29 (MaxPooling (None, 63, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_32 (Conv1D)           (None, 63, 128)           24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_33 (B (None, 63, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_30 (Activation)   (None, 63, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_30 (MaxPooling (None, 32, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_33 (Conv1D)           (None, 32, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_34 (B (None, 32, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_31 (Activation)   (None, 32, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_31 (MaxPooling (None, 16, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_34 (Conv1D)           (None, 16, 256)           98560     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_35 (B (None, 16, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_32 (Activation)   (None, 16, 256)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_32 (MaxPooling (None, 8, 256)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_35 (Conv1D)           (None, 8, 256)            196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_36 (B (None, 8, 256)            1024      \n",
      "_________________________________________________________________\n",
      "activation_33 (Activation)   (None, 8, 256)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling (None, 4, 256)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_36 (Conv1D)           (None, 4, 512)            393728    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_37 (B (None, 4, 512)            2048      \n",
      "_________________________________________________________________\n",
      "activation_34 (Activation)   (None, 4, 512)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling (None, 2, 512)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_37 (Conv1D)           (None, 2, 512)            786944    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_38 (B (None, 2, 512)            2048      \n",
      "_________________________________________________________________\n",
      "activation_35 (Activation)   (None, 2, 512)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling (None, 1, 512)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_38 (Conv1D)           (None, 1, 1024)           525312    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_2 (Glob (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 16)                16400     \n",
      "=================================================================\n",
      "Total params: 3,174,172\n",
      "Trainable params: 3,170,106\n",
      "Non-trainable params: 4,066\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 16000, 1)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_39 (B (None, 16000, 1)          4         \n",
      "_________________________________________________________________\n",
      "conv1d_39 (Conv1D)           (None, 16000, 8)          32        \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_40 (B (None, 16000, 8)          32        \n",
      "_________________________________________________________________\n",
      "activation_36 (Activation)   (None, 16000, 8)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling (None, 8000, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_40 (Conv1D)           (None, 8000, 8)           200       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_41 (B (None, 8000, 8)           32        \n",
      "_________________________________________________________________\n",
      "activation_37 (Activation)   (None, 8000, 8)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling (None, 4000, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_41 (Conv1D)           (None, 4000, 16)          400       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_42 (B (None, 4000, 16)          64        \n",
      "_________________________________________________________________\n",
      "activation_38 (Activation)   (None, 4000, 16)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling (None, 2000, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_42 (Conv1D)           (None, 2000, 16)          784       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_43 (B (None, 2000, 16)          64        \n",
      "_________________________________________________________________\n",
      "activation_39 (Activation)   (None, 2000, 16)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling (None, 1000, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_43 (Conv1D)           (None, 1000, 32)          1568      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_44 (B (None, 1000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_40 (Activation)   (None, 1000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling (None, 500, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_44 (Conv1D)           (None, 500, 32)           3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_45 (B (None, 500, 32)           128       \n",
      "_________________________________________________________________\n",
      "activation_41 (Activation)   (None, 500, 32)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling (None, 250, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_45 (Conv1D)           (None, 250, 64)           6208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_46 (B (None, 250, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_42 (Activation)   (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling (None, 125, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_46 (Conv1D)           (None, 125, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_47 (B (None, 125, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_43 (Activation)   (None, 125, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling (None, 63, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_47 (Conv1D)           (None, 63, 128)           24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_48 (B (None, 63, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_44 (Activation)   (None, 63, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling (None, 32, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_48 (Conv1D)           (None, 32, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_49 (B (None, 32, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_45 (Activation)   (None, 32, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling (None, 16, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_49 (Conv1D)           (None, 16, 256)           98560     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_50 (B (None, 16, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_46 (Activation)   (None, 16, 256)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling (None, 8, 256)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_50 (Conv1D)           (None, 8, 256)            196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_51 (B (None, 8, 256)            1024      \n",
      "_________________________________________________________________\n",
      "activation_47 (Activation)   (None, 8, 256)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling (None, 4, 256)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_51 (Conv1D)           (None, 4, 512)            393728    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_52 (B (None, 4, 512)            2048      \n",
      "_________________________________________________________________\n",
      "activation_48 (Activation)   (None, 4, 512)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling (None, 2, 512)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_52 (Conv1D)           (None, 2, 512)            786944    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_53 (B (None, 2, 512)            2048      \n",
      "_________________________________________________________________\n",
      "activation_49 (Activation)   (None, 2, 512)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling (None, 1, 512)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_53 (Conv1D)           (None, 1, 1024)           1573888   \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_54 (B (None, 1, 1024)           4096      \n",
      "_________________________________________________________________\n",
      "activation_50 (Activation)   (None, 1, 1024)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling (None, 1, 1024)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_54 (Conv1D)           (None, 1, 1024)           3146752   \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_55 (B (None, 1, 1024)           4096      \n",
      "_________________________________________________________________\n",
      "activation_51 (Activation)   (None, 1, 1024)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling (None, 1, 1024)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_55 (Conv1D)           (None, 1, 1024)           1049600   \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_3 (Glob (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 16)                16400     \n",
      "=================================================================\n",
      "Total params: 8,427,292\n",
      "Trainable params: 8,419,130\n",
      "Non-trainable params: 8,162\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         (None, 16000, 1)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_56 (B (None, 16000, 1)          4         \n",
      "_________________________________________________________________\n",
      "conv1d_56 (Conv1D)           (None, 16000, 8)          32        \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_57 (B (None, 16000, 8)          32        \n",
      "_________________________________________________________________\n",
      "activation_52 (Activation)   (None, 16000, 8)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling (None, 8000, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_57 (Conv1D)           (None, 8000, 8)           200       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_58 (B (None, 8000, 8)           32        \n",
      "_________________________________________________________________\n",
      "activation_53 (Activation)   (None, 8000, 8)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling (None, 4000, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_58 (Conv1D)           (None, 4000, 16)          400       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_59 (B (None, 4000, 16)          64        \n",
      "_________________________________________________________________\n",
      "activation_54 (Activation)   (None, 4000, 16)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling (None, 2000, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_59 (Conv1D)           (None, 2000, 16)          784       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_60 (B (None, 2000, 16)          64        \n",
      "_________________________________________________________________\n",
      "activation_55 (Activation)   (None, 2000, 16)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling (None, 1000, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_60 (Conv1D)           (None, 1000, 32)          1568      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_61 (B (None, 1000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_56 (Activation)   (None, 1000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling (None, 500, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_61 (Conv1D)           (None, 500, 32)           3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_62 (B (None, 500, 32)           128       \n",
      "_________________________________________________________________\n",
      "activation_57 (Activation)   (None, 500, 32)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling (None, 250, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_62 (Conv1D)           (None, 250, 64)           6208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_63 (B (None, 250, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_58 (Activation)   (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling (None, 125, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_63 (Conv1D)           (None, 125, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_64 (B (None, 125, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_59 (Activation)   (None, 125, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling (None, 63, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_64 (Conv1D)           (None, 63, 128)           24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_65 (B (None, 63, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_60 (Activation)   (None, 63, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling (None, 32, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_65 (Conv1D)           (None, 32, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_66 (B (None, 32, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_61 (Activation)   (None, 32, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling (None, 16, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_66 (Conv1D)           (None, 16, 256)           98560     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_67 (B (None, 16, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_62 (Activation)   (None, 16, 256)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling (None, 8, 256)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_67 (Conv1D)           (None, 8, 256)            196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_68 (B (None, 8, 256)            1024      \n",
      "_________________________________________________________________\n",
      "activation_63 (Activation)   (None, 8, 256)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling (None, 4, 256)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_68 (Conv1D)           (None, 4, 512)            393728    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_69 (B (None, 4, 512)            2048      \n",
      "_________________________________________________________________\n",
      "activation_64 (Activation)   (None, 4, 512)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling (None, 2, 512)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_69 (Conv1D)           (None, 2, 512)            786944    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_70 (B (None, 2, 512)            2048      \n",
      "_________________________________________________________________\n",
      "activation_65 (Activation)   (None, 2, 512)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling (None, 1, 512)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_70 (Conv1D)           (None, 1, 1024)           1573888   \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_71 (B (None, 1, 1024)           4096      \n",
      "_________________________________________________________________\n",
      "activation_66 (Activation)   (None, 1, 1024)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_66 (MaxPooling (None, 1, 1024)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_71 (Conv1D)           (None, 1, 1024)           3146752   \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_72 (B (None, 1, 1024)           4096      \n",
      "_________________________________________________________________\n",
      "activation_67 (Activation)   (None, 1, 1024)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_67 (MaxPooling (None, 1, 1024)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_72 (Conv1D)           (None, 1, 2048)           6293504   \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_73 (B (None, 1, 2048)           8192      \n",
      "_________________________________________________________________\n",
      "activation_68 (Activation)   (None, 1, 2048)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_68 (MaxPooling (None, 1, 2048)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_73 (Conv1D)           (None, 1, 2048)           12584960  \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_74 (B (None, 1, 2048)           8192      \n",
      "_________________________________________________________________\n",
      "activation_69 (Activation)   (None, 1, 2048)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_69 (MaxPooling (None, 1, 2048)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_74 (Conv1D)           (None, 1, 1024)           2098176   \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_4 (Glob (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 16)                16400     \n",
      "=================================================================\n",
      "Total params: 28,370,716\n",
      "Trainable params: 28,354,362\n",
      "Non-trainable params: 16,354\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in range(5, 10):\n",
    "    model = build_1d_cnn_kaggle_origin(conv_num=i)\n",
    "    model.summary()\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36805 samples, validate on 4293 samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4876 - acc: 0.5268\n",
      "Epoch 00001: val_loss improved from inf to 0.68036, saving model to model/checkpoint/1D_CNN_kaggle_origin_5_conv_checkpoint/001-0.6804.hdf5\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 1.4875 - acc: 0.5269 - val_loss: 0.6804 - val_acc: 0.7843\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6405 - acc: 0.7971\n",
      "Epoch 00002: val_loss improved from 0.68036 to 0.48461, saving model to model/checkpoint/1D_CNN_kaggle_origin_5_conv_checkpoint/002-0.4846.hdf5\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.6404 - acc: 0.7971 - val_loss: 0.4846 - val_acc: 0.8528\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4542 - acc: 0.8546\n",
      "Epoch 00003: val_loss improved from 0.48461 to 0.34593, saving model to model/checkpoint/1D_CNN_kaggle_origin_5_conv_checkpoint/003-0.3459.hdf5\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.4542 - acc: 0.8546 - val_loss: 0.3459 - val_acc: 0.8954\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3597 - acc: 0.8835\n",
      "Epoch 00004: val_loss improved from 0.34593 to 0.29654, saving model to model/checkpoint/1D_CNN_kaggle_origin_5_conv_checkpoint/004-0.2965.hdf5\n",
      "36805/36805 [==============================] - 31s 851us/sample - loss: 0.3597 - acc: 0.8835 - val_loss: 0.2965 - val_acc: 0.9052\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3085 - acc: 0.8996\n",
      "Epoch 00005: val_loss improved from 0.29654 to 0.26472, saving model to model/checkpoint/1D_CNN_kaggle_origin_5_conv_checkpoint/005-0.2647.hdf5\n",
      "36805/36805 [==============================] - 31s 844us/sample - loss: 0.3085 - acc: 0.8996 - val_loss: 0.2647 - val_acc: 0.9166\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2645 - acc: 0.9148\n",
      "Epoch 00006: val_loss improved from 0.26472 to 0.23465, saving model to model/checkpoint/1D_CNN_kaggle_origin_5_conv_checkpoint/006-0.2346.hdf5\n",
      "36805/36805 [==============================] - 31s 841us/sample - loss: 0.2645 - acc: 0.9148 - val_loss: 0.2346 - val_acc: 0.9241\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2310 - acc: 0.9237\n",
      "Epoch 00007: val_loss improved from 0.23465 to 0.23067, saving model to model/checkpoint/1D_CNN_kaggle_origin_5_conv_checkpoint/007-0.2307.hdf5\n",
      "36805/36805 [==============================] - 31s 847us/sample - loss: 0.2310 - acc: 0.9237 - val_loss: 0.2307 - val_acc: 0.9236\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2067 - acc: 0.9330\n",
      "Epoch 00008: val_loss improved from 0.23067 to 0.20180, saving model to model/checkpoint/1D_CNN_kaggle_origin_5_conv_checkpoint/008-0.2018.hdf5\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.2068 - acc: 0.9329 - val_loss: 0.2018 - val_acc: 0.9345\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1829 - acc: 0.9407\n",
      "Epoch 00009: val_loss did not improve from 0.20180\n",
      "36805/36805 [==============================] - 31s 829us/sample - loss: 0.1829 - acc: 0.9407 - val_loss: 0.2224 - val_acc: 0.9264\n",
      "Epoch 10/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1681 - acc: 0.9451\n",
      "Epoch 00010: val_loss did not improve from 0.20180\n",
      "36805/36805 [==============================] - 30s 803us/sample - loss: 0.1682 - acc: 0.9451 - val_loss: 0.2126 - val_acc: 0.9301\n",
      "Epoch 11/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1500 - acc: 0.9520\n",
      "Epoch 00011: val_loss did not improve from 0.20180\n",
      "36805/36805 [==============================] - 29s 790us/sample - loss: 0.1500 - acc: 0.9519 - val_loss: 0.2132 - val_acc: 0.9357\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1368 - acc: 0.9546\n",
      "Epoch 00012: val_loss improved from 0.20180 to 0.19451, saving model to model/checkpoint/1D_CNN_kaggle_origin_5_conv_checkpoint/012-0.1945.hdf5\n",
      "36805/36805 [==============================] - 29s 782us/sample - loss: 0.1369 - acc: 0.9547 - val_loss: 0.1945 - val_acc: 0.9385\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1237 - acc: 0.9592\n",
      "Epoch 00013: val_loss improved from 0.19451 to 0.18736, saving model to model/checkpoint/1D_CNN_kaggle_origin_5_conv_checkpoint/013-0.1874.hdf5\n",
      "36805/36805 [==============================] - 29s 784us/sample - loss: 0.1237 - acc: 0.9592 - val_loss: 0.1874 - val_acc: 0.9390\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1112 - acc: 0.9636\n",
      "Epoch 00014: val_loss did not improve from 0.18736\n",
      "36805/36805 [==============================] - 28s 774us/sample - loss: 0.1112 - acc: 0.9635 - val_loss: 0.1944 - val_acc: 0.9387\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1028 - acc: 0.9660\n",
      "Epoch 00015: val_loss improved from 0.18736 to 0.18479, saving model to model/checkpoint/1D_CNN_kaggle_origin_5_conv_checkpoint/015-0.1848.hdf5\n",
      "36805/36805 [==============================] - 29s 776us/sample - loss: 0.1028 - acc: 0.9660 - val_loss: 0.1848 - val_acc: 0.9415\n",
      "Epoch 16/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0900 - acc: 0.9704\n",
      "Epoch 00016: val_loss did not improve from 0.18479\n",
      "36805/36805 [==============================] - 28s 774us/sample - loss: 0.0899 - acc: 0.9704 - val_loss: 0.1882 - val_acc: 0.9413\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0824 - acc: 0.9733\n",
      "Epoch 00017: val_loss did not improve from 0.18479\n",
      "36805/36805 [==============================] - 29s 781us/sample - loss: 0.0824 - acc: 0.9733 - val_loss: 0.1991 - val_acc: 0.9394\n",
      "Epoch 18/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0723 - acc: 0.9763\n",
      "Epoch 00018: val_loss did not improve from 0.18479\n",
      "36805/36805 [==============================] - 29s 776us/sample - loss: 0.0724 - acc: 0.9763 - val_loss: 0.1979 - val_acc: 0.9418\n",
      "Epoch 19/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0688 - acc: 0.9775\n",
      "Epoch 00019: val_loss did not improve from 0.18479\n",
      "36805/36805 [==============================] - 28s 771us/sample - loss: 0.0688 - acc: 0.9776 - val_loss: 0.1995 - val_acc: 0.9413\n",
      "Epoch 20/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0663 - acc: 0.9777\n",
      "Epoch 00020: val_loss improved from 0.18479 to 0.18110, saving model to model/checkpoint/1D_CNN_kaggle_origin_5_conv_checkpoint/020-0.1811.hdf5\n",
      "36805/36805 [==============================] - 29s 778us/sample - loss: 0.0663 - acc: 0.9777 - val_loss: 0.1811 - val_acc: 0.9460\n",
      "Epoch 21/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0533 - acc: 0.9831\n",
      "Epoch 00021: val_loss did not improve from 0.18110\n",
      "36805/36805 [==============================] - 28s 772us/sample - loss: 0.0534 - acc: 0.9831 - val_loss: 0.1913 - val_acc: 0.9446\n",
      "Epoch 22/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0488 - acc: 0.9848\n",
      "Epoch 00022: val_loss did not improve from 0.18110\n",
      "36805/36805 [==============================] - 29s 778us/sample - loss: 0.0487 - acc: 0.9848 - val_loss: 0.1845 - val_acc: 0.9462\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0488 - acc: 0.9839\n",
      "Epoch 00023: val_loss did not improve from 0.18110\n",
      "36805/36805 [==============================] - 29s 778us/sample - loss: 0.0488 - acc: 0.9839 - val_loss: 0.1964 - val_acc: 0.9476\n",
      "Epoch 24/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0420 - acc: 0.9873\n",
      "Epoch 00024: val_loss did not improve from 0.18110\n",
      "36805/36805 [==============================] - 28s 772us/sample - loss: 0.0420 - acc: 0.9873 - val_loss: 0.2053 - val_acc: 0.9453\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0401 - acc: 0.9870\n",
      "Epoch 00025: val_loss did not improve from 0.18110\n",
      "36805/36805 [==============================] - 29s 775us/sample - loss: 0.0401 - acc: 0.9870 - val_loss: 0.2163 - val_acc: 0.9434\n",
      "Epoch 26/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0356 - acc: 0.9884\n",
      "Epoch 00026: val_loss did not improve from 0.18110\n",
      "36805/36805 [==============================] - 28s 774us/sample - loss: 0.0355 - acc: 0.9884 - val_loss: 0.2103 - val_acc: 0.9425\n",
      "Epoch 27/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0315 - acc: 0.9910\n",
      "Epoch 00027: val_loss did not improve from 0.18110\n",
      "36805/36805 [==============================] - 28s 770us/sample - loss: 0.0317 - acc: 0.9909 - val_loss: 0.2124 - val_acc: 0.9455\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0352 - acc: 0.9890\n",
      "Epoch 00028: val_loss did not improve from 0.18110\n",
      "36805/36805 [==============================] - 28s 766us/sample - loss: 0.0353 - acc: 0.9890 - val_loss: 0.2441 - val_acc: 0.9399\n",
      "Epoch 29/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0360 - acc: 0.9885\n",
      "Epoch 00029: val_loss did not improve from 0.18110\n",
      "36805/36805 [==============================] - 29s 776us/sample - loss: 0.0361 - acc: 0.9885 - val_loss: 0.2124 - val_acc: 0.9441\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0246 - acc: 0.9925\n",
      "Epoch 00030: val_loss did not improve from 0.18110\n",
      "36805/36805 [==============================] - 28s 774us/sample - loss: 0.0246 - acc: 0.9924 - val_loss: 0.2314 - val_acc: 0.9453\n",
      "Epoch 31/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0317 - acc: 0.9901\n",
      "Epoch 00031: val_loss did not improve from 0.18110\n",
      "36805/36805 [==============================] - 29s 776us/sample - loss: 0.0317 - acc: 0.9902 - val_loss: 0.2343 - val_acc: 0.9406\n",
      "Epoch 32/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0243 - acc: 0.9925\n",
      "Epoch 00032: val_loss did not improve from 0.18110\n",
      "36805/36805 [==============================] - 28s 772us/sample - loss: 0.0244 - acc: 0.9924 - val_loss: 0.2140 - val_acc: 0.9483\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0257 - acc: 0.9920\n",
      "Epoch 00033: val_loss did not improve from 0.18110\n",
      "36805/36805 [==============================] - 28s 769us/sample - loss: 0.0259 - acc: 0.9920 - val_loss: 0.2380 - val_acc: 0.9406\n",
      "Epoch 34/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0299 - acc: 0.9906\n",
      "Epoch 00034: val_loss did not improve from 0.18110\n",
      "36805/36805 [==============================] - 28s 771us/sample - loss: 0.0301 - acc: 0.9905 - val_loss: 0.2292 - val_acc: 0.9464\n",
      "Epoch 35/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0265 - acc: 0.9918\n",
      "Epoch 00035: val_loss did not improve from 0.18110\n",
      "36805/36805 [==============================] - 28s 769us/sample - loss: 0.0265 - acc: 0.9918 - val_loss: 0.2097 - val_acc: 0.9497\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0208 - acc: 0.9933\n",
      "Epoch 00036: val_loss did not improve from 0.18110\n",
      "36805/36805 [==============================] - 28s 773us/sample - loss: 0.0209 - acc: 0.9933 - val_loss: 0.2587 - val_acc: 0.9380\n",
      "Epoch 37/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0219 - acc: 0.9932\n",
      "Epoch 00037: val_loss did not improve from 0.18110\n",
      "36805/36805 [==============================] - 28s 768us/sample - loss: 0.0219 - acc: 0.9932 - val_loss: 0.2398 - val_acc: 0.9429\n",
      "Epoch 38/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0160 - acc: 0.9951\n",
      "Epoch 00038: val_loss did not improve from 0.18110\n",
      "36805/36805 [==============================] - 28s 769us/sample - loss: 0.0164 - acc: 0.9951 - val_loss: 0.2242 - val_acc: 0.9464\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0283 - acc: 0.9906\n",
      "Epoch 00039: val_loss did not improve from 0.18110\n",
      "36805/36805 [==============================] - 28s 762us/sample - loss: 0.0287 - acc: 0.9905 - val_loss: 0.2163 - val_acc: 0.9492\n",
      "Epoch 40/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0231 - acc: 0.9931\n",
      "Epoch 00040: val_loss did not improve from 0.18110\n",
      "36805/36805 [==============================] - 28s 767us/sample - loss: 0.0231 - acc: 0.9931 - val_loss: 0.2181 - val_acc: 0.9469\n",
      "Epoch 41/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0172 - acc: 0.9945\n",
      "Epoch 00041: val_loss did not improve from 0.18110\n",
      "36805/36805 [==============================] - 28s 771us/sample - loss: 0.0173 - acc: 0.9945 - val_loss: 0.2192 - val_acc: 0.9457\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0205 - acc: 0.9938\n",
      "Epoch 00042: val_loss did not improve from 0.18110\n",
      "36805/36805 [==============================] - 28s 767us/sample - loss: 0.0205 - acc: 0.9938 - val_loss: 0.2098 - val_acc: 0.9464\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0130 - acc: 0.9964\n",
      "Epoch 00043: val_loss did not improve from 0.18110\n",
      "36805/36805 [==============================] - 29s 781us/sample - loss: 0.0130 - acc: 0.9964 - val_loss: 0.2404 - val_acc: 0.9450\n",
      "Epoch 44/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0182 - acc: 0.9945\n",
      "Epoch 00044: val_loss did not improve from 0.18110\n",
      "36805/36805 [==============================] - 28s 774us/sample - loss: 0.0182 - acc: 0.9945 - val_loss: 0.2477 - val_acc: 0.9408\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0164 - acc: 0.9951\n",
      "Epoch 00045: val_loss did not improve from 0.18110\n",
      "36805/36805 [==============================] - 29s 777us/sample - loss: 0.0164 - acc: 0.9951 - val_loss: 0.2535 - val_acc: 0.9434\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0147 - acc: 0.9958\n",
      "Epoch 00046: val_loss did not improve from 0.18110\n",
      "36805/36805 [==============================] - 28s 769us/sample - loss: 0.0149 - acc: 0.9958 - val_loss: 0.2595 - val_acc: 0.9422\n",
      "Epoch 47/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0196 - acc: 0.9935\n",
      "Epoch 00047: val_loss did not improve from 0.18110\n",
      "36805/36805 [==============================] - 29s 775us/sample - loss: 0.0196 - acc: 0.9935 - val_loss: 0.2423 - val_acc: 0.9474\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0116 - acc: 0.9965\n",
      "Epoch 00048: val_loss did not improve from 0.18110\n",
      "36805/36805 [==============================] - 28s 763us/sample - loss: 0.0116 - acc: 0.9965 - val_loss: 0.2453 - val_acc: 0.9474\n",
      "Epoch 49/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0131 - acc: 0.9960\n",
      "Epoch 00049: val_loss did not improve from 0.18110\n",
      "36805/36805 [==============================] - 28s 772us/sample - loss: 0.0131 - acc: 0.9960 - val_loss: 0.2807 - val_acc: 0.9387\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0160 - acc: 0.9945\n",
      "Epoch 00050: val_loss did not improve from 0.18110\n",
      "36805/36805 [==============================] - 28s 774us/sample - loss: 0.0161 - acc: 0.9945 - val_loss: 0.2580 - val_acc: 0.9455\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0225 - acc: 0.9925\n",
      "Epoch 00051: val_loss did not improve from 0.18110\n",
      "36805/36805 [==============================] - 28s 769us/sample - loss: 0.0225 - acc: 0.9925 - val_loss: 0.2301 - val_acc: 0.9483\n",
      "Epoch 52/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0103 - acc: 0.9967\n",
      "Epoch 00052: val_loss did not improve from 0.18110\n",
      "36805/36805 [==============================] - 28s 771us/sample - loss: 0.0103 - acc: 0.9967 - val_loss: 0.2565 - val_acc: 0.9464\n",
      "Epoch 53/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0138 - acc: 0.9956\n",
      "Epoch 00053: val_loss did not improve from 0.18110\n",
      "36805/36805 [==============================] - 28s 767us/sample - loss: 0.0138 - acc: 0.9956 - val_loss: 0.2578 - val_acc: 0.9436\n",
      "Epoch 54/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0157 - acc: 0.9948\n",
      "Epoch 00054: val_loss did not improve from 0.18110\n",
      "36805/36805 [==============================] - 28s 772us/sample - loss: 0.0158 - acc: 0.9948 - val_loss: 0.2541 - val_acc: 0.9443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0215 - acc: 0.9927\n",
      "Epoch 00055: val_loss did not improve from 0.18110\n",
      "36805/36805 [==============================] - 28s 773us/sample - loss: 0.0215 - acc: 0.9927 - val_loss: 0.2350 - val_acc: 0.9515\n",
      "Epoch 56/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0076 - acc: 0.9978\n",
      "Epoch 00056: val_loss did not improve from 0.18110\n",
      "36805/36805 [==============================] - 28s 773us/sample - loss: 0.0076 - acc: 0.9978 - val_loss: 0.2388 - val_acc: 0.9499\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0083 - acc: 0.9974\n",
      "Epoch 00057: val_loss did not improve from 0.18110\n",
      "36805/36805 [==============================] - 28s 769us/sample - loss: 0.0083 - acc: 0.9974 - val_loss: 0.2456 - val_acc: 0.9488\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0146 - acc: 0.9951\n",
      "Epoch 00058: val_loss did not improve from 0.18110\n",
      "36805/36805 [==============================] - 28s 770us/sample - loss: 0.0146 - acc: 0.9951 - val_loss: 0.2386 - val_acc: 0.9464\n",
      "Epoch 59/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0148 - acc: 0.9950\n",
      "Epoch 00059: val_loss did not improve from 0.18110\n",
      "36805/36805 [==============================] - 28s 765us/sample - loss: 0.0149 - acc: 0.9949 - val_loss: 0.2605 - val_acc: 0.9460\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0104 - acc: 0.9966\n",
      "Epoch 00060: val_loss did not improve from 0.18110\n",
      "36805/36805 [==============================] - 28s 774us/sample - loss: 0.0104 - acc: 0.9966 - val_loss: 0.2564 - val_acc: 0.9446\n",
      "Epoch 61/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0102 - acc: 0.9966\n",
      "Epoch 00061: val_loss did not improve from 0.18110\n",
      "36805/36805 [==============================] - 28s 769us/sample - loss: 0.0102 - acc: 0.9966 - val_loss: 0.2849 - val_acc: 0.9425\n",
      "Epoch 62/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0119 - acc: 0.9959\n",
      "Epoch 00062: val_loss did not improve from 0.18110\n",
      "36805/36805 [==============================] - 28s 766us/sample - loss: 0.0119 - acc: 0.9959 - val_loss: 0.2613 - val_acc: 0.9448\n",
      "Epoch 63/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0126 - acc: 0.9959\n",
      "Epoch 00063: val_loss did not improve from 0.18110\n",
      "36805/36805 [==============================] - 28s 759us/sample - loss: 0.0127 - acc: 0.9959 - val_loss: 0.2721 - val_acc: 0.9429\n",
      "Epoch 64/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0191 - acc: 0.9934\n",
      "Epoch 00064: val_loss did not improve from 0.18110\n",
      "36805/36805 [==============================] - 28s 761us/sample - loss: 0.0191 - acc: 0.9934 - val_loss: 0.2495 - val_acc: 0.9483\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0085 - acc: 0.9971\n",
      "Epoch 00065: val_loss did not improve from 0.18110\n",
      "36805/36805 [==============================] - 28s 772us/sample - loss: 0.0085 - acc: 0.9971 - val_loss: 0.2725 - val_acc: 0.9399\n",
      "Epoch 66/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0105 - acc: 0.9968\n",
      "Epoch 00066: val_loss did not improve from 0.18110\n",
      "36805/36805 [==============================] - 28s 772us/sample - loss: 0.0108 - acc: 0.9967 - val_loss: 0.3322 - val_acc: 0.9345\n",
      "Epoch 67/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0208 - acc: 0.9942\n",
      "Epoch 00067: val_loss did not improve from 0.18110\n",
      "36805/36805 [==============================] - 28s 766us/sample - loss: 0.0208 - acc: 0.9942 - val_loss: 0.2219 - val_acc: 0.9543\n",
      "Epoch 68/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0065 - acc: 0.9980\n",
      "Epoch 00068: val_loss did not improve from 0.18110\n",
      "36805/36805 [==============================] - 28s 766us/sample - loss: 0.0065 - acc: 0.9980 - val_loss: 0.2824 - val_acc: 0.9455\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0075 - acc: 0.9976\n",
      "Epoch 00069: val_loss did not improve from 0.18110\n",
      "36805/36805 [==============================] - 28s 768us/sample - loss: 0.0075 - acc: 0.9976 - val_loss: 0.2567 - val_acc: 0.9478\n",
      "Epoch 70/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0088 - acc: 0.9973\n",
      "Epoch 00070: val_loss did not improve from 0.18110\n",
      "36805/36805 [==============================] - 28s 767us/sample - loss: 0.0088 - acc: 0.9973 - val_loss: 0.2856 - val_acc: 0.9441\n",
      "\n",
      "1D_CNN_kaggle_origin_5_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 594us/sample - loss: 0.2486 - acc: 0.9277\n",
      "Loss: 0.24860892441444804 Accuracy: 0.92772585\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4402 - acc: 0.5341\n",
      "Epoch 00001: val_loss improved from inf to 0.69611, saving model to model/checkpoint/1D_CNN_kaggle_origin_6_conv_checkpoint/001-0.6961.hdf5\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 1.4402 - acc: 0.5342 - val_loss: 0.6961 - val_acc: 0.7703\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6576 - acc: 0.7842\n",
      "Epoch 00002: val_loss improved from 0.69611 to 0.46086, saving model to model/checkpoint/1D_CNN_kaggle_origin_6_conv_checkpoint/002-0.4609.hdf5\n",
      "36805/36805 [==============================] - 32s 876us/sample - loss: 0.6577 - acc: 0.7842 - val_loss: 0.4609 - val_acc: 0.8488\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4658 - acc: 0.8438\n",
      "Epoch 00003: val_loss improved from 0.46086 to 0.38282, saving model to model/checkpoint/1D_CNN_kaggle_origin_6_conv_checkpoint/003-0.3828.hdf5\n",
      "36805/36805 [==============================] - 32s 871us/sample - loss: 0.4661 - acc: 0.8437 - val_loss: 0.3828 - val_acc: 0.8728\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3622 - acc: 0.8805\n",
      "Epoch 00004: val_loss improved from 0.38282 to 0.32038, saving model to model/checkpoint/1D_CNN_kaggle_origin_6_conv_checkpoint/004-0.3204.hdf5\n",
      "36805/36805 [==============================] - 32s 866us/sample - loss: 0.3622 - acc: 0.8805 - val_loss: 0.3204 - val_acc: 0.8980\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3015 - acc: 0.8988\n",
      "Epoch 00005: val_loss improved from 0.32038 to 0.30767, saving model to model/checkpoint/1D_CNN_kaggle_origin_6_conv_checkpoint/005-0.3077.hdf5\n",
      "36805/36805 [==============================] - 32s 872us/sample - loss: 0.3017 - acc: 0.8988 - val_loss: 0.3077 - val_acc: 0.9031\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2519 - acc: 0.9155\n",
      "Epoch 00006: val_loss did not improve from 0.30767\n",
      "36805/36805 [==============================] - 32s 862us/sample - loss: 0.2520 - acc: 0.9155 - val_loss: 0.3228 - val_acc: 0.8940\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2116 - acc: 0.9279\n",
      "Epoch 00007: val_loss improved from 0.30767 to 0.27588, saving model to model/checkpoint/1D_CNN_kaggle_origin_6_conv_checkpoint/007-0.2759.hdf5\n",
      "36805/36805 [==============================] - 32s 876us/sample - loss: 0.2119 - acc: 0.9278 - val_loss: 0.2759 - val_acc: 0.9122\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1836 - acc: 0.9380\n",
      "Epoch 00008: val_loss improved from 0.27588 to 0.26626, saving model to model/checkpoint/1D_CNN_kaggle_origin_6_conv_checkpoint/008-0.2663.hdf5\n",
      "36805/36805 [==============================] - 32s 863us/sample - loss: 0.1836 - acc: 0.9380 - val_loss: 0.2663 - val_acc: 0.9140\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1521 - acc: 0.9485\n",
      "Epoch 00009: val_loss did not improve from 0.26626\n",
      "36805/36805 [==============================] - 32s 871us/sample - loss: 0.1521 - acc: 0.9485 - val_loss: 0.2975 - val_acc: 0.9119\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1399 - acc: 0.9530\n",
      "Epoch 00010: val_loss did not improve from 0.26626\n",
      "36805/36805 [==============================] - 31s 852us/sample - loss: 0.1402 - acc: 0.9529 - val_loss: 0.2709 - val_acc: 0.9185\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1215 - acc: 0.9593\n",
      "Epoch 00011: val_loss did not improve from 0.26626\n",
      "36805/36805 [==============================] - 32s 871us/sample - loss: 0.1216 - acc: 0.9592 - val_loss: 0.2690 - val_acc: 0.9245\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1006 - acc: 0.9653\n",
      "Epoch 00012: val_loss did not improve from 0.26626\n",
      "36805/36805 [==============================] - 32s 865us/sample - loss: 0.1007 - acc: 0.9653 - val_loss: 0.3016 - val_acc: 0.9124\n",
      "Epoch 13/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0924 - acc: 0.9692\n",
      "Epoch 00013: val_loss did not improve from 0.26626\n",
      "36805/36805 [==============================] - 32s 873us/sample - loss: 0.0924 - acc: 0.9692 - val_loss: 0.2882 - val_acc: 0.9236\n",
      "Epoch 14/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0811 - acc: 0.9724\n",
      "Epoch 00014: val_loss did not improve from 0.26626\n",
      "36805/36805 [==============================] - 32s 868us/sample - loss: 0.0811 - acc: 0.9724 - val_loss: 0.2858 - val_acc: 0.9248\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0690 - acc: 0.9769\n",
      "Epoch 00015: val_loss did not improve from 0.26626\n",
      "36805/36805 [==============================] - 32s 872us/sample - loss: 0.0691 - acc: 0.9769 - val_loss: 0.3048 - val_acc: 0.9192\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0653 - acc: 0.9774\n",
      "Epoch 00016: val_loss did not improve from 0.26626\n",
      "36805/36805 [==============================] - 32s 867us/sample - loss: 0.0656 - acc: 0.9773 - val_loss: 0.3504 - val_acc: 0.9124\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0734 - acc: 0.9749\n",
      "Epoch 00017: val_loss did not improve from 0.26626\n",
      "36805/36805 [==============================] - 32s 865us/sample - loss: 0.0736 - acc: 0.9749 - val_loss: 0.2934 - val_acc: 0.9224\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0486 - acc: 0.9842\n",
      "Epoch 00018: val_loss did not improve from 0.26626\n",
      "36805/36805 [==============================] - 32s 862us/sample - loss: 0.0486 - acc: 0.9842 - val_loss: 0.3112 - val_acc: 0.9273\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0548 - acc: 0.9820\n",
      "Epoch 00019: val_loss did not improve from 0.26626\n",
      "36805/36805 [==============================] - 32s 862us/sample - loss: 0.0548 - acc: 0.9820 - val_loss: 0.3093 - val_acc: 0.9255\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0397 - acc: 0.9868\n",
      "Epoch 00020: val_loss did not improve from 0.26626\n",
      "36805/36805 [==============================] - 32s 865us/sample - loss: 0.0397 - acc: 0.9868 - val_loss: 0.3206 - val_acc: 0.9290\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0384 - acc: 0.9874\n",
      "Epoch 00021: val_loss did not improve from 0.26626\n",
      "36805/36805 [==============================] - 32s 864us/sample - loss: 0.0384 - acc: 0.9874 - val_loss: 0.3575 - val_acc: 0.9147\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0521 - acc: 0.9826\n",
      "Epoch 00022: val_loss did not improve from 0.26626\n",
      "36805/36805 [==============================] - 32s 865us/sample - loss: 0.0521 - acc: 0.9826 - val_loss: 0.3306 - val_acc: 0.9234\n",
      "Epoch 23/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0331 - acc: 0.9894\n",
      "Epoch 00023: val_loss did not improve from 0.26626\n",
      "36805/36805 [==============================] - 32s 870us/sample - loss: 0.0332 - acc: 0.9894 - val_loss: 0.2992 - val_acc: 0.9262\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0369 - acc: 0.9878\n",
      "Epoch 00024: val_loss did not improve from 0.26626\n",
      "36805/36805 [==============================] - 32s 866us/sample - loss: 0.0370 - acc: 0.9878 - val_loss: 0.3655 - val_acc: 0.9143\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0388 - acc: 0.9879\n",
      "Epoch 00025: val_loss did not improve from 0.26626\n",
      "36805/36805 [==============================] - 32s 867us/sample - loss: 0.0394 - acc: 0.9878 - val_loss: 0.3420 - val_acc: 0.9231\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0503 - acc: 0.9843\n",
      "Epoch 00026: val_loss did not improve from 0.26626\n",
      "36805/36805 [==============================] - 32s 864us/sample - loss: 0.0507 - acc: 0.9843 - val_loss: 0.3399 - val_acc: 0.9231\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0410 - acc: 0.9869\n",
      "Epoch 00027: val_loss did not improve from 0.26626\n",
      "36805/36805 [==============================] - 32s 864us/sample - loss: 0.0410 - acc: 0.9869 - val_loss: 0.3133 - val_acc: 0.9264\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0226 - acc: 0.9927\n",
      "Epoch 00028: val_loss did not improve from 0.26626\n",
      "36805/36805 [==============================] - 32s 867us/sample - loss: 0.0226 - acc: 0.9927 - val_loss: 0.3605 - val_acc: 0.9241\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0254 - acc: 0.9916\n",
      "Epoch 00029: val_loss did not improve from 0.26626\n",
      "36805/36805 [==============================] - 32s 866us/sample - loss: 0.0254 - acc: 0.9916 - val_loss: 0.3294 - val_acc: 0.9299\n",
      "Epoch 30/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0254 - acc: 0.9911\n",
      "Epoch 00030: val_loss did not improve from 0.26626\n",
      "36805/36805 [==============================] - 32s 868us/sample - loss: 0.0254 - acc: 0.9911 - val_loss: 0.3778 - val_acc: 0.9229\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0270 - acc: 0.9906\n",
      "Epoch 00031: val_loss did not improve from 0.26626\n",
      "36805/36805 [==============================] - 32s 862us/sample - loss: 0.0270 - acc: 0.9906 - val_loss: 0.3751 - val_acc: 0.9229\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0252 - acc: 0.9912\n",
      "Epoch 00032: val_loss did not improve from 0.26626\n",
      "36805/36805 [==============================] - 32s 871us/sample - loss: 0.0252 - acc: 0.9912 - val_loss: 0.3791 - val_acc: 0.9154\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0315 - acc: 0.9892\n",
      "Epoch 00033: val_loss did not improve from 0.26626\n",
      "36805/36805 [==============================] - 32s 869us/sample - loss: 0.0315 - acc: 0.9892 - val_loss: 0.3327 - val_acc: 0.9292\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0242 - acc: 0.9920\n",
      "Epoch 00034: val_loss did not improve from 0.26626\n",
      "36805/36805 [==============================] - 32s 867us/sample - loss: 0.0242 - acc: 0.9920 - val_loss: 0.3629 - val_acc: 0.9264\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0210 - acc: 0.9927\n",
      "Epoch 00035: val_loss did not improve from 0.26626\n",
      "36805/36805 [==============================] - 32s 865us/sample - loss: 0.0210 - acc: 0.9927 - val_loss: 0.3353 - val_acc: 0.9317\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0242 - acc: 0.9921\n",
      "Epoch 00036: val_loss did not improve from 0.26626\n",
      "36805/36805 [==============================] - 32s 863us/sample - loss: 0.0242 - acc: 0.9921 - val_loss: 0.3589 - val_acc: 0.9241\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0274 - acc: 0.9913\n",
      "Epoch 00037: val_loss did not improve from 0.26626\n",
      "36805/36805 [==============================] - 32s 869us/sample - loss: 0.0275 - acc: 0.9913 - val_loss: 0.3888 - val_acc: 0.9257\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0296 - acc: 0.9908\n",
      "Epoch 00038: val_loss did not improve from 0.26626\n",
      "36805/36805 [==============================] - 32s 878us/sample - loss: 0.0297 - acc: 0.9908 - val_loss: 0.3640 - val_acc: 0.9241\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0316 - acc: 0.9900\n",
      "Epoch 00039: val_loss did not improve from 0.26626\n",
      "36805/36805 [==============================] - 32s 872us/sample - loss: 0.0318 - acc: 0.9899 - val_loss: 0.3375 - val_acc: 0.9276\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0225 - acc: 0.9929\n",
      "Epoch 00040: val_loss did not improve from 0.26626\n",
      "36805/36805 [==============================] - 32s 870us/sample - loss: 0.0225 - acc: 0.9929 - val_loss: 0.3474 - val_acc: 0.9271\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0144 - acc: 0.9948\n",
      "Epoch 00041: val_loss did not improve from 0.26626\n",
      "36805/36805 [==============================] - 32s 866us/sample - loss: 0.0144 - acc: 0.9948 - val_loss: 0.3416 - val_acc: 0.9350\n",
      "Epoch 42/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0227 - acc: 0.9927\n",
      "Epoch 00042: val_loss did not improve from 0.26626\n",
      "36805/36805 [==============================] - 32s 872us/sample - loss: 0.0227 - acc: 0.9927 - val_loss: 0.3773 - val_acc: 0.9269\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0198 - acc: 0.9931\n",
      "Epoch 00043: val_loss did not improve from 0.26626\n",
      "36805/36805 [==============================] - 32s 868us/sample - loss: 0.0198 - acc: 0.9931 - val_loss: 0.3696 - val_acc: 0.9280\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0189 - acc: 0.9937\n",
      "Epoch 00044: val_loss did not improve from 0.26626\n",
      "36805/36805 [==============================] - 32s 867us/sample - loss: 0.0189 - acc: 0.9938 - val_loss: 0.4211 - val_acc: 0.9166\n",
      "Epoch 45/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0189 - acc: 0.9935\n",
      "Epoch 00045: val_loss did not improve from 0.26626\n",
      "36805/36805 [==============================] - 32s 862us/sample - loss: 0.0189 - acc: 0.9935 - val_loss: 0.3710 - val_acc: 0.9273\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0214 - acc: 0.9927\n",
      "Epoch 00046: val_loss did not improve from 0.26626\n",
      "36805/36805 [==============================] - 32s 870us/sample - loss: 0.0215 - acc: 0.9927 - val_loss: 0.4371 - val_acc: 0.9161\n",
      "Epoch 47/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0268 - acc: 0.9909\n",
      "Epoch 00047: val_loss did not improve from 0.26626\n",
      "36805/36805 [==============================] - 32s 873us/sample - loss: 0.0269 - acc: 0.9908 - val_loss: 0.3793 - val_acc: 0.9266\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0180 - acc: 0.9940\n",
      "Epoch 00048: val_loss did not improve from 0.26626\n",
      "36805/36805 [==============================] - 32s 876us/sample - loss: 0.0180 - acc: 0.9940 - val_loss: 0.3443 - val_acc: 0.9301\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0129 - acc: 0.9952\n",
      "Epoch 00049: val_loss did not improve from 0.26626\n",
      "36805/36805 [==============================] - 32s 867us/sample - loss: 0.0130 - acc: 0.9952 - val_loss: 0.4229 - val_acc: 0.9283\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0277 - acc: 0.9917\n",
      "Epoch 00050: val_loss did not improve from 0.26626\n",
      "36805/36805 [==============================] - 32s 865us/sample - loss: 0.0277 - acc: 0.9916 - val_loss: 0.3698 - val_acc: 0.9311\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0169 - acc: 0.9943\n",
      "Epoch 00051: val_loss did not improve from 0.26626\n",
      "36805/36805 [==============================] - 32s 877us/sample - loss: 0.0172 - acc: 0.9943 - val_loss: 0.3828 - val_acc: 0.9252\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0214 - acc: 0.9938\n",
      "Epoch 00052: val_loss did not improve from 0.26626\n",
      "36805/36805 [==============================] - 33s 884us/sample - loss: 0.0214 - acc: 0.9938 - val_loss: 0.3324 - val_acc: 0.9336\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0145 - acc: 0.9953\n",
      "Epoch 00053: val_loss did not improve from 0.26626\n",
      "36805/36805 [==============================] - 32s 876us/sample - loss: 0.0145 - acc: 0.9953 - val_loss: 0.3781 - val_acc: 0.9280\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0124 - acc: 0.9957\n",
      "Epoch 00054: val_loss did not improve from 0.26626\n",
      "36805/36805 [==============================] - 32s 876us/sample - loss: 0.0124 - acc: 0.9957 - val_loss: 0.4478 - val_acc: 0.9243\n",
      "Epoch 55/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0165 - acc: 0.9945\n",
      "Epoch 00055: val_loss did not improve from 0.26626\n",
      "36805/36805 [==============================] - 32s 868us/sample - loss: 0.0167 - acc: 0.9945 - val_loss: 0.3945 - val_acc: 0.9271\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0286 - acc: 0.9910\n",
      "Epoch 00056: val_loss did not improve from 0.26626\n",
      "36805/36805 [==============================] - 32s 874us/sample - loss: 0.0286 - acc: 0.9910 - val_loss: 0.3370 - val_acc: 0.9329\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0109 - acc: 0.9964\n",
      "Epoch 00057: val_loss did not improve from 0.26626\n",
      "36805/36805 [==============================] - 32s 868us/sample - loss: 0.0109 - acc: 0.9964 - val_loss: 0.3744 - val_acc: 0.9315\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0144 - acc: 0.9953\n",
      "Epoch 00058: val_loss did not improve from 0.26626\n",
      "36805/36805 [==============================] - 32s 865us/sample - loss: 0.0148 - acc: 0.9953 - val_loss: 0.4442 - val_acc: 0.9161\n",
      "\n",
      "1D_CNN_kaggle_origin_6_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzs3Xd4HMXdwPHvXJN06s1yFbKNq9wtl2BjQ+glpmMIhFBiAiEkhLwEv7QQAgkkJAESCK9DCDgUQ6gmdjAlLhQb997kLsnq/dSuzfvHSCfJKpZlnU+2fp/n2ed0e3s7s6e7+e2UnVVaa4QQQggAS6gzIIQQovuQoCCEECJAgoIQQogACQpCCCECJCgIIYQIkKAghBAiQIKCEEKIAAkKQgghAiQoCCGECLCFOgPHKikpSaelpYU6G0IIcVJZt25dkdY6+WjbnXRBIS0tjbVr14Y6G0IIcVJRSh3syHbSfCSEECJAgoIQQogACQpCCCECTro+hdZ4PB6ys7Opra0NdVZOWuHh4fTv3x+73R7qrAghQuiUCArZ2dlER0eTlpaGUirU2TnpaK0pLi4mOzubgQMHhjo7QogQOiWaj2pra0lMTJSA0ElKKRITE6WmJYQ4NYICIAHhOMnnJ4SAUygoHI3PV0NdXQ5+vyfUWRFCiG6rxwQFv78WtzsXrbs+KJSVlfHCCy906r0XX3wxZWVlHd7+0Ucf5emnn+5UWkIIcTQ9JigoZQ5Va3+X77u9oOD1ett97+LFi4mLi+vyPAkhRGcELSgopV5WShUopbYeZbtJSimvUurqYOXFsNY/+rp8z3PnzmXv3r2MGzeO++67j2XLlnHmmWcya9YsRo4cCcDll1/OxIkTSU9PZ968eYH3pqWlUVRUxIEDBxgxYgRz5swhPT2d888/n5qamnbT3bhxI1OnTmXMmDFcccUVlJaWAvDcc88xcuRIxowZw3XXXQfA8uXLGTduHOPGjWP8+PFUVlZ2+ecghDj5BXNI6ivAX4D5bW2glLICTwGfdFWimZn34HJtbOUVPz5fFRZLBEod22FHRY1jyJBn2nz9ySefZOvWrWzcaNJdtmwZ69evZ+vWrYEhni+//DIJCQnU1NQwadIkrrrqKhITE4/IeyZvvvkmf/vb37j22mt59913ufHGG9tM96abbuLPf/4zM2fO5JFHHuFXv/oVzzzzDE8++ST79+8nLCws0DT19NNP8/zzzzNt2jRcLhfh4eHH9BkIIXqGoNUUtNYrgJKjbHY38C5QEKx8tKRPSCqTJ09uNub/ueeeY+zYsUydOpWsrCwyMzNbvGfgwIGMGzcOgIkTJ3LgwIE2919eXk5ZWRkzZ84E4Pvf/z4rVqwAYMyYMdxwww289tpr2GwmAE6bNo17772X5557jrKyssB6IYRoKmQlg1KqH3AFcDYwqav229YZvd/vpapqI2FhA3A4UroquTZFRkYG/l62bBmfffYZK1euxOl0ctZZZ7V6TUBYWFjgb6vVetTmo7YsWrSIFStW8NFHH/HEE0+wZcsW5s6dyyWXXMLixYuZNm0aS5YsYfjw4Z3avxDi1BXKjuZngPt1B3p+lVK3K6XWKqXWFhYWdiqxYHY0R0dHt9tGX15eTnx8PE6nk507d7Jq1arjTjM2Npb4+Hi++OILAP75z38yc+ZM/H4/WVlZnH322Tz11FOUl5fjcrnYu3cvo0eP5v7772fSpEns3LnzuPMghDj1hLINIQNYUH/RVBJwsVLKq7X+4MgNtdbzgHkAGRkZnWr/MUFBEYyO5sTERKZNm8aoUaO46KKLuOSSS5q9fuGFF/Liiy8yYsQIhg0bxtSpU7sk3VdffZU77riD6upqBg0axD/+8Q98Ph833ngj5eXlaK35yU9+QlxcHA8//DBLly7FYrGQnp7ORRdd1CV5EEKcWpTWwWtjV0qlAf/WWo86ynav1G/3ztH2mZGRoY+8yc6OHTsYMWLEUfNTWbkBuz2R8PDUo27bE3X0cxRCnHyUUuu01hlH2y5oNQWl1JvAWUCSUiob+CVgB9BavxisdNvPkxWtu76mIIQQp4qgBQWt9fXHsO3NwcpHU2YEbNf3KQghxKmix1zRbFikpiCEEO3oUUFBmo+EEKJ9PSwoWJDmIyGEaFuPCgogNQUhhGhPjwoKSlmCcvFaZ0RFRR3TeiGEOBF6VFAwM6VKTUEIIdrSo4KC6VPQXV5bmDt3Ls8//3zgecONcFwuF+eccw4TJkxg9OjRfPjhhx3ep9aa++67j1GjRjF69GjeeustAHJzc5kxYwbjxo1j1KhRfPHFF/h8Pm6++ebAtn/605+69PiEED3HqTdV5j33wMbWps4Gu3Zj9deBNQoz5UUHjRsHz7Q9dfbs2bO55557uOuuuwB4++23WbJkCeHh4bz//vvExMRQVFTE1KlTmTVrVofuh/zee++xceNGNm3aRFFREZMmTWLGjBm88cYbXHDBBTz44IP4fD6qq6vZuHEjOTk5bN1qbl1xLHdyE0KIpk69oNCuhsJYc0xB4SjGjx9PQUEBhw8fprCwkPj4eAYMGIDH4+GBBx5gxYoVWCwWcnJyyM/Pp3fv3kfd55dffsn111+P1WolJSWFmTNnsmbNGiZNmsStt96Kx+Ph8ssvZ9y4cQwaNIh9+/Zx9913c8kll3D++ed32bEJIXqWUy8otHNG7/OUUFu7D6czHas1okuTveaaa3jnnXfIy8tj9uzZALz++usUFhaybt067HY7aWlprU6ZfSxmzJjBihUrWLRoETfffDP33nsvN910E5s2bWLJkiW8+OKLvP3227z88stdcVhCiB6mh/UpBO+WnLNnz2bBggW88847XHPNNYCZMrtXr17Y7XaWLl3KwYMHO7y/M888k7feegufz0dhYSErVqxg8uTJHDx4kJSUFObMmcMPfvAD1q9fT1FREX6/n6uuuorHH3+c9evXd/nxCSF6hlOvptCu4N1TIT09ncrKSvr160efPn0AuOGGG/jOd77D6NGjycjIOKab2lxxxRWsXLmSsWPHopTid7/7Hb179+bVV1/l97//PXa7naioKObPn09OTg633HILfr85rt/+9rddfnxCiJ4hqFNnB8PxTJ3t81VTXb2d8PDB2O3xwcriSUumzhbi1NXRqbN7VPNR4+F2jwvYhBCiu+lRQaGhT0GmuhBCiNb1sKAQvD4FIYQ4FfSooNB4uFJTEEKI1vSooGCuJJaZUoUQoi09KihA95opVQghupugBQWl1MtKqQKl1NY2Xr9BKbVZKbVFKfW1UmpssPLSXNfPlFpWVsYLL7zQqfdefPHFMleREKLbCGZN4RXgwnZe3w/M1FqPBn4NzAtiXgKCUVNoLyh4vd5237t48WLi4uK6ND9CCNFZQQsKWusVQEk7r3+ttS6tf7oK6B+svDRlhqV2bU1h7ty57N27l3HjxnHfffexbNkyzjzzTGbNmsXIkSMBuPzyy5k4cSLp6enMm9cY/9LS0igqKuLAgQOMGDGCOXPmkJ6ezvnnn09NTU2LtD766COmTJnC+PHjOffcc8nPzwfA5XJxyy23MHr0aMaMGcO7774LwMcff8yECRMYO3Ys55xzTpcetxDi1NNdprm4DfhPWy8qpW4HbgdITU1td0ftzJwNgN8/AK01Vmvb2xzpKDNn8+STT7J161Y21ie8bNky1q9fz9atWxk4cCAAL7/8MgkJCdTU1DBp0iSuuuoqEhMTm+0nMzOTN998k7/97W9ce+21vPvuu9x4443Ntpk+fTqrVq1CKcVLL73E7373O/7whz/w61//mtjYWLZs2QJAaWkphYWFzJkzhxUrVjBw4EBKStqM0UIIAXSDoKCUOhsTFKa3tY3Weh71zUsZGRnHOS+H4kRc0Tx58uRAQAB47rnneP/99wHIysoiMzOzRVAYOHAg48aNA2DixIkcOHCgxX6zs7OZPXs2ubm5uN3uQBqfffYZCxYsCGwXHx/PRx99xIwZMwLbJCQkdOkxCiFOPSENCkqpMcBLwEVa6+Ku2Gd7Z/QAtbUFeL1lREUFt187MjIy8PeyZcv47LPPWLlyJU6nk7POOqvVKbTDwsICf1ut1labj+6++27uvfdeZs2axbJly3j00UeDkn8hRM8UsiGpSqlU4D3ge1rr3ScuZUuXX6cQHR1NZWVlm6+Xl5cTHx+P0+lk586drFq1qtNplZeX069fPwBeffXVwPrzzjuv2S1BS0tLmTp1KitWrGD//v0A0nwkhDiqYA5JfRNYCQxTSmUrpW5TSt2hlLqjfpNHgETgBaXURqXU2jZ31qX5sgJ+unJ22MTERKZNm8aoUaO47777Wrx+4YUX4vV6GTFiBHPnzmXq1KmdTuvRRx/lmmuuYeLEiSQlJQXWP/TQQ5SWljJq1CjGjh3L0qVLSU5OZt68eVx55ZWMHTs2cPMfIYRoS4+aOhvA7c6jri6bqKjxTW66I0CmzhbiVCZTZ7dJZkoVQoi29LigIDOlCiFE23pcUGioKchMqUII0VKPCwpyox0hhGhbDwwK0nwkhBBt6XFBQZqPhBCibT0uKHSXmkJUVFRI0xdCiNb0wKAgNQUhhGhLjwsKDYfclTWFuXPnNpti4tFHH+Xpp5/G5XJxzjnnMGHCBEaPHs2HH3541H21NcV2a1NgtzVdthBCdFbIZ0ntavd8fA8b89qZOxvw+VwoZcdiCWt3uwbjeo/jmQvbnmlv9uzZ3HPPPdx1110AvP322yxZsoTw8HDef/99YmJiKCoqYurUqcyaNav+XtGta22Kbb/f3+oU2K1Nly2EEMfjlAsKHdd103uMHz+egoICDh8+TGFhIfHx8QwYMACPx8MDDzzAihUrsFgs5OTkkJ+fT+/evdvcV2tTbBcWFrY6BXZr02ULIcTxOOWCQntn9A1cri1YrZFERAzqsnSvueYa3nnnHfLy8gITz73++usUFhaybt067HY7aWlprU6Z3aCjU2wLIUSw9MA+BdPZ3NUXr82ePZsFCxbwzjvvcM011wBmmutevXpht9tZunQpBw8ebHcfbU2x3dYU2K1Nly2EEMejhwYFC11997X09HQqKyvp168fffr0AeCGG25g7dq1jB49mvnz5zN8+PB299HWFNttTYHd2nTZQghxPHrc1NkA1dWZaO0hMnJkV2fvpCZTZwtx6pKps9uhlCXkF68JIUR31CODgpnqQi5eE0KII50yQeFYmsGC0dF8sjvZmhGFEMERzHs0v6yUKlBKbW3jdaWUek4ptUcptVkpNaGzaYWHh1NcXNzhgq2ho1kKQkNrTXFxMeHh4aHOihAixIJ5ncIrwF+A+W28fhEwpH6ZAvy1/vGY9e/fn+zsbAoLCzu0vddbjtdbRljY9sAEeT1deHg4/fv3D3U2hBAhFrSgoLVeoZRKa2eTy4D52pyur1JKxSml+mitc481LbvdHrjatyNycl4gM/MuzjgjD4cj5ViTE0KIU1Yor2juB2Q1eZ5dv+6Yg8KxslqjAfB6KyUoCCGOm9bg94PPZx61hrAwsHSiIaKuDnJzwWYz+wgPN492O7QzbVqXOSmmuVBK3Q7cDpCamnrc+2sICj5f5XHvS4SO1pCTA9XVEBlpFqcTHI6u2X9tLezdC7t2mWXfPrO+4UfasNjt5gdstbb92LD4/eD1Nl/sdoiJgehos8TEmHUlJWYpLm78u7YWPJ7mi1ImnYZ8NDw6HOZvu73x7yPzY7M1vqdhG4fDfLYNaRcVmcfiYqioMEtlZeOilMlzwxIdbf4XRxaUDcfu8TR/9PtN4WmxmH01/H1knux2s9+oKJNGw6PXC4WFUFDQ+NjwWdXWmkK2thbcboiNhZQU6N278dHhaPx8G5aKCpOHI/+Pbrf5vjVd6urMMbTGboeICPOdCQ+HhASTbsPSu7d57549Ztm7Fw4dan1/SsHcufCb33TN97stoQwKOcCAJs/7169rQWs9D5gH5uK1403YajU3uPH5XMe7K3GE4mL48ks4cMAUELGxjYvTaQrxAwdg/37zeOCAKVi8XlN4NDw6HHDaaZCW1viYkmJ+NFu2mGXrVigra5kHm838AI88q1IKEhObFwgp9RXF0lJTGJSWmiU3Fw4ebP7j7NXLFAwNhUxdncnriWKxNAahpgUltCxoPR5TgHXVWIqYGFOgxcaagrhXLxg82PwN5n/YEDDy8qCqqrFwt1jM56ZU64FLqcYA0hA4fD6zr4bj8HjM511dDS6X+fyPFBcHyclmSU1tLIwbgrfDYb4v+flm2brVPHq9EB9vji8hwXxH0tJMnpp+J71esx+ns3GJiDDrmgZaa/0tW9xuk8+amsbH4mKT5vbt5tHtNtsmJMDpp8MZZ8BNN5n8+/2N37OGZdq0rvl/tieUQWEh8GOl1AJMB3N5Z/oTOsNmk5rCkerqTIF96JBZiorMD6HporUpHOLizI8oLs4UErt3w4oVZtna6lizlqxW88VPSzOFc9MzWJvN/IAOHoRFi0wh01RMDIweDdddZx5jYkxhUVVllurq1gsNn8/8KPPyzFnZV1+Z49TaHEd8fGPhMGUKfO97MGyYWYYObSwAm2o42z+y8Gj4u+l6n6/52XnDMXs8zc++KypMYdG0kEpMNOkfa3OEz9e8YG2al6Z5bah1NGyntUk7Kck8dlXtq6t4POZ/7XKZoJKc3Lk8am2WzjTzHC+tobzcPHanCY6DFhSUUm8CZwFJSqls4JeAHUBr/SKwGLgY2ANUA7cEKy9H6inNR1qbQvDw4caloYrddMnJaVnwHqvISHMWc911MGMGDB9ufrDl5Y1LdTX06QMDB0LfvqZQ7IjaWhOo8vJMEBkwoOvaVr1es6+Gs7tj1VC4d1cNgfZUG21st5uTkri449uPUiemnb6ttI83/8EQzNFH1x/ldQ3cFaz023MqNR9pDdnZsHNn82XfPhMEGqqnTUVENFazk5Jg7Fhz1t50SU5u2f4M5ky2oYmlrMws/fvDhAktC8fk5K45xvBwc6Y+dGjX7K+p7lygCxEKPfIn0XT00cnC7YasLNixw7RHNiw7dpgz8gYxMTBihDlr79fPLH37Ni69epm20M5qODs7hhHAQoiTSA8NCg01he4XFHJz4bPPYP16EwQalry85p2Gffuawv/mm2HkSNNcM2KE6TgNVXVYCHHy65FBwWKxo1RYt2g+qqmBL76ATz4xS/3tlnE6TTPOgAEwalRjs05D4d8d2yKFECe/HhkUwIxACkVNweuFdevg889NjeDrr83IH4cDzjwTnnoKzjvPtPOHYkSEOHl4fB4OlR8i3BZO3+i+qBNURfRrP3tK9rAmZw1rD5t7m0zuN5nJ/SYzKH7QCcvH8dJaU1FXgc1iI9IR2eZ2Hp+HzfmbWZe7jihHFEMShjAkcQhx4Z0/M/NrP4fKD7GraBeltaVE2CKIsEfgtDuJsEWQEJHAwPjQtNH22KBgtUadsJpCUREsXAgffQRLl5qROGAK/rvuMkFgxoxjb+v3+DwcKDtA76jeRIe1Ml7yCBV1FbjcLuq8ddR6a6n11lLnqyPJmURaXBo2S+e+Dl6/l5yKHPaX7edA2QEOlB2gpKaEKEcU0Y5o8xgWTbgtnOLqYvKr8sl35ZNXlUdBVQFDE4dy05ibOHvg2VjamYuq1ltLcXUxxTXFlNSUUFxtHtPi0piZNhOH9djHJGqtqXRXUlBVQGFVIQVVBRRUFVDlqSI2LJbY8FjiwuOIDYvFaXeaH3LxLnYX7w48RtgiGJMyJrCMTRlLamzqMReOfu1n+YHlvL7lddYcXkNceBwJEQkkhCeQ6EwkJiyGw5WH2Vu6lz0lezhYdhBf/Wy/KZEpTOw7kYl9zDIsaRh13jqqPFVUuauo8lRR46nBYXUQ6Ygk0h6J0+4k0hFJnbcu8D9peCytLcWiLFiVFavFilVZ8Ws/24u2s+7wOsrrzJc4whYBwDPfmHujJ0QkMLnfZNKT0/Frv/mOeeuo9dXi8XkY33s8lw69lFG9RrX6+RyuPMwnez9hd/Fuzkw9k7PSziLCHtHq55Vbmctn+z5jW+G2xu+UK4/8qnzKa8tx2p1EOaICS6QjkmpPdeB7U1JTEvj8+kb3ZVjiMIYlDmNo4lCSI5PZkLuBb3K+YV3uOmq9Lcc4JzmTGJIwhIHxA+kT1Yc+UX3oHdWbPtF9SIhIoKKuIpBWcU0xRdVF7Cvdx67iXewp2dPqPpua0GcCt0+4ne+O/m6Hft9d5ZS481pnrFkzhoiIwYwa9X4X5KqlrCx4/32zrFhhLkRJTYULLoBzzoGzzzadvh1R5a5ie+F2thduZ2fRTnYW72RH4Q72lu7F6/cSZg3j3EHncvnwy5k1bBa9Is2OvX4v32R/w+LMxSzKXMSm/E1tpmGz2BgYN5AhiUMYkjCEYYnDGNVrFKNTRrc4IyqpKWHZgWX8d/9/WXpgKbuLd+P1ewOvKxTRYdFUuasCP7qmFIrkyGRSIlNIciaxLncdFXUVDIgZwI1jbuSmsTcxLHEYu4t3szJ7JSuzVrIyeyVbC7aiaf37GhMWw0WnX8Rlwy7joiEXERceR54rj6+zvubrrK/5Kusr1ueux+1rZThWJ8SExQQKkGpPNZvzN7O3dG/gdbvF3iLAWZSFIYlDmNBnAuN7j2dCnwmMTRnL3tK9vL75dd7c+iY5lTlEOaKYnjqdGk9NswBY56sjLjyO0xNOZ3D84MCjy+1iXe461uWuY3vhdvzHeQOpcFs48eHxaDQ+vw+f9uHz+/BrP0MThzKp7yQy+mYwqd8kRiaPRGvNtsJtrM5ZHVh2Fe/CYXUQbgsPLAB7SvYAkBqbyiVDLuHSoZcSZg1jyd4lfLznY7YUmPZThUKjibBF8O2B3+biIRdz7qBz2Ve6j0/3fson+z5ha8HWwGedEpVCSmRK4DE+PJ4abw0ut4tKdyUutwuX20WkPZKEiAQSIxJJdCaSEJFArbc2EOQbztwBwqxhTOw7kSn9pjC1/1Qy+mZQ46khsySTzOJM81iSycGyg+S6co9ayDusDtLi0gLfm6YBqNZbS42nhmpPNTXeGvaX7ufvG/7OloItRNojuX7U9dw+8XYy+mZ0uibW0Tuv9digsH79NCyWCMaN+6wLcmX4fPDhh/Dcc7B8uVmXng5XXAGTL9jHFv0WZbWlLb6oDquDmLAYoh3RxITFEBMWQ7Wnmm2F29hWsI39ZfsDadgtdoYkDmF40nCGJw7n9ITT2VKwhfd3vs+BsgMoFNNSp9E3ui+f7v2U0tpSrMrK9NTpnDvoXJKdyYEfaZgtDIfVQUFVAZnFmewu2R34sld7qgNp9o/pz+heo0mNTWXN4TVsyN2ARhNpj2TGaTOY0GcCaXFppMWlMTBuIANiB+CwOtBaU+erCxxnjaeGhIgEkpxJWC2NFwbUeGpYuGshr256lSV7l+DXfqId0VS6TfNebFgsU/tPZXK/yfSP6R/4USdEJBAXHsem/E0s3LWQj3Z/REFVATaLjb7RfTlUfggwP+5J/SYxue/kVs+4ohxR9IrsRa/IXiQ7k+kV2YtIRyQVdRWU15ZTXldOWW0ZLreL/jH9GZY4jF6RvVr8OCvrKtlasJXN+Zs5UHagRQBz+9zsKNrB+tz1FFQVNHvNZrFx4ekXcsPoG5g1bBZOe/Nqo9Yat89NmC2s3e9gtaeaTXmb2Fe6jwh7BJH2SCIdplbgtDtx+9yBmkPDo8PqoHdU70ChGu2IDloT0OHKw4GTlE/3fkqVpwow3+vpqdO5YPAFXHD6BQxNHMoXB79gUeYiFmUuYl/pvsA+wqxhTE+dzvmDz+e8QecxtvfYdmuYx0JrTXFNMfmufIYkDulw7bOhKSrXlUtuZS6ltaXEhsWS6EwMfFedducxfa5aa1bnrGbeunks2LaAak81P//Wz3n6/Kc7dWwSFI5i06YL8HrLmThxVafer7WmtLaUg2UHidL9WPhmL/7yFzNtw2mnwe23w9VXQ23sZp788kne2vYWfu0nwhYRqM5Gh0UTaY/E4/dQUVcRWKo91dgtdoYlDSM9OZ305HRG9RrFyOSRDE4Y3Gozj9aazfmb+WDnB7y/830Kqws5f/D5XHz6xZw3+Lxjav/UWpNVkcXWgq1syd/CloItbC3Yyv6y/YzvPZ5zBp7Dtwd+m8n9JmO32jv1+bUlz5XH65tfJ7Mkk4y+GZwx4AyGJw3v0I/e5/fxTc43fLjzQw6UH2By38mcMeAMJvSZcNTC9ETSWpPrymVD7gY25G0gyZnE1SOvJsmZFOqsnVC13lpWHFyBx+dhZtpMohxRrW6ntSazJJOl+5cyMH4g01Ontwiap7ry2nLe2PIGY3uP5YwBZ3RqHxIUjmLr1quprt7B5MnbOrR9VnkWL6x5gcySTPaV7mNf6b5AuyoARUNJcU/n6knTuWvWdIpq8njyqydZnLmYKEcUd2bcyT1T76FvdN+jptXQFNPZNn4hhDhSR4NCjy11Ojr6yK/9zFs3j198+gtqvbUMThhMsm0QKflnULF6EJbKVMadvY/IUV+ypex9ns95mef/at6b5Ezi8bMf50eTfkR8RMcnN5FgIIQIlR5b+nRk9FFmcSZzPprD8oPLOXfQudzZbx6vPjuQhQvNXD8/+yHce6+5ahh+gV/72VG4gy8OfYHNYuO7o7/b46q5QoiTWw8OCqamoLVu0fnj9Xt5ZtUzPLz0YcKsYbz0nb9z8MNbuOomRUICPPoo/PjHZubKpizKQnqvdNJ7pZ+4AxFCiC7Ug4NCFFp78fvrsFobp5Bcd3gdP/z3D1mXu47Lhl3G8xe/wB9/1Zc//tFMKfHnP5ubewghxKmoBweFhumzXVit4VTUVfDwfx/mL2v+Qq/IXrx19VtcNfwa7rpL8X//B3ffDc88I1cZCyFObT0+KHi9FXy0Zzk/+fgn5FbmcmfGnTxxzhNE2eK4+WZ47bXGW+CdJFfvCyFEp/XgoBCFX8ONH97J+7s/YWzKWN679j2m9J9CXR1ce625Gvnxx+HBB0OdWyGEODF6bFCw2aL5uhje3/0JD575II+e9WhgKOicOSYg/OlPcM89Ic6oEEKcQD22hdxiiWL+QRgU27dZQPj3v+Gf/4SHH5aAIIToeXpsUPgePMSIAAAgAElEQVT04HoyXXDPxCsCAaGiAu64w9y/4KGHQpxBIYQIgaAGBaXUhUqpXUqpPUqpua28nqqUWqqU2qCU2qyUujiY+WmgteapVS/RJxwuGzQusP7++82dz/7+d3N/AyGE6GmCFhSUUlbgeeAiYCRwvVJq5BGbPQS8rbUeD1wHvBCs/DS1ZO8S1uVt5oZUsOgawMxq+uKL8LOfweTJJyIXQgjR/QSzpjAZ2KO13qe1dgMLgMuO2EYDMfV/xwKHg5gfk6DW/Gr5r0iNGcD5KeY6hepq+MEPYNAgeOyxYOdACCG6r2AGhX5AVpPn2fXrmnoUuFEplQ0sBu4OYn4A+Hz/56zKXsX90+fisNrw+Sp59FHYswdeeunY734mhBCnklB3NF8PvKK17g9cDPxTqZYT5yulbldKrVVKrS0sLOx0Yg21hH7R/bht/G1YrVFs3BjPH/5ghqGefXbnD0QIIU4FwQwKOcCAJs/7169r6jbgbQCt9UogHGhxpxGt9TytdYbWOiM5ObnTGVp+cDlfHvqS+6fdT5gtDIslmoceupbeveH3v+/0boUQ4pQRzKCwBhiilBqolHJgOpIXHrHNIeAcAKXUCExQ6HxV4CgeW/4YvaN684MJPwCgouI0duw4jZ//HGJjg5WqEEKcPDoUFJRSP1VKxSjj70qp9Uqp89t7j9baC/wYWALswIwy2qaUekwpNat+s58Dc5RSm4A3gZt1kG4F9+WhL1l6YCm/OOMXRNgjAMjKMoOh0mWmayGEADo+zcWtWutnlVIXAPHA94B/Ap+09yat9WJMB3LTdY80+Xs7MO2YctxJ4bZwLht2GT/M+GFgXVbWMACGDz8RORBCiO6vo0GhYX7Qi4F/1p/xn1Rzhmb0zeCD6z5otu7gwcGEhdUwYEBEiHIlhBDdS0f7FNYppT7BBIUlSqlowB+8bJ0YBw+mkpq6X+6RIIQQ9TpaHN4GzAUmaa2rATtwS9BydYLs39+PAQN2hzobQgjRbXQ0KHwL2KW1LlNK3YiZnqI8eNkKvro6yM5OYsCAbaHOihBCdBsdDQp/BaqVUmMxI4b2AvODlqsTYM8e8Pst9O+/Db/fE+rsCCFEt9DRoOCtHyp6GfAXrfXzQHTwshV8O3eax9TUnfh8rtBmRgghuomOBoVKpdT/YoaiLqqfisIevGwF365d5nHAgN0SFIQQol5Hg8JsoA5zvUIeZsqKk3piiJ07oW/faiIiqvD5KkOdHSGE6BY6FBTqA8HrQKxS6lKgVmt9cvUprF4N3/8+lJQApqZw+unVABIUhBCiXkenubgWWA1cA1wLfKOUujqYGetyRUUwfz5s347WpqYwdGgdgDQfCSFEvY5e0fwg5hqFAgClVDLwGfBOsDLW5UbW3/Rt+3byT59ORQUMHWquv/N6paYghBDQ8aBgaQgI9YoJ/b0Yjk1qqrmDzvbt7BxqVg0fbg5Bmo+EEMLoaFD4WCm1BDOTKZiO58XtbN/9WCwwYgRs386uEWbV8OF2cnKk+UgIIRp0tKP5PmAeMKZ+mae1vj+YGQuK9HRTU9hpKg1paZGA1BSEEKJBR2sKaK3fBd4NYl6Cb+RImD+fXds8DB1qx2ZzopQdjydo9/URQoiTSrtBQSlVCbR20xsFaK11TFByFSz1nc07t/mYMsOOUorIyNG4XBtDnDEhhOge2g0KWuuTeiqLFkaOpJYwDuSGcZO5vw7R0RkUFv4LrTUn2S0ihBCiy51cI4iOV1oamY5RaK0Cd1uLjs7A6y2ltnZ/aPMmhBDdQM8KClYru/qeDcCwQE1hIgCVlWtDlSshhOg2ghoUlFIXKqV2KaX2KKXmtrHNtUqp7UqpbUqpN4KZH4CdMZMBGFp/rUJk5CiUckhQEEIIjmH00bFSSlmB54HzgGxgjVJqodZ6e5NthgD/C0zTWpcqpXoFKz8NdqnhDOAQkToBiMJicRAVNYbKynXBTloIIbq9YNYUJgN7tNb7tNZuYAHmfgxNzQGe11qXAhxx1XRQ7HT1Zzg7G2+ogOlXqKxch7llhBBC9FzBDAr9gKwmz7Pr1zU1FBiqlPpKKbVKKXVhaztSSt2ulFqrlFpbWNj5awq0hl15MQxjF2wPVFiIjs7A5yunpmZvp/cthBCnglB3NNuAIcBZwPXA35RScUdupLWep7XO0FpnJCcndzqx3FyorLIy3JLZLChERUlnsxBCQHCDQg4woMnz/vXrmsoGFmqtPVrr/cBuTJAIioYWo2GpNc2CQmRkOkqFSVAQQvR4wQwKa4AhSqmBSikHcB2w8IhtPsDUElBKJWGak/YFK0MNt+AcPsrWLChYLHaiosbicklnsxCiZwtaUNBae4EfA0uAHcDbWuttSqnHlFKz6jdbAhQrpbYDS4H7tNbFwcrTzp0QGQn9JvaGffugpibwWmNnsz9YyQshRLcX1D4FrfVirfVQrfVgrfUT9ese0VovrP9ba63v1VqP1FqP1lovCGZ+du0yF62p9JH1vc67Aq+ZzuZKamr2BDMLQgjRrYW6o/mE2rmz/krmJndhayBXNgshRA8KCtXVcOgQZs6jIUPAam0WFJzOkVgs4XIRmxCiR+sxQSEz07QYDRsGOBwmMDTrbLYRFTVOagpCiB6txwSFwMij+tlRGTmyWVAA06/gcq2XzmYhRI/VY4LCzJnw3nuNs6MyciTs2QN1dYFtTGezi+rq3aHJpBBChFiPCQopKXDFFRAeXr9i5Ejw+Uy7Uj25slkI0dP1mKDQQisjkJzO4VgsTrmITQjRY/XcoDB0KFgs0tkshBBN9NygEBEBgwa12tlcWbkBrX0hypgQQoROzw0KAOnpsG1bs1XR0RPx+6uort7VxpuEEOLU1bODwsiRsHs3eDyBVdHRGYB0NgsheiYJCl6vGZpaz+kchsUSKVc2CyF6JAkK0KxfQSkr0dHjqaxcHaJMCSFE6PTsoDB8OCgF33zTbHVc3DlUVHxDbW12iDImhBCh0bODgtNprmibNw/KygKre/f+HqDJz38tdHkTQpyctIb9+4Oz3xOgZwcFgIcfhvJyePbZwKqIiMHExk4nL+8V9An6RwghThHPPmuGu7/WhSeVfj+ccw787W9dt882SFAYNw4uuwyeecYEh3q9e99MTc0u6VsQojuorYXbboMVK0Kdk/aVlMCvfmWape+8E/bu7Zr9vvEGLF1qWjeCTIICmNpCWRn8+c+BVcnJ12CxRJCX92oIMyaEAOBPf4KXX4brroPS0lDnpm1PPAEVFfDRR2CzwfXXg9t9fPusqYEHHoAJE8z+giyoQUEpdaFSapdSao9Sam47212llNJKqYxg5qdNEyfCpZfCH/8IlZUA2GwxJCVdQUHBAvz+uqPsQAjRKZ98AgsXtr9NTo4pbCdPhoICuOeeY0/H7YYbb4TFizuXz47Yt8+cWN5yC1xyCbz0EqxZA488cnz7fe45yMqCp582U/MEm9Y6KAtgBfYCgwAHsAkY2cp20cAKYBWQcbT9Tpw4UQfF6tVag9a/+U1gVXHxEr10KTo//1/BSVOInmz9eq0dDq2tVq2XL297uxtu0DosTOt9+7R+6CHzO1248NjSmjfPvC8mxuwnGK69VmunU+ucnMZ1c+ZorZTWn33WuX0WFJg8f+c7x509YK3uSNndkY06swDfApY0ef6/wP+2st0zwCXAspAGBa21vugirRMTta6s1Fpr7fd79Vdf9dObN18avDSF6InKy7UePFjrfv20HjJE6969tT58uOV2X35piqmHHjLP6+q0Hj1a6z59tC4p6VhabrfWAwdqnZ5uCtipU826rrRypcnnI480X19VpfWIESa/BQXHvt8f/9gEze3bjzuLHQ0KwayL9AOymjzPrl8XoJSaAAzQWi8KYj467pe/hOJieOEFwFzIlpJyI8XF/8Htzg9x5oQ4RWgNt98OBw7AggXw7rtmkMd115kZBhr4fHD33dC/P8ytb312OOCVV0wz0k9/2rH0Xn/dDBF98kkz/HzVKtMZ3JXH8z//Y27act99zV9zOuHNN025csstxzasdPduePFFmDMHRozouvweTUciR2cW4GrgpSbPvwf8pclzC6Z2kFb/fBlt1BSA24G1wNrU1NTjjpjtuuACrZOTtXa5tNZau1zb9dKl6EOH/hDcdIUItpqaUOfA+OtfzVn1b3/buG7+fLPuF79oXNfQ5PPmmy338fDDHWtG8ni0Pv10rceP19rvN+tuvdU06Xz++fEfi9Zav/uuycu8eW1v8+yzZpu//KXj+73iCq2jorTOyzv+POqToPkIiAWKgAP1Sy1wuK3A0LAEtflIa62/+sp8LL//fWDV2rWT9OrVY4KbrhDB4vdr/fzzpv1+1iyts7JCl5f1603/wIUXau3zNX/thz80v70PPjBNQ0lJWp95ZmNh3lRdndZjxphmp+LittP75z/NPt9/v3Gdy6X1sGGmSaewsOV7Kiu13rSpY8dTV2eCTnq6CUBt8fvNMTudWmdmHn2/K1aYfD/+eMfy0QHdISjYgH3AQBo7mtPb2b7NmkLTJehBQWutzzvP9C0UFWmttc7Ofl4vXYquqNgQ/LTFyW/HDq1//nOtf/lLc4b42mtaL15sTjgWLtT6xRfNme5tt5l+rBdeCF5eXC7TUQumLT0iQuvoaJPmkYVyV8nK0vqPfzTH2rTTtbzcFKD9+rXevl5To/WECVrHxmp9zTVaWyxab2jnN7d+vdY2mzmjbq2PwOs1hf+YMS2PdcMGEyQvvdQU2C6X1m+9pfVVV2kdHm4+r/nzj36sf/yj2Xbx4qNvm5Vljm369PY/e79f68mTzedUVXX0/XZQyIOCyQMXA7sxo5AerF/3GDCrlW27T1DYvNl82W69VWuttdtdrJctc+jMzHuCn7Y4uRUUaJ2aajoHTQty64vFYs5UBw0yz19/vevzsmuX1qNGmaaSX//aFER792p97rkmzenTTQDrStu3a92/f/Nj7d1b60suMWf9VqvWX3zR9vv37dM6Pt687447jp5eQ6F80UWBJt+AN980r/2rjdGDDU0606aZYNmQ1x//2KxzOrXesqXttJcv19pu1/rii1uvzbTmlVdMOn/6U+uv+/3mZAK0/sc/OrbPDuoWQSEYywkJClqbtk0w1Tit9ZYtV+kvv0zWPl8Xj1oQXe+117SeMSPwv+sy+fnt//g9Hq3PPts0j6xda85UCwtN4bxypdb/+Y8Z+pyd3djUUFdn8hoWpvWqVW3vu7pa6z//uePDKd9919QIEhO1/uST5q/5/abAiY83Z8uPP95+00dHffONSS8lReuvvzaF/zPPaH3TTaZ5xWLR+ne/O/p+PvnENLW01rTTmnnzzL6nTGl8j89n0hw5su2zcr/f1Ax69dL6zju1XrbM/M+0NiOhUlJMTaOiouV7MzO1Tkgwr3d0FFRDmpdeamoju3a1fG3uXFPu3Hxzl9fkJCgcL5dL69NOM1+sujpdVLS4vsO5jQgvus78+Vp/97uda/suLNQ6Ls6cHYPW3/++KcyP19tvm31efXVgyHIL995r0nz11WPbd2GhGTLZu7fWhw61fH3vXq3HjdOBcfavvdb2voqKtP7BD8y2kydrffBg29vm5Zmx9WAK1CMLqWPx2WdaR0aa49izp/VtGgrcYPjgA1PQDhum9YEDWr/zjjmuN97o/D6XLTPB5tprm58MlJSYdBISOtY/cKScHBOQv/Wtxs/E79f6nntMnn/4w6A07UlQ6AoffWQ+oief1H6/X2/adLFevtypq6v3n7g8dBc7d2p95ZVa794d3HQaqvRgCvfWRp605447TBPF6tVa/+//mup9XJwZ8dLZQmn9etO8MGiQKSRGj255xv766ybPd9/duTS2bjVn9uPHN28GWbTIFCBxcVq//LJp1gCtb7zRtNE38Pm0/vvfzZm61ar1ffdpXVvbsbTffNOkERFhRse0ViDl5JiC9vPPTbBuus0775gax6hRzfsQTrQvvjCfU9++Wg8dapbjDURPPmk+7+eeM8/dbtP8Zre3f8Hd0bz2mg4MaPH5TE0FtP7pTzveFHWMJCh0lSuuMD+W/ft1Tc1BvXx5pN606ULtD9I/rlsqLDQXGjWcUXZFU0NrfvMbk8aVV2q9bZvpGAVTa+hIFX3DBnM2/5OfNK7bvt006YDpxLzrLlNF/81vTAH46qta79/f9j7z8rQeMMC0k+flaf3xx6bgSUhovEp1wwbzHTnzzOO7KOrf/zb5v+oq8xn/8pfm+dixpragtVn/6KMmOA0caJppNm9uDBbTppnnxyonxzTZgBlosXmzaYu/805zVnxkn4jTafL1ne+YvHzrW8fWjBIsW7aYDtqOdhQfjc9njtFuN02ADSOkjre93+/X+vLLTbPhlVeafd5/f9ACgtYSFLpOVpYZK3zJJVr7/Tor61m9dCk6Ly8IHYPdUW2tKezCwszZ55Hjy7uC36/1Aw80ngE3BB2PR+vHHjNnvv37tz+u3O83+UxMbFk4+f3mTH7kSFOY22wtC7gXXmj5g6yra+yEXLeucX1mZmMb+RNPaJ2WZgqirhhP/oc/mDw1BOGbbmp9BMpXX5l0rVazJCaamsTxNDv4/WZklNPZ+Nk0fPefftr0GXz+ual1/exnZv2QIVrPnt2ykzeUDh0y/8+uOnkpKTGfdVSU+Uzmzu2a/eblmf8bmBOAIJ9oSlDoSg0jHN59V/v9Xr127RT95ZfJ2u0uOvF5OZH8flMogdYLFjR2zDkc7Y/KONY0fvpTk8acOa0XaqtXN56tPvpo69ssWGBef/HFjqVZU2NGCm3erPX555v3XnCB6QRu2KahbX7Bgpb7qKhoPMNzOEyB2RUa0rXbTeHbXkFRVqb17bdr/aMfBYZPd4k9e0yn9sqVXT8dxMlq7VrTZ3HllV3b3v/118EZedYKCQpdyeMxHX19+2r99tu6Mm+VXrbMprdv/37z7SoqzEiIKVPMcL+2OiRPFk88Yb4iv/pV47qCAnPF9/jxx1dguFxaf/ihaZ7rSFtqVZXpNAZT7W46IqSqyjTxjBvXuTbkhou7IiJM09Abb5hCEbR+8MG23+fzmTPSf//72NM8Wn5KS7t2n+L4FRQE79qOE0CCQldbt86MDgGtIyJ05YXD9LaH0cUHPjTR/tZbzegL0Hr4cNO00NVnFSfSv/5ljuWGG1oW1u+9pwNV3tbs3m2uJH3vPa0//dQMtdy61SzPPmvOzB0OHWieeOyxjlWd/X4zvttiMc03DaNcHnlENx0+3Gm7d5uA3tB0MmvWyfv/E+IIEhSCwevVeulSre+6S/vrA4TfUl+AREaaav+qVabwamgbPnLWxJPBxx+bs+Yzzmh7vpwbbzRt803b2letMoGwYThoW8uwYaZN+rPPTLv9sfr0UzNaJj7etKOHh2t93XWdO9YjeTymhnTZZc1H9whxkpOgEGxer65Y9Jw+eD069/GzW17g4vebC1DAjHE/Fhs2mIuPOtPxVF5umlcSEkz6S5Z0rMPN7zeF9IwZJs9DhrQ/1W9JiWlOS0830xnMnKkDw0gfeMD0Oaxfb4btLVpk2uXnz+/cuO7W7Nlj0q6vubU6vl8IESBB4QTJzPxZ/WikVi6Sqa01Z9tHjl5pzyuvmJE+YK58zM3teGb27zdjxa1WExhiYsx+kpNNZ+R//2uGNhYXNwYKv9/UDM44w2zbr59pT6+uPnp6ixc3nv3372865Fu7+jNYKirMdQldPB2AEKciCQoniM/n1uvXT9fLlzt1ZWUrI3KOHOfeFo+ncRTOt7+t9VNPmeCQmGguDjqar782hX9cnGle0do0/bz3nrkis2Ful6ZLdLS5xB9MHl94oeMXPDV46SVTA5BRKkJ0ax0NCspse/LIyMjQa9euDXU2mqmry2Xt2vHYbLFMnLgGmy2m+QYbNsD06TB6tLnRx9SpEB7e+HpREcyeDf/9r7lxyNNPm5t+b98ON90E69bB975n7tUaF9cyA2+8Abfeam5G8u9/w/DhLbdxuWDFCpNWaSmUlZmlvBy+9S34/vfNDUyEEKckpdQ6rXXGUbeToNA1yspWsHHjt0lKuoz09HdQSjXf4N134frrweOBsDATGGbOhFGj4Be/gMOH4f/+D26+ufn7PB5z0/LHH4fERFPgR0c3LtXV5s5SM2bAe++ZbYQQ4ggSFEIgK+sP7N37Pwwa9HtSU/+n5QZlZfDll7BsmVk2bAC/H/r2NQX6lClt73zNGnjqKXOmX1lpFpcLqqrMbQz//Gc50xdCtEmCQghordm+/VoKC99n3LjPiYub2f4bysth7VoYOxaSkk5MJoUQPVJHg4LlRGSmp1BKMWzYy0REnM7WrVdRVbWt/TfExsI550hAEEJ0GxIUupjNFs2YMYuwWBxs3HgO1dW7Q50lIYToMAkKQRARMZixYz8H/GzadA41NftDnSUhhOgQCQpBEhk5grFjP8Pnq2LTpm9TW5sV6iwJIcRRBTUoKKUuVErtUkrtUUrNbeX1e5VS25VSm5VSnyulTgtmfk60qKgxjBnzCR5PCZs2nUNdXW6osySEEO0KWlBQSlmB54GLgJHA9UqpkUdstgHI0FqPAd4Bfhes/IRKTEwGY8b8h7q6w2zadC61tQdDnSUhhGhTMGsKk4E9Wut9Wms3sAC4rOkGWuulWuvq+qergP5BzE/IxMaewejRH1FXl8XateMoLHw31FkSQohWBTMo9AOaNqRn169ry23Af4KYn5CKjz+bjIwNREQMYdu2q9m9+0f4fDWhzpYQQjTTLTqalVI3AhnA79t4/Xal1Fql1NrCwsITm7kuFBExmPHjv2TAgP/h8OG/sn79FKqqdoQ6W0IIERDMoJADDGjyvH/9umaUUucCDwKztNZ1re1Iaz1Pa52htc5ITk4OSmZPFIvFweDBv2f06MW43XmsWzeRvLz5oc6WEEIAwQ0Ka4AhSqmBSikHcB2wsOkGSqnxwP9hAkJBEPPS7SQmXkRGxiZiYqawc+f32bPn5/j93lBnSwjRwwUtKGitvcCPgSXADuBtrfU2pdRjSqlZ9Zv9HogC/qWU2qiUWtjG7k5JYWF9GDPmE/r1u5vs7D+yZcsleDyloc6WEKIHkwnxuonc3L+ze/edhIefxqhRC4mMHBHqLAkhTiEyId5Jpk+f2xg3bhlebyXr108hP/91GZ0khDjhJCh0I7GxZzBx4hoiIoayY8eNfPVVElu3Xkle3qu43UWhzp4QogewhToDornw8AFMmLCSsrL/UlT0IUVFCykqeh+wEBt7Jv37/5SkpMtQSuK5EKLrSZ9CN6e1xuVaT1HRh+Tnv0Ft7V6cznROO+0BkpOvxWKRuC6EODrpUzhFKKWIjp7IwIGPMXnyTkaMeAOAHTtuYPXq4Rw+/BJ+vzvEuRRCnCokKJxELBYbKSnXM2nSZtLT38dmi2P37jmsXj2C/PwFaO0PdRaFECc5CQonIaUsJCdfzsSJaxg9ehFWaxQ7dlzPunWTKS39PNTZE0KcxCQonMSUUiQmXkxGxgaGD5+Px1PIpk3nsmnThZSXfyXNSkKIYya9lKcApSz07v09kpOv4fDhFzh48HE2bJiOUmFER08gJmYqMTFTiImZSnj4KXUfIyFEF5PRR6cgr7eckpJPqKj4hsrKb6isXIvfXwtARMRQEhMvITHxEmJjz8RicYQ4t0KIE6Gjo48kKPQAfr+HqqotlJd/SXHxYsrKlqF1HVZrFPHx55GY+B0SE7+Dw5EU6qwKIYKko0FBmo96AIvFTnT0BKKjJ9C//0/w+aooLf2c4uLFlJQsanJx3HSSki4nKekyIiIGtbtPr7cSl2s9LtdGoqMnERt7xok5GCFEUElNoYczF8dtoKjoA4qKPqCqagsA4eFpOBx9cThSsNt74XCkYLVGUVW1lcrKNVRX7wQavzt9+97BoEFPYbPFhOhIhBDtkeYj0Sk1NfsoKvqQyso1uN35uN35eDwFeDxFgMZuTyEmZhLR0WaJjBxJdvazZGc/S1hYX4YM+StJSZeG+jCEEEeQoCC6lN/vxeerxGaLQynV4vWKitXs2nUbVVVb6dXrOlJTH6SuLpvq6h2Bpa4uh4SEC+jT5wdERU1odT+t0VpTWbmGsrJlJCZeSmTkyC46Jg8VFSuxWMKIipqAxWLvkv0K0R1JUBAnnN/v5tChJzl48HG09gTW22wJOJ0jsNuTKC1dgt9fS1TUOPr0+QG9en0Xuz2+1f1VV+8mP/91CgreoKZmT/1aRXLyVZx22kNERY095jzW1eVRUvIfiosXUVr6KT5fBQBWaxSxsdOJizuLuLizJEiIU44EBREyVVU7qaj4ioiIITidw7HbkwO1Ao+njIKCN8nNfQmXaz0WSzhO53Asloj6JRyLJYLa2gO4XOsARVzct0lJ+S5xcWeRm/syOTnP4fNVkph4GWlpDxMVNR6Ppxi3O5e6usO43bl4PAV4vWV4PKV4vWV4vWW43YcDfSYORz8SEy8mIeEitPZSVraMsrJlVFdvB8BmS6R//5/Sr9+P2wxaQpxMJCiIbq+ycj15ea9SW7sfv78Wv78Gn68Gv78Gmy2G5ORr6NXrOsLC+jZ7n8dTSk7Oc2RnP4PXW4ZS9mY1kwZK2bHZ4rHZ4rDZ4rHbE4iNPZPExIuJjBzTavOV251PWdly8vNfo7j4I6zWKPr2/REDBtyLw5HS6WP1+734/TVYLGGtXhvidhdSXb2DqqrtVFfvwOMpJjJyFFFR44iKGkdYWO9Opw3g81Xj8RTi8ZTi81Xg9Zbj9Zbj85Xj9VbUr6vA56vE6zW1p5iYKcTFzSA6OgOLJey40g8WrX0AKGXt1Pu9Xhda12G3J3ZltjqspmYvWntxOocFPS0JCuKU5/WWk5v7Eh5PMQ5HHxyOPoSF9an/OwWLxdnhfovWuFybOXTotxQUvI3F4iAl5Sbs9kQ8nmK83hI8nmI8njWkWpcAAAq9SURBVBK0rmvxXq19+HxV+P3V+HxVzYKWUnas1iis1kgslki83uL6jnzDYonEbo+nri47sM5uTyEqagxhYf3rR4MlY7ebBTQeTyFud0F9wV+I211YP0DArPf7q9s9VqVsWK0x2GwxWK0x+P211NTsrs9PONHRU4iLO5OYmG8RHT0JhyO51f243flUVHxDTc1ebLZYbLYE7PZE7PYEbLZErNYIlHJgsYQd9Z4gfr8br7cUj6ck8HnX1h6gpmYvNTV7qKnZQ23tfpRyEBMzhdjYacTGTiMmZio2W2yb+9VaU17+Bbm5L1NY+C/8/mqiosYTH38+CQnnExs7rVkQ9HorqKvLpq7uMA5Hb5zOYZ1uWtTaT0XFaoqLzb1SGmqm8fHnk5r6C+Livn1c39n2dIugoJS6EHgWsAIvaa2fPOL1MGA+MBEoBmZrrQ+0t08JCuJEq67OJCvrd+TlvQro+oLOFHJ2ewIWS0SL9yhlwWKJxGqNxGp1YrFEYrGEo3UdPl8VPp8r8GizxeJ0jsDpHEFk5EjCwvqjlAWPp5Sqqs24XBvrly31TWOFrdaMTLphOBy9AgGj4W/zmFRfc4rFao3FZoup/zsGiyW8RWHkdhdSXv4l5eVfUF7+BZWV6wEzE294+EBiYqYQHT0ZgIqKb6ioWEVd3cFj+GStWCyO+rP8hrTNo9beNgOZ1RpFRMTpREScTnj4YPz+KsrLv8Ll2lSfP4XTORKncwjh4QMDS1hYH0pKlpCX9w9qavZgtUbTq9f1hIUNoLT0MyoqvkJrLxaLk6io8Xi9JdTVZePzVR7xGTuIjBxJZOQYoqLGEB4+KBBMrdZobLYYlHLUN2dm1QeULGprD1Ba+hludx5gJS5uJklJl+HzVZGT8xxudx5RUeMZMOAXJCdfjcViw+utxO0+TF3dYerqcnA6hxETM+kYPuOm+Q5xUFDmP70bOA/IBtYA12uttzfZ5kfAGK31HUqp64ArtNaz29uvBAURKn6/B6VsQTuT6yitNT5fRX1toBBQgcLfao0KWv68Xhcu1zoqKlZTWbmaiopvqKvLAiAsLLV+fq0pREdPITJyBD6fq7421VizMs2EbrR2Bx619jYcWZPULIEmv8YgHE94+GnN+qia56+SiopVlJd/RWXlWmpr99c3TTa/13lc3Fn07n0ryclXYbU6m72/rGw5paWf4HJtwG7vRVhY/8DicPTB7c7B5dpcH6w343Yf/v/27j5GrrKK4/j31y6z1rZp2e5aYbelQEm0JrgEUnkzqTVKRUL5A3wDQowJ/2ACCUap8bUJMUYj+geJECFWKVrEVoshwVqaKokCBaq8x9oQ2oZ2K/JWU6Zl9vjHfWaY3dbt7MzOztyZ3ydp9t5nbm6f097ZM3OfmXNq/NebQaFwCvPmXUh//2r6+i4ds1Y1OlrkwIF7ePnlH3D48Iv09PQRcYRS6dCYswwN3czSpT+s8e8cqx2SwgXAdyLikrS/BiAivld1zEPpmL9K6gH2AwMxwaScFMzaR7G4H6DhNY9miQiOHh3h8OHdFIt7mDv3XGbNOnPKzn/kyL8pFvdU1mJKpbcold5kdLSYbmcO0du7iELh/TV1SYwY5dVXH+DgwY309JxMb++p9PYOUiicmraHmDlzdl1zbYcyF4PAnqr9vcBH/t8xEfGOpDeABcCYLvWSrgeuB1i8eHGz5mtmk9SuyaBMEoXCwvQhgQum/PyFQv+U1gyTZtDfv5r+/tVTds7JykU/hYi4MyLOi4jzBgaOv8BlZmaNa2ZS2AcsqtofSmPHPSbdPppHtuBsZmYt0Myk8DhwlqTTJRWAzwGbxx2zGbgubV8JPDzReoKZmTVX09YU0hrBl4GHyD6SendEPCtpLbAjIjYDdwG/lLQL+A9Z4jAzsxZpaj+FiHgQeHDc2Leqtt8GrmrmHMzMrHa5WGg2M7Pp4aRgZmYVTgpmZlaRu4J4kg4CkymwUq2fcV+M6xCdGFcnxgSdGZdjyofTIuKEX/TKXVJohKQdtXzNO286Ma5OjAk6My7H1Fl8+8jMzCqcFMzMrKLbksKdrZ5Ak3RiXJ0YE3RmXI6pg3TVmoKZmU2s294pmJnZBLomKUhaJelFSbsk3dLq+dRL0t2SRiQ9UzXWJ2mLpH+mnydPdI52I2mRpG2SnpP0rKQb03hu45L0HkmPSfp7ium7afx0SY+m63BDKhaZK5JmSnpK0h/SfifE9JKkpyXtlLQjjeX2+mtEVySF1Br0duBTwDLg85KWtXZWdfs5sGrc2C3A1og4C9ia9vPkHeDmiFgGnA/ckP5/8hxXEVgZER8GhoFVks4Hvg/cFhFLgdeAL7VwjvW6EXi+ar8TYgL4WEQMV30UNc/XX926IikAy4FdEbE7Io4AvwZa19qoARHxZ7KKstVWA+vS9jrgimmdVIMi4pWIeDJtv0X2C2eQHMcVmXKD3ZPSnwBWAven8VzFBCBpCPg08LO0L3Ie0wRye/01oluSwvFagw62aC7NsDAiXknb+4GFrZxMIyQtAc4BHiXncaXbLDuBEWAL8C/g9Xi3U30er8MfA18FRtP+AvIfE2QJ+4+SnkjtfyHn11+9mlo626ZfRISkXH6kTNIc4LfATRHxZvYiNJPHuCKiBAxLmg9sAj7Q4ik1RNJlwEhEPCFpRavnM8Uujoh9kt4HbJH0QvWDebz+6tUt7xRqaQ2aZwcknQKQfo60eD6TJukksoSwPiI2puHcxwUQEa8D28g6x89PrWchf9fhRcDlkl4iuwW7EvgJ+Y4JgIjYl36OkCXw5XTI9TdZ3ZIUamkNmmfVbU2vA37fwrlMWrovfRfwfET8qOqh3MYlaSC9Q0DSLOATZGsl28haz0LOYoqINRExFBFLyJ5DD0fE1eQ4JgBJsyXNLW8DnwSeIcfXXyO65strki4lux9abg16a4unVBdJvwJWkFVxPAB8G/gdcB+wmKyC7GciYvxidNuSdDHwF+Bp3r1X/XWydYVcxiXpbLLFyZlkL77ui4i1ks4ge5XdBzwFXBMRxdbNtD7p9tFXIuKyvMeU5r8p7fYA90bErZIWkNPrrxFdkxTMzOzEuuX2kZmZ1cBJwczMKpwUzMyswknBzMwqnBTMzKzCScFsGklaUa4uataOnBTMzKzCScHsOCRdk/oh7JR0Rypud0jSbak/wlZJA+nYYUl/k/QPSZvKdfclLZX0p9RT4UlJZ6bTz5F0v6QXJK1XdZEnsxZzUjAbR9IHgc8CF0XEMFACrgZmAzsi4kPAdrJvkwP8AvhaRJxN9q3s8vh64PbUU+FCoFxx8xzgJrLeHmeQ1RQyawuukmp2rI8D5wKPpxfxs8iKoY0CG9Ix9wAbJc0D5kfE9jS+DvhNqqUzGBGbACLibYB0vsciYm/a3wksAR5pflhmJ+akYHYsAesiYs2YQemb446rt0ZMdV2gEn4eWhvx7SOzY20Frky19cu9ek8je76Uq4F+AXgkIt4AXpP00TR+LbA9dZDbK+mKdI5eSe+d1ijM6uBXKGbjRMRzkr5B1olrBnAUuAH4L7A8PTZCtu4AWVnln6Zf+ruBL6bxa4E7JK1N57hqGsMwq4urpJrVSNKhiJjT6nmYNZNvH5mZWYXfKZiZWYXfKZiZWYWTgpmZVTgpmJlZhZOCmZlVOCmYmVmFk4KZmVX8D2wE0uU4PBgUAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 690us/sample - loss: 0.3447 - acc: 0.8922\n",
      "Loss: 0.344692622352612 Accuracy: 0.89221185\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4376 - acc: 0.5325\n",
      "Epoch 00001: val_loss improved from inf to 0.73920, saving model to model/checkpoint/1D_CNN_kaggle_origin_7_conv_checkpoint/001-0.7392.hdf5\n",
      "36805/36805 [==============================] - 50s 1ms/sample - loss: 1.4378 - acc: 0.5325 - val_loss: 0.7392 - val_acc: 0.7582\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6987 - acc: 0.7714\n",
      "Epoch 00002: val_loss improved from 0.73920 to 0.53319, saving model to model/checkpoint/1D_CNN_kaggle_origin_7_conv_checkpoint/002-0.5332.hdf5\n",
      "36805/36805 [==============================] - 36s 978us/sample - loss: 0.6986 - acc: 0.7714 - val_loss: 0.5332 - val_acc: 0.8283\n",
      "Epoch 3/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4992 - acc: 0.8347\n",
      "Epoch 00003: val_loss improved from 0.53319 to 0.42249, saving model to model/checkpoint/1D_CNN_kaggle_origin_7_conv_checkpoint/003-0.4225.hdf5\n",
      "36805/36805 [==============================] - 36s 970us/sample - loss: 0.4994 - acc: 0.8346 - val_loss: 0.4225 - val_acc: 0.8630\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3851 - acc: 0.8698\n",
      "Epoch 00004: val_loss improved from 0.42249 to 0.37858, saving model to model/checkpoint/1D_CNN_kaggle_origin_7_conv_checkpoint/004-0.3786.hdf5\n",
      "36805/36805 [==============================] - 36s 977us/sample - loss: 0.3854 - acc: 0.8698 - val_loss: 0.3786 - val_acc: 0.8765\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3228 - acc: 0.8929\n",
      "Epoch 00005: val_loss improved from 0.37858 to 0.34818, saving model to model/checkpoint/1D_CNN_kaggle_origin_7_conv_checkpoint/005-0.3482.hdf5\n",
      "36805/36805 [==============================] - 36s 973us/sample - loss: 0.3231 - acc: 0.8928 - val_loss: 0.3482 - val_acc: 0.8919\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2778 - acc: 0.9069\n",
      "Epoch 00006: val_loss improved from 0.34818 to 0.32636, saving model to model/checkpoint/1D_CNN_kaggle_origin_7_conv_checkpoint/006-0.3264.hdf5\n",
      "36805/36805 [==============================] - 36s 981us/sample - loss: 0.2777 - acc: 0.9069 - val_loss: 0.3264 - val_acc: 0.8987\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2251 - acc: 0.9249\n",
      "Epoch 00007: val_loss did not improve from 0.32636\n",
      "36805/36805 [==============================] - 36s 967us/sample - loss: 0.2253 - acc: 0.9248 - val_loss: 0.3446 - val_acc: 0.8963\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2115 - acc: 0.9289\n",
      "Epoch 00008: val_loss did not improve from 0.32636\n",
      "36805/36805 [==============================] - 36s 965us/sample - loss: 0.2115 - acc: 0.9289 - val_loss: 0.3493 - val_acc: 0.8933\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1737 - acc: 0.9424\n",
      "Epoch 00009: val_loss did not improve from 0.32636\n",
      "36805/36805 [==============================] - 36s 972us/sample - loss: 0.1737 - acc: 0.9423 - val_loss: 0.3745 - val_acc: 0.8945\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1651 - acc: 0.9440\n",
      "Epoch 00010: val_loss did not improve from 0.32636\n",
      "36805/36805 [==============================] - 36s 965us/sample - loss: 0.1652 - acc: 0.9440 - val_loss: 0.3516 - val_acc: 0.8989\n",
      "Epoch 11/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1405 - acc: 0.9525\n",
      "Epoch 00011: val_loss did not improve from 0.32636\n",
      "36805/36805 [==============================] - 35s 964us/sample - loss: 0.1407 - acc: 0.9524 - val_loss: 0.3266 - val_acc: 0.9050\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1305 - acc: 0.9553\n",
      "Epoch 00012: val_loss did not improve from 0.32636\n",
      "36805/36805 [==============================] - 36s 970us/sample - loss: 0.1306 - acc: 0.9553 - val_loss: 0.3589 - val_acc: 0.9073\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1211 - acc: 0.9588\n",
      "Epoch 00013: val_loss improved from 0.32636 to 0.32218, saving model to model/checkpoint/1D_CNN_kaggle_origin_7_conv_checkpoint/013-0.3222.hdf5\n",
      "36805/36805 [==============================] - 36s 975us/sample - loss: 0.1211 - acc: 0.9588 - val_loss: 0.3222 - val_acc: 0.9187\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1069 - acc: 0.9631\n",
      "Epoch 00014: val_loss did not improve from 0.32218\n",
      "36805/36805 [==============================] - 36s 971us/sample - loss: 0.1071 - acc: 0.9631 - val_loss: 0.3615 - val_acc: 0.9064\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0913 - acc: 0.9686\n",
      "Epoch 00015: val_loss did not improve from 0.32218\n",
      "36805/36805 [==============================] - 35s 955us/sample - loss: 0.0921 - acc: 0.9686 - val_loss: 0.3327 - val_acc: 0.9159\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0938 - acc: 0.9684\n",
      "Epoch 00016: val_loss improved from 0.32218 to 0.32074, saving model to model/checkpoint/1D_CNN_kaggle_origin_7_conv_checkpoint/016-0.3207.hdf5\n",
      "36805/36805 [==============================] - 36s 968us/sample - loss: 0.0938 - acc: 0.9684 - val_loss: 0.3207 - val_acc: 0.9180\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0772 - acc: 0.9734\n",
      "Epoch 00017: val_loss did not improve from 0.32074\n",
      "36805/36805 [==============================] - 35s 943us/sample - loss: 0.0776 - acc: 0.9733 - val_loss: 0.3786 - val_acc: 0.9073\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0810 - acc: 0.9723\n",
      "Epoch 00018: val_loss did not improve from 0.32074\n",
      "36805/36805 [==============================] - 35s 960us/sample - loss: 0.0812 - acc: 0.9723 - val_loss: 0.3453 - val_acc: 0.9189\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0763 - acc: 0.9752\n",
      "Epoch 00019: val_loss did not improve from 0.32074\n",
      "36805/36805 [==============================] - 35s 951us/sample - loss: 0.0763 - acc: 0.9752 - val_loss: 0.3969 - val_acc: 0.9031\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0737 - acc: 0.9751\n",
      "Epoch 00020: val_loss did not improve from 0.32074\n",
      "36805/36805 [==============================] - 36s 968us/sample - loss: 0.0740 - acc: 0.9751 - val_loss: 0.3521 - val_acc: 0.9145\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0689 - acc: 0.9766\n",
      "Epoch 00021: val_loss did not improve from 0.32074\n",
      "36805/36805 [==============================] - 36s 968us/sample - loss: 0.0689 - acc: 0.9766 - val_loss: 0.3330 - val_acc: 0.9245\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0532 - acc: 0.9823\n",
      "Epoch 00022: val_loss did not improve from 0.32074\n",
      "36805/36805 [==============================] - 35s 962us/sample - loss: 0.0534 - acc: 0.9822 - val_loss: 0.3418 - val_acc: 0.9241\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0563 - acc: 0.9813\n",
      "Epoch 00023: val_loss did not improve from 0.32074\n",
      "36805/36805 [==============================] - 36s 972us/sample - loss: 0.0563 - acc: 0.9813 - val_loss: 0.3527 - val_acc: 0.9215\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0616 - acc: 0.9792\n",
      "Epoch 00024: val_loss did not improve from 0.32074\n",
      "36805/36805 [==============================] - 36s 970us/sample - loss: 0.0617 - acc: 0.9791 - val_loss: 0.3837 - val_acc: 0.9178\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0584 - acc: 0.9801\n",
      "Epoch 00025: val_loss did not improve from 0.32074\n",
      "36805/36805 [==============================] - 36s 967us/sample - loss: 0.0586 - acc: 0.9801 - val_loss: 0.3679 - val_acc: 0.9208\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0580 - acc: 0.9812\n",
      "Epoch 00026: val_loss did not improve from 0.32074\n",
      "36805/36805 [==============================] - 36s 976us/sample - loss: 0.0583 - acc: 0.9811 - val_loss: 0.3604 - val_acc: 0.9196\n",
      "Epoch 27/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0656 - acc: 0.9793\n",
      "Epoch 00027: val_loss did not improve from 0.32074\n",
      "36805/36805 [==============================] - 36s 966us/sample - loss: 0.0660 - acc: 0.9792 - val_loss: 0.3521 - val_acc: 0.9252\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0576 - acc: 0.9809\n",
      "Epoch 00028: val_loss did not improve from 0.32074\n",
      "36805/36805 [==============================] - 36s 966us/sample - loss: 0.0576 - acc: 0.9809 - val_loss: 0.3647 - val_acc: 0.9241\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0457 - acc: 0.9843\n",
      "Epoch 00029: val_loss did not improve from 0.32074\n",
      "36805/36805 [==============================] - 35s 961us/sample - loss: 0.0459 - acc: 0.9843 - val_loss: 0.3783 - val_acc: 0.9213\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0431 - acc: 0.9860\n",
      "Epoch 00030: val_loss did not improve from 0.32074\n",
      "36805/36805 [==============================] - 36s 971us/sample - loss: 0.0433 - acc: 0.9860 - val_loss: 0.3574 - val_acc: 0.9217\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0426 - acc: 0.9858\n",
      "Epoch 00031: val_loss did not improve from 0.32074\n",
      "36805/36805 [==============================] - 36s 977us/sample - loss: 0.0428 - acc: 0.9858 - val_loss: 0.3805 - val_acc: 0.9187\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0511 - acc: 0.9827\n",
      "Epoch 00032: val_loss did not improve from 0.32074\n",
      "36805/36805 [==============================] - 36s 981us/sample - loss: 0.0515 - acc: 0.9827 - val_loss: 0.4041 - val_acc: 0.9178\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0412 - acc: 0.9862\n",
      "Epoch 00033: val_loss improved from 0.32074 to 0.31155, saving model to model/checkpoint/1D_CNN_kaggle_origin_7_conv_checkpoint/033-0.3116.hdf5\n",
      "36805/36805 [==============================] - 36s 985us/sample - loss: 0.0412 - acc: 0.9862 - val_loss: 0.3116 - val_acc: 0.9297\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0319 - acc: 0.9892\n",
      "Epoch 00034: val_loss did not improve from 0.31155\n",
      "36805/36805 [==============================] - 36s 971us/sample - loss: 0.0322 - acc: 0.9891 - val_loss: 0.3632 - val_acc: 0.9252\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0496 - acc: 0.9840\n",
      "Epoch 00035: val_loss did not improve from 0.31155\n",
      "36805/36805 [==============================] - 36s 972us/sample - loss: 0.0500 - acc: 0.9839 - val_loss: 0.3723 - val_acc: 0.9213\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0395 - acc: 0.9867\n",
      "Epoch 00036: val_loss did not improve from 0.31155\n",
      "36805/36805 [==============================] - 36s 974us/sample - loss: 0.0395 - acc: 0.9867 - val_loss: 0.3615 - val_acc: 0.9301\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0359 - acc: 0.9878\n",
      "Epoch 00037: val_loss did not improve from 0.31155\n",
      "36805/36805 [==============================] - 36s 980us/sample - loss: 0.0360 - acc: 0.9877 - val_loss: 0.3788 - val_acc: 0.9210\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0450 - acc: 0.9854\n",
      "Epoch 00038: val_loss did not improve from 0.31155\n",
      "36805/36805 [==============================] - 36s 967us/sample - loss: 0.0452 - acc: 0.9854 - val_loss: 0.3650 - val_acc: 0.9266\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0402 - acc: 0.9876\n",
      "Epoch 00039: val_loss did not improve from 0.31155\n",
      "36805/36805 [==============================] - 36s 976us/sample - loss: 0.0402 - acc: 0.9876 - val_loss: 0.4300 - val_acc: 0.9161\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0389 - acc: 0.9872\n",
      "Epoch 00040: val_loss did not improve from 0.31155\n",
      "36805/36805 [==============================] - 36s 967us/sample - loss: 0.0392 - acc: 0.9871 - val_loss: 0.3869 - val_acc: 0.9210\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0376 - acc: 0.9872\n",
      "Epoch 00041: val_loss did not improve from 0.31155\n",
      "36805/36805 [==============================] - 35s 964us/sample - loss: 0.0376 - acc: 0.9872 - val_loss: 0.4196 - val_acc: 0.9217\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0296 - acc: 0.9899\n",
      "Epoch 00042: val_loss did not improve from 0.31155\n",
      "36805/36805 [==============================] - 35s 949us/sample - loss: 0.0296 - acc: 0.9899 - val_loss: 0.4155 - val_acc: 0.9245\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0286 - acc: 0.9908\n",
      "Epoch 00043: val_loss did not improve from 0.31155\n",
      "36805/36805 [==============================] - 35s 953us/sample - loss: 0.0288 - acc: 0.9908 - val_loss: 0.4014 - val_acc: 0.9208\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0520 - acc: 0.9837\n",
      "Epoch 00044: val_loss did not improve from 0.31155\n",
      "36805/36805 [==============================] - 35s 956us/sample - loss: 0.0520 - acc: 0.9837 - val_loss: 0.3443 - val_acc: 0.9283\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0240 - acc: 0.9923\n",
      "Epoch 00045: val_loss did not improve from 0.31155\n",
      "36805/36805 [==============================] - 35s 949us/sample - loss: 0.0243 - acc: 0.9923 - val_loss: 0.4546 - val_acc: 0.9152\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0444 - acc: 0.9865\n",
      "Epoch 00046: val_loss did not improve from 0.31155\n",
      "36805/36805 [==============================] - 35s 963us/sample - loss: 0.0446 - acc: 0.9865 - val_loss: 0.3947 - val_acc: 0.9248\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0348 - acc: 0.9892\n",
      "Epoch 00047: val_loss did not improve from 0.31155\n",
      "36805/36805 [==============================] - 36s 971us/sample - loss: 0.0349 - acc: 0.9891 - val_loss: 0.3813 - val_acc: 0.9276\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0344 - acc: 0.9890\n",
      "Epoch 00048: val_loss did not improve from 0.31155\n",
      "36805/36805 [==============================] - 36s 967us/sample - loss: 0.0347 - acc: 0.9889 - val_loss: 0.4140 - val_acc: 0.9259\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0384 - acc: 0.9882\n",
      "Epoch 00049: val_loss did not improve from 0.31155\n",
      "36805/36805 [==============================] - 36s 965us/sample - loss: 0.0384 - acc: 0.9882 - val_loss: 0.3569 - val_acc: 0.9287\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0230 - acc: 0.9926\n",
      "Epoch 00050: val_loss did not improve from 0.31155\n",
      "36805/36805 [==============================] - 35s 965us/sample - loss: 0.0231 - acc: 0.9925 - val_loss: 0.3824 - val_acc: 0.9280\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0284 - acc: 0.9908\n",
      "Epoch 00051: val_loss did not improve from 0.31155\n",
      "36805/36805 [==============================] - 36s 971us/sample - loss: 0.0284 - acc: 0.9908 - val_loss: 0.3856 - val_acc: 0.9262\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0242 - acc: 0.9921\n",
      "Epoch 00052: val_loss did not improve from 0.31155\n",
      "36805/36805 [==============================] - 35s 960us/sample - loss: 0.0242 - acc: 0.9921 - val_loss: 0.4204 - val_acc: 0.9248\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0261 - acc: 0.9920\n",
      "Epoch 00053: val_loss did not improve from 0.31155\n",
      "36805/36805 [==============================] - 35s 957us/sample - loss: 0.0264 - acc: 0.9920 - val_loss: 0.4191 - val_acc: 0.9224\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0379 - acc: 0.9874\n",
      "Epoch 00054: val_loss did not improve from 0.31155\n",
      "36805/36805 [==============================] - 36s 975us/sample - loss: 0.0380 - acc: 0.9874 - val_loss: 0.3575 - val_acc: 0.9299\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0263 - acc: 0.9910\n",
      "Epoch 00055: val_loss did not improve from 0.31155\n",
      "36805/36805 [==============================] - 36s 971us/sample - loss: 0.0265 - acc: 0.9909 - val_loss: 0.4171 - val_acc: 0.9231\n",
      "Epoch 56/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0334 - acc: 0.9891\n",
      "Epoch 00056: val_loss did not improve from 0.31155\n",
      "36805/36805 [==============================] - 36s 971us/sample - loss: 0.0341 - acc: 0.9890 - val_loss: 0.3757 - val_acc: 0.9255\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0331 - acc: 0.9894\n",
      "Epoch 00057: val_loss did not improve from 0.31155\n",
      "36805/36805 [==============================] - 36s 986us/sample - loss: 0.0331 - acc: 0.9894 - val_loss: 0.3352 - val_acc: 0.9334\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0173 - acc: 0.9943\n",
      "Epoch 00058: val_loss did not improve from 0.31155\n",
      "36805/36805 [==============================] - 36s 968us/sample - loss: 0.0173 - acc: 0.9943 - val_loss: 0.4441 - val_acc: 0.9210\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0281 - acc: 0.9912\n",
      "Epoch 00059: val_loss did not improve from 0.31155\n",
      "36805/36805 [==============================] - 36s 974us/sample - loss: 0.0281 - acc: 0.9912 - val_loss: 0.4525 - val_acc: 0.9252\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0266 - acc: 0.9915\n",
      "Epoch 00060: val_loss did not improve from 0.31155\n",
      "36805/36805 [==============================] - 36s 968us/sample - loss: 0.0266 - acc: 0.9915 - val_loss: 0.4190 - val_acc: 0.9304\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0208 - acc: 0.9937\n",
      "Epoch 00061: val_loss did not improve from 0.31155\n",
      "36805/36805 [==============================] - 36s 971us/sample - loss: 0.0210 - acc: 0.9937 - val_loss: 0.3963 - val_acc: 0.9229\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0309 - acc: 0.9900\n",
      "Epoch 00062: val_loss did not improve from 0.31155\n",
      "36805/36805 [==============================] - 36s 972us/sample - loss: 0.0309 - acc: 0.9900 - val_loss: 0.3816 - val_acc: 0.9311\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0193 - acc: 0.9936\n",
      "Epoch 00063: val_loss did not improve from 0.31155\n",
      "36805/36805 [==============================] - 36s 967us/sample - loss: 0.0193 - acc: 0.9936 - val_loss: 0.3779 - val_acc: 0.9331\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0184 - acc: 0.9940\n",
      "Epoch 00064: val_loss did not improve from 0.31155\n",
      "36805/36805 [==============================] - 36s 971us/sample - loss: 0.0187 - acc: 0.9940 - val_loss: 0.4550 - val_acc: 0.9252\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0262 - acc: 0.9915\n",
      "Epoch 00065: val_loss did not improve from 0.31155\n",
      "36805/36805 [==============================] - 36s 984us/sample - loss: 0.0264 - acc: 0.9914 - val_loss: 0.4134 - val_acc: 0.9278\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0313 - acc: 0.9906\n",
      "Epoch 00066: val_loss did not improve from 0.31155\n",
      "36805/36805 [==============================] - 36s 976us/sample - loss: 0.0315 - acc: 0.9905 - val_loss: 0.3851 - val_acc: 0.9306\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0300 - acc: 0.9902\n",
      "Epoch 00067: val_loss did not improve from 0.31155\n",
      "36805/36805 [==============================] - 36s 966us/sample - loss: 0.0300 - acc: 0.9902 - val_loss: 0.3718 - val_acc: 0.9315\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0158 - acc: 0.9949\n",
      "Epoch 00068: val_loss did not improve from 0.31155\n",
      "36805/36805 [==============================] - 36s 975us/sample - loss: 0.0158 - acc: 0.9949 - val_loss: 0.4166 - val_acc: 0.9283\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0182 - acc: 0.9943\n",
      "Epoch 00069: val_loss did not improve from 0.31155\n",
      "36805/36805 [==============================] - 36s 969us/sample - loss: 0.0184 - acc: 0.9943 - val_loss: 0.4469 - val_acc: 0.9252\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0343 - acc: 0.9896\n",
      "Epoch 00070: val_loss did not improve from 0.31155\n",
      "36805/36805 [==============================] - 36s 974us/sample - loss: 0.0346 - acc: 0.9895 - val_loss: 0.3750 - val_acc: 0.9290\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0223 - acc: 0.9928\n",
      "Epoch 00071: val_loss did not improve from 0.31155\n",
      "36805/36805 [==============================] - 36s 973us/sample - loss: 0.0224 - acc: 0.9927 - val_loss: 0.3785 - val_acc: 0.9334\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0208 - acc: 0.9934\n",
      "Epoch 00072: val_loss did not improve from 0.31155\n",
      "36805/36805 [==============================] - 36s 971us/sample - loss: 0.0208 - acc: 0.9934 - val_loss: 0.4069 - val_acc: 0.9264\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0191 - acc: 0.9938\n",
      "Epoch 00073: val_loss did not improve from 0.31155\n",
      "36805/36805 [==============================] - 35s 962us/sample - loss: 0.0191 - acc: 0.9938 - val_loss: 0.4105 - val_acc: 0.9320\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0228 - acc: 0.9931\n",
      "Epoch 00074: val_loss did not improve from 0.31155\n",
      "36805/36805 [==============================] - 36s 978us/sample - loss: 0.0230 - acc: 0.9931 - val_loss: 0.3561 - val_acc: 0.9352\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0245 - acc: 0.9927\n",
      "Epoch 00075: val_loss did not improve from 0.31155\n",
      "36805/36805 [==============================] - 36s 976us/sample - loss: 0.0245 - acc: 0.9927 - val_loss: 0.4025 - val_acc: 0.9315\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0193 - acc: 0.9936\n",
      "Epoch 00076: val_loss did not improve from 0.31155\n",
      "36805/36805 [==============================] - 36s 970us/sample - loss: 0.0195 - acc: 0.9935 - val_loss: 0.3908 - val_acc: 0.9292\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0214 - acc: 0.9931\n",
      "Epoch 00077: val_loss did not improve from 0.31155\n",
      "36805/36805 [==============================] - 36s 976us/sample - loss: 0.0217 - acc: 0.9930 - val_loss: 0.4144 - val_acc: 0.9299\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0332 - acc: 0.9908\n",
      "Epoch 00078: val_loss did not improve from 0.31155\n",
      "36805/36805 [==============================] - 35s 962us/sample - loss: 0.0333 - acc: 0.9907 - val_loss: 0.3924 - val_acc: 0.9283\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0154 - acc: 0.9950\n",
      "Epoch 00079: val_loss did not improve from 0.31155\n",
      "36805/36805 [==============================] - 36s 978us/sample - loss: 0.0154 - acc: 0.9950 - val_loss: 0.3744 - val_acc: 0.9352\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0133 - acc: 0.9959\n",
      "Epoch 00080: val_loss did not improve from 0.31155\n",
      "36805/36805 [==============================] - 36s 974us/sample - loss: 0.0133 - acc: 0.9959 - val_loss: 0.3885 - val_acc: 0.9329\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0211 - acc: 0.9932\n",
      "Epoch 00081: val_loss did not improve from 0.31155\n",
      "36805/36805 [==============================] - 36s 968us/sample - loss: 0.0212 - acc: 0.9931 - val_loss: 0.3962 - val_acc: 0.9345\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0269 - acc: 0.9914\n",
      "Epoch 00082: val_loss did not improve from 0.31155\n",
      "36805/36805 [==============================] - 36s 970us/sample - loss: 0.0269 - acc: 0.9914 - val_loss: 0.3629 - val_acc: 0.9292\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0169 - acc: 0.9945\n",
      "Epoch 00083: val_loss did not improve from 0.31155\n",
      "36805/36805 [==============================] - 36s 968us/sample - loss: 0.0169 - acc: 0.9945 - val_loss: 0.4364 - val_acc: 0.9262\n",
      "\n",
      "1D_CNN_kaggle_origin_7_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 847us/sample - loss: 0.4602 - acc: 0.9061\n",
      "Loss: 0.46023374948406887 Accuracy: 0.9061267\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4398 - acc: 0.5307\n",
      "Epoch 00001: val_loss improved from inf to 0.72925, saving model to model/checkpoint/1D_CNN_kaggle_origin_8_conv_checkpoint/001-0.7292.hdf5\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 1.4398 - acc: 0.5307 - val_loss: 0.7292 - val_acc: 0.7587\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7209 - acc: 0.7644\n",
      "Epoch 00002: val_loss improved from 0.72925 to 0.52556, saving model to model/checkpoint/1D_CNN_kaggle_origin_8_conv_checkpoint/002-0.5256.hdf5\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.7211 - acc: 0.7644 - val_loss: 0.5256 - val_acc: 0.8258\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5248 - acc: 0.8286\n",
      "Epoch 00003: val_loss improved from 0.52556 to 0.42337, saving model to model/checkpoint/1D_CNN_kaggle_origin_8_conv_checkpoint/003-0.4234.hdf5\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.5249 - acc: 0.8286 - val_loss: 0.4234 - val_acc: 0.8670\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4160 - acc: 0.8623\n",
      "Epoch 00004: val_loss improved from 0.42337 to 0.37244, saving model to model/checkpoint/1D_CNN_kaggle_origin_8_conv_checkpoint/004-0.3724.hdf5\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.4160 - acc: 0.8622 - val_loss: 0.3724 - val_acc: 0.8826\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3610 - acc: 0.8808\n",
      "Epoch 00005: val_loss improved from 0.37244 to 0.32942, saving model to model/checkpoint/1D_CNN_kaggle_origin_8_conv_checkpoint/005-0.3294.hdf5\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.3612 - acc: 0.8807 - val_loss: 0.3294 - val_acc: 0.8966\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3062 - acc: 0.8986\n",
      "Epoch 00006: val_loss improved from 0.32942 to 0.32629, saving model to model/checkpoint/1D_CNN_kaggle_origin_8_conv_checkpoint/006-0.3263.hdf5\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.3064 - acc: 0.8985 - val_loss: 0.3263 - val_acc: 0.8973\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2720 - acc: 0.9084\n",
      "Epoch 00007: val_loss improved from 0.32629 to 0.30884, saving model to model/checkpoint/1D_CNN_kaggle_origin_8_conv_checkpoint/007-0.3088.hdf5\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.2722 - acc: 0.9084 - val_loss: 0.3088 - val_acc: 0.9026\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2463 - acc: 0.9167\n",
      "Epoch 00008: val_loss improved from 0.30884 to 0.29981, saving model to model/checkpoint/1D_CNN_kaggle_origin_8_conv_checkpoint/008-0.2998.hdf5\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.2466 - acc: 0.9167 - val_loss: 0.2998 - val_acc: 0.9115\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2210 - acc: 0.9260\n",
      "Epoch 00009: val_loss improved from 0.29981 to 0.29498, saving model to model/checkpoint/1D_CNN_kaggle_origin_8_conv_checkpoint/009-0.2950.hdf5\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.2210 - acc: 0.9260 - val_loss: 0.2950 - val_acc: 0.9103\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1854 - acc: 0.9399\n",
      "Epoch 00010: val_loss did not improve from 0.29498\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.1857 - acc: 0.9398 - val_loss: 0.3072 - val_acc: 0.9161\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1903 - acc: 0.9360\n",
      "Epoch 00011: val_loss improved from 0.29498 to 0.29141, saving model to model/checkpoint/1D_CNN_kaggle_origin_8_conv_checkpoint/011-0.2914.hdf5\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.1906 - acc: 0.9359 - val_loss: 0.2914 - val_acc: 0.9157\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1714 - acc: 0.9433\n",
      "Epoch 00012: val_loss improved from 0.29141 to 0.28622, saving model to model/checkpoint/1D_CNN_kaggle_origin_8_conv_checkpoint/012-0.2862.hdf5\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.1721 - acc: 0.9432 - val_loss: 0.2862 - val_acc: 0.9157\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1627 - acc: 0.9448\n",
      "Epoch 00013: val_loss improved from 0.28622 to 0.28591, saving model to model/checkpoint/1D_CNN_kaggle_origin_8_conv_checkpoint/013-0.2859.hdf5\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.1627 - acc: 0.9448 - val_loss: 0.2859 - val_acc: 0.9157\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1337 - acc: 0.9550\n",
      "Epoch 00014: val_loss did not improve from 0.28591\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.1338 - acc: 0.9549 - val_loss: 0.3035 - val_acc: 0.9143\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1432 - acc: 0.9524\n",
      "Epoch 00015: val_loss did not improve from 0.28591\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.1435 - acc: 0.9523 - val_loss: 0.3059 - val_acc: 0.9140\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1227 - acc: 0.9585\n",
      "Epoch 00016: val_loss did not improve from 0.28591\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.1228 - acc: 0.9584 - val_loss: 0.2991 - val_acc: 0.9227\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1306 - acc: 0.9563\n",
      "Epoch 00017: val_loss did not improve from 0.28591\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.1306 - acc: 0.9563 - val_loss: 0.2884 - val_acc: 0.9245\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0975 - acc: 0.9664\n",
      "Epoch 00018: val_loss did not improve from 0.28591\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.0977 - acc: 0.9663 - val_loss: 0.3118 - val_acc: 0.9196\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1100 - acc: 0.9636\n",
      "Epoch 00019: val_loss did not improve from 0.28591\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.1101 - acc: 0.9636 - val_loss: 0.3112 - val_acc: 0.9124\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1012 - acc: 0.9658\n",
      "Epoch 00020: val_loss did not improve from 0.28591\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.1012 - acc: 0.9657 - val_loss: 0.2878 - val_acc: 0.9264\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0876 - acc: 0.9709\n",
      "Epoch 00021: val_loss did not improve from 0.28591\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.0876 - acc: 0.9709 - val_loss: 0.3003 - val_acc: 0.9215\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0820 - acc: 0.9715\n",
      "Epoch 00022: val_loss did not improve from 0.28591\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.0824 - acc: 0.9714 - val_loss: 0.3060 - val_acc: 0.9236\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0972 - acc: 0.9667\n",
      "Epoch 00023: val_loss did not improve from 0.28591\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.0973 - acc: 0.9667 - val_loss: 0.2960 - val_acc: 0.9278\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0845 - acc: 0.9723\n",
      "Epoch 00024: val_loss did not improve from 0.28591\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.0846 - acc: 0.9722 - val_loss: 0.2994 - val_acc: 0.9241\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0748 - acc: 0.9748\n",
      "Epoch 00025: val_loss did not improve from 0.28591\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.0750 - acc: 0.9747 - val_loss: 0.3101 - val_acc: 0.9217\n",
      "Epoch 26/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0768 - acc: 0.9745\n",
      "Epoch 00026: val_loss did not improve from 0.28591\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.0769 - acc: 0.9745 - val_loss: 0.3259 - val_acc: 0.9252\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0815 - acc: 0.9726\n",
      "Epoch 00027: val_loss improved from 0.28591 to 0.28073, saving model to model/checkpoint/1D_CNN_kaggle_origin_8_conv_checkpoint/027-0.2807.hdf5\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.0817 - acc: 0.9725 - val_loss: 0.2807 - val_acc: 0.9283\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0681 - acc: 0.9775\n",
      "Epoch 00028: val_loss did not improve from 0.28073\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.0683 - acc: 0.9775 - val_loss: 0.3103 - val_acc: 0.9278\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0677 - acc: 0.9775\n",
      "Epoch 00029: val_loss did not improve from 0.28073\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.0677 - acc: 0.9775 - val_loss: 0.3293 - val_acc: 0.9241\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0573 - acc: 0.9800\n",
      "Epoch 00030: val_loss did not improve from 0.28073\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.0574 - acc: 0.9799 - val_loss: 0.3225 - val_acc: 0.9262\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0572 - acc: 0.9815\n",
      "Epoch 00031: val_loss did not improve from 0.28073\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.0572 - acc: 0.9815 - val_loss: 0.3063 - val_acc: 0.9329\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0592 - acc: 0.9803\n",
      "Epoch 00032: val_loss did not improve from 0.28073\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.0592 - acc: 0.9802 - val_loss: 0.3048 - val_acc: 0.9276\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0537 - acc: 0.9815\n",
      "Epoch 00033: val_loss did not improve from 0.28073\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.0538 - acc: 0.9814 - val_loss: 0.3050 - val_acc: 0.9317\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0690 - acc: 0.9787\n",
      "Epoch 00034: val_loss did not improve from 0.28073\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.0691 - acc: 0.9787 - val_loss: 0.3347 - val_acc: 0.9241\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0648 - acc: 0.9782\n",
      "Epoch 00035: val_loss did not improve from 0.28073\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.0649 - acc: 0.9781 - val_loss: 0.2903 - val_acc: 0.9341\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0560 - acc: 0.9814\n",
      "Epoch 00036: val_loss improved from 0.28073 to 0.27280, saving model to model/checkpoint/1D_CNN_kaggle_origin_8_conv_checkpoint/036-0.2728.hdf5\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.0560 - acc: 0.9814 - val_loss: 0.2728 - val_acc: 0.9355\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0464 - acc: 0.9844\n",
      "Epoch 00037: val_loss did not improve from 0.27280\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.0464 - acc: 0.9844 - val_loss: 0.3509 - val_acc: 0.9245\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0562 - acc: 0.9818\n",
      "Epoch 00038: val_loss did not improve from 0.27280\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.0569 - acc: 0.9817 - val_loss: 0.3238 - val_acc: 0.9311\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0622 - acc: 0.9795\n",
      "Epoch 00039: val_loss did not improve from 0.27280\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.0622 - acc: 0.9795 - val_loss: 0.3071 - val_acc: 0.9362\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0466 - acc: 0.9852\n",
      "Epoch 00040: val_loss did not improve from 0.27280\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.0469 - acc: 0.9852 - val_loss: 0.2948 - val_acc: 0.9357\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0468 - acc: 0.9850\n",
      "Epoch 00041: val_loss did not improve from 0.27280\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.0471 - acc: 0.9850 - val_loss: 0.3481 - val_acc: 0.9287\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0461 - acc: 0.9852\n",
      "Epoch 00042: val_loss did not improve from 0.27280\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.0461 - acc: 0.9852 - val_loss: 0.3036 - val_acc: 0.9352\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0335 - acc: 0.9887\n",
      "Epoch 00043: val_loss did not improve from 0.27280\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.0335 - acc: 0.9888 - val_loss: 0.3490 - val_acc: 0.9299\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0450 - acc: 0.9849\n",
      "Epoch 00044: val_loss did not improve from 0.27280\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.0450 - acc: 0.9849 - val_loss: 0.3295 - val_acc: 0.9327\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0430 - acc: 0.9854\n",
      "Epoch 00045: val_loss improved from 0.27280 to 0.26296, saving model to model/checkpoint/1D_CNN_kaggle_origin_8_conv_checkpoint/045-0.2630.hdf5\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.0431 - acc: 0.9853 - val_loss: 0.2630 - val_acc: 0.9343\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0446 - acc: 0.9852\n",
      "Epoch 00046: val_loss did not improve from 0.26296\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.0448 - acc: 0.9851 - val_loss: 0.2998 - val_acc: 0.9311\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0445 - acc: 0.9858\n",
      "Epoch 00047: val_loss did not improve from 0.26296\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.0447 - acc: 0.9858 - val_loss: 0.2880 - val_acc: 0.9362\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0397 - acc: 0.9866\n",
      "Epoch 00048: val_loss did not improve from 0.26296\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.0401 - acc: 0.9866 - val_loss: 0.2907 - val_acc: 0.9369\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0437 - acc: 0.9861\n",
      "Epoch 00049: val_loss did not improve from 0.26296\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.0437 - acc: 0.9861 - val_loss: 0.2951 - val_acc: 0.9392\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0422 - acc: 0.9860\n",
      "Epoch 00050: val_loss did not improve from 0.26296\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.0422 - acc: 0.9859 - val_loss: 0.3124 - val_acc: 0.9341\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0356 - acc: 0.9887\n",
      "Epoch 00051: val_loss did not improve from 0.26296\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.0356 - acc: 0.9887 - val_loss: 0.3164 - val_acc: 0.9271\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0314 - acc: 0.9898\n",
      "Epoch 00052: val_loss did not improve from 0.26296\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.0314 - acc: 0.9898 - val_loss: 0.2963 - val_acc: 0.9359\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0340 - acc: 0.9890\n",
      "Epoch 00053: val_loss did not improve from 0.26296\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.0340 - acc: 0.9890 - val_loss: 0.3320 - val_acc: 0.9329\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0350 - acc: 0.9882\n",
      "Epoch 00054: val_loss did not improve from 0.26296\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.0353 - acc: 0.9882 - val_loss: 0.3183 - val_acc: 0.9359\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0424 - acc: 0.9863\n",
      "Epoch 00055: val_loss did not improve from 0.26296\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.0424 - acc: 0.9863 - val_loss: 0.3291 - val_acc: 0.9311\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0308 - acc: 0.9902\n",
      "Epoch 00056: val_loss did not improve from 0.26296\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.0308 - acc: 0.9901 - val_loss: 0.3180 - val_acc: 0.9324\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0312 - acc: 0.9899\n",
      "Epoch 00057: val_loss did not improve from 0.26296\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.0312 - acc: 0.9899 - val_loss: 0.3893 - val_acc: 0.9238\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0302 - acc: 0.9903\n",
      "Epoch 00058: val_loss did not improve from 0.26296\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.0304 - acc: 0.9902 - val_loss: 0.3561 - val_acc: 0.9287\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0381 - acc: 0.9876\n",
      "Epoch 00059: val_loss did not improve from 0.26296\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.0381 - acc: 0.9876 - val_loss: 0.3263 - val_acc: 0.9304\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0307 - acc: 0.9899\n",
      "Epoch 00060: val_loss did not improve from 0.26296\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.0307 - acc: 0.9899 - val_loss: 0.3073 - val_acc: 0.9373\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0298 - acc: 0.9900\n",
      "Epoch 00061: val_loss did not improve from 0.26296\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.0298 - acc: 0.9900 - val_loss: 0.3410 - val_acc: 0.9324\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0267 - acc: 0.9914\n",
      "Epoch 00062: val_loss did not improve from 0.26296\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.0271 - acc: 0.9913 - val_loss: 0.3871 - val_acc: 0.9276\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0378 - acc: 0.9880\n",
      "Epoch 00063: val_loss did not improve from 0.26296\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.0379 - acc: 0.9880 - val_loss: 0.3169 - val_acc: 0.9345\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0267 - acc: 0.9911\n",
      "Epoch 00064: val_loss did not improve from 0.26296\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.0267 - acc: 0.9911 - val_loss: 0.3348 - val_acc: 0.9369\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0259 - acc: 0.9916\n",
      "Epoch 00065: val_loss did not improve from 0.26296\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.0262 - acc: 0.9916 - val_loss: 0.3144 - val_acc: 0.9348\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0369 - acc: 0.9884\n",
      "Epoch 00066: val_loss did not improve from 0.26296\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.0369 - acc: 0.9884 - val_loss: 0.3175 - val_acc: 0.9343\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0256 - acc: 0.9917\n",
      "Epoch 00067: val_loss did not improve from 0.26296\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.0257 - acc: 0.9917 - val_loss: 0.3135 - val_acc: 0.9397\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0271 - acc: 0.9916\n",
      "Epoch 00068: val_loss did not improve from 0.26296\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.0272 - acc: 0.9916 - val_loss: 0.3182 - val_acc: 0.9390\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0233 - acc: 0.9932\n",
      "Epoch 00069: val_loss did not improve from 0.26296\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.0234 - acc: 0.9932 - val_loss: 0.3237 - val_acc: 0.9376\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0258 - acc: 0.9915\n",
      "Epoch 00070: val_loss did not improve from 0.26296\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.0258 - acc: 0.9915 - val_loss: 0.3010 - val_acc: 0.9425\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0226 - acc: 0.9924\n",
      "Epoch 00071: val_loss did not improve from 0.26296\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.0229 - acc: 0.9924 - val_loss: 0.3480 - val_acc: 0.9373\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0328 - acc: 0.9896\n",
      "Epoch 00072: val_loss did not improve from 0.26296\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.0329 - acc: 0.9896 - val_loss: 0.3001 - val_acc: 0.9334\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0336 - acc: 0.9889\n",
      "Epoch 00073: val_loss did not improve from 0.26296\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.0336 - acc: 0.9889 - val_loss: 0.3093 - val_acc: 0.9392\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0239 - acc: 0.9918\n",
      "Epoch 00074: val_loss did not improve from 0.26296\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.0244 - acc: 0.9917 - val_loss: 0.3208 - val_acc: 0.9390\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0367 - acc: 0.9886\n",
      "Epoch 00075: val_loss did not improve from 0.26296\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.0370 - acc: 0.9885 - val_loss: 0.2933 - val_acc: 0.9418\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0334 - acc: 0.9889\n",
      "Epoch 00076: val_loss did not improve from 0.26296\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.0336 - acc: 0.9889 - val_loss: 0.3046 - val_acc: 0.9392\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0236 - acc: 0.9923\n",
      "Epoch 00077: val_loss did not improve from 0.26296\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.0239 - acc: 0.9922 - val_loss: 0.3146 - val_acc: 0.9427\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0298 - acc: 0.9902\n",
      "Epoch 00078: val_loss did not improve from 0.26296\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.0303 - acc: 0.9902 - val_loss: 0.3007 - val_acc: 0.9436\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0183 - acc: 0.9941\n",
      "Epoch 00079: val_loss did not improve from 0.26296\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.0183 - acc: 0.9941 - val_loss: 0.3333 - val_acc: 0.9422\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0222 - acc: 0.9933\n",
      "Epoch 00080: val_loss did not improve from 0.26296\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.0222 - acc: 0.9933 - val_loss: 0.3268 - val_acc: 0.9413\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0226 - acc: 0.9930\n",
      "Epoch 00081: val_loss did not improve from 0.26296\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.0227 - acc: 0.9930 - val_loss: 0.3150 - val_acc: 0.9408\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0246 - acc: 0.9923\n",
      "Epoch 00082: val_loss did not improve from 0.26296\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.0246 - acc: 0.9923 - val_loss: 0.3506 - val_acc: 0.9369\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0193 - acc: 0.9941\n",
      "Epoch 00083: val_loss did not improve from 0.26296\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.0194 - acc: 0.9941 - val_loss: 0.3677 - val_acc: 0.9343\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0249 - acc: 0.9922\n",
      "Epoch 00084: val_loss did not improve from 0.26296\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.0254 - acc: 0.9922 - val_loss: 0.3301 - val_acc: 0.9387\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0237 - acc: 0.9922\n",
      "Epoch 00085: val_loss did not improve from 0.26296\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.0242 - acc: 0.9921 - val_loss: 0.3384 - val_acc: 0.9345\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0362 - acc: 0.9893\n",
      "Epoch 00086: val_loss did not improve from 0.26296\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.0364 - acc: 0.9892 - val_loss: 0.2834 - val_acc: 0.9380\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0202 - acc: 0.9936\n",
      "Epoch 00087: val_loss did not improve from 0.26296\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.0205 - acc: 0.9936 - val_loss: 0.3286 - val_acc: 0.9408\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0284 - acc: 0.9908\n",
      "Epoch 00088: val_loss did not improve from 0.26296\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.0286 - acc: 0.9907 - val_loss: 0.3205 - val_acc: 0.9406\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0204 - acc: 0.9935\n",
      "Epoch 00089: val_loss did not improve from 0.26296\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.0205 - acc: 0.9935 - val_loss: 0.3022 - val_acc: 0.9457\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0231 - acc: 0.9930\n",
      "Epoch 00090: val_loss did not improve from 0.26296\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.0231 - acc: 0.9930 - val_loss: 0.2937 - val_acc: 0.9439\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0229 - acc: 0.9931\n",
      "Epoch 00091: val_loss did not improve from 0.26296\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.0233 - acc: 0.9930 - val_loss: 0.3155 - val_acc: 0.9401\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0247 - acc: 0.9920\n",
      "Epoch 00092: val_loss did not improve from 0.26296\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.0251 - acc: 0.9919 - val_loss: 0.3159 - val_acc: 0.9429\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0296 - acc: 0.9912\n",
      "Epoch 00093: val_loss did not improve from 0.26296\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.0299 - acc: 0.9911 - val_loss: 0.2873 - val_acc: 0.9443\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0224 - acc: 0.9928\n",
      "Epoch 00094: val_loss did not improve from 0.26296\n",
      "36805/36805 [==============================] - 40s 1ms/sample - loss: 0.0224 - acc: 0.9928 - val_loss: 0.3316 - val_acc: 0.9420\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0138 - acc: 0.9955\n",
      "Epoch 00095: val_loss did not improve from 0.26296\n",
      "36805/36805 [==============================] - 41s 1ms/sample - loss: 0.0139 - acc: 0.9955 - val_loss: 0.3292 - val_acc: 0.9436\n",
      "\n",
      "1D_CNN_kaggle_origin_8_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 0.3738 - acc: 0.9225\n",
      "Loss: 0.3737751446192386 Accuracy: 0.92253375\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.7132 - acc: 0.4508\n",
      "Epoch 00001: val_loss improved from inf to 1.02559, saving model to model/checkpoint/1D_CNN_kaggle_origin_9_conv_checkpoint/001-1.0256.hdf5\n",
      "36805/36805 [==============================] - 76s 2ms/sample - loss: 1.7131 - acc: 0.4508 - val_loss: 1.0256 - val_acc: 0.6571\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9538 - acc: 0.6889\n",
      "Epoch 00002: val_loss improved from 1.02559 to 0.66505, saving model to model/checkpoint/1D_CNN_kaggle_origin_9_conv_checkpoint/002-0.6651.hdf5\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.9539 - acc: 0.6888 - val_loss: 0.6651 - val_acc: 0.7787\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7087 - acc: 0.7685\n",
      "Epoch 00003: val_loss improved from 0.66505 to 0.50363, saving model to model/checkpoint/1D_CNN_kaggle_origin_9_conv_checkpoint/003-0.5036.hdf5\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.7089 - acc: 0.7684 - val_loss: 0.5036 - val_acc: 0.8360\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5776 - acc: 0.8097\n",
      "Epoch 00004: val_loss improved from 0.50363 to 0.45712, saving model to model/checkpoint/1D_CNN_kaggle_origin_9_conv_checkpoint/004-0.4571.hdf5\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.5780 - acc: 0.8096 - val_loss: 0.4571 - val_acc: 0.8453\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4908 - acc: 0.8403\n",
      "Epoch 00005: val_loss improved from 0.45712 to 0.40622, saving model to model/checkpoint/1D_CNN_kaggle_origin_9_conv_checkpoint/005-0.4062.hdf5\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.4909 - acc: 0.8402 - val_loss: 0.4062 - val_acc: 0.8689\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4202 - acc: 0.8643\n",
      "Epoch 00006: val_loss improved from 0.40622 to 0.36713, saving model to model/checkpoint/1D_CNN_kaggle_origin_9_conv_checkpoint/006-0.3671.hdf5\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.4202 - acc: 0.8643 - val_loss: 0.3671 - val_acc: 0.8793\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3710 - acc: 0.8769\n",
      "Epoch 00007: val_loss did not improve from 0.36713\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.3712 - acc: 0.8768 - val_loss: 0.3702 - val_acc: 0.8835\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3316 - acc: 0.8911\n",
      "Epoch 00008: val_loss did not improve from 0.36713\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.3317 - acc: 0.8911 - val_loss: 0.3719 - val_acc: 0.8826\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3022 - acc: 0.9002\n",
      "Epoch 00009: val_loss did not improve from 0.36713\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.3023 - acc: 0.9002 - val_loss: 0.3786 - val_acc: 0.8805\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2730 - acc: 0.9097\n",
      "Epoch 00010: val_loss improved from 0.36713 to 0.35063, saving model to model/checkpoint/1D_CNN_kaggle_origin_9_conv_checkpoint/010-0.3506.hdf5\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.2732 - acc: 0.9096 - val_loss: 0.3506 - val_acc: 0.8938\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2717 - acc: 0.9096\n",
      "Epoch 00011: val_loss improved from 0.35063 to 0.30944, saving model to model/checkpoint/1D_CNN_kaggle_origin_9_conv_checkpoint/011-0.3094.hdf5\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.2718 - acc: 0.9096 - val_loss: 0.3094 - val_acc: 0.9031\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2280 - acc: 0.9251\n",
      "Epoch 00012: val_loss did not improve from 0.30944\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.2282 - acc: 0.9251 - val_loss: 0.3287 - val_acc: 0.9043\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2246 - acc: 0.9255\n",
      "Epoch 00013: val_loss improved from 0.30944 to 0.29976, saving model to model/checkpoint/1D_CNN_kaggle_origin_9_conv_checkpoint/013-0.2998.hdf5\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.2246 - acc: 0.9255 - val_loss: 0.2998 - val_acc: 0.9126\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1966 - acc: 0.9349\n",
      "Epoch 00014: val_loss improved from 0.29976 to 0.28116, saving model to model/checkpoint/1D_CNN_kaggle_origin_9_conv_checkpoint/014-0.2812.hdf5\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1967 - acc: 0.9349 - val_loss: 0.2812 - val_acc: 0.9171\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1857 - acc: 0.9394\n",
      "Epoch 00015: val_loss did not improve from 0.28116\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1860 - acc: 0.9393 - val_loss: 0.3026 - val_acc: 0.9178\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1802 - acc: 0.9403\n",
      "Epoch 00016: val_loss did not improve from 0.28116\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1804 - acc: 0.9403 - val_loss: 0.3001 - val_acc: 0.9143\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1706 - acc: 0.9446\n",
      "Epoch 00017: val_loss did not improve from 0.28116\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1708 - acc: 0.9445 - val_loss: 0.3255 - val_acc: 0.9082\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1515 - acc: 0.9512\n",
      "Epoch 00018: val_loss did not improve from 0.28116\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1515 - acc: 0.9512 - val_loss: 0.3171 - val_acc: 0.9061\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1308 - acc: 0.9566\n",
      "Epoch 00019: val_loss did not improve from 0.28116\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1308 - acc: 0.9566 - val_loss: 0.2844 - val_acc: 0.9229\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1297 - acc: 0.9575\n",
      "Epoch 00020: val_loss did not improve from 0.28116\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1301 - acc: 0.9575 - val_loss: 0.2967 - val_acc: 0.9213\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1447 - acc: 0.9529\n",
      "Epoch 00021: val_loss improved from 0.28116 to 0.27336, saving model to model/checkpoint/1D_CNN_kaggle_origin_9_conv_checkpoint/021-0.2734.hdf5\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1449 - acc: 0.9528 - val_loss: 0.2734 - val_acc: 0.9201\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1359 - acc: 0.9573\n",
      "Epoch 00022: val_loss did not improve from 0.27336\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1359 - acc: 0.9573 - val_loss: 0.3230 - val_acc: 0.9192\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1053 - acc: 0.9657\n",
      "Epoch 00023: val_loss did not improve from 0.27336\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1054 - acc: 0.9656 - val_loss: 0.2785 - val_acc: 0.9255\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1094 - acc: 0.9644\n",
      "Epoch 00024: val_loss did not improve from 0.27336\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1096 - acc: 0.9644 - val_loss: 0.3116 - val_acc: 0.9185\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1228 - acc: 0.9601\n",
      "Epoch 00025: val_loss did not improve from 0.27336\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1228 - acc: 0.9600 - val_loss: 0.3012 - val_acc: 0.9222\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0945 - acc: 0.9694\n",
      "Epoch 00026: val_loss did not improve from 0.27336\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0946 - acc: 0.9694 - val_loss: 0.2874 - val_acc: 0.9262\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0925 - acc: 0.9707\n",
      "Epoch 00027: val_loss did not improve from 0.27336\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0926 - acc: 0.9707 - val_loss: 0.2918 - val_acc: 0.9245\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0976 - acc: 0.9682\n",
      "Epoch 00028: val_loss did not improve from 0.27336\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0979 - acc: 0.9681 - val_loss: 0.2793 - val_acc: 0.9238\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0973 - acc: 0.9678\n",
      "Epoch 00029: val_loss improved from 0.27336 to 0.24795, saving model to model/checkpoint/1D_CNN_kaggle_origin_9_conv_checkpoint/029-0.2479.hdf5\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0973 - acc: 0.9678 - val_loss: 0.2479 - val_acc: 0.9329\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0802 - acc: 0.9741\n",
      "Epoch 00030: val_loss did not improve from 0.24795\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0804 - acc: 0.9741 - val_loss: 0.2908 - val_acc: 0.9292\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0780 - acc: 0.9748\n",
      "Epoch 00031: val_loss did not improve from 0.24795\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0780 - acc: 0.9748 - val_loss: 0.3338 - val_acc: 0.9208\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0719 - acc: 0.9764\n",
      "Epoch 00032: val_loss did not improve from 0.24795\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0720 - acc: 0.9763 - val_loss: 0.3587 - val_acc: 0.9159\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0900 - acc: 0.9712\n",
      "Epoch 00033: val_loss did not improve from 0.24795\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0903 - acc: 0.9712 - val_loss: 0.3455 - val_acc: 0.9185\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0803 - acc: 0.9744\n",
      "Epoch 00034: val_loss did not improve from 0.24795\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0804 - acc: 0.9744 - val_loss: 0.2704 - val_acc: 0.9366\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0673 - acc: 0.9780\n",
      "Epoch 00035: val_loss did not improve from 0.24795\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0677 - acc: 0.9779 - val_loss: 0.2851 - val_acc: 0.9327\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0648 - acc: 0.9792\n",
      "Epoch 00036: val_loss did not improve from 0.24795\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0648 - acc: 0.9792 - val_loss: 0.2929 - val_acc: 0.9264\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0584 - acc: 0.9812\n",
      "Epoch 00037: val_loss did not improve from 0.24795\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0585 - acc: 0.9812 - val_loss: 0.3170 - val_acc: 0.9317\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0726 - acc: 0.9773\n",
      "Epoch 00038: val_loss did not improve from 0.24795\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0726 - acc: 0.9773 - val_loss: 0.3854 - val_acc: 0.9206\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0607 - acc: 0.9800\n",
      "Epoch 00039: val_loss did not improve from 0.24795\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0610 - acc: 0.9800 - val_loss: 0.3239 - val_acc: 0.9273\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0727 - acc: 0.9766\n",
      "Epoch 00040: val_loss did not improve from 0.24795\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0728 - acc: 0.9766 - val_loss: 0.3215 - val_acc: 0.9297\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0613 - acc: 0.9808\n",
      "Epoch 00041: val_loss did not improve from 0.24795\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0615 - acc: 0.9808 - val_loss: 0.2784 - val_acc: 0.9362\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0567 - acc: 0.9818\n",
      "Epoch 00042: val_loss did not improve from 0.24795\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0572 - acc: 0.9818 - val_loss: 0.3033 - val_acc: 0.9373\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0614 - acc: 0.9800\n",
      "Epoch 00043: val_loss did not improve from 0.24795\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0614 - acc: 0.9800 - val_loss: 0.2892 - val_acc: 0.9369\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0571 - acc: 0.9822\n",
      "Epoch 00044: val_loss did not improve from 0.24795\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0574 - acc: 0.9821 - val_loss: 0.3111 - val_acc: 0.9259\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0523 - acc: 0.9833\n",
      "Epoch 00045: val_loss did not improve from 0.24795\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0525 - acc: 0.9832 - val_loss: 0.2951 - val_acc: 0.9324\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0544 - acc: 0.9830\n",
      "Epoch 00046: val_loss did not improve from 0.24795\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0546 - acc: 0.9830 - val_loss: 0.3414 - val_acc: 0.9234\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0827 - acc: 0.9748\n",
      "Epoch 00047: val_loss did not improve from 0.24795\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0830 - acc: 0.9748 - val_loss: 0.2867 - val_acc: 0.9327\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0546 - acc: 0.9830\n",
      "Epoch 00048: val_loss did not improve from 0.24795\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0548 - acc: 0.9829 - val_loss: 0.3000 - val_acc: 0.9336\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0406 - acc: 0.9871\n",
      "Epoch 00049: val_loss did not improve from 0.24795\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0407 - acc: 0.9871 - val_loss: 0.3023 - val_acc: 0.9376\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0423 - acc: 0.9868\n",
      "Epoch 00050: val_loss did not improve from 0.24795\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0423 - acc: 0.9868 - val_loss: 0.3256 - val_acc: 0.9313\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0362 - acc: 0.9889\n",
      "Epoch 00051: val_loss did not improve from 0.24795\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0364 - acc: 0.9889 - val_loss: 0.3448 - val_acc: 0.9287\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0487 - acc: 0.9851\n",
      "Epoch 00052: val_loss did not improve from 0.24795\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0489 - acc: 0.9850 - val_loss: 0.2742 - val_acc: 0.9324\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0628 - acc: 0.9801\n",
      "Epoch 00053: val_loss did not improve from 0.24795\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0628 - acc: 0.9801 - val_loss: 0.3018 - val_acc: 0.9355\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0430 - acc: 0.9871\n",
      "Epoch 00054: val_loss did not improve from 0.24795\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0437 - acc: 0.9870 - val_loss: 0.3122 - val_acc: 0.9345\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0524 - acc: 0.9839\n",
      "Epoch 00055: val_loss did not improve from 0.24795\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0524 - acc: 0.9839 - val_loss: 0.3077 - val_acc: 0.9362\n",
      "Epoch 56/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0344 - acc: 0.9896\n",
      "Epoch 00056: val_loss did not improve from 0.24795\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0351 - acc: 0.9895 - val_loss: 0.2835 - val_acc: 0.9378\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0453 - acc: 0.9862\n",
      "Epoch 00057: val_loss did not improve from 0.24795\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0456 - acc: 0.9861 - val_loss: 0.3236 - val_acc: 0.9350\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0509 - acc: 0.9844\n",
      "Epoch 00058: val_loss did not improve from 0.24795\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0512 - acc: 0.9843 - val_loss: 0.3243 - val_acc: 0.9376\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0627 - acc: 0.9802\n",
      "Epoch 00059: val_loss did not improve from 0.24795\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0630 - acc: 0.9802 - val_loss: 0.2977 - val_acc: 0.9380\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0412 - acc: 0.9873\n",
      "Epoch 00060: val_loss did not improve from 0.24795\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0413 - acc: 0.9873 - val_loss: 0.2846 - val_acc: 0.9406\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0283 - acc: 0.9910\n",
      "Epoch 00061: val_loss did not improve from 0.24795\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0288 - acc: 0.9910 - val_loss: 0.3271 - val_acc: 0.9399\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0490 - acc: 0.9851\n",
      "Epoch 00062: val_loss did not improve from 0.24795\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0496 - acc: 0.9850 - val_loss: 0.3104 - val_acc: 0.9390\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0509 - acc: 0.9851\n",
      "Epoch 00063: val_loss did not improve from 0.24795\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0510 - acc: 0.9851 - val_loss: 0.3181 - val_acc: 0.9390\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0345 - acc: 0.9893\n",
      "Epoch 00064: val_loss did not improve from 0.24795\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0347 - acc: 0.9893 - val_loss: 0.3261 - val_acc: 0.9394\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0360 - acc: 0.9890\n",
      "Epoch 00065: val_loss did not improve from 0.24795\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0360 - acc: 0.9890 - val_loss: 0.3048 - val_acc: 0.9345\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0386 - acc: 0.9886\n",
      "Epoch 00066: val_loss did not improve from 0.24795\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0392 - acc: 0.9886 - val_loss: 0.3129 - val_acc: 0.9345\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0318 - acc: 0.9904\n",
      "Epoch 00067: val_loss did not improve from 0.24795\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0323 - acc: 0.9904 - val_loss: 0.3175 - val_acc: 0.9390\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0345 - acc: 0.9895\n",
      "Epoch 00068: val_loss did not improve from 0.24795\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0348 - acc: 0.9895 - val_loss: 0.3656 - val_acc: 0.9322\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0385 - acc: 0.9882\n",
      "Epoch 00069: val_loss did not improve from 0.24795\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0385 - acc: 0.9882 - val_loss: 0.3571 - val_acc: 0.9313\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0319 - acc: 0.9898\n",
      "Epoch 00070: val_loss did not improve from 0.24795\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0319 - acc: 0.9898 - val_loss: 0.2985 - val_acc: 0.9373\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0288 - acc: 0.9907\n",
      "Epoch 00071: val_loss did not improve from 0.24795\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0294 - acc: 0.9907 - val_loss: 0.3253 - val_acc: 0.9357\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0608 - acc: 0.9820\n",
      "Epoch 00072: val_loss did not improve from 0.24795\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0614 - acc: 0.9820 - val_loss: 0.3011 - val_acc: 0.9362\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0583 - acc: 0.9831\n",
      "Epoch 00073: val_loss did not improve from 0.24795\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0584 - acc: 0.9831 - val_loss: 0.2801 - val_acc: 0.9415\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0332 - acc: 0.9892\n",
      "Epoch 00074: val_loss did not improve from 0.24795\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0339 - acc: 0.9891 - val_loss: 0.3201 - val_acc: 0.9371\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0302 - acc: 0.9905\n",
      "Epoch 00075: val_loss did not improve from 0.24795\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0308 - acc: 0.9904 - val_loss: 0.2923 - val_acc: 0.9420\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0579 - acc: 0.9823\n",
      "Epoch 00076: val_loss did not improve from 0.24795\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0588 - acc: 0.9822 - val_loss: 0.3168 - val_acc: 0.9348\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0490 - acc: 0.9836\n",
      "Epoch 00077: val_loss did not improve from 0.24795\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0494 - acc: 0.9836 - val_loss: 0.3242 - val_acc: 0.9350\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0293 - acc: 0.9906\n",
      "Epoch 00078: val_loss did not improve from 0.24795\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0297 - acc: 0.9905 - val_loss: 0.3162 - val_acc: 0.9385\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0335 - acc: 0.9902\n",
      "Epoch 00079: val_loss did not improve from 0.24795\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0335 - acc: 0.9902 - val_loss: 0.3241 - val_acc: 0.9371\n",
      "\n",
      "1D_CNN_kaggle_origin_9_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 7s 1ms/sample - loss: 0.3306 - acc: 0.9167\n",
      "Loss: 0.33056128315650785 Accuracy: 0.9167186\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5, 10):\n",
    "    base = '1D_CNN_kaggle_origin'\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    model = build_1d_cnn_kaggle_origin(conv_num=i)\n",
    "    \n",
    "    model = multi_gpu_model(model, gpus=2)\n",
    "    \n",
    "#         model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4),\n",
    "          metrics=['accuracy'])\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model_filename = model_path+'{epoch:03d}-{val_loss:.4f}.hdf5'\n",
    "    checkpointer = ModelCheckpoint(filepath = model_filename, monitor = \"val_loss\", \n",
    "                                   verbose=1, save_best_only=True)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=50)\n",
    "    hist = model.fit(x_train_abs, y_train_onehot, batch_size=64, epochs=500, \n",
    "                     validation_data=[x_val_abs, y_val_onehot], shuffle=True, \n",
    "                     callbacks = [checkpointer, early_stopping])\n",
    "\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "    ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "    ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "    ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    png_path = 'visualization/learning_curve/'\n",
    "    filename = model_name+'.png'\n",
    "    os.makedirs(png_path, exist_ok=True)\n",
    "    fig.savefig(png_path+filename, transparent=True)\n",
    "\n",
    "    del(model)\n",
    "    \n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "    model = load_model(model_filename)\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "    print()\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_kaggle_origin_5_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, 16000, 1)     0           input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 16000, 1)     0           input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "model_5 (Model)                 (None, 16)           1298716     lambda[0][0]                     \n",
      "                                                                 lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Concatenate)          (None, 16)           0           model_5[1][0]                    \n",
      "                                                                 model_5[2][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,298,716\n",
      "Trainable params: 1,297,722\n",
      "Non-trainable params: 994\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.2486 - acc: 0.9277\n",
      "Loss: 0.24860892441444804 Accuracy: 0.92772585\n",
      "\n",
      "1D_CNN_kaggle_origin_6_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 16000, 1)     0           input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (None, 16000, 1)     0           input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "model_7 (Model)                 (None, 16)           1727260     lambda_2[0][0]                   \n",
      "                                                                 lambda_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Concatenate)          (None, 16)           0           model_7[1][0]                    \n",
      "                                                                 model_7[2][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,727,260\n",
      "Trainable params: 1,725,242\n",
      "Non-trainable params: 2,018\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 7s 1ms/sample - loss: 0.3447 - acc: 0.8922\n",
      "Loss: 0.344692622352612 Accuracy: 0.89221185\n",
      "\n",
      "1D_CNN_kaggle_origin_7_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_8 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_4 (Lambda)               (None, 16000, 1)     0           input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_5 (Lambda)               (None, 16000, 1)     0           input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "model_9 (Model)                 (None, 16)           3174172     lambda_4[0][0]                   \n",
      "                                                                 lambda_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_15 (Concatenate)          (None, 16)           0           model_9[1][0]                    \n",
      "                                                                 model_9[2][0]                    \n",
      "==================================================================================================\n",
      "Total params: 3,174,172\n",
      "Trainable params: 3,170,106\n",
      "Non-trainable params: 4,066\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 7s 2ms/sample - loss: 0.4602 - acc: 0.9061\n",
      "Loss: 0.46023374948406887 Accuracy: 0.9061267\n",
      "\n",
      "1D_CNN_kaggle_origin_8_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_6 (Lambda)               (None, 16000, 1)     0           input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_7 (Lambda)               (None, 16000, 1)     0           input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "model_11 (Model)                (None, 16)           8427292     lambda_6[0][0]                   \n",
      "                                                                 lambda_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_17 (Concatenate)          (None, 16)           0           model_11[1][0]                   \n",
      "                                                                 model_11[2][0]                   \n",
      "==================================================================================================\n",
      "Total params: 8,427,292\n",
      "Trainable params: 8,419,130\n",
      "Non-trainable params: 8,162\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 8s 2ms/sample - loss: 0.3738 - acc: 0.9225\n",
      "Loss: 0.3737751446192386 Accuracy: 0.92253375\n",
      "\n",
      "1D_CNN_kaggle_origin_9_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_10 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_8 (Lambda)               (None, 16000, 1)     0           input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_9 (Lambda)               (None, 16000, 1)     0           input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "model_13 (Model)                (None, 16)           28370716    lambda_8[0][0]                   \n",
      "                                                                 lambda_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_19 (Concatenate)          (None, 16)           0           model_13[1][0]                   \n",
      "                                                                 model_13[2][0]                   \n",
      "==================================================================================================\n",
      "Total params: 28,370,716\n",
      "Trainable params: 28,354,362\n",
      "Non-trainable params: 16,354\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 10s 2ms/sample - loss: 0.3306 - acc: 0.9167\n",
      "Loss: 0.33056128315650785 Accuracy: 0.9167186\n"
     ]
    }
   ],
   "source": [
    "log_dir = 'log'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "base = '1D_CNN_kaggle_origin'\n",
    "\n",
    "with open(path.join(log_dir, base), 'w') as log_file:\n",
    "    for i in range(5, 10):\n",
    "        model_name = base+'_{}_conv'.format(i)\n",
    "        print()\n",
    "        print(model_name, 'Model')\n",
    "        model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "        model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "\n",
    "        model = load_model(model_filename)\n",
    "        model.summary()\n",
    "\n",
    "        [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "        print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "        del(model)\n",
    "\n",
    "        log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
