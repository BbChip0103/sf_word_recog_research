{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path as path\n",
    "import itertools\n",
    "from sklearn.preprocessing import maxabs_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Input,InputLayer, Dense, Activation, BatchNormalization, Flatten, Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D, Dropout\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler, \\\n",
    "                                        EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '6'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = path.join('..', 'data')\n",
    "data_dir = path.join(base_dir, 'data_speech_commands_v0.02')\n",
    " \n",
    "train_txt = path.join(data_dir, 'wav_train_16words.txt')\n",
    "val_txt = path.join(data_dir, 'wav_validation_16words.txt')\n",
    "test_txt = path.join(data_dir, 'wav_test_16words.txt')\n",
    "\n",
    "train_data = np.load(path.join(data_dir, 'wav_train_data.npz'))\n",
    "val_data = np.load(path.join(data_dir, 'wav_validation_data.npz'))\n",
    "test_data = np.load(path.join(data_dir, 'wav_test_data.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36805, 16000, 1),\n",
       " (36805,),\n",
       " (4293, 16000, 1),\n",
       " (4293,),\n",
       " (4815, 16000, 1),\n",
       " (4815,),\n",
       " (16, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = train_data['x_train']\n",
    "y_train = train_data['y_train']\n",
    "x_val = val_data['x_val']\n",
    "y_val = val_data['y_val']\n",
    "x_test = test_data['x_test']\n",
    "y_test = test_data['y_test']\n",
    "y_table = test_data['table']\n",
    "\n",
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape, y_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_test[0].shape\n",
    "output_size = y_table.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_abs = np.asarray([maxabs_scale(wav) for wav in x_train])\n",
    "y_train_onehot = np.asarray([to_categorical(label, output_size) for label in y_train])\n",
    "del x_train, y_train\n",
    "\n",
    "x_val_abs = np.asarray([maxabs_scale(wav) for wav in x_val])\n",
    "y_val_onehot = np.asarray([to_categorical(label, output_size) for label in y_val])\n",
    "del x_val, y_val\n",
    "\n",
    "x_test_abs = np.asarray([maxabs_scale(wav) for wav in x_test])\n",
    "y_test_onehot = np.asarray([to_categorical(label, output_size) for label in y_test])\n",
    "del x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_1d_cnn_custom_conv_3_VGG_BN(conv_num=1):\n",
    "    channel_size = 32\n",
    "    \n",
    "    model=Sequential()\n",
    "    model.add(Conv1D (kernel_size=3, filters=channel_size, strides=1, \n",
    "                      padding='same', input_shape=input_shape)) \n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv1D (kernel_size=3, filters=channel_size, strides=1, \n",
    "                      padding='same')) \n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "#     model.add(MaxPooling1D(pool_size=3, strides=3, padding='same'))\n",
    "    \n",
    "    for i in range(conv_num-1):\n",
    "        model.add(Conv1D (kernel_size=3, filters=channel_size*(2**int((i+1)/4)), strides=1, padding='same'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Conv1D (kernel_size=3, filters=channel_size*(2**int((i+1)/4)), strides=1, padding='same'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(MaxPooling1D(pool_size=2, strides=2))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(BatchNormalization())\n",
    "#     model.add(Dropout(0.5))\n",
    "    model.add(Dense(output_size, activation='softmax' ))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1 (Batc (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_1 (Ba (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 512000)            0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_2 (Ba (None, 512000)            2048000   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                8192016   \n",
      "=================================================================\n",
      "Total params: 10,243,504\n",
      "Trainable params: 9,219,376\n",
      "Non-trainable params: 1,024,128\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_2 (Conv1D)            (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_3 (Ba (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_4 (Ba (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_5 (Ba (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_6 (Ba (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 256000)            0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_7 (Ba (None, 256000)            1024000   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                4096016   \n",
      "=================================================================\n",
      "Total params: 5,129,968\n",
      "Trainable params: 4,617,712\n",
      "Non-trainable params: 512,256\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_6 (Conv1D)            (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_8 (Ba (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_9 (Ba (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_10 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_11 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_12 (B (None, 8000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_13 (B (None, 8000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 128000)            0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_14 (B (None, 128000)            512000    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                2048016   \n",
      "=================================================================\n",
      "Total params: 2,576,432\n",
      "Trainable params: 2,320,048\n",
      "Non-trainable params: 256,384\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_12 (Conv1D)           (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_15 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_16 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_17 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_15 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_18 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_16 (Conv1D)           (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_19 (B (None, 8000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_17 (Conv1D)           (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_20 (B (None, 8000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_18 (Conv1D)           (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_21 (B (None, 4000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_19 (Conv1D)           (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_22 (B (None, 4000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 2000, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 64000)             0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_23 (B (None, 64000)             256000    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                1024016   \n",
      "=================================================================\n",
      "Total params: 1,302,896\n",
      "Trainable params: 1,174,384\n",
      "Non-trainable params: 128,512\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_20 (Conv1D)           (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_24 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_21 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_25 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_22 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_26 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_23 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_27 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_24 (Conv1D)           (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_28 (B (None, 8000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_25 (Conv1D)           (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_29 (B (None, 8000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_26 (Conv1D)           (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_30 (B (None, 4000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_27 (Conv1D)           (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_31 (B (None, 4000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_27 (Activation)   (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 2000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_28 (Conv1D)           (None, 2000, 64)          6208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_32 (B (None, 2000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_28 (Activation)   (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_29 (Conv1D)           (None, 2000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_33 (B (None, 2000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_29 (Activation)   (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1 (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 64000)             0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_34 (B (None, 64000)             256000    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                1024016   \n",
      "=================================================================\n",
      "Total params: 1,321,968\n",
      "Trainable params: 1,193,200\n",
      "Non-trainable params: 128,768\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_30 (Conv1D)           (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_35 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_30 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_31 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_36 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_31 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_32 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_37 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_32 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_33 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_38 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_33 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_34 (Conv1D)           (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_39 (B (None, 8000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_34 (Activation)   (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_35 (Conv1D)           (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_40 (B (None, 8000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_35 (Activation)   (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_36 (Conv1D)           (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_41 (B (None, 4000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_36 (Activation)   (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_37 (Conv1D)           (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_42 (B (None, 4000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_37 (Activation)   (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling (None, 2000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_38 (Conv1D)           (None, 2000, 64)          6208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_43 (B (None, 2000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_38 (Activation)   (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_39 (Conv1D)           (None, 2000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_44 (B (None, 2000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_39 (Activation)   (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_40 (Conv1D)           (None, 1000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_45 (B (None, 1000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_40 (Activation)   (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_41 (Conv1D)           (None, 1000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_46 (B (None, 1000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_41 (Activation)   (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 32000)             0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_47 (B (None, 32000)             128000    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 16)                512016    \n",
      "=================================================================\n",
      "Total params: 707,184\n",
      "Trainable params: 642,160\n",
      "Non-trainable params: 65,024\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_42 (Conv1D)           (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_48 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_42 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_43 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_49 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_43 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_44 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_50 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_44 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_45 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_51 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_45 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_46 (Conv1D)           (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_52 (B (None, 8000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_46 (Activation)   (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_47 (Conv1D)           (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_53 (B (None, 8000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_47 (Activation)   (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_48 (Conv1D)           (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_54 (B (None, 4000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_48 (Activation)   (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_49 (Conv1D)           (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_55 (B (None, 4000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_49 (Activation)   (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling (None, 2000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_50 (Conv1D)           (None, 2000, 64)          6208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_56 (B (None, 2000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_50 (Activation)   (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_51 (Conv1D)           (None, 2000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_57 (B (None, 2000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_51 (Activation)   (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_52 (Conv1D)           (None, 1000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_58 (B (None, 1000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_52 (Activation)   (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_53 (Conv1D)           (None, 1000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_59 (B (None, 1000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_53 (Activation)   (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_54 (Conv1D)           (None, 500, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_60 (B (None, 500, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_54 (Activation)   (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_55 (Conv1D)           (None, 500, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_61 (B (None, 500, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_55 (Activation)   (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 16000)             0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_62 (B (None, 16000)             64000     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 16)                256016    \n",
      "=================================================================\n",
      "Total params: 412,400\n",
      "Trainable params: 379,120\n",
      "Non-trainable params: 33,280\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_56 (Conv1D)           (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_63 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_56 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_57 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_64 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_57 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_58 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_65 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_58 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_59 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_66 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_59 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_60 (Conv1D)           (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_67 (B (None, 8000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_60 (Activation)   (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_61 (Conv1D)           (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_68 (B (None, 8000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_61 (Activation)   (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_62 (Conv1D)           (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_69 (B (None, 4000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_62 (Activation)   (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_63 (Conv1D)           (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_70 (B (None, 4000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_63 (Activation)   (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling (None, 2000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_64 (Conv1D)           (None, 2000, 64)          6208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_71 (B (None, 2000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_64 (Activation)   (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_65 (Conv1D)           (None, 2000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_72 (B (None, 2000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_65 (Activation)   (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_66 (Conv1D)           (None, 1000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_73 (B (None, 1000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_66 (Activation)   (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_67 (Conv1D)           (None, 1000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_74 (B (None, 1000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_67 (Activation)   (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_68 (Conv1D)           (None, 500, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_75 (B (None, 500, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_68 (Activation)   (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_69 (Conv1D)           (None, 500, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_76 (B (None, 500, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_69 (Activation)   (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_26 (MaxPooling (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_70 (Conv1D)           (None, 250, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_77 (B (None, 250, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_70 (Activation)   (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_71 (Conv1D)           (None, 250, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_78 (B (None, 250, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_71 (Activation)   (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_27 (MaxPooling (None, 125, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 8000)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_79 (B (None, 8000)              32000     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 16)                128016    \n",
      "=================================================================\n",
      "Total params: 277,616\n",
      "Trainable params: 260,080\n",
      "Non-trainable params: 17,536\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_72 (Conv1D)           (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_80 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_72 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_73 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_81 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_73 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_74 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_82 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_74 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_75 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_83 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_75 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_28 (MaxPooling (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_76 (Conv1D)           (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_84 (B (None, 8000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_76 (Activation)   (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_77 (Conv1D)           (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_85 (B (None, 8000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_77 (Activation)   (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_29 (MaxPooling (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_78 (Conv1D)           (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_86 (B (None, 4000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_78 (Activation)   (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_79 (Conv1D)           (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_87 (B (None, 4000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_79 (Activation)   (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_30 (MaxPooling (None, 2000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_80 (Conv1D)           (None, 2000, 64)          6208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_88 (B (None, 2000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_80 (Activation)   (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_81 (Conv1D)           (None, 2000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_89 (B (None, 2000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_81 (Activation)   (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_31 (MaxPooling (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_82 (Conv1D)           (None, 1000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_90 (B (None, 1000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_82 (Activation)   (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_83 (Conv1D)           (None, 1000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_91 (B (None, 1000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_83 (Activation)   (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_32 (MaxPooling (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_84 (Conv1D)           (None, 500, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_92 (B (None, 500, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_84 (Activation)   (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_85 (Conv1D)           (None, 500, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_93 (B (None, 500, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_85 (Activation)   (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_86 (Conv1D)           (None, 250, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_94 (B (None, 250, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_86 (Activation)   (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_87 (Conv1D)           (None, 250, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_95 (B (None, 250, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_87 (Activation)   (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling (None, 125, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_88 (Conv1D)           (None, 125, 128)          24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_96 (B (None, 125, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_88 (Activation)   (None, 125, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_89 (Conv1D)           (None, 125, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_97 (B (None, 125, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_89 (Activation)   (None, 125, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling (None, 62, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 7936)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_98 (B (None, 7936)              31744     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 16)                126992    \n",
      "=================================================================\n",
      "Total params: 351,344\n",
      "Trainable params: 333,424\n",
      "Non-trainable params: 17,920\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_90 (Conv1D)           (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_99 (B (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_90 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_91 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_100 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_91 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_92 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_101 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_92 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_93 (Conv1D)           (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_102 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_93 (Activation)   (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_94 (Conv1D)           (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_103 ( (None, 8000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_94 (Activation)   (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_95 (Conv1D)           (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_104 ( (None, 8000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_95 (Activation)   (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_96 (Conv1D)           (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_105 ( (None, 4000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_96 (Activation)   (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_97 (Conv1D)           (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_106 ( (None, 4000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_97 (Activation)   (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling (None, 2000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_98 (Conv1D)           (None, 2000, 64)          6208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_107 ( (None, 2000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_98 (Activation)   (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_99 (Conv1D)           (None, 2000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_108 ( (None, 2000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_99 (Activation)   (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_100 (Conv1D)          (None, 1000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_109 ( (None, 1000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_100 (Activation)  (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_101 (Conv1D)          (None, 1000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_110 ( (None, 1000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_101 (Activation)  (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_102 (Conv1D)          (None, 500, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_111 ( (None, 500, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_102 (Activation)  (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_103 (Conv1D)          (None, 500, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_112 ( (None, 500, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_103 (Activation)  (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_104 (Conv1D)          (None, 250, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_113 ( (None, 250, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_104 (Activation)  (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_105 (Conv1D)          (None, 250, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_114 ( (None, 250, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_105 (Activation)  (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling (None, 125, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_106 (Conv1D)          (None, 125, 128)          24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_115 ( (None, 125, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_106 (Activation)  (None, 125, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_107 (Conv1D)          (None, 125, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_116 ( (None, 125, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_107 (Activation)  (None, 125, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling (None, 62, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_108 (Conv1D)          (None, 62, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_117 ( (None, 62, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_108 (Activation)  (None, 62, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_109 (Conv1D)          (None, 62, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_118 ( (None, 62, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_109 (Activation)  (None, 62, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling (None, 31, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 3968)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_119 ( (None, 3968)              15872     \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 16)                63504     \n",
      "=================================================================\n",
      "Total params: 371,568\n",
      "Trainable params: 361,072\n",
      "Non-trainable params: 10,496\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_110 (Conv1D)          (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_120 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_110 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_111 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_121 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_111 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_112 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_122 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_112 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_113 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_123 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_113 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_114 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_124 ( (None, 8000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_114 (Activation)  (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_115 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_125 ( (None, 8000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_115 (Activation)  (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_116 (Conv1D)          (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_126 ( (None, 4000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_116 (Activation)  (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_117 (Conv1D)          (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_127 ( (None, 4000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_117 (Activation)  (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling (None, 2000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_118 (Conv1D)          (None, 2000, 64)          6208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_128 ( (None, 2000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_118 (Activation)  (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_119 (Conv1D)          (None, 2000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_129 ( (None, 2000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_119 (Activation)  (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_120 (Conv1D)          (None, 1000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_130 ( (None, 1000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_120 (Activation)  (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_121 (Conv1D)          (None, 1000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_131 ( (None, 1000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_121 (Activation)  (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_122 (Conv1D)          (None, 500, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_132 ( (None, 500, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_122 (Activation)  (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_123 (Conv1D)          (None, 500, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_133 ( (None, 500, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_123 (Activation)  (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_124 (Conv1D)          (None, 250, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_134 ( (None, 250, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_124 (Activation)  (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_125 (Conv1D)          (None, 250, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_135 ( (None, 250, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_125 (Activation)  (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling (None, 125, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_126 (Conv1D)          (None, 125, 128)          24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_136 ( (None, 125, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_126 (Activation)  (None, 125, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_127 (Conv1D)          (None, 125, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_137 ( (None, 125, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_127 (Activation)  (None, 125, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling (None, 62, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_128 (Conv1D)          (None, 62, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_138 ( (None, 62, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_128 (Activation)  (None, 62, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_129 (Conv1D)          (None, 62, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_139 ( (None, 62, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_129 (Activation)  (None, 62, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling (None, 31, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_130 (Conv1D)          (None, 31, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_140 ( (None, 31, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_130 (Activation)  (None, 31, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_131 (Conv1D)          (None, 31, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_141 ( (None, 31, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_131 (Activation)  (None, 31, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling (None, 15, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 1920)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_142 ( (None, 1920)              7680      \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 16)                30736     \n",
      "=================================================================\n",
      "Total params: 430,192\n",
      "Trainable params: 423,280\n",
      "Non-trainable params: 6,912\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_132 (Conv1D)          (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_143 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_132 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_133 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_144 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_133 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_134 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_145 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_134 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_135 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_146 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_135 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_136 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_147 ( (None, 8000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_136 (Activation)  (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_137 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_148 ( (None, 8000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_137 (Activation)  (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_138 (Conv1D)          (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_149 ( (None, 4000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_138 (Activation)  (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_139 (Conv1D)          (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_150 ( (None, 4000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_139 (Activation)  (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling (None, 2000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_140 (Conv1D)          (None, 2000, 64)          6208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_151 ( (None, 2000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_140 (Activation)  (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_141 (Conv1D)          (None, 2000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_152 ( (None, 2000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_141 (Activation)  (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_142 (Conv1D)          (None, 1000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_153 ( (None, 1000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_142 (Activation)  (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_143 (Conv1D)          (None, 1000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_154 ( (None, 1000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_143 (Activation)  (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_144 (Conv1D)          (None, 500, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_155 ( (None, 500, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_144 (Activation)  (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_145 (Conv1D)          (None, 500, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_156 ( (None, 500, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_145 (Activation)  (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_146 (Conv1D)          (None, 250, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_157 ( (None, 250, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_146 (Activation)  (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_147 (Conv1D)          (None, 250, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_158 ( (None, 250, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_147 (Activation)  (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling (None, 125, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_148 (Conv1D)          (None, 125, 128)          24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_159 ( (None, 125, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_148 (Activation)  (None, 125, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_149 (Conv1D)          (None, 125, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_160 ( (None, 125, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_149 (Activation)  (None, 125, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling (None, 62, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_150 (Conv1D)          (None, 62, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_161 ( (None, 62, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_150 (Activation)  (None, 62, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_151 (Conv1D)          (None, 62, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_162 ( (None, 62, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_151 (Activation)  (None, 62, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling (None, 31, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_152 (Conv1D)          (None, 31, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_163 ( (None, 31, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_152 (Activation)  (None, 31, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_153 (Conv1D)          (None, 31, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_164 ( (None, 31, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_153 (Activation)  (None, 31, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling (None, 15, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_154 (Conv1D)          (None, 15, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_165 ( (None, 15, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_154 (Activation)  (None, 15, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_155 (Conv1D)          (None, 15, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_166 ( (None, 15, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_155 (Activation)  (None, 15, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_167 ( (None, 896)               3584      \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 16)                14352     \n",
      "=================================================================\n",
      "Total params: 509,296\n",
      "Trainable params: 503,920\n",
      "Non-trainable params: 5,376\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_156 (Conv1D)          (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_168 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_156 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_157 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_169 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_157 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_158 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_170 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_158 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_159 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_171 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_159 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_66 (MaxPooling (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_160 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_172 ( (None, 8000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_160 (Activation)  (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_161 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_173 ( (None, 8000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_161 (Activation)  (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_67 (MaxPooling (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_162 (Conv1D)          (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_174 ( (None, 4000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_162 (Activation)  (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_163 (Conv1D)          (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_175 ( (None, 4000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_163 (Activation)  (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_68 (MaxPooling (None, 2000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_164 (Conv1D)          (None, 2000, 64)          6208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_176 ( (None, 2000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_164 (Activation)  (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_165 (Conv1D)          (None, 2000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_177 ( (None, 2000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_165 (Activation)  (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_69 (MaxPooling (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_166 (Conv1D)          (None, 1000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_178 ( (None, 1000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_166 (Activation)  (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_167 (Conv1D)          (None, 1000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_179 ( (None, 1000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_167 (Activation)  (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_70 (MaxPooling (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_168 (Conv1D)          (None, 500, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_180 ( (None, 500, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_168 (Activation)  (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_169 (Conv1D)          (None, 500, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_181 ( (None, 500, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_169 (Activation)  (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_71 (MaxPooling (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_170 (Conv1D)          (None, 250, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_182 ( (None, 250, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_170 (Activation)  (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_171 (Conv1D)          (None, 250, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_183 ( (None, 250, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_171 (Activation)  (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_72 (MaxPooling (None, 125, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_172 (Conv1D)          (None, 125, 128)          24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_184 ( (None, 125, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_172 (Activation)  (None, 125, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_173 (Conv1D)          (None, 125, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_185 ( (None, 125, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_173 (Activation)  (None, 125, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_73 (MaxPooling (None, 62, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_174 (Conv1D)          (None, 62, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_186 ( (None, 62, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_174 (Activation)  (None, 62, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_175 (Conv1D)          (None, 62, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_187 ( (None, 62, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_175 (Activation)  (None, 62, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_74 (MaxPooling (None, 31, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_176 (Conv1D)          (None, 31, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_188 ( (None, 31, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_176 (Activation)  (None, 31, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_177 (Conv1D)          (None, 31, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_189 ( (None, 31, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_177 (Activation)  (None, 31, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_75 (MaxPooling (None, 15, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_178 (Conv1D)          (None, 15, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_190 ( (None, 15, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_178 (Activation)  (None, 15, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_179 (Conv1D)          (None, 15, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_191 ( (None, 15, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_179 (Activation)  (None, 15, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_76 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_180 (Conv1D)          (None, 7, 256)            98560     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_192 ( (None, 7, 256)            1024      \n",
      "_________________________________________________________________\n",
      "activation_180 (Activation)  (None, 7, 256)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_181 (Conv1D)          (None, 7, 256)            196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_193 ( (None, 7, 256)            1024      \n",
      "_________________________________________________________________\n",
      "activation_181 (Activation)  (None, 7, 256)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_77 (MaxPooling (None, 3, 256)            0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_194 ( (None, 768)               3072      \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 16)                12304     \n",
      "=================================================================\n",
      "Total params: 804,208\n",
      "Trainable params: 798,064\n",
      "Non-trainable params: 6,144\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 14):\n",
    "    model = build_1d_cnn_custom_conv_3_VGG_BN(conv_num=i)\n",
    "    model.summary()\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36805 samples, validate on 4293 samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.5281 - acc: 0.3736\n",
      "Epoch 00001: val_loss improved from inf to 2.18427, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_BN_3_conv_checkpoint/001-2.1843.hdf5\n",
      "36805/36805 [==============================] - 94s 3ms/sample - loss: 2.5278 - acc: 0.3736 - val_loss: 2.1843 - val_acc: 0.3659\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8666 - acc: 0.7555\n",
      "Epoch 00002: val_loss did not improve from 2.18427\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.8666 - acc: 0.7554 - val_loss: 2.2440 - val_acc: 0.4451\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3309 - acc: 0.9204\n",
      "Epoch 00003: val_loss improved from 2.18427 to 2.18309, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_BN_3_conv_checkpoint/003-2.1831.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.3309 - acc: 0.9204 - val_loss: 2.1831 - val_acc: 0.4773\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1660 - acc: 0.9716\n",
      "Epoch 00004: val_loss did not improve from 2.18309\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.1662 - acc: 0.9716 - val_loss: 2.2546 - val_acc: 0.4861\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1218 - acc: 0.9818\n",
      "Epoch 00005: val_loss did not improve from 2.18309\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.1219 - acc: 0.9818 - val_loss: 2.2523 - val_acc: 0.4906\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0821 - acc: 0.9892\n",
      "Epoch 00006: val_loss did not improve from 2.18309\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.0821 - acc: 0.9891 - val_loss: 2.3376 - val_acc: 0.4999\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0783 - acc: 0.9881\n",
      "Epoch 00007: val_loss did not improve from 2.18309\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.0784 - acc: 0.9881 - val_loss: 2.5303 - val_acc: 0.4796\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0863 - acc: 0.9858\n",
      "Epoch 00008: val_loss did not improve from 2.18309\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.0865 - acc: 0.9857 - val_loss: 2.6013 - val_acc: 0.4789\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0711 - acc: 0.9877\n",
      "Epoch 00009: val_loss did not improve from 2.18309\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.0711 - acc: 0.9876 - val_loss: 2.8880 - val_acc: 0.4582\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0741 - acc: 0.9865\n",
      "Epoch 00010: val_loss did not improve from 2.18309\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.0741 - acc: 0.9866 - val_loss: 2.8395 - val_acc: 0.4740\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0576 - acc: 0.9899\n",
      "Epoch 00011: val_loss did not improve from 2.18309\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.0578 - acc: 0.9899 - val_loss: 2.7989 - val_acc: 0.4852\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0818 - acc: 0.9832\n",
      "Epoch 00012: val_loss did not improve from 2.18309\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.0820 - acc: 0.9832 - val_loss: 3.0898 - val_acc: 0.4712\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0771 - acc: 0.9836\n",
      "Epoch 00013: val_loss did not improve from 2.18309\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.0773 - acc: 0.9836 - val_loss: 3.1909 - val_acc: 0.4631\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0637 - acc: 0.9873\n",
      "Epoch 00014: val_loss did not improve from 2.18309\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.0640 - acc: 0.9872 - val_loss: 3.5935 - val_acc: 0.4379\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0622 - acc: 0.9879\n",
      "Epoch 00015: val_loss did not improve from 2.18309\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.0625 - acc: 0.9879 - val_loss: 3.3143 - val_acc: 0.4752\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0565 - acc: 0.9900\n",
      "Epoch 00016: val_loss did not improve from 2.18309\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.0567 - acc: 0.9899 - val_loss: 3.3687 - val_acc: 0.4691\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0615 - acc: 0.9882\n",
      "Epoch 00017: val_loss did not improve from 2.18309\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.0615 - acc: 0.9882 - val_loss: 3.6655 - val_acc: 0.4556\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0453 - acc: 0.9926\n",
      "Epoch 00018: val_loss did not improve from 2.18309\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.0453 - acc: 0.9926 - val_loss: 3.7579 - val_acc: 0.4545\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0385 - acc: 0.9939\n",
      "Epoch 00019: val_loss did not improve from 2.18309\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.0385 - acc: 0.9939 - val_loss: 3.6127 - val_acc: 0.4696\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0444 - acc: 0.9928\n",
      "Epoch 00020: val_loss did not improve from 2.18309\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.0444 - acc: 0.9928 - val_loss: 3.7718 - val_acc: 0.4701\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0507 - acc: 0.9912\n",
      "Epoch 00021: val_loss did not improve from 2.18309\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.0507 - acc: 0.9912 - val_loss: 3.9430 - val_acc: 0.4447\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0400 - acc: 0.9931\n",
      "Epoch 00022: val_loss did not improve from 2.18309\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.0401 - acc: 0.9931 - val_loss: 3.9280 - val_acc: 0.4559\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0538 - acc: 0.9890\n",
      "Epoch 00023: val_loss did not improve from 2.18309\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.0538 - acc: 0.9890 - val_loss: 4.4205 - val_acc: 0.4319\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0547 - acc: 0.9896\n",
      "Epoch 00024: val_loss did not improve from 2.18309\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.0552 - acc: 0.9895 - val_loss: 4.0313 - val_acc: 0.4486\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0489 - acc: 0.9905\n",
      "Epoch 00025: val_loss did not improve from 2.18309\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.0489 - acc: 0.9905 - val_loss: 3.9845 - val_acc: 0.4601\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0373 - acc: 0.9939\n",
      "Epoch 00026: val_loss did not improve from 2.18309\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.0373 - acc: 0.9938 - val_loss: 3.9878 - val_acc: 0.4626\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0430 - acc: 0.9918\n",
      "Epoch 00027: val_loss did not improve from 2.18309\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.0433 - acc: 0.9918 - val_loss: 4.4383 - val_acc: 0.4447\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0458 - acc: 0.9921\n",
      "Epoch 00028: val_loss did not improve from 2.18309\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.0458 - acc: 0.9921 - val_loss: 4.3185 - val_acc: 0.4468\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0376 - acc: 0.9943\n",
      "Epoch 00029: val_loss did not improve from 2.18309\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.0379 - acc: 0.9942 - val_loss: 4.5275 - val_acc: 0.4433\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0547 - acc: 0.9893\n",
      "Epoch 00030: val_loss did not improve from 2.18309\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.0558 - acc: 0.9892 - val_loss: 4.3513 - val_acc: 0.4542\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0483 - acc: 0.9910\n",
      "Epoch 00031: val_loss did not improve from 2.18309\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.0483 - acc: 0.9910 - val_loss: 4.2554 - val_acc: 0.4612\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0346 - acc: 0.9947\n",
      "Epoch 00032: val_loss did not improve from 2.18309\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.0346 - acc: 0.9947 - val_loss: 4.2893 - val_acc: 0.4621\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0441 - acc: 0.9920\n",
      "Epoch 00033: val_loss did not improve from 2.18309\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.0444 - acc: 0.9919 - val_loss: 4.3774 - val_acc: 0.4631\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0409 - acc: 0.9927\n",
      "Epoch 00034: val_loss did not improve from 2.18309\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.0414 - acc: 0.9927 - val_loss: 4.3684 - val_acc: 0.4687\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0361 - acc: 0.9945\n",
      "Epoch 00035: val_loss did not improve from 2.18309\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.0361 - acc: 0.9945 - val_loss: 4.4525 - val_acc: 0.4628\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0363 - acc: 0.9942\n",
      "Epoch 00036: val_loss did not improve from 2.18309\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.0366 - acc: 0.9942 - val_loss: 4.5123 - val_acc: 0.4521\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0461 - acc: 0.9910\n",
      "Epoch 00037: val_loss did not improve from 2.18309\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.0461 - acc: 0.9910 - val_loss: 4.4379 - val_acc: 0.4668\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0337 - acc: 0.9944\n",
      "Epoch 00038: val_loss did not improve from 2.18309\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.0343 - acc: 0.9943 - val_loss: 4.5468 - val_acc: 0.4694\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0424 - acc: 0.9929\n",
      "Epoch 00039: val_loss did not improve from 2.18309\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.0425 - acc: 0.9929 - val_loss: 4.6689 - val_acc: 0.4542\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0531 - acc: 0.9901\n",
      "Epoch 00040: val_loss did not improve from 2.18309\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.0531 - acc: 0.9901 - val_loss: 4.6357 - val_acc: 0.4533\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0354 - acc: 0.9946\n",
      "Epoch 00041: val_loss did not improve from 2.18309\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.0354 - acc: 0.9946 - val_loss: 4.7930 - val_acc: 0.4465\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0388 - acc: 0.9937\n",
      "Epoch 00042: val_loss did not improve from 2.18309\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.0388 - acc: 0.9936 - val_loss: 5.3892 - val_acc: 0.4218\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0354 - acc: 0.9948\n",
      "Epoch 00043: val_loss did not improve from 2.18309\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.0360 - acc: 0.9948 - val_loss: 4.7484 - val_acc: 0.4493\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0536 - acc: 0.9889\n",
      "Epoch 00044: val_loss did not improve from 2.18309\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.0543 - acc: 0.9888 - val_loss: 4.8157 - val_acc: 0.4524\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0492 - acc: 0.9914\n",
      "Epoch 00045: val_loss did not improve from 2.18309\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.0492 - acc: 0.9914 - val_loss: 4.6854 - val_acc: 0.4645\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0360 - acc: 0.9948\n",
      "Epoch 00046: val_loss did not improve from 2.18309\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.0361 - acc: 0.9948 - val_loss: 5.0573 - val_acc: 0.4454\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0434 - acc: 0.9931\n",
      "Epoch 00047: val_loss did not improve from 2.18309\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.0434 - acc: 0.9931 - val_loss: 4.8579 - val_acc: 0.4554\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0340 - acc: 0.9951\n",
      "Epoch 00048: val_loss did not improve from 2.18309\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.0341 - acc: 0.9951 - val_loss: 4.9566 - val_acc: 0.4638\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0404 - acc: 0.9930\n",
      "Epoch 00049: val_loss did not improve from 2.18309\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.0406 - acc: 0.9929 - val_loss: 4.8319 - val_acc: 0.4666\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0468 - acc: 0.9920\n",
      "Epoch 00050: val_loss did not improve from 2.18309\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.0468 - acc: 0.9920 - val_loss: 4.8399 - val_acc: 0.4649\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0337 - acc: 0.9953\n",
      "Epoch 00051: val_loss did not improve from 2.18309\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.0338 - acc: 0.9952 - val_loss: 4.8846 - val_acc: 0.4736\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0512 - acc: 0.9910\n",
      "Epoch 00052: val_loss did not improve from 2.18309\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.0512 - acc: 0.9910 - val_loss: 4.9699 - val_acc: 0.4626\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0322 - acc: 0.9958\n",
      "Epoch 00053: val_loss did not improve from 2.18309\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.0328 - acc: 0.9957 - val_loss: 4.9765 - val_acc: 0.4663\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_ch_32_BN_3_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 939us/sample - loss: 2.3763 - acc: 0.4390\n",
      "Loss: 2.3762977059385118 Accuracy: 0.43904465\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.2171 - acc: 0.4011\n",
      "Epoch 00001: val_loss improved from inf to 1.94851, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_BN_4_conv_checkpoint/001-1.9485.hdf5\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 2.2174 - acc: 0.4011 - val_loss: 1.9485 - val_acc: 0.4144\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0612 - acc: 0.6868\n",
      "Epoch 00002: val_loss did not improve from 1.94851\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 1.0612 - acc: 0.6868 - val_loss: 1.9897 - val_acc: 0.4761\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5502 - acc: 0.8440\n",
      "Epoch 00003: val_loss improved from 1.94851 to 1.65778, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_BN_4_conv_checkpoint/003-1.6578.hdf5\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.5505 - acc: 0.8440 - val_loss: 1.6578 - val_acc: 0.5646\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3093 - acc: 0.9267\n",
      "Epoch 00004: val_loss did not improve from 1.65778\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.3094 - acc: 0.9266 - val_loss: 1.7747 - val_acc: 0.5495\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1900 - acc: 0.9649\n",
      "Epoch 00005: val_loss did not improve from 1.65778\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.1901 - acc: 0.9649 - val_loss: 1.7486 - val_acc: 0.5581\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1352 - acc: 0.9795\n",
      "Epoch 00006: val_loss did not improve from 1.65778\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.1354 - acc: 0.9794 - val_loss: 1.7453 - val_acc: 0.5833\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0963 - acc: 0.9875\n",
      "Epoch 00007: val_loss did not improve from 1.65778\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0965 - acc: 0.9875 - val_loss: 1.7702 - val_acc: 0.5849\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0897 - acc: 0.9873\n",
      "Epoch 00008: val_loss did not improve from 1.65778\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0899 - acc: 0.9873 - val_loss: 1.8307 - val_acc: 0.5772\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0788 - acc: 0.9876\n",
      "Epoch 00009: val_loss did not improve from 1.65778\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0789 - acc: 0.9876 - val_loss: 1.9209 - val_acc: 0.5812\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0597 - acc: 0.9920\n",
      "Epoch 00010: val_loss did not improve from 1.65778\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0599 - acc: 0.9920 - val_loss: 1.9152 - val_acc: 0.5891\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0649 - acc: 0.9892\n",
      "Epoch 00011: val_loss did not improve from 1.65778\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0649 - acc: 0.9891 - val_loss: 2.1820 - val_acc: 0.5420\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0559 - acc: 0.9915\n",
      "Epoch 00012: val_loss did not improve from 1.65778\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0564 - acc: 0.9914 - val_loss: 2.0261 - val_acc: 0.5823\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0589 - acc: 0.9895\n",
      "Epoch 00013: val_loss did not improve from 1.65778\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0589 - acc: 0.9895 - val_loss: 2.1730 - val_acc: 0.5602\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0376 - acc: 0.9954\n",
      "Epoch 00014: val_loss did not improve from 1.65778\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0378 - acc: 0.9953 - val_loss: 2.7990 - val_acc: 0.4936\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0513 - acc: 0.9911\n",
      "Epoch 00015: val_loss did not improve from 1.65778\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0513 - acc: 0.9911 - val_loss: 2.3158 - val_acc: 0.5560\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0406 - acc: 0.9937\n",
      "Epoch 00016: val_loss did not improve from 1.65778\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0407 - acc: 0.9936 - val_loss: 2.3588 - val_acc: 0.5392\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0412 - acc: 0.9930\n",
      "Epoch 00017: val_loss did not improve from 1.65778\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0413 - acc: 0.9930 - val_loss: 2.5454 - val_acc: 0.5413\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0410 - acc: 0.9929\n",
      "Epoch 00018: val_loss did not improve from 1.65778\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0413 - acc: 0.9928 - val_loss: 2.3269 - val_acc: 0.5656\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0485 - acc: 0.9898\n",
      "Epoch 00019: val_loss did not improve from 1.65778\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0485 - acc: 0.9898 - val_loss: 2.5828 - val_acc: 0.5458\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0314 - acc: 0.9955\n",
      "Epoch 00020: val_loss did not improve from 1.65778\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0314 - acc: 0.9955 - val_loss: 2.3837 - val_acc: 0.5646\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0279 - acc: 0.9958\n",
      "Epoch 00021: val_loss did not improve from 1.65778\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0279 - acc: 0.9958 - val_loss: 2.8189 - val_acc: 0.5316\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0350 - acc: 0.9939\n",
      "Epoch 00022: val_loss did not improve from 1.65778\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0350 - acc: 0.9939 - val_loss: 2.5917 - val_acc: 0.5544\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0307 - acc: 0.9950\n",
      "Epoch 00023: val_loss did not improve from 1.65778\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0308 - acc: 0.9949 - val_loss: 2.4864 - val_acc: 0.5565\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0317 - acc: 0.9933\n",
      "Epoch 00024: val_loss did not improve from 1.65778\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0318 - acc: 0.9933 - val_loss: 3.1766 - val_acc: 0.5129\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0293 - acc: 0.9952\n",
      "Epoch 00025: val_loss did not improve from 1.65778\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0294 - acc: 0.9951 - val_loss: 2.6126 - val_acc: 0.5630\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0332 - acc: 0.9939\n",
      "Epoch 00026: val_loss did not improve from 1.65778\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0333 - acc: 0.9939 - val_loss: 2.6061 - val_acc: 0.5663\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0304 - acc: 0.9945\n",
      "Epoch 00027: val_loss did not improve from 1.65778\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0308 - acc: 0.9944 - val_loss: 2.7964 - val_acc: 0.5509\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0359 - acc: 0.9929\n",
      "Epoch 00028: val_loss did not improve from 1.65778\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0365 - acc: 0.9928 - val_loss: 2.8746 - val_acc: 0.5467\n",
      "Epoch 29/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0405 - acc: 0.9911\n",
      "Epoch 00029: val_loss did not improve from 1.65778\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0406 - acc: 0.9911 - val_loss: 2.7364 - val_acc: 0.5481\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0313 - acc: 0.9938\n",
      "Epoch 00030: val_loss did not improve from 1.65778\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0313 - acc: 0.9938 - val_loss: 2.7524 - val_acc: 0.5544\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0260 - acc: 0.9960\n",
      "Epoch 00031: val_loss did not improve from 1.65778\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0261 - acc: 0.9960 - val_loss: 2.8025 - val_acc: 0.5516\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0267 - acc: 0.9954\n",
      "Epoch 00032: val_loss did not improve from 1.65778\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0269 - acc: 0.9953 - val_loss: 2.7137 - val_acc: 0.5667\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0290 - acc: 0.9951\n",
      "Epoch 00033: val_loss did not improve from 1.65778\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0290 - acc: 0.9951 - val_loss: 2.9321 - val_acc: 0.5479\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0340 - acc: 0.9937\n",
      "Epoch 00034: val_loss did not improve from 1.65778\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0341 - acc: 0.9937 - val_loss: 2.9499 - val_acc: 0.5530\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0346 - acc: 0.9928\n",
      "Epoch 00035: val_loss did not improve from 1.65778\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0346 - acc: 0.9928 - val_loss: 2.7530 - val_acc: 0.5744\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0229 - acc: 0.9964\n",
      "Epoch 00036: val_loss did not improve from 1.65778\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0232 - acc: 0.9964 - val_loss: 2.8586 - val_acc: 0.5602\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0320 - acc: 0.9936\n",
      "Epoch 00037: val_loss did not improve from 1.65778\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0322 - acc: 0.9936 - val_loss: 2.9595 - val_acc: 0.5604\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0320 - acc: 0.9937\n",
      "Epoch 00038: val_loss did not improve from 1.65778\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0320 - acc: 0.9938 - val_loss: 3.0101 - val_acc: 0.5528\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0209 - acc: 0.9973\n",
      "Epoch 00039: val_loss did not improve from 1.65778\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0209 - acc: 0.9973 - val_loss: 2.9223 - val_acc: 0.5667\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0216 - acc: 0.9970\n",
      "Epoch 00040: val_loss did not improve from 1.65778\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0216 - acc: 0.9970 - val_loss: 3.0026 - val_acc: 0.5516\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0225 - acc: 0.9968\n",
      "Epoch 00041: val_loss did not improve from 1.65778\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0228 - acc: 0.9968 - val_loss: 2.9853 - val_acc: 0.5563\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0326 - acc: 0.9932\n",
      "Epoch 00042: val_loss did not improve from 1.65778\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0327 - acc: 0.9931 - val_loss: 3.2509 - val_acc: 0.5483\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0267 - acc: 0.9949\n",
      "Epoch 00043: val_loss did not improve from 1.65778\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0269 - acc: 0.9949 - val_loss: 3.1002 - val_acc: 0.5511\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0320 - acc: 0.9941\n",
      "Epoch 00044: val_loss did not improve from 1.65778\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0322 - acc: 0.9941 - val_loss: 3.1732 - val_acc: 0.5390\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0291 - acc: 0.9949\n",
      "Epoch 00045: val_loss did not improve from 1.65778\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0297 - acc: 0.9949 - val_loss: 3.2286 - val_acc: 0.5455\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0263 - acc: 0.9952\n",
      "Epoch 00046: val_loss did not improve from 1.65778\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0263 - acc: 0.9952 - val_loss: 3.0501 - val_acc: 0.5544\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0219 - acc: 0.9966\n",
      "Epoch 00047: val_loss did not improve from 1.65778\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0221 - acc: 0.9966 - val_loss: 3.2715 - val_acc: 0.5542\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0262 - acc: 0.9954\n",
      "Epoch 00048: val_loss did not improve from 1.65778\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0263 - acc: 0.9953 - val_loss: 3.1310 - val_acc: 0.5600\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0263 - acc: 0.9952\n",
      "Epoch 00049: val_loss did not improve from 1.65778\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0267 - acc: 0.9952 - val_loss: 3.1445 - val_acc: 0.5600\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0297 - acc: 0.9939\n",
      "Epoch 00050: val_loss did not improve from 1.65778\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0299 - acc: 0.9938 - val_loss: 3.0841 - val_acc: 0.5618\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0281 - acc: 0.9946\n",
      "Epoch 00051: val_loss did not improve from 1.65778\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0282 - acc: 0.9945 - val_loss: 3.1990 - val_acc: 0.5595\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0264 - acc: 0.9947\n",
      "Epoch 00052: val_loss did not improve from 1.65778\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0265 - acc: 0.9947 - val_loss: 3.1582 - val_acc: 0.5593\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0238 - acc: 0.9955\n",
      "Epoch 00053: val_loss did not improve from 1.65778\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0241 - acc: 0.9954 - val_loss: 3.1511 - val_acc: 0.5698\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_ch_32_BN_4_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 1.7769 - acc: 0.5090\n",
      "Loss: 1.7768519674134775 Accuracy: 0.5090343\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.1290 - acc: 0.4158\n",
      "Epoch 00001: val_loss improved from inf to 2.01533, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_BN_5_conv_checkpoint/001-2.0153.hdf5\n",
      "36805/36805 [==============================] - 124s 3ms/sample - loss: 2.1293 - acc: 0.4158 - val_loss: 2.0153 - val_acc: 0.4274\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1135 - acc: 0.6705\n",
      "Epoch 00002: val_loss improved from 2.01533 to 1.54101, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_BN_5_conv_checkpoint/002-1.5410.hdf5\n",
      "36805/36805 [==============================] - 100s 3ms/sample - loss: 1.1136 - acc: 0.6705 - val_loss: 1.5410 - val_acc: 0.5779\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5847 - acc: 0.8251\n",
      "Epoch 00003: val_loss improved from 1.54101 to 1.47902, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_BN_5_conv_checkpoint/003-1.4790.hdf5\n",
      "36805/36805 [==============================] - 100s 3ms/sample - loss: 0.5855 - acc: 0.8250 - val_loss: 1.4790 - val_acc: 0.5935\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3370 - acc: 0.9089\n",
      "Epoch 00004: val_loss did not improve from 1.47902\n",
      "36805/36805 [==============================] - 100s 3ms/sample - loss: 0.3376 - acc: 0.9088 - val_loss: 2.7004 - val_acc: 0.4344\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2252 - acc: 0.9459\n",
      "Epoch 00005: val_loss did not improve from 1.47902\n",
      "36805/36805 [==============================] - 100s 3ms/sample - loss: 0.2253 - acc: 0.9458 - val_loss: 1.6020 - val_acc: 0.5914\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1287 - acc: 0.9767\n",
      "Epoch 00006: val_loss did not improve from 1.47902\n",
      "36805/36805 [==============================] - 100s 3ms/sample - loss: 0.1292 - acc: 0.9766 - val_loss: 1.6824 - val_acc: 0.5903\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1036 - acc: 0.9820\n",
      "Epoch 00007: val_loss did not improve from 1.47902\n",
      "36805/36805 [==============================] - 100s 3ms/sample - loss: 0.1036 - acc: 0.9820 - val_loss: 1.6049 - val_acc: 0.6082\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0722 - acc: 0.9895\n",
      "Epoch 00008: val_loss did not improve from 1.47902\n",
      "36805/36805 [==============================] - 100s 3ms/sample - loss: 0.0723 - acc: 0.9895 - val_loss: 1.6437 - val_acc: 0.6147\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0733 - acc: 0.9877\n",
      "Epoch 00009: val_loss did not improve from 1.47902\n",
      "36805/36805 [==============================] - 100s 3ms/sample - loss: 0.0738 - acc: 0.9876 - val_loss: 1.8007 - val_acc: 0.6133\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0716 - acc: 0.9872\n",
      "Epoch 00010: val_loss did not improve from 1.47902\n",
      "36805/36805 [==============================] - 100s 3ms/sample - loss: 0.0716 - acc: 0.9872 - val_loss: 1.9030 - val_acc: 0.5868\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0576 - acc: 0.9905\n",
      "Epoch 00011: val_loss did not improve from 1.47902\n",
      "36805/36805 [==============================] - 100s 3ms/sample - loss: 0.0576 - acc: 0.9905 - val_loss: 1.8613 - val_acc: 0.5942\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0488 - acc: 0.9914\n",
      "Epoch 00012: val_loss did not improve from 1.47902\n",
      "36805/36805 [==============================] - 100s 3ms/sample - loss: 0.0489 - acc: 0.9914 - val_loss: 1.9355 - val_acc: 0.6003\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0602 - acc: 0.9881\n",
      "Epoch 00013: val_loss did not improve from 1.47902\n",
      "36805/36805 [==============================] - 100s 3ms/sample - loss: 0.0605 - acc: 0.9880 - val_loss: 2.0200 - val_acc: 0.5952\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0567 - acc: 0.9877\n",
      "Epoch 00014: val_loss did not improve from 1.47902\n",
      "36805/36805 [==============================] - 100s 3ms/sample - loss: 0.0568 - acc: 0.9876 - val_loss: 2.0386 - val_acc: 0.5973\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0611 - acc: 0.9853\n",
      "Epoch 00015: val_loss did not improve from 1.47902\n",
      "36805/36805 [==============================] - 100s 3ms/sample - loss: 0.0611 - acc: 0.9853 - val_loss: 2.0061 - val_acc: 0.6094\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0378 - acc: 0.9927\n",
      "Epoch 00016: val_loss did not improve from 1.47902\n",
      "36805/36805 [==============================] - 100s 3ms/sample - loss: 0.0379 - acc: 0.9927 - val_loss: 2.1262 - val_acc: 0.5996\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0412 - acc: 0.9911\n",
      "Epoch 00017: val_loss did not improve from 1.47902\n",
      "36805/36805 [==============================] - 100s 3ms/sample - loss: 0.0412 - acc: 0.9910 - val_loss: 2.0847 - val_acc: 0.5996\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0378 - acc: 0.9916\n",
      "Epoch 00018: val_loss did not improve from 1.47902\n",
      "36805/36805 [==============================] - 100s 3ms/sample - loss: 0.0378 - acc: 0.9916 - val_loss: 2.1795 - val_acc: 0.5898\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0298 - acc: 0.9942\n",
      "Epoch 00019: val_loss did not improve from 1.47902\n",
      "36805/36805 [==============================] - 100s 3ms/sample - loss: 0.0298 - acc: 0.9942 - val_loss: 2.1014 - val_acc: 0.6063\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0297 - acc: 0.9938\n",
      "Epoch 00020: val_loss did not improve from 1.47902\n",
      "36805/36805 [==============================] - 100s 3ms/sample - loss: 0.0299 - acc: 0.9938 - val_loss: 3.0344 - val_acc: 0.5241\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0662 - acc: 0.9833\n",
      "Epoch 00021: val_loss did not improve from 1.47902\n",
      "36805/36805 [==============================] - 100s 3ms/sample - loss: 0.0662 - acc: 0.9833 - val_loss: 2.2457 - val_acc: 0.5973\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0330 - acc: 0.9932\n",
      "Epoch 00022: val_loss did not improve from 1.47902\n",
      "36805/36805 [==============================] - 100s 3ms/sample - loss: 0.0331 - acc: 0.9931 - val_loss: 2.3443 - val_acc: 0.5893\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0310 - acc: 0.9931\n",
      "Epoch 00023: val_loss did not improve from 1.47902\n",
      "36805/36805 [==============================] - 100s 3ms/sample - loss: 0.0312 - acc: 0.9930 - val_loss: 2.2631 - val_acc: 0.6045\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0346 - acc: 0.9922\n",
      "Epoch 00024: val_loss did not improve from 1.47902\n",
      "36805/36805 [==============================] - 100s 3ms/sample - loss: 0.0346 - acc: 0.9922 - val_loss: 2.4347 - val_acc: 0.5898\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0303 - acc: 0.9932\n",
      "Epoch 00025: val_loss did not improve from 1.47902\n",
      "36805/36805 [==============================] - 100s 3ms/sample - loss: 0.0303 - acc: 0.9932 - val_loss: 2.5656 - val_acc: 0.5793\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0234 - acc: 0.9956\n",
      "Epoch 00026: val_loss did not improve from 1.47902\n",
      "36805/36805 [==============================] - 100s 3ms/sample - loss: 0.0234 - acc: 0.9956 - val_loss: 2.3272 - val_acc: 0.6031\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0239 - acc: 0.9956\n",
      "Epoch 00027: val_loss did not improve from 1.47902\n",
      "36805/36805 [==============================] - 100s 3ms/sample - loss: 0.0242 - acc: 0.9955 - val_loss: 2.6158 - val_acc: 0.5940\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0501 - acc: 0.9866\n",
      "Epoch 00028: val_loss did not improve from 1.47902\n",
      "36805/36805 [==============================] - 100s 3ms/sample - loss: 0.0502 - acc: 0.9865 - val_loss: 2.5090 - val_acc: 0.6024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0358 - acc: 0.9912\n",
      "Epoch 00029: val_loss did not improve from 1.47902\n",
      "36805/36805 [==============================] - 100s 3ms/sample - loss: 0.0359 - acc: 0.9912 - val_loss: 2.5323 - val_acc: 0.6040\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0290 - acc: 0.9930\n",
      "Epoch 00030: val_loss did not improve from 1.47902\n",
      "36805/36805 [==============================] - 100s 3ms/sample - loss: 0.0290 - acc: 0.9930 - val_loss: 2.6096 - val_acc: 0.5956\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0261 - acc: 0.9949\n",
      "Epoch 00031: val_loss did not improve from 1.47902\n",
      "36805/36805 [==============================] - 100s 3ms/sample - loss: 0.0263 - acc: 0.9949 - val_loss: 2.5469 - val_acc: 0.5919\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0296 - acc: 0.9927\n",
      "Epoch 00032: val_loss did not improve from 1.47902\n",
      "36805/36805 [==============================] - 100s 3ms/sample - loss: 0.0297 - acc: 0.9927 - val_loss: 2.5382 - val_acc: 0.6089\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0347 - acc: 0.9919\n",
      "Epoch 00033: val_loss did not improve from 1.47902\n",
      "36805/36805 [==============================] - 100s 3ms/sample - loss: 0.0349 - acc: 0.9919 - val_loss: 2.5573 - val_acc: 0.5998\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0316 - acc: 0.9926\n",
      "Epoch 00034: val_loss did not improve from 1.47902\n",
      "36805/36805 [==============================] - 100s 3ms/sample - loss: 0.0324 - acc: 0.9925 - val_loss: 2.6521 - val_acc: 0.5986\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0351 - acc: 0.9910\n",
      "Epoch 00035: val_loss did not improve from 1.47902\n",
      "36805/36805 [==============================] - 100s 3ms/sample - loss: 0.0352 - acc: 0.9910 - val_loss: 2.5720 - val_acc: 0.5991\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0242 - acc: 0.9949\n",
      "Epoch 00036: val_loss did not improve from 1.47902\n",
      "36805/36805 [==============================] - 100s 3ms/sample - loss: 0.0243 - acc: 0.9948 - val_loss: 2.5811 - val_acc: 0.5917\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0287 - acc: 0.9935\n",
      "Epoch 00037: val_loss did not improve from 1.47902\n",
      "36805/36805 [==============================] - 100s 3ms/sample - loss: 0.0287 - acc: 0.9935 - val_loss: 2.5648 - val_acc: 0.6061\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0236 - acc: 0.9956\n",
      "Epoch 00038: val_loss did not improve from 1.47902\n",
      "36805/36805 [==============================] - 100s 3ms/sample - loss: 0.0239 - acc: 0.9955 - val_loss: 2.8579 - val_acc: 0.5779\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0370 - acc: 0.9910\n",
      "Epoch 00039: val_loss did not improve from 1.47902\n",
      "36805/36805 [==============================] - 100s 3ms/sample - loss: 0.0371 - acc: 0.9909 - val_loss: 2.6526 - val_acc: 0.5975\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0272 - acc: 0.9933\n",
      "Epoch 00040: val_loss did not improve from 1.47902\n",
      "36805/36805 [==============================] - 100s 3ms/sample - loss: 0.0271 - acc: 0.9933 - val_loss: 2.6557 - val_acc: 0.6082\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0189 - acc: 0.9966\n",
      "Epoch 00041: val_loss did not improve from 1.47902\n",
      "36805/36805 [==============================] - 100s 3ms/sample - loss: 0.0191 - acc: 0.9965 - val_loss: 2.7264 - val_acc: 0.5966\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0352 - acc: 0.9911\n",
      "Epoch 00042: val_loss did not improve from 1.47902\n",
      "36805/36805 [==============================] - 100s 3ms/sample - loss: 0.0356 - acc: 0.9910 - val_loss: 3.0202 - val_acc: 0.5800\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0328 - acc: 0.9918\n",
      "Epoch 00043: val_loss did not improve from 1.47902\n",
      "36805/36805 [==============================] - 100s 3ms/sample - loss: 0.0330 - acc: 0.9917 - val_loss: 2.7737 - val_acc: 0.6003\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0323 - acc: 0.9918\n",
      "Epoch 00044: val_loss did not improve from 1.47902\n",
      "36805/36805 [==============================] - 100s 3ms/sample - loss: 0.0323 - acc: 0.9918 - val_loss: 2.6516 - val_acc: 0.6061\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0198 - acc: 0.9958\n",
      "Epoch 00045: val_loss did not improve from 1.47902\n",
      "36805/36805 [==============================] - 100s 3ms/sample - loss: 0.0198 - acc: 0.9958 - val_loss: 2.7233 - val_acc: 0.5982\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0178 - acc: 0.9961\n",
      "Epoch 00046: val_loss did not improve from 1.47902\n",
      "36805/36805 [==============================] - 100s 3ms/sample - loss: 0.0178 - acc: 0.9961 - val_loss: 2.8314 - val_acc: 0.6012\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0225 - acc: 0.9955\n",
      "Epoch 00047: val_loss did not improve from 1.47902\n",
      "36805/36805 [==============================] - 100s 3ms/sample - loss: 0.0225 - acc: 0.9955 - val_loss: 2.7368 - val_acc: 0.5998\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0278 - acc: 0.9934\n",
      "Epoch 00048: val_loss did not improve from 1.47902\n",
      "36805/36805 [==============================] - 100s 3ms/sample - loss: 0.0278 - acc: 0.9934 - val_loss: 2.9153 - val_acc: 0.5926\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0228 - acc: 0.9951\n",
      "Epoch 00049: val_loss did not improve from 1.47902\n",
      "36805/36805 [==============================] - 100s 3ms/sample - loss: 0.0229 - acc: 0.9951 - val_loss: 2.8693 - val_acc: 0.5912\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0275 - acc: 0.9933\n",
      "Epoch 00050: val_loss did not improve from 1.47902\n",
      "36805/36805 [==============================] - 100s 3ms/sample - loss: 0.0275 - acc: 0.9933 - val_loss: 2.8645 - val_acc: 0.5970\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0291 - acc: 0.9935\n",
      "Epoch 00051: val_loss did not improve from 1.47902\n",
      "36805/36805 [==============================] - 100s 3ms/sample - loss: 0.0292 - acc: 0.9935 - val_loss: 2.9641 - val_acc: 0.5898\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0308 - acc: 0.9922\n",
      "Epoch 00052: val_loss did not improve from 1.47902\n",
      "36805/36805 [==============================] - 100s 3ms/sample - loss: 0.0308 - acc: 0.9922 - val_loss: 2.8311 - val_acc: 0.6063\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0182 - acc: 0.9967\n",
      "Epoch 00053: val_loss did not improve from 1.47902\n",
      "36805/36805 [==============================] - 100s 3ms/sample - loss: 0.0185 - acc: 0.9966 - val_loss: 2.9768 - val_acc: 0.5856\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_ch_32_BN_5_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 1.5917 - acc: 0.5634\n",
      "Loss: 1.591745559201186 Accuracy: 0.56344754\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.0157 - acc: 0.4157\n",
      "Epoch 00001: val_loss improved from inf to 1.78228, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_BN_6_conv_checkpoint/001-1.7823.hdf5\n",
      "36805/36805 [==============================] - 138s 4ms/sample - loss: 2.0157 - acc: 0.4158 - val_loss: 1.7823 - val_acc: 0.4640\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2304 - acc: 0.6371\n",
      "Epoch 00002: val_loss improved from 1.78228 to 1.44131, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_BN_6_conv_checkpoint/002-1.4413.hdf5\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 1.2305 - acc: 0.6371 - val_loss: 1.4413 - val_acc: 0.5952\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8895 - acc: 0.7333\n",
      "Epoch 00003: val_loss improved from 1.44131 to 1.32439, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_BN_6_conv_checkpoint/003-1.3244.hdf5\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.8898 - acc: 0.7332 - val_loss: 1.3244 - val_acc: 0.6289\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6544 - acc: 0.8012\n",
      "Epoch 00004: val_loss improved from 1.32439 to 1.26756, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_BN_6_conv_checkpoint/004-1.2676.hdf5\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.6547 - acc: 0.8012 - val_loss: 1.2676 - val_acc: 0.6567\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4756 - acc: 0.8618\n",
      "Epoch 00005: val_loss improved from 1.26756 to 1.15969, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_BN_6_conv_checkpoint/005-1.1597.hdf5\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.4756 - acc: 0.8618 - val_loss: 1.1597 - val_acc: 0.6813\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3459 - acc: 0.9036\n",
      "Epoch 00006: val_loss did not improve from 1.15969\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.3461 - acc: 0.9035 - val_loss: 1.2591 - val_acc: 0.6662\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2661 - acc: 0.9299\n",
      "Epoch 00007: val_loss did not improve from 1.15969\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.2661 - acc: 0.9299 - val_loss: 1.2108 - val_acc: 0.6758\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1879 - acc: 0.9573\n",
      "Epoch 00008: val_loss did not improve from 1.15969\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.1881 - acc: 0.9572 - val_loss: 1.2334 - val_acc: 0.6839\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1644 - acc: 0.9629\n",
      "Epoch 00009: val_loss did not improve from 1.15969\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.1645 - acc: 0.9629 - val_loss: 1.2436 - val_acc: 0.6837\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1253 - acc: 0.9757\n",
      "Epoch 00010: val_loss did not improve from 1.15969\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.1254 - acc: 0.9756 - val_loss: 1.3452 - val_acc: 0.6744\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1098 - acc: 0.9787\n",
      "Epoch 00011: val_loss did not improve from 1.15969\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.1098 - acc: 0.9787 - val_loss: 1.4485 - val_acc: 0.6548\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0895 - acc: 0.9846\n",
      "Epoch 00012: val_loss did not improve from 1.15969\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.0895 - acc: 0.9846 - val_loss: 1.4371 - val_acc: 0.6622\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0868 - acc: 0.9831\n",
      "Epoch 00013: val_loss did not improve from 1.15969\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.0870 - acc: 0.9831 - val_loss: 1.3501 - val_acc: 0.6865\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0873 - acc: 0.9822\n",
      "Epoch 00014: val_loss did not improve from 1.15969\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.0874 - acc: 0.9821 - val_loss: 1.6509 - val_acc: 0.6345\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0657 - acc: 0.9880\n",
      "Epoch 00015: val_loss did not improve from 1.15969\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.0659 - acc: 0.9880 - val_loss: 1.3829 - val_acc: 0.6888\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0600 - acc: 0.9899\n",
      "Epoch 00016: val_loss did not improve from 1.15969\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.0599 - acc: 0.9899 - val_loss: 1.3888 - val_acc: 0.6834\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0550 - acc: 0.9910\n",
      "Epoch 00017: val_loss did not improve from 1.15969\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.0550 - acc: 0.9910 - val_loss: 1.5407 - val_acc: 0.6669\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0561 - acc: 0.9898\n",
      "Epoch 00018: val_loss did not improve from 1.15969\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.0562 - acc: 0.9898 - val_loss: 1.5223 - val_acc: 0.6662\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0721 - acc: 0.9845\n",
      "Epoch 00019: val_loss did not improve from 1.15969\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.0722 - acc: 0.9845 - val_loss: 1.4959 - val_acc: 0.6874\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0554 - acc: 0.9892\n",
      "Epoch 00020: val_loss did not improve from 1.15969\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.0558 - acc: 0.9891 - val_loss: 1.4825 - val_acc: 0.6781\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0680 - acc: 0.9845\n",
      "Epoch 00021: val_loss did not improve from 1.15969\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.0683 - acc: 0.9845 - val_loss: 1.5262 - val_acc: 0.6788\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0514 - acc: 0.9894\n",
      "Epoch 00022: val_loss did not improve from 1.15969\n",
      "36805/36805 [==============================] - 106s 3ms/sample - loss: 0.0514 - acc: 0.9894 - val_loss: 1.5644 - val_acc: 0.6797\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0316 - acc: 0.9950\n",
      "Epoch 00023: val_loss did not improve from 1.15969\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.0317 - acc: 0.9949 - val_loss: 1.6540 - val_acc: 0.6818\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0411 - acc: 0.9923\n",
      "Epoch 00024: val_loss did not improve from 1.15969\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.0411 - acc: 0.9922 - val_loss: 1.5538 - val_acc: 0.6813\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0395 - acc: 0.9926\n",
      "Epoch 00025: val_loss did not improve from 1.15969\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.0395 - acc: 0.9926 - val_loss: 1.5602 - val_acc: 0.6900\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0272 - acc: 0.9961\n",
      "Epoch 00026: val_loss did not improve from 1.15969\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.0277 - acc: 0.9959 - val_loss: 1.5217 - val_acc: 0.6914\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0538 - acc: 0.9871\n",
      "Epoch 00027: val_loss did not improve from 1.15969\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.0538 - acc: 0.9871 - val_loss: 1.9345 - val_acc: 0.6350\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0414 - acc: 0.9907\n",
      "Epoch 00028: val_loss did not improve from 1.15969\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.0416 - acc: 0.9906 - val_loss: 1.5847 - val_acc: 0.6790\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0439 - acc: 0.9898\n",
      "Epoch 00029: val_loss did not improve from 1.15969\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.0440 - acc: 0.9898 - val_loss: 1.6131 - val_acc: 0.6897\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0253 - acc: 0.9958\n",
      "Epoch 00030: val_loss did not improve from 1.15969\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.0258 - acc: 0.9957 - val_loss: 1.6372 - val_acc: 0.6827\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0549 - acc: 0.9864\n",
      "Epoch 00031: val_loss did not improve from 1.15969\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.0549 - acc: 0.9864 - val_loss: 1.6548 - val_acc: 0.6806\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0231 - acc: 0.9966\n",
      "Epoch 00032: val_loss did not improve from 1.15969\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.0233 - acc: 0.9965 - val_loss: 2.2413 - val_acc: 0.6247\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0488 - acc: 0.9882\n",
      "Epoch 00033: val_loss did not improve from 1.15969\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.0488 - acc: 0.9882 - val_loss: 1.7044 - val_acc: 0.6844\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0302 - acc: 0.9942\n",
      "Epoch 00034: val_loss did not improve from 1.15969\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.0304 - acc: 0.9941 - val_loss: 1.7379 - val_acc: 0.6688\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0345 - acc: 0.9928\n",
      "Epoch 00035: val_loss did not improve from 1.15969\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.0346 - acc: 0.9928 - val_loss: 1.6944 - val_acc: 0.6925\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0279 - acc: 0.9945\n",
      "Epoch 00036: val_loss did not improve from 1.15969\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.0279 - acc: 0.9945 - val_loss: 1.6519 - val_acc: 0.6876\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0289 - acc: 0.9945\n",
      "Epoch 00037: val_loss did not improve from 1.15969\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.0289 - acc: 0.9945 - val_loss: 1.8301 - val_acc: 0.6678\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0260 - acc: 0.9947\n",
      "Epoch 00038: val_loss did not improve from 1.15969\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.0260 - acc: 0.9946 - val_loss: 1.8028 - val_acc: 0.6825\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0307 - acc: 0.9939\n",
      "Epoch 00039: val_loss did not improve from 1.15969\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.0307 - acc: 0.9939 - val_loss: 1.7675 - val_acc: 0.6895\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0294 - acc: 0.9935\n",
      "Epoch 00040: val_loss did not improve from 1.15969\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.0294 - acc: 0.9935 - val_loss: 1.8430 - val_acc: 0.6825\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0353 - acc: 0.9927\n",
      "Epoch 00041: val_loss did not improve from 1.15969\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.0362 - acc: 0.9926 - val_loss: 1.8609 - val_acc: 0.6762\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0401 - acc: 0.9906\n",
      "Epoch 00042: val_loss did not improve from 1.15969\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.0402 - acc: 0.9905 - val_loss: 1.8556 - val_acc: 0.6839\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0315 - acc: 0.9923\n",
      "Epoch 00043: val_loss did not improve from 1.15969\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.0315 - acc: 0.9923 - val_loss: 1.8311 - val_acc: 0.6802\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0312 - acc: 0.9928\n",
      "Epoch 00044: val_loss did not improve from 1.15969\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.0315 - acc: 0.9927 - val_loss: 1.8087 - val_acc: 0.6823\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0327 - acc: 0.9925\n",
      "Epoch 00045: val_loss did not improve from 1.15969\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.0330 - acc: 0.9924 - val_loss: 1.7969 - val_acc: 0.6751\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0414 - acc: 0.9899\n",
      "Epoch 00046: val_loss did not improve from 1.15969\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.0416 - acc: 0.9899 - val_loss: 1.8098 - val_acc: 0.6823\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0259 - acc: 0.9948\n",
      "Epoch 00047: val_loss did not improve from 1.15969\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.0259 - acc: 0.9948 - val_loss: 1.8083 - val_acc: 0.6837\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0257 - acc: 0.9946\n",
      "Epoch 00048: val_loss did not improve from 1.15969\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.0257 - acc: 0.9946 - val_loss: 1.8106 - val_acc: 0.6804\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0165 - acc: 0.9978\n",
      "Epoch 00049: val_loss did not improve from 1.15969\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.0166 - acc: 0.9978 - val_loss: 1.8328 - val_acc: 0.6881\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0325 - acc: 0.9922\n",
      "Epoch 00050: val_loss did not improve from 1.15969\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.0325 - acc: 0.9922 - val_loss: 1.9201 - val_acc: 0.6883\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0224 - acc: 0.9958\n",
      "Epoch 00051: val_loss did not improve from 1.15969\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.0227 - acc: 0.9958 - val_loss: 1.8800 - val_acc: 0.6776\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0474 - acc: 0.9877\n",
      "Epoch 00052: val_loss did not improve from 1.15969\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.0474 - acc: 0.9877 - val_loss: 1.8204 - val_acc: 0.6916\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0267 - acc: 0.9939\n",
      "Epoch 00053: val_loss did not improve from 1.15969\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.0269 - acc: 0.9938 - val_loss: 1.9808 - val_acc: 0.6676\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0270 - acc: 0.9940\n",
      "Epoch 00054: val_loss did not improve from 1.15969\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.0271 - acc: 0.9940 - val_loss: 1.9093 - val_acc: 0.6804\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0260 - acc: 0.9940\n",
      "Epoch 00055: val_loss did not improve from 1.15969\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.0265 - acc: 0.9940 - val_loss: 1.9246 - val_acc: 0.6804\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_ch_32_BN_6_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 1.2420 - acc: 0.6478\n",
      "Loss: 1.2419819713505382 Accuracy: 0.64776736\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.1096 - acc: 0.3707\n",
      "Epoch 00001: val_loss improved from inf to 1.62609, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_BN_7_conv_checkpoint/001-1.6261.hdf5\n",
      "36805/36805 [==============================] - 148s 4ms/sample - loss: 2.1097 - acc: 0.3707 - val_loss: 1.6261 - val_acc: 0.4871\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3245 - acc: 0.5957\n",
      "Epoch 00002: val_loss improved from 1.62609 to 1.20341, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_BN_7_conv_checkpoint/002-1.2034.hdf5\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 1.3244 - acc: 0.5957 - val_loss: 1.2034 - val_acc: 0.6394\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0733 - acc: 0.6765\n",
      "Epoch 00003: val_loss did not improve from 1.20341\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 1.0733 - acc: 0.6765 - val_loss: 1.4595 - val_acc: 0.5837\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8929 - acc: 0.7324\n",
      "Epoch 00004: val_loss improved from 1.20341 to 1.00023, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_BN_7_conv_checkpoint/004-1.0002.hdf5\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.8931 - acc: 0.7324 - val_loss: 1.0002 - val_acc: 0.7095\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7662 - acc: 0.7717\n",
      "Epoch 00005: val_loss did not improve from 1.00023\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.7662 - acc: 0.7717 - val_loss: 1.0045 - val_acc: 0.7046\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6638 - acc: 0.8030\n",
      "Epoch 00006: val_loss improved from 1.00023 to 0.94686, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_BN_7_conv_checkpoint/006-0.9469.hdf5\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.6639 - acc: 0.8030 - val_loss: 0.9469 - val_acc: 0.7321\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5718 - acc: 0.8297\n",
      "Epoch 00007: val_loss improved from 0.94686 to 0.93713, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_BN_7_conv_checkpoint/007-0.9371.hdf5\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.5718 - acc: 0.8296 - val_loss: 0.9371 - val_acc: 0.7263\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4954 - acc: 0.8516\n",
      "Epoch 00008: val_loss did not improve from 0.93713\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.4957 - acc: 0.8516 - val_loss: 0.9620 - val_acc: 0.7233\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4235 - acc: 0.8761\n",
      "Epoch 00009: val_loss improved from 0.93713 to 0.90744, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_BN_7_conv_checkpoint/009-0.9074.hdf5\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.4234 - acc: 0.8761 - val_loss: 0.9074 - val_acc: 0.7389\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3617 - acc: 0.8957\n",
      "Epoch 00010: val_loss did not improve from 0.90744\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.3619 - acc: 0.8957 - val_loss: 0.9224 - val_acc: 0.7412\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3138 - acc: 0.9102\n",
      "Epoch 00011: val_loss did not improve from 0.90744\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.3138 - acc: 0.9101 - val_loss: 0.9406 - val_acc: 0.7442\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2605 - acc: 0.9284\n",
      "Epoch 00012: val_loss did not improve from 0.90744\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.2605 - acc: 0.9284 - val_loss: 1.1842 - val_acc: 0.6862\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2237 - acc: 0.9418\n",
      "Epoch 00013: val_loss did not improve from 0.90744\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.2238 - acc: 0.9417 - val_loss: 1.0709 - val_acc: 0.7223\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2011 - acc: 0.9491\n",
      "Epoch 00014: val_loss did not improve from 0.90744\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.2013 - acc: 0.9490 - val_loss: 1.0060 - val_acc: 0.7317\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1843 - acc: 0.9530\n",
      "Epoch 00015: val_loss did not improve from 0.90744\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.1846 - acc: 0.9529 - val_loss: 0.9749 - val_acc: 0.7442\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1535 - acc: 0.9630\n",
      "Epoch 00016: val_loss did not improve from 0.90744\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.1538 - acc: 0.9629 - val_loss: 1.0211 - val_acc: 0.7440\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1464 - acc: 0.9642\n",
      "Epoch 00017: val_loss did not improve from 0.90744\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.1464 - acc: 0.9642 - val_loss: 1.0167 - val_acc: 0.7538\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1097 - acc: 0.9769\n",
      "Epoch 00018: val_loss did not improve from 0.90744\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.1098 - acc: 0.9769 - val_loss: 1.0582 - val_acc: 0.7491\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1082 - acc: 0.9767\n",
      "Epoch 00019: val_loss did not improve from 0.90744\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.1084 - acc: 0.9767 - val_loss: 1.0629 - val_acc: 0.7438\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1048 - acc: 0.9768\n",
      "Epoch 00020: val_loss did not improve from 0.90744\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.1049 - acc: 0.9768 - val_loss: 1.1043 - val_acc: 0.7421\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0911 - acc: 0.9799\n",
      "Epoch 00021: val_loss did not improve from 0.90744\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0912 - acc: 0.9799 - val_loss: 1.1335 - val_acc: 0.7214\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0886 - acc: 0.9796\n",
      "Epoch 00022: val_loss did not improve from 0.90744\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0889 - acc: 0.9796 - val_loss: 1.0609 - val_acc: 0.7449\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0845 - acc: 0.9807\n",
      "Epoch 00023: val_loss did not improve from 0.90744\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0846 - acc: 0.9807 - val_loss: 1.0997 - val_acc: 0.7459\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0622 - acc: 0.9883\n",
      "Epoch 00024: val_loss did not improve from 0.90744\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0622 - acc: 0.9883 - val_loss: 1.1363 - val_acc: 0.7382\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0599 - acc: 0.9888\n",
      "Epoch 00025: val_loss did not improve from 0.90744\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0600 - acc: 0.9888 - val_loss: 1.1135 - val_acc: 0.7442\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0583 - acc: 0.9889\n",
      "Epoch 00026: val_loss did not improve from 0.90744\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0584 - acc: 0.9888 - val_loss: 1.1427 - val_acc: 0.7431\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0642 - acc: 0.9864\n",
      "Epoch 00027: val_loss did not improve from 0.90744\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0643 - acc: 0.9864 - val_loss: 1.1889 - val_acc: 0.7300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0596 - acc: 0.9878\n",
      "Epoch 00028: val_loss did not improve from 0.90744\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0597 - acc: 0.9877 - val_loss: 1.2132 - val_acc: 0.7442\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0643 - acc: 0.9857\n",
      "Epoch 00029: val_loss did not improve from 0.90744\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0643 - acc: 0.9857 - val_loss: 1.1132 - val_acc: 0.7477\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0500 - acc: 0.9897\n",
      "Epoch 00030: val_loss did not improve from 0.90744\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0500 - acc: 0.9897 - val_loss: 1.1867 - val_acc: 0.7424\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0481 - acc: 0.9899\n",
      "Epoch 00031: val_loss did not improve from 0.90744\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0483 - acc: 0.9898 - val_loss: 1.3912 - val_acc: 0.7086\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0756 - acc: 0.9811\n",
      "Epoch 00032: val_loss did not improve from 0.90744\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0756 - acc: 0.9811 - val_loss: 1.1994 - val_acc: 0.7379\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0356 - acc: 0.9939\n",
      "Epoch 00033: val_loss did not improve from 0.90744\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0356 - acc: 0.9939 - val_loss: 1.2170 - val_acc: 0.7456\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0369 - acc: 0.9942\n",
      "Epoch 00034: val_loss did not improve from 0.90744\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0369 - acc: 0.9942 - val_loss: 1.2103 - val_acc: 0.7529\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0440 - acc: 0.9918\n",
      "Epoch 00035: val_loss did not improve from 0.90744\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0441 - acc: 0.9918 - val_loss: 1.2919 - val_acc: 0.7228\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0512 - acc: 0.9882\n",
      "Epoch 00036: val_loss did not improve from 0.90744\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0514 - acc: 0.9882 - val_loss: 1.2254 - val_acc: 0.7424\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0492 - acc: 0.9890\n",
      "Epoch 00037: val_loss did not improve from 0.90744\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0492 - acc: 0.9890 - val_loss: 1.2704 - val_acc: 0.7400\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0328 - acc: 0.9939\n",
      "Epoch 00038: val_loss did not improve from 0.90744\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0331 - acc: 0.9939 - val_loss: 1.3602 - val_acc: 0.7319\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0410 - acc: 0.9906\n",
      "Epoch 00039: val_loss did not improve from 0.90744\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0412 - acc: 0.9906 - val_loss: 1.2719 - val_acc: 0.7489\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0448 - acc: 0.9903\n",
      "Epoch 00040: val_loss did not improve from 0.90744\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0451 - acc: 0.9902 - val_loss: 1.2505 - val_acc: 0.7554\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0595 - acc: 0.9851\n",
      "Epoch 00041: val_loss did not improve from 0.90744\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0598 - acc: 0.9851 - val_loss: 1.3350 - val_acc: 0.7277\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0388 - acc: 0.9919\n",
      "Epoch 00042: val_loss did not improve from 0.90744\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0389 - acc: 0.9918 - val_loss: 1.3120 - val_acc: 0.7377\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0343 - acc: 0.9925\n",
      "Epoch 00043: val_loss did not improve from 0.90744\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0345 - acc: 0.9925 - val_loss: 1.2744 - val_acc: 0.7477\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0332 - acc: 0.9934\n",
      "Epoch 00044: val_loss did not improve from 0.90744\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0332 - acc: 0.9934 - val_loss: 1.2741 - val_acc: 0.7512\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0215 - acc: 0.9971\n",
      "Epoch 00045: val_loss did not improve from 0.90744\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0215 - acc: 0.9970 - val_loss: 1.2277 - val_acc: 0.7563\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0285 - acc: 0.9945\n",
      "Epoch 00046: val_loss did not improve from 0.90744\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0286 - acc: 0.9945 - val_loss: 1.2839 - val_acc: 0.7519\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0379 - acc: 0.9910\n",
      "Epoch 00047: val_loss did not improve from 0.90744\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0379 - acc: 0.9910 - val_loss: 1.4451 - val_acc: 0.7326\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0268 - acc: 0.9952\n",
      "Epoch 00048: val_loss did not improve from 0.90744\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0269 - acc: 0.9952 - val_loss: 1.4125 - val_acc: 0.7375\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0441 - acc: 0.9892\n",
      "Epoch 00049: val_loss did not improve from 0.90744\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0443 - acc: 0.9892 - val_loss: 1.4804 - val_acc: 0.7270\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0420 - acc: 0.9902\n",
      "Epoch 00050: val_loss did not improve from 0.90744\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0422 - acc: 0.9902 - val_loss: 1.3432 - val_acc: 0.7482\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0447 - acc: 0.9885\n",
      "Epoch 00051: val_loss did not improve from 0.90744\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0448 - acc: 0.9885 - val_loss: 1.3215 - val_acc: 0.7494\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0260 - acc: 0.9946\n",
      "Epoch 00052: val_loss did not improve from 0.90744\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0260 - acc: 0.9946 - val_loss: 1.3137 - val_acc: 0.7536\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0211 - acc: 0.9969\n",
      "Epoch 00053: val_loss did not improve from 0.90744\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0211 - acc: 0.9968 - val_loss: 1.3718 - val_acc: 0.7489\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0353 - acc: 0.9919\n",
      "Epoch 00054: val_loss did not improve from 0.90744\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0353 - acc: 0.9918 - val_loss: 1.4425 - val_acc: 0.7296\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0285 - acc: 0.9945\n",
      "Epoch 00055: val_loss did not improve from 0.90744\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0286 - acc: 0.9944 - val_loss: 1.5728 - val_acc: 0.7088\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0382 - acc: 0.9904\n",
      "Epoch 00056: val_loss did not improve from 0.90744\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0382 - acc: 0.9904 - val_loss: 1.4481 - val_acc: 0.7377\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0226 - acc: 0.9961\n",
      "Epoch 00057: val_loss did not improve from 0.90744\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0228 - acc: 0.9961 - val_loss: 1.3612 - val_acc: 0.7480\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0294 - acc: 0.9938\n",
      "Epoch 00058: val_loss did not improve from 0.90744\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0294 - acc: 0.9938 - val_loss: 1.3274 - val_acc: 0.7510\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0333 - acc: 0.9919\n",
      "Epoch 00059: val_loss did not improve from 0.90744\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0333 - acc: 0.9919 - val_loss: 1.3908 - val_acc: 0.7512\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_ch_32_BN_7_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.9970 - acc: 0.7086\n",
      "Loss: 0.996963969544582 Accuracy: 0.7086189\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.2275 - acc: 0.3283\n",
      "Epoch 00001: val_loss improved from inf to 1.89366, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_BN_8_conv_checkpoint/001-1.8937.hdf5\n",
      "36805/36805 [==============================] - 157s 4ms/sample - loss: 2.2274 - acc: 0.3283 - val_loss: 1.8937 - val_acc: 0.3969\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4429 - acc: 0.5555\n",
      "Epoch 00002: val_loss improved from 1.89366 to 1.32207, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_BN_8_conv_checkpoint/002-1.3221.hdf5\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 1.4429 - acc: 0.5555 - val_loss: 1.3221 - val_acc: 0.5993\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1412 - acc: 0.6564\n",
      "Epoch 00003: val_loss improved from 1.32207 to 1.02636, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_BN_8_conv_checkpoint/003-1.0264.hdf5\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 1.1412 - acc: 0.6564 - val_loss: 1.0264 - val_acc: 0.7002\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9478 - acc: 0.7185\n",
      "Epoch 00004: val_loss improved from 1.02636 to 0.90324, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_BN_8_conv_checkpoint/004-0.9032.hdf5\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.9478 - acc: 0.7184 - val_loss: 0.9032 - val_acc: 0.7454\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8318 - acc: 0.7550\n",
      "Epoch 00005: val_loss improved from 0.90324 to 0.86700, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_BN_8_conv_checkpoint/005-0.8670.hdf5\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.8317 - acc: 0.7551 - val_loss: 0.8670 - val_acc: 0.7587\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7407 - acc: 0.7816\n",
      "Epoch 00006: val_loss improved from 0.86700 to 0.84048, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_BN_8_conv_checkpoint/006-0.8405.hdf5\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.7408 - acc: 0.7816 - val_loss: 0.8405 - val_acc: 0.7631\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6706 - acc: 0.8041\n",
      "Epoch 00007: val_loss improved from 0.84048 to 0.81440, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_BN_8_conv_checkpoint/007-0.8144.hdf5\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.6707 - acc: 0.8041 - val_loss: 0.8144 - val_acc: 0.7643\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6144 - acc: 0.8210\n",
      "Epoch 00008: val_loss did not improve from 0.81440\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.6145 - acc: 0.8209 - val_loss: 1.0162 - val_acc: 0.7188\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5513 - acc: 0.8397\n",
      "Epoch 00009: val_loss improved from 0.81440 to 0.74516, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_BN_8_conv_checkpoint/009-0.7452.hdf5\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.5515 - acc: 0.8396 - val_loss: 0.7452 - val_acc: 0.7906\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5107 - acc: 0.8506\n",
      "Epoch 00010: val_loss improved from 0.74516 to 0.68573, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_BN_8_conv_checkpoint/010-0.6857.hdf5\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.5107 - acc: 0.8506 - val_loss: 0.6857 - val_acc: 0.8022\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4611 - acc: 0.8658\n",
      "Epoch 00011: val_loss improved from 0.68573 to 0.67838, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_BN_8_conv_checkpoint/011-0.6784.hdf5\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.4611 - acc: 0.8659 - val_loss: 0.6784 - val_acc: 0.8185\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4216 - acc: 0.8795\n",
      "Epoch 00012: val_loss improved from 0.67838 to 0.65659, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_BN_8_conv_checkpoint/012-0.6566.hdf5\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.4216 - acc: 0.8795 - val_loss: 0.6566 - val_acc: 0.8130\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3851 - acc: 0.8899\n",
      "Epoch 00013: val_loss did not improve from 0.65659\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.3852 - acc: 0.8899 - val_loss: 0.7564 - val_acc: 0.8036\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3457 - acc: 0.9003\n",
      "Epoch 00014: val_loss did not improve from 0.65659\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.3458 - acc: 0.9003 - val_loss: 0.7027 - val_acc: 0.8113\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3144 - acc: 0.9091\n",
      "Epoch 00015: val_loss did not improve from 0.65659\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.3147 - acc: 0.9090 - val_loss: 0.7384 - val_acc: 0.8004\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2995 - acc: 0.9128\n",
      "Epoch 00016: val_loss improved from 0.65659 to 0.62021, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_BN_8_conv_checkpoint/016-0.6202.hdf5\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.2997 - acc: 0.9127 - val_loss: 0.6202 - val_acc: 0.8388\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2586 - acc: 0.9271\n",
      "Epoch 00017: val_loss did not improve from 0.62021\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.2586 - acc: 0.9271 - val_loss: 0.7274 - val_acc: 0.8125\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2364 - acc: 0.9332\n",
      "Epoch 00018: val_loss did not improve from 0.62021\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.2364 - acc: 0.9332 - val_loss: 0.7025 - val_acc: 0.8239\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2128 - acc: 0.9402\n",
      "Epoch 00019: val_loss did not improve from 0.62021\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.2129 - acc: 0.9402 - val_loss: 0.6836 - val_acc: 0.8279\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1953 - acc: 0.9453\n",
      "Epoch 00020: val_loss did not improve from 0.62021\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.1954 - acc: 0.9453 - val_loss: 0.6988 - val_acc: 0.8188\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1722 - acc: 0.9534\n",
      "Epoch 00021: val_loss did not improve from 0.62021\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.1724 - acc: 0.9533 - val_loss: 0.6411 - val_acc: 0.8355\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1734 - acc: 0.9524\n",
      "Epoch 00022: val_loss did not improve from 0.62021\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.1734 - acc: 0.9525 - val_loss: 0.6223 - val_acc: 0.8451\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1447 - acc: 0.9627\n",
      "Epoch 00023: val_loss did not improve from 0.62021\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.1447 - acc: 0.9627 - val_loss: 0.6326 - val_acc: 0.8365\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1323 - acc: 0.9659\n",
      "Epoch 00024: val_loss did not improve from 0.62021\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.1323 - acc: 0.9659 - val_loss: 0.6209 - val_acc: 0.8472\n",
      "Epoch 25/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1185 - acc: 0.9707\n",
      "Epoch 00025: val_loss did not improve from 0.62021\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.1186 - acc: 0.9706 - val_loss: 0.7407 - val_acc: 0.8181\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1229 - acc: 0.9669\n",
      "Epoch 00026: val_loss did not improve from 0.62021\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.1229 - acc: 0.9669 - val_loss: 0.8726 - val_acc: 0.7885\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1035 - acc: 0.9746\n",
      "Epoch 00027: val_loss did not improve from 0.62021\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.1038 - acc: 0.9746 - val_loss: 0.6843 - val_acc: 0.8409\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1093 - acc: 0.9728\n",
      "Epoch 00028: val_loss did not improve from 0.62021\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.1094 - acc: 0.9728 - val_loss: 0.7255 - val_acc: 0.8279\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0917 - acc: 0.9771\n",
      "Epoch 00029: val_loss did not improve from 0.62021\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.0917 - acc: 0.9771 - val_loss: 0.6847 - val_acc: 0.8330\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0839 - acc: 0.9799\n",
      "Epoch 00030: val_loss did not improve from 0.62021\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.0840 - acc: 0.9799 - val_loss: 0.7822 - val_acc: 0.8125\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1021 - acc: 0.9721\n",
      "Epoch 00031: val_loss did not improve from 0.62021\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.1022 - acc: 0.9721 - val_loss: 0.6774 - val_acc: 0.8428\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0839 - acc: 0.9796\n",
      "Epoch 00032: val_loss did not improve from 0.62021\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.0839 - acc: 0.9796 - val_loss: 0.6803 - val_acc: 0.8388\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0696 - acc: 0.9846\n",
      "Epoch 00033: val_loss did not improve from 0.62021\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.0698 - acc: 0.9846 - val_loss: 0.6850 - val_acc: 0.8397\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0800 - acc: 0.9795\n",
      "Epoch 00034: val_loss did not improve from 0.62021\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.0801 - acc: 0.9795 - val_loss: 0.6846 - val_acc: 0.8437\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0694 - acc: 0.9835\n",
      "Epoch 00035: val_loss did not improve from 0.62021\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.0696 - acc: 0.9834 - val_loss: 0.6984 - val_acc: 0.8432\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0673 - acc: 0.9840\n",
      "Epoch 00036: val_loss did not improve from 0.62021\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.0673 - acc: 0.9840 - val_loss: 0.7051 - val_acc: 0.8416\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0555 - acc: 0.9879\n",
      "Epoch 00037: val_loss did not improve from 0.62021\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.0555 - acc: 0.9879 - val_loss: 0.7320 - val_acc: 0.8414\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0544 - acc: 0.9884\n",
      "Epoch 00038: val_loss did not improve from 0.62021\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.0547 - acc: 0.9883 - val_loss: 0.7840 - val_acc: 0.8130\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0685 - acc: 0.9829\n",
      "Epoch 00039: val_loss did not improve from 0.62021\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.0688 - acc: 0.9828 - val_loss: 0.8317 - val_acc: 0.8211\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0640 - acc: 0.9843\n",
      "Epoch 00040: val_loss did not improve from 0.62021\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.0645 - acc: 0.9842 - val_loss: 0.7294 - val_acc: 0.8397\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0730 - acc: 0.9802\n",
      "Epoch 00041: val_loss did not improve from 0.62021\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.0731 - acc: 0.9801 - val_loss: 0.6999 - val_acc: 0.8474\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0571 - acc: 0.9862\n",
      "Epoch 00042: val_loss did not improve from 0.62021\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.0573 - acc: 0.9861 - val_loss: 0.7325 - val_acc: 0.8362\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0614 - acc: 0.9848\n",
      "Epoch 00043: val_loss did not improve from 0.62021\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.0614 - acc: 0.9848 - val_loss: 0.7069 - val_acc: 0.8456\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0383 - acc: 0.9929\n",
      "Epoch 00044: val_loss did not improve from 0.62021\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.0384 - acc: 0.9929 - val_loss: 0.7708 - val_acc: 0.8369\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0481 - acc: 0.9886\n",
      "Epoch 00045: val_loss did not improve from 0.62021\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.0483 - acc: 0.9886 - val_loss: 0.7262 - val_acc: 0.8407\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0513 - acc: 0.9881\n",
      "Epoch 00046: val_loss did not improve from 0.62021\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.0513 - acc: 0.9881 - val_loss: 0.7460 - val_acc: 0.8358\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0331 - acc: 0.9939\n",
      "Epoch 00047: val_loss did not improve from 0.62021\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.0331 - acc: 0.9939 - val_loss: 0.7821 - val_acc: 0.8337\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0496 - acc: 0.9884\n",
      "Epoch 00048: val_loss did not improve from 0.62021\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.0500 - acc: 0.9883 - val_loss: 0.7379 - val_acc: 0.8367\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0454 - acc: 0.9893\n",
      "Epoch 00049: val_loss did not improve from 0.62021\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.0455 - acc: 0.9893 - val_loss: 0.7847 - val_acc: 0.8358\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0454 - acc: 0.9890\n",
      "Epoch 00050: val_loss did not improve from 0.62021\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 0.0456 - acc: 0.9889 - val_loss: 0.7483 - val_acc: 0.8379\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0553 - acc: 0.9861\n",
      "Epoch 00051: val_loss did not improve from 0.62021\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.0553 - acc: 0.9861 - val_loss: 0.7454 - val_acc: 0.8474\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0465 - acc: 0.9892\n",
      "Epoch 00052: val_loss did not improve from 0.62021\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.0465 - acc: 0.9892 - val_loss: 0.7498 - val_acc: 0.8430\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0298 - acc: 0.9942\n",
      "Epoch 00053: val_loss did not improve from 0.62021\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.0298 - acc: 0.9942 - val_loss: 0.7642 - val_acc: 0.8407\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0283 - acc: 0.9942\n",
      "Epoch 00054: val_loss did not improve from 0.62021\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.0283 - acc: 0.9942 - val_loss: 0.7448 - val_acc: 0.8474\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0398 - acc: 0.9909\n",
      "Epoch 00055: val_loss did not improve from 0.62021\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.0400 - acc: 0.9908 - val_loss: 0.7956 - val_acc: 0.8341\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0506 - acc: 0.9869\n",
      "Epoch 00056: val_loss did not improve from 0.62021\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.0507 - acc: 0.9868 - val_loss: 0.7405 - val_acc: 0.8488\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0452 - acc: 0.9887\n",
      "Epoch 00057: val_loss did not improve from 0.62021\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.0455 - acc: 0.9886 - val_loss: 0.8028 - val_acc: 0.8339\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0413 - acc: 0.9899\n",
      "Epoch 00058: val_loss did not improve from 0.62021\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.0414 - acc: 0.9899 - val_loss: 0.7667 - val_acc: 0.8444\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0406 - acc: 0.9902\n",
      "Epoch 00059: val_loss did not improve from 0.62021\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.0406 - acc: 0.9902 - val_loss: 0.7186 - val_acc: 0.8537\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0255 - acc: 0.9951\n",
      "Epoch 00060: val_loss did not improve from 0.62021\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.0255 - acc: 0.9951 - val_loss: 0.7362 - val_acc: 0.8498\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0250 - acc: 0.9956\n",
      "Epoch 00061: val_loss did not improve from 0.62021\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.0253 - acc: 0.9955 - val_loss: 0.7852 - val_acc: 0.8360\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0527 - acc: 0.9858\n",
      "Epoch 00062: val_loss did not improve from 0.62021\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.0530 - acc: 0.9858 - val_loss: 0.7667 - val_acc: 0.8488\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0408 - acc: 0.9900\n",
      "Epoch 00063: val_loss did not improve from 0.62021\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.0409 - acc: 0.9899 - val_loss: 0.7839 - val_acc: 0.8458\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0372 - acc: 0.9909\n",
      "Epoch 00064: val_loss did not improve from 0.62021\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.0372 - acc: 0.9909 - val_loss: 0.7411 - val_acc: 0.8542\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0227 - acc: 0.9959\n",
      "Epoch 00065: val_loss did not improve from 0.62021\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.0227 - acc: 0.9959 - val_loss: 0.7834 - val_acc: 0.8465\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0274 - acc: 0.9940\n",
      "Epoch 00066: val_loss did not improve from 0.62021\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 0.0274 - acc: 0.9940 - val_loss: 0.7658 - val_acc: 0.8526\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_ch_32_BN_8_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 7s 1ms/sample - loss: 0.6865 - acc: 0.8123\n",
      "Loss: 0.6865143270631198 Accuracy: 0.81225336\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.2520 - acc: 0.3258\n",
      "Epoch 00001: val_loss improved from inf to 1.95942, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_BN_9_conv_checkpoint/001-1.9594.hdf5\n",
      "36805/36805 [==============================] - 181s 5ms/sample - loss: 2.2520 - acc: 0.3258 - val_loss: 1.9594 - val_acc: 0.3958\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3742 - acc: 0.5827\n",
      "Epoch 00002: val_loss improved from 1.95942 to 1.19885, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_BN_9_conv_checkpoint/002-1.1989.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 1.3745 - acc: 0.5827 - val_loss: 1.1989 - val_acc: 0.6473\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0613 - acc: 0.6840\n",
      "Epoch 00003: val_loss improved from 1.19885 to 1.01629, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_BN_9_conv_checkpoint/003-1.0163.hdf5\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 1.0614 - acc: 0.6840 - val_loss: 1.0163 - val_acc: 0.6990\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8724 - acc: 0.7435\n",
      "Epoch 00004: val_loss improved from 1.01629 to 0.83376, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_BN_9_conv_checkpoint/004-0.8338.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.8726 - acc: 0.7435 - val_loss: 0.8338 - val_acc: 0.7612\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7187 - acc: 0.7943\n",
      "Epoch 00005: val_loss improved from 0.83376 to 0.69724, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_BN_9_conv_checkpoint/005-0.6972.hdf5\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.7187 - acc: 0.7943 - val_loss: 0.6972 - val_acc: 0.8125\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6076 - acc: 0.8235\n",
      "Epoch 00006: val_loss did not improve from 0.69724\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.6077 - acc: 0.8234 - val_loss: 0.6982 - val_acc: 0.8074\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5178 - acc: 0.8510\n",
      "Epoch 00007: val_loss improved from 0.69724 to 0.52947, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_BN_9_conv_checkpoint/007-0.5295.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.5181 - acc: 0.8509 - val_loss: 0.5295 - val_acc: 0.8570\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4507 - acc: 0.8689\n",
      "Epoch 00008: val_loss did not improve from 0.52947\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.4506 - acc: 0.8689 - val_loss: 0.6241 - val_acc: 0.8302\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3916 - acc: 0.8857\n",
      "Epoch 00009: val_loss improved from 0.52947 to 0.48770, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_BN_9_conv_checkpoint/009-0.4877.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.3916 - acc: 0.8857 - val_loss: 0.4877 - val_acc: 0.8663\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3447 - acc: 0.8991\n",
      "Epoch 00010: val_loss improved from 0.48770 to 0.47333, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_BN_9_conv_checkpoint/010-0.4733.hdf5\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.3448 - acc: 0.8991 - val_loss: 0.4733 - val_acc: 0.8703\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3242 - acc: 0.9062\n",
      "Epoch 00011: val_loss did not improve from 0.47333\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.3242 - acc: 0.9062 - val_loss: 0.4803 - val_acc: 0.8595\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2755 - acc: 0.9208\n",
      "Epoch 00012: val_loss did not improve from 0.47333\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.2756 - acc: 0.9208 - val_loss: 0.5026 - val_acc: 0.8786\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2509 - acc: 0.9267\n",
      "Epoch 00013: val_loss improved from 0.47333 to 0.46944, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_BN_9_conv_checkpoint/013-0.4694.hdf5\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.2509 - acc: 0.9267 - val_loss: 0.4694 - val_acc: 0.8756\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2273 - acc: 0.9332\n",
      "Epoch 00014: val_loss improved from 0.46944 to 0.41312, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_BN_9_conv_checkpoint/014-0.4131.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.2275 - acc: 0.9332 - val_loss: 0.4131 - val_acc: 0.8908\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2147 - acc: 0.9373\n",
      "Epoch 00015: val_loss did not improve from 0.41312\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.2147 - acc: 0.9373 - val_loss: 0.4572 - val_acc: 0.8775\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1776 - acc: 0.9492\n",
      "Epoch 00016: val_loss did not improve from 0.41312\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1776 - acc: 0.9491 - val_loss: 0.4620 - val_acc: 0.8828\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1669 - acc: 0.9517\n",
      "Epoch 00017: val_loss did not improve from 0.41312\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1669 - acc: 0.9517 - val_loss: 0.4409 - val_acc: 0.8842\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1464 - acc: 0.9573\n",
      "Epoch 00018: val_loss did not improve from 0.41312\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1465 - acc: 0.9573 - val_loss: 0.4986 - val_acc: 0.8726\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1395 - acc: 0.9596\n",
      "Epoch 00019: val_loss improved from 0.41312 to 0.40682, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_BN_9_conv_checkpoint/019-0.4068.hdf5\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.1397 - acc: 0.9596 - val_loss: 0.4068 - val_acc: 0.8856\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1303 - acc: 0.9614\n",
      "Epoch 00020: val_loss did not improve from 0.40682\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1303 - acc: 0.9614 - val_loss: 0.5031 - val_acc: 0.8772\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1105 - acc: 0.9685\n",
      "Epoch 00021: val_loss did not improve from 0.40682\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.1104 - acc: 0.9685 - val_loss: 0.4616 - val_acc: 0.8912\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0942 - acc: 0.9738\n",
      "Epoch 00022: val_loss did not improve from 0.40682\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0942 - acc: 0.9738 - val_loss: 0.4342 - val_acc: 0.8924\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0914 - acc: 0.9754\n",
      "Epoch 00023: val_loss did not improve from 0.40682\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0914 - acc: 0.9754 - val_loss: 0.4504 - val_acc: 0.8884\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0821 - acc: 0.9773\n",
      "Epoch 00024: val_loss did not improve from 0.40682\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0822 - acc: 0.9773 - val_loss: 0.4390 - val_acc: 0.8917\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0924 - acc: 0.9736\n",
      "Epoch 00025: val_loss did not improve from 0.40682\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0925 - acc: 0.9735 - val_loss: 0.4301 - val_acc: 0.8935\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0918 - acc: 0.9735\n",
      "Epoch 00026: val_loss did not improve from 0.40682\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0918 - acc: 0.9735 - val_loss: 0.4303 - val_acc: 0.8954\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0635 - acc: 0.9837\n",
      "Epoch 00027: val_loss did not improve from 0.40682\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0638 - acc: 0.9836 - val_loss: 0.4675 - val_acc: 0.8894\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0872 - acc: 0.9751\n",
      "Epoch 00028: val_loss did not improve from 0.40682\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0873 - acc: 0.9751 - val_loss: 0.4733 - val_acc: 0.8891\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0626 - acc: 0.9835\n",
      "Epoch 00029: val_loss did not improve from 0.40682\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0627 - acc: 0.9834 - val_loss: 0.4085 - val_acc: 0.8998\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0600 - acc: 0.9843\n",
      "Epoch 00030: val_loss did not improve from 0.40682\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0601 - acc: 0.9843 - val_loss: 0.4681 - val_acc: 0.8912\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0622 - acc: 0.9821\n",
      "Epoch 00031: val_loss did not improve from 0.40682\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0624 - acc: 0.9821 - val_loss: 0.4445 - val_acc: 0.8977\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0610 - acc: 0.9838\n",
      "Epoch 00032: val_loss did not improve from 0.40682\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0610 - acc: 0.9838 - val_loss: 0.4315 - val_acc: 0.9033\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0452 - acc: 0.9893\n",
      "Epoch 00033: val_loss did not improve from 0.40682\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0454 - acc: 0.9893 - val_loss: 0.4199 - val_acc: 0.9005\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0561 - acc: 0.9840\n",
      "Epoch 00034: val_loss did not improve from 0.40682\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0562 - acc: 0.9840 - val_loss: 0.5021 - val_acc: 0.8828\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0541 - acc: 0.9861\n",
      "Epoch 00035: val_loss did not improve from 0.40682\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0542 - acc: 0.9861 - val_loss: 0.4592 - val_acc: 0.8921\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0397 - acc: 0.9909\n",
      "Epoch 00036: val_loss did not improve from 0.40682\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0398 - acc: 0.9909 - val_loss: 0.5358 - val_acc: 0.8768\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0375 - acc: 0.9916\n",
      "Epoch 00037: val_loss did not improve from 0.40682\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0375 - acc: 0.9916 - val_loss: 0.4465 - val_acc: 0.8970\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0373 - acc: 0.9912\n",
      "Epoch 00038: val_loss did not improve from 0.40682\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0376 - acc: 0.9912 - val_loss: 0.5109 - val_acc: 0.8877\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0542 - acc: 0.9839\n",
      "Epoch 00039: val_loss did not improve from 0.40682\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0542 - acc: 0.9839 - val_loss: 0.4613 - val_acc: 0.8982\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0330 - acc: 0.9925\n",
      "Epoch 00040: val_loss did not improve from 0.40682\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0333 - acc: 0.9925 - val_loss: 0.5391 - val_acc: 0.8828\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0485 - acc: 0.9874\n",
      "Epoch 00041: val_loss did not improve from 0.40682\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0487 - acc: 0.9873 - val_loss: 0.4408 - val_acc: 0.8998\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0413 - acc: 0.9892\n",
      "Epoch 00042: val_loss did not improve from 0.40682\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0413 - acc: 0.9892 - val_loss: 0.4497 - val_acc: 0.9047\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0306 - acc: 0.9922\n",
      "Epoch 00043: val_loss did not improve from 0.40682\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0310 - acc: 0.9921 - val_loss: 0.5505 - val_acc: 0.8919\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0516 - acc: 0.9858\n",
      "Epoch 00044: val_loss did not improve from 0.40682\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0519 - acc: 0.9857 - val_loss: 0.4654 - val_acc: 0.8989\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0384 - acc: 0.9902\n",
      "Epoch 00045: val_loss did not improve from 0.40682\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0384 - acc: 0.9902 - val_loss: 0.4673 - val_acc: 0.8931\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0252 - acc: 0.9949\n",
      "Epoch 00046: val_loss did not improve from 0.40682\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0252 - acc: 0.9949 - val_loss: 0.4315 - val_acc: 0.9087\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0224 - acc: 0.9955\n",
      "Epoch 00047: val_loss did not improve from 0.40682\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0225 - acc: 0.9954 - val_loss: 0.5272 - val_acc: 0.8968\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0461 - acc: 0.9867\n",
      "Epoch 00048: val_loss did not improve from 0.40682\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0463 - acc: 0.9867 - val_loss: 0.4570 - val_acc: 0.9010\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0440 - acc: 0.9878\n",
      "Epoch 00049: val_loss did not improve from 0.40682\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0440 - acc: 0.9878 - val_loss: 0.4815 - val_acc: 0.9015\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0274 - acc: 0.9938\n",
      "Epoch 00050: val_loss did not improve from 0.40682\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0277 - acc: 0.9938 - val_loss: 0.4383 - val_acc: 0.9108\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0376 - acc: 0.9900\n",
      "Epoch 00051: val_loss did not improve from 0.40682\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0377 - acc: 0.9900 - val_loss: 0.4882 - val_acc: 0.9029\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0238 - acc: 0.9949\n",
      "Epoch 00052: val_loss did not improve from 0.40682\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0238 - acc: 0.9949 - val_loss: 0.4827 - val_acc: 0.9033\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0281 - acc: 0.9930\n",
      "Epoch 00053: val_loss did not improve from 0.40682\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0281 - acc: 0.9930 - val_loss: 0.4563 - val_acc: 0.9071\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0201 - acc: 0.9955\n",
      "Epoch 00054: val_loss did not improve from 0.40682\n",
      "36805/36805 [==============================] - 107s 3ms/sample - loss: 0.0201 - acc: 0.9955 - val_loss: 0.4399 - val_acc: 0.9110\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0328 - acc: 0.9913\n",
      "Epoch 00055: val_loss did not improve from 0.40682\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0328 - acc: 0.9913 - val_loss: 0.5096 - val_acc: 0.8905\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0231 - acc: 0.9948\n",
      "Epoch 00056: val_loss did not improve from 0.40682\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0232 - acc: 0.9948 - val_loss: 0.4878 - val_acc: 0.8998\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0339 - acc: 0.9907\n",
      "Epoch 00057: val_loss did not improve from 0.40682\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0341 - acc: 0.9907 - val_loss: 0.4912 - val_acc: 0.9022\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0401 - acc: 0.9890\n",
      "Epoch 00058: val_loss did not improve from 0.40682\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0405 - acc: 0.9890 - val_loss: 0.5070 - val_acc: 0.8994\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0437 - acc: 0.9880\n",
      "Epoch 00059: val_loss did not improve from 0.40682\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0437 - acc: 0.9880 - val_loss: 0.4852 - val_acc: 0.9008\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0173 - acc: 0.9965\n",
      "Epoch 00060: val_loss did not improve from 0.40682\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0173 - acc: 0.9965 - val_loss: 0.4777 - val_acc: 0.9057\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0161 - acc: 0.9969\n",
      "Epoch 00061: val_loss did not improve from 0.40682\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0161 - acc: 0.9968 - val_loss: 0.4756 - val_acc: 0.9024\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0304 - acc: 0.9922\n",
      "Epoch 00062: val_loss did not improve from 0.40682\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0304 - acc: 0.9922 - val_loss: 0.4933 - val_acc: 0.9012\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0160 - acc: 0.9970\n",
      "Epoch 00063: val_loss did not improve from 0.40682\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0160 - acc: 0.9970 - val_loss: 0.4767 - val_acc: 0.9019\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0221 - acc: 0.9947\n",
      "Epoch 00064: val_loss did not improve from 0.40682\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0225 - acc: 0.9946 - val_loss: 0.5555 - val_acc: 0.8954\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0432 - acc: 0.9875\n",
      "Epoch 00065: val_loss did not improve from 0.40682\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0433 - acc: 0.9875 - val_loss: 0.4629 - val_acc: 0.9087\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0231 - acc: 0.9945\n",
      "Epoch 00066: val_loss did not improve from 0.40682\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0231 - acc: 0.9945 - val_loss: 0.4766 - val_acc: 0.9031\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0206 - acc: 0.9952\n",
      "Epoch 00067: val_loss did not improve from 0.40682\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0207 - acc: 0.9952 - val_loss: 0.4626 - val_acc: 0.9054\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0222 - acc: 0.9946\n",
      "Epoch 00068: val_loss did not improve from 0.40682\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0225 - acc: 0.9945 - val_loss: 0.5057 - val_acc: 0.8975\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0351 - acc: 0.9898\n",
      "Epoch 00069: val_loss did not improve from 0.40682\n",
      "36805/36805 [==============================] - 108s 3ms/sample - loss: 0.0353 - acc: 0.9897 - val_loss: 0.5338 - val_acc: 0.8919\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_ch_32_BN_9_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 7s 1ms/sample - loss: 0.4907 - acc: 0.8613\n",
      "Loss: 0.4906679555139314 Accuracy: 0.86126685\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.3549 - acc: 0.2873\n",
      "Epoch 00001: val_loss improved from inf to 1.90977, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_BN_10_conv_checkpoint/001-1.9098.hdf5\n",
      "36805/36805 [==============================] - 203s 6ms/sample - loss: 2.3548 - acc: 0.2874 - val_loss: 1.9098 - val_acc: 0.4069\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4207 - acc: 0.5696\n",
      "Epoch 00002: val_loss improved from 1.90977 to 1.23648, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_BN_10_conv_checkpoint/002-1.2365.hdf5\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 1.4206 - acc: 0.5696 - val_loss: 1.2365 - val_acc: 0.6252\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0637 - acc: 0.6824\n",
      "Epoch 00003: val_loss improved from 1.23648 to 0.92698, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_BN_10_conv_checkpoint/003-0.9270.hdf5\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 1.0639 - acc: 0.6824 - val_loss: 0.9270 - val_acc: 0.7396\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8189 - acc: 0.7624\n",
      "Epoch 00004: val_loss improved from 0.92698 to 0.70883, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_BN_10_conv_checkpoint/004-0.7088.hdf5\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.8189 - acc: 0.7624 - val_loss: 0.7088 - val_acc: 0.7927\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6464 - acc: 0.8115\n",
      "Epoch 00005: val_loss improved from 0.70883 to 0.62100, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_BN_10_conv_checkpoint/005-0.6210.hdf5\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.6464 - acc: 0.8116 - val_loss: 0.6210 - val_acc: 0.8174\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5328 - acc: 0.8457\n",
      "Epoch 00006: val_loss improved from 0.62100 to 0.57229, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_BN_10_conv_checkpoint/006-0.5723.hdf5\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.5330 - acc: 0.8456 - val_loss: 0.5723 - val_acc: 0.8383\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4488 - acc: 0.8679\n",
      "Epoch 00007: val_loss improved from 0.57229 to 0.49251, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_BN_10_conv_checkpoint/007-0.4925.hdf5\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.4489 - acc: 0.8678 - val_loss: 0.4925 - val_acc: 0.8621\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3886 - acc: 0.8860\n",
      "Epoch 00008: val_loss improved from 0.49251 to 0.42489, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_BN_10_conv_checkpoint/008-0.4249.hdf5\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.3887 - acc: 0.8859 - val_loss: 0.4249 - val_acc: 0.8730\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3452 - acc: 0.8985\n",
      "Epoch 00009: val_loss improved from 0.42489 to 0.42489, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_BN_10_conv_checkpoint/009-0.4249.hdf5\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.3453 - acc: 0.8984 - val_loss: 0.4249 - val_acc: 0.8763\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3019 - acc: 0.9107\n",
      "Epoch 00010: val_loss did not improve from 0.42489\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.3020 - acc: 0.9107 - val_loss: 0.4293 - val_acc: 0.8803\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2720 - acc: 0.9191\n",
      "Epoch 00011: val_loss improved from 0.42489 to 0.37634, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_BN_10_conv_checkpoint/011-0.3763.hdf5\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.2720 - acc: 0.9191 - val_loss: 0.3763 - val_acc: 0.9022\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2463 - acc: 0.9273\n",
      "Epoch 00012: val_loss improved from 0.37634 to 0.33936, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_BN_10_conv_checkpoint/012-0.3394.hdf5\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.2464 - acc: 0.9272 - val_loss: 0.3394 - val_acc: 0.9045\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2163 - acc: 0.9357\n",
      "Epoch 00013: val_loss did not improve from 0.33936\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.2167 - acc: 0.9356 - val_loss: 0.3554 - val_acc: 0.9019\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2035 - acc: 0.9388\n",
      "Epoch 00014: val_loss improved from 0.33936 to 0.32256, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_BN_10_conv_checkpoint/014-0.3226.hdf5\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.2036 - acc: 0.9387 - val_loss: 0.3226 - val_acc: 0.9068\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1756 - acc: 0.9477\n",
      "Epoch 00015: val_loss did not improve from 0.32256\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.1758 - acc: 0.9477 - val_loss: 0.3279 - val_acc: 0.9085\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1714 - acc: 0.9481\n",
      "Epoch 00016: val_loss improved from 0.32256 to 0.30000, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_BN_10_conv_checkpoint/016-0.3000.hdf5\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.1714 - acc: 0.9481 - val_loss: 0.3000 - val_acc: 0.9227\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1467 - acc: 0.9567\n",
      "Epoch 00017: val_loss did not improve from 0.30000\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.1467 - acc: 0.9567 - val_loss: 0.3212 - val_acc: 0.9106\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1350 - acc: 0.9590\n",
      "Epoch 00018: val_loss did not improve from 0.30000\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.1351 - acc: 0.9590 - val_loss: 0.3399 - val_acc: 0.9043\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1311 - acc: 0.9602\n",
      "Epoch 00019: val_loss did not improve from 0.30000\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.1311 - acc: 0.9602 - val_loss: 0.3224 - val_acc: 0.9054\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1071 - acc: 0.9695\n",
      "Epoch 00020: val_loss improved from 0.30000 to 0.29270, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_BN_10_conv_checkpoint/020-0.2927.hdf5\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.1070 - acc: 0.9695 - val_loss: 0.2927 - val_acc: 0.9203\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0979 - acc: 0.9719\n",
      "Epoch 00021: val_loss did not improve from 0.29270\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0979 - acc: 0.9719 - val_loss: 0.3218 - val_acc: 0.9171\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0991 - acc: 0.9717\n",
      "Epoch 00022: val_loss did not improve from 0.29270\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0993 - acc: 0.9716 - val_loss: 0.3851 - val_acc: 0.9026\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0990 - acc: 0.9700\n",
      "Epoch 00023: val_loss did not improve from 0.29270\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0991 - acc: 0.9700 - val_loss: 0.3147 - val_acc: 0.9164\n",
      "Epoch 24/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0751 - acc: 0.9793\n",
      "Epoch 00024: val_loss improved from 0.29270 to 0.27612, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_BN_10_conv_checkpoint/024-0.2761.hdf5\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0752 - acc: 0.9792 - val_loss: 0.2761 - val_acc: 0.9278\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0757 - acc: 0.9778\n",
      "Epoch 00025: val_loss improved from 0.27612 to 0.27268, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_BN_10_conv_checkpoint/025-0.2727.hdf5\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0759 - acc: 0.9777 - val_loss: 0.2727 - val_acc: 0.9292\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0848 - acc: 0.9752\n",
      "Epoch 00026: val_loss improved from 0.27268 to 0.26963, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_BN_10_conv_checkpoint/026-0.2696.hdf5\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0849 - acc: 0.9752 - val_loss: 0.2696 - val_acc: 0.9271\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0678 - acc: 0.9807\n",
      "Epoch 00027: val_loss did not improve from 0.26963\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0678 - acc: 0.9807 - val_loss: 0.3047 - val_acc: 0.9189\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0546 - acc: 0.9857\n",
      "Epoch 00028: val_loss did not improve from 0.26963\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0548 - acc: 0.9856 - val_loss: 0.2842 - val_acc: 0.9255\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0687 - acc: 0.9800\n",
      "Epoch 00029: val_loss did not improve from 0.26963\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0688 - acc: 0.9799 - val_loss: 0.2967 - val_acc: 0.9241\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0596 - acc: 0.9825\n",
      "Epoch 00030: val_loss did not improve from 0.26963\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0596 - acc: 0.9824 - val_loss: 0.3003 - val_acc: 0.9229\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0565 - acc: 0.9840\n",
      "Epoch 00031: val_loss did not improve from 0.26963\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0565 - acc: 0.9840 - val_loss: 0.2943 - val_acc: 0.9278\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0466 - acc: 0.9870\n",
      "Epoch 00032: val_loss did not improve from 0.26963\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0466 - acc: 0.9870 - val_loss: 0.3012 - val_acc: 0.9257\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0425 - acc: 0.9886\n",
      "Epoch 00033: val_loss did not improve from 0.26963\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0425 - acc: 0.9886 - val_loss: 0.2891 - val_acc: 0.9290\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0409 - acc: 0.9894\n",
      "Epoch 00034: val_loss did not improve from 0.26963\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0409 - acc: 0.9894 - val_loss: 0.3360 - val_acc: 0.9164\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0428 - acc: 0.9887\n",
      "Epoch 00035: val_loss did not improve from 0.26963\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0431 - acc: 0.9886 - val_loss: 0.3157 - val_acc: 0.9285\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0599 - acc: 0.9823\n",
      "Epoch 00036: val_loss did not improve from 0.26963\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0601 - acc: 0.9823 - val_loss: 0.2966 - val_acc: 0.9208\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0517 - acc: 0.9858\n",
      "Epoch 00037: val_loss did not improve from 0.26963\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0519 - acc: 0.9858 - val_loss: 0.3102 - val_acc: 0.9283\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0456 - acc: 0.9866\n",
      "Epoch 00038: val_loss did not improve from 0.26963\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0457 - acc: 0.9866 - val_loss: 0.3003 - val_acc: 0.9262\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0409 - acc: 0.9889\n",
      "Epoch 00039: val_loss did not improve from 0.26963\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0409 - acc: 0.9889 - val_loss: 0.2786 - val_acc: 0.9329\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0245 - acc: 0.9945\n",
      "Epoch 00040: val_loss did not improve from 0.26963\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0246 - acc: 0.9945 - val_loss: 0.3011 - val_acc: 0.9269\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0381 - acc: 0.9892\n",
      "Epoch 00041: val_loss did not improve from 0.26963\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0382 - acc: 0.9892 - val_loss: 0.2875 - val_acc: 0.9308\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0400 - acc: 0.9887\n",
      "Epoch 00042: val_loss did not improve from 0.26963\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0400 - acc: 0.9887 - val_loss: 0.3369 - val_acc: 0.9192\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0229 - acc: 0.9946\n",
      "Epoch 00043: val_loss did not improve from 0.26963\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0229 - acc: 0.9946 - val_loss: 0.3169 - val_acc: 0.9280\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0339 - acc: 0.9905\n",
      "Epoch 00044: val_loss did not improve from 0.26963\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0340 - acc: 0.9905 - val_loss: 0.2900 - val_acc: 0.9290\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0360 - acc: 0.9892\n",
      "Epoch 00045: val_loss did not improve from 0.26963\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0361 - acc: 0.9892 - val_loss: 0.2733 - val_acc: 0.9371\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0305 - acc: 0.9917\n",
      "Epoch 00046: val_loss did not improve from 0.26963\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0306 - acc: 0.9917 - val_loss: 0.3116 - val_acc: 0.9287\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0346 - acc: 0.9903\n",
      "Epoch 00047: val_loss did not improve from 0.26963\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0346 - acc: 0.9903 - val_loss: 0.2839 - val_acc: 0.9334\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0183 - acc: 0.9959\n",
      "Epoch 00048: val_loss did not improve from 0.26963\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0184 - acc: 0.9958 - val_loss: 0.2762 - val_acc: 0.9378\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0319 - acc: 0.9912\n",
      "Epoch 00049: val_loss did not improve from 0.26963\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0322 - acc: 0.9912 - val_loss: 0.2982 - val_acc: 0.9317\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0435 - acc: 0.9872\n",
      "Epoch 00050: val_loss did not improve from 0.26963\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0438 - acc: 0.9872 - val_loss: 0.2943 - val_acc: 0.9334\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0277 - acc: 0.9924\n",
      "Epoch 00051: val_loss improved from 0.26963 to 0.26376, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_BN_10_conv_checkpoint/051-0.2638.hdf5\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0280 - acc: 0.9924 - val_loss: 0.2638 - val_acc: 0.9364\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0249 - acc: 0.9930\n",
      "Epoch 00052: val_loss did not improve from 0.26376\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0249 - acc: 0.9930 - val_loss: 0.2757 - val_acc: 0.9329\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0174 - acc: 0.9961\n",
      "Epoch 00053: val_loss did not improve from 0.26376\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0174 - acc: 0.9961 - val_loss: 0.2787 - val_acc: 0.9380\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0175 - acc: 0.9957\n",
      "Epoch 00054: val_loss improved from 0.26376 to 0.25469, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_BN_10_conv_checkpoint/054-0.2547.hdf5\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0176 - acc: 0.9957 - val_loss: 0.2547 - val_acc: 0.9380\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0311 - acc: 0.9911\n",
      "Epoch 00055: val_loss did not improve from 0.25469\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0313 - acc: 0.9911 - val_loss: 0.3383 - val_acc: 0.9173\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0318 - acc: 0.9911\n",
      "Epoch 00056: val_loss did not improve from 0.25469\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0318 - acc: 0.9911 - val_loss: 0.3087 - val_acc: 0.9350\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0160 - acc: 0.9964\n",
      "Epoch 00057: val_loss did not improve from 0.25469\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0160 - acc: 0.9963 - val_loss: 0.2791 - val_acc: 0.9378\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0191 - acc: 0.9952\n",
      "Epoch 00058: val_loss did not improve from 0.25469\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0192 - acc: 0.9951 - val_loss: 0.3143 - val_acc: 0.9301\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0275 - acc: 0.9921\n",
      "Epoch 00059: val_loss did not improve from 0.25469\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0275 - acc: 0.9921 - val_loss: 0.3113 - val_acc: 0.9294\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0126 - acc: 0.9970\n",
      "Epoch 00060: val_loss did not improve from 0.25469\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0128 - acc: 0.9969 - val_loss: 0.3966 - val_acc: 0.9092\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0371 - acc: 0.9889\n",
      "Epoch 00061: val_loss did not improve from 0.25469\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0372 - acc: 0.9888 - val_loss: 0.2979 - val_acc: 0.9278\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0169 - acc: 0.9957\n",
      "Epoch 00062: val_loss did not improve from 0.25469\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0169 - acc: 0.9956 - val_loss: 0.2787 - val_acc: 0.9408\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0199 - acc: 0.9946\n",
      "Epoch 00063: val_loss did not improve from 0.25469\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0200 - acc: 0.9946 - val_loss: 0.2795 - val_acc: 0.9411\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0225 - acc: 0.9935\n",
      "Epoch 00064: val_loss did not improve from 0.25469\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0228 - acc: 0.9934 - val_loss: 0.2965 - val_acc: 0.9366\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0310 - acc: 0.9906\n",
      "Epoch 00065: val_loss did not improve from 0.25469\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0312 - acc: 0.9906 - val_loss: 0.2907 - val_acc: 0.9329\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0262 - acc: 0.9926\n",
      "Epoch 00066: val_loss improved from 0.25469 to 0.24810, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_BN_10_conv_checkpoint/066-0.2481.hdf5\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0262 - acc: 0.9926 - val_loss: 0.2481 - val_acc: 0.9413\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0093 - acc: 0.9978\n",
      "Epoch 00067: val_loss did not improve from 0.24810\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0094 - acc: 0.9977 - val_loss: 0.2630 - val_acc: 0.9464\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0264 - acc: 0.9920\n",
      "Epoch 00068: val_loss did not improve from 0.24810\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0264 - acc: 0.9920 - val_loss: 0.2736 - val_acc: 0.9406\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0112 - acc: 0.9972\n",
      "Epoch 00069: val_loss did not improve from 0.24810\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0115 - acc: 0.9971 - val_loss: 0.2550 - val_acc: 0.9418\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0261 - acc: 0.9921\n",
      "Epoch 00070: val_loss did not improve from 0.24810\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0261 - acc: 0.9921 - val_loss: 0.2647 - val_acc: 0.9385\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0122 - acc: 0.9971\n",
      "Epoch 00071: val_loss did not improve from 0.24810\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0123 - acc: 0.9971 - val_loss: 0.2613 - val_acc: 0.9411\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0214 - acc: 0.9940\n",
      "Epoch 00072: val_loss did not improve from 0.24810\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0214 - acc: 0.9940 - val_loss: 0.2945 - val_acc: 0.9299\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0110 - acc: 0.9971\n",
      "Epoch 00073: val_loss did not improve from 0.24810\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0110 - acc: 0.9971 - val_loss: 0.2850 - val_acc: 0.9383\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0250 - acc: 0.9930\n",
      "Epoch 00074: val_loss did not improve from 0.24810\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0250 - acc: 0.9930 - val_loss: 0.2644 - val_acc: 0.9418\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0119 - acc: 0.9973\n",
      "Epoch 00075: val_loss did not improve from 0.24810\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0121 - acc: 0.9972 - val_loss: 0.3006 - val_acc: 0.9338\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0250 - acc: 0.9924\n",
      "Epoch 00076: val_loss did not improve from 0.24810\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0253 - acc: 0.9924 - val_loss: 0.3061 - val_acc: 0.9371\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0333 - acc: 0.9909\n",
      "Epoch 00077: val_loss did not improve from 0.24810\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0335 - acc: 0.9908 - val_loss: 0.2775 - val_acc: 0.9385\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0213 - acc: 0.9935\n",
      "Epoch 00078: val_loss did not improve from 0.24810\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0214 - acc: 0.9935 - val_loss: 0.2994 - val_acc: 0.9329\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0146 - acc: 0.9960\n",
      "Epoch 00079: val_loss did not improve from 0.24810\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0147 - acc: 0.9960 - val_loss: 0.2793 - val_acc: 0.9411\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0224 - acc: 0.9936\n",
      "Epoch 00080: val_loss did not improve from 0.24810\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0227 - acc: 0.9936 - val_loss: 0.2717 - val_acc: 0.9422\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0280 - acc: 0.9920\n",
      "Epoch 00081: val_loss did not improve from 0.24810\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0280 - acc: 0.9920 - val_loss: 0.2654 - val_acc: 0.9441\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0095 - acc: 0.9977\n",
      "Epoch 00082: val_loss did not improve from 0.24810\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0095 - acc: 0.9976 - val_loss: 0.2695 - val_acc: 0.9408\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0163 - acc: 0.9955\n",
      "Epoch 00083: val_loss did not improve from 0.24810\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0163 - acc: 0.9955 - val_loss: 0.2598 - val_acc: 0.9462\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0157 - acc: 0.9956\n",
      "Epoch 00084: val_loss did not improve from 0.24810\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0157 - acc: 0.9956 - val_loss: 0.2482 - val_acc: 0.9448\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0124 - acc: 0.9968\n",
      "Epoch 00085: val_loss did not improve from 0.24810\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0124 - acc: 0.9968 - val_loss: 0.2966 - val_acc: 0.9350\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0070 - acc: 0.9985\n",
      "Epoch 00086: val_loss did not improve from 0.24810\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0070 - acc: 0.9985 - val_loss: 0.2975 - val_acc: 0.9338\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0085 - acc: 0.9977\n",
      "Epoch 00087: val_loss did not improve from 0.24810\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0085 - acc: 0.9977 - val_loss: 0.3022 - val_acc: 0.9348\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0138 - acc: 0.9959\n",
      "Epoch 00088: val_loss did not improve from 0.24810\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0140 - acc: 0.9959 - val_loss: 0.3140 - val_acc: 0.9357\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0311 - acc: 0.9898\n",
      "Epoch 00089: val_loss did not improve from 0.24810\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0311 - acc: 0.9898 - val_loss: 0.2722 - val_acc: 0.9404\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0166 - acc: 0.9954\n",
      "Epoch 00090: val_loss did not improve from 0.24810\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0166 - acc: 0.9954 - val_loss: 0.2594 - val_acc: 0.9404\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0074 - acc: 0.9983\n",
      "Epoch 00091: val_loss did not improve from 0.24810\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0076 - acc: 0.9982 - val_loss: 0.2710 - val_acc: 0.9434\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0220 - acc: 0.9933\n",
      "Epoch 00092: val_loss did not improve from 0.24810\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0220 - acc: 0.9933 - val_loss: 0.2578 - val_acc: 0.9432\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0073 - acc: 0.9983\n",
      "Epoch 00093: val_loss did not improve from 0.24810\n",
      "36805/36805 [==============================] - 109s 3ms/sample - loss: 0.0074 - acc: 0.9983 - val_loss: 0.2792 - val_acc: 0.9378\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0176 - acc: 0.9949\n",
      "Epoch 00094: val_loss did not improve from 0.24810\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0176 - acc: 0.9949 - val_loss: 0.2565 - val_acc: 0.9457\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0066 - acc: 0.9987\n",
      "Epoch 00095: val_loss did not improve from 0.24810\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0067 - acc: 0.9986 - val_loss: 0.2900 - val_acc: 0.9385\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0240 - acc: 0.9923\n",
      "Epoch 00096: val_loss did not improve from 0.24810\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0243 - acc: 0.9923 - val_loss: 0.2719 - val_acc: 0.9387\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0223 - acc: 0.9927\n",
      "Epoch 00097: val_loss did not improve from 0.24810\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0224 - acc: 0.9927 - val_loss: 0.2775 - val_acc: 0.9392\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0170 - acc: 0.9950\n",
      "Epoch 00098: val_loss did not improve from 0.24810\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0171 - acc: 0.9950 - val_loss: 0.2668 - val_acc: 0.9399\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0126 - acc: 0.9967\n",
      "Epoch 00099: val_loss did not improve from 0.24810\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0126 - acc: 0.9967 - val_loss: 0.2524 - val_acc: 0.9450\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0054 - acc: 0.9988\n",
      "Epoch 00100: val_loss did not improve from 0.24810\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0054 - acc: 0.9988 - val_loss: 0.2697 - val_acc: 0.9457\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0154 - acc: 0.9953\n",
      "Epoch 00101: val_loss did not improve from 0.24810\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0154 - acc: 0.9953 - val_loss: 0.2693 - val_acc: 0.9432\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0091 - acc: 0.9974\n",
      "Epoch 00102: val_loss did not improve from 0.24810\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0091 - acc: 0.9974 - val_loss: 0.2701 - val_acc: 0.9390\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0070 - acc: 0.9983\n",
      "Epoch 00103: val_loss did not improve from 0.24810\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0070 - acc: 0.9983 - val_loss: 0.3353 - val_acc: 0.9336\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0147 - acc: 0.9957\n",
      "Epoch 00104: val_loss did not improve from 0.24810\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0149 - acc: 0.9956 - val_loss: 0.2679 - val_acc: 0.9429\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0155 - acc: 0.9955\n",
      "Epoch 00105: val_loss did not improve from 0.24810\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0157 - acc: 0.9954 - val_loss: 0.2878 - val_acc: 0.9394\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0247 - acc: 0.9927\n",
      "Epoch 00106: val_loss did not improve from 0.24810\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0250 - acc: 0.9927 - val_loss: 0.3191 - val_acc: 0.9336\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0148 - acc: 0.9953\n",
      "Epoch 00107: val_loss did not improve from 0.24810\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0148 - acc: 0.9953 - val_loss: 0.2887 - val_acc: 0.9453\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0087 - acc: 0.9977\n",
      "Epoch 00108: val_loss did not improve from 0.24810\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0088 - acc: 0.9977 - val_loss: 0.2724 - val_acc: 0.9457\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0125 - acc: 0.9965\n",
      "Epoch 00109: val_loss did not improve from 0.24810\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0125 - acc: 0.9965 - val_loss: 0.2722 - val_acc: 0.9446\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0099 - acc: 0.9975\n",
      "Epoch 00110: val_loss did not improve from 0.24810\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0099 - acc: 0.9975 - val_loss: 0.2920 - val_acc: 0.9392\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0065 - acc: 0.9987\n",
      "Epoch 00111: val_loss did not improve from 0.24810\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0069 - acc: 0.9987 - val_loss: 0.2729 - val_acc: 0.9401\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0234 - acc: 0.9931\n",
      "Epoch 00112: val_loss did not improve from 0.24810\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0237 - acc: 0.9931 - val_loss: 0.2955 - val_acc: 0.9387\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0176 - acc: 0.9946\n",
      "Epoch 00113: val_loss did not improve from 0.24810\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0176 - acc: 0.9946 - val_loss: 0.2605 - val_acc: 0.9448\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0049 - acc: 0.9991\n",
      "Epoch 00114: val_loss did not improve from 0.24810\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0051 - acc: 0.9991 - val_loss: 0.2895 - val_acc: 0.9413\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0156 - acc: 0.9954\n",
      "Epoch 00115: val_loss did not improve from 0.24810\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0156 - acc: 0.9954 - val_loss: 0.2664 - val_acc: 0.9429\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0097 - acc: 0.9975\n",
      "Epoch 00116: val_loss did not improve from 0.24810\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0097 - acc: 0.9975 - val_loss: 0.2887 - val_acc: 0.9418\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_ch_32_BN_10_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 7s 2ms/sample - loss: 0.3123 - acc: 0.9281\n",
      "Loss: 0.31234766842791595 Accuracy: 0.92814124\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.3168 - acc: 0.2977\n",
      "Epoch 00001: val_loss improved from inf to 1.87801, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_BN_11_conv_checkpoint/001-1.8780.hdf5\n",
      "36805/36805 [==============================] - 231s 6ms/sample - loss: 2.3168 - acc: 0.2977 - val_loss: 1.8780 - val_acc: 0.4174\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2914 - acc: 0.6074\n",
      "Epoch 00002: val_loss improved from 1.87801 to 1.03203, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_BN_11_conv_checkpoint/002-1.0320.hdf5\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 1.2915 - acc: 0.6074 - val_loss: 1.0320 - val_acc: 0.6972\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8743 - acc: 0.7356\n",
      "Epoch 00003: val_loss improved from 1.03203 to 0.74158, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_BN_11_conv_checkpoint/003-0.7416.hdf5\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.8743 - acc: 0.7356 - val_loss: 0.7416 - val_acc: 0.7845\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6514 - acc: 0.8035\n",
      "Epoch 00004: val_loss improved from 0.74158 to 0.55077, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_BN_11_conv_checkpoint/004-0.5508.hdf5\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.6514 - acc: 0.8035 - val_loss: 0.5508 - val_acc: 0.8374\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5128 - acc: 0.8447\n",
      "Epoch 00005: val_loss improved from 0.55077 to 0.45549, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_BN_11_conv_checkpoint/005-0.4555.hdf5\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.5128 - acc: 0.8446 - val_loss: 0.4555 - val_acc: 0.8670\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4231 - acc: 0.8711\n",
      "Epoch 00006: val_loss improved from 0.45549 to 0.39368, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_BN_11_conv_checkpoint/006-0.3937.hdf5\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.4233 - acc: 0.8710 - val_loss: 0.3937 - val_acc: 0.8861\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3643 - acc: 0.8892\n",
      "Epoch 00007: val_loss improved from 0.39368 to 0.34224, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_BN_11_conv_checkpoint/007-0.3422.hdf5\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.3643 - acc: 0.8892 - val_loss: 0.3422 - val_acc: 0.9015\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3107 - acc: 0.9057\n",
      "Epoch 00008: val_loss did not improve from 0.34224\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.3107 - acc: 0.9056 - val_loss: 0.3458 - val_acc: 0.8917\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2737 - acc: 0.9180\n",
      "Epoch 00009: val_loss improved from 0.34224 to 0.31768, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_BN_11_conv_checkpoint/009-0.3177.hdf5\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.2737 - acc: 0.9180 - val_loss: 0.3177 - val_acc: 0.9047\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2487 - acc: 0.9242\n",
      "Epoch 00010: val_loss improved from 0.31768 to 0.26974, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_BN_11_conv_checkpoint/010-0.2697.hdf5\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.2488 - acc: 0.9242 - val_loss: 0.2697 - val_acc: 0.9203\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2185 - acc: 0.9353\n",
      "Epoch 00011: val_loss did not improve from 0.26974\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.2185 - acc: 0.9353 - val_loss: 0.3033 - val_acc: 0.9094\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1954 - acc: 0.9401\n",
      "Epoch 00012: val_loss did not improve from 0.26974\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.1956 - acc: 0.9400 - val_loss: 0.3083 - val_acc: 0.9040\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1835 - acc: 0.9445\n",
      "Epoch 00013: val_loss improved from 0.26974 to 0.24319, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_BN_11_conv_checkpoint/013-0.2432.hdf5\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.1838 - acc: 0.9444 - val_loss: 0.2432 - val_acc: 0.9290\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1722 - acc: 0.9477\n",
      "Epoch 00014: val_loss did not improve from 0.24319\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.1722 - acc: 0.9478 - val_loss: 0.2722 - val_acc: 0.9203\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1466 - acc: 0.9546\n",
      "Epoch 00015: val_loss improved from 0.24319 to 0.21328, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_BN_11_conv_checkpoint/015-0.2133.hdf5\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.1466 - acc: 0.9546 - val_loss: 0.2133 - val_acc: 0.9390\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1320 - acc: 0.9610\n",
      "Epoch 00016: val_loss did not improve from 0.21328\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.1321 - acc: 0.9610 - val_loss: 0.2387 - val_acc: 0.9306\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1285 - acc: 0.9605\n",
      "Epoch 00017: val_loss did not improve from 0.21328\n",
      "36805/36805 [==============================] - 115s 3ms/sample - loss: 0.1287 - acc: 0.9604 - val_loss: 0.2326 - val_acc: 0.9299\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1210 - acc: 0.9628\n",
      "Epoch 00018: val_loss did not improve from 0.21328\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.1211 - acc: 0.9628 - val_loss: 0.2147 - val_acc: 0.9399\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1024 - acc: 0.9697\n",
      "Epoch 00019: val_loss did not improve from 0.21328\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.1024 - acc: 0.9697 - val_loss: 0.2284 - val_acc: 0.9311\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0921 - acc: 0.9717\n",
      "Epoch 00020: val_loss improved from 0.21328 to 0.21103, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_BN_11_conv_checkpoint/020-0.2110.hdf5\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0921 - acc: 0.9717 - val_loss: 0.2110 - val_acc: 0.9399\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0890 - acc: 0.9732\n",
      "Epoch 00021: val_loss did not improve from 0.21103\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0890 - acc: 0.9732 - val_loss: 0.2702 - val_acc: 0.9234\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0776 - acc: 0.9769\n",
      "Epoch 00022: val_loss did not improve from 0.21103\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0776 - acc: 0.9769 - val_loss: 0.2190 - val_acc: 0.9315\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0803 - acc: 0.9760\n",
      "Epoch 00023: val_loss did not improve from 0.21103\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0803 - acc: 0.9760 - val_loss: 0.2599 - val_acc: 0.9248\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0616 - acc: 0.9829\n",
      "Epoch 00024: val_loss did not improve from 0.21103\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0618 - acc: 0.9828 - val_loss: 0.2151 - val_acc: 0.9401\n",
      "Epoch 25/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0733 - acc: 0.9787\n",
      "Epoch 00025: val_loss did not improve from 0.21103\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0734 - acc: 0.9787 - val_loss: 0.2340 - val_acc: 0.9350\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0727 - acc: 0.9782\n",
      "Epoch 00026: val_loss did not improve from 0.21103\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0727 - acc: 0.9781 - val_loss: 0.2272 - val_acc: 0.9415\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0553 - acc: 0.9838\n",
      "Epoch 00027: val_loss improved from 0.21103 to 0.20892, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_BN_11_conv_checkpoint/027-0.2089.hdf5\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0555 - acc: 0.9838 - val_loss: 0.2089 - val_acc: 0.9439\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0592 - acc: 0.9833\n",
      "Epoch 00028: val_loss did not improve from 0.20892\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0592 - acc: 0.9832 - val_loss: 0.2232 - val_acc: 0.9385\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0476 - acc: 0.9869\n",
      "Epoch 00029: val_loss improved from 0.20892 to 0.18411, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_BN_11_conv_checkpoint/029-0.1841.hdf5\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0477 - acc: 0.9869 - val_loss: 0.1841 - val_acc: 0.9481\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0506 - acc: 0.9851\n",
      "Epoch 00030: val_loss did not improve from 0.18411\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0506 - acc: 0.9851 - val_loss: 0.1939 - val_acc: 0.9467\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0340 - acc: 0.9918\n",
      "Epoch 00031: val_loss did not improve from 0.18411\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0340 - acc: 0.9918 - val_loss: 0.2245 - val_acc: 0.9352\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0372 - acc: 0.9907\n",
      "Epoch 00032: val_loss did not improve from 0.18411\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0373 - acc: 0.9907 - val_loss: 0.2399 - val_acc: 0.9378\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0557 - acc: 0.9832\n",
      "Epoch 00033: val_loss did not improve from 0.18411\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0557 - acc: 0.9832 - val_loss: 0.1894 - val_acc: 0.9509\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0323 - acc: 0.9917\n",
      "Epoch 00034: val_loss did not improve from 0.18411\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0324 - acc: 0.9917 - val_loss: 0.1907 - val_acc: 0.9529\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0340 - acc: 0.9905\n",
      "Epoch 00035: val_loss did not improve from 0.18411\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0342 - acc: 0.9904 - val_loss: 0.1919 - val_acc: 0.9495\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0478 - acc: 0.9857\n",
      "Epoch 00036: val_loss did not improve from 0.18411\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0481 - acc: 0.9857 - val_loss: 0.1976 - val_acc: 0.9439\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0364 - acc: 0.9903\n",
      "Epoch 00037: val_loss improved from 0.18411 to 0.17596, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_BN_11_conv_checkpoint/037-0.1760.hdf5\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0365 - acc: 0.9903 - val_loss: 0.1760 - val_acc: 0.9502\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0321 - acc: 0.9911\n",
      "Epoch 00038: val_loss did not improve from 0.17596\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0322 - acc: 0.9911 - val_loss: 0.2308 - val_acc: 0.9383\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0375 - acc: 0.9896\n",
      "Epoch 00039: val_loss did not improve from 0.17596\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0376 - acc: 0.9895 - val_loss: 0.1902 - val_acc: 0.9506\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0303 - acc: 0.9921\n",
      "Epoch 00040: val_loss did not improve from 0.17596\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0303 - acc: 0.9921 - val_loss: 0.1814 - val_acc: 0.9520\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0203 - acc: 0.9951\n",
      "Epoch 00041: val_loss did not improve from 0.17596\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0203 - acc: 0.9951 - val_loss: 0.1820 - val_acc: 0.9532\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0196 - acc: 0.9954\n",
      "Epoch 00042: val_loss did not improve from 0.17596\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0196 - acc: 0.9954 - val_loss: 0.2368 - val_acc: 0.9401\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0239 - acc: 0.9937\n",
      "Epoch 00043: val_loss did not improve from 0.17596\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0239 - acc: 0.9937 - val_loss: 0.2161 - val_acc: 0.9455\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0315 - acc: 0.9905\n",
      "Epoch 00044: val_loss did not improve from 0.17596\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0315 - acc: 0.9905 - val_loss: 0.2370 - val_acc: 0.9427\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0224 - acc: 0.9940\n",
      "Epoch 00045: val_loss did not improve from 0.17596\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0224 - acc: 0.9940 - val_loss: 0.2165 - val_acc: 0.9483\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0211 - acc: 0.9946\n",
      "Epoch 00046: val_loss did not improve from 0.17596\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0213 - acc: 0.9946 - val_loss: 0.2050 - val_acc: 0.9471\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0357 - acc: 0.9899\n",
      "Epoch 00047: val_loss did not improve from 0.17596\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0358 - acc: 0.9899 - val_loss: 0.1852 - val_acc: 0.9536\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0249 - acc: 0.9928\n",
      "Epoch 00048: val_loss did not improve from 0.17596\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0249 - acc: 0.9928 - val_loss: 0.1914 - val_acc: 0.9518\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0181 - acc: 0.9955\n",
      "Epoch 00049: val_loss did not improve from 0.17596\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0182 - acc: 0.9955 - val_loss: 0.1903 - val_acc: 0.9546\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0212 - acc: 0.9939\n",
      "Epoch 00050: val_loss did not improve from 0.17596\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0212 - acc: 0.9939 - val_loss: 0.2124 - val_acc: 0.9457\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0195 - acc: 0.9940\n",
      "Epoch 00051: val_loss did not improve from 0.17596\n",
      "36805/36805 [==============================] - 110s 3ms/sample - loss: 0.0195 - acc: 0.9939 - val_loss: 0.2333 - val_acc: 0.9446\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0282 - acc: 0.9921\n",
      "Epoch 00052: val_loss did not improve from 0.17596\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0282 - acc: 0.9921 - val_loss: 0.1983 - val_acc: 0.9515\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0136 - acc: 0.9969\n",
      "Epoch 00053: val_loss did not improve from 0.17596\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0141 - acc: 0.9968 - val_loss: 0.1780 - val_acc: 0.9518\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0333 - acc: 0.9902\n",
      "Epoch 00054: val_loss did not improve from 0.17596\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0333 - acc: 0.9902 - val_loss: 0.2222 - val_acc: 0.9506\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0113 - acc: 0.9975\n",
      "Epoch 00055: val_loss did not improve from 0.17596\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0113 - acc: 0.9975 - val_loss: 0.2107 - val_acc: 0.9495\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0153 - acc: 0.9961\n",
      "Epoch 00056: val_loss did not improve from 0.17596\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0153 - acc: 0.9961 - val_loss: 0.2065 - val_acc: 0.9504\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0255 - acc: 0.9925\n",
      "Epoch 00057: val_loss did not improve from 0.17596\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0256 - acc: 0.9925 - val_loss: 0.1960 - val_acc: 0.9532\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0265 - acc: 0.9925\n",
      "Epoch 00058: val_loss did not improve from 0.17596\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0265 - acc: 0.9925 - val_loss: 0.2067 - val_acc: 0.9527\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0106 - acc: 0.9976\n",
      "Epoch 00059: val_loss did not improve from 0.17596\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0106 - acc: 0.9976 - val_loss: 0.1990 - val_acc: 0.9536\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0101 - acc: 0.9977\n",
      "Epoch 00060: val_loss did not improve from 0.17596\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0102 - acc: 0.9977 - val_loss: 0.2925 - val_acc: 0.9387\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0298 - acc: 0.9912\n",
      "Epoch 00061: val_loss did not improve from 0.17596\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0298 - acc: 0.9912 - val_loss: 0.2006 - val_acc: 0.9515\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0105 - acc: 0.9975\n",
      "Epoch 00062: val_loss did not improve from 0.17596\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0106 - acc: 0.9975 - val_loss: 0.2167 - val_acc: 0.9483\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0234 - acc: 0.9934\n",
      "Epoch 00063: val_loss did not improve from 0.17596\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0235 - acc: 0.9934 - val_loss: 0.2132 - val_acc: 0.9485\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0115 - acc: 0.9976\n",
      "Epoch 00064: val_loss did not improve from 0.17596\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0115 - acc: 0.9976 - val_loss: 0.2038 - val_acc: 0.9520\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0106 - acc: 0.9976\n",
      "Epoch 00065: val_loss did not improve from 0.17596\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0107 - acc: 0.9976 - val_loss: 0.2115 - val_acc: 0.9522\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0201 - acc: 0.9945\n",
      "Epoch 00066: val_loss did not improve from 0.17596\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0201 - acc: 0.9944 - val_loss: 0.2170 - val_acc: 0.9513\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0215 - acc: 0.9936\n",
      "Epoch 00067: val_loss did not improve from 0.17596\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0215 - acc: 0.9936 - val_loss: 0.2877 - val_acc: 0.9362\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0143 - acc: 0.9963\n",
      "Epoch 00068: val_loss did not improve from 0.17596\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0144 - acc: 0.9963 - val_loss: 0.2058 - val_acc: 0.9529\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0157 - acc: 0.9955\n",
      "Epoch 00069: val_loss did not improve from 0.17596\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0157 - acc: 0.9955 - val_loss: 0.1928 - val_acc: 0.9557\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0068 - acc: 0.9988\n",
      "Epoch 00070: val_loss did not improve from 0.17596\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0069 - acc: 0.9988 - val_loss: 0.1938 - val_acc: 0.9555\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0148 - acc: 0.9960\n",
      "Epoch 00071: val_loss did not improve from 0.17596\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0148 - acc: 0.9960 - val_loss: 0.2262 - val_acc: 0.9509\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0141 - acc: 0.9962\n",
      "Epoch 00072: val_loss did not improve from 0.17596\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0141 - acc: 0.9962 - val_loss: 0.2306 - val_acc: 0.9478\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0100 - acc: 0.9978\n",
      "Epoch 00073: val_loss did not improve from 0.17596\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0100 - acc: 0.9978 - val_loss: 0.1965 - val_acc: 0.9555\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0123 - acc: 0.9969\n",
      "Epoch 00074: val_loss did not improve from 0.17596\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0124 - acc: 0.9969 - val_loss: 0.2251 - val_acc: 0.9495\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0294 - acc: 0.9907\n",
      "Epoch 00075: val_loss did not improve from 0.17596\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0295 - acc: 0.9907 - val_loss: 0.1908 - val_acc: 0.9518\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0161 - acc: 0.9955\n",
      "Epoch 00076: val_loss did not improve from 0.17596\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0161 - acc: 0.9955 - val_loss: 0.1999 - val_acc: 0.9525\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0056 - acc: 0.9991\n",
      "Epoch 00077: val_loss did not improve from 0.17596\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0056 - acc: 0.9990 - val_loss: 0.2332 - val_acc: 0.9513\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0152 - acc: 0.9955\n",
      "Epoch 00078: val_loss did not improve from 0.17596\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0153 - acc: 0.9955 - val_loss: 0.2529 - val_acc: 0.9476\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0181 - acc: 0.9946\n",
      "Epoch 00079: val_loss did not improve from 0.17596\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0181 - acc: 0.9946 - val_loss: 0.2079 - val_acc: 0.9497\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0157 - acc: 0.9958\n",
      "Epoch 00080: val_loss did not improve from 0.17596\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0157 - acc: 0.9958 - val_loss: 0.1906 - val_acc: 0.9571\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0109 - acc: 0.9972\n",
      "Epoch 00081: val_loss did not improve from 0.17596\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0110 - acc: 0.9972 - val_loss: 0.2074 - val_acc: 0.9553\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0062 - acc: 0.9988\n",
      "Epoch 00082: val_loss did not improve from 0.17596\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0062 - acc: 0.9988 - val_loss: 0.2032 - val_acc: 0.9578\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0148 - acc: 0.9957\n",
      "Epoch 00083: val_loss did not improve from 0.17596\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0148 - acc: 0.9957 - val_loss: 0.2414 - val_acc: 0.9462\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0130 - acc: 0.9963\n",
      "Epoch 00084: val_loss did not improve from 0.17596\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0130 - acc: 0.9963 - val_loss: 0.1868 - val_acc: 0.9585\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0057 - acc: 0.9987\n",
      "Epoch 00085: val_loss did not improve from 0.17596\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0065 - acc: 0.9986 - val_loss: 0.2382 - val_acc: 0.9483\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0213 - acc: 0.9932\n",
      "Epoch 00086: val_loss did not improve from 0.17596\n",
      "36805/36805 [==============================] - 112s 3ms/sample - loss: 0.0214 - acc: 0.9932 - val_loss: 0.2151 - val_acc: 0.9506\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0118 - acc: 0.9963\n",
      "Epoch 00087: val_loss did not improve from 0.17596\n",
      "36805/36805 [==============================] - 111s 3ms/sample - loss: 0.0118 - acc: 0.9963 - val_loss: 0.1860 - val_acc: 0.9590\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_ch_32_BN_11_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 8s 2ms/sample - loss: 0.2364 - acc: 0.9317\n",
      "Loss: 0.23642852384662827 Accuracy: 0.93167186\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.3741 - acc: 0.2783\n",
      "Epoch 00001: val_loss improved from inf to 2.03832, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_BN_12_conv_checkpoint/001-2.0383.hdf5\n",
      "36805/36805 [==============================] - 254s 7ms/sample - loss: 2.3741 - acc: 0.2783 - val_loss: 2.0383 - val_acc: 0.3515\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3797 - acc: 0.5714\n",
      "Epoch 00002: val_loss improved from 2.03832 to 0.99967, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_BN_12_conv_checkpoint/002-0.9997.hdf5\n",
      "36805/36805 [==============================] - 115s 3ms/sample - loss: 1.3798 - acc: 0.5714 - val_loss: 0.9997 - val_acc: 0.6951\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8701 - acc: 0.7298\n",
      "Epoch 00003: val_loss improved from 0.99967 to 0.70120, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_BN_12_conv_checkpoint/003-0.7012.hdf5\n",
      "36805/36805 [==============================] - 115s 3ms/sample - loss: 0.8702 - acc: 0.7298 - val_loss: 0.7012 - val_acc: 0.7780\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6189 - acc: 0.8092\n",
      "Epoch 00004: val_loss improved from 0.70120 to 0.47663, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_BN_12_conv_checkpoint/004-0.4766.hdf5\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.6190 - acc: 0.8092 - val_loss: 0.4766 - val_acc: 0.8532\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4703 - acc: 0.8552\n",
      "Epoch 00005: val_loss improved from 0.47663 to 0.39975, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_BN_12_conv_checkpoint/005-0.3997.hdf5\n",
      "36805/36805 [==============================] - 115s 3ms/sample - loss: 0.4704 - acc: 0.8551 - val_loss: 0.3997 - val_acc: 0.8744\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3770 - acc: 0.8848\n",
      "Epoch 00006: val_loss improved from 0.39975 to 0.35023, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_BN_12_conv_checkpoint/006-0.3502.hdf5\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.3770 - acc: 0.8848 - val_loss: 0.3502 - val_acc: 0.8956\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3205 - acc: 0.8996\n",
      "Epoch 00007: val_loss improved from 0.35023 to 0.31158, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_BN_12_conv_checkpoint/007-0.3116.hdf5\n",
      "36805/36805 [==============================] - 115s 3ms/sample - loss: 0.3205 - acc: 0.8996 - val_loss: 0.3116 - val_acc: 0.8998\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2720 - acc: 0.9149\n",
      "Epoch 00008: val_loss did not improve from 0.31158\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.2720 - acc: 0.9149 - val_loss: 0.3176 - val_acc: 0.8989\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2324 - acc: 0.9297\n",
      "Epoch 00009: val_loss did not improve from 0.31158\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.2324 - acc: 0.9297 - val_loss: 0.3220 - val_acc: 0.8966\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2096 - acc: 0.9345\n",
      "Epoch 00010: val_loss improved from 0.31158 to 0.24333, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_BN_12_conv_checkpoint/010-0.2433.hdf5\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.2095 - acc: 0.9345 - val_loss: 0.2433 - val_acc: 0.9229\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1857 - acc: 0.9419\n",
      "Epoch 00011: val_loss did not improve from 0.24333\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.1858 - acc: 0.9419 - val_loss: 0.2442 - val_acc: 0.9297\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1697 - acc: 0.9471\n",
      "Epoch 00012: val_loss improved from 0.24333 to 0.22229, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_BN_12_conv_checkpoint/012-0.2223.hdf5\n",
      "36805/36805 [==============================] - 115s 3ms/sample - loss: 0.1697 - acc: 0.9470 - val_loss: 0.2223 - val_acc: 0.9278\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1514 - acc: 0.9529\n",
      "Epoch 00013: val_loss improved from 0.22229 to 0.21129, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_BN_12_conv_checkpoint/013-0.2113.hdf5\n",
      "36805/36805 [==============================] - 115s 3ms/sample - loss: 0.1514 - acc: 0.9528 - val_loss: 0.2113 - val_acc: 0.9364\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1370 - acc: 0.9568\n",
      "Epoch 00014: val_loss did not improve from 0.21129\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.1373 - acc: 0.9567 - val_loss: 0.2222 - val_acc: 0.9324\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1321 - acc: 0.9577\n",
      "Epoch 00015: val_loss improved from 0.21129 to 0.19372, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_BN_12_conv_checkpoint/015-0.1937.hdf5\n",
      "36805/36805 [==============================] - 115s 3ms/sample - loss: 0.1321 - acc: 0.9577 - val_loss: 0.1937 - val_acc: 0.9373\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1172 - acc: 0.9633\n",
      "Epoch 00016: val_loss did not improve from 0.19372\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.1173 - acc: 0.9633 - val_loss: 0.2241 - val_acc: 0.9329\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1039 - acc: 0.9682\n",
      "Epoch 00017: val_loss improved from 0.19372 to 0.17710, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_BN_12_conv_checkpoint/017-0.1771.hdf5\n",
      "36805/36805 [==============================] - 115s 3ms/sample - loss: 0.1039 - acc: 0.9682 - val_loss: 0.1771 - val_acc: 0.9497\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0879 - acc: 0.9737\n",
      "Epoch 00018: val_loss did not improve from 0.17710\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0879 - acc: 0.9736 - val_loss: 0.1983 - val_acc: 0.9415\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0909 - acc: 0.9711\n",
      "Epoch 00019: val_loss did not improve from 0.17710\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0909 - acc: 0.9711 - val_loss: 0.2076 - val_acc: 0.9406\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0744 - acc: 0.9774\n",
      "Epoch 00020: val_loss did not improve from 0.17710\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0744 - acc: 0.9774 - val_loss: 0.2232 - val_acc: 0.9311\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0670 - acc: 0.9805\n",
      "Epoch 00021: val_loss did not improve from 0.17710\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0671 - acc: 0.9805 - val_loss: 0.1929 - val_acc: 0.9413\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0709 - acc: 0.9792\n",
      "Epoch 00022: val_loss did not improve from 0.17710\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0713 - acc: 0.9791 - val_loss: 0.2088 - val_acc: 0.9394\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0776 - acc: 0.9760\n",
      "Epoch 00023: val_loss did not improve from 0.17710\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0776 - acc: 0.9760 - val_loss: 0.1917 - val_acc: 0.9436\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0521 - acc: 0.9853\n",
      "Epoch 00024: val_loss did not improve from 0.17710\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0522 - acc: 0.9853 - val_loss: 0.2116 - val_acc: 0.9415\n",
      "Epoch 25/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0554 - acc: 0.9836\n",
      "Epoch 00025: val_loss did not improve from 0.17710\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0555 - acc: 0.9836 - val_loss: 0.1949 - val_acc: 0.9448\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0546 - acc: 0.9843\n",
      "Epoch 00026: val_loss did not improve from 0.17710\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0546 - acc: 0.9842 - val_loss: 0.1843 - val_acc: 0.9460\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0476 - acc: 0.9862\n",
      "Epoch 00027: val_loss did not improve from 0.17710\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0476 - acc: 0.9862 - val_loss: 0.1958 - val_acc: 0.9471\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0381 - acc: 0.9897\n",
      "Epoch 00028: val_loss did not improve from 0.17710\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0381 - acc: 0.9897 - val_loss: 0.1941 - val_acc: 0.9422\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0410 - acc: 0.9878\n",
      "Epoch 00029: val_loss did not improve from 0.17710\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0410 - acc: 0.9878 - val_loss: 0.1912 - val_acc: 0.9450\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0340 - acc: 0.9903\n",
      "Epoch 00030: val_loss did not improve from 0.17710\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0340 - acc: 0.9903 - val_loss: 0.2077 - val_acc: 0.9446\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0356 - acc: 0.9895\n",
      "Epoch 00031: val_loss did not improve from 0.17710\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0356 - acc: 0.9895 - val_loss: 0.2166 - val_acc: 0.9394\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0335 - acc: 0.9904\n",
      "Epoch 00032: val_loss did not improve from 0.17710\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0335 - acc: 0.9904 - val_loss: 0.1786 - val_acc: 0.9515\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0279 - acc: 0.9921\n",
      "Epoch 00033: val_loss improved from 0.17710 to 0.17680, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_BN_12_conv_checkpoint/033-0.1768.hdf5\n",
      "36805/36805 [==============================] - 115s 3ms/sample - loss: 0.0281 - acc: 0.9920 - val_loss: 0.1768 - val_acc: 0.9511\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0470 - acc: 0.9857\n",
      "Epoch 00034: val_loss did not improve from 0.17680\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0471 - acc: 0.9857 - val_loss: 0.1773 - val_acc: 0.9550\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0273 - acc: 0.9926\n",
      "Epoch 00035: val_loss improved from 0.17680 to 0.16582, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_BN_12_conv_checkpoint/035-0.1658.hdf5\n",
      "36805/36805 [==============================] - 115s 3ms/sample - loss: 0.0273 - acc: 0.9926 - val_loss: 0.1658 - val_acc: 0.9560\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0215 - acc: 0.9950\n",
      "Epoch 00036: val_loss did not improve from 0.16582\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0216 - acc: 0.9950 - val_loss: 0.2017 - val_acc: 0.9471\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0313 - acc: 0.9907\n",
      "Epoch 00037: val_loss did not improve from 0.16582\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0313 - acc: 0.9907 - val_loss: 0.1875 - val_acc: 0.9515\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0207 - acc: 0.9947\n",
      "Epoch 00038: val_loss improved from 0.16582 to 0.16100, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_BN_12_conv_checkpoint/038-0.1610.hdf5\n",
      "36805/36805 [==============================] - 115s 3ms/sample - loss: 0.0207 - acc: 0.9947 - val_loss: 0.1610 - val_acc: 0.9562\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0201 - acc: 0.9944\n",
      "Epoch 00039: val_loss did not improve from 0.16100\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0201 - acc: 0.9944 - val_loss: 0.2508 - val_acc: 0.9376\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0209 - acc: 0.9944\n",
      "Epoch 00040: val_loss did not improve from 0.16100\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0209 - acc: 0.9944 - val_loss: 0.2164 - val_acc: 0.9450\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0196 - acc: 0.9947\n",
      "Epoch 00041: val_loss did not improve from 0.16100\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0198 - acc: 0.9947 - val_loss: 0.2808 - val_acc: 0.9311\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0389 - acc: 0.9885\n",
      "Epoch 00042: val_loss did not improve from 0.16100\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0389 - acc: 0.9885 - val_loss: 0.1892 - val_acc: 0.9511\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0146 - acc: 0.9965\n",
      "Epoch 00043: val_loss did not improve from 0.16100\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0149 - acc: 0.9964 - val_loss: 0.1753 - val_acc: 0.9543\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0358 - acc: 0.9887\n",
      "Epoch 00044: val_loss did not improve from 0.16100\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0359 - acc: 0.9887 - val_loss: 0.1732 - val_acc: 0.9518\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0350 - acc: 0.9897\n",
      "Epoch 00045: val_loss did not improve from 0.16100\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0350 - acc: 0.9897 - val_loss: 0.1628 - val_acc: 0.9578\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0121 - acc: 0.9973\n",
      "Epoch 00046: val_loss did not improve from 0.16100\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0123 - acc: 0.9972 - val_loss: 0.1913 - val_acc: 0.9548\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0275 - acc: 0.9918\n",
      "Epoch 00047: val_loss did not improve from 0.16100\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0275 - acc: 0.9918 - val_loss: 0.1750 - val_acc: 0.9571\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0153 - acc: 0.9962\n",
      "Epoch 00048: val_loss improved from 0.16100 to 0.15881, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_BN_12_conv_checkpoint/048-0.1588.hdf5\n",
      "36805/36805 [==============================] - 115s 3ms/sample - loss: 0.0153 - acc: 0.9962 - val_loss: 0.1588 - val_acc: 0.9590\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0123 - acc: 0.9968\n",
      "Epoch 00049: val_loss did not improve from 0.15881\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0123 - acc: 0.9968 - val_loss: 0.1714 - val_acc: 0.9571\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0110 - acc: 0.9975\n",
      "Epoch 00050: val_loss did not improve from 0.15881\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0110 - acc: 0.9975 - val_loss: 0.1702 - val_acc: 0.9564\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0130 - acc: 0.9966\n",
      "Epoch 00051: val_loss did not improve from 0.15881\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0131 - acc: 0.9966 - val_loss: 0.1989 - val_acc: 0.9509\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0304 - acc: 0.9910\n",
      "Epoch 00052: val_loss did not improve from 0.15881\n",
      "36805/36805 [==============================] - 113s 3ms/sample - loss: 0.0304 - acc: 0.9910 - val_loss: 0.1891 - val_acc: 0.9550\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0103 - acc: 0.9976\n",
      "Epoch 00053: val_loss did not improve from 0.15881\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0103 - acc: 0.9976 - val_loss: 0.1999 - val_acc: 0.9518\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0167 - acc: 0.9955\n",
      "Epoch 00054: val_loss did not improve from 0.15881\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0167 - acc: 0.9955 - val_loss: 0.1887 - val_acc: 0.9525\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0123 - acc: 0.9969\n",
      "Epoch 00055: val_loss did not improve from 0.15881\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0123 - acc: 0.9969 - val_loss: 0.2189 - val_acc: 0.9497\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0120 - acc: 0.9967\n",
      "Epoch 00056: val_loss did not improve from 0.15881\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0122 - acc: 0.9966 - val_loss: 0.3449 - val_acc: 0.9294\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0317 - acc: 0.9901\n",
      "Epoch 00057: val_loss did not improve from 0.15881\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0318 - acc: 0.9901 - val_loss: 0.2091 - val_acc: 0.9509\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0217 - acc: 0.9935\n",
      "Epoch 00058: val_loss did not improve from 0.15881\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0217 - acc: 0.9935 - val_loss: 0.1676 - val_acc: 0.9571\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0106 - acc: 0.9976\n",
      "Epoch 00059: val_loss did not improve from 0.15881\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0106 - acc: 0.9976 - val_loss: 0.1833 - val_acc: 0.9560\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0113 - acc: 0.9970\n",
      "Epoch 00060: val_loss did not improve from 0.15881\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0113 - acc: 0.9970 - val_loss: 0.1796 - val_acc: 0.9557\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0084 - acc: 0.9978\n",
      "Epoch 00061: val_loss did not improve from 0.15881\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0085 - acc: 0.9978 - val_loss: 0.1817 - val_acc: 0.9571\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0283 - acc: 0.9912\n",
      "Epoch 00062: val_loss did not improve from 0.15881\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0284 - acc: 0.9912 - val_loss: 0.1857 - val_acc: 0.9539\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0155 - acc: 0.9957\n",
      "Epoch 00063: val_loss did not improve from 0.15881\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0155 - acc: 0.9957 - val_loss: 0.1665 - val_acc: 0.9606\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0068 - acc: 0.9987\n",
      "Epoch 00064: val_loss did not improve from 0.15881\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0068 - acc: 0.9988 - val_loss: 0.1664 - val_acc: 0.9583\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0068 - acc: 0.9984\n",
      "Epoch 00065: val_loss did not improve from 0.15881\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0068 - acc: 0.9984 - val_loss: 0.2066 - val_acc: 0.9539\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0209 - acc: 0.9935\n",
      "Epoch 00066: val_loss did not improve from 0.15881\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0209 - acc: 0.9935 - val_loss: 0.1989 - val_acc: 0.9511\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0086 - acc: 0.9978\n",
      "Epoch 00067: val_loss did not improve from 0.15881\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0086 - acc: 0.9978 - val_loss: 0.1714 - val_acc: 0.9571\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0118 - acc: 0.9963\n",
      "Epoch 00068: val_loss did not improve from 0.15881\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0118 - acc: 0.9963 - val_loss: 0.1931 - val_acc: 0.9574\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0173 - acc: 0.9945\n",
      "Epoch 00069: val_loss did not improve from 0.15881\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0174 - acc: 0.9945 - val_loss: 0.1789 - val_acc: 0.9567\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0075 - acc: 0.9983\n",
      "Epoch 00070: val_loss did not improve from 0.15881\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0076 - acc: 0.9983 - val_loss: 0.1780 - val_acc: 0.9595\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0166 - acc: 0.9954\n",
      "Epoch 00071: val_loss did not improve from 0.15881\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0166 - acc: 0.9954 - val_loss: 0.1680 - val_acc: 0.9590\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0097 - acc: 0.9973\n",
      "Epoch 00072: val_loss improved from 0.15881 to 0.15481, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_BN_12_conv_checkpoint/072-0.1548.hdf5\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0097 - acc: 0.9973 - val_loss: 0.1548 - val_acc: 0.9630\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0077 - acc: 0.9979\n",
      "Epoch 00073: val_loss did not improve from 0.15481\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0077 - acc: 0.9979 - val_loss: 0.2188 - val_acc: 0.9492\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0207 - acc: 0.9937\n",
      "Epoch 00074: val_loss did not improve from 0.15481\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0207 - acc: 0.9937 - val_loss: 0.1702 - val_acc: 0.9597\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0063 - acc: 0.9985\n",
      "Epoch 00075: val_loss did not improve from 0.15481\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0063 - acc: 0.9985 - val_loss: 0.1998 - val_acc: 0.9543\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0217 - acc: 0.9933\n",
      "Epoch 00076: val_loss improved from 0.15481 to 0.15373, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_BN_12_conv_checkpoint/076-0.1537.hdf5\n",
      "36805/36805 [==============================] - 115s 3ms/sample - loss: 0.0218 - acc: 0.9933 - val_loss: 0.1537 - val_acc: 0.9627\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0083 - acc: 0.9979\n",
      "Epoch 00077: val_loss did not improve from 0.15373\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0083 - acc: 0.9979 - val_loss: 0.1611 - val_acc: 0.9611\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0046 - acc: 0.9991\n",
      "Epoch 00078: val_loss did not improve from 0.15373\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0046 - acc: 0.9991 - val_loss: 0.1635 - val_acc: 0.9644\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0058 - acc: 0.9986\n",
      "Epoch 00079: val_loss did not improve from 0.15373\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0059 - acc: 0.9986 - val_loss: 0.1818 - val_acc: 0.9569\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0173 - acc: 0.9947\n",
      "Epoch 00080: val_loss did not improve from 0.15373\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0173 - acc: 0.9947 - val_loss: 0.1664 - val_acc: 0.9588\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0056 - acc: 0.9987\n",
      "Epoch 00081: val_loss did not improve from 0.15373\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0056 - acc: 0.9987 - val_loss: 0.1763 - val_acc: 0.9562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0074 - acc: 0.9981\n",
      "Epoch 00082: val_loss did not improve from 0.15373\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0074 - acc: 0.9981 - val_loss: 0.1768 - val_acc: 0.9578\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0086 - acc: 0.9974\n",
      "Epoch 00083: val_loss did not improve from 0.15373\n",
      "36805/36805 [==============================] - 115s 3ms/sample - loss: 0.0088 - acc: 0.9973 - val_loss: 0.1884 - val_acc: 0.9590\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0240 - acc: 0.9928\n",
      "Epoch 00084: val_loss did not improve from 0.15373\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0240 - acc: 0.9928 - val_loss: 0.1912 - val_acc: 0.9585\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0048 - acc: 0.9990\n",
      "Epoch 00085: val_loss did not improve from 0.15373\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0048 - acc: 0.9990 - val_loss: 0.1670 - val_acc: 0.9604\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0037 - acc: 0.9992\n",
      "Epoch 00086: val_loss did not improve from 0.15373\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0037 - acc: 0.9992 - val_loss: 0.1675 - val_acc: 0.9599\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0132 - acc: 0.9961\n",
      "Epoch 00087: val_loss did not improve from 0.15373\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0132 - acc: 0.9961 - val_loss: 0.1719 - val_acc: 0.9595\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0043 - acc: 0.9992\n",
      "Epoch 00088: val_loss did not improve from 0.15373\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0048 - acc: 0.9991 - val_loss: 0.2493 - val_acc: 0.9490\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0188 - acc: 0.9943\n",
      "Epoch 00089: val_loss did not improve from 0.15373\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0189 - acc: 0.9943 - val_loss: 0.1714 - val_acc: 0.9571\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0171 - acc: 0.9945\n",
      "Epoch 00090: val_loss did not improve from 0.15373\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0175 - acc: 0.9944 - val_loss: 0.1923 - val_acc: 0.9553\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0251 - acc: 0.9928\n",
      "Epoch 00091: val_loss did not improve from 0.15373\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0251 - acc: 0.9928 - val_loss: 0.1650 - val_acc: 0.9620\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0042 - acc: 0.9993\n",
      "Epoch 00092: val_loss did not improve from 0.15373\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0044 - acc: 0.9993 - val_loss: 0.1739 - val_acc: 0.9609\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0149 - acc: 0.9954\n",
      "Epoch 00093: val_loss did not improve from 0.15373\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0150 - acc: 0.9954 - val_loss: 0.1728 - val_acc: 0.9576\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0046 - acc: 0.9990\n",
      "Epoch 00094: val_loss did not improve from 0.15373\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0046 - acc: 0.9989 - val_loss: 0.1783 - val_acc: 0.9583\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0077 - acc: 0.9980\n",
      "Epoch 00095: val_loss did not improve from 0.15373\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0079 - acc: 0.9980 - val_loss: 0.1927 - val_acc: 0.9571\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0158 - acc: 0.9951\n",
      "Epoch 00096: val_loss did not improve from 0.15373\n",
      "36805/36805 [==============================] - 115s 3ms/sample - loss: 0.0158 - acc: 0.9951 - val_loss: 0.1858 - val_acc: 0.9557\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0042 - acc: 0.9992\n",
      "Epoch 00097: val_loss did not improve from 0.15373\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0042 - acc: 0.9992 - val_loss: 0.1751 - val_acc: 0.9611\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0034 - acc: 0.9993\n",
      "Epoch 00098: val_loss did not improve from 0.15373\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0035 - acc: 0.9993 - val_loss: 0.2001 - val_acc: 0.9562\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0189 - acc: 0.9941\n",
      "Epoch 00099: val_loss did not improve from 0.15373\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0190 - acc: 0.9941 - val_loss: 0.1797 - val_acc: 0.9576\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0158 - acc: 0.9951\n",
      "Epoch 00100: val_loss did not improve from 0.15373\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0158 - acc: 0.9950 - val_loss: 0.1795 - val_acc: 0.9604\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0130 - acc: 0.9963\n",
      "Epoch 00101: val_loss did not improve from 0.15373\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0130 - acc: 0.9963 - val_loss: 0.1852 - val_acc: 0.9597\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0029 - acc: 0.9994\n",
      "Epoch 00102: val_loss did not improve from 0.15373\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0029 - acc: 0.9994 - val_loss: 0.1636 - val_acc: 0.9623\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0031 - acc: 0.9995\n",
      "Epoch 00103: val_loss did not improve from 0.15373\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0032 - acc: 0.9995 - val_loss: 0.1712 - val_acc: 0.9616\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0155 - acc: 0.9950\n",
      "Epoch 00104: val_loss did not improve from 0.15373\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0155 - acc: 0.9950 - val_loss: 0.1880 - val_acc: 0.9595\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0034 - acc: 0.9991\n",
      "Epoch 00105: val_loss did not improve from 0.15373\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0036 - acc: 0.9991 - val_loss: 0.2186 - val_acc: 0.9499\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0197 - acc: 0.9940\n",
      "Epoch 00106: val_loss did not improve from 0.15373\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0197 - acc: 0.9940 - val_loss: 0.1775 - val_acc: 0.9585\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0098 - acc: 0.9972\n",
      "Epoch 00107: val_loss did not improve from 0.15373\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0099 - acc: 0.9972 - val_loss: 0.1721 - val_acc: 0.9609\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0103 - acc: 0.9973\n",
      "Epoch 00108: val_loss did not improve from 0.15373\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0103 - acc: 0.9973 - val_loss: 0.1973 - val_acc: 0.9562\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0039 - acc: 0.9992\n",
      "Epoch 00109: val_loss did not improve from 0.15373\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0039 - acc: 0.9991 - val_loss: 0.1873 - val_acc: 0.9569\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0118 - acc: 0.9968\n",
      "Epoch 00110: val_loss did not improve from 0.15373\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0118 - acc: 0.9968 - val_loss: 0.1756 - val_acc: 0.9597\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0041 - acc: 0.9991\n",
      "Epoch 00111: val_loss did not improve from 0.15373\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0041 - acc: 0.9991 - val_loss: 0.1821 - val_acc: 0.9613\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0093 - acc: 0.9974\n",
      "Epoch 00112: val_loss did not improve from 0.15373\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0093 - acc: 0.9974 - val_loss: 0.1748 - val_acc: 0.9606\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0044 - acc: 0.9989\n",
      "Epoch 00113: val_loss did not improve from 0.15373\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0045 - acc: 0.9989 - val_loss: 0.2083 - val_acc: 0.9541\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0166 - acc: 0.9947\n",
      "Epoch 00114: val_loss did not improve from 0.15373\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0166 - acc: 0.9947 - val_loss: 0.1804 - val_acc: 0.9583\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0103 - acc: 0.9968\n",
      "Epoch 00115: val_loss did not improve from 0.15373\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0103 - acc: 0.9968 - val_loss: 0.1820 - val_acc: 0.9604\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0037 - acc: 0.9990\n",
      "Epoch 00116: val_loss did not improve from 0.15373\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0037 - acc: 0.9990 - val_loss: 0.1624 - val_acc: 0.9613\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0041 - acc: 0.9990\n",
      "Epoch 00117: val_loss did not improve from 0.15373\n",
      "36805/36805 [==============================] - 115s 3ms/sample - loss: 0.0043 - acc: 0.9989 - val_loss: 0.2242 - val_acc: 0.9541\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0169 - acc: 0.9947\n",
      "Epoch 00118: val_loss did not improve from 0.15373\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0172 - acc: 0.9947 - val_loss: 0.1952 - val_acc: 0.9557\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0126 - acc: 0.9962\n",
      "Epoch 00119: val_loss did not improve from 0.15373\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0126 - acc: 0.9962 - val_loss: 0.1788 - val_acc: 0.9630\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0036 - acc: 0.9992\n",
      "Epoch 00120: val_loss did not improve from 0.15373\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0036 - acc: 0.9992 - val_loss: 0.1668 - val_acc: 0.9639\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0056 - acc: 0.9986\n",
      "Epoch 00121: val_loss did not improve from 0.15373\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0056 - acc: 0.9986 - val_loss: 0.1833 - val_acc: 0.9606\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0067 - acc: 0.9979\n",
      "Epoch 00122: val_loss did not improve from 0.15373\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0068 - acc: 0.9979 - val_loss: 0.1936 - val_acc: 0.9583\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0107 - acc: 0.9965\n",
      "Epoch 00123: val_loss did not improve from 0.15373\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0107 - acc: 0.9965 - val_loss: 0.1712 - val_acc: 0.9632\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0031 - acc: 0.9992\n",
      "Epoch 00124: val_loss did not improve from 0.15373\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0032 - acc: 0.9992 - val_loss: 0.1749 - val_acc: 0.9611\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0176 - acc: 0.9947\n",
      "Epoch 00125: val_loss did not improve from 0.15373\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0176 - acc: 0.9947 - val_loss: 0.1721 - val_acc: 0.9620\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0094 - acc: 0.9970\n",
      "Epoch 00126: val_loss did not improve from 0.15373\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 0.0094 - acc: 0.9970 - val_loss: 0.1840 - val_acc: 0.9578\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_ch_32_BN_12_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 9s 2ms/sample - loss: 0.2247 - acc: 0.9466\n",
      "Loss: 0.22471464893249707 Accuracy: 0.9466251\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.2481 - acc: 0.3051\n",
      "Epoch 00001: val_loss improved from inf to 1.81755, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_BN_13_conv_checkpoint/001-1.8175.hdf5\n",
      "36805/36805 [==============================] - 290s 8ms/sample - loss: 2.2481 - acc: 0.3051 - val_loss: 1.8175 - val_acc: 0.4083\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1109 - acc: 0.6427\n",
      "Epoch 00002: val_loss improved from 1.81755 to 0.88608, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_BN_13_conv_checkpoint/002-0.8861.hdf5\n",
      "36805/36805 [==============================] - 117s 3ms/sample - loss: 1.1109 - acc: 0.6427 - val_loss: 0.8861 - val_acc: 0.7193\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7133 - acc: 0.7702\n",
      "Epoch 00003: val_loss improved from 0.88608 to 0.62146, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_BN_13_conv_checkpoint/003-0.6215.hdf5\n",
      "36805/36805 [==============================] - 117s 3ms/sample - loss: 0.7133 - acc: 0.7702 - val_loss: 0.6215 - val_acc: 0.7976\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5335 - acc: 0.8292\n",
      "Epoch 00004: val_loss improved from 0.62146 to 0.50915, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_BN_13_conv_checkpoint/004-0.5092.hdf5\n",
      "36805/36805 [==============================] - 116s 3ms/sample - loss: 0.5335 - acc: 0.8292 - val_loss: 0.5092 - val_acc: 0.8325\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4153 - acc: 0.8667\n",
      "Epoch 00005: val_loss improved from 0.50915 to 0.37826, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_BN_13_conv_checkpoint/005-0.3783.hdf5\n",
      "36805/36805 [==============================] - 117s 3ms/sample - loss: 0.4153 - acc: 0.8666 - val_loss: 0.3783 - val_acc: 0.8768\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3468 - acc: 0.8873\n",
      "Epoch 00006: val_loss improved from 0.37826 to 0.33839, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_BN_13_conv_checkpoint/006-0.3384.hdf5\n",
      "36805/36805 [==============================] - 117s 3ms/sample - loss: 0.3469 - acc: 0.8873 - val_loss: 0.3384 - val_acc: 0.8931\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2940 - acc: 0.9042\n",
      "Epoch 00007: val_loss improved from 0.33839 to 0.28124, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_BN_13_conv_checkpoint/007-0.2812.hdf5\n",
      "36805/36805 [==============================] - 117s 3ms/sample - loss: 0.2941 - acc: 0.9042 - val_loss: 0.2812 - val_acc: 0.9068\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2446 - acc: 0.9202\n",
      "Epoch 00008: val_loss did not improve from 0.28124\n",
      "36805/36805 [==============================] - 116s 3ms/sample - loss: 0.2446 - acc: 0.9202 - val_loss: 0.3140 - val_acc: 0.9017\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2153 - acc: 0.9302\n",
      "Epoch 00009: val_loss improved from 0.28124 to 0.26445, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_BN_13_conv_checkpoint/009-0.2645.hdf5\n",
      "36805/36805 [==============================] - 117s 3ms/sample - loss: 0.2153 - acc: 0.9302 - val_loss: 0.2645 - val_acc: 0.9138\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1878 - acc: 0.9385\n",
      "Epoch 00010: val_loss improved from 0.26445 to 0.25951, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_BN_13_conv_checkpoint/010-0.2595.hdf5\n",
      "36805/36805 [==============================] - 117s 3ms/sample - loss: 0.1878 - acc: 0.9385 - val_loss: 0.2595 - val_acc: 0.9185\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1614 - acc: 0.9466\n",
      "Epoch 00011: val_loss improved from 0.25951 to 0.22525, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_BN_13_conv_checkpoint/011-0.2252.hdf5\n",
      "36805/36805 [==============================] - 117s 3ms/sample - loss: 0.1614 - acc: 0.9466 - val_loss: 0.2252 - val_acc: 0.9262\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1413 - acc: 0.9538\n",
      "Epoch 00012: val_loss did not improve from 0.22525\n",
      "36805/36805 [==============================] - 116s 3ms/sample - loss: 0.1415 - acc: 0.9538 - val_loss: 0.2980 - val_acc: 0.9082\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1340 - acc: 0.9569\n",
      "Epoch 00013: val_loss did not improve from 0.22525\n",
      "36805/36805 [==============================] - 116s 3ms/sample - loss: 0.1340 - acc: 0.9569 - val_loss: 0.2486 - val_acc: 0.9217\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1101 - acc: 0.9645\n",
      "Epoch 00014: val_loss did not improve from 0.22525\n",
      "36805/36805 [==============================] - 116s 3ms/sample - loss: 0.1101 - acc: 0.9645 - val_loss: 0.2295 - val_acc: 0.9248\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1014 - acc: 0.9678\n",
      "Epoch 00015: val_loss did not improve from 0.22525\n",
      "36805/36805 [==============================] - 116s 3ms/sample - loss: 0.1015 - acc: 0.9677 - val_loss: 0.2574 - val_acc: 0.9215\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0995 - acc: 0.9680\n",
      "Epoch 00016: val_loss improved from 0.22525 to 0.22183, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_BN_13_conv_checkpoint/016-0.2218.hdf5\n",
      "36805/36805 [==============================] - 117s 3ms/sample - loss: 0.0997 - acc: 0.9680 - val_loss: 0.2218 - val_acc: 0.9336\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0890 - acc: 0.9721\n",
      "Epoch 00017: val_loss did not improve from 0.22183\n",
      "36805/36805 [==============================] - 115s 3ms/sample - loss: 0.0891 - acc: 0.9721 - val_loss: 0.2718 - val_acc: 0.9194\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0765 - acc: 0.9764\n",
      "Epoch 00018: val_loss did not improve from 0.22183\n",
      "36805/36805 [==============================] - 115s 3ms/sample - loss: 0.0765 - acc: 0.9764 - val_loss: 0.2247 - val_acc: 0.9338\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0707 - acc: 0.9780\n",
      "Epoch 00019: val_loss improved from 0.22183 to 0.21854, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_BN_13_conv_checkpoint/019-0.2185.hdf5\n",
      "36805/36805 [==============================] - 117s 3ms/sample - loss: 0.0707 - acc: 0.9780 - val_loss: 0.2185 - val_acc: 0.9357\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0587 - acc: 0.9823\n",
      "Epoch 00020: val_loss improved from 0.21854 to 0.21421, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_BN_13_conv_checkpoint/020-0.2142.hdf5\n",
      "36805/36805 [==============================] - 116s 3ms/sample - loss: 0.0587 - acc: 0.9823 - val_loss: 0.2142 - val_acc: 0.9371\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0549 - acc: 0.9829\n",
      "Epoch 00021: val_loss did not improve from 0.21421\n",
      "36805/36805 [==============================] - 116s 3ms/sample - loss: 0.0550 - acc: 0.9828 - val_loss: 0.2274 - val_acc: 0.9341\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0617 - acc: 0.9801\n",
      "Epoch 00022: val_loss did not improve from 0.21421\n",
      "36805/36805 [==============================] - 116s 3ms/sample - loss: 0.0617 - acc: 0.9801 - val_loss: 0.2324 - val_acc: 0.9313\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0443 - acc: 0.9873\n",
      "Epoch 00023: val_loss did not improve from 0.21421\n",
      "36805/36805 [==============================] - 116s 3ms/sample - loss: 0.0443 - acc: 0.9873 - val_loss: 0.2267 - val_acc: 0.9357\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0430 - acc: 0.9868\n",
      "Epoch 00024: val_loss did not improve from 0.21421\n",
      "36805/36805 [==============================] - 116s 3ms/sample - loss: 0.0431 - acc: 0.9867 - val_loss: 0.2709 - val_acc: 0.9250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0611 - acc: 0.9800\n",
      "Epoch 00025: val_loss improved from 0.21421 to 0.19495, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_BN_13_conv_checkpoint/025-0.1949.hdf5\n",
      "36805/36805 [==============================] - 117s 3ms/sample - loss: 0.0611 - acc: 0.9800 - val_loss: 0.1949 - val_acc: 0.9462\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0332 - acc: 0.9908\n",
      "Epoch 00026: val_loss did not improve from 0.19495\n",
      "36805/36805 [==============================] - 116s 3ms/sample - loss: 0.0332 - acc: 0.9908 - val_loss: 0.2388 - val_acc: 0.9380\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0388 - acc: 0.9882\n",
      "Epoch 00027: val_loss did not improve from 0.19495\n",
      "36805/36805 [==============================] - 116s 3ms/sample - loss: 0.0388 - acc: 0.9882 - val_loss: 0.2430 - val_acc: 0.9341\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0367 - acc: 0.9890\n",
      "Epoch 00028: val_loss did not improve from 0.19495\n",
      "36805/36805 [==============================] - 116s 3ms/sample - loss: 0.0367 - acc: 0.9890 - val_loss: 0.2205 - val_acc: 0.9406\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0414 - acc: 0.9868\n",
      "Epoch 00029: val_loss did not improve from 0.19495\n",
      "36805/36805 [==============================] - 117s 3ms/sample - loss: 0.0414 - acc: 0.9868 - val_loss: 0.2149 - val_acc: 0.9413\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0260 - acc: 0.9932\n",
      "Epoch 00030: val_loss did not improve from 0.19495\n",
      "36805/36805 [==============================] - 117s 3ms/sample - loss: 0.0260 - acc: 0.9932 - val_loss: 0.2480 - val_acc: 0.9313\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0285 - acc: 0.9918\n",
      "Epoch 00031: val_loss did not improve from 0.19495\n",
      "36805/36805 [==============================] - 117s 3ms/sample - loss: 0.0286 - acc: 0.9918 - val_loss: 0.2484 - val_acc: 0.9399\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0417 - acc: 0.9874\n",
      "Epoch 00032: val_loss did not improve from 0.19495\n",
      "36805/36805 [==============================] - 117s 3ms/sample - loss: 0.0417 - acc: 0.9874 - val_loss: 0.2399 - val_acc: 0.9329\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0231 - acc: 0.9934\n",
      "Epoch 00033: val_loss did not improve from 0.19495\n",
      "36805/36805 [==============================] - 116s 3ms/sample - loss: 0.0232 - acc: 0.9934 - val_loss: 0.2321 - val_acc: 0.9439\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0310 - acc: 0.9907\n",
      "Epoch 00034: val_loss did not improve from 0.19495\n",
      "36805/36805 [==============================] - 116s 3ms/sample - loss: 0.0310 - acc: 0.9907 - val_loss: 0.2111 - val_acc: 0.9422\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0208 - acc: 0.9940\n",
      "Epoch 00035: val_loss did not improve from 0.19495\n",
      "36805/36805 [==============================] - 116s 3ms/sample - loss: 0.0208 - acc: 0.9940 - val_loss: 0.2815 - val_acc: 0.9271\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0263 - acc: 0.9916\n",
      "Epoch 00036: val_loss did not improve from 0.19495\n",
      "36805/36805 [==============================] - 117s 3ms/sample - loss: 0.0263 - acc: 0.9916 - val_loss: 0.2285 - val_acc: 0.9429\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0233 - acc: 0.9930\n",
      "Epoch 00037: val_loss did not improve from 0.19495\n",
      "36805/36805 [==============================] - 116s 3ms/sample - loss: 0.0233 - acc: 0.9930 - val_loss: 0.2458 - val_acc: 0.9411\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0180 - acc: 0.9947\n",
      "Epoch 00038: val_loss did not improve from 0.19495\n",
      "36805/36805 [==============================] - 117s 3ms/sample - loss: 0.0180 - acc: 0.9947 - val_loss: 0.2183 - val_acc: 0.9464\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0313 - acc: 0.9898\n",
      "Epoch 00039: val_loss did not improve from 0.19495\n",
      "36805/36805 [==============================] - 117s 3ms/sample - loss: 0.0315 - acc: 0.9897 - val_loss: 0.2396 - val_acc: 0.9383\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0286 - acc: 0.9910\n",
      "Epoch 00040: val_loss did not improve from 0.19495\n",
      "36805/36805 [==============================] - 117s 3ms/sample - loss: 0.0286 - acc: 0.9910 - val_loss: 0.2232 - val_acc: 0.9432\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0138 - acc: 0.9965\n",
      "Epoch 00041: val_loss did not improve from 0.19495\n",
      "36805/36805 [==============================] - 117s 3ms/sample - loss: 0.0138 - acc: 0.9965 - val_loss: 0.2076 - val_acc: 0.9476\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0161 - acc: 0.9954\n",
      "Epoch 00042: val_loss did not improve from 0.19495\n",
      "36805/36805 [==============================] - 117s 3ms/sample - loss: 0.0161 - acc: 0.9954 - val_loss: 0.2913 - val_acc: 0.9311\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0199 - acc: 0.9937\n",
      "Epoch 00043: val_loss did not improve from 0.19495\n",
      "36805/36805 [==============================] - 116s 3ms/sample - loss: 0.0199 - acc: 0.9937 - val_loss: 0.2476 - val_acc: 0.9380\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0198 - acc: 0.9940\n",
      "Epoch 00044: val_loss did not improve from 0.19495\n",
      "36805/36805 [==============================] - 116s 3ms/sample - loss: 0.0198 - acc: 0.9940 - val_loss: 0.2463 - val_acc: 0.9420\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0214 - acc: 0.9933\n",
      "Epoch 00045: val_loss did not improve from 0.19495\n",
      "36805/36805 [==============================] - 116s 3ms/sample - loss: 0.0214 - acc: 0.9933 - val_loss: 0.2062 - val_acc: 0.9502\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0125 - acc: 0.9965\n",
      "Epoch 00046: val_loss did not improve from 0.19495\n",
      "36805/36805 [==============================] - 116s 3ms/sample - loss: 0.0125 - acc: 0.9965 - val_loss: 0.2180 - val_acc: 0.9495\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0126 - acc: 0.9961\n",
      "Epoch 00047: val_loss did not improve from 0.19495\n",
      "36805/36805 [==============================] - 116s 3ms/sample - loss: 0.0126 - acc: 0.9961 - val_loss: 0.2506 - val_acc: 0.9450\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0130 - acc: 0.9961\n",
      "Epoch 00048: val_loss did not improve from 0.19495\n",
      "36805/36805 [==============================] - 117s 3ms/sample - loss: 0.0131 - acc: 0.9961 - val_loss: 0.2354 - val_acc: 0.9448\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0283 - acc: 0.9912\n",
      "Epoch 00049: val_loss did not improve from 0.19495\n",
      "36805/36805 [==============================] - 117s 3ms/sample - loss: 0.0285 - acc: 0.9912 - val_loss: 0.2493 - val_acc: 0.9408\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0273 - acc: 0.9915\n",
      "Epoch 00050: val_loss did not improve from 0.19495\n",
      "36805/36805 [==============================] - 117s 3ms/sample - loss: 0.0273 - acc: 0.9915 - val_loss: 0.2197 - val_acc: 0.9462\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0095 - acc: 0.9974\n",
      "Epoch 00051: val_loss did not improve from 0.19495\n",
      "36805/36805 [==============================] - 117s 3ms/sample - loss: 0.0095 - acc: 0.9974 - val_loss: 0.2529 - val_acc: 0.9434\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0118 - acc: 0.9967\n",
      "Epoch 00052: val_loss did not improve from 0.19495\n",
      "36805/36805 [==============================] - 117s 3ms/sample - loss: 0.0118 - acc: 0.9967 - val_loss: 0.2540 - val_acc: 0.9387\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0102 - acc: 0.9971\n",
      "Epoch 00053: val_loss did not improve from 0.19495\n",
      "36805/36805 [==============================] - 117s 3ms/sample - loss: 0.0102 - acc: 0.9971 - val_loss: 0.2254 - val_acc: 0.9469\n",
      "Epoch 54/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0113 - acc: 0.9968\n",
      "Epoch 00054: val_loss did not improve from 0.19495\n",
      "36805/36805 [==============================] - 116s 3ms/sample - loss: 0.0114 - acc: 0.9968 - val_loss: 0.2607 - val_acc: 0.9448\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0271 - acc: 0.9911\n",
      "Epoch 00055: val_loss did not improve from 0.19495\n",
      "36805/36805 [==============================] - 116s 3ms/sample - loss: 0.0271 - acc: 0.9911 - val_loss: 0.2319 - val_acc: 0.9436\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0094 - acc: 0.9976\n",
      "Epoch 00056: val_loss did not improve from 0.19495\n",
      "36805/36805 [==============================] - 117s 3ms/sample - loss: 0.0094 - acc: 0.9976 - val_loss: 0.2521 - val_acc: 0.9413\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0132 - acc: 0.9957\n",
      "Epoch 00057: val_loss did not improve from 0.19495\n",
      "36805/36805 [==============================] - 117s 3ms/sample - loss: 0.0135 - acc: 0.9957 - val_loss: 0.2974 - val_acc: 0.9376\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0264 - acc: 0.9914\n",
      "Epoch 00058: val_loss did not improve from 0.19495\n",
      "36805/36805 [==============================] - 117s 3ms/sample - loss: 0.0266 - acc: 0.9914 - val_loss: 0.2105 - val_acc: 0.9497\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0179 - acc: 0.9943\n",
      "Epoch 00059: val_loss did not improve from 0.19495\n",
      "36805/36805 [==============================] - 117s 3ms/sample - loss: 0.0179 - acc: 0.9943 - val_loss: 0.2049 - val_acc: 0.9532\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0064 - acc: 0.9985\n",
      "Epoch 00060: val_loss did not improve from 0.19495\n",
      "36805/36805 [==============================] - 116s 3ms/sample - loss: 0.0064 - acc: 0.9985 - val_loss: 0.2181 - val_acc: 0.9495\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0063 - acc: 0.9982\n",
      "Epoch 00061: val_loss did not improve from 0.19495\n",
      "36805/36805 [==============================] - 116s 3ms/sample - loss: 0.0064 - acc: 0.9982 - val_loss: 0.2615 - val_acc: 0.9420\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0294 - acc: 0.9914\n",
      "Epoch 00062: val_loss did not improve from 0.19495\n",
      "36805/36805 [==============================] - 117s 3ms/sample - loss: 0.0294 - acc: 0.9914 - val_loss: 0.2306 - val_acc: 0.9439\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0053 - acc: 0.9988\n",
      "Epoch 00063: val_loss did not improve from 0.19495\n",
      "36805/36805 [==============================] - 117s 3ms/sample - loss: 0.0053 - acc: 0.9988 - val_loss: 0.1984 - val_acc: 0.9529\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0053 - acc: 0.9985\n",
      "Epoch 00064: val_loss did not improve from 0.19495\n",
      "36805/36805 [==============================] - 117s 3ms/sample - loss: 0.0054 - acc: 0.9985 - val_loss: 0.2385 - val_acc: 0.9476\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0229 - acc: 0.9928\n",
      "Epoch 00065: val_loss did not improve from 0.19495\n",
      "36805/36805 [==============================] - 116s 3ms/sample - loss: 0.0229 - acc: 0.9928 - val_loss: 0.2198 - val_acc: 0.9499\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0106 - acc: 0.9971\n",
      "Epoch 00066: val_loss did not improve from 0.19495\n",
      "36805/36805 [==============================] - 117s 3ms/sample - loss: 0.0106 - acc: 0.9971 - val_loss: 0.2416 - val_acc: 0.9495\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0072 - acc: 0.9982\n",
      "Epoch 00067: val_loss did not improve from 0.19495\n",
      "36805/36805 [==============================] - 117s 3ms/sample - loss: 0.0073 - acc: 0.9982 - val_loss: 0.2195 - val_acc: 0.9511\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0224 - acc: 0.9922\n",
      "Epoch 00068: val_loss did not improve from 0.19495\n",
      "36805/36805 [==============================] - 116s 3ms/sample - loss: 0.0223 - acc: 0.9922 - val_loss: 0.2150 - val_acc: 0.9497\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0088 - acc: 0.9975\n",
      "Epoch 00069: val_loss did not improve from 0.19495\n",
      "36805/36805 [==============================] - 117s 3ms/sample - loss: 0.0089 - acc: 0.9974 - val_loss: 0.2242 - val_acc: 0.9478\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0161 - acc: 0.9954\n",
      "Epoch 00070: val_loss did not improve from 0.19495\n",
      "36805/36805 [==============================] - 117s 3ms/sample - loss: 0.0161 - acc: 0.9954 - val_loss: 0.2309 - val_acc: 0.9488\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0054 - acc: 0.9987\n",
      "Epoch 00071: val_loss improved from 0.19495 to 0.19435, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_BN_13_conv_checkpoint/071-0.1943.hdf5\n",
      "36805/36805 [==============================] - 117s 3ms/sample - loss: 0.0054 - acc: 0.9987 - val_loss: 0.1943 - val_acc: 0.9562\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0050 - acc: 0.9986\n",
      "Epoch 00072: val_loss did not improve from 0.19435\n",
      "36805/36805 [==============================] - 116s 3ms/sample - loss: 0.0050 - acc: 0.9986 - val_loss: 0.2167 - val_acc: 0.9520\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0130 - acc: 0.9963\n",
      "Epoch 00073: val_loss did not improve from 0.19435\n",
      "36805/36805 [==============================] - 116s 3ms/sample - loss: 0.0131 - acc: 0.9963 - val_loss: 0.3001 - val_acc: 0.9364\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0207 - acc: 0.9934\n",
      "Epoch 00074: val_loss did not improve from 0.19435\n",
      "36805/36805 [==============================] - 117s 3ms/sample - loss: 0.0207 - acc: 0.9934 - val_loss: 0.2196 - val_acc: 0.9497\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0070 - acc: 0.9980\n",
      "Epoch 00075: val_loss did not improve from 0.19435\n",
      "36805/36805 [==============================] - 116s 3ms/sample - loss: 0.0070 - acc: 0.9980 - val_loss: 0.2251 - val_acc: 0.9518\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0105 - acc: 0.9968\n",
      "Epoch 00076: val_loss did not improve from 0.19435\n",
      "36805/36805 [==============================] - 116s 3ms/sample - loss: 0.0105 - acc: 0.9967 - val_loss: 0.2469 - val_acc: 0.9495\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0173 - acc: 0.9948\n",
      "Epoch 00077: val_loss did not improve from 0.19435\n",
      "36805/36805 [==============================] - 116s 3ms/sample - loss: 0.0174 - acc: 0.9948 - val_loss: 0.2145 - val_acc: 0.9506\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0180 - acc: 0.9946\n",
      "Epoch 00078: val_loss did not improve from 0.19435\n",
      "36805/36805 [==============================] - 116s 3ms/sample - loss: 0.0180 - acc: 0.9946 - val_loss: 0.2066 - val_acc: 0.9553\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0155 - acc: 0.9951\n",
      "Epoch 00079: val_loss did not improve from 0.19435\n",
      "36805/36805 [==============================] - 116s 3ms/sample - loss: 0.0155 - acc: 0.9951 - val_loss: 0.2056 - val_acc: 0.9520\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0112 - acc: 0.9967\n",
      "Epoch 00080: val_loss improved from 0.19435 to 0.18747, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_pool_2_ch_32_BN_13_conv_checkpoint/080-0.1875.hdf5\n",
      "36805/36805 [==============================] - 116s 3ms/sample - loss: 0.0114 - acc: 0.9967 - val_loss: 0.1875 - val_acc: 0.9569\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0189 - acc: 0.9940\n",
      "Epoch 00081: val_loss did not improve from 0.18747\n",
      "36805/36805 [==============================] - 116s 3ms/sample - loss: 0.0189 - acc: 0.9940 - val_loss: 0.1903 - val_acc: 0.9560\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0063 - acc: 0.9984\n",
      "Epoch 00082: val_loss did not improve from 0.18747\n",
      "36805/36805 [==============================] - 116s 3ms/sample - loss: 0.0063 - acc: 0.9984 - val_loss: 0.2265 - val_acc: 0.9497\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0047 - acc: 0.9988\n",
      "Epoch 00083: val_loss did not improve from 0.18747\n",
      "36805/36805 [==============================] - 116s 3ms/sample - loss: 0.0047 - acc: 0.9988 - val_loss: 0.1976 - val_acc: 0.9576\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0088 - acc: 0.9972\n",
      "Epoch 00084: val_loss did not improve from 0.18747\n",
      "36805/36805 [==============================] - 117s 3ms/sample - loss: 0.0088 - acc: 0.9972 - val_loss: 0.2111 - val_acc: 0.9520\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0079 - acc: 0.9976\n",
      "Epoch 00085: val_loss did not improve from 0.18747\n",
      "36805/36805 [==============================] - 119s 3ms/sample - loss: 0.0079 - acc: 0.9976 - val_loss: 0.2370 - val_acc: 0.9497\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0079 - acc: 0.9979\n",
      "Epoch 00086: val_loss did not improve from 0.18747\n",
      "36805/36805 [==============================] - 118s 3ms/sample - loss: 0.0079 - acc: 0.9979 - val_loss: 0.2563 - val_acc: 0.9453\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0100 - acc: 0.9970\n",
      "Epoch 00087: val_loss did not improve from 0.18747\n",
      "36805/36805 [==============================] - 119s 3ms/sample - loss: 0.0101 - acc: 0.9970 - val_loss: 0.2298 - val_acc: 0.9488\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0189 - acc: 0.9935\n",
      "Epoch 00088: val_loss did not improve from 0.18747\n",
      "36805/36805 [==============================] - 119s 3ms/sample - loss: 0.0189 - acc: 0.9935 - val_loss: 0.2151 - val_acc: 0.9534\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0053 - acc: 0.9983\n",
      "Epoch 00089: val_loss did not improve from 0.18747\n",
      "36805/36805 [==============================] - 119s 3ms/sample - loss: 0.0056 - acc: 0.9983 - val_loss: 0.2346 - val_acc: 0.9467\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0165 - acc: 0.9947\n",
      "Epoch 00090: val_loss did not improve from 0.18747\n",
      "36805/36805 [==============================] - 119s 3ms/sample - loss: 0.0166 - acc: 0.9947 - val_loss: 0.2014 - val_acc: 0.9562\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0088 - acc: 0.9976\n",
      "Epoch 00091: val_loss did not improve from 0.18747\n",
      "36805/36805 [==============================] - 119s 3ms/sample - loss: 0.0088 - acc: 0.9976 - val_loss: 0.1964 - val_acc: 0.9574\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0104 - acc: 0.9972\n",
      "Epoch 00092: val_loss did not improve from 0.18747\n",
      "36805/36805 [==============================] - 119s 3ms/sample - loss: 0.0104 - acc: 0.9972 - val_loss: 0.2064 - val_acc: 0.9562\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0030 - acc: 0.9992\n",
      "Epoch 00093: val_loss did not improve from 0.18747\n",
      "36805/36805 [==============================] - 119s 3ms/sample - loss: 0.0030 - acc: 0.9992 - val_loss: 0.1998 - val_acc: 0.9581\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0041 - acc: 0.9988\n",
      "Epoch 00094: val_loss did not improve from 0.18747\n",
      "36805/36805 [==============================] - 119s 3ms/sample - loss: 0.0041 - acc: 0.9988 - val_loss: 0.2409 - val_acc: 0.9525\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0123 - acc: 0.9961- ETA: 0s - loss: 0.0123 - acc: 0.996\n",
      "Epoch 00095: val_loss did not improve from 0.18747\n",
      "36805/36805 [==============================] - 119s 3ms/sample - loss: 0.0123 - acc: 0.9961 - val_loss: 0.2226 - val_acc: 0.9520\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0066 - acc: 0.9980\n",
      "Epoch 00096: val_loss did not improve from 0.18747\n",
      "36805/36805 [==============================] - 119s 3ms/sample - loss: 0.0069 - acc: 0.9980 - val_loss: 0.2350 - val_acc: 0.9560\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0175 - acc: 0.9946\n",
      "Epoch 00097: val_loss did not improve from 0.18747\n",
      "36805/36805 [==============================] - 119s 3ms/sample - loss: 0.0175 - acc: 0.9946 - val_loss: 0.2159 - val_acc: 0.9515\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0112 - acc: 0.9967\n",
      "Epoch 00098: val_loss did not improve from 0.18747\n",
      "36805/36805 [==============================] - 118s 3ms/sample - loss: 0.0113 - acc: 0.9967 - val_loss: 0.1971 - val_acc: 0.9541\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0056 - acc: 0.9986\n",
      "Epoch 00099: val_loss did not improve from 0.18747\n",
      "36805/36805 [==============================] - 116s 3ms/sample - loss: 0.0056 - acc: 0.9986 - val_loss: 0.1926 - val_acc: 0.9585\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0024 - acc: 0.9996\n",
      "Epoch 00100: val_loss did not improve from 0.18747\n",
      "36805/36805 [==============================] - 116s 3ms/sample - loss: 0.0024 - acc: 0.9996 - val_loss: 0.1963 - val_acc: 0.9569\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0024 - acc: 0.9994\n",
      "Epoch 00101: val_loss did not improve from 0.18747\n",
      "36805/36805 [==============================] - 116s 3ms/sample - loss: 0.0024 - acc: 0.9994 - val_loss: 0.2309 - val_acc: 0.9529\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0109 - acc: 0.9966\n",
      "Epoch 00102: val_loss did not improve from 0.18747\n",
      "36805/36805 [==============================] - 116s 3ms/sample - loss: 0.0109 - acc: 0.9966 - val_loss: 0.2415 - val_acc: 0.9506\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0074 - acc: 0.9975\n",
      "Epoch 00103: val_loss did not improve from 0.18747\n",
      "36805/36805 [==============================] - 116s 3ms/sample - loss: 0.0074 - acc: 0.9975 - val_loss: 0.2332 - val_acc: 0.9506\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0175 - acc: 0.9944\n",
      "Epoch 00104: val_loss did not improve from 0.18747\n",
      "36805/36805 [==============================] - 116s 3ms/sample - loss: 0.0175 - acc: 0.9943 - val_loss: 0.2060 - val_acc: 0.9541\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0117 - acc: 0.9965\n",
      "Epoch 00105: val_loss did not improve from 0.18747\n",
      "36805/36805 [==============================] - 116s 3ms/sample - loss: 0.0117 - acc: 0.9965 - val_loss: 0.2205 - val_acc: 0.9550\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0054 - acc: 0.9984\n",
      "Epoch 00106: val_loss did not improve from 0.18747\n",
      "36805/36805 [==============================] - 116s 3ms/sample - loss: 0.0054 - acc: 0.9984 - val_loss: 0.1923 - val_acc: 0.9557\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0100 - acc: 0.9969\n",
      "Epoch 00107: val_loss did not improve from 0.18747\n",
      "36805/36805 [==============================] - 116s 3ms/sample - loss: 0.0100 - acc: 0.9969 - val_loss: 0.2292 - val_acc: 0.9478\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0040 - acc: 0.9989\n",
      "Epoch 00108: val_loss did not improve from 0.18747\n",
      "36805/36805 [==============================] - 116s 3ms/sample - loss: 0.0040 - acc: 0.9989 - val_loss: 0.2162 - val_acc: 0.9557\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0042 - acc: 0.9988\n",
      "Epoch 00109: val_loss did not improve from 0.18747\n",
      "36805/36805 [==============================] - 117s 3ms/sample - loss: 0.0042 - acc: 0.9988 - val_loss: 0.2607 - val_acc: 0.9483\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0139 - acc: 0.9956\n",
      "Epoch 00110: val_loss did not improve from 0.18747\n",
      "36805/36805 [==============================] - 116s 3ms/sample - loss: 0.0139 - acc: 0.9956 - val_loss: 0.2327 - val_acc: 0.9520\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0083 - acc: 0.9973\n",
      "Epoch 00111: val_loss did not improve from 0.18747\n",
      "36805/36805 [==============================] - 117s 3ms/sample - loss: 0.0083 - acc: 0.9973 - val_loss: 0.2059 - val_acc: 0.9548\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9995\n",
      "Epoch 00112: val_loss did not improve from 0.18747\n",
      "36805/36805 [==============================] - 116s 3ms/sample - loss: 0.0022 - acc: 0.9995 - val_loss: 0.2201 - val_acc: 0.9532\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0072 - acc: 0.9979\n",
      "Epoch 00113: val_loss did not improve from 0.18747\n",
      "36805/36805 [==============================] - 116s 3ms/sample - loss: 0.0079 - acc: 0.9979 - val_loss: 0.3521 - val_acc: 0.9308\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0171 - acc: 0.9942\n",
      "Epoch 00114: val_loss did not improve from 0.18747\n",
      "36805/36805 [==============================] - 116s 3ms/sample - loss: 0.0171 - acc: 0.9941 - val_loss: 0.2269 - val_acc: 0.9502\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0104 - acc: 0.9966\n",
      "Epoch 00115: val_loss did not improve from 0.18747\n",
      "36805/36805 [==============================] - 116s 3ms/sample - loss: 0.0104 - acc: 0.9966 - val_loss: 0.1937 - val_acc: 0.9574\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0027 - acc: 0.9992\n",
      "Epoch 00116: val_loss did not improve from 0.18747\n",
      "36805/36805 [==============================] - 117s 3ms/sample - loss: 0.0029 - acc: 0.9992 - val_loss: 0.2500 - val_acc: 0.9522\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0105 - acc: 0.9968\n",
      "Epoch 00117: val_loss did not improve from 0.18747\n",
      "36805/36805 [==============================] - 116s 3ms/sample - loss: 0.0105 - acc: 0.9968 - val_loss: 0.2019 - val_acc: 0.9583\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0028 - acc: 0.9994\n",
      "Epoch 00118: val_loss did not improve from 0.18747\n",
      "36805/36805 [==============================] - 116s 3ms/sample - loss: 0.0028 - acc: 0.9994 - val_loss: 0.1985 - val_acc: 0.9618\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0087 - acc: 0.9973\n",
      "Epoch 00119: val_loss did not improve from 0.18747\n",
      "36805/36805 [==============================] - 116s 3ms/sample - loss: 0.0087 - acc: 0.9973 - val_loss: 0.2138 - val_acc: 0.9534\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0042 - acc: 0.9987\n",
      "Epoch 00120: val_loss did not improve from 0.18747\n",
      "36805/36805 [==============================] - 116s 3ms/sample - loss: 0.0044 - acc: 0.9987 - val_loss: 0.2329 - val_acc: 0.9513\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0135 - acc: 0.9955\n",
      "Epoch 00121: val_loss did not improve from 0.18747\n",
      "36805/36805 [==============================] - 117s 3ms/sample - loss: 0.0135 - acc: 0.9955 - val_loss: 0.2034 - val_acc: 0.9562\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0032 - acc: 0.9991\n",
      "Epoch 00122: val_loss did not improve from 0.18747\n",
      "36805/36805 [==============================] - 117s 3ms/sample - loss: 0.0033 - acc: 0.9991 - val_loss: 0.2121 - val_acc: 0.9539\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0133 - acc: 0.9954\n",
      "Epoch 00123: val_loss did not improve from 0.18747\n",
      "36805/36805 [==============================] - 116s 3ms/sample - loss: 0.0133 - acc: 0.9954 - val_loss: 0.2173 - val_acc: 0.9529\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0029 - acc: 0.9992\n",
      "Epoch 00124: val_loss did not improve from 0.18747\n",
      "36805/36805 [==============================] - 116s 3ms/sample - loss: 0.0030 - acc: 0.9992 - val_loss: 0.2130 - val_acc: 0.9534\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0153 - acc: 0.9955\n",
      "Epoch 00125: val_loss did not improve from 0.18747\n",
      "36805/36805 [==============================] - 116s 3ms/sample - loss: 0.0154 - acc: 0.9955 - val_loss: 0.2114 - val_acc: 0.9529\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0089 - acc: 0.9974\n",
      "Epoch 00126: val_loss did not improve from 0.18747\n",
      "36805/36805 [==============================] - 116s 3ms/sample - loss: 0.0089 - acc: 0.9973 - val_loss: 0.2046 - val_acc: 0.9595\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0057 - acc: 0.9983\n",
      "Epoch 00127: val_loss did not improve from 0.18747\n",
      "36805/36805 [==============================] - 116s 3ms/sample - loss: 0.0057 - acc: 0.9983 - val_loss: 0.2049 - val_acc: 0.9578\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0019 - acc: 0.9996\n",
      "Epoch 00128: val_loss did not improve from 0.18747\n",
      "36805/36805 [==============================] - 116s 3ms/sample - loss: 0.0019 - acc: 0.9996 - val_loss: 0.1967 - val_acc: 0.9613\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0017 - acc: 0.9996\n",
      "Epoch 00129: val_loss did not improve from 0.18747\n",
      "36805/36805 [==============================] - 116s 3ms/sample - loss: 0.0017 - acc: 0.9996 - val_loss: 0.2045 - val_acc: 0.9576\n",
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0051 - acc: 0.9987\n",
      "Epoch 00130: val_loss did not improve from 0.18747\n",
      "36805/36805 [==============================] - 117s 3ms/sample - loss: 0.0053 - acc: 0.9986 - val_loss: 0.2337 - val_acc: 0.9525\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_ch_32_BN_13_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 9s 2ms/sample - loss: 0.2468 - acc: 0.9466\n",
      "Loss: 0.2468474530619748 Accuracy: 0.9466251\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base = '1D_CNN_custom_conv_3_VGG_pool_2_ch_32_BN'\n",
    "\n",
    "for i in range(3, 14):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    model = build_1d_cnn_custom_conv_3_VGG_BN(conv_num=i)\n",
    "#         model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4),\n",
    "          metrics=['accuracy'])\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model_filename = model_path+'{epoch:03d}-{val_loss:.4f}.hdf5'\n",
    "    checkpointer = ModelCheckpoint(filepath = model_filename, monitor = \"val_loss\", \n",
    "                                   verbose=1, save_best_only=True)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=50)\n",
    "    hist = model.fit(x_train_abs, y_train_onehot, batch_size=64, epochs=500, \n",
    "                     validation_data=[x_val_abs, y_val_onehot], shuffle=True, \n",
    "                     callbacks = [checkpointer, early_stopping])\n",
    "\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "    ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "    ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "    ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    png_path = 'visualization/learning_curve/'\n",
    "    filename = model_name+'.png'\n",
    "    os.makedirs(png_path, exist_ok=True)\n",
    "    fig.savefig(png_path+filename, transparent=True)\n",
    "\n",
    "    model.save(model_path+'000_last.hdf5')\n",
    "    del(model)\n",
    "\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "    model = load_model(model_filename)\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "    print()\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_ch_32_BN_3_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_182 (Conv1D)          (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_195 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_182 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_183 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_196 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_183 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_184 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_197 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_184 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_185 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_198 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_185 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_78 (MaxPooling (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_186 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_199 ( (None, 8000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_186 (Activation)  (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_187 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_200 ( (None, 8000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_187 (Activation)  (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_79 (MaxPooling (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 128000)            0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_201 ( (None, 128000)            512000    \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 16)                2048016   \n",
      "=================================================================\n",
      "Total params: 2,576,432\n",
      "Trainable params: 2,320,048\n",
      "Non-trainable params: 256,384\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 7s 1ms/sample - loss: 2.3763 - acc: 0.4390\n",
      "Loss: 2.3762977059385118 Accuracy: 0.43904465\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_ch_32_BN_4_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_188 (Conv1D)          (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_202 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_188 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_189 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_203 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_189 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_190 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_204 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_190 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_191 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_205 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_191 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_80 (MaxPooling (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_192 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_206 ( (None, 8000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_192 (Activation)  (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_193 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_207 ( (None, 8000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_193 (Activation)  (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_81 (MaxPooling (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_194 (Conv1D)          (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_208 ( (None, 4000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_194 (Activation)  (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_195 (Conv1D)          (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_209 ( (None, 4000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_195 (Activation)  (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_82 (MaxPooling (None, 2000, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_14 (Flatten)         (None, 64000)             0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_210 ( (None, 64000)             256000    \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 16)                1024016   \n",
      "=================================================================\n",
      "Total params: 1,302,896\n",
      "Trainable params: 1,174,384\n",
      "Non-trainable params: 128,512\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 8s 2ms/sample - loss: 1.7769 - acc: 0.5090\n",
      "Loss: 1.7768519674134775 Accuracy: 0.5090343\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_ch_32_BN_5_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_196 (Conv1D)          (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_211 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_196 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_197 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_212 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_197 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_198 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_213 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_198 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_199 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_214 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_199 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_83 (MaxPooling (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_200 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_215 ( (None, 8000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_200 (Activation)  (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_201 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_216 ( (None, 8000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_201 (Activation)  (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_84 (MaxPooling (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_202 (Conv1D)          (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_217 ( (None, 4000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_202 (Activation)  (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_203 (Conv1D)          (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_218 ( (None, 4000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_203 (Activation)  (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_85 (MaxPooling (None, 2000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_204 (Conv1D)          (None, 2000, 64)          6208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_219 ( (None, 2000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_204 (Activation)  (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_205 (Conv1D)          (None, 2000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_220 ( (None, 2000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_205 (Activation)  (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_86 (MaxPooling (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_15 (Flatten)         (None, 64000)             0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_221 ( (None, 64000)             256000    \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 16)                1024016   \n",
      "=================================================================\n",
      "Total params: 1,321,968\n",
      "Trainable params: 1,193,200\n",
      "Non-trainable params: 128,768\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 9s 2ms/sample - loss: 1.5917 - acc: 0.5634\n",
      "Loss: 1.591745559201186 Accuracy: 0.56344754\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_ch_32_BN_6_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_206 (Conv1D)          (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_222 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_206 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_207 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_223 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_207 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_208 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_224 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_208 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_209 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_225 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_209 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_87 (MaxPooling (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_210 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_226 ( (None, 8000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_210 (Activation)  (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_211 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_227 ( (None, 8000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_211 (Activation)  (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_88 (MaxPooling (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_212 (Conv1D)          (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_228 ( (None, 4000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_212 (Activation)  (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_213 (Conv1D)          (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_229 ( (None, 4000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_213 (Activation)  (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_89 (MaxPooling (None, 2000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_214 (Conv1D)          (None, 2000, 64)          6208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_230 ( (None, 2000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_214 (Activation)  (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_215 (Conv1D)          (None, 2000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_231 ( (None, 2000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_215 (Activation)  (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_90 (MaxPooling (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_216 (Conv1D)          (None, 1000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_232 ( (None, 1000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_216 (Activation)  (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_217 (Conv1D)          (None, 1000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_233 ( (None, 1000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_217 (Activation)  (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_91 (MaxPooling (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_16 (Flatten)         (None, 32000)             0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_234 ( (None, 32000)             128000    \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 16)                512016    \n",
      "=================================================================\n",
      "Total params: 707,184\n",
      "Trainable params: 642,160\n",
      "Non-trainable params: 65,024\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 9s 2ms/sample - loss: 1.2420 - acc: 0.6478\n",
      "Loss: 1.2419819713505382 Accuracy: 0.64776736\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_ch_32_BN_7_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_218 (Conv1D)          (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_235 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_218 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_219 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_236 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_219 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_220 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_237 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_220 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_221 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_238 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_221 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_92 (MaxPooling (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_222 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_239 ( (None, 8000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_222 (Activation)  (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_223 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_240 ( (None, 8000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_223 (Activation)  (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_93 (MaxPooling (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_224 (Conv1D)          (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_241 ( (None, 4000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_224 (Activation)  (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_225 (Conv1D)          (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_242 ( (None, 4000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_225 (Activation)  (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_94 (MaxPooling (None, 2000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_226 (Conv1D)          (None, 2000, 64)          6208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_243 ( (None, 2000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_226 (Activation)  (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_227 (Conv1D)          (None, 2000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_244 ( (None, 2000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_227 (Activation)  (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_95 (MaxPooling (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_228 (Conv1D)          (None, 1000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_245 ( (None, 1000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_228 (Activation)  (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_229 (Conv1D)          (None, 1000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_246 ( (None, 1000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_229 (Activation)  (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_96 (MaxPooling (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_230 (Conv1D)          (None, 500, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_247 ( (None, 500, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_230 (Activation)  (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_231 (Conv1D)          (None, 500, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_248 ( (None, 500, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_231 (Activation)  (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_97 (MaxPooling (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_17 (Flatten)         (None, 16000)             0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_249 ( (None, 16000)             64000     \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 16)                256016    \n",
      "=================================================================\n",
      "Total params: 412,400\n",
      "Trainable params: 379,120\n",
      "Non-trainable params: 33,280\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 9s 2ms/sample - loss: 0.9970 - acc: 0.7086\n",
      "Loss: 0.996963969544582 Accuracy: 0.7086189\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_ch_32_BN_8_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_232 (Conv1D)          (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_250 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_232 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_233 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_251 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_233 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_234 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_252 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_234 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_235 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_253 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_235 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_98 (MaxPooling (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_236 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_254 ( (None, 8000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_236 (Activation)  (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_237 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_255 ( (None, 8000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_237 (Activation)  (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_99 (MaxPooling (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_238 (Conv1D)          (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_256 ( (None, 4000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_238 (Activation)  (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_239 (Conv1D)          (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_257 ( (None, 4000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_239 (Activation)  (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_100 (MaxPoolin (None, 2000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_240 (Conv1D)          (None, 2000, 64)          6208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_258 ( (None, 2000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_240 (Activation)  (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_241 (Conv1D)          (None, 2000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_259 ( (None, 2000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_241 (Activation)  (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_101 (MaxPoolin (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_242 (Conv1D)          (None, 1000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_260 ( (None, 1000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_242 (Activation)  (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_243 (Conv1D)          (None, 1000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_261 ( (None, 1000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_243 (Activation)  (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_102 (MaxPoolin (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_244 (Conv1D)          (None, 500, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_262 ( (None, 500, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_244 (Activation)  (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_245 (Conv1D)          (None, 500, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_263 ( (None, 500, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_245 (Activation)  (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_103 (MaxPoolin (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_246 (Conv1D)          (None, 250, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_264 ( (None, 250, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_246 (Activation)  (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_247 (Conv1D)          (None, 250, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_265 ( (None, 250, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_247 (Activation)  (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_104 (MaxPoolin (None, 125, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_18 (Flatten)         (None, 8000)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_266 ( (None, 8000)              32000     \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 16)                128016    \n",
      "=================================================================\n",
      "Total params: 277,616\n",
      "Trainable params: 260,080\n",
      "Non-trainable params: 17,536\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 10s 2ms/sample - loss: 0.6865 - acc: 0.8123\n",
      "Loss: 0.6865143270631198 Accuracy: 0.81225336\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_ch_32_BN_9_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_248 (Conv1D)          (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_267 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_248 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_249 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_268 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_249 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_250 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_269 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_250 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_251 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_270 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_251 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_105 (MaxPoolin (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_252 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_271 ( (None, 8000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_252 (Activation)  (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_253 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_272 ( (None, 8000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_253 (Activation)  (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_106 (MaxPoolin (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_254 (Conv1D)          (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_273 ( (None, 4000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_254 (Activation)  (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_255 (Conv1D)          (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_274 ( (None, 4000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_255 (Activation)  (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_107 (MaxPoolin (None, 2000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_256 (Conv1D)          (None, 2000, 64)          6208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_275 ( (None, 2000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_256 (Activation)  (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_257 (Conv1D)          (None, 2000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_276 ( (None, 2000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_257 (Activation)  (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_108 (MaxPoolin (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_258 (Conv1D)          (None, 1000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_277 ( (None, 1000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_258 (Activation)  (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_259 (Conv1D)          (None, 1000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_278 ( (None, 1000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_259 (Activation)  (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_109 (MaxPoolin (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_260 (Conv1D)          (None, 500, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_279 ( (None, 500, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_260 (Activation)  (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_261 (Conv1D)          (None, 500, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_280 ( (None, 500, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_261 (Activation)  (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_110 (MaxPoolin (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_262 (Conv1D)          (None, 250, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_281 ( (None, 250, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_262 (Activation)  (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_263 (Conv1D)          (None, 250, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_282 ( (None, 250, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_263 (Activation)  (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_111 (MaxPoolin (None, 125, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_264 (Conv1D)          (None, 125, 128)          24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_283 ( (None, 125, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_264 (Activation)  (None, 125, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_265 (Conv1D)          (None, 125, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_284 ( (None, 125, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_265 (Activation)  (None, 125, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_112 (MaxPoolin (None, 62, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_19 (Flatten)         (None, 7936)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_285 ( (None, 7936)              31744     \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 16)                126992    \n",
      "=================================================================\n",
      "Total params: 351,344\n",
      "Trainable params: 333,424\n",
      "Non-trainable params: 17,920\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 10s 2ms/sample - loss: 0.4907 - acc: 0.8613\n",
      "Loss: 0.4906679555139314 Accuracy: 0.86126685\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_ch_32_BN_10_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_266 (Conv1D)          (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_286 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_266 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_267 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_287 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_267 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_268 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_288 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_268 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_269 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_289 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_269 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_113 (MaxPoolin (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_270 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_290 ( (None, 8000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_270 (Activation)  (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_271 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_291 ( (None, 8000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_271 (Activation)  (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_114 (MaxPoolin (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_272 (Conv1D)          (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_292 ( (None, 4000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_272 (Activation)  (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_273 (Conv1D)          (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_293 ( (None, 4000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_273 (Activation)  (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_115 (MaxPoolin (None, 2000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_274 (Conv1D)          (None, 2000, 64)          6208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_294 ( (None, 2000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_274 (Activation)  (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_275 (Conv1D)          (None, 2000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_295 ( (None, 2000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_275 (Activation)  (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_116 (MaxPoolin (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_276 (Conv1D)          (None, 1000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_296 ( (None, 1000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_276 (Activation)  (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_277 (Conv1D)          (None, 1000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_297 ( (None, 1000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_277 (Activation)  (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_117 (MaxPoolin (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_278 (Conv1D)          (None, 500, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_298 ( (None, 500, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_278 (Activation)  (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_279 (Conv1D)          (None, 500, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_299 ( (None, 500, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_279 (Activation)  (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_118 (MaxPoolin (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_280 (Conv1D)          (None, 250, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_300 ( (None, 250, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_280 (Activation)  (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_281 (Conv1D)          (None, 250, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_301 ( (None, 250, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_281 (Activation)  (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_119 (MaxPoolin (None, 125, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_282 (Conv1D)          (None, 125, 128)          24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_302 ( (None, 125, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_282 (Activation)  (None, 125, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_283 (Conv1D)          (None, 125, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_303 ( (None, 125, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_283 (Activation)  (None, 125, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_120 (MaxPoolin (None, 62, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_284 (Conv1D)          (None, 62, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_304 ( (None, 62, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_284 (Activation)  (None, 62, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_285 (Conv1D)          (None, 62, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_305 ( (None, 62, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_285 (Activation)  (None, 62, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_121 (MaxPoolin (None, 31, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_20 (Flatten)         (None, 3968)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_306 ( (None, 3968)              15872     \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 16)                63504     \n",
      "=================================================================\n",
      "Total params: 371,568\n",
      "Trainable params: 361,072\n",
      "Non-trainable params: 10,496\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 11s 2ms/sample - loss: 0.3123 - acc: 0.9281\n",
      "Loss: 0.31234766842791595 Accuracy: 0.92814124\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_ch_32_BN_11_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_286 (Conv1D)          (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_307 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_286 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_287 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_308 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_287 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_288 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_309 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_288 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_289 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_310 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_289 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_122 (MaxPoolin (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_290 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_311 ( (None, 8000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_290 (Activation)  (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_291 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_312 ( (None, 8000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_291 (Activation)  (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_123 (MaxPoolin (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_292 (Conv1D)          (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_313 ( (None, 4000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_292 (Activation)  (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_293 (Conv1D)          (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_314 ( (None, 4000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_293 (Activation)  (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_124 (MaxPoolin (None, 2000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_294 (Conv1D)          (None, 2000, 64)          6208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_315 ( (None, 2000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_294 (Activation)  (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_295 (Conv1D)          (None, 2000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_316 ( (None, 2000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_295 (Activation)  (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_125 (MaxPoolin (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_296 (Conv1D)          (None, 1000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_317 ( (None, 1000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_296 (Activation)  (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_297 (Conv1D)          (None, 1000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_318 ( (None, 1000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_297 (Activation)  (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_126 (MaxPoolin (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_298 (Conv1D)          (None, 500, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_319 ( (None, 500, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_298 (Activation)  (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_299 (Conv1D)          (None, 500, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_320 ( (None, 500, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_299 (Activation)  (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_127 (MaxPoolin (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_300 (Conv1D)          (None, 250, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_321 ( (None, 250, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_300 (Activation)  (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_301 (Conv1D)          (None, 250, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_322 ( (None, 250, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_301 (Activation)  (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_128 (MaxPoolin (None, 125, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_302 (Conv1D)          (None, 125, 128)          24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_323 ( (None, 125, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_302 (Activation)  (None, 125, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_303 (Conv1D)          (None, 125, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_324 ( (None, 125, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_303 (Activation)  (None, 125, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_129 (MaxPoolin (None, 62, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_304 (Conv1D)          (None, 62, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_325 ( (None, 62, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_304 (Activation)  (None, 62, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_305 (Conv1D)          (None, 62, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_326 ( (None, 62, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_305 (Activation)  (None, 62, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_130 (MaxPoolin (None, 31, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_306 (Conv1D)          (None, 31, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_327 ( (None, 31, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_306 (Activation)  (None, 31, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_307 (Conv1D)          (None, 31, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_328 ( (None, 31, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_307 (Activation)  (None, 31, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_131 (MaxPoolin (None, 15, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_21 (Flatten)         (None, 1920)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_329 ( (None, 1920)              7680      \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 16)                30736     \n",
      "=================================================================\n",
      "Total params: 430,192\n",
      "Trainable params: 423,280\n",
      "Non-trainable params: 6,912\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 11s 2ms/sample - loss: 0.2364 - acc: 0.9317\n",
      "Loss: 0.23642852384662827 Accuracy: 0.93167186\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_ch_32_BN_12_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_308 (Conv1D)          (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_330 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_308 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_309 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_331 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_309 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_310 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_332 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_310 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_311 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_333 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_311 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_132 (MaxPoolin (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_312 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_334 ( (None, 8000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_312 (Activation)  (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_313 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_335 ( (None, 8000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_313 (Activation)  (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_133 (MaxPoolin (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_314 (Conv1D)          (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_336 ( (None, 4000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_314 (Activation)  (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_315 (Conv1D)          (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_337 ( (None, 4000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_315 (Activation)  (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_134 (MaxPoolin (None, 2000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_316 (Conv1D)          (None, 2000, 64)          6208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_338 ( (None, 2000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_316 (Activation)  (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_317 (Conv1D)          (None, 2000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_339 ( (None, 2000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_317 (Activation)  (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_135 (MaxPoolin (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_318 (Conv1D)          (None, 1000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_340 ( (None, 1000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_318 (Activation)  (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_319 (Conv1D)          (None, 1000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_341 ( (None, 1000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_319 (Activation)  (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_136 (MaxPoolin (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_320 (Conv1D)          (None, 500, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_342 ( (None, 500, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_320 (Activation)  (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_321 (Conv1D)          (None, 500, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_343 ( (None, 500, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_321 (Activation)  (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_137 (MaxPoolin (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_322 (Conv1D)          (None, 250, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_344 ( (None, 250, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_322 (Activation)  (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_323 (Conv1D)          (None, 250, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_345 ( (None, 250, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_323 (Activation)  (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_138 (MaxPoolin (None, 125, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_324 (Conv1D)          (None, 125, 128)          24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_346 ( (None, 125, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_324 (Activation)  (None, 125, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_325 (Conv1D)          (None, 125, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_347 ( (None, 125, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_325 (Activation)  (None, 125, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_139 (MaxPoolin (None, 62, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_326 (Conv1D)          (None, 62, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_348 ( (None, 62, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_326 (Activation)  (None, 62, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_327 (Conv1D)          (None, 62, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_349 ( (None, 62, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_327 (Activation)  (None, 62, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_140 (MaxPoolin (None, 31, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_328 (Conv1D)          (None, 31, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_350 ( (None, 31, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_328 (Activation)  (None, 31, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_329 (Conv1D)          (None, 31, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_351 ( (None, 31, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_329 (Activation)  (None, 31, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_141 (MaxPoolin (None, 15, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_330 (Conv1D)          (None, 15, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_352 ( (None, 15, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_330 (Activation)  (None, 15, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_331 (Conv1D)          (None, 15, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_353 ( (None, 15, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_331 (Activation)  (None, 15, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_142 (MaxPoolin (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_22 (Flatten)         (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_354 ( (None, 896)               3584      \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 16)                14352     \n",
      "=================================================================\n",
      "Total params: 509,296\n",
      "Trainable params: 503,920\n",
      "Non-trainable params: 5,376\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 11s 2ms/sample - loss: 0.2247 - acc: 0.9466\n",
      "Loss: 0.22471464893249707 Accuracy: 0.9466251\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_ch_32_BN_13_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_332 (Conv1D)          (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_355 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_332 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_333 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_356 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_333 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_334 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_357 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_334 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_335 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_358 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_335 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_143 (MaxPoolin (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_336 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_359 ( (None, 8000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_336 (Activation)  (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_337 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_360 ( (None, 8000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_337 (Activation)  (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_144 (MaxPoolin (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_338 (Conv1D)          (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_361 ( (None, 4000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_338 (Activation)  (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_339 (Conv1D)          (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_362 ( (None, 4000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_339 (Activation)  (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_145 (MaxPoolin (None, 2000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_340 (Conv1D)          (None, 2000, 64)          6208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_363 ( (None, 2000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_340 (Activation)  (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_341 (Conv1D)          (None, 2000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_364 ( (None, 2000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_341 (Activation)  (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_146 (MaxPoolin (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_342 (Conv1D)          (None, 1000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_365 ( (None, 1000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_342 (Activation)  (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_343 (Conv1D)          (None, 1000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_366 ( (None, 1000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_343 (Activation)  (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_147 (MaxPoolin (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_344 (Conv1D)          (None, 500, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_367 ( (None, 500, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_344 (Activation)  (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_345 (Conv1D)          (None, 500, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_368 ( (None, 500, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_345 (Activation)  (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_148 (MaxPoolin (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_346 (Conv1D)          (None, 250, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_369 ( (None, 250, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_346 (Activation)  (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_347 (Conv1D)          (None, 250, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_370 ( (None, 250, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_347 (Activation)  (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_149 (MaxPoolin (None, 125, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_348 (Conv1D)          (None, 125, 128)          24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_371 ( (None, 125, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_348 (Activation)  (None, 125, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_349 (Conv1D)          (None, 125, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_372 ( (None, 125, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_349 (Activation)  (None, 125, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_150 (MaxPoolin (None, 62, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_350 (Conv1D)          (None, 62, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_373 ( (None, 62, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_350 (Activation)  (None, 62, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_351 (Conv1D)          (None, 62, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_374 ( (None, 62, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_351 (Activation)  (None, 62, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_151 (MaxPoolin (None, 31, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_352 (Conv1D)          (None, 31, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_375 ( (None, 31, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_352 (Activation)  (None, 31, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_353 (Conv1D)          (None, 31, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_376 ( (None, 31, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_353 (Activation)  (None, 31, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_152 (MaxPoolin (None, 15, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_354 (Conv1D)          (None, 15, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_377 ( (None, 15, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_354 (Activation)  (None, 15, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_355 (Conv1D)          (None, 15, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_378 ( (None, 15, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_355 (Activation)  (None, 15, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_153 (MaxPoolin (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_356 (Conv1D)          (None, 7, 256)            98560     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_379 ( (None, 7, 256)            1024      \n",
      "_________________________________________________________________\n",
      "activation_356 (Activation)  (None, 7, 256)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_357 (Conv1D)          (None, 7, 256)            196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_380 ( (None, 7, 256)            1024      \n",
      "_________________________________________________________________\n",
      "activation_357 (Activation)  (None, 7, 256)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_154 (MaxPoolin (None, 3, 256)            0         \n",
      "_________________________________________________________________\n",
      "flatten_23 (Flatten)         (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_381 ( (None, 768)               3072      \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 16)                12304     \n",
      "=================================================================\n",
      "Total params: 804,208\n",
      "Trainable params: 798,064\n",
      "Non-trainable params: 6,144\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 12s 2ms/sample - loss: 0.2468 - acc: 0.9466\n",
      "Loss: 0.2468474530619748 Accuracy: 0.9466251\n"
     ]
    }
   ],
   "source": [
    "log_dir = 'log'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "base = '1D_CNN_custom_conv_3_VGG_pool_2_ch_32_BN'\n",
    "\n",
    "with open(path.join(log_dir, base), 'w') as log_file:\n",
    "    for i in range(3, 14):\n",
    "        model_name = base+'_{}_conv'.format(i)\n",
    "        print()\n",
    "        print(model_name, 'Model')\n",
    "        model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "        model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "\n",
    "        model = load_model(model_filename)\n",
    "        model.summary()\n",
    "\n",
    "        [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "        print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "        del(model)\n",
    "\n",
    "        log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_ch_32_BN_3_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_182 (Conv1D)          (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_195 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_182 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_183 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_196 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_183 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_184 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_197 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_184 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_185 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_198 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_185 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_78 (MaxPooling (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_186 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_199 ( (None, 8000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_186 (Activation)  (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_187 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_200 ( (None, 8000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_187 (Activation)  (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_79 (MaxPooling (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 128000)            0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_201 ( (None, 128000)            512000    \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 16)                2048016   \n",
      "=================================================================\n",
      "Total params: 2,576,432\n",
      "Trainable params: 2,320,048\n",
      "Non-trainable params: 256,384\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 10s 2ms/sample - loss: 5.4164 - acc: 0.4201\n",
      "Loss: 5.416360019770986 Accuracy: 0.4201454\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_ch_32_BN_4_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_188 (Conv1D)          (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_202 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_188 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_189 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_203 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_189 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_190 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_204 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_190 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_191 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_205 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_191 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_80 (MaxPooling (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_192 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_206 ( (None, 8000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_192 (Activation)  (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_193 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_207 ( (None, 8000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_193 (Activation)  (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_81 (MaxPooling (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_194 (Conv1D)          (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_208 ( (None, 4000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_194 (Activation)  (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_195 (Conv1D)          (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_209 ( (None, 4000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_195 (Activation)  (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_82 (MaxPooling (None, 2000, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_14 (Flatten)         (None, 64000)             0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_210 ( (None, 64000)             256000    \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 16)                1024016   \n",
      "=================================================================\n",
      "Total params: 1,302,896\n",
      "Trainable params: 1,174,384\n",
      "Non-trainable params: 128,512\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 11s 2ms/sample - loss: 3.5136 - acc: 0.5115\n",
      "Loss: 3.513641679076515 Accuracy: 0.51152647\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_ch_32_BN_5_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_196 (Conv1D)          (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_211 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_196 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_197 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_212 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_197 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_198 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_213 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_198 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_199 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_214 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_199 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_83 (MaxPooling (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_200 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_215 ( (None, 8000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_200 (Activation)  (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_201 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_216 ( (None, 8000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_201 (Activation)  (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_84 (MaxPooling (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_202 (Conv1D)          (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_217 ( (None, 4000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_202 (Activation)  (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_203 (Conv1D)          (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_218 ( (None, 4000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_203 (Activation)  (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_85 (MaxPooling (None, 2000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_204 (Conv1D)          (None, 2000, 64)          6208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_219 ( (None, 2000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_204 (Activation)  (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_205 (Conv1D)          (None, 2000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_220 ( (None, 2000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_205 (Activation)  (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_86 (MaxPooling (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_15 (Flatten)         (None, 64000)             0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_221 ( (None, 64000)             256000    \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 16)                1024016   \n",
      "=================================================================\n",
      "Total params: 1,321,968\n",
      "Trainable params: 1,193,200\n",
      "Non-trainable params: 128,768\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 12s 2ms/sample - loss: 3.3140 - acc: 0.5497\n",
      "Loss: 3.313974734135011 Accuracy: 0.5497404\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_ch_32_BN_6_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_206 (Conv1D)          (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_222 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_206 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_207 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_223 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_207 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_208 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_224 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_208 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_209 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_225 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_209 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_87 (MaxPooling (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_210 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_226 ( (None, 8000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_210 (Activation)  (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_211 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_227 ( (None, 8000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_211 (Activation)  (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_88 (MaxPooling (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_212 (Conv1D)          (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_228 ( (None, 4000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_212 (Activation)  (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_213 (Conv1D)          (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_229 ( (None, 4000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_213 (Activation)  (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_89 (MaxPooling (None, 2000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_214 (Conv1D)          (None, 2000, 64)          6208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_230 ( (None, 2000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_214 (Activation)  (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_215 (Conv1D)          (None, 2000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_231 ( (None, 2000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_215 (Activation)  (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_90 (MaxPooling (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_216 (Conv1D)          (None, 1000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_232 ( (None, 1000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_216 (Activation)  (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_217 (Conv1D)          (None, 1000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_233 ( (None, 1000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_217 (Activation)  (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_91 (MaxPooling (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_16 (Flatten)         (None, 32000)             0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_234 ( (None, 32000)             128000    \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 16)                512016    \n",
      "=================================================================\n",
      "Total params: 707,184\n",
      "Trainable params: 642,160\n",
      "Non-trainable params: 65,024\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 12s 3ms/sample - loss: 2.1443 - acc: 0.6459\n",
      "Loss: 2.144323431665652 Accuracy: 0.6458982\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_ch_32_BN_7_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_218 (Conv1D)          (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_235 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_218 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_219 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_236 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_219 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_220 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_237 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_220 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_221 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_238 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_221 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_92 (MaxPooling (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_222 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_239 ( (None, 8000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_222 (Activation)  (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_223 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_240 ( (None, 8000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_223 (Activation)  (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_93 (MaxPooling (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_224 (Conv1D)          (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_241 ( (None, 4000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_224 (Activation)  (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_225 (Conv1D)          (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_242 ( (None, 4000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_225 (Activation)  (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_94 (MaxPooling (None, 2000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_226 (Conv1D)          (None, 2000, 64)          6208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_243 ( (None, 2000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_226 (Activation)  (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_227 (Conv1D)          (None, 2000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_244 ( (None, 2000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_227 (Activation)  (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_95 (MaxPooling (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_228 (Conv1D)          (None, 1000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_245 ( (None, 1000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_228 (Activation)  (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_229 (Conv1D)          (None, 1000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_246 ( (None, 1000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_229 (Activation)  (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_96 (MaxPooling (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_230 (Conv1D)          (None, 500, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_247 ( (None, 500, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_230 (Activation)  (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_231 (Conv1D)          (None, 500, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_248 ( (None, 500, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_231 (Activation)  (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_97 (MaxPooling (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_17 (Flatten)         (None, 16000)             0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_249 ( (None, 16000)             64000     \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 16)                256016    \n",
      "=================================================================\n",
      "Total params: 412,400\n",
      "Trainable params: 379,120\n",
      "Non-trainable params: 33,280\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 13s 3ms/sample - loss: 1.4852 - acc: 0.7252\n",
      "Loss: 1.4851543031377585 Accuracy: 0.7252337\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_ch_32_BN_8_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_232 (Conv1D)          (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_250 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_232 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_233 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_251 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_233 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_234 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_252 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_234 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_235 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_253 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_235 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_98 (MaxPooling (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_236 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_254 ( (None, 8000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_236 (Activation)  (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_237 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_255 ( (None, 8000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_237 (Activation)  (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_99 (MaxPooling (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_238 (Conv1D)          (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_256 ( (None, 4000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_238 (Activation)  (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_239 (Conv1D)          (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_257 ( (None, 4000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_239 (Activation)  (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_100 (MaxPoolin (None, 2000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_240 (Conv1D)          (None, 2000, 64)          6208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_258 ( (None, 2000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_240 (Activation)  (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_241 (Conv1D)          (None, 2000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_259 ( (None, 2000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_241 (Activation)  (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_101 (MaxPoolin (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_242 (Conv1D)          (None, 1000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_260 ( (None, 1000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_242 (Activation)  (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_243 (Conv1D)          (None, 1000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_261 ( (None, 1000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_243 (Activation)  (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_102 (MaxPoolin (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_244 (Conv1D)          (None, 500, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_262 ( (None, 500, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_244 (Activation)  (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_245 (Conv1D)          (None, 500, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_263 ( (None, 500, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_245 (Activation)  (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_103 (MaxPoolin (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_246 (Conv1D)          (None, 250, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_264 ( (None, 250, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_246 (Activation)  (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_247 (Conv1D)          (None, 250, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_265 ( (None, 250, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_247 (Activation)  (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_104 (MaxPoolin (None, 125, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_18 (Flatten)         (None, 8000)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_266 ( (None, 8000)              32000     \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 16)                128016    \n",
      "=================================================================\n",
      "Total params: 277,616\n",
      "Trainable params: 260,080\n",
      "Non-trainable params: 17,536\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 13s 3ms/sample - loss: 0.8582 - acc: 0.8249\n",
      "Loss: 0.8581665458956488 Accuracy: 0.82492214\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_ch_32_BN_9_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_248 (Conv1D)          (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_267 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_248 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_249 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_268 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_249 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_250 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_269 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_250 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_251 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_270 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_251 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_105 (MaxPoolin (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_252 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_271 ( (None, 8000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_252 (Activation)  (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_253 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_272 ( (None, 8000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_253 (Activation)  (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_106 (MaxPoolin (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_254 (Conv1D)          (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_273 ( (None, 4000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_254 (Activation)  (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_255 (Conv1D)          (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_274 ( (None, 4000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_255 (Activation)  (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_107 (MaxPoolin (None, 2000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_256 (Conv1D)          (None, 2000, 64)          6208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_275 ( (None, 2000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_256 (Activation)  (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_257 (Conv1D)          (None, 2000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_276 ( (None, 2000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_257 (Activation)  (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_108 (MaxPoolin (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_258 (Conv1D)          (None, 1000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_277 ( (None, 1000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_258 (Activation)  (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_259 (Conv1D)          (None, 1000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_278 ( (None, 1000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_259 (Activation)  (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_109 (MaxPoolin (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_260 (Conv1D)          (None, 500, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_279 ( (None, 500, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_260 (Activation)  (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_261 (Conv1D)          (None, 500, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_280 ( (None, 500, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_261 (Activation)  (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_110 (MaxPoolin (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_262 (Conv1D)          (None, 250, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_281 ( (None, 250, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_262 (Activation)  (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_263 (Conv1D)          (None, 250, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_282 ( (None, 250, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_263 (Activation)  (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_111 (MaxPoolin (None, 125, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_264 (Conv1D)          (None, 125, 128)          24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_283 ( (None, 125, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_264 (Activation)  (None, 125, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_265 (Conv1D)          (None, 125, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_284 ( (None, 125, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_265 (Activation)  (None, 125, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_112 (MaxPoolin (None, 62, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_19 (Flatten)         (None, 7936)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_285 ( (None, 7936)              31744     \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 16)                126992    \n",
      "=================================================================\n",
      "Total params: 351,344\n",
      "Trainable params: 333,424\n",
      "Non-trainable params: 17,920\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 14s 3ms/sample - loss: 0.6301 - acc: 0.8694\n",
      "Loss: 0.6301451848612891 Accuracy: 0.8693666\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_ch_32_BN_10_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_266 (Conv1D)          (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_286 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_266 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_267 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_287 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_267 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_268 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_288 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_268 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_269 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_289 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_269 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_113 (MaxPoolin (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_270 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_290 ( (None, 8000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_270 (Activation)  (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_271 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_291 ( (None, 8000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_271 (Activation)  (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_114 (MaxPoolin (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_272 (Conv1D)          (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_292 ( (None, 4000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_272 (Activation)  (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_273 (Conv1D)          (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_293 ( (None, 4000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_273 (Activation)  (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_115 (MaxPoolin (None, 2000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_274 (Conv1D)          (None, 2000, 64)          6208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_294 ( (None, 2000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_274 (Activation)  (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_275 (Conv1D)          (None, 2000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_295 ( (None, 2000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_275 (Activation)  (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_116 (MaxPoolin (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_276 (Conv1D)          (None, 1000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_296 ( (None, 1000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_276 (Activation)  (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_277 (Conv1D)          (None, 1000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_297 ( (None, 1000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_277 (Activation)  (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_117 (MaxPoolin (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_278 (Conv1D)          (None, 500, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_298 ( (None, 500, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_278 (Activation)  (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_279 (Conv1D)          (None, 500, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_299 ( (None, 500, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_279 (Activation)  (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_118 (MaxPoolin (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_280 (Conv1D)          (None, 250, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_300 ( (None, 250, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_280 (Activation)  (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_281 (Conv1D)          (None, 250, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_301 ( (None, 250, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_281 (Activation)  (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_119 (MaxPoolin (None, 125, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_282 (Conv1D)          (None, 125, 128)          24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_302 ( (None, 125, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_282 (Activation)  (None, 125, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_283 (Conv1D)          (None, 125, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_303 ( (None, 125, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_283 (Activation)  (None, 125, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_120 (MaxPoolin (None, 62, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_284 (Conv1D)          (None, 62, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_304 ( (None, 62, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_284 (Activation)  (None, 62, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_285 (Conv1D)          (None, 62, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_305 ( (None, 62, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_285 (Activation)  (None, 62, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_121 (MaxPoolin (None, 31, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_20 (Flatten)         (None, 3968)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_306 ( (None, 3968)              15872     \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 16)                63504     \n",
      "=================================================================\n",
      "Total params: 371,568\n",
      "Trainable params: 361,072\n",
      "Non-trainable params: 10,496\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 14s 3ms/sample - loss: 0.3344 - acc: 0.9279\n",
      "Loss: 0.33438433541965634 Accuracy: 0.9279335\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_ch_32_BN_11_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_286 (Conv1D)          (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_307 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_286 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_287 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_308 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_287 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_288 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_309 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_288 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_289 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_310 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_289 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_122 (MaxPoolin (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_290 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_311 ( (None, 8000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_290 (Activation)  (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_291 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_312 ( (None, 8000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_291 (Activation)  (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_123 (MaxPoolin (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_292 (Conv1D)          (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_313 ( (None, 4000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_292 (Activation)  (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_293 (Conv1D)          (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_314 ( (None, 4000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_293 (Activation)  (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_124 (MaxPoolin (None, 2000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_294 (Conv1D)          (None, 2000, 64)          6208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_315 ( (None, 2000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_294 (Activation)  (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_295 (Conv1D)          (None, 2000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_316 ( (None, 2000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_295 (Activation)  (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_125 (MaxPoolin (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_296 (Conv1D)          (None, 1000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_317 ( (None, 1000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_296 (Activation)  (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_297 (Conv1D)          (None, 1000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_318 ( (None, 1000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_297 (Activation)  (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_126 (MaxPoolin (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_298 (Conv1D)          (None, 500, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_319 ( (None, 500, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_298 (Activation)  (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_299 (Conv1D)          (None, 500, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_320 ( (None, 500, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_299 (Activation)  (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_127 (MaxPoolin (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_300 (Conv1D)          (None, 250, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_321 ( (None, 250, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_300 (Activation)  (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_301 (Conv1D)          (None, 250, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_322 ( (None, 250, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_301 (Activation)  (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_128 (MaxPoolin (None, 125, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_302 (Conv1D)          (None, 125, 128)          24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_323 ( (None, 125, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_302 (Activation)  (None, 125, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_303 (Conv1D)          (None, 125, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_324 ( (None, 125, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_303 (Activation)  (None, 125, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_129 (MaxPoolin (None, 62, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_304 (Conv1D)          (None, 62, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_325 ( (None, 62, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_304 (Activation)  (None, 62, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_305 (Conv1D)          (None, 62, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_326 ( (None, 62, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_305 (Activation)  (None, 62, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_130 (MaxPoolin (None, 31, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_306 (Conv1D)          (None, 31, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_327 ( (None, 31, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_306 (Activation)  (None, 31, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_307 (Conv1D)          (None, 31, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_328 ( (None, 31, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_307 (Activation)  (None, 31, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_131 (MaxPoolin (None, 15, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_21 (Flatten)         (None, 1920)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_329 ( (None, 1920)              7680      \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 16)                30736     \n",
      "=================================================================\n",
      "Total params: 430,192\n",
      "Trainable params: 423,280\n",
      "Non-trainable params: 6,912\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 14s 3ms/sample - loss: 0.2348 - acc: 0.9445\n",
      "Loss: 0.23478842405051087 Accuracy: 0.9445483\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_ch_32_BN_12_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_308 (Conv1D)          (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_330 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_308 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_309 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_331 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_309 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_310 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_332 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_310 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_311 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_333 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_311 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_132 (MaxPoolin (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_312 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_334 ( (None, 8000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_312 (Activation)  (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_313 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_335 ( (None, 8000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_313 (Activation)  (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_133 (MaxPoolin (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_314 (Conv1D)          (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_336 ( (None, 4000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_314 (Activation)  (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_315 (Conv1D)          (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_337 ( (None, 4000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_315 (Activation)  (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_134 (MaxPoolin (None, 2000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_316 (Conv1D)          (None, 2000, 64)          6208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_338 ( (None, 2000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_316 (Activation)  (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_317 (Conv1D)          (None, 2000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_339 ( (None, 2000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_317 (Activation)  (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_135 (MaxPoolin (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_318 (Conv1D)          (None, 1000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_340 ( (None, 1000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_318 (Activation)  (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_319 (Conv1D)          (None, 1000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_341 ( (None, 1000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_319 (Activation)  (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_136 (MaxPoolin (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_320 (Conv1D)          (None, 500, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_342 ( (None, 500, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_320 (Activation)  (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_321 (Conv1D)          (None, 500, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_343 ( (None, 500, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_321 (Activation)  (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_137 (MaxPoolin (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_322 (Conv1D)          (None, 250, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_344 ( (None, 250, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_322 (Activation)  (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_323 (Conv1D)          (None, 250, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_345 ( (None, 250, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_323 (Activation)  (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_138 (MaxPoolin (None, 125, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_324 (Conv1D)          (None, 125, 128)          24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_346 ( (None, 125, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_324 (Activation)  (None, 125, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_325 (Conv1D)          (None, 125, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_347 ( (None, 125, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_325 (Activation)  (None, 125, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_139 (MaxPoolin (None, 62, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_326 (Conv1D)          (None, 62, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_348 ( (None, 62, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_326 (Activation)  (None, 62, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_327 (Conv1D)          (None, 62, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_349 ( (None, 62, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_327 (Activation)  (None, 62, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_140 (MaxPoolin (None, 31, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_328 (Conv1D)          (None, 31, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_350 ( (None, 31, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_328 (Activation)  (None, 31, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_329 (Conv1D)          (None, 31, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_351 ( (None, 31, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_329 (Activation)  (None, 31, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_141 (MaxPoolin (None, 15, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_330 (Conv1D)          (None, 15, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_352 ( (None, 15, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_330 (Activation)  (None, 15, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_331 (Conv1D)          (None, 15, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_353 ( (None, 15, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_331 (Activation)  (None, 15, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_142 (MaxPoolin (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_22 (Flatten)         (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_354 ( (None, 896)               3584      \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 16)                14352     \n",
      "=================================================================\n",
      "Total params: 509,296\n",
      "Trainable params: 503,920\n",
      "Non-trainable params: 5,376\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 15s 3ms/sample - loss: 0.2314 - acc: 0.9489\n",
      "Loss: 0.23144411892859365 Accuracy: 0.94890964\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_pool_2_ch_32_BN_13_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_332 (Conv1D)          (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_355 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_332 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_333 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_356 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_333 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_334 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_357 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_334 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_335 (Conv1D)          (None, 16000, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_358 ( (None, 16000, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_335 (Activation)  (None, 16000, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_143 (MaxPoolin (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_336 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_359 ( (None, 8000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_336 (Activation)  (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_337 (Conv1D)          (None, 8000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_360 ( (None, 8000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_337 (Activation)  (None, 8000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_144 (MaxPoolin (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_338 (Conv1D)          (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_361 ( (None, 4000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_338 (Activation)  (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_339 (Conv1D)          (None, 4000, 32)          3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_362 ( (None, 4000, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_339 (Activation)  (None, 4000, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_145 (MaxPoolin (None, 2000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_340 (Conv1D)          (None, 2000, 64)          6208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_363 ( (None, 2000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_340 (Activation)  (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_341 (Conv1D)          (None, 2000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_364 ( (None, 2000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_341 (Activation)  (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_146 (MaxPoolin (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_342 (Conv1D)          (None, 1000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_365 ( (None, 1000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_342 (Activation)  (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_343 (Conv1D)          (None, 1000, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_366 ( (None, 1000, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_343 (Activation)  (None, 1000, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_147 (MaxPoolin (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_344 (Conv1D)          (None, 500, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_367 ( (None, 500, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_344 (Activation)  (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_345 (Conv1D)          (None, 500, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_368 ( (None, 500, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_345 (Activation)  (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_148 (MaxPoolin (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_346 (Conv1D)          (None, 250, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_369 ( (None, 250, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_346 (Activation)  (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_347 (Conv1D)          (None, 250, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_370 ( (None, 250, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_347 (Activation)  (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_149 (MaxPoolin (None, 125, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_348 (Conv1D)          (None, 125, 128)          24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_371 ( (None, 125, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_348 (Activation)  (None, 125, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_349 (Conv1D)          (None, 125, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_372 ( (None, 125, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_349 (Activation)  (None, 125, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_150 (MaxPoolin (None, 62, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_350 (Conv1D)          (None, 62, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_373 ( (None, 62, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_350 (Activation)  (None, 62, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_351 (Conv1D)          (None, 62, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_374 ( (None, 62, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_351 (Activation)  (None, 62, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_151 (MaxPoolin (None, 31, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_352 (Conv1D)          (None, 31, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_375 ( (None, 31, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_352 (Activation)  (None, 31, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_353 (Conv1D)          (None, 31, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_376 ( (None, 31, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_353 (Activation)  (None, 31, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_152 (MaxPoolin (None, 15, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_354 (Conv1D)          (None, 15, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_377 ( (None, 15, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_354 (Activation)  (None, 15, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_355 (Conv1D)          (None, 15, 128)           49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_378 ( (None, 15, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_355 (Activation)  (None, 15, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_153 (MaxPoolin (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_356 (Conv1D)          (None, 7, 256)            98560     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_379 ( (None, 7, 256)            1024      \n",
      "_________________________________________________________________\n",
      "activation_356 (Activation)  (None, 7, 256)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_357 (Conv1D)          (None, 7, 256)            196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_380 ( (None, 7, 256)            1024      \n",
      "_________________________________________________________________\n",
      "activation_357 (Activation)  (None, 7, 256)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_154 (MaxPoolin (None, 3, 256)            0         \n",
      "_________________________________________________________________\n",
      "flatten_23 (Flatten)         (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_381 ( (None, 768)               3072      \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 16)                12304     \n",
      "=================================================================\n",
      "Total params: 804,208\n",
      "Trainable params: 798,064\n",
      "Non-trainable params: 6,144\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 16s 3ms/sample - loss: 0.3126 - acc: 0.9391\n",
      "Loss: 0.3125982077776813 Accuracy: 0.9391485\n"
     ]
    }
   ],
   "source": [
    "# log_dir = 'log'\n",
    "# os.makedirs(log_dir, exist_ok=True)\n",
    "# base = '1D_CNN_custom_DO_BN'\n",
    "\n",
    "# with open(path.join(log_dir, base), 'w') as log_file:\n",
    "for i in range(3, 14):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + '000_last.hdf5'\n",
    "\n",
    "    model = load_model(model_filename)\n",
    "    model.summary()\n",
    "\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "    del(model)\n",
    "\n",
    "#         log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
