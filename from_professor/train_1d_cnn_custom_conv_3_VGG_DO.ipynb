{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path as path\n",
    "import itertools\n",
    "from sklearn.preprocessing import maxabs_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Input,InputLayer, Dense, Activation, BatchNormalization, Flatten, Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D, Dropout\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler, \\\n",
    "                                        EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = path.join('..', 'data')\n",
    "data_dir = path.join(base_dir, 'data_speech_commands_v0.02')\n",
    " \n",
    "train_txt = path.join(data_dir, 'wav_train_16words.txt')\n",
    "val_txt = path.join(data_dir, 'wav_validation_16words.txt')\n",
    "test_txt = path.join(data_dir, 'wav_test_16words.txt')\n",
    "\n",
    "train_data = np.load(path.join(data_dir, 'wav_train_data.npz'))\n",
    "val_data = np.load(path.join(data_dir, 'wav_validation_data.npz'))\n",
    "test_data = np.load(path.join(data_dir, 'wav_test_data.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36805, 16000, 1),\n",
       " (36805,),\n",
       " (4293, 16000, 1),\n",
       " (4293,),\n",
       " (4815, 16000, 1),\n",
       " (4815,),\n",
       " (16, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = train_data['x_train']\n",
    "y_train = train_data['y_train']\n",
    "x_val = val_data['x_val']\n",
    "y_val = val_data['y_val']\n",
    "x_test = test_data['x_test']\n",
    "y_test = test_data['y_test']\n",
    "y_table = test_data['table']\n",
    "\n",
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape, y_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_test[0].shape\n",
    "output_size = y_table.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_abs = np.asarray([maxabs_scale(wav) for wav in x_train])\n",
    "y_train_onehot = np.asarray([to_categorical(label, output_size) for label in y_train])\n",
    "del x_train, y_train\n",
    "\n",
    "x_val_abs = np.asarray([maxabs_scale(wav) for wav in x_val])\n",
    "y_val_onehot = np.asarray([to_categorical(label, output_size) for label in y_val])\n",
    "del x_val, y_val\n",
    "\n",
    "x_test_abs = np.asarray([maxabs_scale(wav) for wav in x_test])\n",
    "y_test_onehot = np.asarray([to_categorical(label, output_size) for label in y_test])\n",
    "del x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_1d_cnn_custom_conv_3_VGG_DO(conv_num=1):\n",
    "    model=Sequential()\n",
    "    model.add(Conv1D (kernel_size=3, filters=64, strides=1, padding='same', \n",
    "                      activation='relu', input_shape=input_shape)) \n",
    "    model.add(Conv1D (kernel_size=3, filters=64, strides=1, padding='same', \n",
    "                  activation='relu')) \n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(MaxPooling1D(pool_size=3, strides=3))\n",
    "    \n",
    "    for i in range(conv_num-1):\n",
    "        model.add(Conv1D (kernel_size=3, filters=64*(2**int((i+1)/4)), strides=1, padding='same', \n",
    "                          activation='relu'))\n",
    "        model.add(Conv1D (kernel_size=3, filters=64*(2**int((i+1)/4)), strides=1, padding='same', \n",
    "                          activation='relu'))         \n",
    "#         model.add(BatchNormalization())\n",
    "        model.add(MaxPooling1D(pool_size=3, strides=3))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(output_size, activation='softmax' ))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1024000)           0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1024000)           0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                16384016  \n",
      "=================================================================\n",
      "Total params: 16,396,624\n",
      "Trainable params: 16,396,624\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_2 (Conv1D)            (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 341312)            0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 341312)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                5461008   \n",
      "=================================================================\n",
      "Total params: 5,498,320\n",
      "Trainable params: 5,498,320\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_6 (Conv1D)            (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                1819664   \n",
      "=================================================================\n",
      "Total params: 1,881,680\n",
      "Trainable params: 1,881,680\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_12 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_15 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_16 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_17 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_18 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_19 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                606224    \n",
      "=================================================================\n",
      "Total params: 692,944\n",
      "Trainable params: 692,944\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_20 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "conv1d_21 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_22 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_23 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_24 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_25 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_26 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_27 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_28 (Conv1D)           (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "conv1d_29 (Conv1D)           (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1 (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                403472    \n",
      "=================================================================\n",
      "Total params: 564,176\n",
      "Trainable params: 564,176\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_30 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "conv1d_31 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_32 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_33 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_34 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_35 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_36 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_37 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_38 (Conv1D)           (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "conv1d_39 (Conv1D)           (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_40 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "conv1d_41 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 16)                133136    \n",
      "=================================================================\n",
      "Total params: 392,400\n",
      "Trainable params: 392,400\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_42 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "conv1d_43 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_44 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_45 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_46 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_47 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_48 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_49 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_50 (Conv1D)           (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "conv1d_51 (Conv1D)           (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_52 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "conv1d_53 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_54 (Conv1D)           (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "conv1d_55 (Conv1D)           (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 16)                43024     \n",
      "=================================================================\n",
      "Total params: 400,848\n",
      "Trainable params: 400,848\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_56 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "conv1d_57 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_58 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_59 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_60 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_61 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_62 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_63 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_64 (Conv1D)           (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "conv1d_65 (Conv1D)           (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_66 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "conv1d_67 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_68 (Conv1D)           (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "conv1d_69 (Conv1D)           (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_26 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_70 (Conv1D)           (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "conv1d_71 (Conv1D)           (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_27 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 16)                14352     \n",
      "=================================================================\n",
      "Total params: 470,736\n",
      "Trainable params: 470,736\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_72 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "conv1d_73 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_74 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_75 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_28 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_76 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_77 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_29 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_78 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_79 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_30 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_80 (Conv1D)           (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "conv1d_81 (Conv1D)           (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_31 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_82 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "conv1d_83 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_32 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_84 (Conv1D)           (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "conv1d_85 (Conv1D)           (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_86 (Conv1D)           (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "conv1d_87 (Conv1D)           (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_88 (Conv1D)           (None, 7, 256)            98560     \n",
      "_________________________________________________________________\n",
      "conv1d_89 (Conv1D)           (None, 7, 256)            196864    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling (None, 2, 256)            0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 16)                8208      \n",
      "=================================================================\n",
      "Total params: 760,016\n",
      "Trainable params: 760,016\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 10):\n",
    "    model = build_1d_cnn_custom_conv_3_VGG_DO(conv_num=i)\n",
    "    model.summary()\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36805 samples, validate on 4293 samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.3342 - acc: 0.2681\n",
      "Epoch 00001: val_loss improved from inf to 2.11902, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_1_conv_checkpoint/001-2.1190.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 2.3342 - acc: 0.2680 - val_loss: 2.1190 - val_acc: 0.3361\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.8677 - acc: 0.4412\n",
      "Epoch 00002: val_loss improved from 2.11902 to 1.97704, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_1_conv_checkpoint/002-1.9770.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 1.8677 - acc: 0.4411 - val_loss: 1.9770 - val_acc: 0.3946\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4978 - acc: 0.5650\n",
      "Epoch 00003: val_loss did not improve from 1.97704\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 1.4979 - acc: 0.5650 - val_loss: 1.9807 - val_acc: 0.4018\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1864 - acc: 0.6626\n",
      "Epoch 00004: val_loss did not improve from 1.97704\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 1.1863 - acc: 0.6626 - val_loss: 2.0509 - val_acc: 0.3755\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9279 - acc: 0.7422\n",
      "Epoch 00005: val_loss did not improve from 1.97704\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.9279 - acc: 0.7422 - val_loss: 2.1401 - val_acc: 0.3755\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7272 - acc: 0.8094\n",
      "Epoch 00006: val_loss did not improve from 1.97704\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.7272 - acc: 0.8094 - val_loss: 2.2356 - val_acc: 0.3876\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5647 - acc: 0.8575\n",
      "Epoch 00007: val_loss did not improve from 1.97704\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.5647 - acc: 0.8575 - val_loss: 2.4169 - val_acc: 0.3736\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4370 - acc: 0.8951\n",
      "Epoch 00008: val_loss did not improve from 1.97704\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.4370 - acc: 0.8951 - val_loss: 2.5836 - val_acc: 0.3671\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3378 - acc: 0.9243\n",
      "Epoch 00009: val_loss did not improve from 1.97704\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.3378 - acc: 0.9244 - val_loss: 2.7661 - val_acc: 0.3648\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2655 - acc: 0.9439\n",
      "Epoch 00010: val_loss did not improve from 1.97704\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2656 - acc: 0.9438 - val_loss: 2.8919 - val_acc: 0.3655\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2039 - acc: 0.9610\n",
      "Epoch 00011: val_loss did not improve from 1.97704\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.2040 - acc: 0.9609 - val_loss: 3.1045 - val_acc: 0.3648\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1660 - acc: 0.9704\n",
      "Epoch 00012: val_loss did not improve from 1.97704\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1660 - acc: 0.9704 - val_loss: 3.2424 - val_acc: 0.3580\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1342 - acc: 0.9777\n",
      "Epoch 00013: val_loss did not improve from 1.97704\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1342 - acc: 0.9777 - val_loss: 3.4207 - val_acc: 0.3608\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1126 - acc: 0.9831\n",
      "Epoch 00014: val_loss did not improve from 1.97704\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.1126 - acc: 0.9831 - val_loss: 3.4965 - val_acc: 0.3692\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0970 - acc: 0.9863\n",
      "Epoch 00015: val_loss did not improve from 1.97704\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.0970 - acc: 0.9863 - val_loss: 3.6642 - val_acc: 0.3552\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0794 - acc: 0.9901\n",
      "Epoch 00016: val_loss did not improve from 1.97704\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.0794 - acc: 0.9901 - val_loss: 3.8058 - val_acc: 0.3662\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0717 - acc: 0.9913\n",
      "Epoch 00017: val_loss did not improve from 1.97704\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.0717 - acc: 0.9913 - val_loss: 3.9215 - val_acc: 0.3587\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0673 - acc: 0.9919\n",
      "Epoch 00018: val_loss did not improve from 1.97704\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.0673 - acc: 0.9919 - val_loss: 4.0192 - val_acc: 0.3485\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0633 - acc: 0.9924\n",
      "Epoch 00019: val_loss did not improve from 1.97704\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.0633 - acc: 0.9924 - val_loss: 4.0036 - val_acc: 0.3676\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0532 - acc: 0.9937\n",
      "Epoch 00020: val_loss did not improve from 1.97704\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.0532 - acc: 0.9937 - val_loss: 4.1217 - val_acc: 0.3636\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0472 - acc: 0.9949\n",
      "Epoch 00021: val_loss did not improve from 1.97704\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.0472 - acc: 0.9949 - val_loss: 4.2238 - val_acc: 0.3594\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0508 - acc: 0.9946\n",
      "Epoch 00022: val_loss did not improve from 1.97704\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.0508 - acc: 0.9946 - val_loss: 4.2844 - val_acc: 0.3620\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0421 - acc: 0.9953\n",
      "Epoch 00023: val_loss did not improve from 1.97704\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.0421 - acc: 0.9953 - val_loss: 4.3568 - val_acc: 0.3631\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0397 - acc: 0.9951\n",
      "Epoch 00024: val_loss did not improve from 1.97704\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.0397 - acc: 0.9951 - val_loss: 4.4896 - val_acc: 0.3550\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0368 - acc: 0.9960\n",
      "Epoch 00025: val_loss did not improve from 1.97704\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.0368 - acc: 0.9960 - val_loss: 4.5489 - val_acc: 0.3566\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0387 - acc: 0.9956\n",
      "Epoch 00026: val_loss did not improve from 1.97704\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.0387 - acc: 0.9956 - val_loss: 4.6514 - val_acc: 0.3576\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0437 - acc: 0.9941\n",
      "Epoch 00027: val_loss did not improve from 1.97704\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.0437 - acc: 0.9941 - val_loss: 4.6112 - val_acc: 0.3538\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0367 - acc: 0.9954\n",
      "Epoch 00028: val_loss did not improve from 1.97704\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.0367 - acc: 0.9954 - val_loss: 4.5717 - val_acc: 0.3664\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0339 - acc: 0.9967\n",
      "Epoch 00029: val_loss did not improve from 1.97704\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.0339 - acc: 0.9967 - val_loss: 4.6855 - val_acc: 0.3627\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0350 - acc: 0.9958\n",
      "Epoch 00030: val_loss did not improve from 1.97704\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.0350 - acc: 0.9958 - val_loss: 4.6992 - val_acc: 0.3657\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0310 - acc: 0.9970\n",
      "Epoch 00031: val_loss did not improve from 1.97704\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.0310 - acc: 0.9970 - val_loss: 4.7316 - val_acc: 0.3604\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0310 - acc: 0.9963\n",
      "Epoch 00032: val_loss did not improve from 1.97704\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.0310 - acc: 0.9963 - val_loss: 4.8478 - val_acc: 0.3683\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0326 - acc: 0.9961\n",
      "Epoch 00033: val_loss did not improve from 1.97704\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.0326 - acc: 0.9961 - val_loss: 4.8750 - val_acc: 0.3627\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0267 - acc: 0.9975\n",
      "Epoch 00034: val_loss did not improve from 1.97704\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.0267 - acc: 0.9975 - val_loss: 4.8643 - val_acc: 0.3634\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0257 - acc: 0.9971\n",
      "Epoch 00035: val_loss did not improve from 1.97704\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.0257 - acc: 0.9971 - val_loss: 4.9156 - val_acc: 0.3641\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0271 - acc: 0.9968\n",
      "Epoch 00036: val_loss did not improve from 1.97704\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.0271 - acc: 0.9968 - val_loss: 4.9692 - val_acc: 0.3683\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0279 - acc: 0.9969\n",
      "Epoch 00037: val_loss did not improve from 1.97704\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.0279 - acc: 0.9969 - val_loss: 5.0658 - val_acc: 0.3645\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0219 - acc: 0.9985\n",
      "Epoch 00038: val_loss did not improve from 1.97704\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.0219 - acc: 0.9985 - val_loss: 5.0794 - val_acc: 0.3552\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0232 - acc: 0.9981\n",
      "Epoch 00039: val_loss did not improve from 1.97704\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.0232 - acc: 0.9981 - val_loss: 5.1830 - val_acc: 0.3566\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0271 - acc: 0.9966\n",
      "Epoch 00040: val_loss did not improve from 1.97704\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.0271 - acc: 0.9966 - val_loss: 5.1424 - val_acc: 0.3611\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0295 - acc: 0.9961\n",
      "Epoch 00041: val_loss did not improve from 1.97704\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.0295 - acc: 0.9961 - val_loss: 5.1795 - val_acc: 0.3594\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0257 - acc: 0.9973\n",
      "Epoch 00042: val_loss did not improve from 1.97704\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.0257 - acc: 0.9973 - val_loss: 5.2156 - val_acc: 0.3638\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0261 - acc: 0.9970\n",
      "Epoch 00043: val_loss did not improve from 1.97704\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.0261 - acc: 0.9970 - val_loss: 5.2150 - val_acc: 0.3634\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0239 - acc: 0.9977\n",
      "Epoch 00044: val_loss did not improve from 1.97704\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.0239 - acc: 0.9977 - val_loss: 5.1809 - val_acc: 0.3680\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0242 - acc: 0.9976\n",
      "Epoch 00045: val_loss did not improve from 1.97704\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.0242 - acc: 0.9976 - val_loss: 5.2029 - val_acc: 0.3604\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0206 - acc: 0.9985\n",
      "Epoch 00046: val_loss did not improve from 1.97704\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.0206 - acc: 0.9985 - val_loss: 5.2594 - val_acc: 0.3636\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0246 - acc: 0.9974\n",
      "Epoch 00047: val_loss did not improve from 1.97704\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.0246 - acc: 0.9974 - val_loss: 5.2570 - val_acc: 0.3613\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0237 - acc: 0.9972\n",
      "Epoch 00048: val_loss did not improve from 1.97704\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.0237 - acc: 0.9972 - val_loss: 5.3461 - val_acc: 0.3510\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0231 - acc: 0.9978\n",
      "Epoch 00049: val_loss did not improve from 1.97704\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.0231 - acc: 0.9978 - val_loss: 5.2409 - val_acc: 0.3599\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0211 - acc: 0.9978\n",
      "Epoch 00050: val_loss did not improve from 1.97704\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 0.0211 - acc: 0.9978 - val_loss: 5.3052 - val_acc: 0.3590\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0206 - acc: 0.9984\n",
      "Epoch 00051: val_loss did not improve from 1.97704\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.0206 - acc: 0.9984 - val_loss: 5.3588 - val_acc: 0.3615\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0234 - acc: 0.9974\n",
      "Epoch 00052: val_loss did not improve from 1.97704\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.0234 - acc: 0.9974 - val_loss: 5.4649 - val_acc: 0.3594\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_1_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 465us/sample - loss: 1.9917 - acc: 0.3877\n",
      "Loss: 1.9917109709043492 Accuracy: 0.38774663\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.2231 - acc: 0.3038\n",
      "Epoch 00001: val_loss improved from inf to 1.88607, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_2_conv_checkpoint/001-1.8861.hdf5\n",
      "36805/36805 [==============================] - 73s 2ms/sample - loss: 2.2230 - acc: 0.3039 - val_loss: 1.8861 - val_acc: 0.4386\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.7233 - acc: 0.4796\n",
      "Epoch 00002: val_loss improved from 1.88607 to 1.68747, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_2_conv_checkpoint/002-1.6875.hdf5\n",
      "36805/36805 [==============================] - 71s 2ms/sample - loss: 1.7232 - acc: 0.4797 - val_loss: 1.6875 - val_acc: 0.4908\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4425 - acc: 0.5717\n",
      "Epoch 00003: val_loss improved from 1.68747 to 1.64934, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_2_conv_checkpoint/003-1.6493.hdf5\n",
      "36805/36805 [==============================] - 71s 2ms/sample - loss: 1.4424 - acc: 0.5717 - val_loss: 1.6493 - val_acc: 0.4901\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1851 - acc: 0.6544\n",
      "Epoch 00004: val_loss did not improve from 1.64934\n",
      "36805/36805 [==============================] - 71s 2ms/sample - loss: 1.1851 - acc: 0.6544 - val_loss: 1.6799 - val_acc: 0.4976\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9444 - acc: 0.7265\n",
      "Epoch 00005: val_loss did not improve from 1.64934\n",
      "36805/36805 [==============================] - 71s 2ms/sample - loss: 0.9444 - acc: 0.7265 - val_loss: 1.7603 - val_acc: 0.4878\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7388 - acc: 0.7870\n",
      "Epoch 00006: val_loss did not improve from 1.64934\n",
      "36805/36805 [==============================] - 71s 2ms/sample - loss: 0.7387 - acc: 0.7870 - val_loss: 1.8669 - val_acc: 0.4850\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5634 - acc: 0.8388\n",
      "Epoch 00007: val_loss did not improve from 1.64934\n",
      "36805/36805 [==============================] - 71s 2ms/sample - loss: 0.5633 - acc: 0.8387 - val_loss: 2.0537 - val_acc: 0.4803\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4304 - acc: 0.8756\n",
      "Epoch 00008: val_loss did not improve from 1.64934\n",
      "36805/36805 [==============================] - 71s 2ms/sample - loss: 0.4304 - acc: 0.8756 - val_loss: 2.1998 - val_acc: 0.4754\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3319 - acc: 0.9072\n",
      "Epoch 00009: val_loss did not improve from 1.64934\n",
      "36805/36805 [==============================] - 71s 2ms/sample - loss: 0.3319 - acc: 0.9072 - val_loss: 2.3268 - val_acc: 0.4771\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2609 - acc: 0.9301\n",
      "Epoch 00010: val_loss did not improve from 1.64934\n",
      "36805/36805 [==============================] - 71s 2ms/sample - loss: 0.2609 - acc: 0.9301 - val_loss: 2.5561 - val_acc: 0.4801\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2032 - acc: 0.9445\n",
      "Epoch 00011: val_loss did not improve from 1.64934\n",
      "36805/36805 [==============================] - 71s 2ms/sample - loss: 0.2032 - acc: 0.9445 - val_loss: 2.7347 - val_acc: 0.4768\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1691 - acc: 0.9543\n",
      "Epoch 00012: val_loss did not improve from 1.64934\n",
      "36805/36805 [==============================] - 71s 2ms/sample - loss: 0.1691 - acc: 0.9543 - val_loss: 2.9239 - val_acc: 0.4624\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1507 - acc: 0.9599\n",
      "Epoch 00013: val_loss did not improve from 1.64934\n",
      "36805/36805 [==============================] - 71s 2ms/sample - loss: 0.1507 - acc: 0.9598 - val_loss: 3.0228 - val_acc: 0.4612\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1334 - acc: 0.9650\n",
      "Epoch 00014: val_loss did not improve from 1.64934\n",
      "36805/36805 [==============================] - 71s 2ms/sample - loss: 0.1335 - acc: 0.9650 - val_loss: 3.0994 - val_acc: 0.4628\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1171 - acc: 0.9690\n",
      "Epoch 00015: val_loss did not improve from 1.64934\n",
      "36805/36805 [==============================] - 71s 2ms/sample - loss: 0.1171 - acc: 0.9690 - val_loss: 3.1822 - val_acc: 0.4766\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1056 - acc: 0.9738\n",
      "Epoch 00016: val_loss did not improve from 1.64934\n",
      "36805/36805 [==============================] - 71s 2ms/sample - loss: 0.1056 - acc: 0.9738 - val_loss: 3.3066 - val_acc: 0.4684\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0952 - acc: 0.9760\n",
      "Epoch 00017: val_loss did not improve from 1.64934\n",
      "36805/36805 [==============================] - 71s 2ms/sample - loss: 0.0952 - acc: 0.9760 - val_loss: 3.4309 - val_acc: 0.4703\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0939 - acc: 0.9767\n",
      "Epoch 00018: val_loss did not improve from 1.64934\n",
      "36805/36805 [==============================] - 71s 2ms/sample - loss: 0.0939 - acc: 0.9767 - val_loss: 3.4582 - val_acc: 0.4631\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0850 - acc: 0.9785\n",
      "Epoch 00019: val_loss did not improve from 1.64934\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.0850 - acc: 0.9785 - val_loss: 3.4582 - val_acc: 0.4722\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0909 - acc: 0.9769\n",
      "Epoch 00020: val_loss did not improve from 1.64934\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.0909 - acc: 0.9769 - val_loss: 3.5330 - val_acc: 0.4570\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0753 - acc: 0.9814\n",
      "Epoch 00021: val_loss did not improve from 1.64934\n",
      "36805/36805 [==============================] - 71s 2ms/sample - loss: 0.0753 - acc: 0.9814 - val_loss: 3.5833 - val_acc: 0.4724\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0691 - acc: 0.9834\n",
      "Epoch 00022: val_loss did not improve from 1.64934\n",
      "36805/36805 [==============================] - 71s 2ms/sample - loss: 0.0690 - acc: 0.9834 - val_loss: 3.6398 - val_acc: 0.4731\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0647 - acc: 0.9848\n",
      "Epoch 00023: val_loss did not improve from 1.64934\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.0647 - acc: 0.9848 - val_loss: 3.6943 - val_acc: 0.4864\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0718 - acc: 0.9826\n",
      "Epoch 00024: val_loss did not improve from 1.64934\n",
      "36805/36805 [==============================] - 71s 2ms/sample - loss: 0.0718 - acc: 0.9826 - val_loss: 3.7163 - val_acc: 0.4715\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0724 - acc: 0.9817\n",
      "Epoch 00025: val_loss did not improve from 1.64934\n",
      "36805/36805 [==============================] - 71s 2ms/sample - loss: 0.0724 - acc: 0.9817 - val_loss: 3.6584 - val_acc: 0.4785\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0605 - acc: 0.9860\n",
      "Epoch 00026: val_loss did not improve from 1.64934\n",
      "36805/36805 [==============================] - 71s 2ms/sample - loss: 0.0605 - acc: 0.9860 - val_loss: 3.7579 - val_acc: 0.4715\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0606 - acc: 0.9871\n",
      "Epoch 00027: val_loss did not improve from 1.64934\n",
      "36805/36805 [==============================] - 71s 2ms/sample - loss: 0.0606 - acc: 0.9871 - val_loss: 3.8602 - val_acc: 0.4582\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0607 - acc: 0.9856\n",
      "Epoch 00028: val_loss did not improve from 1.64934\n",
      "36805/36805 [==============================] - 71s 2ms/sample - loss: 0.0607 - acc: 0.9856 - val_loss: 3.8860 - val_acc: 0.4733\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0650 - acc: 0.9839\n",
      "Epoch 00029: val_loss did not improve from 1.64934\n",
      "36805/36805 [==============================] - 71s 2ms/sample - loss: 0.0650 - acc: 0.9839 - val_loss: 3.8565 - val_acc: 0.4857\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0517 - acc: 0.9885\n",
      "Epoch 00030: val_loss did not improve from 1.64934\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.0517 - acc: 0.9885 - val_loss: 3.9517 - val_acc: 0.4845\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0583 - acc: 0.9870\n",
      "Epoch 00031: val_loss did not improve from 1.64934\n",
      "36805/36805 [==============================] - 71s 2ms/sample - loss: 0.0583 - acc: 0.9870 - val_loss: 3.8449 - val_acc: 0.4819\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0525 - acc: 0.9884\n",
      "Epoch 00032: val_loss did not improve from 1.64934\n",
      "36805/36805 [==============================] - 71s 2ms/sample - loss: 0.0525 - acc: 0.9884 - val_loss: 3.8503 - val_acc: 0.4836\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0552 - acc: 0.9870\n",
      "Epoch 00033: val_loss did not improve from 1.64934\n",
      "36805/36805 [==============================] - 71s 2ms/sample - loss: 0.0552 - acc: 0.9870 - val_loss: 3.8762 - val_acc: 0.4815\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0476 - acc: 0.9893\n",
      "Epoch 00034: val_loss did not improve from 1.64934\n",
      "36805/36805 [==============================] - 71s 2ms/sample - loss: 0.0476 - acc: 0.9893 - val_loss: 3.8535 - val_acc: 0.4850\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0478 - acc: 0.9903\n",
      "Epoch 00035: val_loss did not improve from 1.64934\n",
      "36805/36805 [==============================] - 71s 2ms/sample - loss: 0.0478 - acc: 0.9903 - val_loss: 3.8204 - val_acc: 0.4878\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0476 - acc: 0.9893\n",
      "Epoch 00036: val_loss did not improve from 1.64934\n",
      "36805/36805 [==============================] - 71s 2ms/sample - loss: 0.0476 - acc: 0.9893 - val_loss: 4.0205 - val_acc: 0.4733\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0528 - acc: 0.9883\n",
      "Epoch 00037: val_loss did not improve from 1.64934\n",
      "36805/36805 [==============================] - 71s 2ms/sample - loss: 0.0528 - acc: 0.9883 - val_loss: 3.8648 - val_acc: 0.4801\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0504 - acc: 0.9896\n",
      "Epoch 00038: val_loss did not improve from 1.64934\n",
      "36805/36805 [==============================] - 71s 2ms/sample - loss: 0.0504 - acc: 0.9896 - val_loss: 3.9693 - val_acc: 0.4789\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0469 - acc: 0.9896\n",
      "Epoch 00039: val_loss did not improve from 1.64934\n",
      "36805/36805 [==============================] - 71s 2ms/sample - loss: 0.0469 - acc: 0.9896 - val_loss: 3.9250 - val_acc: 0.4908\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0471 - acc: 0.9897\n",
      "Epoch 00040: val_loss did not improve from 1.64934\n",
      "36805/36805 [==============================] - 71s 2ms/sample - loss: 0.0471 - acc: 0.9897 - val_loss: 4.0918 - val_acc: 0.4694\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0448 - acc: 0.9906\n",
      "Epoch 00041: val_loss did not improve from 1.64934\n",
      "36805/36805 [==============================] - 71s 2ms/sample - loss: 0.0448 - acc: 0.9906 - val_loss: 4.0546 - val_acc: 0.4794\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0392 - acc: 0.9924\n",
      "Epoch 00042: val_loss did not improve from 1.64934\n",
      "36805/36805 [==============================] - 71s 2ms/sample - loss: 0.0392 - acc: 0.9924 - val_loss: 4.1617 - val_acc: 0.4768\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0447 - acc: 0.9907\n",
      "Epoch 00043: val_loss did not improve from 1.64934\n",
      "36805/36805 [==============================] - 71s 2ms/sample - loss: 0.0447 - acc: 0.9907 - val_loss: 4.0562 - val_acc: 0.4894\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0439 - acc: 0.9915\n",
      "Epoch 00044: val_loss did not improve from 1.64934\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.0439 - acc: 0.9916 - val_loss: 3.9701 - val_acc: 0.4901\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0417 - acc: 0.9914\n",
      "Epoch 00045: val_loss did not improve from 1.64934\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.0417 - acc: 0.9914 - val_loss: 3.9810 - val_acc: 0.4964\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0391 - acc: 0.9922\n",
      "Epoch 00046: val_loss did not improve from 1.64934\n",
      "36805/36805 [==============================] - 71s 2ms/sample - loss: 0.0391 - acc: 0.9922 - val_loss: 4.2705 - val_acc: 0.4752\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0416 - acc: 0.9911\n",
      "Epoch 00047: val_loss did not improve from 1.64934\n",
      "36805/36805 [==============================] - 71s 2ms/sample - loss: 0.0416 - acc: 0.9911 - val_loss: 4.1379 - val_acc: 0.4812\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0399 - acc: 0.9920\n",
      "Epoch 00048: val_loss did not improve from 1.64934\n",
      "36805/36805 [==============================] - 71s 2ms/sample - loss: 0.0399 - acc: 0.9920 - val_loss: 4.0835 - val_acc: 0.4817\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0440 - acc: 0.9907\n",
      "Epoch 00049: val_loss did not improve from 1.64934\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.0440 - acc: 0.9907 - val_loss: 4.1667 - val_acc: 0.4745\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0389 - acc: 0.9924\n",
      "Epoch 00050: val_loss did not improve from 1.64934\n",
      "36805/36805 [==============================] - 71s 2ms/sample - loss: 0.0389 - acc: 0.9924 - val_loss: 4.1195 - val_acc: 0.4824\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0395 - acc: 0.9927\n",
      "Epoch 00051: val_loss did not improve from 1.64934\n",
      "36805/36805 [==============================] - 71s 2ms/sample - loss: 0.0395 - acc: 0.9927 - val_loss: 4.1374 - val_acc: 0.4896\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0397 - acc: 0.9920\n",
      "Epoch 00052: val_loss did not improve from 1.64934\n",
      "36805/36805 [==============================] - 71s 2ms/sample - loss: 0.0397 - acc: 0.9920 - val_loss: 4.1176 - val_acc: 0.4852\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0406 - acc: 0.9918\n",
      "Epoch 00053: val_loss did not improve from 1.64934\n",
      "36805/36805 [==============================] - 71s 2ms/sample - loss: 0.0406 - acc: 0.9918 - val_loss: 4.1606 - val_acc: 0.4887\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_2_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 769us/sample - loss: 1.7163 - acc: 0.4752\n",
      "Loss: 1.7163452906276826 Accuracy: 0.47518173\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.2427 - acc: 0.2877\n",
      "Epoch 00001: val_loss improved from inf to 1.86165, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_3_conv_checkpoint/001-1.8617.hdf5\n",
      "36805/36805 [==============================] - 83s 2ms/sample - loss: 2.2427 - acc: 0.2876 - val_loss: 1.8617 - val_acc: 0.4088\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6555 - acc: 0.4901\n",
      "Epoch 00002: val_loss improved from 1.86165 to 1.58989, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_3_conv_checkpoint/002-1.5899.hdf5\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 1.6555 - acc: 0.4901 - val_loss: 1.5899 - val_acc: 0.5236\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3620 - acc: 0.5856\n",
      "Epoch 00003: val_loss improved from 1.58989 to 1.41779, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_3_conv_checkpoint/003-1.4178.hdf5\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 1.3621 - acc: 0.5856 - val_loss: 1.4178 - val_acc: 0.5560\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1719 - acc: 0.6452\n",
      "Epoch 00004: val_loss improved from 1.41779 to 1.36893, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_3_conv_checkpoint/004-1.3689.hdf5\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 1.1718 - acc: 0.6452 - val_loss: 1.3689 - val_acc: 0.5733\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0111 - acc: 0.6928\n",
      "Epoch 00005: val_loss did not improve from 1.36893\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 1.0111 - acc: 0.6929 - val_loss: 1.4107 - val_acc: 0.5614\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8679 - acc: 0.7391\n",
      "Epoch 00006: val_loss did not improve from 1.36893\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.8681 - acc: 0.7391 - val_loss: 1.3838 - val_acc: 0.5924\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7480 - acc: 0.7732\n",
      "Epoch 00007: val_loss did not improve from 1.36893\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.7483 - acc: 0.7731 - val_loss: 1.4656 - val_acc: 0.5782\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6420 - acc: 0.8010\n",
      "Epoch 00008: val_loss did not improve from 1.36893\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.6420 - acc: 0.8011 - val_loss: 1.5133 - val_acc: 0.5798\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5498 - acc: 0.8286\n",
      "Epoch 00009: val_loss did not improve from 1.36893\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.5498 - acc: 0.8286 - val_loss: 1.6002 - val_acc: 0.5898\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4692 - acc: 0.8540\n",
      "Epoch 00010: val_loss did not improve from 1.36893\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.4692 - acc: 0.8540 - val_loss: 1.7021 - val_acc: 0.5681\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3995 - acc: 0.8750\n",
      "Epoch 00011: val_loss did not improve from 1.36893\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.3995 - acc: 0.8750 - val_loss: 1.7236 - val_acc: 0.5819\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3482 - acc: 0.8903\n",
      "Epoch 00012: val_loss did not improve from 1.36893\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.3482 - acc: 0.8903 - val_loss: 1.7801 - val_acc: 0.5956\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3013 - acc: 0.9052\n",
      "Epoch 00013: val_loss did not improve from 1.36893\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.3012 - acc: 0.9053 - val_loss: 1.9166 - val_acc: 0.5821\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2642 - acc: 0.9164\n",
      "Epoch 00014: val_loss did not improve from 1.36893\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.2642 - acc: 0.9163 - val_loss: 1.9004 - val_acc: 0.6056\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2394 - acc: 0.9255\n",
      "Epoch 00015: val_loss did not improve from 1.36893\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.2394 - acc: 0.9255 - val_loss: 2.0522 - val_acc: 0.5949\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2137 - acc: 0.9327\n",
      "Epoch 00016: val_loss did not improve from 1.36893\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.2137 - acc: 0.9327 - val_loss: 2.0799 - val_acc: 0.5980\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1961 - acc: 0.9409\n",
      "Epoch 00017: val_loss did not improve from 1.36893\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.1961 - acc: 0.9409 - val_loss: 2.1171 - val_acc: 0.6042\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1802 - acc: 0.9453\n",
      "Epoch 00018: val_loss did not improve from 1.36893\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.1803 - acc: 0.9453 - val_loss: 2.1780 - val_acc: 0.6012\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1718 - acc: 0.9486\n",
      "Epoch 00019: val_loss did not improve from 1.36893\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.1718 - acc: 0.9486 - val_loss: 2.1357 - val_acc: 0.6091\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1604 - acc: 0.9515\n",
      "Epoch 00020: val_loss did not improve from 1.36893\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.1603 - acc: 0.9516 - val_loss: 2.1922 - val_acc: 0.6129\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1449 - acc: 0.9571\n",
      "Epoch 00021: val_loss did not improve from 1.36893\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.1449 - acc: 0.9571 - val_loss: 2.2131 - val_acc: 0.6031\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1447 - acc: 0.9590\n",
      "Epoch 00022: val_loss did not improve from 1.36893\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.1447 - acc: 0.9590 - val_loss: 2.2093 - val_acc: 0.6143\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1325 - acc: 0.9599\n",
      "Epoch 00023: val_loss did not improve from 1.36893\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.1325 - acc: 0.9598 - val_loss: 2.2505 - val_acc: 0.6143\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1233 - acc: 0.9639\n",
      "Epoch 00024: val_loss did not improve from 1.36893\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.1233 - acc: 0.9639 - val_loss: 2.3105 - val_acc: 0.6063\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1243 - acc: 0.9632\n",
      "Epoch 00025: val_loss did not improve from 1.36893\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.1243 - acc: 0.9632 - val_loss: 2.2257 - val_acc: 0.6122\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1129 - acc: 0.9671\n",
      "Epoch 00026: val_loss did not improve from 1.36893\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.1129 - acc: 0.9671 - val_loss: 2.3252 - val_acc: 0.6133\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1074 - acc: 0.9695\n",
      "Epoch 00027: val_loss did not improve from 1.36893\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.1074 - acc: 0.9695 - val_loss: 2.4009 - val_acc: 0.6017\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1064 - acc: 0.9699\n",
      "Epoch 00028: val_loss did not improve from 1.36893\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.1064 - acc: 0.9699 - val_loss: 2.3222 - val_acc: 0.6245\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1024 - acc: 0.9709\n",
      "Epoch 00029: val_loss did not improve from 1.36893\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.1023 - acc: 0.9709 - val_loss: 2.3492 - val_acc: 0.6189\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0927 - acc: 0.9735\n",
      "Epoch 00030: val_loss did not improve from 1.36893\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.0929 - acc: 0.9734 - val_loss: 2.4099 - val_acc: 0.6205\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0977 - acc: 0.9722\n",
      "Epoch 00031: val_loss did not improve from 1.36893\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.0977 - acc: 0.9722 - val_loss: 2.3612 - val_acc: 0.6236\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0906 - acc: 0.9758\n",
      "Epoch 00032: val_loss did not improve from 1.36893\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.0906 - acc: 0.9758 - val_loss: 2.3412 - val_acc: 0.6212\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0853 - acc: 0.9758\n",
      "Epoch 00033: val_loss did not improve from 1.36893\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.0853 - acc: 0.9758 - val_loss: 2.3562 - val_acc: 0.6257\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0888 - acc: 0.9756\n",
      "Epoch 00034: val_loss did not improve from 1.36893\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.0888 - acc: 0.9756 - val_loss: 2.3754 - val_acc: 0.6236\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0824 - acc: 0.9770\n",
      "Epoch 00035: val_loss did not improve from 1.36893\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.0824 - acc: 0.9770 - val_loss: 2.3804 - val_acc: 0.6273\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0868 - acc: 0.9759\n",
      "Epoch 00036: val_loss did not improve from 1.36893\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.0868 - acc: 0.9759 - val_loss: 2.4734 - val_acc: 0.6203\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0767 - acc: 0.9795\n",
      "Epoch 00037: val_loss did not improve from 1.36893\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.0767 - acc: 0.9795 - val_loss: 2.4426 - val_acc: 0.6203\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0758 - acc: 0.9794\n",
      "Epoch 00038: val_loss did not improve from 1.36893\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.0758 - acc: 0.9794 - val_loss: 2.4322 - val_acc: 0.6182\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0751 - acc: 0.9795\n",
      "Epoch 00039: val_loss did not improve from 1.36893\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.0751 - acc: 0.9795 - val_loss: 2.4554 - val_acc: 0.6268\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0772 - acc: 0.9793\n",
      "Epoch 00040: val_loss did not improve from 1.36893\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.0772 - acc: 0.9794 - val_loss: 2.4758 - val_acc: 0.6322\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0713 - acc: 0.9819\n",
      "Epoch 00041: val_loss did not improve from 1.36893\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.0713 - acc: 0.9819 - val_loss: 2.4982 - val_acc: 0.6194\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0710 - acc: 0.9815\n",
      "Epoch 00042: val_loss did not improve from 1.36893\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.0710 - acc: 0.9815 - val_loss: 2.5451 - val_acc: 0.6159\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0727 - acc: 0.9817\n",
      "Epoch 00043: val_loss did not improve from 1.36893\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.0727 - acc: 0.9817 - val_loss: 2.4605 - val_acc: 0.6231\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0724 - acc: 0.9811\n",
      "Epoch 00044: val_loss did not improve from 1.36893\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.0724 - acc: 0.9811 - val_loss: 2.4102 - val_acc: 0.6355\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0636 - acc: 0.9841\n",
      "Epoch 00045: val_loss did not improve from 1.36893\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.0636 - acc: 0.9841 - val_loss: 2.4372 - val_acc: 0.6308\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0660 - acc: 0.9835\n",
      "Epoch 00046: val_loss did not improve from 1.36893\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.0660 - acc: 0.9835 - val_loss: 2.4093 - val_acc: 0.6275\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0617 - acc: 0.9844\n",
      "Epoch 00047: val_loss did not improve from 1.36893\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.0617 - acc: 0.9844 - val_loss: 2.4658 - val_acc: 0.6394\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0612 - acc: 0.9847\n",
      "Epoch 00048: val_loss did not improve from 1.36893\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.0612 - acc: 0.9847 - val_loss: 2.5588 - val_acc: 0.6310\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0651 - acc: 0.9838\n",
      "Epoch 00049: val_loss did not improve from 1.36893\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.0651 - acc: 0.9838 - val_loss: 2.5034 - val_acc: 0.6282\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0608 - acc: 0.9848\n",
      "Epoch 00050: val_loss did not improve from 1.36893\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.0608 - acc: 0.9848 - val_loss: 2.6234 - val_acc: 0.6203\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0549 - acc: 0.9865\n",
      "Epoch 00051: val_loss did not improve from 1.36893\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.0549 - acc: 0.9865 - val_loss: 2.4700 - val_acc: 0.6273\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0620 - acc: 0.9847\n",
      "Epoch 00052: val_loss did not improve from 1.36893\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.0620 - acc: 0.9847 - val_loss: 2.5316 - val_acc: 0.6296\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0593 - acc: 0.9855\n",
      "Epoch 00053: val_loss did not improve from 1.36893\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.0593 - acc: 0.9855 - val_loss: 2.4410 - val_acc: 0.6362\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0626 - acc: 0.9845\n",
      "Epoch 00054: val_loss did not improve from 1.36893\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.0626 - acc: 0.9845 - val_loss: 2.4072 - val_acc: 0.6396\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_3_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 859us/sample - loss: 1.4290 - acc: 0.5626\n",
      "Loss: 1.4289896430008633 Accuracy: 0.5626168\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.1544 - acc: 0.3033\n",
      "Epoch 00001: val_loss improved from inf to 1.62908, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_4_conv_checkpoint/001-1.6291.hdf5\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 2.1543 - acc: 0.3033 - val_loss: 1.6291 - val_acc: 0.4754\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4514 - acc: 0.5414\n",
      "Epoch 00002: val_loss improved from 1.62908 to 1.26991, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_4_conv_checkpoint/002-1.2699.hdf5\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 1.4514 - acc: 0.5414 - val_loss: 1.2699 - val_acc: 0.6147\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2212 - acc: 0.6229\n",
      "Epoch 00003: val_loss improved from 1.26991 to 1.20942, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_4_conv_checkpoint/003-1.2094.hdf5\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 1.2211 - acc: 0.6229 - val_loss: 1.2094 - val_acc: 0.6238\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0717 - acc: 0.6714\n",
      "Epoch 00004: val_loss improved from 1.20942 to 1.08847, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_4_conv_checkpoint/004-1.0885.hdf5\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 1.0716 - acc: 0.6714 - val_loss: 1.0885 - val_acc: 0.6601\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9434 - acc: 0.7113\n",
      "Epoch 00005: val_loss improved from 1.08847 to 1.01836, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_4_conv_checkpoint/005-1.0184.hdf5\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.9434 - acc: 0.7113 - val_loss: 1.0184 - val_acc: 0.6925\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8413 - acc: 0.7450\n",
      "Epoch 00006: val_loss improved from 1.01836 to 0.98150, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_4_conv_checkpoint/006-0.9815.hdf5\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.8413 - acc: 0.7450 - val_loss: 0.9815 - val_acc: 0.7044\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7365 - acc: 0.7755\n",
      "Epoch 00007: val_loss did not improve from 0.98150\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.7364 - acc: 0.7755 - val_loss: 0.9967 - val_acc: 0.6983\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6505 - acc: 0.7988\n",
      "Epoch 00008: val_loss improved from 0.98150 to 0.97701, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_4_conv_checkpoint/008-0.9770.hdf5\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.6505 - acc: 0.7988 - val_loss: 0.9770 - val_acc: 0.7077\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5736 - acc: 0.8218\n",
      "Epoch 00009: val_loss did not improve from 0.97701\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.5735 - acc: 0.8218 - val_loss: 1.1013 - val_acc: 0.6942\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5055 - acc: 0.8420\n",
      "Epoch 00010: val_loss did not improve from 0.97701\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.5055 - acc: 0.8419 - val_loss: 1.0116 - val_acc: 0.7065\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4515 - acc: 0.8598\n",
      "Epoch 00011: val_loss did not improve from 0.97701\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.4515 - acc: 0.8598 - val_loss: 0.9783 - val_acc: 0.7193\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3975 - acc: 0.8748\n",
      "Epoch 00012: val_loss did not improve from 0.97701\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.3975 - acc: 0.8749 - val_loss: 1.0541 - val_acc: 0.7207\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3528 - acc: 0.8874\n",
      "Epoch 00013: val_loss did not improve from 0.97701\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.3528 - acc: 0.8874 - val_loss: 1.0739 - val_acc: 0.7300\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3214 - acc: 0.8979\n",
      "Epoch 00014: val_loss did not improve from 0.97701\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.3214 - acc: 0.8978 - val_loss: 1.1663 - val_acc: 0.7147\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2972 - acc: 0.9059\n",
      "Epoch 00015: val_loss did not improve from 0.97701\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2972 - acc: 0.9059 - val_loss: 1.1027 - val_acc: 0.7147\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2674 - acc: 0.9142\n",
      "Epoch 00016: val_loss did not improve from 0.97701\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2674 - acc: 0.9142 - val_loss: 1.1086 - val_acc: 0.7317\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2478 - acc: 0.9201\n",
      "Epoch 00017: val_loss did not improve from 0.97701\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2478 - acc: 0.9201 - val_loss: 1.1266 - val_acc: 0.7317\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2241 - acc: 0.9267\n",
      "Epoch 00018: val_loss did not improve from 0.97701\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2241 - acc: 0.9267 - val_loss: 1.2380 - val_acc: 0.7345\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2139 - acc: 0.9308\n",
      "Epoch 00019: val_loss did not improve from 0.97701\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2140 - acc: 0.9308 - val_loss: 1.1835 - val_acc: 0.7377\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1977 - acc: 0.9356\n",
      "Epoch 00020: val_loss did not improve from 0.97701\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1977 - acc: 0.9356 - val_loss: 1.2217 - val_acc: 0.7298\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1899 - acc: 0.9388\n",
      "Epoch 00021: val_loss did not improve from 0.97701\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1899 - acc: 0.9388 - val_loss: 1.2106 - val_acc: 0.7410\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1792 - acc: 0.9418\n",
      "Epoch 00022: val_loss did not improve from 0.97701\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1792 - acc: 0.9418 - val_loss: 1.2177 - val_acc: 0.7445\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1739 - acc: 0.9442\n",
      "Epoch 00023: val_loss did not improve from 0.97701\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1739 - acc: 0.9442 - val_loss: 1.2132 - val_acc: 0.7440\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1632 - acc: 0.9467\n",
      "Epoch 00024: val_loss did not improve from 0.97701\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1632 - acc: 0.9467 - val_loss: 1.2237 - val_acc: 0.7393\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1506 - acc: 0.9523\n",
      "Epoch 00025: val_loss did not improve from 0.97701\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1506 - acc: 0.9523 - val_loss: 1.2297 - val_acc: 0.7503\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1550 - acc: 0.9490\n",
      "Epoch 00026: val_loss did not improve from 0.97701\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1550 - acc: 0.9491 - val_loss: 1.3101 - val_acc: 0.7428\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1432 - acc: 0.9545\n",
      "Epoch 00027: val_loss did not improve from 0.97701\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1432 - acc: 0.9545 - val_loss: 1.2528 - val_acc: 0.7491\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1376 - acc: 0.9565\n",
      "Epoch 00028: val_loss did not improve from 0.97701\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1376 - acc: 0.9566 - val_loss: 1.2277 - val_acc: 0.7543\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1328 - acc: 0.9587\n",
      "Epoch 00029: val_loss did not improve from 0.97701\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1328 - acc: 0.9587 - val_loss: 1.2834 - val_acc: 0.7463\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1254 - acc: 0.9594\n",
      "Epoch 00030: val_loss did not improve from 0.97701\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1254 - acc: 0.9594 - val_loss: 1.2573 - val_acc: 0.7512\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1272 - acc: 0.9601\n",
      "Epoch 00031: val_loss did not improve from 0.97701\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1272 - acc: 0.9601 - val_loss: 1.3088 - val_acc: 0.7563\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1166 - acc: 0.9635\n",
      "Epoch 00032: val_loss did not improve from 0.97701\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1166 - acc: 0.9635 - val_loss: 1.3402 - val_acc: 0.7519\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1178 - acc: 0.9621\n",
      "Epoch 00033: val_loss did not improve from 0.97701\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1178 - acc: 0.9622 - val_loss: 1.2699 - val_acc: 0.7526\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1108 - acc: 0.9651\n",
      "Epoch 00034: val_loss did not improve from 0.97701\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1108 - acc: 0.9651 - val_loss: 1.4132 - val_acc: 0.7484\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1102 - acc: 0.9651\n",
      "Epoch 00035: val_loss did not improve from 0.97701\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1102 - acc: 0.9651 - val_loss: 1.3315 - val_acc: 0.7519\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1136 - acc: 0.9651\n",
      "Epoch 00036: val_loss did not improve from 0.97701\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1135 - acc: 0.9651 - val_loss: 1.3034 - val_acc: 0.7589\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1066 - acc: 0.9673\n",
      "Epoch 00037: val_loss did not improve from 0.97701\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1065 - acc: 0.9673 - val_loss: 1.2891 - val_acc: 0.7573\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1051 - acc: 0.9679\n",
      "Epoch 00038: val_loss did not improve from 0.97701\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1050 - acc: 0.9679 - val_loss: 1.3043 - val_acc: 0.7566\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1026 - acc: 0.9676\n",
      "Epoch 00039: val_loss did not improve from 0.97701\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1026 - acc: 0.9676 - val_loss: 1.3648 - val_acc: 0.7515\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0962 - acc: 0.9707\n",
      "Epoch 00040: val_loss did not improve from 0.97701\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.0962 - acc: 0.9707 - val_loss: 1.2815 - val_acc: 0.7584\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1014 - acc: 0.9684\n",
      "Epoch 00041: val_loss did not improve from 0.97701\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1014 - acc: 0.9684 - val_loss: 1.2798 - val_acc: 0.7626\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0930 - acc: 0.9721\n",
      "Epoch 00042: val_loss did not improve from 0.97701\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.0930 - acc: 0.9721 - val_loss: 1.3323 - val_acc: 0.7636\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0932 - acc: 0.9712\n",
      "Epoch 00043: val_loss did not improve from 0.97701\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.0933 - acc: 0.9711 - val_loss: 1.3948 - val_acc: 0.7508\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1017 - acc: 0.9677\n",
      "Epoch 00044: val_loss did not improve from 0.97701\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1017 - acc: 0.9677 - val_loss: 1.3555 - val_acc: 0.7570\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0898 - acc: 0.9718\n",
      "Epoch 00045: val_loss did not improve from 0.97701\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.0898 - acc: 0.9719 - val_loss: 1.4100 - val_acc: 0.7589\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0847 - acc: 0.9744\n",
      "Epoch 00046: val_loss did not improve from 0.97701\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.0847 - acc: 0.9744 - val_loss: 1.3008 - val_acc: 0.7598\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0829 - acc: 0.9749\n",
      "Epoch 00047: val_loss did not improve from 0.97701\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.0830 - acc: 0.9749 - val_loss: 1.4419 - val_acc: 0.7363\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0954 - acc: 0.9705\n",
      "Epoch 00048: val_loss did not improve from 0.97701\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.0953 - acc: 0.9705 - val_loss: 1.3805 - val_acc: 0.7631\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0806 - acc: 0.9757\n",
      "Epoch 00049: val_loss did not improve from 0.97701\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.0806 - acc: 0.9757 - val_loss: 1.3711 - val_acc: 0.7584\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0844 - acc: 0.9750\n",
      "Epoch 00050: val_loss did not improve from 0.97701\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.0844 - acc: 0.9749 - val_loss: 1.3315 - val_acc: 0.7626\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0840 - acc: 0.9748\n",
      "Epoch 00051: val_loss did not improve from 0.97701\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.0840 - acc: 0.9748 - val_loss: 1.3670 - val_acc: 0.7654\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0788 - acc: 0.9757\n",
      "Epoch 00052: val_loss did not improve from 0.97701\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.0788 - acc: 0.9757 - val_loss: 1.4042 - val_acc: 0.7519\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0815 - acc: 0.9755\n",
      "Epoch 00053: val_loss did not improve from 0.97701\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.0815 - acc: 0.9755 - val_loss: 1.3700 - val_acc: 0.7601\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0755 - acc: 0.9773\n",
      "Epoch 00054: val_loss did not improve from 0.97701\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.0755 - acc: 0.9773 - val_loss: 1.4054 - val_acc: 0.7596\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0819 - acc: 0.9754\n",
      "Epoch 00055: val_loss did not improve from 0.97701\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.0819 - acc: 0.9754 - val_loss: 1.3717 - val_acc: 0.7643\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0742 - acc: 0.9774\n",
      "Epoch 00056: val_loss did not improve from 0.97701\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.0742 - acc: 0.9774 - val_loss: 1.3874 - val_acc: 0.7710\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0776 - acc: 0.9768\n",
      "Epoch 00057: val_loss did not improve from 0.97701\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.0775 - acc: 0.9768 - val_loss: 1.3407 - val_acc: 0.7710\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0742 - acc: 0.9769\n",
      "Epoch 00058: val_loss did not improve from 0.97701\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.0742 - acc: 0.9769 - val_loss: 1.3651 - val_acc: 0.7624\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_4_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 890us/sample - loss: 1.0904 - acc: 0.6719\n",
      "Loss: 1.0903628553928244 Accuracy: 0.6718588\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.1243 - acc: 0.3067\n",
      "Epoch 00001: val_loss improved from inf to 1.55428, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_5_conv_checkpoint/001-1.5543.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 2.1242 - acc: 0.3067 - val_loss: 1.5543 - val_acc: 0.4997\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4237 - acc: 0.5414\n",
      "Epoch 00002: val_loss improved from 1.55428 to 1.27438, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_5_conv_checkpoint/002-1.2744.hdf5\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 1.4237 - acc: 0.5413 - val_loss: 1.2744 - val_acc: 0.5954\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2156 - acc: 0.6175\n",
      "Epoch 00003: val_loss improved from 1.27438 to 1.10489, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_5_conv_checkpoint/003-1.1049.hdf5\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 1.2155 - acc: 0.6175 - val_loss: 1.1049 - val_acc: 0.6560\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0650 - acc: 0.6675\n",
      "Epoch 00004: val_loss improved from 1.10489 to 1.01711, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_5_conv_checkpoint/004-1.0171.hdf5\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 1.0650 - acc: 0.6675 - val_loss: 1.0171 - val_acc: 0.6795\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9360 - acc: 0.7130\n",
      "Epoch 00005: val_loss did not improve from 1.01711\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.9360 - acc: 0.7129 - val_loss: 1.1054 - val_acc: 0.6508\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8312 - acc: 0.7451\n",
      "Epoch 00006: val_loss improved from 1.01711 to 0.83001, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_5_conv_checkpoint/006-0.8300.hdf5\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.8314 - acc: 0.7450 - val_loss: 0.8300 - val_acc: 0.7501\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7405 - acc: 0.7761\n",
      "Epoch 00007: val_loss improved from 0.83001 to 0.79698, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_5_conv_checkpoint/007-0.7970.hdf5\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.7404 - acc: 0.7761 - val_loss: 0.7970 - val_acc: 0.7692\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6745 - acc: 0.7940\n",
      "Epoch 00008: val_loss improved from 0.79698 to 0.78419, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_5_conv_checkpoint/008-0.7842.hdf5\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.6745 - acc: 0.7940 - val_loss: 0.7842 - val_acc: 0.7717\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6023 - acc: 0.8152\n",
      "Epoch 00009: val_loss did not improve from 0.78419\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.6022 - acc: 0.8153 - val_loss: 0.8549 - val_acc: 0.7494\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5464 - acc: 0.8327\n",
      "Epoch 00010: val_loss improved from 0.78419 to 0.74132, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_5_conv_checkpoint/010-0.7413.hdf5\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.5464 - acc: 0.8327 - val_loss: 0.7413 - val_acc: 0.7838\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5000 - acc: 0.8471\n",
      "Epoch 00011: val_loss did not improve from 0.74132\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.5000 - acc: 0.8471 - val_loss: 0.7642 - val_acc: 0.7822\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4524 - acc: 0.8587\n",
      "Epoch 00012: val_loss improved from 0.74132 to 0.73630, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_5_conv_checkpoint/012-0.7363.hdf5\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.4524 - acc: 0.8588 - val_loss: 0.7363 - val_acc: 0.7927\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4090 - acc: 0.8731\n",
      "Epoch 00013: val_loss did not improve from 0.73630\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.4089 - acc: 0.8731 - val_loss: 0.7863 - val_acc: 0.7920\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3709 - acc: 0.8840\n",
      "Epoch 00014: val_loss did not improve from 0.73630\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.3710 - acc: 0.8840 - val_loss: 0.7791 - val_acc: 0.7873\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3446 - acc: 0.8916\n",
      "Epoch 00015: val_loss improved from 0.73630 to 0.73621, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_5_conv_checkpoint/015-0.7362.hdf5\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.3447 - acc: 0.8916 - val_loss: 0.7362 - val_acc: 0.8015\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3100 - acc: 0.9017\n",
      "Epoch 00016: val_loss did not improve from 0.73621\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.3101 - acc: 0.9017 - val_loss: 0.7578 - val_acc: 0.8020\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2849 - acc: 0.9089\n",
      "Epoch 00017: val_loss did not improve from 0.73621\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2849 - acc: 0.9089 - val_loss: 0.8109 - val_acc: 0.8036\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2545 - acc: 0.9181\n",
      "Epoch 00018: val_loss did not improve from 0.73621\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2545 - acc: 0.9181 - val_loss: 0.8054 - val_acc: 0.7948\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2362 - acc: 0.9244\n",
      "Epoch 00019: val_loss did not improve from 0.73621\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2362 - acc: 0.9244 - val_loss: 0.8351 - val_acc: 0.7997\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2240 - acc: 0.9283\n",
      "Epoch 00020: val_loss did not improve from 0.73621\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2240 - acc: 0.9283 - val_loss: 0.8424 - val_acc: 0.7927\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2067 - acc: 0.9327\n",
      "Epoch 00021: val_loss did not improve from 0.73621\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2067 - acc: 0.9328 - val_loss: 0.8392 - val_acc: 0.7983\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1885 - acc: 0.9398\n",
      "Epoch 00022: val_loss did not improve from 0.73621\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1885 - acc: 0.9398 - val_loss: 0.8675 - val_acc: 0.7990\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1819 - acc: 0.9410\n",
      "Epoch 00023: val_loss did not improve from 0.73621\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1820 - acc: 0.9410 - val_loss: 0.8880 - val_acc: 0.7948\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1720 - acc: 0.9443\n",
      "Epoch 00024: val_loss did not improve from 0.73621\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1720 - acc: 0.9444 - val_loss: 0.8663 - val_acc: 0.8083\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1653 - acc: 0.9473\n",
      "Epoch 00025: val_loss did not improve from 0.73621\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1653 - acc: 0.9473 - val_loss: 0.8060 - val_acc: 0.8213\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1556 - acc: 0.9507\n",
      "Epoch 00026: val_loss did not improve from 0.73621\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1556 - acc: 0.9507 - val_loss: 0.8698 - val_acc: 0.8111\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1489 - acc: 0.9517\n",
      "Epoch 00027: val_loss did not improve from 0.73621\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1489 - acc: 0.9517 - val_loss: 0.8396 - val_acc: 0.8171\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1438 - acc: 0.9524\n",
      "Epoch 00028: val_loss did not improve from 0.73621\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1439 - acc: 0.9524 - val_loss: 0.8545 - val_acc: 0.8116\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1403 - acc: 0.9550\n",
      "Epoch 00029: val_loss did not improve from 0.73621\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1403 - acc: 0.9550 - val_loss: 0.9271 - val_acc: 0.8039\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1310 - acc: 0.9588\n",
      "Epoch 00030: val_loss did not improve from 0.73621\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1310 - acc: 0.9588 - val_loss: 0.8583 - val_acc: 0.8150\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1257 - acc: 0.9611\n",
      "Epoch 00031: val_loss did not improve from 0.73621\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1257 - acc: 0.9611 - val_loss: 0.9097 - val_acc: 0.8134\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1263 - acc: 0.9604\n",
      "Epoch 00032: val_loss did not improve from 0.73621\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1263 - acc: 0.9604 - val_loss: 0.9035 - val_acc: 0.8085\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1187 - acc: 0.9617\n",
      "Epoch 00033: val_loss did not improve from 0.73621\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1187 - acc: 0.9617 - val_loss: 0.8536 - val_acc: 0.8202\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1151 - acc: 0.9635\n",
      "Epoch 00034: val_loss did not improve from 0.73621\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1152 - acc: 0.9634 - val_loss: 0.9029 - val_acc: 0.8218\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1117 - acc: 0.9656\n",
      "Epoch 00035: val_loss did not improve from 0.73621\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1117 - acc: 0.9656 - val_loss: 0.8380 - val_acc: 0.8286\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1142 - acc: 0.9647\n",
      "Epoch 00036: val_loss did not improve from 0.73621\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1142 - acc: 0.9647 - val_loss: 0.9272 - val_acc: 0.8227\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1051 - acc: 0.9679\n",
      "Epoch 00037: val_loss did not improve from 0.73621\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1051 - acc: 0.9679 - val_loss: 0.9669 - val_acc: 0.8137\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1080 - acc: 0.9664\n",
      "Epoch 00038: val_loss did not improve from 0.73621\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1079 - acc: 0.9664 - val_loss: 0.8660 - val_acc: 0.8279\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0954 - acc: 0.9705\n",
      "Epoch 00039: val_loss did not improve from 0.73621\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.0954 - acc: 0.9705 - val_loss: 0.9144 - val_acc: 0.8185\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0982 - acc: 0.9705\n",
      "Epoch 00040: val_loss did not improve from 0.73621\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.0982 - acc: 0.9705 - val_loss: 0.8667 - val_acc: 0.8239\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0925 - acc: 0.9712\n",
      "Epoch 00041: val_loss did not improve from 0.73621\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.0924 - acc: 0.9712 - val_loss: 0.8941 - val_acc: 0.8248\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0935 - acc: 0.9714\n",
      "Epoch 00042: val_loss did not improve from 0.73621\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.0935 - acc: 0.9714 - val_loss: 0.8730 - val_acc: 0.8237\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0897 - acc: 0.9719\n",
      "Epoch 00043: val_loss did not improve from 0.73621\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.0897 - acc: 0.9719 - val_loss: 0.9020 - val_acc: 0.8341\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0859 - acc: 0.9737\n",
      "Epoch 00044: val_loss did not improve from 0.73621\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.0860 - acc: 0.9737 - val_loss: 0.8842 - val_acc: 0.8248\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0877 - acc: 0.9734\n",
      "Epoch 00045: val_loss did not improve from 0.73621\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.0877 - acc: 0.9734 - val_loss: 0.8877 - val_acc: 0.8295\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0842 - acc: 0.9736\n",
      "Epoch 00046: val_loss did not improve from 0.73621\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.0842 - acc: 0.9736 - val_loss: 0.9266 - val_acc: 0.8197\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0872 - acc: 0.9735\n",
      "Epoch 00047: val_loss did not improve from 0.73621\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.0872 - acc: 0.9735 - val_loss: 0.8879 - val_acc: 0.8265\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0820 - acc: 0.9750\n",
      "Epoch 00048: val_loss did not improve from 0.73621\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.0820 - acc: 0.9750 - val_loss: 0.9098 - val_acc: 0.8162\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0812 - acc: 0.9751\n",
      "Epoch 00049: val_loss did not improve from 0.73621\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.0812 - acc: 0.9751 - val_loss: 0.8764 - val_acc: 0.8290\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0803 - acc: 0.9753\n",
      "Epoch 00050: val_loss did not improve from 0.73621\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.0803 - acc: 0.9753 - val_loss: 0.8870 - val_acc: 0.8328\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0774 - acc: 0.9764\n",
      "Epoch 00051: val_loss did not improve from 0.73621\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.0774 - acc: 0.9764 - val_loss: 0.9019 - val_acc: 0.8302\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0800 - acc: 0.9751\n",
      "Epoch 00052: val_loss did not improve from 0.73621\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.0800 - acc: 0.9751 - val_loss: 0.8904 - val_acc: 0.8288\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0747 - acc: 0.9770\n",
      "Epoch 00053: val_loss did not improve from 0.73621\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.0747 - acc: 0.9770 - val_loss: 0.9035 - val_acc: 0.8265\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0752 - acc: 0.9779\n",
      "Epoch 00054: val_loss did not improve from 0.73621\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.0752 - acc: 0.9779 - val_loss: 0.9213 - val_acc: 0.8344\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0754 - acc: 0.9781\n",
      "Epoch 00055: val_loss did not improve from 0.73621\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.0754 - acc: 0.9781 - val_loss: 0.8901 - val_acc: 0.8309\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0689 - acc: 0.9786\n",
      "Epoch 00056: val_loss did not improve from 0.73621\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.0689 - acc: 0.9786 - val_loss: 0.9131 - val_acc: 0.8290\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0685 - acc: 0.9794\n",
      "Epoch 00057: val_loss did not improve from 0.73621\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.0685 - acc: 0.9794 - val_loss: 0.9247 - val_acc: 0.8337\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0680 - acc: 0.9796\n",
      "Epoch 00058: val_loss did not improve from 0.73621\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.0680 - acc: 0.9796 - val_loss: 0.9255 - val_acc: 0.8344\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0691 - acc: 0.9792\n",
      "Epoch 00059: val_loss did not improve from 0.73621\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.0690 - acc: 0.9792 - val_loss: 0.9196 - val_acc: 0.8307\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0715 - acc: 0.9788\n",
      "Epoch 00060: val_loss did not improve from 0.73621\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.0715 - acc: 0.9788 - val_loss: 0.9302 - val_acc: 0.8376\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0711 - acc: 0.9787\n",
      "Epoch 00061: val_loss did not improve from 0.73621\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.0711 - acc: 0.9788 - val_loss: 0.9167 - val_acc: 0.8253\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0643 - acc: 0.9814\n",
      "Epoch 00062: val_loss did not improve from 0.73621\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.0642 - acc: 0.9814 - val_loss: 0.8507 - val_acc: 0.8404\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0688 - acc: 0.9795\n",
      "Epoch 00063: val_loss did not improve from 0.73621\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.0688 - acc: 0.9795 - val_loss: 0.9381 - val_acc: 0.8316\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0646 - acc: 0.9810\n",
      "Epoch 00064: val_loss did not improve from 0.73621\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.0646 - acc: 0.9810 - val_loss: 0.9057 - val_acc: 0.8428\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0656 - acc: 0.9806\n",
      "Epoch 00065: val_loss did not improve from 0.73621\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.0657 - acc: 0.9805 - val_loss: 0.9376 - val_acc: 0.8302\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_5_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzs3Xl8VNX5+PHPmSWTfSEhC4QtyE5C2EEE3FAUxRWpdam2Lv3VVq3WSuve1rXWUqx+rVZbbauiqK0LbigULSCbQFBACFsSErJvZJvl/P44M1kgCQEyGZJ53q/XfU1y5869z0wm57nn3Hufq7TWCCGEEACWQAcghBDi5CFJQQghRCNJCkIIIRpJUhBCCNFIkoIQQohGkhSEEEI0kqQghBCikSQFIYQQjSQpCCGEaGQLdADHKiEhQQ8cODDQYQghRLeyYcOGYq1176Mt1+2SwsCBA1m/fn2gwxBCiG5FKbWvI8vJ8JEQQohGkhSEEEI0kqQghBCiUbc7ptAap9NJbm4udXV1gQ6l2woNDSU1NRW73R7oUIQQAdQjkkJubi5RUVEMHDgQpVSgw+l2tNaUlJSQm5vLoEGDAh2OECKAesTwUV1dHfHx8ZIQjpNSivj4eOlpCSF6RlIAJCGcIPn8hBDQg5LC0bjdtdTX5+HxuAIdihBCnLSCJil4PHU0NOSjdUOnr7u8vJxnn332uF57/vnnU15e3uHlH3zwQZ588snj2pYQQhxN0CQFpcwxda07v6fQXlJwudrf3tKlS4mNje30mIQQ4ngEUVKwAv5JCgsWLCA7O5vMzEzuuusuVqxYwfTp05k7dy4jR44E4OKLL2b8+PGMGjWK559/vvG1AwcOpLi4mL179zJixAhuvPFGRo0axTnnnENtbW272920aRNTpkwhIyODSy65hLKyMgAWLVrEyJEjycjI4Hvf+x4A//3vf8nMzCQzM5OxY8dSVVXV6Z+DEKL76xGnpDa3c+ftVFdvauUZjdtdjcUSilLHdi5+ZGQmQ4YsbPP5xx57jK1bt7Jpk9nuihUr2LhxI1u3bm08xfOll16iV69e1NbWMnHiRC677DLi4+MPi30nr732Gi+88AJXXHEFb731FldffXWb27322mt5+umnmTlzJvfffz8PPfQQCxcu5LHHHmPPnj04HI7Goaknn3ySZ555hmnTplFdXU1oaOgxfQZCiOAQND0F8J1do7tka5MmTWpxzv+iRYsYM2YMU6ZMIScnh507dx7xmkGDBpGZmQnA+PHj2bt3b5vrr6iooLy8nJkzZwLwgx/8gJUrVwKQkZHBVVddxT//+U9sNpP3p02bxh133MGiRYsoLy9vnC+EEM31uJahvT36qqoN2O1JhIam+j2OiIiIxp9XrFjBsmXLWL16NeHh4Zx++umtXhPgcDgaf7ZarUcdPmrLBx98wMqVK3nvvfd4+OGHycrKYsGCBcyZM4elS5cybdo0Pv74Y4YPH35c6xdC9FxB1FMwB5v9cUwhKiqq3TH6iooK4uLiCA8PZ/v27axZs+aEtxkTE0NcXBxffPEFAP/4xz+YOXMmHo+HnJwczjjjDB5//HEqKiqorq4mOzub9PR07r77biZOnMj27dtPOAYhRM/T43oK7TFnIHV+UoiPj2fatGmMHj2a8847jzlz5rR4fvbs2Tz33HOMGDGCYcOGMWXKlE7Z7ssvv8yPf/xjampqSEtL429/+xtut5urr76aiooKtNbceuutxMbGct9997F8+XIsFgujRo3ivPPO65QYhBA9i9K6a8bYO8uECRP04TfZ2bZtGyNGjDjqa2tqdgCa8HAZNmlNRz9HIUT3o5TaoLWecLTl/DZ8pJTqp5RarpT6Vin1jVLqtlaWUUqpRUqpXUqpLUqpcf6Kx2zPitZuf25CCCG6NX8OH7mAO7XWG5VSUcAGpdSnWutvmy1zHjDEO00G/s/76Cc2tD7kv9ULIUQ357eegtY6X2u90ftzFbAN6HvYYhcBr2hjDRCrlErxV0z+OtAshBA9RZecfaSUGgiMBb467Km+QE6z33M5MnGglLpJKbVeKbW+qKjoBOKwAhqtPce9DiGE6Mn8nhSUUpHAW8DtWuvK41mH1vp5rfUErfWE3r17n0As/qt/JIQQPYFfk4Iy9STeAv6ltX67lUXygH7Nfk/1zvNTPP6rfySEED2BP88+UsCLwDat9VNtLPYucK33LKQpQIXWOt9/Mfl6CoE/AykyMvKY5gshRFfw59lH04BrgCyllK9C3a+B/gBa6+eApcD5wC6gBrjej/HI8JEQQhyFP88++lJrrbTWGVrrTO+0VGv9nDch4D3r6Bat9WCtdbrWev3R1nsi/JUUFixYwDPPPNP4u+9GONXV1Zx11lmMGzeO9PR0/vOf/3R4nVpr7rrrLkaPHk16ejqLFy8GID8/nxkzZpCZmcno0aP54osvcLvdXHfddY3L/vGPf+zU9yeECB49r8zF7bfDptZKZ4NCE+auxqIcYAnp+DozM2Fh24X25s+fz+23384tt9wCwBtvvMHHH39MaGgo77zzDtHR0RQXFzNlyhTmzp3bofshv/3222zatInNmzdTXFzMxIkTmTFjBq+++irnnnsu99xzD263m5qaGjZt2kReXh5bt24FOKY7uQkhRHM9Lym0yz/ls8eOHUthYSEHDhygqKiIuLg4+vXrh9Pp5Ne//jUrV67EYrGQl5fHwYMHSU5OPuo6v/zyS6688kqsVitJSUnMnDmTdevWMXHiRH74wx/idDq5+OKLyczMJC0tjd27d/Ozn/2MOXPmcM4553Tq+xNCBI+elxTa2aNXQF31Zmy2GEJDB3bqZufNm8eSJUsoKChg/vz5APzrX/+iqKiIDRs2YLfbGThwYKsls4/FjBkzWLlyJR988AHXXXcdd9xxB9deey2bN2/m448/5rnnnuONN97gpZde6oy3JYQIMkFVOhv8V/9o/vz5vP766yxZsoR58+YBpmR2YmIidrud5cuXs2/fvg6vb/r06SxevBi3201RURErV65k0qRJ7Nu3j6SkJG688UZuuOEGNm7cSHFxMR6Ph8suu4zf/e53bNy4sdPfnxAiOPS8nsJR+KvUxahRo6iqqqJv376kpJhKHVdddRUXXngh6enpTJgw4ZhuanPJJZewevVqxowZg1KKJ554guTkZF5++WV+//vfY7fbiYyM5JVXXiEvL4/rr78ej8dcqf3oo492+vsTQgSHoCqdDVBTswut64mIGOWP8Lo1KZ0tRM8V8NLZJyspny2EEG0LwqQglVKFEKItQZkUwCOVUoUQohVBmBSkKJ4QQrQlCJPCyVMUTwghTjZBnBSkpyCEEIcLwqTgGz7qvJ5CeXk5zz777HG99vzzz5daRUKIk0YQJoXO7ym0lxRcrva3s3TpUmJjYzstFiGEOBFBmxSg85LCggULyM7OJjMzk7vuuosVK1Ywffp05s6dy8iRIwG4+OKLGT9+PKNGjeL5559vfO3AgQMpLi5m7969jBgxghtvvJFRo0ZxzjnnUFtbe8S23nvvPSZPnszYsWM5++yzOXjwIADV1dVcf/31pKenk5GRwVtvvQXARx99xLhx4xgzZgxnnXVWp71nIUTP1OPKXLRTOdvLgts9DKVCsHQwJR6lcjaPPfYYW7duZZN3wytWrGDjxo1s3bqVQYMGAfDSSy/Rq1cvamtrmThxIpdddhnx8fEt1rNz505ee+01XnjhBa644greeustrr766hbLnHbaaaxZswalFH/961954okn+MMf/sBvf/tbYmJiyMrKAqCsrIyioiJuvPFGVq5cyaBBgygtLe3YGxZCBK0elxSOTnkn/5b3mDRpUmNCAFi0aBHvvPMOADk5OezcufOIpDBo0CAyMzMBGD9+PHv37j1ivbm5ucyfP5/8/HwaGhoat7Fs2TJef/31xuXi4uJ47733mDFjRuMyvXr16tT3KIToeXpcUmhvj97n0KG9WCxhhIUN9lscERERjT+vWLGCZcuWsXr1asLDwzn99NNbLaHtcDgaf7Zara0OH/3sZz/jjjvuYO7cuaxYsYIHH3zQL/ELIYJT0B1TMDq31EVUVBRVVVVtPl9RUUFcXBzh4eFs376dNWvWHPe2Kioq6Nu3LwAvv/xy4/xZs2a1uCVoWVkZU6ZMYeXKlezZswdAho+EEEcVlEmhs4vixcfHM23aNEaPHs1dd911xPOzZ8/G5XIxYsQIFixYwJQpU457Ww8++CDz5s1j/PjxJCQkNM6/9957KSsrY/To0YwZM4bly5fTu3dvnn/+eS699FLGjBnTePMfIYRoS9CVzgaord2D211FZGRGZ4fXrUnpbCF6Limd3Q6plCqEEK0L0qRgRSqlCiHEkYI0KUhRPCGEaE2QJgUpny2EEK0J0qQgPQUhhGhNkCcF6SkIIURzQZ0UOrMo3rGKjIwM2LaFEKItQZkUoPPvqSCEED1BUCaFzj7QvGDBghYlJh588EGefPJJqqurOeussxg3bhzp6en85z//Oeq62iqx3VoJ7LbKZQshxPHqcQXxbv/odjYVtFs7GwC3uxqlbFgsoUddNjM5k4Wz2660N3/+fG6//XZuueUWAN544w0+/vhjQkNDeeedd4iOjqa4uJgpU6Ywd+5clFJtrqu1Etsej6fVEtitlcsWQogT0eOSQse13TAfq7Fjx1JYWMiBAwcoKioiLi6Ofv364XQ6+fWvf83KlSuxWCzk5eVx8OBBkpOT21xXayW2i4qKWi2B3Vq5bCGEOBE9Lim0t0ff3KFD21DKSnj40E7Z7rx581iyZAkFBQWNhef+9a9/UVRUxIYNG7Db7QwcOLDVktk+HS2xLYQQ/hKUxxSg8+sfzZ8/n9dff50lS5Ywb948wJS5TkxMxG63s3z5cvbt29fuOtoqsd1WCezWymULIcSJCOKk0Lnls0eNGkVVVRV9+/YlJSUFgKuuuor169eTnp7OK6+8wvDhw9tdR1slttsqgd1auWwhhDgRQVk6G6Cubj9OZwlRUWM7M7xuTUpnC9FzSensw1VUwNat0NAA+C5gc9PdkqIQQvhT8CQFiwXq6sB732MpiieEEEfyW1JQSr2klCpUSm1t4/nTlVIVSqlN3un+E9neUff4w8LMY2NSkKJ4zUmPSQgB/u0p/B2YfZRlvtBaZ3qn3xzvhkJDQykpKWm/YbPZwG6HmhpAiuI1p7WmpKSE0NCjX8gnhOjZ/HadgtZ6pVJqoL/W31xqaiq5ubkUFRW1v2B5OZSUQH09Hk89DQ3F2O3fYbWGdUWYJ7XQ0FBSU1MDHYYQIsACffHaVKXUZuAA8Aut9TfHsxK73d54tW+7/v53WLgQqqupce5l7drzGD78HyQnX308mxVCiB4nkAeaNwIDtNZjgKeBf7e1oFLqJqXUeqXU+qP2BtqTnm7OPvruO+x2UyrC5So5/vUJIUQPE7CkoLWu1FpXe39eCtiVUgltLPu81nqC1npC7969j3+j6enmMSsLmy0WUDidpce/PiGE6GEClhSUUsnKWy5UKTXJG4t/d9uHDwerFbKyUMqKzRaLyyVJQQghfPx2TEEp9RpwOpCglMoFHgDsAFrr54DLgf+nlHIBtcD3tL/Pi3Q4TGLYsgUAm62X9BSEEKIZf559dOVRnv8z8Gd/bb9N6emwejUAdnsv6SkIIUQzwXNFs096OuzbBxUV0lMQQojDBF9SyMgwj1u3Sk9BCCEOE3xJodkZSHZ7IvX1+VLqQgghvIIvKfTvD9HRkJVFdPQkPJ5DVFdvCXRUQghxUgi+pKCU6S1kZRETMx2AioovAhyUEEKcHIIvKYBJClu2EOpIxeEYIElBCCG8gjcpVFRAbi4xMadRXv6FlI4WQgiCOSkAZGURGzsdp/MgtbW7AhuTEEKcBII7KWzZIscVhBCimeBMCrGx0K8fZGURHj4Cmy1ekoIQQhCsSQEaz0BSSjUeVxBCiGAXvEkhIwO2bYOGBmJjp1NXl019fX6goxJCiIAK3qSQng4uF+zYIccVhBDCK7iTAkBWFpGRY7FYwiUpCCGCXvAmhWHDwGaDLVuwWOxER0+V4wpCiKAXvEkhJARGjICsLABiY6dz6NAWnM7yAAcmhBCBE7xJAcwQ0qZNoLX3uIKmsnJ1oKMSQoiACe6kMH06HDgA27YRHT0FpWxyXEEIEdSCOymcf755/OADrNZwIiPHS1IQQgS14E4K/fubIaSlSwFzXKGyci1ud12AAxNCiMAI7qQAMGcOfPklVFQQEzMdrRuoqloX6KiEECIgJCnMmWMuYvvkE2JipgFyEZsQInhJUpgyBeLi4IMPsNvjiYgYTVnZskBHJYQQASFJwWaD2bPhww/B4yEh4WLKy/9LQ0NRoCMTQoguJ0kBzBBSYSGsX0/v3pcDHoqL/x3oqIQQostJUgA491xQCpYuJSIig7CwUygqejPQUQkhRJeTpACQkGCOLXzwAUopeveeR1nZ5zidJYGOTAghulSHkoJS6jalVLQyXlRKbVRKnePv4LrUnDmwfj0UFHiHkNwUF/8n0FEJIUSX6mhP4Yda60rgHCAOuAZ4zG9RBcKcOebxww+JjBxLaOggioqWBDYmIYToYh1NCsr7eD7wD631N83m9QxjxkDfvocNIS3D6SwLdGRCCNFlOpoUNiilPsEkhY+VUlGAx39hBYBSphbSJ59AQwO9e1+O1k5KSt4NdGRCCNFlOpoUfgQsACZqrWsAO3C936IKlDlzoKoK/vc/oqIm4HD0lyEkIURQ6WhSmArs0FqXK6WuBu4FKvwXVoCcdZa5+c5773mHkC6ntPQTXK7D3uru3XDNNSaBCCFED9LRpPB/QI1SagxwJ5ANvOK3qAIlMhIuugheeMF7FtI8tG6guPi9lsvddhv885/whdRIEkL0LB1NCi6ttQYuAv6stX4GiPJfWAH08MNQXw/33Ud09CQcjtSWQ0iffQbvv29+3rIlMDEKIYSfdDQpVCmlfoU5FfUDpZQFc1yh5xkyBH76U3jxRdSWLBISLqO09CNcripwu+HOO2HAAHOmkiQFIUQP09GkMB+ox1yvUACkAr/3W1SBdt99pnLqHXeQ2PtytK6npOQ9ePll2LwZHn8cxo83PwshRA/SoaTgTQT/AmKUUhcAdVrrnndMwScuDh58ED7/nOiVJTgcAyjc/SLcc48ph3HFFZCRATt2QJ3cpU0I0XN0tMzFFcBaYB5wBfCVUupyfwYWcD/+MQwbhrrrlyTHX0PUc59DQQE89ZS5piEjwwwnbdsW6EiFEKLTdHT46B7MNQo/0FpfC0wC7mvvBUqpl5RShUqprW08r5RSi5RSu5RSW5RS444tdD+z2+HJJ+G77+j3lyL6LYZDF4yGqVPN8xkZ5lGOKwghepCOJgWL1rqw2e8lHXjt34HZ7Tx/HjDEO92EOe315DJnDpx9Nran/oLSFr77YRlaey/kPuUUCA2VpCCE6FE6mhQ+Ukp9rJS6Til1HfABsLS9F2itVwKl7SxyEfCKNtYAsUqplA7G0zWUgj/8Aex2an98ARVxeZSXrzDPWa0werQkBSFEj2LryEJa67uUUpcB07yzntdav3OC2+4L5DT7Pdc7L/8E19u5MjIgJ4fQ+Ghsa/qQn/8icXFnNj3nu2ZBCCF6gA7fZEdr/ZbW+g7vdKIJ4ZgopW5SSq1XSq0vKgrAvZOTkrDawkhMvIqioreaKqdmZJjbeB482PUxCSGEH7TbU1BKVQG6tacArbWOPoFt5wH9mv2e6p13BK3188DzABMmTGgtni6RkvJDDhx4hsLC1+jb9yctDzbPmhWosIToVE4n1Naaye0Gi8WMpPoetTaTx9P0M5jnfJPHY87Wbj65XGY53/K+1x4+OZ3Q0GAenU4zLySk5eTxQE1Ny6mhwWzD6TSPLlfLGH2T223mezzmZ7e75et822z+3nzvr/ln4Zus1qaffe+/uZoaqKw0pdIqK83vISEQHg5hYebRbjeFFOrrzWdVX2/iar5uiwWuvtqcGOlP7SYFrbU/S1m8C/xUKfU6MBmo0FqfXENHh4mMHEtExBjy818ySSE93TwhSUE042tUa2rM46FDLSenE2w20xD4HmtroawMysuhvMRNxbrvsA0bjCMqBIfDnNOgFJSUQFGR6aAWFUF19ZHb9zV2vobR1wjCkY1j88kXt9vdtZ+XvzRvxJtPVmvLxtZma5p8fxOLpWkdvkbelwh9iaZ5cvF95vqwXVatTaMfFQXR0ebOvxERJoH5vh8lJeaz9/2dHQ5Ths1qbbl+j8fM87cOHVM4Hkqp14DTgQSlVC7wAN7SGFrr5zAHqs8HdgE1dINS3EopUlJ+xK5dt1JdvZnIhDHQp48cbD5JNW/4fA3k4XttlZXmn9I3FRebxrm6ummqqjL/tFZrU+NhtZr11tebf3DfY+c0qlZCSMPzgRXXYXctUQri46F3bzOlpLTcM9W6ZUPXvAH0LXd44+ib7Haz59p8slpb9go8niP3mJv3Hpr3GsLCTCPna+js9pbvo3kszaeQELOsb1Kqqffg+6yt1pZ72mFhZhutNezi2PgtKWitrzzK8xq4xV/b95ekpO+Tnf0L8vNfYsiQP5khJEkKncbthooK0zCXljbtPVdUND36nm++TGVlU6PhG3Y41sY5JMTsyfXqZfbsIiMhKck82u0t96pdLtMwORzmdb5HX2Pqa6jCwsyeYfPJbm9KUr5Yw8IgLjeL2Nt+QEzJbkLTh8DWrbiz91Ifm9Q4nBAX1zV7iyJ4+S0p9FR2ezwJCRdz8OA/GTz4CSwZGfD55+Y/294zawSeCLfb7H3n50N2Nuza1TTl5TWNo/oa9JqaI7vgzVksphveq5dpIOPioH9/M8833uzbw2y+1+jbG46IMA2+b/Kty9etP3w8uMu88grcdJPJQqs+N5loxAisC/9A+BNPEB4eoLiE/61fb/7+v/qV6fq1prwcfvlLuPRSmN3e5V8nTpLCcUhJuZGiojc4ePBfpGRkmNbsu+9g1KhAh9ZlKipgzx7YuxcOHDDj28XFTY8HD5qpuPjIRj4x0RSjHT++aWjB16CHhzc1+L7H2FgzxcSYtrLTGu7qavj97+HGGyEytZNWehx+/Wt49FE4/XR44w0zLgRw5ZXwzDNw111N80422dnwj3+YysIJCYGOpiWP59jGkLQ2N9Bas8Y0wr7unO8IeUqK2QPxVUkGM0rw1Vewdq2Z7HZzfHHWLJgxg6Nmc5cLfvAD+PZb8zk+9RRcd13LL/k778Att5h/qKFD/Z4U0Fp3q2n8+PE60Dwej163bqxes2ao9mz+2gylvvpqoMPyi4ICrT/+WOsnntD6qqu0HjtW69jY1s8biY3VesgQradO1fqii7S+6Sat77tP6z//Wes339R6wwatKyoC/Y6auekmE/ipp2rtdAYmhtWrTQzXX39kDN9+q7VSWv/qV62/Njtb65Ur/R9jW95/v+nLMGCA1uvWBS4Wn8JC84WbPFlrq1XrW2/VurKy7eV37ND6j3/U+rLLtE5ObuuEqCMnpbS225t+T0rSeu5crc8+W2uHw8wLCdH6rLO0Xru27e0/+6xZ9g9/0Pq008zP55yj9d69Wufnm7hA68xM8w90AoD1ugNtbMAb+WOdToakoLXWBw++oZcvRx/MedV8Oe6+O9AhHTeXS+tt27R++22tH39c6xtu0HrGDPM9b/5/0K+f1uedp/Utt2j9+9+bhn79evPdbWgI9Ls4RkuXmjc1bZp5fOCB1pdzOrW+7jqte/fWevx48096551aP/201nl57W+jrk7rTZvaft7jMdtPStK6qqr1ZebP1zoqSuuSkpbzN2zQulcv0zj95S/tx9ERVVVa79vXsWXdbq0ffLCpsXr7ba379zeN4AsvtFy2ulrrZ57ROj3dfHn27Dm2uMrLtf7uO7PNthQWmp2yCy7Q2mYzcWVkaH3llebzSU3V+j//afmaVavMnovvyz1okNZXX631c89pvWWLWWdpqUkoNTXmfXz3ndbLlmn94ovm+3L33Vq/8Yb53DyepnUfOmT2pO68U+uUFJNsCgqOjLusTOuEBK1nzjSvd7tNQouIMFNsrEkwjz7aKf9gkhT8zONx6TVrhup168ZqT0aG+cJ3A/X1Zgf0tdfMd3bmTK0jI1s2/omJpq26/nqtn3pK688/17q4ONCRd6KSEvPPOmqU1rW1Wl97rdYWi9ZffNFyOZfLdI9A68sv13r2bK2HD9c6NNTMS03Veteu1rdx6JDZSwTzYbdmyRLz/PPPtx3rli1mmfvvb5r31VemwejfX+tZs8zzCxe2/vrqaq1feUXrrKzWn3e7TSOXmGjWM3682XPOz299+dJSrefMMctee61pMLXWuqioKZYf/UjrnTtNoxkXZ+aNG2eSW2SkSWLNG9HD7dxpYjjzzKZGPi7ObPfRR03v6MMPzRc4M7Ppi9u3r9a//KXWmzc3rWv1aq1HjzbPX3qpacSnT29a5/33a71/f9uxnKjNm8335eyzzfepuTvvNElr48aW8/fuNQlu1izTk+kkkhS6wIEDf9XLl6Pr5p1pvpAnEY/HjC7885/mu3fBBVqfcorpUfv+hxwOradM0fqnP9X6b38zvf/y8gAE63KZPbk5c0zX+d//PvIfqDN9//umsfF1xysrtU5LM41saamZ53abLhNo/cgjLV/v8ZgPKz7edJ+ys1s+f+iQadCUMh96RITJxM3V12s9eLBJTEcburr0Uq1jYsye5f/+ZxrXQYNM41Ffb54H02D6uFxmrz0lpekPfsYZWr/1VtP2Vq/WeuJE89zUqeb148aZ3y0Wrc891wy/XH21+dtMnWp6TDab2fs/vGF3ubS+556m7VksJpn+739m2b17mxLluedqnZNjXpOVZWK94Qathw1rev2oUVovWND03PDhLfdeQkLMe/rd78x7aes709Bg3psvmffrZ5JOW72zzvbCC2a7v/1t07ydO80Iw/XXd00MWpJCl3C76/WqVak697ZB5qMM4O60222Gch55ROsLLzT/u77/ndBQ05ueN0/re+/V+h//MDsnAR/yKSjQ+uGHTWMMWvfp0/RzWprZ++3IQQiPp+MHK95806z/oYfFcFndAAAgAElEQVRazv/qK9PYXXGFWd9Pf2qWu/fettf19ddmCKd/f6137zbzmieEV17ROjfX/DFGjjR77T4LF5r1f/jh0WP+2nvc6vLLzZ72kCGmQfVxOk2iA3MQZ+lS06D6GvuPP9b6sceaPtt+/ZqGTlJSzBeieQP/7bemcU9LM8lo4ECTLM4+W+vvfc8MvbTno4+0/s1vmj6T5txuk1DCw817ad5NjYszPe5Fi45MtD5FRVq/+655T4cOHf2za273bq0/+KDrv/gej+lxWixaL19u5l1yidlZOHCgy8KQpNBF9u//o970uPdL7fuDd5GyMtMbvu66luP/w4ebeX/5i+m9+nOn+7gUFZkDE74DdWedZfZgGxpMA/fmm01j/VFRpnFtzy9+YRr0X/yi/b2/ggKzdz9hQusNwyOPmG2efrp5vPPO9oc5tDbZNS7OHGj95huz52qxmIbW59NPTZK4+mqzvtJSk0xmzTr6+n3mzm3647Z2LMPl0vqHP2z6EgwebD7H5ut3uUwv7KyzTIO0YEH7B2H9adcura+5xnwPXnnFjNd39LPojqqqTC8oOVnrxYv1ET2HLiBJoYu4XNV6zTvecdM//enYV7Bvn9Y//7lp4Y8iJ0fr1183O7GZmabt8e1gXXmlaYcKC4/jTXSVhgazhxwba8axfvxjrbdvb3v5devM+K/NpvV//9v6Mr5/sIwM3TjOv2RJywamuto0hqedZsbMDh/K8XG5TKMOprHqaCO1YYN5TxaLmf75zyOXeeghs96//MUkL6XaPwh9uB07tL7xxtYPWPq43WaY5OmnzbCSOLn4ji/4emu+4zFdRJJCF9qz+0FdH4NuuPaSY3uhy9W0R/zww0c8XVtresm33mqGpn07gRERpif/wANaf/ll4M6mbOHQIfOlf/NNs8f96KPmTI7Fi7X+5BPTpfGNF59zjtZbt3ZsvWVl5nUJCUeeufLtt+bDmDrVNIKrVmk9ZozZxuzZ5kyO889v+keMjjYHVdtTXGwyb3tnu7Rm3TqTmNo6NdntNu/b4TBj4V04liy6hsvt0m5P+9+bsucX6Y8Ho7e9/AftdB/5j1vnrNOrc1brp1Y9pZ9a9ZTecGCDdrk7p6vf0aSgzLLdx4QJE/T69esDHUYLTmcph6Yk4qiPIWxrScdf+MgjcM89TQVs9uyhsi6Ed96Bt9+GZcvMFb6hoXDmmeZ6mOnTYcwYc5VuwBUWwn33wYcfQk7O0ZcfOtRcnHP++cd2Bdp338HkydCvH6xaZa5gq6qCSZNMwaKNGyHVe/GZy2Uu+LrvPrNMWhpceKGZpk83V8gFiKfwIPnTx1JXXUbq6m9w9E/r8Gu11pTVlREXGofqpKv33B4324q3sTZvLWvz1lJeV05GUgZjk8eSmZxJcmQySim01pTXlVNQXUBRTREDYgbQP6b/CcXh9rjZWriVvKo8SmpKKKktobimmMr6SiLsEUQ5oogKiSLKEUVkSCQOqwOHzYHD6iDUFkq4PZzIkMjGyWFzkFuZy7aibWwv3s624m3sq9hHZEgkvUJ70SusF/Hh8cQ4Ygi3hxNuDyfMHka4PZxaZy0F1QWN08FDB6l11eL2uHF5XLi1G601MaExJIQlEB8eT3xYPFaLlezSbHaV7WJX6S72lO0hzB7GjAEzOH3A6Zw+8HQykzPZX7Gfd3e8y7vfvcvKfStxeczFcKG2UEb2HklGUgZxoXF8lfcVGw5soN5d3+KziguNY8aAGZwx8AzOG3IeQ+OHHtdnrpTaoLWecNTlJCl0jtKfnkqvZ1bjvP5y7AtfNPUT2rNhA0yZApdeSv33r2fpxX/h1Ul/4v0t/amrMxdNXnCBaT/POMPUxjlpuFzw3HOm4T10CC67zFzNPXQoDBtmblWqVFOBorIyU8ti5szjLwXyySc0zJnNV/Onsf4nl5D24ttMXLKKPu8sMxnzMPUH83CXFBM+IsPvtSu01uwu2826A+soryunzlVHrbOWOlcdFfUV7C7bTXZZNrvLdlPnqmt8XXJkMv1j+tM/pj9Dew1lTPIYxiSN4ZRep2C1WKmsr+Sz3Z+xdOdSPtz1IXlVefQO701mcmbjlBiRSF5lHnlVeeRW5pJbmUtVQxVOtxOnx4nL48LlcWGz2AixhjROda46vs7/mkPOQwDEOGKIC4tjb/nexvgSIxIJtYVSUF1Ag7uhxXuOD4tnXMo4xqeMZ2j8UPKq8sguyya7NJvssmwa3A2MSxnHxD4TzdR3IuV15Xy2+zM+3/s5K/auoLyuvMU6LcpCVEgUNc4anB7nCf1N4kLjGBQ3iFpnLaW1pZTUljQ2xu2JDIkkKSKJcHs4NosNq8WKzWL2wMpqyyipLaG0thSP97a8kSGRnNLrFE7pdQqD4wZTVlvGin0r+K7kOwDCbGHUumoBGNl7JBcNu4gzBp7BgaoDZBVmseXgFrYc3EJZXRnjU8Zzar9TObXfqUxNnYpHe1i+dznL9yxnxb4V7C7bzd3T7uaxsx87rs9EkkIXc1blUXBzGqmLG1Cp/eGvf227nHZNDYwbx67yBJ65ZBl/f91Bebki0VbC/B/34vtXKSZPDmAdHl+Mr7xialCkppqpXz/YvNlccr95M5x9Njz9NAwf7pcQtNZkFWaxbPcylu1exspdn3GIlo1Tn6g+TOwzkcFxg8mrymNfxT72lu+loLoAMI1Dv5h+9Is20+BegxmeMJzhCcMZGDsQm8VGVX0Va/PWsipnFatyV7GjeAch1hBCbaGE2cMItYUS7YimT2QfUqJSSIlMITkymT3le/hy/5d8uf9L8qtbr/oeGRLJwNiBDI4bbKZegwm3h5NTkcO+in3sr9jPvop9ZJdm49amgl+4PZy0uDS2F2/H5XER7YhmVtosJvSZwM6SnWw6uImthVtbbaj7RvclxhGD3WrHbrFjs9iwWWy4tZsGd0PjZFEWMpMymdR3EpP6TmJI/BAsykJFXQVbDm5hU8EmNhVswq3dJEcmkxyZTFJEEr3CepFdls3G/I1szN/I1sKtjQ14n6g+DI4bTFpcGlZlZUP+BrYWbm18Xz6DYgdx5qAzOWPgGQzuNZj4sHgSwhOICY3BokxZinpXPVUNVVTVV1HdUE29u556Vz317vrGpFvdUN04HXIeIiUyhRG9RzAiYQSJEYktejJaaw45D1FeV06ts5YaZ03j5LA5SIlMISkyiciQyKN+Lz3aQ3ldOS6Pi97hvVvtMeVX5fPfff9lVc4qBsUO4sJhF3JKr1PaXafvvbdlX/k+rBYrqdHHV5JFkkIA7N//BMXv382Yhf2w7swxNXWeeMIU7vHSGj696M88/d4APlAXYLUqLr8crot/j7OeuQTbqi9g6lS/xKe1ZvPBzXy06yP2lu/Foz14tAe3duP2mEaj1lVLXUMNdZs34C4vY0gJZByE9ELz2PsQVKT14cBDvyB/ajoHqvOxWWyNe0txYXEttllVX0VOZQ75Vfk0uBsa91xdHhdWi5WE8AQSIxLpHd6buLA4DlYfZNnuZXyy+xM+zf6Ug4fMXe2GxQ/jrEFncvaHO5jy8ufsmTuddT+7lHUH1rPuwDr2le8jNTqVgbEDGRAzgAGxA7Bb7ORW5pJTmcP+iv3kVOZQWtt02/AQawh9ovqwv2I/Hu1BoRiVOIqMpAzcHrdpfFxmj7+stoz86nyKa4pbvL8BMQM4rf9pnNb/NKamTiUxIrExkTisjg4PsdS56vi26Fs2F2xm88HNfFfyHRlJGZw/5Hympk7Fbm3Zw3K6nWwr3kZpbSmp0an0jepLmL3ru5P1rnpyKnPoE9WHcPuRdX5qnDVsKtjE+gPribBHcOagMxkUN6jL4xSSFALC7a5l7dqhODyJjP33mainnjK7+5MmsXfcpbxaewkvf5TIdweiSAqv5OZfRHPzzeaWDFRXmz3xc86BxYvb3EZJTQnrDqxjbd5aNuRvoNZZ26KbG+JR9I5OITkqpXEPr9ZZy0e7PuLDXR827tH2Du+NzWLDoiyNk8PmIMwWRtieXEIPlsCwoWy3llFQ23QLVDtWnLRdk7pXWC/S4tKoc9WRU5FDRX1Fhz8/q7I27lUmhCcwK22WmQbPato7qq83ReMuvtiUOT1GpbWl7Cjewfbi7Wwv3s6+in0Mix/Gqf1OZXLqZGJDY9t9fYO7oXHsOSUyhX4x/dpdXoiThSSFACkoeJnt269j5MjXcewYwRuP7eYfXwzki6pMAKazkptSP2Te1gdwxIS2fPFdd1G36Cnue/UGluR+Qog1hDBbWOMwRk5FDtll2QAoFMMThhMbGtu45+0uLKCuMJ+iCCg7bKcxxhHDuaecy3mnnMfsU2aTHJl8ZPBaw803wwsvmOqhv/gFAEWHihrHP/Or8kmKTCIlMqVxKMXpcZoDbqXmgFt2WTbh9nAzZOMduukT1YdQW2iLBObyuCiuKabwUGHjFOOIYdbgWWQmZx61Oy2E6DhJCgGitZt168by9ntn8cyyiVSHbSMuuYq0oeUkRexFVedx1vh53HDur4hytNzT3bRxKVe/OIdvEuHCoRcSbg+nzlXXOIyREJ7A5L6TmdR3EuNTxrd8/cKF8POfw5w5MGoU9bu2czBnOwXFe9ENDYyfezO2RX9u/7SlBQvg8cdNKeeHH/bTJySECARJCgGgtebtNRv46UvPUJDwDoRWoFAtTq9TKLYVbyPGEcNN42/i1sm3khKZwpOrnuS+5feRUG/lpfetzP7fQXPXl7o6c8D3T38yp2I+9pg5Ham5RYvgttvMWUCvvdbyDB+32zTyTzxhTmVavNispzmn09Tzf+AB+MlP4M9/DvBRbiFEZ+toUgj4xWjHOp2MF69prfXyXf/TSQ9maB5Ec0+YTr9/vP7Tf2J0fUPpEcuuzV2r5785X1sesmjbb2x66NNDNQ+iL3/jcl38+fu6sbjZww831a8YN66pds3cuU3VE59+2sy75JL2a7o895y5ijgz09Tj0dpcqPXII6aYH5j6Ocd60ZYQoltArmjuOl+ucmnHz0dqfp6qx9/8f3rH3nJdWbleL1+O3rXrl22+bk/ZHn37h7frjP/L0K9sekV7PB5TWmH8eN14+fK552r92Wdmfk2NSRZRUab0g68ezsUXd6yswdKlpgBZaqq5otZ3pe+sWeaGKZIQhOixJCl0gaoqrW+7TWtGv655EH33K4tbPL9t2/V6+XKlS0s/O7YVf/GF1j/5Sdu1cQoKtL75ZlNnZ+7cY6tzs2mT6RmEhZk7j3W03IQQolvraFKQYwrH6dNPzX3W9+5zE3dPBkmJ8M1Ps1qcMeNyVbNx40SczjImTNiEw9HKGT8norDQ3Bf3WO5DC1BZafohMTGdG48Q4qTV0WMKcs7fMdIafvtbczlBSAg88MYSymzf8uAZ9x9xCqXNFsnIkW/idleybdtVaN32+f3HJTHx2BMCmBIckhCEEK2QpHAMPB649Va4/3645hrY+LWbNwt/w8jeI7l85OWtviYycjRDhvyZ8vLP2bfvd10csRBCHBtJCh3U0ABXX23O1rzjDvj73+H93Uv4tuhb7p9xP1aLtc3XJidfT1LStezd+xBlZZ93XdBCCHGMJCm0o95VzwsbXuDUv05nxC338tpb1Tz6KDz5JGjc/GZl+70EH6UUQ4c+S3j4cL799vvU1xd00TsQQohjI0mhFdUN1Ty1+inSFqVx0/s3sXHnAXanPkzsvcNIPf+faDws+db0Eu6bcV+7vQQfqzWCUaPM8YUdO35IdzvAL4QIDpIUDvPsumcZsHAAd35yJ2kxQxm27mPcf9zFI2mrGJLcl2veuYZTXzyV+5bfx4iEEcwbOa/D646IGEVa2hOUln5Ifv4LfnwXQghxfCQpNPNa1mvcsvQWxiaP5dP5q3D9dTnZH5/DkjcVv7pmKmtuWMPfL/o7+yr2sbN0Z4d7Cc317fsTYmPPYteuO6it3e2ndyKEEMdHrlPw2nBgA6f97TQm9JnAvy/5jAvPD2HdOlOl+ZJLWi5bVV/F6tzVzEqbdVy3JKyry2HdunQiI9PJzFyBUseWWIQQ4ljJdQrHoKC6gIsXX0xiRCIvn/8WF11gEsLixUcmBIAoRxTnDD7nuO9RGxrajyFDFlFR8SU5OX88weiFEKLzBH1SqHfVc+niSymtLeWdK/7DzVcnsmYNvP46XHqp/7ablHQNCQmXsGfPPVRXb/XfhoQQ4hgEdVLQWvP/Pvh/rM5dzd8v+jtr381k2TJ49llThdqfzGmqf8Fmi2H79mvweOr9u0EhhOiAoE4KL2x8gb9t+hv3Tr+XKdHz+OUvzb3ob7yxa7YfEtKbYcNeoLp6E9u3/xCtPV2zYSGEaEPQJoXqhmru/fxeZg6YyYOnP8TNN5v70Tz/fNfeXyYh4SIGDXqEwsJX2b37V123YSGEaEU792bs2RZ9tYiimiIeP/txXnvVwocfmpubDRrU9bH077+A+voccnKewOFIJTX1Z10fhBBCEKRJobyunN+v+j0XDr2QQSGTOf82mDoVbrklMPEopRgy5GkaGvLZtes2HI4+9O7t54MaQgjRiqAcPvrDqj9QXlfOb874DT/7GVRXw4svgjWAlwsoZWXEiFeJjp7Ct99eRXn5F4ELRggRtIIuKRQdKmLhVwu5YtQV5KzL5I03TCnsESMCHRlYrWGkp79HaOhAsrIupKJiVaBDEkIEGb8mBaXUbKXUDqXULqXUglaev04pVaSU2uSdbvBnPACP/+9xapw1PHT6Q7z8MqSmwi9/6e+tdpzdHs+YMZ8QEpLI5s2zKC39JNAhCSGCiN+SgjK1G54BzgNGAlcqpUa2suhirXWmd/qrv+IBOFB1gGfWPcM1GdcwPGE4a9fC9Olgt/tzq8cuNLQ/Y8d+QVjYELKyLqCo6K1AhySECBL+7ClMAnZprXdrrRuA14GL/Li9o3p45cO4PC4emPkA+fmQkwOTJgUyoraFhCSRmbmCqKiJfPPNFeTn/y3QIQkhgoA/k0JfIKfZ77neeYe7TCm1RSm1RCnVz1/B7C3fywsbX+CGsTcwKG4Q69aZ+SdrUgCw22MZM+YT4uLOZseOH5KTszDQIQkherhAH2h+Dxiotc4APgVebm0hpdRNSqn1Sqn1RUVFx7WhzQWbiQ2N5d4Z9wKwdq0522js2OOMvItYrRGkp79LQsJlZGf/nH37Hgl0SEKIHsyfSSEPaL7nn+qd10hrXaK19hX9+SswvrUVaa2f11pP0FpP6N2793EFc9Hwi8j5eQ59o01nZe1ayMiAsLDjWl2XslgcjBz5OomJV7Fnzz3s3n2v3LlNCOEX/kwK64AhSqlBSqkQ4HvAu80XUEqlNPt1LrDNj/HgsDkA8Hhg3bqTe+jocBaLjREjXiYl5Qb273+Y7OxfSGIQQnQ6v13RrLV2KaV+CnwMWIGXtNbfKKV+A6zXWr8L3KqUmgu4gFLgOn/F09yuXVBe3r2SApgL3IYOfR6LJZzc3KfweGoZMuTPKBXoUUAhRE/h1zIXWuulwNLD5t3f7OdfAV1eBW7tWvPY3ZICmJIYp5yyEIsljJycx3G5yhk+/G9YLI5AhyaE6AGCsvbR2rUQEXFyXMV8PJRSpKU9it0ex+7dC6ivP8Do0e9gt8cFOjQhRDcXlOMOa9fChAmBrXV0opRS9O9/NyNGvEpl5Wq+/noadXX7Ah2WEKKbC7qk0NAAX3/dPYeOWpOUdCVjxnxCQ0M+GzdOoapqY6BDEkJ0Y0GXFDZvNolh8uRAR9J5YmNnMnbs/1AqhK+/nkFh4ZuBDkkI0U0FXVLozgeZ2xMRMZJx49YQGZnBt99ewc6dt+PxNAQ6LCFENxOUSSE52VRH7WkcjhQyM1eQmno7eXl/YtOmmdTV5Rz9hUII4RWUSWHSpK69D3NXslhCOOWUPzJy5JscOvQNGzaMo6Tko0CHJYToJoIqKVRUwPbtPW/oqDWJiZczfvx6QkKSyco6j6ysC6mu3hLosIQQJ7mgSgrr15vHYEgKAOHhQxk3bi1paY9RUfEl69dnsm3bNdTW7g50aEKIk1RQJQXfQeYJEwIbR1eyWsPo3/9uJk/eTf/+d1NU9BZr1w5n165f4HbXBTo8IcRJJuiSwtChEBeEF/7a7XGkpT3K5Mm7SE7+Abm5f2DDhglUVW0KdGhCiJNI0CWFYBk6aovD0Ydhw14gPf1DXK5SNm6cxP79j6O1O9ChCSFOAkGTFPLy4MABSQo+8fGzmTgxi/j4uezevYBNm86gpmZHoMMSQgRY0CSFnnrR2omw2+MZNepNhg9/merqTaxdO5Jt235ATc2uQIcmhAiQoEkKY8fCwoWQmRnoSE4uSimSk69l8uRdpKb+nKKiN1i7djjbt/+I2to9gQ5PCNHFVHe7e9eECRP0et+5paLT1dfns3//4xw48BzgJjX1TgYOvA+rNSLQoQkhToBSaoPW+qjnXgZNT0F0jMORwpAhC5kyJZukpGvIyXmctWtHUVz87tFfLITo9iQpiFY5HH0ZPvwlMjO/wGqNZOvWi8jKukju2SBEDydJQbQrNvY0Jkz4mrS0xykrW8ZXXw1h69ZLKCr6t1RhFaIHCsrbcYpjY7HY6d//lyQmfo/c3D9x8OC/KC7+NzZbPImJ3yMx8QqioyfLfaKF6AHkQLM4Zh6Pi7KyTykoeJni4n+jdT1KOYiOnkJs7ExiY2cSEzNNkoQQJ5GOHmiWpCBOiMtVQXn5CsrL/0t5+X+prt4EeLDZYklIuIykpO8TGzsTpbrxDbGF6AE6mhRk+EicEJsthoSEi0hIuAjwJYn/UlT0JkVFiykoeJGQkBQSE+eTlHQNkZFjUT31ZhZC9ADSUxB+43bXUFLyPoWFr1FSshStGwgPH0Vy8rUkJV2Fw9E30CEKETRk+EicVJzOUgoL3+DgwVeorFwNKGJiphEePpzQ0DRCQwcRFpZGePgwbLaYQIcrRI8jw0fipGK396Jv3x/Tt++PqanZycGD/6Ss7BOKi9/F6SxssWxY2ClERU0gKmoCkZHjiYgYid3eW4adhOgC0lMQAedyVVNXt5e6ut0cOrSVqqoNVFWtp75+f+MyVmskoaFphIUNJixsMBERY4iKGkd4+DA5iC1EB0hPQXQbNlskkZGjiYwcTULC3Mb5DQ2FVFVtpLb2O2prs6mr201NzXbv8Yl6ACyWcCIjxxAZOYawsFMIDR3sTRxpUq9JiOMgSUGctEJCEomPnw3MbjHf43FRW7uDqqqNVFdvpKpqI4WFr+NylbdYzm5PICSkDw5HH0JC+hASkkJISCI2Wy/s9njv1JvQ0P7S2xDCS5KC6HYsFhsREaOIiBgFXNM43+ks8/Yosqmtzaa+Pof6+gM0NBygujqLhoYC4Mg7zFksYUREZBAZmemd0nE4+hESkoLFYu+6NybESUCSgugx7PY47PYJREe3PmyqtRuXqxynswSnswSXq5SGhoMcOpRFdfUmiooWk5//l2avUNjtiTgcfbHZYr23LHWjtRutPYSE9CYsbBjh4WYKCxtKSEhv6XWIbk2SgggaSlkbh41ao7Wmvn4/hw59S319LvX1eTQ05FFfn4fLVYlSVpSyY7GEAhbq6vZSWvpp4/EN71aw2WKx2+Mbh6nMYxw2Wy9stjgslhDc7mrc7ipcrirc7mpCQhK9vZUxhIUNwWKxHREbeCThCL+TpCCEl1KK0NABhIYO6PBrtHZTV7ef2trvqKnZidNZ1NgLcTpLaGgopKZmBy5XmfeYR/Oz/RRWaxRWawROZxFauwCwWEIJDx+O1hq3uxKXqxK3uwKAqKhJxMaeQVzcGURHn4rVGta4No/HhdtdhcUShtUa2hkfiQhCckqqEF3EDF9V4PE0YLNFYbGEN1574fHUU1OznerqzVRXb6amZhtK2bDZYrBao7HZYvB4Gqio+IKqqvWYXkMIoaEDcLurveutadyW3Z5IaGh/HI5+OBypKGXF42lAa6d30t7htgRstnjs9gTvFN84NS9o6PG48Hhq8XjqsVoj2006WnvQ2nNEb0cElpySKsRJxgxf9Wr1OYvF0Xhq7dG4XJVUVHxBWdly6utzsNmisVpjvI/RuN3V1Nfvp65uPzU1Oygr+8y7fTsWix2l7N71lOF2V7e5HYslwptMatHaedhzYd5hsV5YrZHexFSOy1WB210F0Hg8xjfZbLEoZWtjsnvjC8FqjfImwxhsthgsFod3qK0St7vK2xsKJSTErNduT5ALGzuRJAUhuhmbLZr4+DnEx8854XV5PPXeA+/F3qkUl6uk8WC81m6s1jAsljAslnBvA13tHR4rxeUqxeWqIiQkyduIxzaWKWloOEB9fR51dfuoqFiF213lHSLznHDczSkVgsPRB6VCvCcDmJ6KOSlAY4bsPGitsVhCCQsb5L0Q0pRXMceYLChlOezR2uxRobXL28tqwONx4vHUNSYp3/EhrV0o1fK1ISHJhIWdQljY4MZem++zb2gopKHhIB7PIWy2eEJCErHb4wN67EiSghBBzGJx4HCYazm6ihlecnsbWFdjY+vxmAbXdwzF5TKT1g3eYy9R3l5EFG53beNJAOaEgAPehNO8QbYAqvERFG73Ie8JAh/S0JDfye/M4u2Fub1J6cjk50tg5r2VtbEehc3WC6s1/LAhPyepqXeSlva7To67JUkKQoguZfakLUBgrwFxu2uoq9vbeAKAryH3nXLs+9nX81DKhsUS0myoy9EiUTU/RgTmjDGt3TQ05FFbm+2ddlFfn4vNFkdISBIhIcneXlaEt3dWRENDIU5nER5PLUqFNBv2CyEm5jS/fy6SFIQQQclqDSciYqTf1q+UQilb4xltcXFn+m1bncniz5UrpWYrpXYopXYppRa08rxDKbXY+/xXSqmB/oxHCCFE+/yWFJQZ2HsGOA8YCVyplDo8Lf8IKNNanwL8EXjcX/EIIYQ4On/2FCYBu7TWu7XWDcDrwGkLptcAAAZ0SURBVEWHLXMR8LL35yXAWUrOLRNCiIDxZ1LoC+Q0+z3XO6/VZbQ5daACaL0GgRBCCL/z6zGFzqKUukkptV4ptb6oqCjQ4QghRI/lz6SQB/Rr9nuqd16ryyilbEAMUHL4irTWz2utJ2itJ/Tu3dtP4QohhPBnUlgHDFFKDVJKhQDfA949bJl3gR94f74c+Fx3t2JMQgjRg/jtOgWttUsp9VPgY8AKvKS1/kYp9Rtgvdb6XeBF4B9KqV1AKSZxCCGECJBuVyVVKVUE7DvOlycAxZ0YTleT+AOnO8cO3Tv+7hw7nDzxD9BaH3X8vdslhROhlFrfkdKxJyuJP3C6c+zQvePvzrFD94u/W5x9JIQQomtIUhBCCNEo2JLC84EO4ARJ/IHTnWOH7h1/d44duln8QXVMQQghRPuCracghBCiHUGTFI5Wxvtko5R6SSlVqJTa2mxeL6XUp0qpnd7HuEDG2BalVD+l1HKl1LdKqW+UUrd553eX+EOVUmuVUpu98T/knT/IW+J9l7fke0igY22LUsqqlPpaKfW+9/fuFPtepVSWUmqTUmq9d153+e7EKqWWKKW2K6W2KaWmdpfYfYIiKXSwjPfJ5u/A7MPmLQA+01oPAT7z/n4ycgF3aq1HAlOAW7yfd3eJvx44U2s9BsgEZiulpmBKu//RW+q9DFP6/WR1G7Ct2e/dKXaAM7TWmc1O5ewu350/AR9prYcDYzB/g+4Su2FuGdezJ2Aq8HGz338F/CrQcXUg7oHA1ma/7wBSvD+nADsCHWMH38d/gFndMX4gHNgITMZcgGRr7Tt1Mk2YOmOfAWcC72NuUNwtYvfGtxdIOGzeSf/dwdRu24P3WG13ir35FBQ9BTpWxrs7SNJa++42XgAkBTKYjvDeTW8s8BXdKH7v8MsmoBD4FMgGyrUp8Q4n93doIfBLmu4cH0/3iR1AA58opTYopW7yzusO351BQBHwN+/Q3V+VUhF0j9gbBUtS6HG02e04qU8dU0pFAm8Bt2utK5s/d7LHr7V2a60zMXvdk4DhAQ6pQ5RSFwCFWusNgY7lBJymtR6HGe69RSk1o/mTJ/F3xwaMA/5Paz0WOMRhQ0UnceyNgiUpdKSMd3dwUCmVAuB9LAxwPG1SStkxCeFfWuu3vbO7Tfw+/7+9+3mxKQ7jOP7+SMmvDMWGIpSkpJSFoURZWMiClB9JljZ2kl/lD2ClWFgQSWQsLA1NWfgV43chKSOyQSxIPBbf556uQTNNmXtP83nVac793nNPz6nvmeec7+k834j4CFyjDLl0ZIl3aN8+1AmslfSKMtvhSso4dx1iByAi3uTf90AXJSnXoe/0AX0RcTM/X6AkiTrEXhkpSWEwZbzroLnU+DbKWH3bySlVTwBPI+Jw01d1iX+qpI5cH0t5HvKUkhzW52ZtGX9E7ImIGRExi9LPr0bEZmoQO4Ck8ZImNtaB1cAjatB3IuId8FrSvGxaBTyhBrH/ptUPNYZrAdYAzyhjw3tbHc8g4j0LvAW+U65AdlDGhruB58AVYEqr4/xH7Msot8gPgN5c1tQo/oXAvYz/EXAg22cDt4AXwHlgTKtjHeA4VgCX6xR7xnk/l8eNc7VGfWcRcCf7ziVgcl1ibyx+o9nMzCojZfjIzMwGwUnBzMwqTgpmZlZxUjAzs4qTgpmZVZwUzIaRpBWNyqVm7chJwczMKk4KZn8haUvOqdAr6XgWyPsi6UjOsdAtaWpuu0jSDUkPJHU16uVLmivpSs7LcFfSnNz9hKaa+2fyDXCztuCkYNaPpPnARqAzSlG8H8BmYDxwJyIWAD3AwfzJKWB3RCwEHja1nwGORpmXYSnlDXUoVWN3Ueb2mE2pV2TWFkYPvInZiLMKWAzczov4sZQiZj+Bc7nNaeCipElAR0T0ZPtJ4HzW75keEV0AEfEVIPd3KyL68nMvZd6M6///sMwG5qRg9icBJyNiz2+N0v5+2w21Rsy3pvUf+Dy0NuLhI7M/dQPrJU2Dan7gmZTzpVFpdBNwPSI+AR8kLc/2rUBPRHwG+iSty32MkTRuWI/CbAh8hWLWT0Q8kbSPMvvXKEql2p2USVOW5HfvKc8doJRDPpb/9F8C27N9K3Bc0qHcx4ZhPAyzIXGVVLNBkvQlIia0Og6z/8nDR2ZmVvGdgpmZVXynYGZmFScFMzOrOCmYmVnFScHMzCpOCmZmVnFSMDOzyi9+AOcHxNsdawAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 945us/sample - loss: 0.8833 - acc: 0.7666\n",
      "Loss: 0.883316714778496 Accuracy: 0.7665628\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.4018 - acc: 0.2090\n",
      "Epoch 00001: val_loss improved from inf to 1.67515, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_6_conv_checkpoint/001-1.6752.hdf5\n",
      "36805/36805 [==============================] - 94s 3ms/sample - loss: 2.4017 - acc: 0.2090 - val_loss: 1.6752 - val_acc: 0.4663\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5659 - acc: 0.4914\n",
      "Epoch 00002: val_loss improved from 1.67515 to 1.34717, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_6_conv_checkpoint/002-1.3472.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 1.5657 - acc: 0.4914 - val_loss: 1.3472 - val_acc: 0.5840\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3392 - acc: 0.5676\n",
      "Epoch 00003: val_loss improved from 1.34717 to 1.20077, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_6_conv_checkpoint/003-1.2008.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 1.3392 - acc: 0.5676 - val_loss: 1.2008 - val_acc: 0.6084\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2186 - acc: 0.6087\n",
      "Epoch 00004: val_loss improved from 1.20077 to 1.07433, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_6_conv_checkpoint/004-1.0743.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 1.2186 - acc: 0.6088 - val_loss: 1.0743 - val_acc: 0.6730\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1096 - acc: 0.6462\n",
      "Epoch 00005: val_loss improved from 1.07433 to 0.97878, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_6_conv_checkpoint/005-0.9788.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 1.1095 - acc: 0.6462 - val_loss: 0.9788 - val_acc: 0.7067\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9998 - acc: 0.6863\n",
      "Epoch 00006: val_loss improved from 0.97878 to 0.90228, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_6_conv_checkpoint/006-0.9023.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.9997 - acc: 0.6863 - val_loss: 0.9023 - val_acc: 0.7303\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8967 - acc: 0.7213\n",
      "Epoch 00007: val_loss improved from 0.90228 to 0.75282, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_6_conv_checkpoint/007-0.7528.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.8969 - acc: 0.7213 - val_loss: 0.7528 - val_acc: 0.7813\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8122 - acc: 0.7486\n",
      "Epoch 00008: val_loss improved from 0.75282 to 0.67530, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_6_conv_checkpoint/008-0.6753.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.8121 - acc: 0.7486 - val_loss: 0.6753 - val_acc: 0.8055\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7303 - acc: 0.7766\n",
      "Epoch 00009: val_loss improved from 0.67530 to 0.61138, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_6_conv_checkpoint/009-0.6114.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.7302 - acc: 0.7766 - val_loss: 0.6114 - val_acc: 0.8283\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6572 - acc: 0.7998\n",
      "Epoch 00010: val_loss improved from 0.61138 to 0.58938, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_6_conv_checkpoint/010-0.5894.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.6572 - acc: 0.7998 - val_loss: 0.5894 - val_acc: 0.8314\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5928 - acc: 0.8207\n",
      "Epoch 00011: val_loss improved from 0.58938 to 0.56331, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_6_conv_checkpoint/011-0.5633.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.5927 - acc: 0.8208 - val_loss: 0.5633 - val_acc: 0.8397\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5548 - acc: 0.8290\n",
      "Epoch 00012: val_loss improved from 0.56331 to 0.49751, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_6_conv_checkpoint/012-0.4975.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.5547 - acc: 0.8290 - val_loss: 0.4975 - val_acc: 0.8591\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5151 - acc: 0.8419\n",
      "Epoch 00013: val_loss did not improve from 0.49751\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.5151 - acc: 0.8419 - val_loss: 0.5345 - val_acc: 0.8526\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4735 - acc: 0.8566\n",
      "Epoch 00014: val_loss did not improve from 0.49751\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.4735 - acc: 0.8565 - val_loss: 0.5197 - val_acc: 0.8551\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4500 - acc: 0.8650\n",
      "Epoch 00015: val_loss improved from 0.49751 to 0.45783, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_6_conv_checkpoint/015-0.4578.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.4499 - acc: 0.8650 - val_loss: 0.4578 - val_acc: 0.8691\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4157 - acc: 0.8744\n",
      "Epoch 00016: val_loss did not improve from 0.45783\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.4157 - acc: 0.8744 - val_loss: 0.4581 - val_acc: 0.8703\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3902 - acc: 0.8835\n",
      "Epoch 00017: val_loss improved from 0.45783 to 0.45472, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_6_conv_checkpoint/017-0.4547.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.3902 - acc: 0.8835 - val_loss: 0.4547 - val_acc: 0.8754\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3711 - acc: 0.8863\n",
      "Epoch 00018: val_loss improved from 0.45472 to 0.43325, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_6_conv_checkpoint/018-0.4332.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.3711 - acc: 0.8863 - val_loss: 0.4332 - val_acc: 0.8798\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3529 - acc: 0.8923\n",
      "Epoch 00019: val_loss improved from 0.43325 to 0.39886, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_6_conv_checkpoint/019-0.3989.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.3530 - acc: 0.8923 - val_loss: 0.3989 - val_acc: 0.8912\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3342 - acc: 0.8973\n",
      "Epoch 00020: val_loss improved from 0.39886 to 0.38753, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_6_conv_checkpoint/020-0.3875.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.3342 - acc: 0.8973 - val_loss: 0.3875 - val_acc: 0.8961\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3054 - acc: 0.9054\n",
      "Epoch 00021: val_loss did not improve from 0.38753\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.3053 - acc: 0.9054 - val_loss: 0.3962 - val_acc: 0.8889\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2957 - acc: 0.9101\n",
      "Epoch 00022: val_loss did not improve from 0.38753\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2958 - acc: 0.9100 - val_loss: 0.3953 - val_acc: 0.8975\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2865 - acc: 0.9120\n",
      "Epoch 00023: val_loss did not improve from 0.38753\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2866 - acc: 0.9119 - val_loss: 0.4049 - val_acc: 0.8952\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2694 - acc: 0.9154\n",
      "Epoch 00024: val_loss improved from 0.38753 to 0.36484, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_6_conv_checkpoint/024-0.3648.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2694 - acc: 0.9154 - val_loss: 0.3648 - val_acc: 0.9059\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2552 - acc: 0.9205\n",
      "Epoch 00025: val_loss did not improve from 0.36484\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2552 - acc: 0.9205 - val_loss: 0.3684 - val_acc: 0.9061\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2361 - acc: 0.9248\n",
      "Epoch 00026: val_loss improved from 0.36484 to 0.36309, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_6_conv_checkpoint/026-0.3631.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2361 - acc: 0.9248 - val_loss: 0.3631 - val_acc: 0.9059\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2299 - acc: 0.9275\n",
      "Epoch 00027: val_loss did not improve from 0.36309\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2299 - acc: 0.9274 - val_loss: 0.3798 - val_acc: 0.9068\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2234 - acc: 0.9293\n",
      "Epoch 00028: val_loss improved from 0.36309 to 0.36234, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_6_conv_checkpoint/028-0.3623.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2234 - acc: 0.9293 - val_loss: 0.3623 - val_acc: 0.9026\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2078 - acc: 0.9340\n",
      "Epoch 00029: val_loss did not improve from 0.36234\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2078 - acc: 0.9340 - val_loss: 0.3760 - val_acc: 0.9119\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1986 - acc: 0.9359\n",
      "Epoch 00030: val_loss did not improve from 0.36234\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1986 - acc: 0.9359 - val_loss: 0.4064 - val_acc: 0.8966\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1894 - acc: 0.9395\n",
      "Epoch 00031: val_loss did not improve from 0.36234\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1893 - acc: 0.9395 - val_loss: 0.4091 - val_acc: 0.8984\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1828 - acc: 0.9418\n",
      "Epoch 00032: val_loss did not improve from 0.36234\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1828 - acc: 0.9418 - val_loss: 0.3846 - val_acc: 0.9015\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1801 - acc: 0.9427\n",
      "Epoch 00033: val_loss did not improve from 0.36234\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1801 - acc: 0.9427 - val_loss: 0.4071 - val_acc: 0.9075\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1648 - acc: 0.9471\n",
      "Epoch 00034: val_loss did not improve from 0.36234\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1648 - acc: 0.9471 - val_loss: 0.3913 - val_acc: 0.9122\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1674 - acc: 0.9442\n",
      "Epoch 00035: val_loss did not improve from 0.36234\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1674 - acc: 0.9442 - val_loss: 0.3915 - val_acc: 0.9147\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1545 - acc: 0.9489\n",
      "Epoch 00036: val_loss did not improve from 0.36234\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1544 - acc: 0.9489 - val_loss: 0.4010 - val_acc: 0.9071\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1508 - acc: 0.9515\n",
      "Epoch 00037: val_loss did not improve from 0.36234\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1508 - acc: 0.9515 - val_loss: 0.3681 - val_acc: 0.9180\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1456 - acc: 0.9531\n",
      "Epoch 00038: val_loss did not improve from 0.36234\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1456 - acc: 0.9531 - val_loss: 0.3964 - val_acc: 0.9080\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1410 - acc: 0.9545\n",
      "Epoch 00039: val_loss did not improve from 0.36234\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1410 - acc: 0.9545 - val_loss: 0.4005 - val_acc: 0.9045\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1352 - acc: 0.9563\n",
      "Epoch 00040: val_loss did not improve from 0.36234\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1352 - acc: 0.9563 - val_loss: 0.4063 - val_acc: 0.9126\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1307 - acc: 0.9570\n",
      "Epoch 00041: val_loss did not improve from 0.36234\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1307 - acc: 0.9570 - val_loss: 0.3728 - val_acc: 0.9166\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1328 - acc: 0.9567\n",
      "Epoch 00042: val_loss did not improve from 0.36234\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1328 - acc: 0.9567 - val_loss: 0.4009 - val_acc: 0.9164\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1247 - acc: 0.9601\n",
      "Epoch 00043: val_loss did not improve from 0.36234\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1247 - acc: 0.9601 - val_loss: 0.4229 - val_acc: 0.9133\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1246 - acc: 0.9591\n",
      "Epoch 00044: val_loss improved from 0.36234 to 0.36054, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_6_conv_checkpoint/044-0.3605.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1246 - acc: 0.9591 - val_loss: 0.3605 - val_acc: 0.9220\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1105 - acc: 0.9628\n",
      "Epoch 00045: val_loss improved from 0.36054 to 0.35622, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_6_conv_checkpoint/045-0.3562.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1105 - acc: 0.9627 - val_loss: 0.3562 - val_acc: 0.9227\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1134 - acc: 0.9620\n",
      "Epoch 00046: val_loss did not improve from 0.35622\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1134 - acc: 0.9620 - val_loss: 0.3747 - val_acc: 0.9217\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1120 - acc: 0.9626\n",
      "Epoch 00047: val_loss did not improve from 0.35622\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1120 - acc: 0.9626 - val_loss: 0.3807 - val_acc: 0.9157\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1084 - acc: 0.9635\n",
      "Epoch 00048: val_loss did not improve from 0.35622\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1083 - acc: 0.9635 - val_loss: 0.3928 - val_acc: 0.9152\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1038 - acc: 0.9651\n",
      "Epoch 00049: val_loss did not improve from 0.35622\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1040 - acc: 0.9651 - val_loss: 0.4237 - val_acc: 0.9150\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1041 - acc: 0.9658\n",
      "Epoch 00050: val_loss did not improve from 0.35622\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1042 - acc: 0.9657 - val_loss: 0.4074 - val_acc: 0.9182\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1021 - acc: 0.9670\n",
      "Epoch 00051: val_loss did not improve from 0.35622\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1021 - acc: 0.9670 - val_loss: 0.4040 - val_acc: 0.9157\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0942 - acc: 0.9694\n",
      "Epoch 00052: val_loss did not improve from 0.35622\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0942 - acc: 0.9694 - val_loss: 0.4090 - val_acc: 0.9231\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1004 - acc: 0.9667\n",
      "Epoch 00053: val_loss did not improve from 0.35622\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1004 - acc: 0.9667 - val_loss: 0.4178 - val_acc: 0.9208\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0944 - acc: 0.9683\n",
      "Epoch 00054: val_loss did not improve from 0.35622\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0944 - acc: 0.9683 - val_loss: 0.3780 - val_acc: 0.9213\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0860 - acc: 0.9720\n",
      "Epoch 00055: val_loss did not improve from 0.35622\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0860 - acc: 0.9720 - val_loss: 0.4317 - val_acc: 0.9143\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0869 - acc: 0.9714\n",
      "Epoch 00056: val_loss did not improve from 0.35622\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0869 - acc: 0.9714 - val_loss: 0.4315 - val_acc: 0.9180\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0885 - acc: 0.9710\n",
      "Epoch 00057: val_loss did not improve from 0.35622\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0885 - acc: 0.9710 - val_loss: 0.4095 - val_acc: 0.9182\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0867 - acc: 0.9713\n",
      "Epoch 00058: val_loss did not improve from 0.35622\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0867 - acc: 0.9713 - val_loss: 0.4331 - val_acc: 0.9201\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0829 - acc: 0.9730\n",
      "Epoch 00059: val_loss did not improve from 0.35622\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0829 - acc: 0.9730 - val_loss: 0.3983 - val_acc: 0.9196\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0832 - acc: 0.9732\n",
      "Epoch 00060: val_loss did not improve from 0.35622\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0832 - acc: 0.9732 - val_loss: 0.4100 - val_acc: 0.9187\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0833 - acc: 0.9722\n",
      "Epoch 00061: val_loss did not improve from 0.35622\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0833 - acc: 0.9722 - val_loss: 0.4045 - val_acc: 0.9231\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0808 - acc: 0.9733\n",
      "Epoch 00062: val_loss did not improve from 0.35622\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0808 - acc: 0.9733 - val_loss: 0.4311 - val_acc: 0.9215\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0800 - acc: 0.9730\n",
      "Epoch 00063: val_loss did not improve from 0.35622\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0800 - acc: 0.9730 - val_loss: 0.4199 - val_acc: 0.9250\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0702 - acc: 0.9778\n",
      "Epoch 00064: val_loss did not improve from 0.35622\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0702 - acc: 0.9778 - val_loss: 0.4208 - val_acc: 0.9180\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0751 - acc: 0.9756\n",
      "Epoch 00065: val_loss did not improve from 0.35622\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0751 - acc: 0.9756 - val_loss: 0.4280 - val_acc: 0.9171\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0767 - acc: 0.9748\n",
      "Epoch 00066: val_loss did not improve from 0.35622\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0767 - acc: 0.9748 - val_loss: 0.4315 - val_acc: 0.9210\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0723 - acc: 0.9764\n",
      "Epoch 00067: val_loss did not improve from 0.35622\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0723 - acc: 0.9764 - val_loss: 0.4540 - val_acc: 0.9185\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0678 - acc: 0.9775\n",
      "Epoch 00068: val_loss did not improve from 0.35622\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0679 - acc: 0.9775 - val_loss: 0.4024 - val_acc: 0.9210\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0718 - acc: 0.9769\n",
      "Epoch 00069: val_loss did not improve from 0.35622\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0718 - acc: 0.9769 - val_loss: 0.4177 - val_acc: 0.9236\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0694 - acc: 0.9777\n",
      "Epoch 00070: val_loss did not improve from 0.35622\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0694 - acc: 0.9777 - val_loss: 0.4301 - val_acc: 0.9231\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0663 - acc: 0.9778\n",
      "Epoch 00071: val_loss did not improve from 0.35622\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0663 - acc: 0.9777 - val_loss: 0.4098 - val_acc: 0.9243\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0692 - acc: 0.9781\n",
      "Epoch 00072: val_loss did not improve from 0.35622\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0692 - acc: 0.9781 - val_loss: 0.4099 - val_acc: 0.9269\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0659 - acc: 0.9788\n",
      "Epoch 00073: val_loss did not improve from 0.35622\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0659 - acc: 0.9788 - val_loss: 0.4144 - val_acc: 0.9224\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0625 - acc: 0.9800\n",
      "Epoch 00074: val_loss did not improve from 0.35622\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0625 - acc: 0.9800 - val_loss: 0.4164 - val_acc: 0.9266\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0658 - acc: 0.9783\n",
      "Epoch 00075: val_loss did not improve from 0.35622\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0658 - acc: 0.9783 - val_loss: 0.4008 - val_acc: 0.9262\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0638 - acc: 0.9792\n",
      "Epoch 00076: val_loss did not improve from 0.35622\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0638 - acc: 0.9792 - val_loss: 0.4024 - val_acc: 0.9248\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0614 - acc: 0.9799\n",
      "Epoch 00077: val_loss did not improve from 0.35622\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0613 - acc: 0.9799 - val_loss: 0.4175 - val_acc: 0.9220\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0608 - acc: 0.9798\n",
      "Epoch 00078: val_loss did not improve from 0.35622\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0608 - acc: 0.9798 - val_loss: 0.4188 - val_acc: 0.9203\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0565 - acc: 0.9818\n",
      "Epoch 00079: val_loss did not improve from 0.35622\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0566 - acc: 0.9818 - val_loss: 0.4287 - val_acc: 0.9259\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0667 - acc: 0.9789\n",
      "Epoch 00080: val_loss did not improve from 0.35622\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0667 - acc: 0.9789 - val_loss: 0.4161 - val_acc: 0.9222\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0588 - acc: 0.9818\n",
      "Epoch 00081: val_loss did not improve from 0.35622\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0588 - acc: 0.9818 - val_loss: 0.4386 - val_acc: 0.9187\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0548 - acc: 0.9826\n",
      "Epoch 00082: val_loss did not improve from 0.35622\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0548 - acc: 0.9826 - val_loss: 0.4204 - val_acc: 0.9206\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0546 - acc: 0.9821\n",
      "Epoch 00083: val_loss did not improve from 0.35622\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0546 - acc: 0.9821 - val_loss: 0.4875 - val_acc: 0.9154\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0591 - acc: 0.9806\n",
      "Epoch 00084: val_loss did not improve from 0.35622\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0591 - acc: 0.9806 - val_loss: 0.4242 - val_acc: 0.9248\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0563 - acc: 0.9821\n",
      "Epoch 00085: val_loss did not improve from 0.35622\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0563 - acc: 0.9821 - val_loss: 0.4311 - val_acc: 0.9203\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0547 - acc: 0.9827\n",
      "Epoch 00086: val_loss did not improve from 0.35622\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0547 - acc: 0.9827 - val_loss: 0.4451 - val_acc: 0.9243\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0542 - acc: 0.9825\n",
      "Epoch 00087: val_loss did not improve from 0.35622\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0543 - acc: 0.9825 - val_loss: 0.4325 - val_acc: 0.9222\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0533 - acc: 0.9827\n",
      "Epoch 00088: val_loss did not improve from 0.35622\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0533 - acc: 0.9827 - val_loss: 0.4176 - val_acc: 0.9248\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0533 - acc: 0.9824\n",
      "Epoch 00089: val_loss did not improve from 0.35622\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0533 - acc: 0.9824 - val_loss: 0.4247 - val_acc: 0.9264\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0494 - acc: 0.9841\n",
      "Epoch 00090: val_loss did not improve from 0.35622\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0494 - acc: 0.9841 - val_loss: 0.4201 - val_acc: 0.9273\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0541 - acc: 0.9835\n",
      "Epoch 00091: val_loss did not improve from 0.35622\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0540 - acc: 0.9835 - val_loss: 0.4246 - val_acc: 0.9231\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0474 - acc: 0.9846\n",
      "Epoch 00092: val_loss did not improve from 0.35622\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0474 - acc: 0.9846 - val_loss: 0.4136 - val_acc: 0.9290\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0500 - acc: 0.9845\n",
      "Epoch 00093: val_loss did not improve from 0.35622\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0500 - acc: 0.9845 - val_loss: 0.3928 - val_acc: 0.9304\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0461 - acc: 0.9862\n",
      "Epoch 00094: val_loss did not improve from 0.35622\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0461 - acc: 0.9862 - val_loss: 0.4322 - val_acc: 0.9290\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0458 - acc: 0.9851\n",
      "Epoch 00095: val_loss did not improve from 0.35622\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0458 - acc: 0.9851 - val_loss: 0.4486 - val_acc: 0.9271\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_6_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 970us/sample - loss: 0.4131 - acc: 0.9018\n",
      "Loss: 0.41305809186255077 Accuracy: 0.9017653\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.5060 - acc: 0.1717\n",
      "Epoch 00001: val_loss improved from inf to 1.94137, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_7_conv_checkpoint/001-1.9414.hdf5\n",
      "36805/36805 [==============================] - 97s 3ms/sample - loss: 2.5060 - acc: 0.1717 - val_loss: 1.9414 - val_acc: 0.3638\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6701 - acc: 0.4447\n",
      "Epoch 00002: val_loss improved from 1.94137 to 1.24963, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_7_conv_checkpoint/002-1.2496.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 1.6702 - acc: 0.4446 - val_loss: 1.2496 - val_acc: 0.6052\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3199 - acc: 0.5615\n",
      "Epoch 00003: val_loss improved from 1.24963 to 1.05219, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_7_conv_checkpoint/003-1.0522.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 1.3199 - acc: 0.5616 - val_loss: 1.0522 - val_acc: 0.6804\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1089 - acc: 0.6401\n",
      "Epoch 00004: val_loss improved from 1.05219 to 0.89823, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_7_conv_checkpoint/004-0.8982.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 1.1088 - acc: 0.6401 - val_loss: 0.8982 - val_acc: 0.7144\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9484 - acc: 0.6968\n",
      "Epoch 00005: val_loss improved from 0.89823 to 0.76811, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_7_conv_checkpoint/005-0.7681.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.9484 - acc: 0.6968 - val_loss: 0.7681 - val_acc: 0.7619\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8319 - acc: 0.7352\n",
      "Epoch 00006: val_loss improved from 0.76811 to 0.61852, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_7_conv_checkpoint/006-0.6185.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.8319 - acc: 0.7352 - val_loss: 0.6185 - val_acc: 0.8123\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7317 - acc: 0.7667\n",
      "Epoch 00007: val_loss improved from 0.61852 to 0.58055, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_7_conv_checkpoint/007-0.5805.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.7316 - acc: 0.7667 - val_loss: 0.5805 - val_acc: 0.8274\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6345 - acc: 0.7991\n",
      "Epoch 00008: val_loss improved from 0.58055 to 0.49445, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_7_conv_checkpoint/008-0.4944.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.6345 - acc: 0.7991 - val_loss: 0.4944 - val_acc: 0.8558\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5627 - acc: 0.8237\n",
      "Epoch 00009: val_loss improved from 0.49445 to 0.43004, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_7_conv_checkpoint/009-0.4300.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.5627 - acc: 0.8237 - val_loss: 0.4300 - val_acc: 0.8754\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4979 - acc: 0.8442\n",
      "Epoch 00010: val_loss improved from 0.43004 to 0.37113, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_7_conv_checkpoint/010-0.3711.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.4978 - acc: 0.8442 - val_loss: 0.3711 - val_acc: 0.8903\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4500 - acc: 0.8589\n",
      "Epoch 00011: val_loss improved from 0.37113 to 0.33107, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_7_conv_checkpoint/011-0.3311.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.4502 - acc: 0.8589 - val_loss: 0.3311 - val_acc: 0.9050\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4130 - acc: 0.8711\n",
      "Epoch 00012: val_loss improved from 0.33107 to 0.31315, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_7_conv_checkpoint/012-0.3131.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.4130 - acc: 0.8711 - val_loss: 0.3131 - val_acc: 0.9117\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3801 - acc: 0.8809\n",
      "Epoch 00013: val_loss improved from 0.31315 to 0.27041, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_7_conv_checkpoint/013-0.2704.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.3801 - acc: 0.8809 - val_loss: 0.2704 - val_acc: 0.9243\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3501 - acc: 0.8893\n",
      "Epoch 00014: val_loss improved from 0.27041 to 0.25189, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_7_conv_checkpoint/014-0.2519.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.3505 - acc: 0.8893 - val_loss: 0.2519 - val_acc: 0.9269\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3327 - acc: 0.8946\n",
      "Epoch 00015: val_loss did not improve from 0.25189\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.3326 - acc: 0.8946 - val_loss: 0.2736 - val_acc: 0.9259\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3113 - acc: 0.9023\n",
      "Epoch 00016: val_loss did not improve from 0.25189\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.3113 - acc: 0.9023 - val_loss: 0.2635 - val_acc: 0.9280\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2928 - acc: 0.9089\n",
      "Epoch 00017: val_loss did not improve from 0.25189\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2928 - acc: 0.9089 - val_loss: 0.2765 - val_acc: 0.9222\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2751 - acc: 0.9135\n",
      "Epoch 00018: val_loss improved from 0.25189 to 0.22053, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_7_conv_checkpoint/018-0.2205.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2751 - acc: 0.9135 - val_loss: 0.2205 - val_acc: 0.9380\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2626 - acc: 0.9172\n",
      "Epoch 00019: val_loss did not improve from 0.22053\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2625 - acc: 0.9172 - val_loss: 0.2541 - val_acc: 0.9271\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2541 - acc: 0.9202\n",
      "Epoch 00020: val_loss improved from 0.22053 to 0.18887, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_7_conv_checkpoint/020-0.1889.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2541 - acc: 0.9202 - val_loss: 0.1889 - val_acc: 0.9439\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2397 - acc: 0.9249\n",
      "Epoch 00021: val_loss did not improve from 0.18887\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2397 - acc: 0.9249 - val_loss: 0.2191 - val_acc: 0.9387\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2285 - acc: 0.9277\n",
      "Epoch 00022: val_loss did not improve from 0.18887\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2284 - acc: 0.9277 - val_loss: 0.1967 - val_acc: 0.9413\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2199 - acc: 0.9304\n",
      "Epoch 00023: val_loss did not improve from 0.18887\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2200 - acc: 0.9304 - val_loss: 0.1978 - val_acc: 0.9425\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2121 - acc: 0.9333\n",
      "Epoch 00024: val_loss did not improve from 0.18887\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2122 - acc: 0.9333 - val_loss: 0.2142 - val_acc: 0.9436\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2036 - acc: 0.9341\n",
      "Epoch 00025: val_loss did not improve from 0.18887\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2036 - acc: 0.9341 - val_loss: 0.2062 - val_acc: 0.9425\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1938 - acc: 0.9374\n",
      "Epoch 00026: val_loss improved from 0.18887 to 0.18481, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_7_conv_checkpoint/026-0.1848.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1938 - acc: 0.9374 - val_loss: 0.1848 - val_acc: 0.9488\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1868 - acc: 0.9414\n",
      "Epoch 00027: val_loss did not improve from 0.18481\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1867 - acc: 0.9414 - val_loss: 0.2157 - val_acc: 0.9385\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1819 - acc: 0.9420\n",
      "Epoch 00028: val_loss improved from 0.18481 to 0.18194, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_7_conv_checkpoint/028-0.1819.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1819 - acc: 0.9420 - val_loss: 0.1819 - val_acc: 0.9497\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1724 - acc: 0.9438\n",
      "Epoch 00029: val_loss improved from 0.18194 to 0.17784, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_7_conv_checkpoint/029-0.1778.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1723 - acc: 0.9438 - val_loss: 0.1778 - val_acc: 0.9490\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1703 - acc: 0.9465\n",
      "Epoch 00030: val_loss improved from 0.17784 to 0.17586, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_7_conv_checkpoint/030-0.1759.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1703 - acc: 0.9466 - val_loss: 0.1759 - val_acc: 0.9497\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1627 - acc: 0.9452\n",
      "Epoch 00031: val_loss improved from 0.17586 to 0.16324, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_7_conv_checkpoint/031-0.1632.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1626 - acc: 0.9452 - val_loss: 0.1632 - val_acc: 0.9506\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1518 - acc: 0.9517\n",
      "Epoch 00032: val_loss did not improve from 0.16324\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1518 - acc: 0.9517 - val_loss: 0.1710 - val_acc: 0.9483\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1538 - acc: 0.9504\n",
      "Epoch 00033: val_loss did not improve from 0.16324\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1538 - acc: 0.9504 - val_loss: 0.1777 - val_acc: 0.9522\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1506 - acc: 0.9520\n",
      "Epoch 00034: val_loss did not improve from 0.16324\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1506 - acc: 0.9520 - val_loss: 0.1710 - val_acc: 0.9502\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1423 - acc: 0.9531\n",
      "Epoch 00035: val_loss did not improve from 0.16324\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1423 - acc: 0.9531 - val_loss: 0.1826 - val_acc: 0.9488\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1400 - acc: 0.9544\n",
      "Epoch 00036: val_loss did not improve from 0.16324\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1400 - acc: 0.9544 - val_loss: 0.1704 - val_acc: 0.9574\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1320 - acc: 0.9569\n",
      "Epoch 00037: val_loss improved from 0.16324 to 0.15672, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_7_conv_checkpoint/037-0.1567.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1320 - acc: 0.9569 - val_loss: 0.1567 - val_acc: 0.9581\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1310 - acc: 0.9564\n",
      "Epoch 00038: val_loss did not improve from 0.15672\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1310 - acc: 0.9564 - val_loss: 0.1803 - val_acc: 0.9481\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1317 - acc: 0.9570\n",
      "Epoch 00039: val_loss did not improve from 0.15672\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1317 - acc: 0.9570 - val_loss: 0.1757 - val_acc: 0.9532\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1223 - acc: 0.9596\n",
      "Epoch 00040: val_loss improved from 0.15672 to 0.15373, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_7_conv_checkpoint/040-0.1537.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1223 - acc: 0.9596 - val_loss: 0.1537 - val_acc: 0.9574\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1160 - acc: 0.9620\n",
      "Epoch 00041: val_loss did not improve from 0.15373\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1160 - acc: 0.9620 - val_loss: 0.1736 - val_acc: 0.9513\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1148 - acc: 0.9622\n",
      "Epoch 00042: val_loss did not improve from 0.15373\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1148 - acc: 0.9622 - val_loss: 0.1603 - val_acc: 0.9564\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1104 - acc: 0.9629\n",
      "Epoch 00043: val_loss did not improve from 0.15373\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1104 - acc: 0.9629 - val_loss: 0.1615 - val_acc: 0.9576\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1137 - acc: 0.9633\n",
      "Epoch 00044: val_loss did not improve from 0.15373\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1137 - acc: 0.9633 - val_loss: 0.1873 - val_acc: 0.9513\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1066 - acc: 0.9661\n",
      "Epoch 00045: val_loss did not improve from 0.15373\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1067 - acc: 0.9661 - val_loss: 0.1759 - val_acc: 0.9574\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1070 - acc: 0.9643\n",
      "Epoch 00046: val_loss did not improve from 0.15373\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1070 - acc: 0.9643 - val_loss: 0.1738 - val_acc: 0.9529\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1033 - acc: 0.9658\n",
      "Epoch 00047: val_loss did not improve from 0.15373\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1034 - acc: 0.9658 - val_loss: 0.1677 - val_acc: 0.9520\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1022 - acc: 0.9661\n",
      "Epoch 00048: val_loss did not improve from 0.15373\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1022 - acc: 0.9661 - val_loss: 0.1746 - val_acc: 0.9550\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0986 - acc: 0.9665\n",
      "Epoch 00049: val_loss did not improve from 0.15373\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0987 - acc: 0.9665 - val_loss: 0.1683 - val_acc: 0.9553\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1047 - acc: 0.9654\n",
      "Epoch 00050: val_loss did not improve from 0.15373\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1047 - acc: 0.9654 - val_loss: 0.1578 - val_acc: 0.9571\n",
      "Epoch 51/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0930 - acc: 0.9694\n",
      "Epoch 00051: val_loss did not improve from 0.15373\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0930 - acc: 0.9694 - val_loss: 0.1642 - val_acc: 0.9550\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0915 - acc: 0.9691\n",
      "Epoch 00052: val_loss did not improve from 0.15373\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0915 - acc: 0.9691 - val_loss: 0.1667 - val_acc: 0.9583\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0877 - acc: 0.9708\n",
      "Epoch 00053: val_loss did not improve from 0.15373\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0877 - acc: 0.9708 - val_loss: 0.1721 - val_acc: 0.9550\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0907 - acc: 0.9696\n",
      "Epoch 00054: val_loss did not improve from 0.15373\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0907 - acc: 0.9696 - val_loss: 0.1765 - val_acc: 0.9527\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0862 - acc: 0.9708\n",
      "Epoch 00055: val_loss did not improve from 0.15373\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0862 - acc: 0.9707 - val_loss: 0.1905 - val_acc: 0.9555\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0875 - acc: 0.9710\n",
      "Epoch 00056: val_loss did not improve from 0.15373\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0875 - acc: 0.9710 - val_loss: 0.1747 - val_acc: 0.9604\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0797 - acc: 0.9722\n",
      "Epoch 00057: val_loss did not improve from 0.15373\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0797 - acc: 0.9722 - val_loss: 0.1682 - val_acc: 0.9588\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0804 - acc: 0.9732\n",
      "Epoch 00058: val_loss did not improve from 0.15373\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0804 - acc: 0.9732 - val_loss: 0.1588 - val_acc: 0.9562\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0741 - acc: 0.9745\n",
      "Epoch 00059: val_loss did not improve from 0.15373\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0742 - acc: 0.9745 - val_loss: 0.1666 - val_acc: 0.9609\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0774 - acc: 0.9733\n",
      "Epoch 00060: val_loss did not improve from 0.15373\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0774 - acc: 0.9733 - val_loss: 0.1798 - val_acc: 0.9541\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0740 - acc: 0.9744\n",
      "Epoch 00061: val_loss did not improve from 0.15373\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0740 - acc: 0.9744 - val_loss: 0.1761 - val_acc: 0.9571\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0739 - acc: 0.9747\n",
      "Epoch 00062: val_loss did not improve from 0.15373\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0739 - acc: 0.9747 - val_loss: 0.1608 - val_acc: 0.9606\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0716 - acc: 0.9754\n",
      "Epoch 00063: val_loss did not improve from 0.15373\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0716 - acc: 0.9754 - val_loss: 0.1931 - val_acc: 0.9571\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0770 - acc: 0.9737\n",
      "Epoch 00064: val_loss did not improve from 0.15373\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0770 - acc: 0.9738 - val_loss: 0.1752 - val_acc: 0.9595\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0680 - acc: 0.9773\n",
      "Epoch 00065: val_loss did not improve from 0.15373\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0679 - acc: 0.9773 - val_loss: 0.1637 - val_acc: 0.9627\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0595 - acc: 0.9798\n",
      "Epoch 00066: val_loss did not improve from 0.15373\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0595 - acc: 0.9798 - val_loss: 0.1831 - val_acc: 0.9609\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0694 - acc: 0.9769\n",
      "Epoch 00067: val_loss did not improve from 0.15373\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0694 - acc: 0.9769 - val_loss: 0.1550 - val_acc: 0.9599\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0616 - acc: 0.9787\n",
      "Epoch 00068: val_loss did not improve from 0.15373\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0616 - acc: 0.9787 - val_loss: 0.1768 - val_acc: 0.9571\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0679 - acc: 0.9765\n",
      "Epoch 00069: val_loss did not improve from 0.15373\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0679 - acc: 0.9765 - val_loss: 0.1903 - val_acc: 0.9569\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0621 - acc: 0.9783\n",
      "Epoch 00070: val_loss did not improve from 0.15373\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0621 - acc: 0.9783 - val_loss: 0.1782 - val_acc: 0.9602\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0599 - acc: 0.9798\n",
      "Epoch 00071: val_loss did not improve from 0.15373\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0600 - acc: 0.9798 - val_loss: 0.1900 - val_acc: 0.9583\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0642 - acc: 0.9778\n",
      "Epoch 00072: val_loss did not improve from 0.15373\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0642 - acc: 0.9778 - val_loss: 0.1598 - val_acc: 0.9609\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0605 - acc: 0.9792\n",
      "Epoch 00073: val_loss did not improve from 0.15373\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0605 - acc: 0.9792 - val_loss: 0.1845 - val_acc: 0.9581\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0604 - acc: 0.9795\n",
      "Epoch 00074: val_loss did not improve from 0.15373\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0604 - acc: 0.9795 - val_loss: 0.1852 - val_acc: 0.9534\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0568 - acc: 0.9807\n",
      "Epoch 00075: val_loss did not improve from 0.15373\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0568 - acc: 0.9807 - val_loss: 0.1883 - val_acc: 0.9590\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0577 - acc: 0.9806\n",
      "Epoch 00076: val_loss did not improve from 0.15373\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0577 - acc: 0.9806 - val_loss: 0.1781 - val_acc: 0.9592\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0537 - acc: 0.9819\n",
      "Epoch 00077: val_loss did not improve from 0.15373\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0539 - acc: 0.9819 - val_loss: 0.1830 - val_acc: 0.9536\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0670 - acc: 0.9784\n",
      "Epoch 00078: val_loss did not improve from 0.15373\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0670 - acc: 0.9784 - val_loss: 0.1948 - val_acc: 0.9597\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0527 - acc: 0.9821\n",
      "Epoch 00079: val_loss did not improve from 0.15373\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0527 - acc: 0.9821 - val_loss: 0.1881 - val_acc: 0.9585\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0526 - acc: 0.9829\n",
      "Epoch 00080: val_loss did not improve from 0.15373\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0526 - acc: 0.9829 - val_loss: 0.1660 - val_acc: 0.9627\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0550 - acc: 0.9820\n",
      "Epoch 00081: val_loss did not improve from 0.15373\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0550 - acc: 0.9820 - val_loss: 0.1749 - val_acc: 0.9611\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0503 - acc: 0.9834\n",
      "Epoch 00082: val_loss did not improve from 0.15373\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0503 - acc: 0.9834 - val_loss: 0.1996 - val_acc: 0.9616\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0526 - acc: 0.9814\n",
      "Epoch 00083: val_loss did not improve from 0.15373\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0526 - acc: 0.9814 - val_loss: 0.1830 - val_acc: 0.9592\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0516 - acc: 0.9823\n",
      "Epoch 00084: val_loss did not improve from 0.15373\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0516 - acc: 0.9823 - val_loss: 0.1750 - val_acc: 0.9611\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0507 - acc: 0.9821\n",
      "Epoch 00085: val_loss did not improve from 0.15373\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0507 - acc: 0.9821 - val_loss: 0.1897 - val_acc: 0.9611\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0522 - acc: 0.9814\n",
      "Epoch 00086: val_loss did not improve from 0.15373\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0522 - acc: 0.9814 - val_loss: 0.1732 - val_acc: 0.9599\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0477 - acc: 0.9839\n",
      "Epoch 00087: val_loss did not improve from 0.15373\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0477 - acc: 0.9839 - val_loss: 0.1788 - val_acc: 0.9609\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0465 - acc: 0.9843\n",
      "Epoch 00088: val_loss did not improve from 0.15373\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0465 - acc: 0.9843 - val_loss: 0.2147 - val_acc: 0.9606\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0464 - acc: 0.9840\n",
      "Epoch 00089: val_loss did not improve from 0.15373\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0464 - acc: 0.9840 - val_loss: 0.1944 - val_acc: 0.9625\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0477 - acc: 0.9847\n",
      "Epoch 00090: val_loss did not improve from 0.15373\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0477 - acc: 0.9847 - val_loss: 0.1837 - val_acc: 0.9611\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_7_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzs3Xl8VNXd+PHPmT3JTPYQIGEXZQuEHcUFpaJoRRQRrUtrW+1iW3nsz0pttdbW1lr7WNdaam3d0Qd3pcVaQbQVlU3ZZAcJEMieTJJZ7/n9cSYhCQlEyDDAfN+v131l5s6de8+9mTnfOcs9R2mtEUIIIQBsiU6AEEKIY4cEBSGEEM0kKAghhGgmQUEIIUQzCQpCCCGaSVAQQgjRTIKCEEKIZhIUhBBCNJOgIIQQopkj0Qn4snJzc3Xfvn0TnQwhhDiuLF++vFxrnXeo7Y67oNC3b1+WLVuW6GQIIcRxRSm1ozPbSfWREEKIZhIUhBBCNJOgIIQQotlx16bQnnA4TElJCYFAINFJOW55PB4KCwtxOp2JTooQIoFOiKBQUlKCz+ejb9++KKUSnZzjjtaaiooKSkpK6NevX6KTI4RIoLhVHymleimlFiml1iml1iqlbmpnm0lKqRql1KrYcsfhHCsQCJCTkyMB4TAppcjJyZGSlhAiriWFCPBjrfUKpZQPWK6U+pfWel2b7d7XWn/1SA8mAeHIyPUTQkAcSwpa6z1a6xWxx3XAeqAgXsc7lGi0kWBwF5YVTlQShBDimHdUeh8ppfoCI4GP2nn5VKXUp0qpfyilhsYrDZYVIBTag9ZdHxSqq6t59NFHD+u9F1xwAdXV1Z3e/s477+S+++47rGMJIcShxD0oKKW8wEvAbK11bZuXVwB9tNYjgIeAVzvYxw1KqWVKqWVlZWWHmQ47AFpHD+v9B3OwoBCJRA763gULFpCZmdnlaRJCiMMR16CglHJiAsKzWuuX276uta7VWvtjjxcATqVUbjvbzdVaj9Faj8nLO+TQHR1oOlXrMN/fsTlz5rBlyxaKi4u55ZZbWLx4MWeccQbTpk1jyJAhAEyfPp3Ro0czdOhQ5s6d2/zevn37Ul5ezvbt2xk8eDDXX389Q4cOZcqUKTQ2Nh70uKtWrWLChAkMHz6cSy65hKqqKgAefPBBhgwZwvDhw7niiisAeO+99yguLqa4uJiRI0dSV1fX5ddBCHH8i1tDszItl38F1mut/7eDbboDe7XWWik1DpNzVxzJcTdtmo3fv6qdVyyi0XpsthSU+nKn7fUWM3DgHzt8/Z577mHNmjWsWmWOu3jxYlasWMGaNWuau3g+8cQTZGdn09jYyNixY5kxYwY5OTlt0r6J559/nr/85S9cfvnlvPTSS1x99dUdHvfaa6/loYce4qyzzuKOO+7gl7/8JX/84x+555572LZtG263u7lq6r777uORRx5h4sSJ+P1+PB7Pl7oGQojkEM+SwkTgGuCcFl1OL1BKfVcp9d3YNpcBa5RSnwIPAldorXUc0wTEefcx48aNa9Xn/8EHH2TEiBFMmDCBnTt3smnTpgPe069fP4qLiwEYPXo027dv73D/NTU1VFdXc9ZZZwHw9a9/nSVLlgAwfPhwrrrqKp555hkcDhMAJ06cyM0338yDDz5IdXV183ohhGgpbjmD1voD4KD9HLXWDwMPd+VxO/pFb1kR6utX4Xb3wuXK78pDtistLa358eLFi3nnnXf48MMPSU1NZdKkSe3eE+B2u5sf2+32Q1YfdeStt95iyZIlvPHGG9x9992sXr2aOXPmcOGFF7JgwQImTpzIwoULGTRo0GHtXwhx4kqasY/i2dDs8/kOWkdfU1NDVlYWqampfP755yxduvSIj5mRkUFWVhbvv/8+AE8//TRnnXUWlmWxc+dOzj77bH73u99RU1OD3+9ny5YtFBUVceuttzJ27Fg+//zzI06DEOLEkzR1CKaJQ6F11zc05+TkMHHiRIYNG8bUqVO58MILW71+/vnn89hjjzF48GBOOeUUJkyY0CXHffLJJ/nud79LQ0MD/fv3529/+xvRaJSrr76ampoatNb86Ec/IjMzk9tvv51FixZhs9kYOnQoU6dO7ZI0CCFOLCruVfhdbMyYMbrtJDvr169n8ODBh3yv378KhyMLj6dPvJJ3XOvsdRRCHH+UUsu11mMOtV3SVB8Z9rhUHwkhxIkiqYKCUra4VB8JIcSJIsmCgh2QkoIQQnQkqYKCVB8JIcTBJVVQkOojIYQ4uKQKCiDVR0IIcTBJFRSUOnaqj7xe75daL4QQR0OSBQUbYHG83ZshhBBHS1IFBVN9BF09fPacOXN45JFHmp83TYTj9/uZPHkyo0aNoqioiNdee63T+9Rac8sttzBs2DCKiop44YUXANizZw9nnnkmxcXFDBs2jPfff59oNMo3vvGN5m3vv//+Lj0/IUTyOPGGuZg9G1a1N3Q2OHUYuxUAu5dDjNXXWnEx/LHjobNnzZrF7NmzufHGGwF48cUXWbhwIR6Ph1deeYX09HTKy8uZMGEC06ZN69R8yC+//DKrVq3i008/pby8nLFjx3LmmWfy3HPPcd555/Gzn/2MaDRKQ0MDq1atYteuXaxZswbgS83kJoQQLZ14QaEztIYunKh+5MiR7Nu3j927d1NWVkZWVha9evUiHA5z2223sWTJEmw2G7t27WLv3r107979kPv84IMPuPLKK7Hb7eTn53PWWWfxySefMHbsWL75zW8SDoeZPn06xcXF9O/fn61bt/LDH/6QCy+8kClTpnTZuQkhksuJFxQO8os+Gq4mENhMaupg7Pa0Drc7HDNnzmT+/PmUlpYya9YsAJ599lnKyspYvnw5TqeTvn37tjtk9pdx5plnsmTJEt566y2+8Y1vcPPNN3Pttdfy6aefsnDhQh577DFefPFFnnjiia44LSFEkkmqNgXT0Exc7lWYNWsW8+bNY/78+cycORMwQ2Z369YNp9PJokWL2LFjR6f3d8YZZ/DCCy8QjUYpKytjyZIljBs3jh07dpCfn8/111/Pt7/9bVasWEF5eTmWZTFjxgx+/etfs2LFii4/PyFEcjjxSgoHEc85FYYOHUpdXR0FBQX06NEDgKuuuoqLLrqIoqIixowZ86Umtbnkkkv48MMPGTFiBEop7r33Xrp3786TTz7J73//e5xOJ16vl6eeeopdu3Zx3XXXYVkm2P32t7/t8vMTQiSHpBo6OxoN0NCwBo+nH05nziG3TzYydLYQJy4ZOrsd8aw+EkKIE0GSBYX4VR8JIcSJIKmCwv7TlaAghBDtSaqgYG4ak5FShRCiI0kVFODYGhRPCCGONUkXFGT4bCGE6FjSBYV4TLRTXV3No48+eljvveCCC2SsIiHEMSMJg0LXlxQOFhQikchB37tgwQIyMzO7ND1CCHG4ki4omIbmrg0Kc+bMYcuWLRQXF3PLLbewePFizjjjDKZNm8aQIUMAmD59OqNHj2bo0KHMnTu3+b19+/alvLyc7du3M3jwYK6//nqGDh3KlClTaGxsPOBYb7zxBuPHj2fkyJF85StfYe/evQD4/X6uu+46ioqKGD58OC+99BIA//znPxk1ahQjRoxg8uTJXXreQogTzwk3zMVBRs4GwLIK0TqK3d7xNm0dYuRs7rnnHtasWcOq2IEXL17MihUrWLNmDf369QPgiSeeIDs7m8bGRsaOHcuMGTPIyWl9V/WmTZt4/vnn+ctf/sLll1/OSy+9xNVXX91qm9NPP52lS5eilOLxxx/n3nvv5Q9/+AO/+tWvyMjIYPXq1QBUVVVRVlbG9ddfz5IlS+jXrx+VlZWdP2khRFI64YLCoXXdkNkHM27cuOaAAPDggw/yyiuvALBz5042bdp0QFDo168fxcXFAIwePZrt27cfsN+SkhJmzZrFnj17CIVCzcd45513mDdvXvN2WVlZvPHGG5x55pnN22RnZ3fpOQohTjwnXFA42C96gECgnHB4Lz7f6LimIy1t/9Dcixcv5p133uHDDz8kNTWVSZMmtTuEttvtbn5st9vbrT764Q9/yM0338y0adNYvHgxd955Z1zSL4RITknXpmDGP9JdOk+zz+ejrq6uw9dramrIysoiNTWVzz//nKVLlx72sWpqaigoKADgySefbF5/7rnntpoStKqqigkTJrBkyRK2bdsGINVHQohDiltQUEr1UkotUkqtU0qtVUrd1M42Sin1oFJqs1LqM6XUqHilZ/8xu378o5ycHCZOnMiwYcO45ZZbDnj9/PPPJxKJMHjwYObMmcOECRMO+1h33nknM2fOZPTo0eTm5jav//nPf05VVRXDhg1jxIgRLFq0iLy8PObOncull17KiBEjmif/EUKIjsRt6GylVA+gh9Z6hVLKBywHpmut17XY5gLgh8AFwHjgAa31+IPt90iGzgYIhcoIBneQllaEzeY+9BuSiAydLcSJK+FDZ2ut92itV8Qe1wHrgYI2m10MPKWNpUBmLJjEzf6Sgox/JIQQbR2VNgWlVF9gJPBRm5cKgJ0tnpdwYODo4rTI8NlCCNGRuAcFpZQXeAmYrbWuPcx93KCUWqaUWlZWVnaEKWo6ZSkpCCFEW3ENCkopJyYgPKu1frmdTXYBvVo8L4yta0VrPVdrPUZrPSYvL+8I0yQlBSGE6Eg8ex8p4K/Aeq31/3aw2evAtbFeSBOAGq31nnilyaSr6VZmCQpCCNFWPG9emwhcA6xWSjUNPHEb0BtAa/0YsADT82gz0ABcF7fURKMQDoNT5mkWQoiOxC0oaK0/4BBjSmjTH/bGeKWhlZoa2LoVNXRI7NiJLSl4vV78fn9C0yCEEG0lzx3NTSPgRS1MrJLqIyGEaCvpgoKKRunqeZrnzJnTaoiJO++8k/vuuw+/38/kyZMZNWoURUVFvPbaa4fcV0dDbLc3BHZHw2ULIcThOuEGxJv9z9msKm1n7GzLgvp6WJVCVAVRyo7N5unUPou7F/PH8zseaW/WrFnMnj2bG280NWEvvvgiCxcuxOPx8Morr5Cenk55eTkTJkxg2rRpmDb49rU3xLZlWe0Ogd3ecNlCCHEkTrig0KGmjFjrWEtH1w3vMXLkSPbt28fu3bspKysjKyuLXr16EQ6Hue2221iyZAk2m41du3axd+9eunfv3uG+2htiu6ysrN0hsNsbLlsIIY7ECRcUOvxFb1mwYgUUFFCfXo1SdlJTT+6y486cOZP58+dTWlraPPDcs88+S1lZGcuXL8fpdNK3b992h8xu0tkhtoUQIl6Sp01BKbNEoyhl7/LeR7NmzWLevHnMnz+fmTNnAmaY627duuF0Olm0aBE7duw46D46GmK7oyGw2xsuWwghjkRyBQW7PRYUbHT1MBdDhw6lrq6OgoICevQwY/pdddVVLFu2jKKiIp566ikGDRp00H10NMR2R0NgtzdcthBCHIm4DZ0dL0c0dPbq1ZCWRmMPRTRah9c7PE6pPD7J0NlCnLgSPnT2McnhgEgkLtVHQghxIkiuoNCq+ijapVNyCiHEieCECQqdyuBjQQGaBsWToNBEAqQQAk6QoODxeKioqDh0xtZcUpDhs1vSWlNRUYHH07mb+YQQJ64T4j6FwsJCSkpKOOQEPFVVUFdH1B4iHK7A7V6PmfJBeDweCgsLE50MIUSCnRBBwel0Nt/te1B33w0//zllJfNYu+kKRo9eic8nvW2EEKLJCVF91GmZmQA46s1pR6N1iUyNEEIcc5IrKGRkAOCITWMgQUEIIVpLrqDQVFLwmwZmCQpCCNFaUgYFe50JCpGIBAUhhGgpKYOCrS4MSElBCCHaSq6gEGtTsNUGAQkKQgjRVnIFhaaSQm0dNptHgoIQQrSRXEHB6wWbDaqrcTgyCYdl/gEhhGgpuYKCUqa0UFOD292LYPCLRKdICCGOKckVFMC0K1RX4/H0IRA4+ExoQgiRbJIvKGRmQnU1bncfgsEvZHRQIYRoIWmDgsfTB8sKEA7vS3SKhBDimJGcQaGmBo+nD4BUIQkhRAvJFxRatCmABAUhhGgp+YJCizYFkKAghBAtJWdQqKvDafNht6cTDEpQEEKIJskZFABqa/F4+hIIbE9ocoQQ4lgSt6CglHpCKbVPKbWmg9cnKaVqlFKrYssd8UpLK7Hxj+ReBSGEOFA8Swp/B84/xDbva62LY8tdcUzLfk0lBQkKQghxgLgFBa31EqAyXvs/bE1BoaYGt7sP0Wgt4XB1YtMkhBDHiES3KZyqlPpUKfUPpdTQo3LENiUFQBqbhRAiJpFBYQXQR2s9AngIeLWjDZVSNyillimllpWVlR3ZUdu0KYB0SxVCiCYJCwpa61qttT/2eAHgVErldrDtXK31GK31mLy8vCM7cDslBQkKQghhJCwoKKW6K6VU7PG4WFoq4n7g9HTzt6YGp7MbNptHgoIQQsQ44rVjpdTzwCQgVylVAvwCcAJorR8DLgO+p5SKAI3AFfpoDFlqt4PPB9XVKKVwu3tLm4IQQsTELShora88xOsPAw/H6/gHFRvqAoh1S92ekGQIIcSxJtG9jxLjgKAgJQUhhIBkDgo1NQB4PH0Jh8uIRhsSnCghhEi85AwKseGzgRajpcp8zUIIkZxBoU31EcgNbEIIARIU5F4FIYRoIXmDQk0NaI3L1ROwS1AQQgg6GRSUUjcppdKV8Vel1Aql1JR4Jy5uMjLAssDvx2Zz4HYXSlAQQgg6X1L4pta6FpgCZAHXAPfELVXx1mKoCzBVSNKmIIQQnQ8KKvb3AuBprfXaFuuOP+0EBSkpCCFE54PCcqXU25igsFAp5QOs+CUrzlrMqQDmXoVgcBfRaCCBiRJCiMTrbFD4FjAHGKu1bsCMYXRd3FIVby2GzwbwekcAFvX1nyUuTUIIcQzobFA4Fdigta5WSl0N/ByoiV+y4qxN9ZHPNwaAurpliUqREEIcEzobFP4ENCilRgA/BrYAT8UtVfHWpvrI7e6Nw5FDXd3yBCZKCCESr7NBIRIb1vpi4GGt9SOAL37JirM21UdKKXy+MVJSEEIkvc4GhTql1E8xXVHfUkrZiM2NcFxyuSAlpTkogKlCqq9fSzTamMCECSFEYnU2KMwCgpj7FUqBQuD3cUvV0dCjB2zf3vzU5xsNRPH7P01YkoQQItE6FRRigeBZIEMp9VUgoLU+ftsUAMaOhY8/bn4qjc1CCNH5YS4uBz4GZgKXAx8ppS6LZ8Libvx4+OIL2LMHALe7EKczD79fGpuFEMmrs9Nx/gxzj8I+AKVUHvAOMD9eCYu78ePN348+gunTpbFZCCHofJuCrSkgxFR8ifcem0aOBKfTBIUY09i8TmZhE0Ikrc6WFP6plFoIPB97PgtYEJ8kHSUpKTBiRJugMBqw8PtXkZFxWuLSJoQQCdLZhuZbgLnA8NgyV2t9azwTdlSMHw+ffALRKCCNzUII0ekqIK31S1rrm2PLK/FM1FEzfjz4/bBuHQAuV0+czny5s1kIkbQOWn2klKoDdHsvAVprnR6XVB0tLRubi4qksVkIkfQOWlLQWvu01untLL7jPiAADBwIWVkHNDY3NKwnEvEnMGFCCJEYx3cPoiOlFIwb105js8bvX5W4dAkhRIIkd1AAmDAB1qyBujpgf2Nzbe3SRKZKCCESQoLC+PGgNSwz7Qhudw9SUk6huvrdBCdMCCGOPgkK48aZvy2qkLKyJlNd/R6WFUpQooQQIjEkKOTkwEkntQkKX8GyGqit/eggbxRCiBOPBAUwVUhLl5pqJCAzcxJgo6rqnYQmSwghjjYJCgCnnw6lpbBxIwBOZxY+3xgJCkKIpBO3oKCUekIptU8ptaaD15VS6kGl1Gal1GdKqVHxSsshTZli/i5c2LwqK+sr1NZ+RCRSm6BECSHE0RfPksLfgfMP8vpUYGBsuQH4UxzTcnD9+5t2hbffbl6VlfUVIEp19XsJS5YQQhxtcQsKWuslQOVBNrkYeEobS4FMpVSPeKXnkKZMgUWLIBgEID39VGy2FKlCEkIklc4OnR0PBcDOFs9LYuv2tN1QKXUDpjRB796945Oa886DRx+F//wHzjkHu91DRsYZVFX9Oz7HE0IkjNZmsbXzs1hraGiAcBgiETOIst1upl9xucCyIBCAxkazTWoqeL3mL5jtw+HmwZdRyiwOh1lsNvNaXR3U1Ji/TceJRs22Ntv+tEUi+5eCAlOxEU+JDAqdprWeixm6mzFjxrQ3QN+RO/ts8x97+2045xzAVCFt3foTgsE9uN2JK8SI5NOUOTQt4bDJqBoaTObgcoHbbf5GIqaAGwyax5ZllmgUQqH9r4VCrZdAYP9rNpvZn9ttHje9LxTav7+mfTZlUJZlpiVpWhoaoKzMLBUVJsOrqYHaWvN6VpZZ3G6zbX29SUNKCvh8ZlFq/2sNDSbjbWw02zkcJvP1ek0G3ZQxBwImXZZlMvSm6xEImPTD/usYiZj1gYDZNjsbunWD3FxzzL17Yd8+s92XpZT5qw+RQ9lsJq2H49Zb4Z57Du+9nZXIoLAL6NXieWFsXWL4fDBxomlsjl31rKzJAFRV/Zvu3a9OWNLEoWmtqQvVEbEiRKwIWkNeah42m2q1nWWZjKIpwyqrDFFf66SuTlFTYzIirfdnMOGwJhgJE4wEIepGWS4sy2TS1dVmqaiKoFB40+ykpZmMuinjCQT2Z6Ba7/812JSJOZ2xzN0dpbER9pbaKS2FqqpYgu0hSKmAqAsCWaAPVeOrwVMNGV+At9S8L5xqlpAPAhkQTDf7SS2H9BJI3wUNOVA+GAKZrXdnD0FqGaSVmb9KQ30eNOSZbZ0N4K41S0olpJaTmltBaoaf1PRUvLlpeF1pRIIedtY6+Xyvi0gghTRbFj5nJmn2TMoqHWzboairs6Ed9bgzK3BlluNIqyXFm4LX6SXPkUrIClIbrKEiUkPYCpHiySQtNYtMRxapVj4O5Wr+Re7xgPLUEvLsxFIRLG2htcZrzybXVUiqx2R9FRVQUl7FroatpPSMMHacm7xsN970MA32PdSrPdSzDywHdisVm5WKHQcOVxSHM4rdDirsRQfSsRrT0VhYjnosux/LFox9Nps+fE7zGYq6sdtseNLCuFNCuFLCOOxgsynsNhsRHaIxWk9jpJ6wFcTtcONxekh1pHDawGHAiCP6rhxKIoPC68APlFLzgPFAjdb6gKqjo+q88+C228zPhfx8vN5iHI5sqqrekaDQRigaYlvVNjZVbiLTk8nYnmNxO9zNr2+v3s5729/DH/LjtDtx2ByAor4xQl19BH9DGIsw2h5CqxChoIP6ynRq96VTVe4h7Cwn5Col4ChFW6AiXlTIByEvOuyBcArRsIMa5+dUpS7D71tO1FnTOpG1PWHn6dhKTodgOpZvB2TsgIyd4NsN3j2QWgmhNKg8ySyBTEjfCZk7TIbpqt+/P22Hmr5QcTK2+gIcWSXoXpsID9qOQuEIdMdWX4BqyMfhcWNPceJQTrTTT8RVQcRZieXwA5bJXJVF1NZA1FaPZQ8AYLdSceMjQ7kJqCqC1DUfXmEjTWWTZs/GTTpunY5De4moBgJU0UgVddZegrqeQ3EoBxF94M/hXE8+2e586sJV1IQqaYgcel9tNcSWo0mhyPfmU5heCMC2qm1UNFa0u63D5qB3Rm+yPFls826jsls7TZ/BQxww3M46e+yvBg5W0mja7jAu1Jy8OXw1zkFB6UOVdQ53x0o9D0wCcoG9wC8AJ4DW+jGllAIexvRQagCu01ofciKDMWPG6GXL4jTfwfLlMGYMPP00XG2CwNq1V1BdvYhTT92FzXbs17Z9Xv45CzcvZHDeYCb2mkiaKw1LW7yz9R0eX/E47257l25p3eiT2Yfe6b1RSlETrKE6UE1dsI5gNEgwEtz/NxIkEAliaY3L5sZpc4NW7G0swSLafFy7dpPpH4+9rh81me8TTN165Cdj2aC+G6DAXQeuA4czV5YTr38EmY1jyLQGkOp2kZbiwOEOsVt9whe8T63a33TlpTsZqhe5zgJyPT3IT8sn7KiiNLSZXY2bqAvXUODrRS9fHwp9vUn3+Ehxukl1uakJVbO5chMbKzZSUltCr4xenJR9EidlnYRGs7tuN7vqdlFWX0YoGiIUDRG2wnhdXnJScshOycbr8mJTNpRS2JSNVEcqaa400pxpANSF6qgL1hGIBsj2ZJOTmkNOSg6haIiKxgrKG8qpaKygLlhHbbCWulAdqc5UsjxZZKVkkZeaR++M3vRK70UPXw8iVoTGcCP14Xr8IT/VgWpqAjUEIgF6+npSmF5ID18PyhvKWV+2nvXl6ylvKCc7Jbt5yUvNIy8tj7zUPJRSlNWXUdZQRnWgGq/LS7o7HZ/LR3aKSW9uai5pzjQaI43Uh+qpD9c3X49QNER9qJ7qQDVVgSpqAjVEdRRLW1jaItWZSm5qLrmpufhcPgKRAP6Qn/pwPW67m0xPJhmeDFx2l9lHYxWVjZXsrttNSW0JO2vN/7pfZj/6Z/WnV0Yv3HY3SikUivKGcrZWbWVbtQka/TL7MTB7IP2z+uN2uJs/+3Zlp4evBz28PeiW1g1LWzSEG6gP12NpC5uyYVcmd/eH/NQGa6kJ1mBX9ub/p8fhQcXqlLTWhK1w8/4tbeGyu3DZXbEfTGabpvVN+3A73ISiIRrDjQQiAdLd6eR78w/r66SUWq61HnPI7eIVFOIlrkHBsqB7d1NiePppAMrLX2PNmukUFS0gJ2dqfI4bs7lyM0t2LKGioYKaYA01gRosbeG0O3HZXSgU/pAff9hPfaievNQ8BmQPYEDWAEr9pTz56ZN8tGv/0BxOm5PxheMpqS1he/V2clJyOLfPReyrqeGLmi8obfwCrRXOaAaOSAaEfFghD5Ggm0jATbjRTTjghogbUGAPgiMIyoLqPlBxMlQOBO9eUgYtgd7vE/FuJ7PuNPL8k+lWfw4Zjm6kpIVxp0RITbPIznCSleEg3WdHWS6iIRehRiepvgiF/evI712LM6WxOWNA27HZTH2tpa3mL0djpJFgJEhhemGrEkp7dtbsJBAJ0CujFx6HJ67/QyGOVZ0NCsf+T9+jyWaDc881jc2WBTYb2dlTcThy2Lv3qS4NChErwraqbawvX8+SHUsIe7JEAAAgAElEQVR4c+ObbKjYsD8pykaGOwO7zd78C8vSFl6XF5/LR6ozlVJ/KVWBqub39PEM41LvffSovpQtVZvYHH2XlZWLsBoGkvbpb6lYfgnzIgdmoKmpkJYFmZn7GwMzMyGrh3mcnb2/ca+pB0ZOjmmca/rrcFx8hFfEDrgxBcv22ZTN/IJypX2pPffK6HXojYQQgASFA513Hjz3HHz2GRQXY7O56NbtCkpL/0okUovDcfgTzm0o38C8NfN4dcOrrCtbRyhquka47C4m9Z3EjWNvZMqAKfT09cTr8jYXPZvU1ppkNS3r18O6bVWUR7ZC1MWOfcPYgcJmg27d+tGt2xROy49l3mMh+zyTiRcWmqWgAPLyTCYvhBAgQeFATUNevPUWFBcD0L37Neze/QhlZfPp0eObX2p3O2t2Mm/NPJ5f8zwrS1eiUJzR5wxmj5/N4LzBDModxLBuw/C6vIDpqbBjB/xrBaxYAZs2wfbtZtm3b/9+MzNh6FCYdm4WgwaNZsAA6NXLZPT5+aZftRBCfFnSptCe004zfQlXrABMA9DHHw/C7e5JcfGiQ769MdzIM589w9OfPc37X7wPwLiCcVw57EpmDplJQXpB87aBAHzyCXzwgblvbulS000OTMbevz/06QN9+5rHw4ebpbBwf79oIYQ4FGlTOBKXXQY//jFs2QIDBqCUIj//GrZvv51AYAceT59231bZWMmjnzzKgx89SFlDGYNyB/Grs3/FlcOuZED2gObttm+HBQvM8u675gYcgEGD4OKLYexYGDUKiorMjT1CCHG0SFBoz4wZJii89BL85CcA5Odfzfbtt7N377P06XNbq80jVoQ//PcP3LXkLhrCDVww8AJ+ctpPOLPPmc3tAiUl8OKLMG+eKRkADBgA3/qWadueONHU9wshRCJJ9VFHxo41vZFazMi2cuVZhEJ7GTdufXNmv65sHde9dh0f7/qYSwZdwl1n38WwbsOa37N0Kdx9N7z5pnk+ejTMmgXTp8PAgfE/DSGEAKk+OnKXXQZz5phW3z6muqh792vZsOHb1NZ+RFk0l7+v+jv3/fc+vC4v82bM4/KhlzcHiyVL4K674N//Nl06b7/d3A938smJPCkhhDg4mXmtIzNmmL8vv9y8KivnUl7f4+bMpy5i4EMD+c37v2HaKdNY+/21zBo2C6UUW7fCpZfCWWfB2rVw330mrtx1lwQEIcSxT4JCR046CUaMgPnzm1f9v3du5/6NQeqDFfzm7DvYMXsHL858kXxvPvX1ZtikwYPNvW933w1bt5qmCa83gechhBBfglQfHcxll5l6n127eKjkZR755BF+NOY6Lkn7G/36uJrvlF2/HmbONCWDa66B3/7W3C8ghBDHGykpHEysCmnBC79m9sLZTDtlGv879S9kZU1h164/YVlhnn/etEnv3WtG3X7qKQkIQojjlwSFgxk8mNWn9ueKqr8wPH84z176LHabnYKCH+D3l/PNb37B174GI0fCqlX7b4YWQojjlVQfHURJbQkXTCnD64/yxgXPNA9FUVZ2ATfeuJLNmwdwyy2m/cDpTHBihRCiC0hJoQNVjVVMfXYqNY4oC56Fwo/WA2ZE7bFj7VRW9uGee6Zyxx2fSUAQQpwwJCi0IxAJMP2F6Wwo38Ars16huDEDveAf3HorXHutuQFt+fIgp576Hrt2PZTo5AohRJeRoNBGKBriqpevYsmOJTx1yVNMHjiF8OTz+ca887n3Xvje98wNaX37ZpGffw179z5DKFSe6GQLIUSXkKDQwp66PZz95Nm8vP5l7j/vfq4YdgV+P0zbeB9PNc7kVzfu4ZFHzMTgAIWFN2FZAfbsmZvYhAshRBeRoBDz4c4PGT13NKtKV/HCZS8we8JsolHTK/XtdQX8hW/z815PtRquOi1tCFlZ57Fr18NYVihxiRdCiC4iQQF4a+NbnPX3s0hxpvDhtz7k8qGXA+YmtLffhkcfVXx7xDL4xz8OeG9h4WxCoT2Ulf3f0U62EEJ0uaQPCo3hRr6/4PsMyh3EJ9d/wvD84QC89x784hfwta/BDTcAU6eaWXBqa1u9Pzt7Cqmpgygp+SPH24izQgjRVtIHhfuX3s8XNV/wwPkPkJ2SDZhpL6+80gx/9NhjsRnOzj8fIhF4551W71fKRkHBTdTVLaOm5j8JOAMhhOg6SR0USv2l/PaD33LxKRdzdr+zAbAsM35RVRX83/+Bzxfb+LTTID293Sqk7t2vweHIoqTkj0cx9UII0fWSOij8/N2fE4wE+f25v29eN3++aUf4wx/MXMjNnE74ylfgn/+ENtVEdnsaPXt+h/LyV6ivX3eUUi+EEF0vaYPCqtJVPLHyCX4w7gcMzDFToIVCZvjroiL4znfaedPUqWZezTZVSACFhTfjcKSzefNN0rYghDhuJW1Q+Mm/fkJWSha3n3l787q5c2HLFrjnHrDb23nTjBlmDs1p0+DVV1u95HLl0bfvXVRVvUN5+cvtvFkIIY59SRkUApEA7257l+tHXU9WShZgOhXddRdMmmQKBO3KyoL//tdMvjNjhmmFbqFnz++RllbE5s03E402xPckhBAiDpIyKKwvW09URxnVY1Tzuvvug7IyuPdeWt2gdoDcXDPOxQUXmDEv/vd/m1+y2RwMHPgwweAXfPHFPXE8AyGEiI+kDAqr960GoKhbEQClpaZheeZMM2HOIaWlwSuvwFe/CnfcAeX7xz7KzDyTbt2u5Isv7qWxcWs8ki+EEHGTnEFh72rcdndzA/Mf/wjBoJkXodMcDtP40NAA99/f6qUBA36PzeZk48bvSaOzEOK4kpxBYd9qBucNxmFzEI3CM8+YdoSBA7/kjoYONfM4P/QQVFY2r3a7C+jf/x6qqt6mtPTvXZp2IYSIp7gGBaXU+UqpDUqpzUqpOe28/g2lVJlSalVs+XY809Nk9b7VzVVHixbBrl3mhrXDcvvtUFdnihst9Oz5PTIyzmDz5v8hGNx9hCkWQoijI25BQSllBx4BpgJDgCuVUkPa2fQFrXVxbHk8XulpUtlYye663c1B4emnISMDLrroMHdYVASXXgoPPADV1c2rlbJxyil/ReugVCMJIY4b8SwpjAM2a623aq1DwDzg4jger1NW7401MucXUV8PL71kGphTUo5gp7ffbvq0PvBAq9WpqQPp1+/XVFS8zr59LxzBAYQQ4uiIZ1AoAHa2eF4SW9fWDKXUZ0qp+UqpXu3tSCl1g1JqmVJqWVlZ2REl6rO9nwEwPH84r74K9fVHUHXUpLgYLr7YVCGVt56FrbBwNj7fODZt+j719Z8f4YGEECK+Et3Q/AbQV2s9HPgX8GR7G2mt52qtx2itx+Tl5R3RAVfvW012SjY9vD146ino0wdOP/2Idmn8+tcmwtx0U6vVStkZMuR5lHLy2WfnS/uCEOKYFs+gsAto+cu/MLaumda6QmsdjD19HBgdx/QA+xuZS0sV77xjSgm2rrgKw4bBz34Gzz0Hr7/e6qWUlP4MH76ASKSCzz67gEikpgsOKIQQXS+eQeETYKBSqp9SygVcAbTKLZVSPVo8nQasj2N6sLTFmn1rKOpWxHPP7R8mu8v89KdmaNXvfKdVF1UAn280Q4e+REPDWtasuRTLCnawEyGESJy4BQWtdQT4AbAQk9m/qLVeq5S6Syk1LbbZj5RSa5VSnwI/Ar4Rr/QA7KjegT/kpyi/iGefhXHj4OSTu/AALhf8/e9mvIz/+Z8DXs7OnsIppzxBdfW7UmIQQhyT4tqmoLVeoLU+WWs9QGt9d2zdHVrr12OPf6q1Hqq1HqG1PltrHdeW2KbhLU7OLOLTT+G88+JwkJEjTYnhqafgzTcPeLl792sYNOhJamqWsHLl6QQCO9vZiRBCJEaiG5qPqqbuqCm1w7AsGDw4Tgf6+c/N/Qvf+hbs3XvAy927X8vw4f8kEPiCFSsmUFe3Kk4JEUKILye5gsK+1fTL7EfJVjPH5qBBcTqQ2w3z5pl7F77+ddN40UZW1mRGjvwApWysXHkapaXPxCkxQgjReUkXFIryi/g8VknVpe0JbQ0ZYgbKW7jwgCEwmni9RYwa9Qk+3zg+//waNm78vjRACyESKmmCQjASZEP5Boq6maDQu7cZATuuvvMdmD4d5syBFSva3cTt7s6IEe/Qq9dP2L37T6xceQaNjVvinDAhhGhf0gSF9eVmYp2moBC3qqOWlILHH4du3eDyy838zu2w2RwMGPA7hg59mYaGjXzyyQh2735cxksSQhx1SRMUmhqZhx3NoACQkwPz55tuqqefDps3d7hpXt4ljB27mvT08WzceD1r1lwsd0ALIY6qpAkKlw+9nE+/+yne4Mn4/UcxKABMmGDG6Pb74YwzYM2aDjf1eHoxYsS/GDDgfior32bp0j6sW3cl1dUfSMlBCBF3SRMU3A43w/OHs3mjAzjKQQFg1ChYssSMqXHmmfDnP5sg0Q6lbPTqNZtx49ZSUPADKir+wapVZ7B8+SjKyl5C6wN7MwkhRFdImqDQpKnn0VEPCmB6JH3wAQwYAN/9LhQWwuzZsH17u5unpAzgpJPu57TTdnHyyX8mGm1g7drLWLZsJGVlL0twEEJ0uaQMCj4fdO+eoAT06wcffwz/+Q9ceCE8+iiMHg2rOr6BzW5Po2fPGxg7di2DBj2NZTWydu0MPv54CLt2PUok0n6JQwghvqykDAqDBpmOQQmjFJx2Gjz7LKxfD14vTJ580MAAppdS9+5XM3bsOgYPfhaHw8emTTfy4YeFbN78YxoaOm7EFkKIzkjaoHDMGDDANEKnpZnA8Omn7W+nNTQ2AiY45Od/jVGjPmZk8Qf0qJxA1QcP8PHHA/nss6mUl7+OZYWO4kkIIU4UjkQn4Gjy+82tAsdUUADo398EhkmT4Jxz4LbbzLhJmZkmGPzrX3DHHfDRRzB0qOnBNH486rPPyHj9dTK2bEF7POx++QZ2+F9jzZqLsdt9ZGVNISfnq2RlTcbtLkQltHgkhDgeqOOtm+OYMWP0smXLDuu9K1aY6vuXXoJLL+3ihHWFLVvgm980vZTS0sxkD2vWmMbpXr3giivgs8/gv/+FujozVPfkyXDBBXD33ZCejvXxh1SG/0tFxRtUVLyJ973dRNOgYXQeXu8o0tPHkZs7Ha93pAQJIZKIUmq51nrMobZLqpJCQnsedcaAAfDee7ByJTzwADzxBOTmwiOPmJKD2222i0ZhwwYTKHxmcD+KiuCcc7B95/vkPv88uRnnoR92oR5+GMvrYesbZ1MV2sCOHb9hx45fkZJyEnl5s8jNnYbPNxql7Ik7byHEMSOpSgp33AG/+Y2ZSrkpfz2m+f2mNOBydW773/7WVD398pemyumDD+D6602D9tlnwxtvEI5UUlb2Mvv2vUB19SLAwuHIJDNzEpmZZ+PzjcHrLcZuT43rqQkhji4pKbTj889N9f1xERDA9Er6Mm691QSCX/wCUlPh+edNldOQIWYmuOeew3nVVfTseT09Uy4j+uf/R0P3EHumO6j0L6a8/NXYjmykpg7G6y3G6x1B+p5s0upycI4+C7KyOj5+aSm8/76pm7NLyUOIL231avPdGTIkcWnQWh9Xy+jRo/XhKirS+qKLDvvtx4fycq1/+EOtP/10/7pIROsJE7TOydF6716tFy3SurBQa6W0Bq0LCrSeO1cH/Nt1WdlreuvWO/TqD8/Tm27L0tVDMdvElmDPVO2fOlTXvnqfjoTr9h/jrbe0zssz202dqnVl5VE/dSGOKsvSurq66/b3xBNaO53mO3TRRVp/+GHX7VtrDSzTnchjk6b6KBo1bbc/+hHce28cEnasW7fOTBXarx9s3AgnnQTPPWeqqH76U1i61LRPKAXBoFkAffJJNH7tHOr7a6yVH2NfsxXf8jrclVA9HPbdMJDcTzxkP7mayJB+cPlM7Hffj+rdG155xbR1nIjKykx7z7HSWB+JmA4KgQB85Sudr3I8XBs2wNNPw5QppjfckV6HYBAWLzbDwJxxBng8+1+rqYEPP4Thw6Fnz87tq7ISKiqgoQGGDTMl57a0Nt+LhQvNsdPSzHSMgwaZCdz79u34GGvXmtL3v/4F48fDt78Ns2btb+NrqbHRlOCHDm0//ZZlqn1/9zvTceSMM+DBB805nHGGucl10iTTS8Zx+JU7na0+SpqgsHWracf9619NB5+k9JvfwM9+ZtoZ7r9//4QSWsMbb8Dbb4PTaTKUlBQ491xzk12bL3y4bg+hR3+F+/6ncew1d1OXXAJbvwuWCzLW2Rn6C7D7NQ1n98dWeBKu3iNw9BiI8vlMtVhGhglS7X1ZwXyRNm40mU9amvmiNn1Jly83af3Pf8w/depU02bS3r4CAfjLX+CLL8ywIoWFMHCgyWDao/XBM7ht28wvizffhPx8kwGfe65Jn89nlqys9qv+/H6TSeXlHVkmqjVUVcHu3SY9b7xhAnB5uXk9Nxe+9jXTe23ECPM/7YzycnNeGzaYNHbrBj16wKmntr62L7xgMsGmsbuKiuAHPzBVlenp+7eLROD//g+eecZkdjfd1LpaMRyGV1813QEXLDA96sB89s4+2+z3gw/MD5Zo1Hxm/vxnk/mCyUz/9jfT866szByvaWnJ4YAxY0wG6/XCzp3m87B2LezaZbY5+WQIhWDHDnN9bTa49lpTFdsyOOzda4736KPmf/2Nb5jP4rp15nM6aRIUF5vF6TTn/9pr5lrZbKan4Le+Za7pjh0mY3r+eXj9dTP/ykMPmff5/TB3rsmw1q0zx/Z6zVS/t97auf9nGxIU2liwwATcDz6AiRPjkLDjgdbmy9CnT9fsLxCAJ59E9+5N6JyRNDSspaFhA8HgTsI7N9Lt1+/h/rwSV4XG0Xjg2y23neDEgUTPn4y792ica7eZLrerV5vMru1n0+02S22teT5kiBk3qqHBrD/nHNOecfHFZsjyF14wpaAdO0ygC7W4oW/yZPjVr8yXMxo1GdPvfmfuMJ8yxUyO9NWvmgw+GDTHeOwxkyHY7SYT3LkT3nkH9u078OS6dTPBp18/k2GtX2+uPZjM5OST4ZRTzPEnTTLnYrOZa7plizn/qiqorja/lEtKzLrt281+AoH9x/J64aKL4LLLzHV48kmTEYVCJkPs398cb8QIM2LvhAkmcOzda673ypXmC/L++yajtdlaTyHr9cKMGSbQvPaayRBPO81kyO+/Dw8/bO7Gt9vNr+ZzzzXn+NBD5trn5pqAM2GCec/AgaaU+stfmnPNyzP/s+nTzbH/8Q/45z/NMPOjR8N555n33n23CRDXXGOWn/7U/EA49VTzusNhMtSUFPP/z8kxzz/+2JSiPvnEBKL8fNNzb8AA8zk47zwz6xaY//PGjaYU9Mgj5jpcc41Z//HHJhO32UwGftdd5ty0Nun6299MiWb9evOZAvP5mTEDpk0zr/3977BnT+vPisNhqi9mz27/x8LevSb9ixebz/iMGQdu0wkSFNr45BP405/gvvsgOzsOCRPtsqwI9fVrqNvzLsFda9D+GvDXosrK8XywmcwP/KTEviPaBuG+WVhDTyZyck9CA3II9U/H1qhxb6vHta0KRz3YJ1+M7dwpJjMJBMwX5h//MBnWtm3mS9url8mQRoyAP/zBfJkqK03m+u9/wz33mMx6yhSTMW3ZYjLOSZPMvnbubP+ELrvMlLIKC5tO0Pzi3LnT/NKtqzMZ4JYtsGmTyURyc02mP3iwySw3bTLL2rX7J17KyTG/srdvPzAYgtlH374myPTpAwUFpiqioMBknC2rW8AElAULzK/MDRvM0jKzysw0AadJUZHJlKdPN79ya2tNZrRtm/m1O3/+/mD84x+bnm5NJZCmTPHNN011yrJlZt3pp8Mtt5jgOm8e/PCHputfr14mwy8uNoHhwgvb75gQDLbuFRKJwK9/bYK5ZZlz//3vTQmlMyWvQMBs19meJiUl5nh//aspMY0bZ5YLLzRVQQc7ztq1JpiffnrrqrxIxAS8rVvN/7N/f/M/jfs0kBIUxHEiHKqgYeVbNJR9xL7cddSElmJZgYO+x2bzkJY2HK93JG53D+z2dByOdFzO7vi2OnG+vgT10Udw9dVmaS/D8fvNL9n77zdfzjlzzK9Vu91kaCtXmmqBcNhkIi6Xqe4666yuvQDbt5tfgO+9ZzKTU04xwWnAABMoMjNNtUlnq4AOpr7e/LL+8EOT2Q8aZILm8OHmWAfT2GiCZW6uGfr9YCorTcA95ZTW60tL4eabzbH/3/+DSy4xAfzL+ugjcwPnDTcclcyUaPSE6E0nQUEclywrSEPDBmw2D3a7D7s9DcsKEA5XEA6XEwx+QV3dCvz+Ffj9q4hEqg/Yh9OZS1paEXZ7OjabC6XMLzWtI2gdwWZzxu7unoDPN1ruyRBJQe5TEMclm82N19u2ETgdl6tb87P8/KuaH1tWhGjUTzRaSyDwBfX1n1JXt5KGhnWEw5VoHcKygoBCKQdKObCsevbtmweAUg5SUk4iJeUUUlNPxu3ujc3mwWZzY7N5cLm64XL1xO0uQClnc3CyrHo8nv64XHlH4aoIcfRIUBDHNZvNgc2WidOZicfTm8zM0zv1vlCojNrapdTWLqWhYT0NDRuprPwHWn+50WWdzlxSU4fgdGbHpku1UMqO05mL09kNpzMPuz2tOdDY7V6czjyczjwcjgyCwV00Nm6msXEzDkc6WVlTSEnp++UvhBBdRIKCSEouVx65uReRm3tR8zqto4TDFVhWMLY0EgrtJRTaTTC4C63Dscw+D5sthcbGzTQ0rKW+fh2NjVsAG0rZ0DpMbe1SQqEyIPql05aSMpDMzLNwOLJiwSQFpzMbpzMfl6s7druXSKSScLiSSKQapzMLl6sAt7sAhyMdywpgWQG0jsZKOMd/fbg4eiQoCBGjlL1VNZVx+DffaW0RiVQRjTZgWUG0DhKJ1BEOlxEOlxGJVONy9YxVXw0gFNpLVdXbVFa+TXn5q0Sj/kM2uh+KzZZKWtowvN7hOJ15ze0qStnxeAaQmnoyKSknxdLaVDUWag5+TmcOSjlb7M+Nzbb/uWVFCAZLCAS243TmkJY2RILQcU6CghBxopQNpzMHp/MQPXtinM4s0tIGUVj4o+Z1WusWDe17CYX2Eo36cTiycTpzcDgyiESqCAZ3EQyWEI36sdlSsNk8gKahYT1+/6eUlb1MNFrbol0ljNbBwzov0wnANOIHg3toWRqy2dLw+caQnj4On280Xu9oUlIGEA7vo6bmP9TUfEAkUk1aWhFe73BSUk4mFCqlsXETjY2bAPB4BpCS0h+Ppy8ORwY2WypKKbSOEgrtIxTaTTRaj9PZDZerGw5H1hENA9/YuIXKyn+RkjKAzMyzsdmSO1tM7rMX4hinlMJuT8FuL8TjKexwO59v9Jfar9ZWrD1jE42Nm1HKEQtguSjlIhwuj5VoKtB6f6ZvWQGi0Tqi0VosK4DLVYDH0xePpw+hUCl1dR9TW/sxJSUPNLfP2GypWFZD7LEJKKWlf2vvbJtS12a9DbvdSzTqByzaMoGuqfSisNlScLnyYm062WhtoXUYrcPYbGm4XPm4XN2wrBAVFW/S0LCueV8ORw55eZeSmTkJpRyxNGmi0ToikVoikRpsNhdudyFudy+czjwsq5FotJZIpC52jqbHm/nrbP7bVCVpWY0o5WhR5dfO0BgJJF1ShRBdzrJC5qbFuuXU16/G7e5NRsbp+HyjsNlchEL78Ps/o7FxIy5XT1JTT8bj6Q9AILCdQGALgcBOotHaWIZch8Phi/UE64nNlko4XEYotJdwuAytIzQFk2i0PvZaGZFIJWDDZnOilJNo1B97TzmgyMw8k9zci8nOnkpDw3r27XuRiorXYwHo6LDbvdhsqbGqOXcsGEFTQDLBJIBlBSks/BF9+/7isI5zTNynoJQ6H3gAsAOPa63vafO6G3gKGA1UALO01tsPtk8JCkKII6V1FMsKYbenHPBaNNpIILCNliUWu92L3Z6Bw+HDsoKx6rqdhMNl2GxpOBzp2O0+QMW6QYfQOhirpgujdQil3NjtKbGSUzDWgaGEUKg0VoIIxNqeoq2O3dRzTSk32dlTyM2ddljnnPD7FJRpbXoEOBcoAT5RSr2utV7XYrNvAVVa65OUUlcAvwNmxStNQggBplNBewEBwG5PIS2t4/kM7PZUUlMHkpo6MF7JS6jDuMe808YBm7XWW7WpXJwHXNxmm4uBJ2OP5wOTlUwcLIQQCRPPoFAAtBxVrCS2rt1ttKkUrAE611VDCCFEl4tnUOgySqkblFLLlFLLysrKEp0cIYQ4YcUzKOwCerV4Xhhb1+42yjS5Z2AanFvRWs/VWo/RWo/Jy5OxZoQQIl7iGRQ+AQYqpfopM0zlFcDrbbZ5Hfh67PFlwLv6eOsjK4QQJ5C49T7SWkeUUj8AFmK6pD6htV6rlLoLM4H068BfgaeVUpuBSkzgEEIIkSBxvaNZa70AWNBm3R0tHgeAmfFMgxBCiM47LhqahRBCHB3H3TAXSqkyYMdhvj0XKO/C5JwI5Jq0JtfjQHJNWjter0cfrfUhe+ocd0HhSCillnXmNu9kItekNbkeB5Jr0tqJfj2k+kgIIUQzCQpCCCGaJVtQmJvoBByD5Jq0JtfjQHJNWjuhr0dStSkIIYQ4uGQrKQghhDiIpAkKSqnzlVIblFKblVJzEp2eo00p1UsptUgptU4ptVYpdVNsfbZS6l9KqU2xv1mJTuvRpJSyK6VWKqXejD3vp5T6KPY5eSE2REvSUEplKqXmK6U+V0qtV0qdmsyfEaXU/8S+L2uUUs8rpTwn+mckKYJCiwl/pgJDgCuVUh3PonFiigA/1loPASYAN8auwRzg31rrgcC/Y8+TyU3A+hbPfwfcr7U+CajCTASVTB4A/qm1HgSMwFybpPyMKKUKgB8BY7TWwzDD9TRNBnbCfkaSIijQuW/0S3UAAAP/SURBVAl/Tmha6z1a6xWxx3WYL3sBrSc6ehKYnpgUHn1KqULgQuDx2HMFnIOZ8AmS73pkAGdixiRDax3SWleTxJ8RzFBAKbFRnFOBPZzgn5FkCQqdmfAnaSil+gIjgY+AfK31nthLpUB+gpKVCH8EfgJYsec5QHVswidIvs9JP6AM+FusSu1xpVQaSfoZ0Vrv4v+3dz8hVpVhHMe/v9BCm0CDAivKTIgIbCqIyALJFhEiLfoDakTQzo0LQZRCCtqJraJctDCaRf9G2ooWgy7KMq1AdyY1i5qgUAwS0V+L972n2xjMMOA9l3t+n90958zhPZf3zHPOc+55HtgD/EwJBueA44z4HOlKUIhK0hjwGbDN9vn+dbVseSd+jiZpAzBj+3jbYxkii4CHgHdtPwj8xaxUUcfmyHLKXdLdwG3AjcDTrQ5qALoSFObT8GfkSVpMCQgTtifr4t8krajrVwAzbY1vwNYCGyWdpaQTn6Tk05fVVAF0b55MA9O2v66fP6UEia7OkaeAn2z/bvsSMEmZNyM9R7oSFObT8Gek1Xz5+8Bp23v7VvU3OnoZ+HzQY2uD7Z2277C9kjIfvrC9GfiS0vAJOvR9ANj+FfhF0r110XrgFB2dI5S00aOSltbzp/d9jPQc6czLa5KeoeSQew1/3mp5SAMl6XHgCPAj/+bQd1GeK3wM3EmpPvuC7T9aGWRLJK0DttveIGkV5c7hZuAEsMX2xTbHN0iSxikP3q8HzgCvUC4eOzlHJL0BvEj59d4J4FXKM4SRnSOdCQoRETG3rqSPIiJiHhIUIiKikaAQERGNBIWIiGgkKERERCNBIWKAJK3rVWSNGEYJChER0UhQiPgfkrZIOibppKR9te/CBUlv1/r6hyXdUrcdl/SVpB8kHej1G5C0WtIhSd9L+k7SPXX3Y309Cybq27IRQyFBIWIWSfdR3mJda3scuAxsphRE+9b2/cAUsLv+yQfADttrKG+M95ZPAO/YfgB4jFJpE0qF2m2U3h6rKPV0IobCork3ieic9cDDwDf1In4JpQjcFeCjus2HwGTtQbDM9lRdvh/4RNJNwO22DwDY/hug7u+Y7en6+SSwEjh67Q8rYm4JChFXE7Df9s7/LJRen7XdQmvE9NfJuUzOwxgiSR9FXO0w8JykW6HpY30X5XzpVcfcBBy1fQ74U9ITdflLwFTtbjct6dm6jxskLR3oUUQsQK5QImaxfUrSa8BBSdcBl4CtlKYzj9R1M5TnDlDKJ79X/+n3KotCCRD7JL1Z9/H8AA8jYkFSJTViniRdsD3W9jgirqWkjyIiopE7hYiIaOROISIiGgkKERHRSFCIiIhGgkJERDQSFCIiopGgEBERjX8AXI+kbyOfRHgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 0.2009 - acc: 0.9427\n",
      "Loss: 0.20089558003352315 Accuracy: 0.9426791\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.5030 - acc: 0.1666\n",
      "Epoch 00001: val_loss improved from inf to 1.77017, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_8_conv_checkpoint/001-1.7702.hdf5\n",
      "36805/36805 [==============================] - 101s 3ms/sample - loss: 2.5029 - acc: 0.1667 - val_loss: 1.7702 - val_acc: 0.4377\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5336 - acc: 0.4921\n",
      "Epoch 00002: val_loss improved from 1.77017 to 1.02237, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_8_conv_checkpoint/002-1.0224.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 1.5335 - acc: 0.4921 - val_loss: 1.0224 - val_acc: 0.6776\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1249 - acc: 0.6308\n",
      "Epoch 00003: val_loss improved from 1.02237 to 0.81245, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_8_conv_checkpoint/003-0.8125.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 1.1249 - acc: 0.6308 - val_loss: 0.8125 - val_acc: 0.7477\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9261 - acc: 0.6983\n",
      "Epoch 00004: val_loss improved from 0.81245 to 0.66076, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_8_conv_checkpoint/004-0.6608.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.9263 - acc: 0.6982 - val_loss: 0.6608 - val_acc: 0.8022\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7937 - acc: 0.7420\n",
      "Epoch 00005: val_loss improved from 0.66076 to 0.60150, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_8_conv_checkpoint/005-0.6015.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.7937 - acc: 0.7420 - val_loss: 0.6015 - val_acc: 0.8074\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6864 - acc: 0.7756\n",
      "Epoch 00006: val_loss improved from 0.60150 to 0.47397, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_8_conv_checkpoint/006-0.4740.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.6863 - acc: 0.7756 - val_loss: 0.4740 - val_acc: 0.8509\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5950 - acc: 0.8085\n",
      "Epoch 00007: val_loss improved from 0.47397 to 0.39878, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_8_conv_checkpoint/007-0.3988.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.5950 - acc: 0.8085 - val_loss: 0.3988 - val_acc: 0.8749\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5115 - acc: 0.8357\n",
      "Epoch 00008: val_loss improved from 0.39878 to 0.33302, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_8_conv_checkpoint/008-0.3330.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.5114 - acc: 0.8357 - val_loss: 0.3330 - val_acc: 0.9005\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4549 - acc: 0.8548\n",
      "Epoch 00009: val_loss improved from 0.33302 to 0.31605, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_8_conv_checkpoint/009-0.3160.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.4548 - acc: 0.8548 - val_loss: 0.3160 - val_acc: 0.9031\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4147 - acc: 0.8668\n",
      "Epoch 00010: val_loss improved from 0.31605 to 0.28011, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_8_conv_checkpoint/010-0.2801.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.4147 - acc: 0.8668 - val_loss: 0.2801 - val_acc: 0.9217\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3804 - acc: 0.8804\n",
      "Epoch 00011: val_loss improved from 0.28011 to 0.25815, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_8_conv_checkpoint/011-0.2581.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.3804 - acc: 0.8804 - val_loss: 0.2581 - val_acc: 0.9245\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3425 - acc: 0.8915\n",
      "Epoch 00012: val_loss improved from 0.25815 to 0.22267, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_8_conv_checkpoint/012-0.2227.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.3425 - acc: 0.8915 - val_loss: 0.2227 - val_acc: 0.9364\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3146 - acc: 0.8997\n",
      "Epoch 00013: val_loss did not improve from 0.22267\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.3146 - acc: 0.8997 - val_loss: 0.2308 - val_acc: 0.9324\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3000 - acc: 0.9046\n",
      "Epoch 00014: val_loss improved from 0.22267 to 0.21192, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_8_conv_checkpoint/014-0.2119.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.2999 - acc: 0.9046 - val_loss: 0.2119 - val_acc: 0.9359\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2791 - acc: 0.9123\n",
      "Epoch 00015: val_loss improved from 0.21192 to 0.18893, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_8_conv_checkpoint/015-0.1889.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.2791 - acc: 0.9123 - val_loss: 0.1889 - val_acc: 0.9455\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2580 - acc: 0.9197\n",
      "Epoch 00016: val_loss improved from 0.18893 to 0.18810, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_8_conv_checkpoint/016-0.1881.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.2580 - acc: 0.9197 - val_loss: 0.1881 - val_acc: 0.9443\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2507 - acc: 0.9205\n",
      "Epoch 00017: val_loss improved from 0.18810 to 0.18106, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_8_conv_checkpoint/017-0.1811.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.2507 - acc: 0.9205 - val_loss: 0.1811 - val_acc: 0.9464\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2290 - acc: 0.9279\n",
      "Epoch 00018: val_loss improved from 0.18106 to 0.15562, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_8_conv_checkpoint/018-0.1556.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.2290 - acc: 0.9279 - val_loss: 0.1556 - val_acc: 0.9522\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2135 - acc: 0.9317\n",
      "Epoch 00019: val_loss did not improve from 0.15562\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.2134 - acc: 0.9317 - val_loss: 0.1599 - val_acc: 0.9555\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2078 - acc: 0.9323\n",
      "Epoch 00020: val_loss improved from 0.15562 to 0.15507, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_8_conv_checkpoint/020-0.1551.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.2078 - acc: 0.9323 - val_loss: 0.1551 - val_acc: 0.9529\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1950 - acc: 0.9376\n",
      "Epoch 00021: val_loss improved from 0.15507 to 0.14213, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_8_conv_checkpoint/021-0.1421.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1950 - acc: 0.9376 - val_loss: 0.1421 - val_acc: 0.9620\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1863 - acc: 0.9405\n",
      "Epoch 00022: val_loss did not improve from 0.14213\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1863 - acc: 0.9405 - val_loss: 0.1510 - val_acc: 0.9548\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1801 - acc: 0.9423\n",
      "Epoch 00023: val_loss improved from 0.14213 to 0.13079, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_8_conv_checkpoint/023-0.1308.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1801 - acc: 0.9423 - val_loss: 0.1308 - val_acc: 0.9606\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1703 - acc: 0.9438\n",
      "Epoch 00024: val_loss did not improve from 0.13079\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1702 - acc: 0.9438 - val_loss: 0.1422 - val_acc: 0.9562\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1627 - acc: 0.9480\n",
      "Epoch 00025: val_loss did not improve from 0.13079\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1627 - acc: 0.9480 - val_loss: 0.1373 - val_acc: 0.9597\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1572 - acc: 0.9489\n",
      "Epoch 00026: val_loss did not improve from 0.13079\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1572 - acc: 0.9489 - val_loss: 0.1454 - val_acc: 0.9567\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1516 - acc: 0.9511\n",
      "Epoch 00027: val_loss did not improve from 0.13079\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1516 - acc: 0.9511 - val_loss: 0.1315 - val_acc: 0.9637\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1437 - acc: 0.9523\n",
      "Epoch 00028: val_loss did not improve from 0.13079\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1436 - acc: 0.9523 - val_loss: 0.1357 - val_acc: 0.9578\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1388 - acc: 0.9555\n",
      "Epoch 00029: val_loss did not improve from 0.13079\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1388 - acc: 0.9555 - val_loss: 0.1324 - val_acc: 0.9599\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1324 - acc: 0.9564\n",
      "Epoch 00030: val_loss did not improve from 0.13079\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1325 - acc: 0.9564 - val_loss: 0.1322 - val_acc: 0.9627\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1323 - acc: 0.9556\n",
      "Epoch 00031: val_loss did not improve from 0.13079\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1324 - acc: 0.9556 - val_loss: 0.1314 - val_acc: 0.9616\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1259 - acc: 0.9580\n",
      "Epoch 00032: val_loss improved from 0.13079 to 0.11777, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_8_conv_checkpoint/032-0.1178.hdf5\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.1259 - acc: 0.9580 - val_loss: 0.1178 - val_acc: 0.9658\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1230 - acc: 0.9597\n",
      "Epoch 00033: val_loss did not improve from 0.11777\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.1230 - acc: 0.9597 - val_loss: 0.1321 - val_acc: 0.9616\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1182 - acc: 0.9611\n",
      "Epoch 00034: val_loss did not improve from 0.11777\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1183 - acc: 0.9610 - val_loss: 0.1266 - val_acc: 0.9634\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1125 - acc: 0.9619\n",
      "Epoch 00035: val_loss did not improve from 0.11777\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1125 - acc: 0.9619 - val_loss: 0.1235 - val_acc: 0.9653\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1096 - acc: 0.9638\n",
      "Epoch 00036: val_loss did not improve from 0.11777\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1096 - acc: 0.9638 - val_loss: 0.1353 - val_acc: 0.9618\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1065 - acc: 0.9645\n",
      "Epoch 00037: val_loss improved from 0.11777 to 0.11737, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_8_conv_checkpoint/037-0.1174.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1065 - acc: 0.9645 - val_loss: 0.1174 - val_acc: 0.9655\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1022 - acc: 0.9666\n",
      "Epoch 00038: val_loss did not improve from 0.11737\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1022 - acc: 0.9666 - val_loss: 0.1299 - val_acc: 0.9627\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0943 - acc: 0.9688\n",
      "Epoch 00039: val_loss improved from 0.11737 to 0.11534, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_8_conv_checkpoint/039-0.1153.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0943 - acc: 0.9688 - val_loss: 0.1153 - val_acc: 0.9686\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0954 - acc: 0.9679\n",
      "Epoch 00040: val_loss did not improve from 0.11534\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0954 - acc: 0.9679 - val_loss: 0.1242 - val_acc: 0.9604\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0904 - acc: 0.9696\n",
      "Epoch 00041: val_loss did not improve from 0.11534\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0904 - acc: 0.9696 - val_loss: 0.1342 - val_acc: 0.9606\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0929 - acc: 0.9686\n",
      "Epoch 00042: val_loss improved from 0.11534 to 0.11337, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_8_conv_checkpoint/042-0.1134.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0929 - acc: 0.9686 - val_loss: 0.1134 - val_acc: 0.9660\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0895 - acc: 0.9696\n",
      "Epoch 00043: val_loss did not improve from 0.11337\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0895 - acc: 0.9696 - val_loss: 0.1276 - val_acc: 0.9620\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0868 - acc: 0.9721\n",
      "Epoch 00044: val_loss did not improve from 0.11337\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0868 - acc: 0.9721 - val_loss: 0.1187 - val_acc: 0.9690\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0833 - acc: 0.9724\n",
      "Epoch 00045: val_loss improved from 0.11337 to 0.11122, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_8_conv_checkpoint/045-0.1112.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0833 - acc: 0.9724 - val_loss: 0.1112 - val_acc: 0.9697\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0789 - acc: 0.9744\n",
      "Epoch 00046: val_loss did not improve from 0.11122\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0789 - acc: 0.9744 - val_loss: 0.1236 - val_acc: 0.9662\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0798 - acc: 0.9732\n",
      "Epoch 00047: val_loss improved from 0.11122 to 0.10720, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_8_conv_checkpoint/047-0.1072.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0798 - acc: 0.9732 - val_loss: 0.1072 - val_acc: 0.9695\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0760 - acc: 0.9747\n",
      "Epoch 00048: val_loss did not improve from 0.10720\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0760 - acc: 0.9747 - val_loss: 0.1161 - val_acc: 0.9660\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0734 - acc: 0.9752\n",
      "Epoch 00049: val_loss did not improve from 0.10720\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0734 - acc: 0.9752 - val_loss: 0.1292 - val_acc: 0.9641\n",
      "Epoch 50/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0717 - acc: 0.9754\n",
      "Epoch 00050: val_loss did not improve from 0.10720\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0718 - acc: 0.9753 - val_loss: 0.1090 - val_acc: 0.9695\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0728 - acc: 0.9755\n",
      "Epoch 00051: val_loss did not improve from 0.10720\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0728 - acc: 0.9755 - val_loss: 0.1210 - val_acc: 0.9690\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0670 - acc: 0.9774\n",
      "Epoch 00052: val_loss did not improve from 0.10720\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0670 - acc: 0.9774 - val_loss: 0.1252 - val_acc: 0.9651\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0656 - acc: 0.9780\n",
      "Epoch 00053: val_loss did not improve from 0.10720\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0656 - acc: 0.9780 - val_loss: 0.1301 - val_acc: 0.9630\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0663 - acc: 0.9780\n",
      "Epoch 00054: val_loss did not improve from 0.10720\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0663 - acc: 0.9780 - val_loss: 0.1285 - val_acc: 0.9618\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0650 - acc: 0.9787\n",
      "Epoch 00055: val_loss did not improve from 0.10720\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0650 - acc: 0.9787 - val_loss: 0.1260 - val_acc: 0.9686\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0660 - acc: 0.9778\n",
      "Epoch 00056: val_loss did not improve from 0.10720\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0661 - acc: 0.9777 - val_loss: 0.1275 - val_acc: 0.9658\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0591 - acc: 0.9797\n",
      "Epoch 00057: val_loss did not improve from 0.10720\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0591 - acc: 0.9796 - val_loss: 0.1290 - val_acc: 0.9667\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0623 - acc: 0.9790\n",
      "Epoch 00058: val_loss did not improve from 0.10720\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0623 - acc: 0.9790 - val_loss: 0.1314 - val_acc: 0.9674\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0595 - acc: 0.9789\n",
      "Epoch 00059: val_loss did not improve from 0.10720\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0595 - acc: 0.9789 - val_loss: 0.1259 - val_acc: 0.9676\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0630 - acc: 0.9790\n",
      "Epoch 00060: val_loss did not improve from 0.10720\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0630 - acc: 0.9790 - val_loss: 0.1094 - val_acc: 0.9695\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0548 - acc: 0.9821\n",
      "Epoch 00061: val_loss did not improve from 0.10720\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0548 - acc: 0.9821 - val_loss: 0.1302 - val_acc: 0.9679\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0555 - acc: 0.9820\n",
      "Epoch 00062: val_loss did not improve from 0.10720\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0555 - acc: 0.9820 - val_loss: 0.1188 - val_acc: 0.9672\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0520 - acc: 0.9822\n",
      "Epoch 00063: val_loss did not improve from 0.10720\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0520 - acc: 0.9822 - val_loss: 0.1318 - val_acc: 0.9674\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0516 - acc: 0.9826\n",
      "Epoch 00064: val_loss did not improve from 0.10720\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0516 - acc: 0.9826 - val_loss: 0.1196 - val_acc: 0.9709\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0549 - acc: 0.9818\n",
      "Epoch 00065: val_loss did not improve from 0.10720\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0549 - acc: 0.9818 - val_loss: 0.1449 - val_acc: 0.9693\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0502 - acc: 0.9830\n",
      "Epoch 00066: val_loss did not improve from 0.10720\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0502 - acc: 0.9830 - val_loss: 0.1297 - val_acc: 0.9648\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0486 - acc: 0.9839\n",
      "Epoch 00067: val_loss did not improve from 0.10720\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0486 - acc: 0.9839 - val_loss: 0.1137 - val_acc: 0.9713\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0476 - acc: 0.9843\n",
      "Epoch 00068: val_loss did not improve from 0.10720\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0476 - acc: 0.9843 - val_loss: 0.1406 - val_acc: 0.9690\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0549 - acc: 0.9824\n",
      "Epoch 00069: val_loss did not improve from 0.10720\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0549 - acc: 0.9824 - val_loss: 0.1395 - val_acc: 0.9681\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0449 - acc: 0.9846\n",
      "Epoch 00070: val_loss did not improve from 0.10720\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0449 - acc: 0.9846 - val_loss: 0.1263 - val_acc: 0.9686\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0488 - acc: 0.9831\n",
      "Epoch 00071: val_loss did not improve from 0.10720\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0488 - acc: 0.9831 - val_loss: 0.1252 - val_acc: 0.9725\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0449 - acc: 0.9854\n",
      "Epoch 00072: val_loss did not improve from 0.10720\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0450 - acc: 0.9853 - val_loss: 0.1543 - val_acc: 0.9637\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0477 - acc: 0.9835\n",
      "Epoch 00073: val_loss did not improve from 0.10720\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0477 - acc: 0.9835 - val_loss: 0.1233 - val_acc: 0.9706\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0414 - acc: 0.9860\n",
      "Epoch 00074: val_loss did not improve from 0.10720\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0414 - acc: 0.9860 - val_loss: 0.1218 - val_acc: 0.9697\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0429 - acc: 0.9848\n",
      "Epoch 00075: val_loss did not improve from 0.10720\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0430 - acc: 0.9847 - val_loss: 0.1429 - val_acc: 0.9641\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0471 - acc: 0.9841\n",
      "Epoch 00076: val_loss did not improve from 0.10720\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0471 - acc: 0.9841 - val_loss: 0.1196 - val_acc: 0.9718\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0438 - acc: 0.9850\n",
      "Epoch 00077: val_loss did not improve from 0.10720\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0438 - acc: 0.9850 - val_loss: 0.1292 - val_acc: 0.9702\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0382 - acc: 0.9874\n",
      "Epoch 00078: val_loss did not improve from 0.10720\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0382 - acc: 0.9874 - val_loss: 0.1358 - val_acc: 0.9688\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0414 - acc: 0.9863\n",
      "Epoch 00079: val_loss did not improve from 0.10720\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0414 - acc: 0.9863 - val_loss: 0.1172 - val_acc: 0.9674\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0372 - acc: 0.9870\n",
      "Epoch 00080: val_loss did not improve from 0.10720\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0371 - acc: 0.9870 - val_loss: 0.1406 - val_acc: 0.9681\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0416 - acc: 0.9860\n",
      "Epoch 00081: val_loss did not improve from 0.10720\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0416 - acc: 0.9860 - val_loss: 0.1220 - val_acc: 0.9702\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0376 - acc: 0.9878\n",
      "Epoch 00082: val_loss did not improve from 0.10720\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0376 - acc: 0.9878 - val_loss: 0.1387 - val_acc: 0.9686\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0381 - acc: 0.9871\n",
      "Epoch 00083: val_loss did not improve from 0.10720\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0382 - acc: 0.9871 - val_loss: 0.1362 - val_acc: 0.9674\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0395 - acc: 0.9869\n",
      "Epoch 00084: val_loss did not improve from 0.10720\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0395 - acc: 0.9869 - val_loss: 0.1403 - val_acc: 0.9706\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0356 - acc: 0.9879\n",
      "Epoch 00085: val_loss did not improve from 0.10720\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0356 - acc: 0.9878 - val_loss: 0.1271 - val_acc: 0.9702\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0385 - acc: 0.9875\n",
      "Epoch 00086: val_loss did not improve from 0.10720\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0385 - acc: 0.9875 - val_loss: 0.1385 - val_acc: 0.9690\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0353 - acc: 0.9883\n",
      "Epoch 00087: val_loss did not improve from 0.10720\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0353 - acc: 0.9883 - val_loss: 0.1231 - val_acc: 0.9686\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0338 - acc: 0.9892\n",
      "Epoch 00088: val_loss did not improve from 0.10720\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0338 - acc: 0.9891 - val_loss: 0.1417 - val_acc: 0.9674\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0378 - acc: 0.9872\n",
      "Epoch 00089: val_loss did not improve from 0.10720\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0378 - acc: 0.9872 - val_loss: 0.1656 - val_acc: 0.9690\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0321 - acc: 0.9893\n",
      "Epoch 00090: val_loss did not improve from 0.10720\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0321 - acc: 0.9893 - val_loss: 0.1633 - val_acc: 0.9683\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0318 - acc: 0.9892\n",
      "Epoch 00091: val_loss did not improve from 0.10720\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0318 - acc: 0.9892 - val_loss: 0.1343 - val_acc: 0.9695\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0329 - acc: 0.9889\n",
      "Epoch 00092: val_loss did not improve from 0.10720\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0330 - acc: 0.9888 - val_loss: 0.1343 - val_acc: 0.9695\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0387 - acc: 0.9875\n",
      "Epoch 00093: val_loss improved from 0.10720 to 0.10587, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_8_conv_checkpoint/093-0.1059.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0387 - acc: 0.9875 - val_loss: 0.1059 - val_acc: 0.9734\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0318 - acc: 0.9887\n",
      "Epoch 00094: val_loss did not improve from 0.10587\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0318 - acc: 0.9888 - val_loss: 0.1410 - val_acc: 0.9679\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0301 - acc: 0.9899\n",
      "Epoch 00095: val_loss did not improve from 0.10587\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0301 - acc: 0.9899 - val_loss: 0.1385 - val_acc: 0.9704\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0344 - acc: 0.9885\n",
      "Epoch 00096: val_loss did not improve from 0.10587\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0344 - acc: 0.9885 - val_loss: 0.1546 - val_acc: 0.9681\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0314 - acc: 0.9897\n",
      "Epoch 00097: val_loss did not improve from 0.10587\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0314 - acc: 0.9897 - val_loss: 0.1385 - val_acc: 0.9697\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0302 - acc: 0.9897\n",
      "Epoch 00098: val_loss did not improve from 0.10587\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0302 - acc: 0.9897 - val_loss: 0.1502 - val_acc: 0.9676\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0257 - acc: 0.9916\n",
      "Epoch 00099: val_loss did not improve from 0.10587\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0257 - acc: 0.9916 - val_loss: 0.1740 - val_acc: 0.9637\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0308 - acc: 0.9899\n",
      "Epoch 00100: val_loss did not improve from 0.10587\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0308 - acc: 0.9899 - val_loss: 0.1368 - val_acc: 0.9709\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0293 - acc: 0.9901\n",
      "Epoch 00101: val_loss did not improve from 0.10587\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0293 - acc: 0.9901 - val_loss: 0.1331 - val_acc: 0.9683\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0269 - acc: 0.9912\n",
      "Epoch 00102: val_loss did not improve from 0.10587\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0269 - acc: 0.9912 - val_loss: 0.1498 - val_acc: 0.9697\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0286 - acc: 0.9905\n",
      "Epoch 00103: val_loss did not improve from 0.10587\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0286 - acc: 0.9905 - val_loss: 0.1774 - val_acc: 0.9676\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0283 - acc: 0.9911\n",
      "Epoch 00104: val_loss did not improve from 0.10587\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0283 - acc: 0.9911 - val_loss: 0.1269 - val_acc: 0.9700\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0273 - acc: 0.9909\n",
      "Epoch 00105: val_loss did not improve from 0.10587\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0273 - acc: 0.9909 - val_loss: 0.1934 - val_acc: 0.9651\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0273 - acc: 0.9914\n",
      "Epoch 00106: val_loss did not improve from 0.10587\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0273 - acc: 0.9914 - val_loss: 0.1728 - val_acc: 0.9700\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0303 - acc: 0.9902\n",
      "Epoch 00107: val_loss did not improve from 0.10587\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0303 - acc: 0.9902 - val_loss: 0.1383 - val_acc: 0.9700\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0273 - acc: 0.9908\n",
      "Epoch 00108: val_loss did not improve from 0.10587\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0273 - acc: 0.9908 - val_loss: 0.1288 - val_acc: 0.9725\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0264 - acc: 0.9914\n",
      "Epoch 00109: val_loss did not improve from 0.10587\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0264 - acc: 0.9914 - val_loss: 0.1539 - val_acc: 0.9706\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0242 - acc: 0.9917\n",
      "Epoch 00110: val_loss did not improve from 0.10587\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0242 - acc: 0.9917 - val_loss: 0.1478 - val_acc: 0.9709\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0236 - acc: 0.9921\n",
      "Epoch 00111: val_loss did not improve from 0.10587\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0236 - acc: 0.9921 - val_loss: 0.1598 - val_acc: 0.9660\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0258 - acc: 0.9914\n",
      "Epoch 00112: val_loss did not improve from 0.10587\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0258 - acc: 0.9914 - val_loss: 0.1685 - val_acc: 0.9676\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0235 - acc: 0.9920\n",
      "Epoch 00113: val_loss did not improve from 0.10587\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0235 - acc: 0.9920 - val_loss: 0.1577 - val_acc: 0.9681\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0243 - acc: 0.9921\n",
      "Epoch 00114: val_loss did not improve from 0.10587\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0242 - acc: 0.9921 - val_loss: 0.1439 - val_acc: 0.9709\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0297 - acc: 0.9899\n",
      "Epoch 00115: val_loss did not improve from 0.10587\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0297 - acc: 0.9899 - val_loss: 0.1259 - val_acc: 0.9713\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0217 - acc: 0.9927\n",
      "Epoch 00116: val_loss did not improve from 0.10587\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0218 - acc: 0.9927 - val_loss: 0.1457 - val_acc: 0.9674\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0313 - acc: 0.9898\n",
      "Epoch 00117: val_loss did not improve from 0.10587\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0313 - acc: 0.9898 - val_loss: 0.1431 - val_acc: 0.9695\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0201 - acc: 0.9931\n",
      "Epoch 00118: val_loss did not improve from 0.10587\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0201 - acc: 0.9931 - val_loss: 0.1269 - val_acc: 0.9725\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0211 - acc: 0.9932\n",
      "Epoch 00119: val_loss did not improve from 0.10587\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0211 - acc: 0.9932 - val_loss: 0.1261 - val_acc: 0.9711\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0239 - acc: 0.9926\n",
      "Epoch 00120: val_loss did not improve from 0.10587\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0239 - acc: 0.9926 - val_loss: 0.1356 - val_acc: 0.9690\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0218 - acc: 0.9931\n",
      "Epoch 00121: val_loss did not improve from 0.10587\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0218 - acc: 0.9931 - val_loss: 0.1516 - val_acc: 0.9697\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0185 - acc: 0.9941\n",
      "Epoch 00122: val_loss did not improve from 0.10587\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0185 - acc: 0.9941 - val_loss: 0.1515 - val_acc: 0.9700\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0236 - acc: 0.9919\n",
      "Epoch 00123: val_loss did not improve from 0.10587\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0236 - acc: 0.9919 - val_loss: 0.1514 - val_acc: 0.9690\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0234 - acc: 0.9926\n",
      "Epoch 00124: val_loss did not improve from 0.10587\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0234 - acc: 0.9926 - val_loss: 0.1527 - val_acc: 0.9723\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0218 - acc: 0.9929\n",
      "Epoch 00125: val_loss did not improve from 0.10587\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0218 - acc: 0.9929 - val_loss: 0.1524 - val_acc: 0.9700\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0212 - acc: 0.9929\n",
      "Epoch 00126: val_loss did not improve from 0.10587\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0212 - acc: 0.9929 - val_loss: 0.1700 - val_acc: 0.9681\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0215 - acc: 0.9926\n",
      "Epoch 00127: val_loss did not improve from 0.10587\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0215 - acc: 0.9926 - val_loss: 0.1371 - val_acc: 0.9704\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0190 - acc: 0.9942\n",
      "Epoch 00128: val_loss did not improve from 0.10587\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0190 - acc: 0.9942 - val_loss: 0.1631 - val_acc: 0.9690\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0203 - acc: 0.9934\n",
      "Epoch 00129: val_loss did not improve from 0.10587\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0203 - acc: 0.9934 - val_loss: 0.1528 - val_acc: 0.9723\n",
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0200 - acc: 0.9935\n",
      "Epoch 00130: val_loss did not improve from 0.10587\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0200 - acc: 0.9935 - val_loss: 0.1474 - val_acc: 0.9693\n",
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0200 - acc: 0.9936\n",
      "Epoch 00131: val_loss did not improve from 0.10587\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0200 - acc: 0.9936 - val_loss: 0.1487 - val_acc: 0.9709\n",
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0184 - acc: 0.9939\n",
      "Epoch 00132: val_loss did not improve from 0.10587\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0184 - acc: 0.9939 - val_loss: 0.1606 - val_acc: 0.9667\n",
      "Epoch 133/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0210 - acc: 0.9937\n",
      "Epoch 00133: val_loss did not improve from 0.10587\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0210 - acc: 0.9937 - val_loss: 0.1733 - val_acc: 0.9688\n",
      "Epoch 134/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0198 - acc: 0.9934\n",
      "Epoch 00134: val_loss did not improve from 0.10587\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0198 - acc: 0.9934 - val_loss: 0.1564 - val_acc: 0.9702\n",
      "Epoch 135/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0194 - acc: 0.9940\n",
      "Epoch 00135: val_loss did not improve from 0.10587\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0194 - acc: 0.9940 - val_loss: 0.1489 - val_acc: 0.9706\n",
      "Epoch 136/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0206 - acc: 0.9932\n",
      "Epoch 00136: val_loss did not improve from 0.10587\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0206 - acc: 0.9932 - val_loss: 0.1445 - val_acc: 0.9713\n",
      "Epoch 137/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0169 - acc: 0.9946\n",
      "Epoch 00137: val_loss did not improve from 0.10587\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0169 - acc: 0.9946 - val_loss: 0.1459 - val_acc: 0.9727\n",
      "Epoch 138/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0193 - acc: 0.9936\n",
      "Epoch 00138: val_loss did not improve from 0.10587\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0193 - acc: 0.9936 - val_loss: 0.1578 - val_acc: 0.9704\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 139/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0205 - acc: 0.9934\n",
      "Epoch 00139: val_loss did not improve from 0.10587\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0205 - acc: 0.9934 - val_loss: 0.1362 - val_acc: 0.9720\n",
      "Epoch 140/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0207 - acc: 0.9937\n",
      "Epoch 00140: val_loss did not improve from 0.10587\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0207 - acc: 0.9937 - val_loss: 0.1383 - val_acc: 0.9709\n",
      "Epoch 141/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0153 - acc: 0.9951\n",
      "Epoch 00141: val_loss did not improve from 0.10587\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0153 - acc: 0.9951 - val_loss: 0.1399 - val_acc: 0.9711\n",
      "Epoch 142/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0190 - acc: 0.9941\n",
      "Epoch 00142: val_loss did not improve from 0.10587\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0190 - acc: 0.9941 - val_loss: 0.1624 - val_acc: 0.9688\n",
      "Epoch 143/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0165 - acc: 0.9945\n",
      "Epoch 00143: val_loss did not improve from 0.10587\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0165 - acc: 0.9945 - val_loss: 0.1732 - val_acc: 0.9681\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_8_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 0.1829 - acc: 0.9630\n",
      "Loss: 0.18293105663304596 Accuracy: 0.9630322\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.2604 - acc: 0.2589\n",
      "Epoch 00001: val_loss improved from inf to 1.45279, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_9_conv_checkpoint/001-1.4528.hdf5\n",
      "36805/36805 [==============================] - 104s 3ms/sample - loss: 2.2604 - acc: 0.2589 - val_loss: 1.4528 - val_acc: 0.5316\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2838 - acc: 0.5808\n",
      "Epoch 00002: val_loss improved from 1.45279 to 0.93213, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_9_conv_checkpoint/002-0.9321.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 1.2837 - acc: 0.5808 - val_loss: 0.9321 - val_acc: 0.7009\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8632 - acc: 0.7179\n",
      "Epoch 00003: val_loss improved from 0.93213 to 0.63588, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_9_conv_checkpoint/003-0.6359.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.8632 - acc: 0.7179 - val_loss: 0.6359 - val_acc: 0.7936\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6488 - acc: 0.7901\n",
      "Epoch 00004: val_loss improved from 0.63588 to 0.59140, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_9_conv_checkpoint/004-0.5914.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.6488 - acc: 0.7901 - val_loss: 0.5914 - val_acc: 0.8178\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5368 - acc: 0.8268\n",
      "Epoch 00005: val_loss improved from 0.59140 to 0.47067, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_9_conv_checkpoint/005-0.4707.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.5369 - acc: 0.8267 - val_loss: 0.4707 - val_acc: 0.8528\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4689 - acc: 0.8490\n",
      "Epoch 00006: val_loss improved from 0.47067 to 0.32111, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_9_conv_checkpoint/006-0.3211.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.4689 - acc: 0.8490 - val_loss: 0.3211 - val_acc: 0.8959\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4123 - acc: 0.8689\n",
      "Epoch 00007: val_loss did not improve from 0.32111\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.4124 - acc: 0.8688 - val_loss: 0.3538 - val_acc: 0.8896\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3639 - acc: 0.8827\n",
      "Epoch 00008: val_loss improved from 0.32111 to 0.28435, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_9_conv_checkpoint/008-0.2843.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.3639 - acc: 0.8827 - val_loss: 0.2843 - val_acc: 0.9092\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3215 - acc: 0.8956\n",
      "Epoch 00009: val_loss improved from 0.28435 to 0.26217, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_9_conv_checkpoint/009-0.2622.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.3214 - acc: 0.8956 - val_loss: 0.2622 - val_acc: 0.9166\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2969 - acc: 0.9048\n",
      "Epoch 00010: val_loss improved from 0.26217 to 0.25965, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_9_conv_checkpoint/010-0.2597.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.2970 - acc: 0.9047 - val_loss: 0.2597 - val_acc: 0.9187\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2707 - acc: 0.9131\n",
      "Epoch 00011: val_loss improved from 0.25965 to 0.22228, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_9_conv_checkpoint/011-0.2223.hdf5\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.2708 - acc: 0.9131 - val_loss: 0.2223 - val_acc: 0.9329\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2529 - acc: 0.9181\n",
      "Epoch 00012: val_loss improved from 0.22228 to 0.20242, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_9_conv_checkpoint/012-0.2024.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.2529 - acc: 0.9181 - val_loss: 0.2024 - val_acc: 0.9357\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2287 - acc: 0.9254\n",
      "Epoch 00013: val_loss improved from 0.20242 to 0.20198, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_9_conv_checkpoint/013-0.2020.hdf5\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.2287 - acc: 0.9254 - val_loss: 0.2020 - val_acc: 0.9369\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2101 - acc: 0.9317\n",
      "Epoch 00014: val_loss improved from 0.20198 to 0.19779, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_9_conv_checkpoint/014-0.1978.hdf5\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.2101 - acc: 0.9317 - val_loss: 0.1978 - val_acc: 0.9357\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1985 - acc: 0.9348\n",
      "Epoch 00015: val_loss did not improve from 0.19779\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1985 - acc: 0.9348 - val_loss: 0.2298 - val_acc: 0.9264\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1830 - acc: 0.9401\n",
      "Epoch 00016: val_loss improved from 0.19779 to 0.16430, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_9_conv_checkpoint/016-0.1643.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1831 - acc: 0.9401 - val_loss: 0.1643 - val_acc: 0.9469\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1749 - acc: 0.9427\n",
      "Epoch 00017: val_loss did not improve from 0.16430\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1750 - acc: 0.9427 - val_loss: 0.1700 - val_acc: 0.9495\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1569 - acc: 0.9489\n",
      "Epoch 00018: val_loss did not improve from 0.16430\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1569 - acc: 0.9489 - val_loss: 0.1908 - val_acc: 0.9399\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1491 - acc: 0.9510\n",
      "Epoch 00019: val_loss did not improve from 0.16430\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1491 - acc: 0.9510 - val_loss: 0.1780 - val_acc: 0.9464\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1402 - acc: 0.9528\n",
      "Epoch 00020: val_loss did not improve from 0.16430\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1402 - acc: 0.9528 - val_loss: 0.1766 - val_acc: 0.9432\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1344 - acc: 0.9555\n",
      "Epoch 00021: val_loss did not improve from 0.16430\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1344 - acc: 0.9555 - val_loss: 0.1685 - val_acc: 0.9434\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1290 - acc: 0.9573\n",
      "Epoch 00022: val_loss improved from 0.16430 to 0.16262, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_9_conv_checkpoint/022-0.1626.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1291 - acc: 0.9573 - val_loss: 0.1626 - val_acc: 0.9515\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1264 - acc: 0.9584\n",
      "Epoch 00023: val_loss did not improve from 0.16262\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1265 - acc: 0.9584 - val_loss: 0.1639 - val_acc: 0.9539\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1124 - acc: 0.9626\n",
      "Epoch 00024: val_loss improved from 0.16262 to 0.16232, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_9_conv_checkpoint/024-0.1623.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1124 - acc: 0.9626 - val_loss: 0.1623 - val_acc: 0.9553\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1053 - acc: 0.9652\n",
      "Epoch 00025: val_loss improved from 0.16232 to 0.14835, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_9_conv_checkpoint/025-0.1484.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1054 - acc: 0.9652 - val_loss: 0.1484 - val_acc: 0.9539\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1038 - acc: 0.9655\n",
      "Epoch 00026: val_loss did not improve from 0.14835\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1038 - acc: 0.9655 - val_loss: 0.1747 - val_acc: 0.9515\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0965 - acc: 0.9673\n",
      "Epoch 00027: val_loss improved from 0.14835 to 0.14019, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_9_conv_checkpoint/027-0.1402.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0965 - acc: 0.9673 - val_loss: 0.1402 - val_acc: 0.9567\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0901 - acc: 0.9689\n",
      "Epoch 00028: val_loss did not improve from 0.14019\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0901 - acc: 0.9689 - val_loss: 0.1557 - val_acc: 0.9546\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0838 - acc: 0.9718\n",
      "Epoch 00029: val_loss did not improve from 0.14019\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0839 - acc: 0.9718 - val_loss: 0.1608 - val_acc: 0.9574\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0893 - acc: 0.9704\n",
      "Epoch 00030: val_loss did not improve from 0.14019\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0893 - acc: 0.9704 - val_loss: 0.1684 - val_acc: 0.9562\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0787 - acc: 0.9725\n",
      "Epoch 00031: val_loss did not improve from 0.14019\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0787 - acc: 0.9725 - val_loss: 0.1536 - val_acc: 0.9595\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0721 - acc: 0.9758\n",
      "Epoch 00032: val_loss did not improve from 0.14019\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0721 - acc: 0.9758 - val_loss: 0.1444 - val_acc: 0.9630\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0712 - acc: 0.9756\n",
      "Epoch 00033: val_loss did not improve from 0.14019\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0712 - acc: 0.9756 - val_loss: 0.1492 - val_acc: 0.9595\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0720 - acc: 0.9763\n",
      "Epoch 00034: val_loss did not improve from 0.14019\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0720 - acc: 0.9763 - val_loss: 0.1529 - val_acc: 0.9583\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0659 - acc: 0.9776\n",
      "Epoch 00035: val_loss did not improve from 0.14019\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0659 - acc: 0.9776 - val_loss: 0.1591 - val_acc: 0.9553\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0621 - acc: 0.9789\n",
      "Epoch 00036: val_loss did not improve from 0.14019\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0621 - acc: 0.9789 - val_loss: 0.1823 - val_acc: 0.9590\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0595 - acc: 0.9798\n",
      "Epoch 00037: val_loss did not improve from 0.14019\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0595 - acc: 0.9798 - val_loss: 0.1453 - val_acc: 0.9585\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0566 - acc: 0.9809\n",
      "Epoch 00038: val_loss did not improve from 0.14019\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0566 - acc: 0.9809 - val_loss: 0.1669 - val_acc: 0.9571\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0586 - acc: 0.9807\n",
      "Epoch 00039: val_loss did not improve from 0.14019\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0586 - acc: 0.9807 - val_loss: 0.1619 - val_acc: 0.9597\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0531 - acc: 0.9820\n",
      "Epoch 00040: val_loss did not improve from 0.14019\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0531 - acc: 0.9820 - val_loss: 0.1597 - val_acc: 0.9597\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0510 - acc: 0.9822\n",
      "Epoch 00041: val_loss did not improve from 0.14019\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0510 - acc: 0.9822 - val_loss: 0.1574 - val_acc: 0.9620\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0483 - acc: 0.9839\n",
      "Epoch 00042: val_loss did not improve from 0.14019\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0483 - acc: 0.9839 - val_loss: 0.1568 - val_acc: 0.9646\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0484 - acc: 0.9841\n",
      "Epoch 00043: val_loss did not improve from 0.14019\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0484 - acc: 0.9841 - val_loss: 0.1726 - val_acc: 0.9611\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0529 - acc: 0.9829\n",
      "Epoch 00044: val_loss did not improve from 0.14019\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0529 - acc: 0.9829 - val_loss: 0.1600 - val_acc: 0.9602\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0477 - acc: 0.9842\n",
      "Epoch 00045: val_loss did not improve from 0.14019\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0477 - acc: 0.9842 - val_loss: 0.1610 - val_acc: 0.9609\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0458 - acc: 0.9845\n",
      "Epoch 00046: val_loss did not improve from 0.14019\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0458 - acc: 0.9845 - val_loss: 0.1731 - val_acc: 0.9597\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0424 - acc: 0.9865\n",
      "Epoch 00047: val_loss did not improve from 0.14019\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0424 - acc: 0.9865 - val_loss: 0.1576 - val_acc: 0.9627\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0366 - acc: 0.9877\n",
      "Epoch 00048: val_loss did not improve from 0.14019\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0366 - acc: 0.9877 - val_loss: 0.1653 - val_acc: 0.9620\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0409 - acc: 0.9862\n",
      "Epoch 00049: val_loss did not improve from 0.14019\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0409 - acc: 0.9862 - val_loss: 0.1968 - val_acc: 0.9606\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0420 - acc: 0.9861\n",
      "Epoch 00050: val_loss did not improve from 0.14019\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0420 - acc: 0.9861 - val_loss: 0.1892 - val_acc: 0.9532\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0383 - acc: 0.9874\n",
      "Epoch 00051: val_loss did not improve from 0.14019\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0383 - acc: 0.9874 - val_loss: 0.1772 - val_acc: 0.9609\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0389 - acc: 0.9871\n",
      "Epoch 00052: val_loss did not improve from 0.14019\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0389 - acc: 0.9871 - val_loss: 0.1847 - val_acc: 0.9609\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0338 - acc: 0.9885\n",
      "Epoch 00053: val_loss did not improve from 0.14019\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0338 - acc: 0.9885 - val_loss: 0.1901 - val_acc: 0.9606\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0349 - acc: 0.9881\n",
      "Epoch 00054: val_loss did not improve from 0.14019\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0349 - acc: 0.9881 - val_loss: 0.1763 - val_acc: 0.9606\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0379 - acc: 0.9874\n",
      "Epoch 00055: val_loss did not improve from 0.14019\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0379 - acc: 0.9874 - val_loss: 0.1862 - val_acc: 0.9576\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0320 - acc: 0.9899\n",
      "Epoch 00056: val_loss did not improve from 0.14019\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0320 - acc: 0.9899 - val_loss: 0.1476 - val_acc: 0.9630\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0322 - acc: 0.9894\n",
      "Epoch 00057: val_loss did not improve from 0.14019\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0322 - acc: 0.9894 - val_loss: 0.1549 - val_acc: 0.9634\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0357 - acc: 0.9885\n",
      "Epoch 00058: val_loss did not improve from 0.14019\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0357 - acc: 0.9885 - val_loss: 0.2162 - val_acc: 0.9546\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0312 - acc: 0.9896\n",
      "Epoch 00059: val_loss did not improve from 0.14019\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0312 - acc: 0.9896 - val_loss: 0.1700 - val_acc: 0.9676\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0311 - acc: 0.9898\n",
      "Epoch 00060: val_loss did not improve from 0.14019\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0311 - acc: 0.9898 - val_loss: 0.1761 - val_acc: 0.9637\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0339 - acc: 0.9892\n",
      "Epoch 00061: val_loss did not improve from 0.14019\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0339 - acc: 0.9892 - val_loss: 0.1822 - val_acc: 0.9641\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0285 - acc: 0.9907\n",
      "Epoch 00062: val_loss did not improve from 0.14019\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0285 - acc: 0.9907 - val_loss: 0.1775 - val_acc: 0.9658\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0255 - acc: 0.9915\n",
      "Epoch 00063: val_loss did not improve from 0.14019\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0254 - acc: 0.9915 - val_loss: 0.1910 - val_acc: 0.9627\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0269 - acc: 0.9913\n",
      "Epoch 00064: val_loss did not improve from 0.14019\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0269 - acc: 0.9913 - val_loss: 0.1732 - val_acc: 0.9634\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0286 - acc: 0.9905\n",
      "Epoch 00065: val_loss did not improve from 0.14019\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0286 - acc: 0.9905 - val_loss: 0.1748 - val_acc: 0.9627\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0274 - acc: 0.9912\n",
      "Epoch 00066: val_loss did not improve from 0.14019\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0274 - acc: 0.9913 - val_loss: 0.1731 - val_acc: 0.9613\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0264 - acc: 0.9915\n",
      "Epoch 00067: val_loss did not improve from 0.14019\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0264 - acc: 0.9916 - val_loss: 0.1931 - val_acc: 0.9651\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0260 - acc: 0.9916\n",
      "Epoch 00068: val_loss did not improve from 0.14019\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0260 - acc: 0.9916 - val_loss: 0.2012 - val_acc: 0.9611\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0281 - acc: 0.9910\n",
      "Epoch 00069: val_loss did not improve from 0.14019\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0281 - acc: 0.9910 - val_loss: 0.1744 - val_acc: 0.9660\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0282 - acc: 0.9908\n",
      "Epoch 00070: val_loss did not improve from 0.14019\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0282 - acc: 0.9908 - val_loss: 0.1630 - val_acc: 0.9665\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0250 - acc: 0.9918\n",
      "Epoch 00071: val_loss did not improve from 0.14019\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0250 - acc: 0.9918 - val_loss: 0.1772 - val_acc: 0.9683\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0252 - acc: 0.9924\n",
      "Epoch 00072: val_loss did not improve from 0.14019\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0252 - acc: 0.9924 - val_loss: 0.1842 - val_acc: 0.9618\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0247 - acc: 0.9921\n",
      "Epoch 00073: val_loss did not improve from 0.14019\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0247 - acc: 0.9921 - val_loss: 0.1915 - val_acc: 0.9611\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0228 - acc: 0.9929\n",
      "Epoch 00074: val_loss did not improve from 0.14019\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0228 - acc: 0.9929 - val_loss: 0.1726 - val_acc: 0.9630\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0207 - acc: 0.9937\n",
      "Epoch 00075: val_loss did not improve from 0.14019\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0206 - acc: 0.9937 - val_loss: 0.2027 - val_acc: 0.9653\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0254 - acc: 0.9923\n",
      "Epoch 00076: val_loss did not improve from 0.14019\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0254 - acc: 0.9923 - val_loss: 0.2188 - val_acc: 0.9583\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0216 - acc: 0.9928\n",
      "Epoch 00077: val_loss did not improve from 0.14019\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0216 - acc: 0.9928 - val_loss: 0.1828 - val_acc: 0.9648\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_9_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 0.2276 - acc: 0.9350\n",
      "Loss: 0.2276410343955239 Accuracy: 0.9349948\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 10):\n",
    "    model_name = '1D_CNN_custom_conv_3_VGG_DO_{}_conv'.format(i)\n",
    "    model = build_1d_cnn_custom_conv_3_VGG_DO(conv_num=i)\n",
    "#         model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4),\n",
    "          metrics=['accuracy'])\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model_filename = model_path+'{epoch:03d}-{val_loss:.4f}.hdf5'\n",
    "    checkpointer = ModelCheckpoint(filepath = model_filename, monitor = \"val_loss\", \n",
    "                                   verbose=1, save_best_only=True)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=50)\n",
    "    hist = model.fit(x_train_abs, y_train_onehot, batch_size=64, epochs=500, \n",
    "                     validation_data=[x_val_abs, y_val_onehot], shuffle=True, \n",
    "                     callbacks = [checkpointer, early_stopping])\n",
    "\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "    ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "    ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "    ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    png_path = 'visualization/learning_curve/'\n",
    "    filename = model_name+'.png'\n",
    "    os.makedirs(png_path, exist_ok=True)\n",
    "    fig.savefig(png_path+filename, transparent=True)\n",
    "\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "    model = load_model(model_filename)\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "    print()\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_1_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_90 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "conv1d_91 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 1024000)           0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 1024000)           0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 16)                16384016  \n",
      "=================================================================\n",
      "Total params: 16,396,624\n",
      "Trainable params: 16,396,624\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 556us/sample - loss: 1.9917 - acc: 0.3877\n",
      "Loss: 1.9917109709043492 Accuracy: 0.38774663\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_2_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_92 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "conv1d_93 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_94 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_95 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 341312)            0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 341312)            0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 16)                5461008   \n",
      "=================================================================\n",
      "Total params: 5,498,320\n",
      "Trainable params: 5,498,320\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 882us/sample - loss: 1.7163 - acc: 0.4752\n",
      "Loss: 1.7163452906276826 Accuracy: 0.47518173\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_3_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_96 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "conv1d_97 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_98 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_99 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_100 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_101 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 16)                1819664   \n",
      "=================================================================\n",
      "Total params: 1,881,680\n",
      "Trainable params: 1,881,680\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 1.4290 - acc: 0.5626\n",
      "Loss: 1.4289896430008633 Accuracy: 0.5626168\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_4_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_102 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "conv1d_103 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_104 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_105 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_106 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_107 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_108 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_109 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 16)                606224    \n",
      "=================================================================\n",
      "Total params: 692,944\n",
      "Trainable params: 692,944\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 1.0904 - acc: 0.6719\n",
      "Loss: 1.0903628553928244 Accuracy: 0.6718588\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_5_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_110 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "conv1d_111 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_112 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_113 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_114 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_115 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_116 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_117 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_118 (Conv1D)          (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "conv1d_119 (Conv1D)          (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 16)                403472    \n",
      "=================================================================\n",
      "Total params: 564,176\n",
      "Trainable params: 564,176\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 0.8833 - acc: 0.7666\n",
      "Loss: 0.883316714778496 Accuracy: 0.7665628\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_6_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_120 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "conv1d_121 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_122 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_123 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_124 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_125 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_126 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_127 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_128 (Conv1D)          (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "conv1d_129 (Conv1D)          (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_130 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "conv1d_131 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_14 (Flatten)         (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 16)                133136    \n",
      "=================================================================\n",
      "Total params: 392,400\n",
      "Trainable params: 392,400\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 0.4131 - acc: 0.9018\n",
      "Loss: 0.41305809186255077 Accuracy: 0.9017653\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_7_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_132 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "conv1d_133 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_134 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_135 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_136 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_137 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_138 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_139 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_140 (Conv1D)          (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "conv1d_141 (Conv1D)          (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_142 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "conv1d_143 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_144 (Conv1D)          (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "conv1d_145 (Conv1D)          (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_15 (Flatten)         (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 16)                43024     \n",
      "=================================================================\n",
      "Total params: 400,848\n",
      "Trainable params: 400,848\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.2009 - acc: 0.9427\n",
      "Loss: 0.20089558003352315 Accuracy: 0.9426791\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_8_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_146 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "conv1d_147 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_148 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_149 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_150 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_151 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_152 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_153 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_154 (Conv1D)          (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "conv1d_155 (Conv1D)          (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_156 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "conv1d_157 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_158 (Conv1D)          (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "conv1d_159 (Conv1D)          (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_160 (Conv1D)          (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "conv1d_161 (Conv1D)          (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_16 (Flatten)         (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 16)                14352     \n",
      "=================================================================\n",
      "Total params: 470,736\n",
      "Trainable params: 470,736\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.1829 - acc: 0.9630\n",
      "Loss: 0.18293105663304596 Accuracy: 0.9630322\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_9_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_162 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "conv1d_163 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_164 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_165 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_166 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_167 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_168 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_169 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_66 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_170 (Conv1D)          (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "conv1d_171 (Conv1D)          (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_67 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_172 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "conv1d_173 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_68 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_174 (Conv1D)          (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "conv1d_175 (Conv1D)          (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_69 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_176 (Conv1D)          (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "conv1d_177 (Conv1D)          (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_70 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_178 (Conv1D)          (None, 7, 256)            98560     \n",
      "_________________________________________________________________\n",
      "conv1d_179 (Conv1D)          (None, 7, 256)            196864    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_71 (MaxPooling (None, 2, 256)            0         \n",
      "_________________________________________________________________\n",
      "flatten_17 (Flatten)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 16)                8208      \n",
      "=================================================================\n",
      "Total params: 760,016\n",
      "Trainable params: 760,016\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.2276 - acc: 0.9350\n",
      "Loss: 0.2276410343955239 Accuracy: 0.9349948\n"
     ]
    }
   ],
   "source": [
    "log_dir = 'log'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "base = '1D_CNN_custom_conv_3_VGG_DO'\n",
    "\n",
    "with open(path.join(log_dir, base), 'w') as log_file:\n",
    "    for i in range(1, 10):\n",
    "        model_name = base+'_{}_conv'.format(i)\n",
    "        print()\n",
    "        print(model_name, 'Model')\n",
    "        model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "        model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "\n",
    "        model = load_model(model_filename)\n",
    "        model.summary()\n",
    "\n",
    "        [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "        print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "        del(model)\n",
    "\n",
    "        log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.3996 - acc: 0.2123\n",
      "Epoch 00001: val_loss improved from inf to 1.69751, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_6_conv_checkpoint/001-1.6975.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 2.3996 - acc: 0.2123 - val_loss: 1.6975 - val_acc: 0.4577\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5830 - acc: 0.4828\n",
      "Epoch 00002: val_loss improved from 1.69751 to 1.44006, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_6_conv_checkpoint/002-1.4401.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 1.5829 - acc: 0.4828 - val_loss: 1.4401 - val_acc: 0.5607\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3668 - acc: 0.5561\n",
      "Epoch 00003: val_loss improved from 1.44006 to 1.20253, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_6_conv_checkpoint/003-1.2025.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 1.3668 - acc: 0.5561 - val_loss: 1.2025 - val_acc: 0.6364\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2562 - acc: 0.5941\n",
      "Epoch 00004: val_loss improved from 1.20253 to 1.13797, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_6_conv_checkpoint/004-1.1380.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 1.2562 - acc: 0.5941 - val_loss: 1.1380 - val_acc: 0.6557\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1539 - acc: 0.6330\n",
      "Epoch 00005: val_loss improved from 1.13797 to 1.07953, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_6_conv_checkpoint/005-1.0795.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 1.1539 - acc: 0.6330 - val_loss: 1.0795 - val_acc: 0.6723\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0287 - acc: 0.6819\n",
      "Epoch 00006: val_loss improved from 1.07953 to 0.93325, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_6_conv_checkpoint/006-0.9333.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 1.0287 - acc: 0.6818 - val_loss: 0.9333 - val_acc: 0.7272\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9118 - acc: 0.7223\n",
      "Epoch 00007: val_loss improved from 0.93325 to 0.79259, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_6_conv_checkpoint/007-0.7926.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.9118 - acc: 0.7223 - val_loss: 0.7926 - val_acc: 0.7675\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8061 - acc: 0.7578\n",
      "Epoch 00008: val_loss improved from 0.79259 to 0.72012, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_6_conv_checkpoint/008-0.7201.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.8062 - acc: 0.7578 - val_loss: 0.7201 - val_acc: 0.7915\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7279 - acc: 0.7808\n",
      "Epoch 00009: val_loss improved from 0.72012 to 0.61174, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_6_conv_checkpoint/009-0.6117.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.7279 - acc: 0.7808 - val_loss: 0.6117 - val_acc: 0.8253\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6575 - acc: 0.8048\n",
      "Epoch 00010: val_loss improved from 0.61174 to 0.58965, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_6_conv_checkpoint/010-0.5897.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.6574 - acc: 0.8048 - val_loss: 0.5897 - val_acc: 0.8309\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6028 - acc: 0.8213\n",
      "Epoch 00011: val_loss improved from 0.58965 to 0.55835, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_6_conv_checkpoint/011-0.5583.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.6028 - acc: 0.8213 - val_loss: 0.5583 - val_acc: 0.8430\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5541 - acc: 0.8352\n",
      "Epoch 00012: val_loss improved from 0.55835 to 0.49925, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_6_conv_checkpoint/012-0.4993.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.5540 - acc: 0.8352 - val_loss: 0.4993 - val_acc: 0.8581\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5098 - acc: 0.8461\n",
      "Epoch 00013: val_loss did not improve from 0.49925\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.5098 - acc: 0.8461 - val_loss: 0.5606 - val_acc: 0.8477\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4796 - acc: 0.8552\n",
      "Epoch 00014: val_loss improved from 0.49925 to 0.45987, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_6_conv_checkpoint/014-0.4599.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.4795 - acc: 0.8552 - val_loss: 0.4599 - val_acc: 0.8798\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4506 - acc: 0.8668\n",
      "Epoch 00015: val_loss improved from 0.45987 to 0.45160, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_6_conv_checkpoint/015-0.4516.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.4506 - acc: 0.8668 - val_loss: 0.4516 - val_acc: 0.8763\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4205 - acc: 0.8757\n",
      "Epoch 00016: val_loss did not improve from 0.45160\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.4205 - acc: 0.8757 - val_loss: 0.4955 - val_acc: 0.8682\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3987 - acc: 0.8797\n",
      "Epoch 00017: val_loss improved from 0.45160 to 0.41648, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_6_conv_checkpoint/017-0.4165.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.3988 - acc: 0.8797 - val_loss: 0.4165 - val_acc: 0.8833\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3725 - acc: 0.8874\n",
      "Epoch 00018: val_loss improved from 0.41648 to 0.40004, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_6_conv_checkpoint/018-0.4000.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.3725 - acc: 0.8874 - val_loss: 0.4000 - val_acc: 0.8903\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3489 - acc: 0.8949\n",
      "Epoch 00019: val_loss did not improve from 0.40004\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.3489 - acc: 0.8949 - val_loss: 0.4105 - val_acc: 0.8903\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3297 - acc: 0.8989\n",
      "Epoch 00020: val_loss improved from 0.40004 to 0.39175, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_6_conv_checkpoint/020-0.3917.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.3297 - acc: 0.8989 - val_loss: 0.3917 - val_acc: 0.8919\n",
      "Epoch 21/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3164 - acc: 0.9015\n",
      "Epoch 00021: val_loss did not improve from 0.39175\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.3164 - acc: 0.9016 - val_loss: 0.3933 - val_acc: 0.8924\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2963 - acc: 0.9091\n",
      "Epoch 00022: val_loss improved from 0.39175 to 0.38238, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_6_conv_checkpoint/022-0.3824.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2964 - acc: 0.9091 - val_loss: 0.3824 - val_acc: 0.8968\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2844 - acc: 0.9132\n",
      "Epoch 00023: val_loss improved from 0.38238 to 0.35920, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_6_conv_checkpoint/023-0.3592.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2845 - acc: 0.9131 - val_loss: 0.3592 - val_acc: 0.9012\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2761 - acc: 0.9136\n",
      "Epoch 00024: val_loss did not improve from 0.35920\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2761 - acc: 0.9136 - val_loss: 0.3673 - val_acc: 0.9045\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2539 - acc: 0.9207\n",
      "Epoch 00025: val_loss did not improve from 0.35920\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2543 - acc: 0.9207 - val_loss: 0.3753 - val_acc: 0.9017\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2432 - acc: 0.9256\n",
      "Epoch 00026: val_loss did not improve from 0.35920\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2432 - acc: 0.9256 - val_loss: 0.3664 - val_acc: 0.9068\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2422 - acc: 0.9250\n",
      "Epoch 00027: val_loss did not improve from 0.35920\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2422 - acc: 0.9250 - val_loss: 0.3645 - val_acc: 0.9087\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2256 - acc: 0.9302\n",
      "Epoch 00028: val_loss did not improve from 0.35920\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2256 - acc: 0.9302 - val_loss: 0.3656 - val_acc: 0.9094\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2089 - acc: 0.9337\n",
      "Epoch 00029: val_loss improved from 0.35920 to 0.35613, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_6_conv_checkpoint/029-0.3561.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2089 - acc: 0.9337 - val_loss: 0.3561 - val_acc: 0.9085\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2138 - acc: 0.9336\n",
      "Epoch 00030: val_loss did not improve from 0.35613\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2138 - acc: 0.9336 - val_loss: 0.3951 - val_acc: 0.9015\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1956 - acc: 0.9388\n",
      "Epoch 00031: val_loss did not improve from 0.35613\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1957 - acc: 0.9387 - val_loss: 0.3674 - val_acc: 0.9092\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1899 - acc: 0.9399\n",
      "Epoch 00032: val_loss improved from 0.35613 to 0.32931, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_6_conv_checkpoint/032-0.3293.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1899 - acc: 0.9399 - val_loss: 0.3293 - val_acc: 0.9119\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1780 - acc: 0.9424\n",
      "Epoch 00033: val_loss did not improve from 0.32931\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1780 - acc: 0.9424 - val_loss: 0.3893 - val_acc: 0.9015\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1759 - acc: 0.9431\n",
      "Epoch 00034: val_loss did not improve from 0.32931\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1759 - acc: 0.9431 - val_loss: 0.3818 - val_acc: 0.9075\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1653 - acc: 0.9466\n",
      "Epoch 00035: val_loss did not improve from 0.32931\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1653 - acc: 0.9466 - val_loss: 0.3990 - val_acc: 0.9108\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1598 - acc: 0.9485\n",
      "Epoch 00036: val_loss did not improve from 0.32931\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1597 - acc: 0.9485 - val_loss: 0.3551 - val_acc: 0.9194\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1565 - acc: 0.9496\n",
      "Epoch 00037: val_loss did not improve from 0.32931\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1564 - acc: 0.9497 - val_loss: 0.3425 - val_acc: 0.9199\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1487 - acc: 0.9503\n",
      "Epoch 00038: val_loss did not improve from 0.32931\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1487 - acc: 0.9503 - val_loss: 0.3389 - val_acc: 0.9231\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1452 - acc: 0.9515\n",
      "Epoch 00039: val_loss did not improve from 0.32931\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1452 - acc: 0.9515 - val_loss: 0.3657 - val_acc: 0.9201\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1435 - acc: 0.9539\n",
      "Epoch 00040: val_loss did not improve from 0.32931\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1436 - acc: 0.9539 - val_loss: 0.3526 - val_acc: 0.9206\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1501 - acc: 0.9505\n",
      "Epoch 00041: val_loss did not improve from 0.32931\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1502 - acc: 0.9505 - val_loss: 0.3614 - val_acc: 0.9180\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1285 - acc: 0.9583\n",
      "Epoch 00042: val_loss did not improve from 0.32931\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1285 - acc: 0.9583 - val_loss: 0.3546 - val_acc: 0.9208\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1271 - acc: 0.9570\n",
      "Epoch 00043: val_loss did not improve from 0.32931\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1270 - acc: 0.9570 - val_loss: 0.3713 - val_acc: 0.9166\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1247 - acc: 0.9590\n",
      "Epoch 00044: val_loss did not improve from 0.32931\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1247 - acc: 0.9590 - val_loss: 0.3549 - val_acc: 0.9220\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1196 - acc: 0.9610\n",
      "Epoch 00045: val_loss did not improve from 0.32931\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1196 - acc: 0.9610 - val_loss: 0.3694 - val_acc: 0.9182\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1151 - acc: 0.9626\n",
      "Epoch 00046: val_loss did not improve from 0.32931\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1152 - acc: 0.9625 - val_loss: 0.3931 - val_acc: 0.9180\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1152 - acc: 0.9624\n",
      "Epoch 00047: val_loss did not improve from 0.32931\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1152 - acc: 0.9624 - val_loss: 0.3614 - val_acc: 0.9245\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1095 - acc: 0.9635\n",
      "Epoch 00048: val_loss did not improve from 0.32931\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1095 - acc: 0.9635 - val_loss: 0.3696 - val_acc: 0.9234\n",
      "Epoch 49/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1053 - acc: 0.9657\n",
      "Epoch 00049: val_loss did not improve from 0.32931\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1053 - acc: 0.9657 - val_loss: 0.3819 - val_acc: 0.9203\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1035 - acc: 0.9657\n",
      "Epoch 00050: val_loss did not improve from 0.32931\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1035 - acc: 0.9657 - val_loss: 0.3793 - val_acc: 0.9208\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1029 - acc: 0.9657\n",
      "Epoch 00051: val_loss did not improve from 0.32931\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1029 - acc: 0.9657 - val_loss: 0.3977 - val_acc: 0.9208\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1012 - acc: 0.9675\n",
      "Epoch 00052: val_loss did not improve from 0.32931\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1012 - acc: 0.9675 - val_loss: 0.3697 - val_acc: 0.9231\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0980 - acc: 0.9686\n",
      "Epoch 00053: val_loss did not improve from 0.32931\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0980 - acc: 0.9686 - val_loss: 0.3840 - val_acc: 0.9210\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0986 - acc: 0.9679\n",
      "Epoch 00054: val_loss did not improve from 0.32931\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0986 - acc: 0.9679 - val_loss: 0.3872 - val_acc: 0.9192\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0964 - acc: 0.9684\n",
      "Epoch 00055: val_loss did not improve from 0.32931\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0964 - acc: 0.9684 - val_loss: 0.3507 - val_acc: 0.9271\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0910 - acc: 0.9709\n",
      "Epoch 00056: val_loss did not improve from 0.32931\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0910 - acc: 0.9709 - val_loss: 0.3682 - val_acc: 0.9213\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0951 - acc: 0.9698\n",
      "Epoch 00057: val_loss did not improve from 0.32931\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0951 - acc: 0.9698 - val_loss: 0.3687 - val_acc: 0.9262\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0875 - acc: 0.9714\n",
      "Epoch 00058: val_loss did not improve from 0.32931\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.0875 - acc: 0.9714 - val_loss: 0.3969 - val_acc: 0.9250\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0864 - acc: 0.9718\n",
      "Epoch 00059: val_loss did not improve from 0.32931\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0863 - acc: 0.9718 - val_loss: 0.3809 - val_acc: 0.9271\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0871 - acc: 0.9727\n",
      "Epoch 00060: val_loss did not improve from 0.32931\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0871 - acc: 0.9727 - val_loss: 0.3937 - val_acc: 0.9234\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0858 - acc: 0.9718\n",
      "Epoch 00061: val_loss did not improve from 0.32931\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.0858 - acc: 0.9719 - val_loss: 0.4060 - val_acc: 0.9229\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0852 - acc: 0.9723\n",
      "Epoch 00062: val_loss did not improve from 0.32931\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0852 - acc: 0.9723 - val_loss: 0.4232 - val_acc: 0.9255\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0828 - acc: 0.9740\n",
      "Epoch 00063: val_loss did not improve from 0.32931\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0829 - acc: 0.9740 - val_loss: 0.4247 - val_acc: 0.9192\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0781 - acc: 0.9748\n",
      "Epoch 00064: val_loss did not improve from 0.32931\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.0782 - acc: 0.9748 - val_loss: 0.3992 - val_acc: 0.9192\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0835 - acc: 0.9731\n",
      "Epoch 00065: val_loss did not improve from 0.32931\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0835 - acc: 0.9731 - val_loss: 0.3711 - val_acc: 0.9264\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0781 - acc: 0.9742\n",
      "Epoch 00066: val_loss did not improve from 0.32931\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.0781 - acc: 0.9742 - val_loss: 0.3954 - val_acc: 0.9257\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0724 - acc: 0.9764\n",
      "Epoch 00067: val_loss did not improve from 0.32931\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.0725 - acc: 0.9764 - val_loss: 0.4209 - val_acc: 0.9147\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0819 - acc: 0.9735\n",
      "Epoch 00068: val_loss did not improve from 0.32931\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.0819 - acc: 0.9735 - val_loss: 0.4168 - val_acc: 0.9283\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0687 - acc: 0.9778\n",
      "Epoch 00069: val_loss did not improve from 0.32931\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.0687 - acc: 0.9778 - val_loss: 0.4041 - val_acc: 0.9234\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0711 - acc: 0.9764\n",
      "Epoch 00070: val_loss did not improve from 0.32931\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.0710 - acc: 0.9764 - val_loss: 0.3760 - val_acc: 0.9262\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0712 - acc: 0.9767\n",
      "Epoch 00071: val_loss did not improve from 0.32931\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.0712 - acc: 0.9767 - val_loss: 0.4054 - val_acc: 0.9264\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0705 - acc: 0.9765\n",
      "Epoch 00072: val_loss did not improve from 0.32931\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.0705 - acc: 0.9766 - val_loss: 0.3944 - val_acc: 0.9255\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0674 - acc: 0.9790\n",
      "Epoch 00073: val_loss did not improve from 0.32931\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.0674 - acc: 0.9791 - val_loss: 0.3889 - val_acc: 0.9248\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0681 - acc: 0.9781\n",
      "Epoch 00074: val_loss did not improve from 0.32931\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.0680 - acc: 0.9781 - val_loss: 0.3967 - val_acc: 0.9250\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0649 - acc: 0.9790\n",
      "Epoch 00075: val_loss did not improve from 0.32931\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.0649 - acc: 0.9790 - val_loss: 0.4340 - val_acc: 0.9199\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0638 - acc: 0.9803\n",
      "Epoch 00076: val_loss did not improve from 0.32931\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.0638 - acc: 0.9803 - val_loss: 0.3777 - val_acc: 0.9278\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0670 - acc: 0.9786\n",
      "Epoch 00077: val_loss did not improve from 0.32931\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.0670 - acc: 0.9786 - val_loss: 0.4312 - val_acc: 0.9166\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0671 - acc: 0.9785\n",
      "Epoch 00078: val_loss did not improve from 0.32931\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.0671 - acc: 0.9785 - val_loss: 0.3799 - val_acc: 0.9276\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0592 - acc: 0.9810\n",
      "Epoch 00079: val_loss did not improve from 0.32931\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.0596 - acc: 0.9809 - val_loss: 0.3883 - val_acc: 0.9317\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0638 - acc: 0.9798\n",
      "Epoch 00080: val_loss did not improve from 0.32931\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.0638 - acc: 0.9798 - val_loss: 0.3781 - val_acc: 0.9292\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0592 - acc: 0.9810\n",
      "Epoch 00081: val_loss did not improve from 0.32931\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.0592 - acc: 0.9810 - val_loss: 0.4343 - val_acc: 0.9259\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0554 - acc: 0.9829\n",
      "Epoch 00082: val_loss did not improve from 0.32931\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.0554 - acc: 0.9829 - val_loss: 0.4470 - val_acc: 0.9255\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_6_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzs3Xd8VFX6+PHPmZ5JJwlJSICA9CJBilixrK4NrIi97K6ubdW1/GRd27bvuuquvXxxZS2romt39SsrSlEXlWKQItIJIaT3NvX8/jiTAoQQIJMJmef9et1Xkjt37n3uJDnPOeeee67SWiOEEEIAWCIdgBBCiJ5DkoIQQogWkhSEEEK0kKQghBCihSQFIYQQLSQpCCGEaCFJQQghRIuwJQWlVH+l1AKl1Fql1Bql1C3tbHOCUqpaKZUXWu4LVzxCCCH2zRbGffuB27XWK5RS8cBypdSnWuu1u233hdb6rDDGIYQQopPClhS01juBnaHva5VSPwBZwO5JYb+kpqbqnJycgw9QCCGiyPLly8u01mn72i6cLYUWSqkcYDzwTTsvH6WUWgkUAndordd0tK+cnByWLVvW5TEKIURvppTa1pntwp4UlFJxwNvArVrrmt1eXgEM1FrXKaXOAN4Dhrazj2uBawEGDBgQ5oiFECJ6hXX0kVLKjkkIr2qt39n9da11jda6LvT9x4BdKZXaznaztdYTtdYT09L22foRQghxgMI5+kgBLwA/aK3/tpdtMkLboZSaHIqnPFwxCSGE6Fg4u4+OAS4HViml8kLr7gYGAGitnwMuAK5XSvmBRuAifQBzeft8PgoKCmhqauqayKOQy+UiOzsbu90e6VCEEBEUztFHXwJqH9s8BTx1sMcqKCggPj6enJwcQg0PsR+01pSXl1NQUMCgQYMiHY4QIoJ6xR3NTU1NpKSkSEI4QEopUlJSpKUlhOgdSQGQhHCQ5PMTQkAvSgr7Egg04vHsIBj0RToUIYTosaImKQSDTXi9O9G665NCVVUVzzzzzAG994wzzqCqqqrT2z/wwAM88sgjB3QsIYTYl6hJCkpZAdA60OX77igp+P3+Dt/78ccfk5SU1OUxCSHEgYi6pADBLt/3rFmz2LRpE7m5udx5550sXLiQ4447junTpzNq1CgAzjnnHCZMmMDo0aOZPXt2y3tzcnIoKytj69atjBw5kmuuuYbRo0dz6qmn0tjY2OFx8/LymDJlCocffjjnnnsulZWVADzxxBOMGjWKww8/nIsuugiARYsWkZubS25uLuPHj6e2trbLPwchxKGvW+Y+6k4bNtxKXV1eO68ECQTqsVhiUGr/TjsuLpehQx/b6+sPPvggq1evJi/PHHfhwoWsWLGC1atXtwzxnDNnDn369KGxsZFJkyZx/vnnk5KSslvsG3j99dd5/vnnufDCC3n77be57LLL9nrcK664gieffJKpU6dy33338bvf/Y7HHnuMBx98kC1btuB0Olu6ph555BGefvppjjnmGOrq6nC5XPv1GQghokPUtBRab5nY73vjDsjkyZN3GfP/xBNPMG7cOKZMmcL27dvZsGHDHu8ZNGgQubm5AEyYMIGtW7fudf/V1dVUVVUxdepUAK688koWL14MwOGHH86ll17KP//5T2w2kwCPOeYYbrvtNp544gmqqqpa1gshRFu9rmTYW41e6wB1dd/hcGTjdGaEPY7Y2NiW7xcuXMj8+fNZsmQJbrebE044od17ApxOZ8v3Vqt1n91He/PRRx+xePFiPvzwQ/70pz+xatUqZs2axZlnnsnHH3/MMcccw7x58xgxYsQB7V8I0XtFUUuh+VS7/kJzfHx8h3301dXVJCcn43a7WbduHV9//fVBHzMxMZHk5GS++OILAF555RWmTp1KMBhk+/btnHjiifzlL3+hurqauro6Nm3axNixY7nrrruYNGkS69atO+gYhBC9T69rKeyNuTnLGpbRRykpKRxzzDGMGTOG008/nTPPPHOX10877TSee+45Ro4cyfDhw5kyZUqXHPell17iuuuuo6GhgcGDB/OPf/yDQCDAZZddRnV1NVprbr75ZpKSkrj33ntZsGABFouF0aNHc/rpp3dJDEKI3kUdwPxzETVx4kS9+0N2fvjhB0aOHLnP99bVfY/VmkBMTE6Yoju0dfZzFEIcepRSy7XWE/e1XRR1HzUPS+36loIQQvQWUZUUwBKW7iMhhOgtoiopKBWeawpCCNFbRF1SkO4jIYTYu6hKCuEafSSEEL1FVCUF6T4SQoiORVlSsABBesIw3Li4uP1aL4QQ3SHKkkL4ZkoVQojeIKqSAoTnmQqzZs3i6aefbvm5+UE4dXV1nHzyyRxxxBGMHTuW999/v9P71Fpz5513MmbMGMaOHcsbb7wBwM6dOzn++OPJzc1lzJgxfPHFFwQCAa666qqWbR999NEuPT8hRPTofdNc3Hor5LU3dTbYtB9LsBFliQW1H/kwNxce2/vU2TNnzuTWW2/lxhtvBODNN99k3rx5uFwu3n33XRISEigrK2PKlClMnz69U89Dfuedd8jLy2PlypWUlZUxadIkjj/+eF577TV++tOf8tvf/pZAIEBDQwN5eXns2LGD1atXA+zXk9yEEKKt3pcUOtBaFHftNYXx48dTUlJCYWEhpaWlJCcn079/f3w+H3fffTeLFy/GYrGwY8cOiouLycjY9yytX375JRdffDFWq5X09HSmTp3K0qVLmTRpEj/72c/w+Xycc8455ObmMnjwYDZv3syvfvUrzjzzTE499dQuPT8hRPTofUmhgxp9wF9HY+M6YmKGYrMldulhZ8yYwVtvvUVRUREzZ84E4NVXX6W0tJTly5djt9vJyclpd8rs/XH88cezePFiPvroI6666ipuu+02rrjiClauXMm8efN47rnnePPNN5kzZ05XnJYQIspE1TUFFeoyCsew1JkzZzJ37lzeeustZsyYAZgps/v27YvdbmfBggVs27at0/s77rjjeOONNwgEApSWlrJ48WImT57Mtm3bSE9P55prruEXv/gFK1asoKysjGAwyPnnn88f//hHVqxY0eXnJ4SIDr2vpdCB5tFHWnf96KPRo0dTW1tLVlYWmZmZAFx66aVMmzaNsWPHMnHixP16qM25557LkiVLGDduHEopHnroITIyMnjppZd4+OGHsdvtxMXF8fLLL7Njxw6uvvpqgkFzXn/+85+7/PyEENEhqqbODgb91Nfn4XT2x+FID1eIhyyZOluI3kumzm5Ha0tB7moWQoj2RFlSUMj02UIIsXdRlRSgubUgdzQLIUR7oi4pyEypQgixd1GXFJSS7iMhhNibKEwK8qAdIYTYm7AlBaVUf6XUAqXUWqXUGqXULe1so5RSTyilNiqlvldKHRGueFqP2fXdR1VVVTzzzDMH9N4zzjhD5ioSQvQY4Wwp+IHbtdajgCnAjUqpUbttczowNLRcCzwbxnhCujcp+P3+Dt/78ccfk5SU1KXxCCHEgQpbUtBa79Rarwh9Xwv8AGTtttnZwMva+BpIUkplhismaG4pdO3oo1mzZrFp0yZyc3O58847WbhwIccddxzTp09n1CiTB8855xwmTJjA6NGjmT17dst7c3JyKCsrY+vWrYwcOZJrrrmG0aNHc+qpp9LY2LjHsT788EOOPPJIxo8fz09+8hOKi4sBqKur4+qrr2bs2LEcfvjhvP322wB88sknHHHEEYwbN46TTz65S89bCNH7dMs0F0qpHGA88M1uL2UB29v8XBBat/NAj9XBzNkABIPpaJ2M1appO29qR/YxczYPPvggq1evJi904IULF7JixQpWr17NoEGDAJgzZw59+vShsbGRSZMmcf7555OSkrLLfjZs2MDrr7/O888/z4UXXsjbb7/NZZddtss2xx57LF9//TVKKf7+97/z0EMP8de//pU//OEPJCYmsmrVKgAqKyspLS3lmmuuYfHixQwaNIiKiopOna8QInqFPSkopeKAt4FbtdY1B7iPazHdSwwYMKALowufyZMntyQEgCeeeIJ3330XgO3bt7Nhw4Y9ksKgQYPIzc0FYMKECWzdunWP/RYUFDBz5kx27tyJ1+ttOcb8+fOZO3duy3bJycl8+OGHHH/88S3b9OnTp0vPUQjR+4Q1KSil7JiE8KrW+p12NtkB9G/zc3Zo3S601rOB2WDmPuromB3V6AG83mo8nnxiY8dhsdg73vggxMbGtny/cOFC5s+fz5IlS3C73ZxwwgntTqHtdDpbvrdare12H/3qV7/itttuY/r06SxcuJAHHnggLPELIaJTOEcfKeAF4Aet9d/2stkHwBWhUUhTgGqt9QF3HXUurq6f/yg+Pp7a2tq9vl5dXU1ycjJut5t169bx9ddfH/Cxqqurycoyl2ZeeumllvWnnHLKLo8EraysZMqUKSxevJgtW7YASPeREGKfwjn66BjgcuAkpVReaDlDKXWdUuq60DYfA5uBjcDzwA1hjCfEGvradUkhJSWFY445hjFjxnDnnXfu8fppp52G3+9n5MiRzJo1iylTphzwsR544AFmzJjBhAkTSE1NbVl/zz33UFlZyZgxYxg3bhwLFiwgLS2N2bNnc9555zFu3LiWh/8IIcTeRNXU2QB+fy2NjT8SEzMMmy0hHCEesmTqbCF6L5k6ey/C+aAdIYQ41EVdUmg9ZZnqQgghdhd1SUEetCOEEHsnSUEIIUSLqEsK5i5mhXQfCSHEnqIuKZjbJ+RBO0II0Z6oSwoQnumz91dcXFxEjy+EEO2J0qRgkSGpQgjRjihNCl379LVZs2btMsXEAw88wCOPPEJdXR0nn3wyRxxxBGPHjuX999/f5772NsV2e1Ng7226bCGEOFDdMnV2d7r1k1vJK+pg7mwgGGxEa43V6u7UPnMzcnnstL3PtDdz5kxuvfVWbrzxRgDefPNN5s2bh8vl4t133yUhIYGysjKmTJnC9OnTQ9c12tfeFNvBYLDdKbDbmy5bCCEORq9LCp2jgK7rPho/fjwlJSUUFhZSWlpKcnIy/fv3x+fzcffdd7N48WIsFgs7duyguLiYjIyMve6rvSm2S0tL250Cu73psoUQ4mD0uqTQUY2+WVPTNvz+SuLicrvsuDNmzOCtt96iqKioZeK5V199ldLSUpYvX47dbicnJ6fdKbObdXaKbSGECJeovKYQjiGpM2fOZO7cubz11lvMmDEDMNNc9+3bF7vdzoIFC9i2bVuH+9jbFNt7mwK7vemyhRDiYERlUlDKAuguHYE0evRoamtrycrKIjPTPGb60ksvZdmyZYwdO5aXX36ZESNGdLiPvU2xvbcpsNubLlsIIQ5G1E2dDeD1FuPxbCc2NheLpdf1oB0wmTpbiN5Lps7uUNc/aEcIIXqDqEwKMimeEEK0r9ckhf3pBpOksKdDrRtRCBEevSIpuFwuysvLO12wNScF6T4ytNaUl5fjcrkiHYoQIsJ6xVXW7OxsCgoKKC0t3ftGgQA0NYHbTVD78XrLsNvBao3tvkB7MJfLRXZ2dqTDEEJEWK9ICna7veVu3716802YORO++w7PyL4sWTKOYcOeo1+/X3ZPkEIIcQjoFd1HnTJmjPm6ejVWawIAfn9NBAMSQoieJ3qSwtCh4HCEkkIsoAgEJCkIIURb0ZMU7HYYMQJWrUIphdWaIC0FIYTYTfQkBTBdSKtXA2CzJRAI1EY4ICGE6FmiKymMHQv5+VBTIy0FIYRoR3QlhTYXm01LQZKCEEK0FbVJQVoKQgixp+hKCgMGQFxcqKUQLy0FIYTYTXQlBYvFtBZWrZKWghBCtCO6kgK0JAWnox9ebxGBQEOkIxJCiB4jOpNCeTkJTUOBAHV1eZGOSAgheozoSwpjxwIQv9UBQG3t0khGI4QQPUrYkoJSao5SqkQptXovr5+glKpWSuWFlvvCFcsuQiOQHOuLcDj6UVMjSUEIIZqFs6XwInDaPrb5QmudG1p+H8ZYWvXtC2lpsHo18fGTpKUghBBthC0paK0XAxXh2v9BGTsWVq0iIWESjY3r8furIx2REEL0CJG+pnCUUmqlUur/lFKju+2oY8bAmjXEx04AoLZ2ebcdWggherJIJoUVwECt9TjgSeC9vW2olLpWKbVMKbWsw6erddaYMVBfT3xFX0AuNgshRLOIJQWtdY3Wui70/ceAXSmVupdtZ2utJ2qtJ6alpR38wUMjkOzrCnC5BsvFZiGECIlYUlBKZSilVOj7yaFYyrvl4KNGma8tF5uXdcthhRCipwvnkNTXgSXAcKVUgVLq50qp65RS14U2uQBYrZRaCTwBXKS11uGKZxcJCTBwIKxeTULCJDyebXi9XdAtJYQQhzhbuHastb54H68/BTwVruPvU2gEUny8yVG1tUtJSTkjYuEIIURPEOnRR5EzdiysW0ecfRSg5GKzEEIQzUkhNxf8fmzrt+N2j5TrCkIIQbQnBYC8POLjJ1FTs5TuuqQhhBA9VfQmhcMOg9hYyMsjIWESPl8xHk9BpKMSQoiIit6kYLXC4Ye3tBRAbmITQojoTQpgupDy8oh1j0UpmyQFIUTUk6RQU4N1exHx8ROpqPhErisIIaKaJAWAvDzS06+kri5PWgtCiKgW3UlhzBiwWEJJ4RIsllgKC/830lEJIUTERHdScLth+HDIy8NmSyA9/RJKSubK8xWEEFErupMCtFxsBujX75cEgw0UF/8zwkEJIURkdCopKKVuUUolKOMFpdQKpdSp4Q6uW+TmQn4+VFQQHz+BuLgJFBY+JxechRBRqbMthZ9prWuAU4Fk4HLgwbBF1Z2aLzavXAmY1kJ9/WpqapZEMCghhIiMziYFFfp6BvCK1npNm3WHtnHjzNdQF1LfvhdjtcbLBWchRFTqbFJYrpT6DyYpzFNKxQPB8IXVjdLTITOzJSnYbHGkp19Gaemb+HyVEQ5OCCG6V2eTws+BWcAkrXUDYAeuDltU3a3NxWZovuDcRFHRi5GLSQghIqCzSeEo4EetdZVS6jLgHqD3jNvMzYW1a8HjASAubhyJiceyY8eTaB2IcHBCCNF9OpsUngUalFLjgNuBTcDLYYuqu4WercDatS2rsrJuoalpC+Xl/45gYEII0b06mxT8oecnnw08pbV+GogPX1jdrM10F81SU8/B6RxAQcHjEQpKCCG6X2eTQq1S6jeYoagfKaUsmOsKvUPzsxVWrGhZZbHYyMq6iaqqBdTVfR/B4IQQovt0NinMBDyY+xWKgGzg4bBF1d2sVjjxRHjpJdi2rWV1ZuYvsFjc0loQQkSNTiWFUCJ4FUhUSp0FNGmte881BYAnnwSt4aqrIGhG29rtyWRkXElx8at4vaWRjU8IIbpBZ6e5uBD4FpgBXAh8o5S6IJyBdbucHHj8cVi40HwNycq6Ga09cjObECIqdLb76LeYexSu1FpfAUwG7g1fWBFy9dUwfTr85jewZg0AsbEj6NPnNAoLnyYY9EY4QCGECK/OJgWL1rqkzc/l+/HeQ4dSMHs2JCTA5ZeD1ySBrKxf4fUWyfBUIUSv19mC/ROl1Dyl1FVKqauAj4CPwxdWBKWnm8Tw3Xdwr2kMJSefit2eTnHxqxEOTgghwquzF5rvBGYDh4eW2Vrru8IZWESdcw5cdx089BB88AEWi42+fS+ivPzf+HxVkY5OCCHCptNdQFrrt7XWt4WWd8MZVI/w6KNwxBFw5ZWwZQvp6ZeitZeysrcjHZkQQoRNh0lBKVWrlKppZ6lVStV0V5AR4XLBv/5lhqnOmEG8YywxMUOlC0kI0at1mBS01vFa64R2lnitdUJ3BRkxgwebG9qWL0fddhvp6ZdSVbUQj2dHpCMTQoiw6H0jiLra2WfDHXfAs8+SUXA4oCkufj3SUQkhRFhIUuiM3/wGrFZcnywnPn4yxcX/jHREQggRFpIUOqNPHzjuOPjgA9LTL6O+fiX19WsiHZUQQnQ5SQqdNX06rFpF3/opgFUuOAsheqWwJQWl1BylVIlSavVeXldKqSeUUhuVUt8rpY4IVyxdYvp0ABzzltCnzykUF7+K1r3jMdVCCNEsnC2FF4HTOnj9dGBoaLkW83S3nuuww2DUqFAX0hV4PPlUVS2MdFRCCNGlwpYUtNaLgYoONjkbeFkbXwNJSqnMcMXTJaZPh0WLSLWdgNWayM6dcyIdkRBCdKlIXlPIAra3+bkgtG4PSqlrlVLLlFLLSksj+FyD6dPB78f66SLS0y+hrOxt/P7qyMUjhIgKWkNjo1nCzRb+Qxw8rfVszNxLTJw4UUcskMmToW9f+OADMs74NYWFz1JSMpd+/X4ZsZCECLdAAPx+sNvBYtl1fWMjNDWZ74NBswQC4POZSYZ9PrPObm9dlGrdPhAw729oaC30fL7WRSlwu80SE2N+9nrB4zFf6+qgttYsTU1mguPkZLNYLFBUBDt3mq8+HzidZrICp9PsqzlmHSpVlGqNr7oaamrM0nz+Dof52nyOzYvHY5amJrO/tjE3n7NS5hj19a37rasDm83s1+EwD4Fs3pfXaz6PhgazaG1Gx//P/4T39x3JpLAD6N/m5+zQup7LaoWzzoK33ybe9SKxsWPYufMfkhREpwUCplCorzcFQn39rgViU5PZrrkAgdZCwuMx29TVtS6BQOt2zQVb89dAoLVAaT6G19u6gCmwbDazNBfSzQVeTY0pGOvqWo/RXID5/a37OBQkJZlk0NTUWniDSRwWi/m8tW5drFZITDRJJiHBnHdzAvB6zettE53TabZLSzP7ay7MKyvNZ9X2dxMba7bNzDTfBwKtv5NAwOzL4WhNYLGxZnG74eijw/9ZRTIpfADcpJSaCxwJVGutd0Ywns6ZPh3mzEF9+SUZQ69m06bbqa9fQ2zs6EhHJvahuWbbtpBsWyvz+VprxX7/noVxTY2pkdbUmEIiJQVSU02Bk58Pq1aZZeNGU2g0/1NbLK2JoLkwOlhOJ8TFmTjaak4mSpnjut2tBYrTCfHx5qvdbrbz+815+/1me6vVLA5Ha4GYmGi2by64PB7zc0yM2a/LZeJoLmAtltYatcNhYmlbq4bWY1ks5v3NNWuXa9fCtrnbpPl3Bq0FpsNhPoO4OHNeLpf53VRWthbGGRlmcbm65nOPBmFLCkqp14ETgFSlVAFwP2AH0Fo/h3kewxnARqABuDpcsXSpn/zE/IV98AHpD93N5s13sXPnPxgy5JFIR9ZrNTVBQQFs3w6lpbsW5HV1rU3xmhqzrm2hXl5u3lNWZl4/WDabKYACgT33l50NY8fCSSeZn5vjDARMwRwX11rray7MYmNN4dq8NHdrQGvt0ulsXVwu877mQl3sqk8fs4gDp7SOXBf9gZg4caJetmxZZIOYNg1Wr4bNm1m95jyqq//LUUcVYLHIf2pbHg/s2GEKZYvFFKhWq6nx5ee3LiUlptAuKzOFeHOXSHP/cVlZx8exWExttrkW3Nwd4nC01ubT0kw/c3Otubm/t22ts21XisWicblUS2EcE2NqzW0Lba8XKirMkplp9t+VGn2NlDaUYrfYcdqcOKwO3HY3FrX/40Oa/89V236pdjT4GlhTsoYdtTvon9CfwcmDSY4xJ+YNeNlZu5OCmgJ8QR9OqxOnzYndYqfJ30Stt5Y6bx3egJeUmBT6xvYlLTaNRGciVosVi7JgURY8fg/Vnmqqm6qp99WTGZdJ39i+HcZW0VjBurJ11HnrsFls2Cw2HFYHo9JGkeDcdW5Ob8DLoq2L2Fy5mX7x/chOyCY7IRubxUZVUxVVTVVUe6rxBXxoNMHQ/UZ2ix271b7L1+ZjAWg0Wms0mjR3Gqnu1JaYtdYU1hayqmQV1U3VHDPgGLITsneJa2ftTr7M/5KtVVspqiuiqL6I8oZyYuwxJDoTSXQmkuRKok9MH1LcKaTEpBDriMWqrC2fX/P5HAil1HKt9cR9bXdIXGjucaZPh3//G77/noysn1FW9h4VFR+Tmnp2pCMLO48HiotNLbn5YqHfb2rxP/xglh9/bC3s9yU+Hvr1M4X3wIEwfnxrtwGYJJKdDf37w4ABpnB3ucDh0ASsDaQmxhAXa2EfZR0ANZ4aVhatZMXOFSws+o7yxnICDQEC9QECwQBVTVVUNFZQ3lhOraeWeGc8ya5k+sT0IdYRS1AHCQQDBHQAp9VJoiuRBGcCCY4EmtY3tRQ49d56UtwpZMZlkhGXgcvmIr86n23V28ivzscf9JPmTmspNJ1WJxZlQaHwB/1srtrM+vL1bK/ejmbXSptCkehKpE9MH5JdyVgt1pa4grvdTOkP+qn11lLjqaHWU4tGE+eIa1li7bHEOmJx293YLXbWl69nY8XGPY6Z6Ewkxh5DcV3xHq91lThHHEP6DGFg4sCWQhhMMlhbupbi+uJ232dRFsZnjGfqwKkMTRnK51s+55ONn1DrrQ1LnG25bC76J/QnxZ3C+vL1VDTuOgL/sOTDODHnRDSaRdsWsbFiY8trMbYYMuMzSYlJobG2keqmamo8NdR4ajr8jO865i4e/MmDYTsnkJbCgSkpMR2V995L8P57WbIkm4SESYwd+2Fk4zpIzSMuqqtNjX3dOlizxiwbNpgRHFU1PogrhtgSiCkHdznEVEDQBk1JpMUnkdMvjqSsEmwp29Hx2/HYd9IYrKMxUI8n0ICfJqx2PxabnyB+lFI4rI6WGnFOUg4jUkYwInUEGXEZrC9fz+qS1awuXc3Wqq0tha8/6CfRmcjR/Y/muAHHcWT2kdR569hUsYlNlZvYVr2NsoYyU9A3lFPeWN5yrhlxGfSL74dVmRqY1WIl0WkK25SYFOKd8dR6aqlsqqSisYJ6X/0u23r8Hmo8NVR7zD9zjC2GJFcSSa4k3HY35Y3lpjZYV4Q34CUzLpOBSQMZmDgQu9VOaX0ppQ2llNSX4A140drUWK0WKzlJOQztM5RhKcPoF98Pf9CPx+/BE/BQ562jsrGSiqYKKhsrCejALjVJRWt2tFqsxDviiXfEk+BMQClFvbeeOm8dtd5a6n311HvrqffV4/F7GNJnCGP7juXw9MPJTsimoKaAzZWb2Vy5GU/AQ/+E/mQnZJOVkIXT6sQT8OANePEGvMTYYlqSjc1io7yxvOUcazw1LUkroAO4bC7WAuCyAAAgAElEQVQSnSahuu1uCmsL2VixkY2VG9levX2X5BbvjGdk6khGpY1iZOpIkmOS8Qf9+IN+6r31LC1cyqJti/im4Bs8AQ8ZcRlMGzaNacOmMS5jHEV1RRTUFFBQU0BQB1t+R4nOROxW+y6fmT/oxxvw4gv68Aa8BIIB/EE/vqC5EKJQWJQFjaakvoTt1dvJr8mnrKGMIclDGJtuPrtYeyxf5H/Bwq0LWbRtEQrFcQOPY+rAqRw/8HiGpwwnzhHXbsuouXJS3lhOeUM5Db4GAjrQkvgHJw9mZNrIA/r/7mxLQZLCgTr+eKiqgu+/Z/Pme8jP/x+OPHIzMTE5kY5sD0EdZHv1dn4s2s6WrZqtW2DLVigs8lFe3UhFXQNVdY00eX2ggmAJgMUU/iphJ7HphViTduJ17qTJUrpftUWn1UlmfCbxjviWWqnL5mppmlstVrTWLf+Mjb5GNlduZlv1tl32E2OLYXTf0RyWfBh9YvqQ5EoiwZnA1qqtfJH/BWtL1+6yfYIzgUFJg0iLTSMlJoU+MX3Iis9ifOZ4xmeMJzO+e+6T1FrjD/qxW6VrMZya/E3kV+czpM+QA+peC5fmBNcTYpLuo3A791y47TbYtIl+Wb8kP/9BCguf5bDD/hK2Q5Y3lLNo2yISnYmkulNJdaeSHpfe2uepTf/25oJ63v/hQz7f8SGbatZQptcTtO5214sbGNzx8SxYyIjLIDM+k8z4/vSLO5LM+Ez6xfejb2xfUmJSSHGbAre5hlPVVEWtt5ZUdyoDEgeQ5k7bZz92e+q99awvX09RXRHDUoYxKHlQh/9Y5Q3lLN+5nCRXUkviOJDjdjWllCSEbuCyuRiWMizSYeyhJySD/SUthQO1dSsMGgQPPwx33MHq1RdQVbWAo44qwGqN6fLDzd88nyvevYKddbuO2rXhINk/GorHUbVhFL7UZTD8Q7A3Ql06aucE+ugRDI4fwaisgQzOsTIwx1wYddpsxNhjcNvdxNhisFvtLV0kNouNPjF9sFqsXX4uQojuJ91H3WH8eDOM5auvqKxcyMqVJzJ8+BwyMw9sdO368vV88OMHpLpTOXbAsRyWfBi+oI97Pr+Hh//7MIcljOT8mKdY+Z2NpWtLqWgqheRNWLJWYs38Hp+zmDiVxpTECzhz4ExOGXYsQ4dYcTi6+LyFEIcc6T7qDuedB/ffD0VFJKVPxe0ezY4dT5KRcVWnui601myo2MA7P7zDG2veIK8ob5fXk+0Z4Imn0rIBx/e/ZNOHf+Mhn5uUFDj5JDj5ZPPsn+HDzSid8oZyEl2Ju4zeEEKI/SGlx8E491y47z54/33UL39JVtZNbNhwPTU1X5OYeNQem3v8HvKr88kryuPTzZ/yn03/abmgOjnrSG4a8je8K8/ni29rWVf/JZX9v0SlbmBw4duckn0eE5+BSZPMDVKWdroqU9wp4T5jIUQvJ91HB0NrGDYMBg+GefPw++tYsiSLlJSzGDXKPJntq/yvuGfBPWwo30BhbWHLyJ0EZwInDjyJLM+plHx1Ogvfy6GszNxEddRRMHWqWaZMMT1UQghxMKT7qDsoZVoLjz4KVVXYkpLIyLiawsJnaGp6hKdWvMqs+bPISsji1MNOJScph5ykHFTFMFb8eyKvP2ajuNjcjXvmmXDOOXDaaeaGLiGEiARJCgfr3HPNCKSPPoJLLyUr6wbWbX2cs149ns8KNnL+yPN5YfoLJLoSWboUZt0Gn39u7to96yy44go44wzkYrAQokc49AbR9jRHHmnGd77yCkEd5N2NS7k2z82iHRt55OT7+NeMf1GyPZGZM83jGL7/Hh55BAoL4Z13TOtAEoIQoqeQlsLBsljQt97KvOfvYtbDQ1jZuIXc9DHcP2IDQz0lXH+94oUXzERq990Ht99uJlYTQoieSJLCQar31nNp/y95/zIYVJLPaxfN4YTsK/nt3d9y8z9zgSDXXmvh3nvNdElCCNGTSVI4CGUNZZz52pksK1zGQ0Ou55Yrn+XjzRaGz7fQ0HAkZ5wxl2uueZWzzvoAdQje7i6EiD5SUh2gbVXbOHbOsXxf/D3vXPgOt130DH8Y8wHnvnslwwc0sHatYvbsAPHxH1Fc/M9IhyuEEJ0iSeEArClZw9Fzjqa4vphPL/+U49PPZto0+OPKaVzteJUvEqcxbEiQ9PRLiI+fzObNs/D76/a9YyGEiDBJCvuptL6UM187k6AO8sXVX9BfH8tRR8H8+fDMM/DCMx5c//0cXn8dpSwMGfIYXu9Otm69P9KhCyHEPklS2A++gI8Z/5pBcX0xH178IZSM4eijzZPI5s+H668HdfVVMGoUPPYYAImJR5GZ+UsKCh6jtnZFZE9ACCH2QZLCfrht3m0s2raI56c9T9PmiRx3nFm/eLF55g5gJiW64QZYtgyWLgVg8OAHcTj68uOP1xAM+iMTvBBCdIIkhU56YcULPLX0KW4/6nZSCy/jlFOgb1/46iszQd0uLr/cPCH+2WcBsNuTGDLkCerqVrBjxxPdH7wQQnSSJIV9WFm0ktvm3cb1H13PKYNPYbr7Qc47D0aOhC+/hJycdt6UkACXXgpz50JlJQBpaReQknIWW7bcS2Pj1u48BSGE6DRJCu2oaqri8a8fJ/e5XHL/N5envn2Ks0eczf2j53L2NBvZ2fDJJ5CW1sFOrr8eGhvhpZcA81jGoUOfBhTr11/HoTY7rRAiOkhSaGN9+Xpu+vgmsv+Wza3zbsVutfPk6U9SeHshj0z+FxdO60NMDPznP6brqEO5uWYO7GefNVNsAy7XAA477C9UVs5j27Y/hv+EhBBiP8kdzSE3fHQDzy57FofVwcVjLubmI2/miMwjACgqgp/+FBoazEXldruM2nP99WYa1M8/N49JA/r1u4Gamq/ZuvU+YmPHkpZ2TnhOSAghDoC0FIBPNn7Cs8ue5ZojriH/1nxePOfFloTwww/mQTfbt8MHH7RzUbkjM2ZASkrLBWcw3UjDhs0mPn4S69ZdTl3dqi4+GyGEOHBRnxS8AS+3fHILQ/sM5cnTnyQ9Lr3ltUWL4OijoanJfN88BLXTXC64+mp47z144w0IBgGwWmMYM+ZdrNZ4Vq8+G6+3rAvPSAghDlzUJ4XHv36c9eXrefy0x3HanC3rX3sNTjnFPCrh669h4j4fYrcXt95qHtl50UWmmTF3LgQCOJ1ZjBnzLh5PIWvWnE8g0NQ1JySEEAchqpNCYW0hv1/8e6YNm8bpQ08HzDXhP/zBjCg9+mhzH0KnryG0JysLVq2C1183P198MYwfD999R0LCkYwY8Q+qqxfzww+XoHXgoM9JCCEORlQnhVnzZ+ENePnbT/8GgMcDV15pHoZz+eUwbx4kJ3fBgaxW01JYtcq0FMrKzBPb/vIX0lMvZMiQxykre5f166+XoapCiIiK2qTw3+3/5ZXvX+GOo+5gSJ8hlJeb7qJXXjEthZdeMk9L61IWC8ycaZLD9OkwaxacdBLZwXMYMOC37Nz5PFu23Nu6vdcL//43+GVqDCFE94japPDAwgfIjMvk7uPuxu+HU0+Fb7811xLuuQeUCuPBU1LgX/+CF1+EFStg6lQGJd9OZuY15Of/ie3b/2q2mzULpk2D2bPDGIwQQrSKyqSwtnQtn27+lJsm30SsI5annjJl8yuvmC7/bqGU6av69FMoKED9/OcMHfI0aWkz2LTpDkpe/Dk8+ijYbGbG1dDIJSGECKewJgWl1GlKqR+VUhuVUrPaef0qpVSpUiovtPwinPE0e+KbJ3DZXFw74VoKC801hNNPhwsu6I6j72bKFPjLX+Ddd7E8+TQjR75GJtNJ+vUcvCMyTCthwwb4+OMIBCeEiDZhSwpKKSvwNHA6MAq4WCk1qp1N39Ba54aWv4crnmYVjRW8vPJlLh17KanuVG6/3XTdP/lkmLuMOvLrX8PZZ8Odd2L5ZinD/liPrclK3l1FbDuuELKzTatBCCHCLJwthcnARq31Zq21F5gLnB3G43XKCyteoNHfyC1H3sJnn5nBQL/5DRx2WASDUgr+8Q9T+J90Euqzz+Cxp4ibfDFbCu6heGaamSojLy+CQQohokE4k0IWsL3NzwWhdbs7Xyn1vVLqLaVU//Z2pJS6Vim1TCm1rLS09IAD8gf9PLX0KU7MOZFhSWO58UaTDO6664B32XWSk+HNN821gwsuwHLtLxkx4mUGDryfDVNXEnApmv5y597fn59vhrnKRWkhxEGI9IXmD4EcrfXhwKfAS+1tpLWerbWeqLWemNbhfNUde3/d++RX53PzkTfz7LPw44/w1FNmNooeYdIk2LrVNF+UwmKxMWjQA4w74VvKpiXjeHs+G7+8DL+/etf3bd0KU6ea4VN33GGeDyqEEAcgnElhB9C25p8dWtdCa12utfaEfvw7MCGM8fD4N48zKGkQ04ZNY+FC86Cc004L5xEPQGamudmtjfj4CaT+YTHKD7bZr/Ltt6MpL//IvLhpk0kI1dVmPG1jo7lyLoQQByCcU2cvBYYqpQZhksFFwCVtN1BKZWqtd4Z+nA78EK5gvtv5HV/kf8FfT/0rVouVtWvNIw8OFdbho2HadAb+ax7JP1ZRPegsPKOPJHP2dlRDE3z2mZk+45tvzFXzm27azyldhRA9ntZhHxETtpaC1toP3ATMwxT2b2qt1yilfq+Umh7a7Gal1Bql1ErgZuCqcMVT1VTFpH6T+Nn4n9HUZCrYo9obC9WTPfEE6pJLSfAPo/+7Nvrd9w3+up1UvPVbkxDAtBISE+H221se7iOEOET5fOYhLr/5janFdscoRK31IbVMmDBBH6y8PK1B6zfeOOhdRY7Xq+u/fVevmH+4XrAAvWbNJdrrLTevPfaYOcGPPopsjEL0dt98o/WFF2q9ZUv7r3/+udYLFhzYvn//e60TEsz/ss2m9dSpB1VoAct0J8pYpQ+x2uTEiRP1smXLDmofr78Ol1xipiAaM6aLAouQYNBHfv6f2bbtD9jtqQwe/BB9k87HcniuuTaxfDm43ZEOU4hd7dgBL79spnspLzfXwpqazKiPO++Em2/ev8nHFi40E5YlJEB6OmRkmKGFRx0FDkd4zmHuXPO8lKYm01L/6iuIiWl9/YsvWp64yKefmmt/nfXKK+apjdOmwVVXmf0kJh5UuEqp5VrrfT8EoDOZoyctXdFSuOcera1WrZuaDnpXPUZNzXd66dLxesEC9JIlh+nyF28yNYz4eK1//nOtFy3S2uPReulSrR99VOsLLtD6ssu0/uILrYPBSIcvokFFhdZz52p95plaWyzm7/PYY7W+6iqtr79e61//WuvTTjPrDztM6/fe2/ff5qZNWp93nnlPUlJrzbp5iYvT+uyztX7uOa1LStrfR0OD1v/3f1pv29a58wgGtb7//tb4X3zRfH/55a3xbt6sdWqq1sOGaT1ypIlt7drO7X/5cq1dLq1POEFrn69z7+kEOtlSiHghv79LVySF887TesSIg95NjxMMBnRJybt66dIJesEC9Kpn++qGi6bqYFyc+VVbra3/LAMHmj9U0HrcOK1nz9b6q6+0nj9f6w8/1Prdd80fdiQTRnGx1v/7v1rX1h74PoJBrZcs0foXv9D6oosObl/h5vWa5H2wBUFFhUn8N964926NrrJ1q9b/7/9p/c9/al1Xt+trfr/57O+/X+spU1oTQWam1r/5jdYbNrS/z08+MQUpaP2Tn2i9fv2e21RVaX3XXVo7HFrHxmr9xz+awl1r83XLFpNUfvlLrQcMaE0as2drHQi07uerr7QePrz1/yInR+srr9T62We1njdP6x9/1LqxUevycvO7eeoprU8/3Wx71VWtNcsHHjDrnnxS6+pqrUePNsf78UcTS9++Zt9FRR1/nqWl5n8zO9v8/XchSQodGD7cJIbeKhgM6rKyj/XSpUfoBQvQ3311tG78+5+0vuMOU1Pbvt1sWFdn/knGjdu1dtV26d/ftCheesm0NLrLDz+Yf6LmGN5/v+PtN2wwhc3gwVpPm2YKnT/9SetRo8w+3G6TFI85Ruuamu45h84KBLR+7TVTOwatjz7aJOSOVFaaz+SNN0whumSJKbR+9jOtY2LMfux2c94PP9yaaIJBrdes0frPf9b62mu1Pv98rU880Rzz4487H7PPp/Xf/mb23/y3Ehtr/laee07rSy/VOiXFrFdK68mTtb73Xq2//LJzSc/nMwVsYqLWTqcp9D0eUwg/+mjrvq+4QusdOzreVzCo9XffmT755tr9smVa33qriW3gQK1ff91cizv33NZ9721JTjafadsKUyBg/u5sNq2PPNL8rc2f3/r6t9+az2rSJJNAb79d65NOMsc+4wzzt7pwoUmCTqfZvotJUtiLpibz+7rnnoPazSEhGPTrHTtm6y++SNELFlj0jz9erz2edmoqwaD5I/zkE1OwfPutWZ56SusZM0wtp7l1MWfOnv/URUVa/+c/Wj/yiPknnTJF65kzzT/O55+b2o/X2/nAFy82/3h9+5rjjRljjn/OOVrn5++5fVOT1kccYd5z4YWmlmazmfdMmaL188+bRPCvf5n1Rx1laprhFgiYi4wvvNB+14XXq/UHH2idm6tbWmx/+pPpAomP1/qVV1q3ragwhcbvfmcK8LatvraL2631NddovWKF+aymTzfrc3NNjb5trbhvX1MjP/ZYrYcMMQXk7oVd87G//trUqhcvNi3JCRPMPs4809SEFy82Saa59ZmaarpTXntN67KyA/8MCwvN3yCYBD9okG5pQSxfvn/7CgbN7yI5ufUzuOGGPSsJgYBpAS1apPXLL5sLvg89ZLqYCgr23nquqtJ66FCz32ee2fP19983nzGYgn/iRPP32twqal5eeGH/zquTJCnsxapV5qxfe+2gdnNI8Xor9Pr1N+kFC6x60aJYvXnzPdrn249CMRg0CaO5IBg+3NSyTjmlNWE0L/36mRpZc5O97WK3m0IjLc3UAGNiTOE2YIC5xvHww1o/8YTpEhg+vLW27PVq/eCDZvvk5D1Hc9x8s9l/29aEx9N+U/2dd0ximDzZ1Lb3ZeNGUwPs29ckupdfNkmuocEUlM88o/V115mujDlzTMG5erXpMmlu6TSPHpk+Xeu33zaF6lVXtRZOgwaZ2mNzt8aWLaagBhNn//6t+1HK1DZ/+1tTaK1aZWrfH31kzm33ZBcMmmP262c+65NP1vrpp/esXdfVtRa+l19uzu/zz7W+5BJTgO3+u0xPN62U3QvIpibTd962i6YrfPihKXCPOMJ06xyM4mLz+zrQUUEd2brVVD72Zt068zvbvZJUVmbOsaP3HqTOJoWoG3305pvm4Wd5eTBuXBcGdghoaFjPli33Ulr6JjZbH/r3v5N+/a7Fbu/TuR1oDe+9Z+6F2LABRo+Gww83H+S4ceZmudTU1u1LS83op3XroK7OLPX1Zuy102kWu93cNPLtt7Bli3nfscea46Sk7Hr8jRvNE+s2boTnnzfPo3jvPTj3XDPT7N/+1rnzeP99mDHDjEo56SRzW/tpp8GgQa03BjU2minNH3zQxHj66bBoEZSUmG0sFgiEnqmdlNR6Xs2Ugp/8xIwcGTHCDHn75z+hqMi8nphozuX8882+dx8h4/eb47/zDgwf3voZT5y462fcWR6PWRIS9r6N1vCnP8G995oRaw0N5twuuwx++lMTo9VqnvExfnzH+xI9TmdHH0VdUnjgAfO4zfr6HjTnUTerrV3Bli2/paLiEyyWGNLTLyMr62bi4vZjfG4waArGrlRSYpLNxIl7H45YVWUefPHZZ/CrX5mhe0OGmOGA+zP08JtvzHv/7/9g82azzmYzBW5qKlRWmmGTF18MjzwC/fqZc16+3LzH54MjjoAJE6B/f5MgtmwxE2oVFZlCtP9u8zv6/bBggdnPiSeGb6jkwXr/fTPc8qyz4Lzzdh1mKQ5ZkhT2YsYMWLkS1q/vwqAOUXV1q9ix40mKi18hGGwiLm4CqanTSEk5i7i4I1ARe8DEPvh8cP318MILprb63XcwePCB7Utr0/KYPx+2bzetm7Iyc4zbbzeFtxC9gCSFvRg9GoYONb0OwvD5ytm58x+Ulb1DTc3XgMbh6Edm5i/IyroRh6NvpEPck9ampj94sOluEkJ0SJJCO3w+01V6553wP//TxYH1El5vKRUV/0dJyZtUVHyEUk4yMi4nK+sWYmNH99zWgxCiQ51NCuGcJbXH2bjRdOsechPhdSOHI42MjCvIyLiChoYf2b79UYqLX2Lnzr/jcuWQnPwTkpNPISlpKg5HeqTDFUJ0sahKCmvWmK+SFDrH7R7O8OHPMWjQHygt/ReVlZ9SUvImO3eaR2k7HJnExY0jLi6X1NRzSUiYHOGIhRAHK6qSwtq1ZqTgiBGRjuTQ4nCkkZV1A1lZNxAM+qmtXUZNzdfU1eVRV5dHZeVn5Oc/SHLyqQwceC9JSdLHL8ShKuqSwqBBMmnowbBYbCQmTiExcUrLOr+/jsLCZ9m+/WHy8o4jKekE0tJmkpR0Am73cLkOIcQhJOqSgnQddT2bLY4BA+4kK+tGCgtnU1DwKBs2XA+A3Z5OQsJkrNY4lHJgsThwuQbSt+8lxMQMinDkQojdRU1S8PvNfUWnnx7pSHovq9VN//63kp19C42Nm6iqWkh19SLq6vIIBj0Eg1609uD1FrNlyz0kJh5PRsaVpKXNwGaLj3T4QgiiKCls2gRer7QUuoNSCrd7CG73EPr1+8Uerzc15VNc/ApFRS/x448/Z8OGX5GWdgEZGVeRlDQVpSxoHcDrLSIQaCQm5jDpghKim0RNUli71nyVpBB5LtcABg78LQMG3E1NzRKKil6ipGQuxcUv43BkoZQFj6cQMHMLxcWNp1+/6+jb9xJstrjIBi9ELxc1N69t2GCmdLn+eoiNDUNg4qAEAo2Ulb1HWdm7WCxunM5snM5stPayc+ffqa9fhdUaT0rKNGJihhITMwiXazCxsWOx25MiHb4QPZ7c0Sx6Da01NTVfU1j4LFVVC/B4dgDNf7cW4uMn0afPKSQlnYzL1R+rNQ6rNQ6LxS3dTkKESFIQvVYw6KGpKZ/Gxk3U1PyXysr51NR8S3N3UzOlHMTHTyIp6TgSE48jPn4ydnuKJAoRlSQpiKji91dTXf0VPl8ZgUAdgUAdXm8R1dX/pa5uOVr7AbBa43G5BuJy5eBwZGK3p2CzpWC3p+J2DyU2dgw2W2KEz0aIridzH4moYrMlkpJyRruvBQL11NR8S13ddzQ1baOpaStNTVuoqfkWn6+c3VsYTmd/3O5ROBwZ2O2p2O2p2GxJWK1uLJYYLJYY/P5KGhvX09CwnqamrSQlnUD//rf3zBllhdgP0lIQUU1rTSBQg9dbSkPDOurrV1Nfv5qGhnX4fKX4fGUEgw17ebc11OLIoKZmCRaLk8zMaxkw4E6czqxuPQ8h9kVaCkJ0glIKmy0Rmy0Rt3sIqaln7bFNINCA319FMNhIINBIMNiIzZaAyzUIi8U8Pa2hYT35+X9mx46n2LHj8dDd2zFYrTGhr/HYbAlYrfHY7ak4nf1xOvvjcvXH6czG4egXut7RxU+zE2I/SUtBiC7U2LiVkpLX8PtrCAYbCQabCAYb8PtrCQRqCARq8XpLQiOodr8wbsNu74tSrXU1i8UZugYyCJcrB6dzAA5HBk5nJg5HBlZrIhaL1O3EvklLQYgIiInJYeDAu/e5ndYBPJ6deDzb8XoL8XgK8XoL8XpLgGDLdoFAA01N2ygrex+fr6TdfSnlCF3vcKOUDaWsocWOxeIOtVbcoVZKn9DF9T4oZUPrABBA6yBWq7tlOK/NloTTOQCXayBWqzsUs8bvr8bnK8FmS8bhSOuKj0z0MJIUhIgApay4XNm4XNmdfk8gUB9KHjtDS1GoRdJAIFBPINAQGmUVQOsAwaC3TUulGo9nOz5fBX5/BVr7On1cuz0ViyUGr7cErT1t1qcRGzsGt3sUNlsiFosDpZyhlk6AYNDXMurLYnG2LEo5QknLhlK2UCJq7l5LxOnMxGrd9Q5Tv78WjycfpRy4XAOwWJwdxqx1EK39Ld17ovMkKQhxiLBaY3G7h+J2Dz2o/ZiL6/Vo7W9pWYAKXTOpJRCow+erwOPZ1jJaKxhswuHICI3I6ovPV0p9/Rrq61dTXPxPAoE6du8OOxgmOWRhsThoatqG31+5y+sOR2bL0OLmxWZLoq5uJTU131Jbu5Rg0ENy8kmkpJxJnz5n4nINDCXPWgKB2paE1XojZGuislic2O199pl8eiO5piCE6BKmdeJDa1+ocLWHEo5umSE3GGxCa3/LEgz6CAbrW665+P1VodbQDjyeHWjtC3VjDcDpHIDW3lCiah5avA2PJ7+l5aOUjdjYcSQkTEYpBxUVH9HYuDEUoYW2XXOdYbG4sdv7oJSjTVdbAKs1rqUbzmZLwmJxhVpKDiCIz1eJ31+Bz1dBMNhIa+IBpzOb2NjRuN2jiYkZQiBQjddbFJoAsh6HIxOnsx8OR1YoEadjtcYc9O9HrikIIbqVUlasVivg2v0VrFZXaH3X3xjYfH3G7y8nJmZ46FjNHqOhYT3l5R/j95djtZoRYGYaFDugdtmPWfwEg434/ZX4fOX4/RUEg96WVoRSllBrqhyvt5iGhnUEgx609qK1D631LtduLJaM0F30Cq0DNDVtpbLyM7T2tnM2VtprcZkk1JesrBvo3//2Lv4EdyVJQQhxSGu+PgPtX59xu4fhdg/r3qD2IRj009S0mcbGzdjtyaFuuXQsFkco2RTi8ewItSBK8PlK8HpLcDgywx5bWJOCUuo04HFM+vu71vrB3V53Ai8DE4ByYKbWems4YxJCiEizWGx7TVYORxoORxpxceMiEJnpZAsLZToTnwZOB0YBFyuldn+awc+BSq31EOBR4C/hikcIIcS+hfP2ycnARq31Zm06z+YCZ++2zdnAS6Hv324tJM8AAAc3SURBVAJOVjKFpRBCREw4k0IWsL3NzwWhde1uo834sGogJYwxCSGE6MAhMdGKUupapdQypdSy0tLSSIcjhBC9VjiTwg6gf5ufs0Pr2t1GmdsgEzEXnHehtZ6ttZ6otZ6Ylia31gshRLiEMyksBYYqpQYpc0fHRcAHu23zAXBl6PsLgM/1oXY3nRBC9CJhG5KqtfYrpW4C5mGGpM7RWq9RSv0eWKa1/gB4AXhFKbURqMAkDiGEEBES1vsUtNYfAx/vtu6+Nt83ATPCGYMQQojOO+TmPlJKlQLbDvDtqUBZF4bTVSSuzuuJMUHPjKsnxgQ9M66eGBN0bVwDtdb7vCh7yCWFg6GUWtaZCaG6m8TVeT0xJuiZcfXEmKBnxtUTY4LIxHVIDEkVQgjRPSQpCCGEaBFtSWF2pAPYC4mr83piTNAz4+qJMUHPjKsnxgQRiCuqrikIIYToWLS1FIQQQnQgapKCUuo0pdSPSqmNSqlZEYxjjlKqRCm1us26PkqpT5VSG0Jfk7s5pv5KqQVKqbVKqTVKqVt6SFwupdS3SqmVobh+F1o/SCn1Teh3+UbojvlupZSyKqW+U0r9uwfFtFUptUoplaeUWhZaF+nfYZJS6i2l1Dql1A9KqaN6QEzDQ59R81KjlLq1B8T169Df+Wql1Ouhv/9u/7uKiqTQyWc7dJcXgdN2WzcL+ExrPRT4LPRzd/IDt2utRwFTgBtDn0+k4/IAJ2mtxwG5wGlKqSmY5248GnoORyXmuRzd7Rb4/+3dW4hVVRzH8e8vLNExNMvEFDILMgobJeyihWkXkrAejCyTiKAXIXwqhm7Uc3R5iBKCsBIJSwt86KKFoJCmNplpdlNsQh0fsrIoRP89rDXb09FwENxr1/w+cHDvfY5n/mevdea/99qz158dLetNiAngxojobPkzxtJt+CLwfkRMBK4k7bOiMUXEzryPOkkFvv4AVpaMS9JY4GHgqoi4gjQLxDxK9KuI+N8/gGuBD1rWu4CugvGMB7a1rO8ExuTlMcDOwvvrPeDmJsUFDAW2AFeTbuYZdKK2rSmWcaRfGjOBVaRCv0Vjyj93N3Be27ZibUia4HIX+dplE2I6QYy3AOtLx8WxMgIjSTNNrAJuLdGvBsSZAv2r7VDS6IjYm5f3AaNLBSJpPDAZ2EAD4srDNN1AL/AR8D1wMFL9DSjTli8AjwBH8/q5DYgJIIAPJW2W9FDeVrINLwIOAK/lobZXJXUUjqndPGBZXi4WV0T8BDwL7AH2kmrLbKZAvxooSeE/I9IhQZE/CZM0DHgHWBQRvzYhrog4Euk0fxypmt/EumNoJel2oDciNpeM419Mj4gppGHShZJuaH2yQBsOAqYAL0fEZOB32oZkCvf3s4A5wPL25+qOK1+/uIOUSC8AOjh+mLkWAyUp9Ke2Q0n7JY0ByP/21h2ApDNJCWFpRKxoSlx9IuIg8AnpFHpErr8B9bflNGCOpN2kErMzSePmJWMCqqNNIqKXNEY+lbJt2AP0RMSGvP42KUk0pV/dBmyJiP15vWRcNwG7IuJARBwGVpD6Wu39aqAkhf7Udiipta7E/aQx/dpIEmka8x0R8VyD4holaUReHkK6zrGDlBzmlogrIroiYlxEjCf1o48jYn7JmAAkdUg6u2+ZNFa+jYJtGBH7gB8lXZo3zQK2l4ypzT0cGzqCsnHtAa6RNDR/H/v2Vf39qtQFnrofwGzgG9KY9GMF41hGGjM8TDqSepA0Jr0G+BZYDYysOabppFPlrUB3fsxuQFyTgM9zXNuAJ/P2CcBG4DvSqf/gQm05A1jVhJjyz/8iP77q6+MNaMNOYFNuw3eBc0rHlOPqIFV5HN6yrfS+ehr4Ovf1N4DBJfqV72g2M7PKQBk+MjOzfnBSMDOzipOCmZlVnBTMzKzipGBmZhUnBbMaSZrRN7OqWRM5KZiZWcVJwewEJN2Xazl0S1qcJ+Y7JOn5POf9Gkmj8ms7JX0qaauklX3z8Eu6RNLqXA9ii6SL89sPa6kxsDTfwWrWCE4KZm0kXQbcDUyLNBnfEWA+6S7YTRFxObAWeCr/l9eBRyNiEvBly/alwEuR6kFcR7qTHdIstItItT0mkOa4MWuEQSd/idmAM4tUfOWzfBA/hDQ52lHgrfyaN4EVkoYDIyJibd6+BFie5yEaGxErASLiT4D8fhsjoievd5Pqa6w7/R/L7OScFMyOJ2BJRHT9Y6P0RNvrTnWOmL9alo/g76E1iIePzI63Bpgr6Xyo6hxfSPq+9M1YeS+wLiJ+AX6WdH3evgBYGxG/AT2S7szvMVjS0Fo/hdkp8BGKWZuI2C7pcVIVszNIM9ouJBWJmZqf6yVdd4A0pfEr+Zf+D8ADefsCYLGkZ/J73FXjxzA7JZ4l1ayfJB2KiGGl4zA7nTx8ZGZmFZ8pmJlZxWcKZmZWcVIwM7OKk4KZmVWcFMzMrOKkYGZmFScFMzOr/A2/IMIZlYRVEAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 952us/sample - loss: 0.3998 - acc: 0.8955\n",
      "Loss: 0.39981899699689444 Accuracy: 0.8955348\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.4949 - acc: 0.1770\n",
      "Epoch 00001: val_loss improved from inf to 1.76621, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_7_conv_checkpoint/001-1.7662.hdf5\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 2.4948 - acc: 0.1770 - val_loss: 1.7662 - val_acc: 0.4160\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6554 - acc: 0.4478\n",
      "Epoch 00002: val_loss improved from 1.76621 to 1.29319, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_7_conv_checkpoint/002-1.2932.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 1.6553 - acc: 0.4479 - val_loss: 1.2932 - val_acc: 0.5649\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3200 - acc: 0.5600\n",
      "Epoch 00003: val_loss improved from 1.29319 to 1.05133, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_7_conv_checkpoint/003-1.0513.hdf5\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 1.3201 - acc: 0.5600 - val_loss: 1.0513 - val_acc: 0.6788\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0982 - acc: 0.6461\n",
      "Epoch 00004: val_loss improved from 1.05133 to 0.86514, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_7_conv_checkpoint/004-0.8651.hdf5\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 1.0983 - acc: 0.6461 - val_loss: 0.8651 - val_acc: 0.7372\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9346 - acc: 0.7017\n",
      "Epoch 00005: val_loss improved from 0.86514 to 0.68933, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_7_conv_checkpoint/005-0.6893.hdf5\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.9346 - acc: 0.7017 - val_loss: 0.6893 - val_acc: 0.7973\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7976 - acc: 0.7462\n",
      "Epoch 00006: val_loss improved from 0.68933 to 0.63205, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_7_conv_checkpoint/006-0.6321.hdf5\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.7976 - acc: 0.7463 - val_loss: 0.6321 - val_acc: 0.8074\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6939 - acc: 0.7792\n",
      "Epoch 00007: val_loss improved from 0.63205 to 0.51414, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_7_conv_checkpoint/007-0.5141.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.6939 - acc: 0.7792 - val_loss: 0.5141 - val_acc: 0.8484\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6227 - acc: 0.8037\n",
      "Epoch 00008: val_loss improved from 0.51414 to 0.45797, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_7_conv_checkpoint/008-0.4580.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.6228 - acc: 0.8037 - val_loss: 0.4580 - val_acc: 0.8642\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5727 - acc: 0.8207\n",
      "Epoch 00009: val_loss improved from 0.45797 to 0.41849, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_7_conv_checkpoint/009-0.4185.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.5727 - acc: 0.8207 - val_loss: 0.4185 - val_acc: 0.8742\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5103 - acc: 0.8410\n",
      "Epoch 00010: val_loss improved from 0.41849 to 0.38465, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_7_conv_checkpoint/010-0.3847.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.5102 - acc: 0.8410 - val_loss: 0.3847 - val_acc: 0.8875\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4677 - acc: 0.8551\n",
      "Epoch 00011: val_loss improved from 0.38465 to 0.35420, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_7_conv_checkpoint/011-0.3542.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.4677 - acc: 0.8551 - val_loss: 0.3542 - val_acc: 0.8889\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4381 - acc: 0.8636\n",
      "Epoch 00012: val_loss improved from 0.35420 to 0.31426, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_7_conv_checkpoint/012-0.3143.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.4381 - acc: 0.8636 - val_loss: 0.3143 - val_acc: 0.9080\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4043 - acc: 0.8728\n",
      "Epoch 00013: val_loss improved from 0.31426 to 0.30181, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_7_conv_checkpoint/013-0.3018.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.4043 - acc: 0.8728 - val_loss: 0.3018 - val_acc: 0.9133\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3723 - acc: 0.8824\n",
      "Epoch 00014: val_loss improved from 0.30181 to 0.27917, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_7_conv_checkpoint/014-0.2792.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.3723 - acc: 0.8825 - val_loss: 0.2792 - val_acc: 0.9215\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3564 - acc: 0.8882\n",
      "Epoch 00015: val_loss improved from 0.27917 to 0.26716, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_7_conv_checkpoint/015-0.2672.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.3564 - acc: 0.8882 - val_loss: 0.2672 - val_acc: 0.9257\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3324 - acc: 0.8945\n",
      "Epoch 00016: val_loss did not improve from 0.26716\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.3323 - acc: 0.8945 - val_loss: 0.2706 - val_acc: 0.9192\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3168 - acc: 0.9002\n",
      "Epoch 00017: val_loss did not improve from 0.26716\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.3170 - acc: 0.9002 - val_loss: 0.2824 - val_acc: 0.9185\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3043 - acc: 0.9048\n",
      "Epoch 00018: val_loss improved from 0.26716 to 0.25454, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_7_conv_checkpoint/018-0.2545.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.3042 - acc: 0.9048 - val_loss: 0.2545 - val_acc: 0.9236\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2807 - acc: 0.9120\n",
      "Epoch 00019: val_loss improved from 0.25454 to 0.24692, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_7_conv_checkpoint/019-0.2469.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2807 - acc: 0.9121 - val_loss: 0.2469 - val_acc: 0.9245\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2743 - acc: 0.9142\n",
      "Epoch 00020: val_loss did not improve from 0.24692\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2743 - acc: 0.9142 - val_loss: 0.2798 - val_acc: 0.9157\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2618 - acc: 0.9188\n",
      "Epoch 00021: val_loss did not improve from 0.24692\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2620 - acc: 0.9188 - val_loss: 0.2692 - val_acc: 0.9189\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2539 - acc: 0.9194\n",
      "Epoch 00022: val_loss improved from 0.24692 to 0.20793, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_7_conv_checkpoint/022-0.2079.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2539 - acc: 0.9194 - val_loss: 0.2079 - val_acc: 0.9397\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2347 - acc: 0.9263\n",
      "Epoch 00023: val_loss did not improve from 0.20793\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2346 - acc: 0.9263 - val_loss: 0.2136 - val_acc: 0.9453\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2283 - acc: 0.9289\n",
      "Epoch 00024: val_loss improved from 0.20793 to 0.19989, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_7_conv_checkpoint/024-0.1999.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2283 - acc: 0.9289 - val_loss: 0.1999 - val_acc: 0.9483\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2248 - acc: 0.9281\n",
      "Epoch 00025: val_loss improved from 0.19989 to 0.19974, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_7_conv_checkpoint/025-0.1997.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2248 - acc: 0.9281 - val_loss: 0.1997 - val_acc: 0.9450\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2107 - acc: 0.9321\n",
      "Epoch 00026: val_loss did not improve from 0.19974\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2107 - acc: 0.9321 - val_loss: 0.2013 - val_acc: 0.9441\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2035 - acc: 0.9362\n",
      "Epoch 00027: val_loss improved from 0.19974 to 0.19567, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_7_conv_checkpoint/027-0.1957.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2035 - acc: 0.9363 - val_loss: 0.1957 - val_acc: 0.9464\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1928 - acc: 0.9376\n",
      "Epoch 00028: val_loss improved from 0.19567 to 0.18282, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_7_conv_checkpoint/028-0.1828.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1928 - acc: 0.9376 - val_loss: 0.1828 - val_acc: 0.9474\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1866 - acc: 0.9395\n",
      "Epoch 00029: val_loss did not improve from 0.18282\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1866 - acc: 0.9395 - val_loss: 0.1850 - val_acc: 0.9497\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1786 - acc: 0.9430\n",
      "Epoch 00030: val_loss did not improve from 0.18282\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1786 - acc: 0.9430 - val_loss: 0.2005 - val_acc: 0.9411\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1706 - acc: 0.9451\n",
      "Epoch 00031: val_loss did not improve from 0.18282\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1706 - acc: 0.9451 - val_loss: 0.1880 - val_acc: 0.9471\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1719 - acc: 0.9434\n",
      "Epoch 00032: val_loss did not improve from 0.18282\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1719 - acc: 0.9434 - val_loss: 0.1876 - val_acc: 0.9492\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1600 - acc: 0.9471\n",
      "Epoch 00033: val_loss did not improve from 0.18282\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1600 - acc: 0.9472 - val_loss: 0.1912 - val_acc: 0.9457\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1564 - acc: 0.9485\n",
      "Epoch 00034: val_loss improved from 0.18282 to 0.18128, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_7_conv_checkpoint/034-0.1813.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1564 - acc: 0.9485 - val_loss: 0.1813 - val_acc: 0.9506\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1555 - acc: 0.9494\n",
      "Epoch 00035: val_loss did not improve from 0.18128\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1555 - acc: 0.9494 - val_loss: 0.2023 - val_acc: 0.9420\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1479 - acc: 0.9511\n",
      "Epoch 00036: val_loss improved from 0.18128 to 0.17758, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_7_conv_checkpoint/036-0.1776.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1479 - acc: 0.9511 - val_loss: 0.1776 - val_acc: 0.9548\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1443 - acc: 0.9525\n",
      "Epoch 00037: val_loss improved from 0.17758 to 0.16443, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_7_conv_checkpoint/037-0.1644.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1443 - acc: 0.9525 - val_loss: 0.1644 - val_acc: 0.9488\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1385 - acc: 0.9539\n",
      "Epoch 00038: val_loss did not improve from 0.16443\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1387 - acc: 0.9539 - val_loss: 0.1839 - val_acc: 0.9460\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1368 - acc: 0.9549\n",
      "Epoch 00039: val_loss did not improve from 0.16443\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1368 - acc: 0.9550 - val_loss: 0.1830 - val_acc: 0.9492\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1294 - acc: 0.9563\n",
      "Epoch 00040: val_loss did not improve from 0.16443\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1294 - acc: 0.9563 - val_loss: 0.1715 - val_acc: 0.9509\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1252 - acc: 0.9585\n",
      "Epoch 00041: val_loss did not improve from 0.16443\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1252 - acc: 0.9585 - val_loss: 0.1839 - val_acc: 0.9504\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1295 - acc: 0.9569\n",
      "Epoch 00042: val_loss did not improve from 0.16443\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1295 - acc: 0.9569 - val_loss: 0.1687 - val_acc: 0.9532\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1193 - acc: 0.9616\n",
      "Epoch 00043: val_loss did not improve from 0.16443\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1193 - acc: 0.9616 - val_loss: 0.1897 - val_acc: 0.9539\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1190 - acc: 0.9605\n",
      "Epoch 00044: val_loss improved from 0.16443 to 0.15809, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_7_conv_checkpoint/044-0.1581.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1190 - acc: 0.9604 - val_loss: 0.1581 - val_acc: 0.9562\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1172 - acc: 0.9606\n",
      "Epoch 00045: val_loss did not improve from 0.15809\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1172 - acc: 0.9606 - val_loss: 0.1706 - val_acc: 0.9553\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1094 - acc: 0.9636\n",
      "Epoch 00046: val_loss did not improve from 0.15809\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1094 - acc: 0.9636 - val_loss: 0.1886 - val_acc: 0.9506\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1068 - acc: 0.9635\n",
      "Epoch 00047: val_loss did not improve from 0.15809\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1068 - acc: 0.9635 - val_loss: 0.1973 - val_acc: 0.9522\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1048 - acc: 0.9650\n",
      "Epoch 00048: val_loss did not improve from 0.15809\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1048 - acc: 0.9650 - val_loss: 0.1859 - val_acc: 0.9562\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1058 - acc: 0.9654\n",
      "Epoch 00049: val_loss did not improve from 0.15809\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1058 - acc: 0.9654 - val_loss: 0.1704 - val_acc: 0.9557\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1030 - acc: 0.9657\n",
      "Epoch 00050: val_loss did not improve from 0.15809\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1029 - acc: 0.9657 - val_loss: 0.1766 - val_acc: 0.9557\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0969 - acc: 0.9671\n",
      "Epoch 00051: val_loss did not improve from 0.15809\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0969 - acc: 0.9671 - val_loss: 0.2147 - val_acc: 0.9443\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0956 - acc: 0.9677\n",
      "Epoch 00052: val_loss did not improve from 0.15809\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0956 - acc: 0.9677 - val_loss: 0.1616 - val_acc: 0.9569\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0976 - acc: 0.9672\n",
      "Epoch 00053: val_loss did not improve from 0.15809\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0976 - acc: 0.9672 - val_loss: 0.1650 - val_acc: 0.9592\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0915 - acc: 0.9690\n",
      "Epoch 00054: val_loss did not improve from 0.15809\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0915 - acc: 0.9690 - val_loss: 0.1741 - val_acc: 0.9518\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0876 - acc: 0.9704\n",
      "Epoch 00055: val_loss did not improve from 0.15809\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0878 - acc: 0.9704 - val_loss: 0.1924 - val_acc: 0.9504\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1061 - acc: 0.9647\n",
      "Epoch 00056: val_loss did not improve from 0.15809\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1061 - acc: 0.9647 - val_loss: 0.1780 - val_acc: 0.9620\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0840 - acc: 0.9717\n",
      "Epoch 00057: val_loss did not improve from 0.15809\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0840 - acc: 0.9717 - val_loss: 0.1746 - val_acc: 0.9597\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0799 - acc: 0.9720\n",
      "Epoch 00058: val_loss did not improve from 0.15809\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0799 - acc: 0.9720 - val_loss: 0.1803 - val_acc: 0.9574\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0819 - acc: 0.9719\n",
      "Epoch 00059: val_loss did not improve from 0.15809\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0819 - acc: 0.9719 - val_loss: 0.1951 - val_acc: 0.9532\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0793 - acc: 0.9721\n",
      "Epoch 00060: val_loss did not improve from 0.15809\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0792 - acc: 0.9721 - val_loss: 0.1961 - val_acc: 0.9553\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0771 - acc: 0.9739\n",
      "Epoch 00061: val_loss did not improve from 0.15809\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0771 - acc: 0.9739 - val_loss: 0.1850 - val_acc: 0.9574\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0816 - acc: 0.9728\n",
      "Epoch 00062: val_loss did not improve from 0.15809\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0816 - acc: 0.9728 - val_loss: 0.1712 - val_acc: 0.9546\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0768 - acc: 0.9753\n",
      "Epoch 00063: val_loss did not improve from 0.15809\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0768 - acc: 0.9753 - val_loss: 0.1674 - val_acc: 0.9578\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0739 - acc: 0.9750\n",
      "Epoch 00064: val_loss did not improve from 0.15809\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0738 - acc: 0.9750 - val_loss: 0.1830 - val_acc: 0.9590\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0737 - acc: 0.9758\n",
      "Epoch 00065: val_loss did not improve from 0.15809\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0737 - acc: 0.9758 - val_loss: 0.1776 - val_acc: 0.9546\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0684 - acc: 0.9774\n",
      "Epoch 00066: val_loss did not improve from 0.15809\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0684 - acc: 0.9774 - val_loss: 0.1677 - val_acc: 0.9609\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0730 - acc: 0.9758\n",
      "Epoch 00067: val_loss did not improve from 0.15809\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0730 - acc: 0.9758 - val_loss: 0.1848 - val_acc: 0.9604\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0703 - acc: 0.9760\n",
      "Epoch 00068: val_loss did not improve from 0.15809\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0702 - acc: 0.9760 - val_loss: 0.1906 - val_acc: 0.9578\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0654 - acc: 0.9779\n",
      "Epoch 00069: val_loss did not improve from 0.15809\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0654 - acc: 0.9779 - val_loss: 0.1857 - val_acc: 0.9578\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0698 - acc: 0.9768\n",
      "Epoch 00070: val_loss did not improve from 0.15809\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0698 - acc: 0.9769 - val_loss: 0.2026 - val_acc: 0.9602\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0668 - acc: 0.9771\n",
      "Epoch 00071: val_loss did not improve from 0.15809\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0669 - acc: 0.9771 - val_loss: 0.1968 - val_acc: 0.9562\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0626 - acc: 0.9785\n",
      "Epoch 00072: val_loss did not improve from 0.15809\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0626 - acc: 0.9785 - val_loss: 0.1811 - val_acc: 0.9574\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0626 - acc: 0.9793\n",
      "Epoch 00073: val_loss did not improve from 0.15809\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0626 - acc: 0.9793 - val_loss: 0.1861 - val_acc: 0.9553\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0628 - acc: 0.9785\n",
      "Epoch 00074: val_loss did not improve from 0.15809\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0628 - acc: 0.9785 - val_loss: 0.2053 - val_acc: 0.9599\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0609 - acc: 0.9796\n",
      "Epoch 00075: val_loss did not improve from 0.15809\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0609 - acc: 0.9796 - val_loss: 0.1824 - val_acc: 0.9634\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0574 - acc: 0.9806\n",
      "Epoch 00076: val_loss did not improve from 0.15809\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0574 - acc: 0.9806 - val_loss: 0.1902 - val_acc: 0.9585\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0576 - acc: 0.9807\n",
      "Epoch 00077: val_loss did not improve from 0.15809\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0576 - acc: 0.9807 - val_loss: 0.1867 - val_acc: 0.9595\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0542 - acc: 0.9818\n",
      "Epoch 00078: val_loss did not improve from 0.15809\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0542 - acc: 0.9818 - val_loss: 0.1847 - val_acc: 0.9581\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0574 - acc: 0.9799\n",
      "Epoch 00079: val_loss did not improve from 0.15809\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0574 - acc: 0.9799 - val_loss: 0.2067 - val_acc: 0.9585\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0584 - acc: 0.9800\n",
      "Epoch 00080: val_loss did not improve from 0.15809\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0584 - acc: 0.9800 - val_loss: 0.1900 - val_acc: 0.9592\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0631 - acc: 0.9794\n",
      "Epoch 00081: val_loss did not improve from 0.15809\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0631 - acc: 0.9794 - val_loss: 0.1873 - val_acc: 0.9599\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0522 - acc: 0.9821\n",
      "Epoch 00082: val_loss did not improve from 0.15809\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0522 - acc: 0.9821 - val_loss: 0.1843 - val_acc: 0.9616\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0561 - acc: 0.9809\n",
      "Epoch 00083: val_loss did not improve from 0.15809\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0561 - acc: 0.9809 - val_loss: 0.1727 - val_acc: 0.9620\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0488 - acc: 0.9837\n",
      "Epoch 00084: val_loss did not improve from 0.15809\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0488 - acc: 0.9837 - val_loss: 0.1753 - val_acc: 0.9599\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0519 - acc: 0.9822\n",
      "Epoch 00085: val_loss did not improve from 0.15809\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0519 - acc: 0.9822 - val_loss: 0.1848 - val_acc: 0.9618\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0497 - acc: 0.9821\n",
      "Epoch 00086: val_loss did not improve from 0.15809\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0497 - acc: 0.9821 - val_loss: 0.1644 - val_acc: 0.9646\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0520 - acc: 0.9827\n",
      "Epoch 00087: val_loss did not improve from 0.15809\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0520 - acc: 0.9827 - val_loss: 0.1857 - val_acc: 0.9616\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0476 - acc: 0.9838\n",
      "Epoch 00088: val_loss did not improve from 0.15809\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0476 - acc: 0.9838 - val_loss: 0.1875 - val_acc: 0.9588\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0453 - acc: 0.9845\n",
      "Epoch 00089: val_loss did not improve from 0.15809\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0453 - acc: 0.9845 - val_loss: 0.2102 - val_acc: 0.9562\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0473 - acc: 0.9842\n",
      "Epoch 00090: val_loss did not improve from 0.15809\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0472 - acc: 0.9842 - val_loss: 0.2047 - val_acc: 0.9557\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0490 - acc: 0.9829\n",
      "Epoch 00091: val_loss did not improve from 0.15809\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0490 - acc: 0.9829 - val_loss: 0.1991 - val_acc: 0.9606\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0472 - acc: 0.9846\n",
      "Epoch 00092: val_loss did not improve from 0.15809\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0472 - acc: 0.9846 - val_loss: 0.2134 - val_acc: 0.9581\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0437 - acc: 0.9846\n",
      "Epoch 00093: val_loss did not improve from 0.15809\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0437 - acc: 0.9846 - val_loss: 0.2033 - val_acc: 0.9613\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0444 - acc: 0.9846\n",
      "Epoch 00094: val_loss did not improve from 0.15809\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0445 - acc: 0.9846 - val_loss: 0.1799 - val_acc: 0.9555\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_7_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 978us/sample - loss: 0.1860 - acc: 0.9450\n",
      "Loss: 0.18600267374178206 Accuracy: 0.94496363\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.4622 - acc: 0.1899\n",
      "Epoch 00001: val_loss improved from inf to 1.81917, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_8_conv_checkpoint/001-1.8192.hdf5\n",
      "36805/36805 [==============================] - 96s 3ms/sample - loss: 2.4621 - acc: 0.1899 - val_loss: 1.8192 - val_acc: 0.4062\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5126 - acc: 0.5037\n",
      "Epoch 00002: val_loss improved from 1.81917 to 0.99189, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_8_conv_checkpoint/002-0.9919.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 1.5125 - acc: 0.5038 - val_loss: 0.9919 - val_acc: 0.6818\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0853 - acc: 0.6402\n",
      "Epoch 00003: val_loss improved from 0.99189 to 0.77011, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_8_conv_checkpoint/003-0.7701.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 1.0853 - acc: 0.6402 - val_loss: 0.7701 - val_acc: 0.7433\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8736 - acc: 0.7132\n",
      "Epoch 00004: val_loss improved from 0.77011 to 0.57402, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_8_conv_checkpoint/004-0.5740.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.8735 - acc: 0.7132 - val_loss: 0.5740 - val_acc: 0.8185\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7293 - acc: 0.7610\n",
      "Epoch 00005: val_loss improved from 0.57402 to 0.47546, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_8_conv_checkpoint/005-0.4755.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.7294 - acc: 0.7610 - val_loss: 0.4755 - val_acc: 0.8572\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6074 - acc: 0.8036\n",
      "Epoch 00006: val_loss improved from 0.47546 to 0.41551, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_8_conv_checkpoint/006-0.4155.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.6074 - acc: 0.8036 - val_loss: 0.4155 - val_acc: 0.8675\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5288 - acc: 0.8293\n",
      "Epoch 00007: val_loss improved from 0.41551 to 0.36490, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_8_conv_checkpoint/007-0.3649.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.5288 - acc: 0.8293 - val_loss: 0.3649 - val_acc: 0.8894\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4732 - acc: 0.8479\n",
      "Epoch 00008: val_loss improved from 0.36490 to 0.31699, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_8_conv_checkpoint/008-0.3170.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.4732 - acc: 0.8479 - val_loss: 0.3170 - val_acc: 0.9022\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4253 - acc: 0.8636\n",
      "Epoch 00009: val_loss improved from 0.31699 to 0.27523, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_8_conv_checkpoint/009-0.2752.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.4253 - acc: 0.8636 - val_loss: 0.2752 - val_acc: 0.9154\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3856 - acc: 0.8776\n",
      "Epoch 00010: val_loss improved from 0.27523 to 0.26075, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_8_conv_checkpoint/010-0.2607.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.3856 - acc: 0.8776 - val_loss: 0.2607 - val_acc: 0.9203\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3621 - acc: 0.8860\n",
      "Epoch 00011: val_loss improved from 0.26075 to 0.24575, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_8_conv_checkpoint/011-0.2457.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.3620 - acc: 0.8860 - val_loss: 0.2457 - val_acc: 0.9250\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3301 - acc: 0.8948\n",
      "Epoch 00012: val_loss improved from 0.24575 to 0.24446, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_8_conv_checkpoint/012-0.2445.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.3301 - acc: 0.8948 - val_loss: 0.2445 - val_acc: 0.9285\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3087 - acc: 0.9028\n",
      "Epoch 00013: val_loss improved from 0.24446 to 0.22824, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_8_conv_checkpoint/013-0.2282.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.3088 - acc: 0.9028 - val_loss: 0.2282 - val_acc: 0.9327\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2921 - acc: 0.9055\n",
      "Epoch 00014: val_loss improved from 0.22824 to 0.19315, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_8_conv_checkpoint/014-0.1932.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.2921 - acc: 0.9056 - val_loss: 0.1932 - val_acc: 0.9420\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2744 - acc: 0.9127\n",
      "Epoch 00015: val_loss improved from 0.19315 to 0.19012, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_8_conv_checkpoint/015-0.1901.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.2744 - acc: 0.9127 - val_loss: 0.1901 - val_acc: 0.9408\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2625 - acc: 0.9162\n",
      "Epoch 00016: val_loss improved from 0.19012 to 0.17707, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_8_conv_checkpoint/016-0.1771.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.2625 - acc: 0.9162 - val_loss: 0.1771 - val_acc: 0.9453\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2483 - acc: 0.9205\n",
      "Epoch 00017: val_loss did not improve from 0.17707\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.2484 - acc: 0.9204 - val_loss: 0.1798 - val_acc: 0.9492\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2364 - acc: 0.9240\n",
      "Epoch 00018: val_loss improved from 0.17707 to 0.16697, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_8_conv_checkpoint/018-0.1670.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.2364 - acc: 0.9240 - val_loss: 0.1670 - val_acc: 0.9504\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2202 - acc: 0.9286\n",
      "Epoch 00019: val_loss improved from 0.16697 to 0.15917, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_8_conv_checkpoint/019-0.1592.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.2202 - acc: 0.9286 - val_loss: 0.1592 - val_acc: 0.9557\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2173 - acc: 0.9307\n",
      "Epoch 00020: val_loss improved from 0.15917 to 0.14972, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_8_conv_checkpoint/020-0.1497.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.2174 - acc: 0.9307 - val_loss: 0.1497 - val_acc: 0.9595\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2029 - acc: 0.9339\n",
      "Epoch 00021: val_loss did not improve from 0.14972\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.2029 - acc: 0.9339 - val_loss: 0.1623 - val_acc: 0.9520\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1987 - acc: 0.9352\n",
      "Epoch 00022: val_loss did not improve from 0.14972\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1987 - acc: 0.9352 - val_loss: 0.1610 - val_acc: 0.9543\n",
      "Epoch 23/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1851 - acc: 0.9406\n",
      "Epoch 00023: val_loss did not improve from 0.14972\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1851 - acc: 0.9406 - val_loss: 0.1606 - val_acc: 0.9497\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1772 - acc: 0.9412\n",
      "Epoch 00024: val_loss improved from 0.14972 to 0.14886, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_8_conv_checkpoint/024-0.1489.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1772 - acc: 0.9412 - val_loss: 0.1489 - val_acc: 0.9557\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1741 - acc: 0.9425\n",
      "Epoch 00025: val_loss improved from 0.14886 to 0.13367, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_8_conv_checkpoint/025-0.1337.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1741 - acc: 0.9425 - val_loss: 0.1337 - val_acc: 0.9632\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1644 - acc: 0.9465\n",
      "Epoch 00026: val_loss did not improve from 0.13367\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1644 - acc: 0.9465 - val_loss: 0.1397 - val_acc: 0.9623\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1546 - acc: 0.9501\n",
      "Epoch 00027: val_loss improved from 0.13367 to 0.13282, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_8_conv_checkpoint/027-0.1328.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1546 - acc: 0.9501 - val_loss: 0.1328 - val_acc: 0.9618\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1476 - acc: 0.9510\n",
      "Epoch 00028: val_loss did not improve from 0.13282\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1476 - acc: 0.9510 - val_loss: 0.1593 - val_acc: 0.9546\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1456 - acc: 0.9528\n",
      "Epoch 00029: val_loss did not improve from 0.13282\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1456 - acc: 0.9528 - val_loss: 0.1372 - val_acc: 0.9611\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1414 - acc: 0.9547\n",
      "Epoch 00030: val_loss did not improve from 0.13282\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1413 - acc: 0.9547 - val_loss: 0.1589 - val_acc: 0.9532\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1367 - acc: 0.9551\n",
      "Epoch 00031: val_loss improved from 0.13282 to 0.12752, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_8_conv_checkpoint/031-0.1275.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1367 - acc: 0.9551 - val_loss: 0.1275 - val_acc: 0.9602\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1297 - acc: 0.9580\n",
      "Epoch 00032: val_loss did not improve from 0.12752\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1298 - acc: 0.9579 - val_loss: 0.1453 - val_acc: 0.9599\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1321 - acc: 0.9560\n",
      "Epoch 00033: val_loss did not improve from 0.12752\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1321 - acc: 0.9560 - val_loss: 0.1375 - val_acc: 0.9634\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1191 - acc: 0.9612\n",
      "Epoch 00034: val_loss did not improve from 0.12752\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1191 - acc: 0.9612 - val_loss: 0.1490 - val_acc: 0.9541\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1167 - acc: 0.9610\n",
      "Epoch 00035: val_loss improved from 0.12752 to 0.12729, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_8_conv_checkpoint/035-0.1273.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1170 - acc: 0.9610 - val_loss: 0.1273 - val_acc: 0.9625\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1158 - acc: 0.9615\n",
      "Epoch 00036: val_loss improved from 0.12729 to 0.11312, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_8_conv_checkpoint/036-0.1131.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1158 - acc: 0.9616 - val_loss: 0.1131 - val_acc: 0.9688\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1090 - acc: 0.9641\n",
      "Epoch 00037: val_loss did not improve from 0.11312\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1090 - acc: 0.9641 - val_loss: 0.1219 - val_acc: 0.9646\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1040 - acc: 0.9656\n",
      "Epoch 00038: val_loss did not improve from 0.11312\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1040 - acc: 0.9656 - val_loss: 0.1355 - val_acc: 0.9637\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1057 - acc: 0.9645\n",
      "Epoch 00039: val_loss did not improve from 0.11312\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1057 - acc: 0.9644 - val_loss: 0.1270 - val_acc: 0.9637\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1055 - acc: 0.9656\n",
      "Epoch 00040: val_loss did not improve from 0.11312\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.1055 - acc: 0.9656 - val_loss: 0.1144 - val_acc: 0.9683\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0999 - acc: 0.9666\n",
      "Epoch 00041: val_loss did not improve from 0.11312\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0999 - acc: 0.9666 - val_loss: 0.1193 - val_acc: 0.9639\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0926 - acc: 0.9697\n",
      "Epoch 00042: val_loss did not improve from 0.11312\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0926 - acc: 0.9697 - val_loss: 0.1198 - val_acc: 0.9658\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0927 - acc: 0.9690\n",
      "Epoch 00043: val_loss did not improve from 0.11312\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0927 - acc: 0.9690 - val_loss: 0.1219 - val_acc: 0.9648\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0900 - acc: 0.9699\n",
      "Epoch 00044: val_loss did not improve from 0.11312\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0900 - acc: 0.9699 - val_loss: 0.1148 - val_acc: 0.9672\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0864 - acc: 0.9712\n",
      "Epoch 00045: val_loss did not improve from 0.11312\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0865 - acc: 0.9712 - val_loss: 0.1187 - val_acc: 0.9653\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0852 - acc: 0.9707\n",
      "Epoch 00046: val_loss did not improve from 0.11312\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0852 - acc: 0.9707 - val_loss: 0.1354 - val_acc: 0.9658\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0812 - acc: 0.9732\n",
      "Epoch 00047: val_loss improved from 0.11312 to 0.11111, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_8_conv_checkpoint/047-0.1111.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0812 - acc: 0.9732 - val_loss: 0.1111 - val_acc: 0.9695\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0812 - acc: 0.9728\n",
      "Epoch 00048: val_loss improved from 0.11111 to 0.10452, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_8_conv_checkpoint/048-0.1045.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0812 - acc: 0.9728 - val_loss: 0.1045 - val_acc: 0.9713\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0773 - acc: 0.9737\n",
      "Epoch 00049: val_loss did not improve from 0.10452\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0773 - acc: 0.9737 - val_loss: 0.1173 - val_acc: 0.9702\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0766 - acc: 0.9740\n",
      "Epoch 00050: val_loss did not improve from 0.10452\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0766 - acc: 0.9741 - val_loss: 0.1199 - val_acc: 0.9676\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0738 - acc: 0.9751\n",
      "Epoch 00051: val_loss did not improve from 0.10452\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0737 - acc: 0.9751 - val_loss: 0.1482 - val_acc: 0.9651\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0758 - acc: 0.9738\n",
      "Epoch 00052: val_loss did not improve from 0.10452\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0758 - acc: 0.9738 - val_loss: 0.1194 - val_acc: 0.9688\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0711 - acc: 0.9756\n",
      "Epoch 00053: val_loss did not improve from 0.10452\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0711 - acc: 0.9756 - val_loss: 0.1099 - val_acc: 0.9704\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0679 - acc: 0.9765\n",
      "Epoch 00054: val_loss did not improve from 0.10452\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0679 - acc: 0.9766 - val_loss: 0.1267 - val_acc: 0.9667\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0658 - acc: 0.9783\n",
      "Epoch 00055: val_loss did not improve from 0.10452\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0658 - acc: 0.9783 - val_loss: 0.1217 - val_acc: 0.9693\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0693 - acc: 0.9761\n",
      "Epoch 00056: val_loss did not improve from 0.10452\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0693 - acc: 0.9761 - val_loss: 0.1185 - val_acc: 0.9713\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0650 - acc: 0.9772\n",
      "Epoch 00057: val_loss did not improve from 0.10452\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0650 - acc: 0.9772 - val_loss: 0.1319 - val_acc: 0.9681\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0653 - acc: 0.9783\n",
      "Epoch 00058: val_loss did not improve from 0.10452\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0653 - acc: 0.9783 - val_loss: 0.1249 - val_acc: 0.9646\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0604 - acc: 0.9797\n",
      "Epoch 00059: val_loss did not improve from 0.10452\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0604 - acc: 0.9797 - val_loss: 0.1435 - val_acc: 0.9660\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0594 - acc: 0.9796\n",
      "Epoch 00060: val_loss did not improve from 0.10452\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0594 - acc: 0.9796 - val_loss: 0.1296 - val_acc: 0.9679\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0579 - acc: 0.9809\n",
      "Epoch 00061: val_loss did not improve from 0.10452\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0579 - acc: 0.9809 - val_loss: 0.1413 - val_acc: 0.9653\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0588 - acc: 0.9803\n",
      "Epoch 00062: val_loss did not improve from 0.10452\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0588 - acc: 0.9803 - val_loss: 0.1375 - val_acc: 0.9658\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0560 - acc: 0.9809\n",
      "Epoch 00063: val_loss did not improve from 0.10452\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0560 - acc: 0.9809 - val_loss: 0.1364 - val_acc: 0.9665\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0545 - acc: 0.9818\n",
      "Epoch 00064: val_loss did not improve from 0.10452\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0545 - acc: 0.9818 - val_loss: 0.1413 - val_acc: 0.9646\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0539 - acc: 0.9818\n",
      "Epoch 00065: val_loss did not improve from 0.10452\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0539 - acc: 0.9818 - val_loss: 0.1124 - val_acc: 0.9697\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0558 - acc: 0.9810\n",
      "Epoch 00066: val_loss did not improve from 0.10452\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0558 - acc: 0.9810 - val_loss: 0.1290 - val_acc: 0.9658\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0537 - acc: 0.9814\n",
      "Epoch 00067: val_loss did not improve from 0.10452\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0537 - acc: 0.9814 - val_loss: 0.1091 - val_acc: 0.9720\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0474 - acc: 0.9840\n",
      "Epoch 00068: val_loss did not improve from 0.10452\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0474 - acc: 0.9839 - val_loss: 0.1218 - val_acc: 0.9713\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0491 - acc: 0.9833\n",
      "Epoch 00069: val_loss did not improve from 0.10452\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0491 - acc: 0.9833 - val_loss: 0.1305 - val_acc: 0.9702\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0483 - acc: 0.9831\n",
      "Epoch 00070: val_loss did not improve from 0.10452\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0483 - acc: 0.9831 - val_loss: 0.1513 - val_acc: 0.9630\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0621 - acc: 0.9789\n",
      "Epoch 00071: val_loss did not improve from 0.10452\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0621 - acc: 0.9789 - val_loss: 0.1326 - val_acc: 0.9683\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0492 - acc: 0.9835\n",
      "Epoch 00072: val_loss did not improve from 0.10452\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0492 - acc: 0.9835 - val_loss: 0.1212 - val_acc: 0.9683\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0456 - acc: 0.9849\n",
      "Epoch 00073: val_loss did not improve from 0.10452\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0456 - acc: 0.9849 - val_loss: 0.1409 - val_acc: 0.9644\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0443 - acc: 0.9848\n",
      "Epoch 00074: val_loss did not improve from 0.10452\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0444 - acc: 0.9848 - val_loss: 0.1440 - val_acc: 0.9641\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0461 - acc: 0.9842\n",
      "Epoch 00075: val_loss did not improve from 0.10452\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0460 - acc: 0.9842 - val_loss: 0.1347 - val_acc: 0.9658\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0476 - acc: 0.9847\n",
      "Epoch 00076: val_loss did not improve from 0.10452\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0476 - acc: 0.9847 - val_loss: 0.1283 - val_acc: 0.9693\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0423 - acc: 0.9863\n",
      "Epoch 00077: val_loss did not improve from 0.10452\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0423 - acc: 0.9863 - val_loss: 0.1199 - val_acc: 0.9688\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0407 - acc: 0.9862\n",
      "Epoch 00078: val_loss did not improve from 0.10452\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0407 - acc: 0.9862 - val_loss: 0.1771 - val_acc: 0.9637\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0446 - acc: 0.9846\n",
      "Epoch 00079: val_loss did not improve from 0.10452\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0446 - acc: 0.9846 - val_loss: 0.1160 - val_acc: 0.9737\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0392 - acc: 0.9866\n",
      "Epoch 00080: val_loss did not improve from 0.10452\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0392 - acc: 0.9866 - val_loss: 0.1301 - val_acc: 0.9674\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0412 - acc: 0.9865\n",
      "Epoch 00081: val_loss did not improve from 0.10452\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0412 - acc: 0.9865 - val_loss: 0.1346 - val_acc: 0.9695\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0385 - acc: 0.9870\n",
      "Epoch 00082: val_loss did not improve from 0.10452\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0385 - acc: 0.9870 - val_loss: 0.1421 - val_acc: 0.9690\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0407 - acc: 0.9858\n",
      "Epoch 00083: val_loss did not improve from 0.10452\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0407 - acc: 0.9858 - val_loss: 0.1527 - val_acc: 0.9688\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0405 - acc: 0.9869\n",
      "Epoch 00084: val_loss did not improve from 0.10452\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0405 - acc: 0.9869 - val_loss: 0.1397 - val_acc: 0.9683\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0383 - acc: 0.9879\n",
      "Epoch 00085: val_loss did not improve from 0.10452\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0383 - acc: 0.9879 - val_loss: 0.1180 - val_acc: 0.9704\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0387 - acc: 0.9870\n",
      "Epoch 00086: val_loss did not improve from 0.10452\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0387 - acc: 0.9870 - val_loss: 0.1403 - val_acc: 0.9697\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0357 - acc: 0.9877\n",
      "Epoch 00087: val_loss did not improve from 0.10452\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0357 - acc: 0.9877 - val_loss: 0.1394 - val_acc: 0.9690\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0348 - acc: 0.9884\n",
      "Epoch 00088: val_loss did not improve from 0.10452\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0348 - acc: 0.9884 - val_loss: 0.1374 - val_acc: 0.9676\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0364 - acc: 0.9881\n",
      "Epoch 00089: val_loss did not improve from 0.10452\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0364 - acc: 0.9881 - val_loss: 0.1295 - val_acc: 0.9713\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0347 - acc: 0.9884\n",
      "Epoch 00090: val_loss did not improve from 0.10452\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0347 - acc: 0.9884 - val_loss: 0.1320 - val_acc: 0.9718\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0362 - acc: 0.9879\n",
      "Epoch 00091: val_loss did not improve from 0.10452\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0362 - acc: 0.9879 - val_loss: 0.1292 - val_acc: 0.9727\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0378 - acc: 0.9874\n",
      "Epoch 00092: val_loss did not improve from 0.10452\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0378 - acc: 0.9874 - val_loss: 0.1559 - val_acc: 0.9669\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0351 - acc: 0.9886\n",
      "Epoch 00093: val_loss did not improve from 0.10452\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0351 - acc: 0.9886 - val_loss: 0.1433 - val_acc: 0.9679\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0313 - acc: 0.9896\n",
      "Epoch 00094: val_loss did not improve from 0.10452\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0313 - acc: 0.9896 - val_loss: 0.1315 - val_acc: 0.9720\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0369 - acc: 0.9874\n",
      "Epoch 00095: val_loss did not improve from 0.10452\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0369 - acc: 0.9874 - val_loss: 0.1248 - val_acc: 0.9713\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0326 - acc: 0.9889\n",
      "Epoch 00096: val_loss did not improve from 0.10452\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0326 - acc: 0.9889 - val_loss: 0.1402 - val_acc: 0.9711\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0332 - acc: 0.9885\n",
      "Epoch 00097: val_loss did not improve from 0.10452\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0333 - acc: 0.9884 - val_loss: 0.1619 - val_acc: 0.9634\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0347 - acc: 0.9885\n",
      "Epoch 00098: val_loss did not improve from 0.10452\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0347 - acc: 0.9885 - val_loss: 0.1259 - val_acc: 0.9718\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_8_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 0.1765 - acc: 0.9502\n",
      "Loss: 0.17650031864349097 Accuracy: 0.95015574\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.3490 - acc: 0.2313\n",
      "Epoch 00001: val_loss improved from inf to 1.68440, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_9_conv_checkpoint/001-1.6844.hdf5\n",
      "36805/36805 [==============================] - 100s 3ms/sample - loss: 2.3490 - acc: 0.2313 - val_loss: 1.6844 - val_acc: 0.4528\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4261 - acc: 0.5374\n",
      "Epoch 00002: val_loss improved from 1.68440 to 0.94756, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_9_conv_checkpoint/002-0.9476.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 1.4261 - acc: 0.5374 - val_loss: 0.9476 - val_acc: 0.6879\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9823 - acc: 0.6790\n",
      "Epoch 00003: val_loss improved from 0.94756 to 0.69761, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_9_conv_checkpoint/003-0.6976.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.9822 - acc: 0.6790 - val_loss: 0.6976 - val_acc: 0.7666\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7189 - acc: 0.7654\n",
      "Epoch 00004: val_loss improved from 0.69761 to 0.51278, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_9_conv_checkpoint/004-0.5128.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.7188 - acc: 0.7654 - val_loss: 0.5128 - val_acc: 0.8358\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5733 - acc: 0.8165\n",
      "Epoch 00005: val_loss improved from 0.51278 to 0.44682, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_9_conv_checkpoint/005-0.4468.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.5734 - acc: 0.8165 - val_loss: 0.4468 - val_acc: 0.8539\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4919 - acc: 0.8407\n",
      "Epoch 00006: val_loss improved from 0.44682 to 0.31717, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_9_conv_checkpoint/006-0.3172.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.4918 - acc: 0.8407 - val_loss: 0.3172 - val_acc: 0.8994\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4247 - acc: 0.8632\n",
      "Epoch 00007: val_loss improved from 0.31717 to 0.31024, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_9_conv_checkpoint/007-0.3102.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.4247 - acc: 0.8632 - val_loss: 0.3102 - val_acc: 0.9015\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3744 - acc: 0.8791\n",
      "Epoch 00008: val_loss improved from 0.31024 to 0.26580, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_9_conv_checkpoint/008-0.2658.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.3744 - acc: 0.8791 - val_loss: 0.2658 - val_acc: 0.9168\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3373 - acc: 0.8924\n",
      "Epoch 00009: val_loss improved from 0.26580 to 0.25408, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_9_conv_checkpoint/009-0.2541.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.3372 - acc: 0.8924 - val_loss: 0.2541 - val_acc: 0.9234\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3057 - acc: 0.8996\n",
      "Epoch 00010: val_loss improved from 0.25408 to 0.23199, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_9_conv_checkpoint/010-0.2320.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.3059 - acc: 0.8995 - val_loss: 0.2320 - val_acc: 0.9285\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2841 - acc: 0.9105\n",
      "Epoch 00011: val_loss improved from 0.23199 to 0.22538, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_9_conv_checkpoint/011-0.2254.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.2841 - acc: 0.9105 - val_loss: 0.2254 - val_acc: 0.9262\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2538 - acc: 0.9176\n",
      "Epoch 00012: val_loss improved from 0.22538 to 0.20268, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_9_conv_checkpoint/012-0.2027.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.2539 - acc: 0.9176 - val_loss: 0.2027 - val_acc: 0.9376\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2364 - acc: 0.9235\n",
      "Epoch 00013: val_loss did not improve from 0.20268\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.2363 - acc: 0.9235 - val_loss: 0.2106 - val_acc: 0.9327\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2192 - acc: 0.9308\n",
      "Epoch 00014: val_loss improved from 0.20268 to 0.18102, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_9_conv_checkpoint/014-0.1810.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.2192 - acc: 0.9308 - val_loss: 0.1810 - val_acc: 0.9432\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2030 - acc: 0.9343\n",
      "Epoch 00015: val_loss did not improve from 0.18102\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.2030 - acc: 0.9343 - val_loss: 0.1867 - val_acc: 0.9401\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1896 - acc: 0.9401\n",
      "Epoch 00016: val_loss improved from 0.18102 to 0.16214, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_9_conv_checkpoint/016-0.1621.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1895 - acc: 0.9401 - val_loss: 0.1621 - val_acc: 0.9492\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1731 - acc: 0.9428\n",
      "Epoch 00017: val_loss did not improve from 0.16214\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1731 - acc: 0.9428 - val_loss: 0.1792 - val_acc: 0.9443\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1655 - acc: 0.9454\n",
      "Epoch 00018: val_loss improved from 0.16214 to 0.15927, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_9_conv_checkpoint/018-0.1593.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1655 - acc: 0.9454 - val_loss: 0.1593 - val_acc: 0.9502\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1614 - acc: 0.9477\n",
      "Epoch 00019: val_loss improved from 0.15927 to 0.15789, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_9_conv_checkpoint/019-0.1579.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1615 - acc: 0.9476 - val_loss: 0.1579 - val_acc: 0.9497\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1473 - acc: 0.9512\n",
      "Epoch 00020: val_loss did not improve from 0.15789\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1473 - acc: 0.9512 - val_loss: 0.1694 - val_acc: 0.9467\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1360 - acc: 0.9530\n",
      "Epoch 00021: val_loss did not improve from 0.15789\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1360 - acc: 0.9530 - val_loss: 0.1652 - val_acc: 0.9502\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1269 - acc: 0.9586\n",
      "Epoch 00022: val_loss improved from 0.15789 to 0.14530, saving model to model/checkpoint/1D_CNN_custom_conv_3_VGG_DO_9_conv_checkpoint/022-0.1453.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1269 - acc: 0.9585 - val_loss: 0.1453 - val_acc: 0.9562\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1219 - acc: 0.9599\n",
      "Epoch 00023: val_loss did not improve from 0.14530\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1219 - acc: 0.9599 - val_loss: 0.1632 - val_acc: 0.9541\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1181 - acc: 0.9605\n",
      "Epoch 00024: val_loss did not improve from 0.14530\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1181 - acc: 0.9605 - val_loss: 0.1720 - val_acc: 0.9534\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1079 - acc: 0.9638\n",
      "Epoch 00025: val_loss did not improve from 0.14530\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1079 - acc: 0.9638 - val_loss: 0.1493 - val_acc: 0.9546\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1007 - acc: 0.9671\n",
      "Epoch 00026: val_loss did not improve from 0.14530\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1007 - acc: 0.9671 - val_loss: 0.1492 - val_acc: 0.9546\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0972 - acc: 0.9673\n",
      "Epoch 00027: val_loss did not improve from 0.14530\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0973 - acc: 0.9672 - val_loss: 0.1606 - val_acc: 0.9525\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0932 - acc: 0.9682\n",
      "Epoch 00028: val_loss did not improve from 0.14530\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0931 - acc: 0.9682 - val_loss: 0.1555 - val_acc: 0.9546\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0855 - acc: 0.9714\n",
      "Epoch 00029: val_loss did not improve from 0.14530\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0854 - acc: 0.9714 - val_loss: 0.1505 - val_acc: 0.9595\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0832 - acc: 0.9714\n",
      "Epoch 00030: val_loss did not improve from 0.14530\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0832 - acc: 0.9714 - val_loss: 0.1786 - val_acc: 0.9541\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0779 - acc: 0.9735\n",
      "Epoch 00031: val_loss did not improve from 0.14530\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0780 - acc: 0.9734 - val_loss: 0.1675 - val_acc: 0.9578\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0824 - acc: 0.9723\n",
      "Epoch 00032: val_loss did not improve from 0.14530\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0824 - acc: 0.9723 - val_loss: 0.1761 - val_acc: 0.9509\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0751 - acc: 0.9747\n",
      "Epoch 00033: val_loss did not improve from 0.14530\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0751 - acc: 0.9747 - val_loss: 0.1490 - val_acc: 0.9599\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0671 - acc: 0.9766\n",
      "Epoch 00034: val_loss did not improve from 0.14530\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0671 - acc: 0.9766 - val_loss: 0.1730 - val_acc: 0.9548\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0662 - acc: 0.9778\n",
      "Epoch 00035: val_loss did not improve from 0.14530\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0662 - acc: 0.9778 - val_loss: 0.1526 - val_acc: 0.9588\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0682 - acc: 0.9772\n",
      "Epoch 00036: val_loss did not improve from 0.14530\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0682 - acc: 0.9772 - val_loss: 0.1487 - val_acc: 0.9564\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0580 - acc: 0.9800\n",
      "Epoch 00037: val_loss did not improve from 0.14530\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0580 - acc: 0.9800 - val_loss: 0.1780 - val_acc: 0.9581\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0571 - acc: 0.9808\n",
      "Epoch 00038: val_loss did not improve from 0.14530\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0571 - acc: 0.9808 - val_loss: 0.1534 - val_acc: 0.9578\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0598 - acc: 0.9796\n",
      "Epoch 00039: val_loss did not improve from 0.14530\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0598 - acc: 0.9796 - val_loss: 0.1752 - val_acc: 0.9574\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0517 - acc: 0.9828\n",
      "Epoch 00040: val_loss did not improve from 0.14530\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0517 - acc: 0.9828 - val_loss: 0.1646 - val_acc: 0.9611\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0531 - acc: 0.9825\n",
      "Epoch 00041: val_loss did not improve from 0.14530\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0531 - acc: 0.9825 - val_loss: 0.1578 - val_acc: 0.9613\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0497 - acc: 0.9835\n",
      "Epoch 00042: val_loss did not improve from 0.14530\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0497 - acc: 0.9835 - val_loss: 0.1746 - val_acc: 0.9588\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0500 - acc: 0.9837\n",
      "Epoch 00043: val_loss did not improve from 0.14530\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0500 - acc: 0.9837 - val_loss: 0.1775 - val_acc: 0.9618\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0514 - acc: 0.9833\n",
      "Epoch 00044: val_loss did not improve from 0.14530\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0514 - acc: 0.9833 - val_loss: 0.1547 - val_acc: 0.9637\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0468 - acc: 0.9842\n",
      "Epoch 00045: val_loss did not improve from 0.14530\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0468 - acc: 0.9842 - val_loss: 0.1811 - val_acc: 0.9581\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0418 - acc: 0.9858\n",
      "Epoch 00046: val_loss did not improve from 0.14530\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0418 - acc: 0.9858 - val_loss: 0.1791 - val_acc: 0.9630\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0462 - acc: 0.9851\n",
      "Epoch 00047: val_loss did not improve from 0.14530\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0462 - acc: 0.9851 - val_loss: 0.1893 - val_acc: 0.9557\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0471 - acc: 0.9849\n",
      "Epoch 00048: val_loss did not improve from 0.14530\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0471 - acc: 0.9849 - val_loss: 0.1569 - val_acc: 0.9604\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0413 - acc: 0.9864\n",
      "Epoch 00049: val_loss did not improve from 0.14530\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0413 - acc: 0.9864 - val_loss: 0.1496 - val_acc: 0.9599\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0384 - acc: 0.9871\n",
      "Epoch 00050: val_loss did not improve from 0.14530\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0384 - acc: 0.9871 - val_loss: 0.2126 - val_acc: 0.9557\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0360 - acc: 0.9878\n",
      "Epoch 00051: val_loss did not improve from 0.14530\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0360 - acc: 0.9878 - val_loss: 0.1854 - val_acc: 0.9602\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0403 - acc: 0.9863\n",
      "Epoch 00052: val_loss did not improve from 0.14530\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0403 - acc: 0.9863 - val_loss: 0.1893 - val_acc: 0.9620\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0410 - acc: 0.9863\n",
      "Epoch 00053: val_loss did not improve from 0.14530\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0410 - acc: 0.9863 - val_loss: 0.1736 - val_acc: 0.9625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0382 - acc: 0.9879\n",
      "Epoch 00054: val_loss did not improve from 0.14530\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0382 - acc: 0.9879 - val_loss: 0.2183 - val_acc: 0.9604\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0373 - acc: 0.9881\n",
      "Epoch 00055: val_loss did not improve from 0.14530\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0373 - acc: 0.9881 - val_loss: 0.1797 - val_acc: 0.9590\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0335 - acc: 0.9892\n",
      "Epoch 00056: val_loss did not improve from 0.14530\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0335 - acc: 0.9892 - val_loss: 0.1707 - val_acc: 0.9634\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0332 - acc: 0.9888\n",
      "Epoch 00057: val_loss did not improve from 0.14530\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0331 - acc: 0.9888 - val_loss: 0.1829 - val_acc: 0.9613\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0356 - acc: 0.9886\n",
      "Epoch 00058: val_loss did not improve from 0.14530\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0356 - acc: 0.9886 - val_loss: 0.1731 - val_acc: 0.9637\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0313 - acc: 0.9898\n",
      "Epoch 00059: val_loss did not improve from 0.14530\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0313 - acc: 0.9898 - val_loss: 0.1813 - val_acc: 0.9627\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0280 - acc: 0.9904\n",
      "Epoch 00060: val_loss did not improve from 0.14530\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0280 - acc: 0.9904 - val_loss: 0.1897 - val_acc: 0.9613\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0295 - acc: 0.9904\n",
      "Epoch 00061: val_loss did not improve from 0.14530\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0295 - acc: 0.9904 - val_loss: 0.2034 - val_acc: 0.9606\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0363 - acc: 0.9882\n",
      "Epoch 00062: val_loss did not improve from 0.14530\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0363 - acc: 0.9882 - val_loss: 0.1559 - val_acc: 0.9658\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0285 - acc: 0.9905\n",
      "Epoch 00063: val_loss did not improve from 0.14530\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0285 - acc: 0.9905 - val_loss: 0.1670 - val_acc: 0.9655\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0281 - acc: 0.9908\n",
      "Epoch 00064: val_loss did not improve from 0.14530\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0281 - acc: 0.9908 - val_loss: 0.1757 - val_acc: 0.9611\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0294 - acc: 0.9903\n",
      "Epoch 00065: val_loss did not improve from 0.14530\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0294 - acc: 0.9903 - val_loss: 0.1969 - val_acc: 0.9611\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0267 - acc: 0.9913\n",
      "Epoch 00066: val_loss did not improve from 0.14530\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0267 - acc: 0.9913 - val_loss: 0.2081 - val_acc: 0.9639\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0281 - acc: 0.9905\n",
      "Epoch 00067: val_loss did not improve from 0.14530\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0281 - acc: 0.9905 - val_loss: 0.1873 - val_acc: 0.9623\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0244 - acc: 0.9920\n",
      "Epoch 00068: val_loss did not improve from 0.14530\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0245 - acc: 0.9920 - val_loss: 0.1974 - val_acc: 0.9630\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0286 - acc: 0.9906\n",
      "Epoch 00069: val_loss did not improve from 0.14530\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0286 - acc: 0.9906 - val_loss: 0.1841 - val_acc: 0.9648\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0243 - acc: 0.9922\n",
      "Epoch 00070: val_loss did not improve from 0.14530\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0243 - acc: 0.9922 - val_loss: 0.1944 - val_acc: 0.9578\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0264 - acc: 0.9916\n",
      "Epoch 00071: val_loss did not improve from 0.14530\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0264 - acc: 0.9916 - val_loss: 0.1743 - val_acc: 0.9653\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0242 - acc: 0.9923\n",
      "Epoch 00072: val_loss did not improve from 0.14530\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0242 - acc: 0.9923 - val_loss: 0.1662 - val_acc: 0.9662\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_9_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 0.2050 - acc: 0.9439\n",
      "Loss: 0.2050418087843175 Accuracy: 0.94392526\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(6, 10):\n",
    "    base = '1D_CNN_custom_conv_3_VGG_DO'\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    model = build_1d_cnn_custom_conv_3_VGG_DO(conv_num=i)\n",
    "#         model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4),\n",
    "          metrics=['accuracy'])\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model_filename = model_path+'{epoch:03d}-{val_loss:.4f}.hdf5'\n",
    "    checkpointer = ModelCheckpoint(filepath = model_filename, monitor = \"val_loss\", \n",
    "                                   verbose=1, save_best_only=True)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=50)\n",
    "    hist = model.fit(x_train_abs, y_train_onehot, batch_size=64, epochs=500, \n",
    "                     validation_data=[x_val_abs, y_val_onehot], shuffle=True, \n",
    "                     callbacks = [checkpointer, early_stopping])\n",
    "\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "    ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "    ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "    ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    png_path = 'visualization/learning_curve/'\n",
    "    filename = model_name+'.png'\n",
    "    os.makedirs(png_path, exist_ok=True)\n",
    "    fig.savefig(png_path+filename, transparent=True)\n",
    "\n",
    "    model.save(model_path+'000_last.hdf5')\n",
    "    del(model)\n",
    "    \n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "    model = load_model(model_filename)\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "    print()\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_1_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_90 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "conv1d_91 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 1024000)           0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 1024000)           0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 16)                16384016  \n",
      "=================================================================\n",
      "Total params: 16,396,624\n",
      "Trainable params: 16,396,624\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 533us/sample - loss: 1.9976 - acc: 0.3909\n",
      "Loss: 1.9976032813885254 Accuracy: 0.3908619\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_2_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_2 (Conv1D)            (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 341312)            0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 341312)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                5461008   \n",
      "=================================================================\n",
      "Total params: 5,498,320\n",
      "Trainable params: 5,498,320\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 842us/sample - loss: 1.7659 - acc: 0.4573\n",
      "Loss: 1.7659185970807496 Accuracy: 0.45732087\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_3_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_96 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "conv1d_97 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_98 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_99 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_100 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_101 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 16)                1819664   \n",
      "=================================================================\n",
      "Total params: 1,881,680\n",
      "Trainable params: 1,881,680\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 5s 960us/sample - loss: 1.4214 - acc: 0.5601\n",
      "Loss: 1.4213561559639492 Accuracy: 0.56012464\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_4_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_12 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_15 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_16 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_17 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_18 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_19 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                606224    \n",
      "=================================================================\n",
      "Total params: 692,944\n",
      "Trainable params: 692,944\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 1.0469 - acc: 0.6789\n",
      "Loss: 1.0469354746745259 Accuracy: 0.67892003\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_5_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_110 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "conv1d_111 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_112 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_113 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_114 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_115 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_116 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_117 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_118 (Conv1D)          (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "conv1d_119 (Conv1D)          (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 16)                403472    \n",
      "=================================================================\n",
      "Total params: 564,176\n",
      "Trainable params: 564,176\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 0.8236 - acc: 0.7593\n",
      "Loss: 0.8235816266554044 Accuracy: 0.75929385\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_6_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                133136    \n",
      "=================================================================\n",
      "Total params: 392,400\n",
      "Trainable params: 392,400\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 0.3998 - acc: 0.8955\n",
      "Loss: 0.39981899699689444 Accuracy: 0.8955348\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_7_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_12 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_15 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_16 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_17 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_18 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_19 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_20 (Conv1D)           (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "conv1d_21 (Conv1D)           (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_22 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "conv1d_23 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1 (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_24 (Conv1D)           (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "conv1d_25 (Conv1D)           (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                43024     \n",
      "=================================================================\n",
      "Total params: 400,848\n",
      "Trainable params: 400,848\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 0.1860 - acc: 0.9450\n",
      "Loss: 0.18600267374178206 Accuracy: 0.94496363\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_8_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_26 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "conv1d_27 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_28 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_29 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_30 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_31 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_32 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_33 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_34 (Conv1D)           (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "conv1d_35 (Conv1D)           (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_36 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "conv1d_37 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_38 (Conv1D)           (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "conv1d_39 (Conv1D)           (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_40 (Conv1D)           (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "conv1d_41 (Conv1D)           (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                14352     \n",
      "=================================================================\n",
      "Total params: 470,736\n",
      "Trainable params: 470,736\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 0.1765 - acc: 0.9502\n",
      "Loss: 0.17650031864349097 Accuracy: 0.95015574\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_9_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_42 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "conv1d_43 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_44 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_45 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_46 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_47 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_48 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_49 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_50 (Conv1D)           (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "conv1d_51 (Conv1D)           (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_52 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "conv1d_53 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_54 (Conv1D)           (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "conv1d_55 (Conv1D)           (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_56 (Conv1D)           (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "conv1d_57 (Conv1D)           (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_58 (Conv1D)           (None, 7, 256)            98560     \n",
      "_________________________________________________________________\n",
      "conv1d_59 (Conv1D)           (None, 7, 256)            196864    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling (None, 2, 256)            0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                8208      \n",
      "=================================================================\n",
      "Total params: 760,016\n",
      "Trainable params: 760,016\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.2050 - acc: 0.9439\n",
      "Loss: 0.2050418087843175 Accuracy: 0.94392526\n"
     ]
    }
   ],
   "source": [
    "# log_dir = 'log'\n",
    "# os.makedirs(log_dir, exist_ok=True)\n",
    "base = '1D_CNN_custom_conv_3_VGG_DO'\n",
    "\n",
    "# with open(path.join(log_dir, base), 'w') as log_file:\n",
    "for i in range(1, 10):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "\n",
    "    model = load_model(model_filename)\n",
    "    model.summary()\n",
    "\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "    del(model)\n",
    "\n",
    "#         log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_1_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_90 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "conv1d_91 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 1024000)           0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 1024000)           0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 16)                16384016  \n",
      "=================================================================\n",
      "Total params: 16,396,624\n",
      "Trainable params: 16,396,624\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 639us/sample - loss: 5.7123 - acc: 0.3238\n",
      "Loss: 5.712339253390937 Accuracy: 0.32377985\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_2_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_92 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "conv1d_93 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_94 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_95 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 341312)            0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 341312)            0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 16)                5461008   \n",
      "=================================================================\n",
      "Total params: 5,498,320\n",
      "Trainable params: 5,498,320\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 5s 959us/sample - loss: 4.4435 - acc: 0.4530\n",
      "Loss: 4.443544545178597 Accuracy: 0.4529595\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_3_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_96 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "conv1d_97 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_98 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_99 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_100 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_101 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 16)                1819664   \n",
      "=================================================================\n",
      "Total params: 1,881,680\n",
      "Trainable params: 1,881,680\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 2.6758 - acc: 0.6087\n",
      "Loss: 2.675766536181837 Accuracy: 0.60872275\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_4_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_102 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "conv1d_103 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_104 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_105 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_106 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_107 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_108 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_109 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 16)                606224    \n",
      "=================================================================\n",
      "Total params: 692,944\n",
      "Trainable params: 692,944\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 1.5002 - acc: 0.7244\n",
      "Loss: 1.5001820875229246 Accuracy: 0.7244029\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_5_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_110 (Conv1D)          (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "conv1d_111 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_112 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_113 (Conv1D)          (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_114 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_115 (Conv1D)          (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_116 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_117 (Conv1D)          (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_118 (Conv1D)          (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "conv1d_119 (Conv1D)          (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 16)                403472    \n",
      "=================================================================\n",
      "Total params: 564,176\n",
      "Trainable params: 564,176\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 1.0908 - acc: 0.7952\n",
      "Loss: 1.0908317271546535 Accuracy: 0.79522324\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_6_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                133136    \n",
      "=================================================================\n",
      "Total params: 392,400\n",
      "Trainable params: 392,400\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.5422 - acc: 0.9028\n",
      "Loss: 0.542166012884548 Accuracy: 0.9028037\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_7_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_12 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_15 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_16 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_17 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_18 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_19 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_20 (Conv1D)           (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "conv1d_21 (Conv1D)           (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_22 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "conv1d_23 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1 (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_24 (Conv1D)           (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "conv1d_25 (Conv1D)           (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                43024     \n",
      "=================================================================\n",
      "Total params: 400,848\n",
      "Trainable params: 400,848\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.2172 - acc: 0.9462\n",
      "Loss: 0.21720281954378912 Accuracy: 0.9462098\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_8_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_26 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "conv1d_27 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_28 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_29 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_30 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_31 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_32 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_33 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_34 (Conv1D)           (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "conv1d_35 (Conv1D)           (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_36 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "conv1d_37 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_38 (Conv1D)           (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "conv1d_39 (Conv1D)           (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_40 (Conv1D)           (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "conv1d_41 (Conv1D)           (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                14352     \n",
      "=================================================================\n",
      "Total params: 470,736\n",
      "Trainable params: 470,736\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.2163 - acc: 0.9551\n",
      "Loss: 0.21633543381240192 Accuracy: 0.9551402\n",
      "\n",
      "1D_CNN_custom_conv_3_VGG_DO_9_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_42 (Conv1D)           (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "conv1d_43 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_44 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "conv1d_45 (Conv1D)           (None, 16000, 64)         12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_46 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_47 (Conv1D)           (None, 5333, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_48 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "conv1d_49 (Conv1D)           (None, 1777, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_50 (Conv1D)           (None, 592, 128)          24704     \n",
      "_________________________________________________________________\n",
      "conv1d_51 (Conv1D)           (None, 592, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_52 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "conv1d_53 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_54 (Conv1D)           (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "conv1d_55 (Conv1D)           (None, 65, 128)           49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_56 (Conv1D)           (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "conv1d_57 (Conv1D)           (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_58 (Conv1D)           (None, 7, 256)            98560     \n",
      "_________________________________________________________________\n",
      "conv1d_59 (Conv1D)           (None, 7, 256)            196864    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling (None, 2, 256)            0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                8208      \n",
      "=================================================================\n",
      "Total params: 760,016\n",
      "Trainable params: 760,016\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.2293 - acc: 0.9543\n",
      "Loss: 0.22934781985661074 Accuracy: 0.95430946\n"
     ]
    }
   ],
   "source": [
    "# log_dir = 'log'\n",
    "# os.makedirs(log_dir, exist_ok=True)\n",
    "# base = '1D_CNN_custom_DO_BN'\n",
    "\n",
    "# with open(path.join(log_dir, base), 'w') as log_file:\n",
    "for i in range(1, 10):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + '000_last.hdf5'\n",
    "\n",
    "    model = load_model(model_filename)\n",
    "    model.summary()\n",
    "\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "    del(model)\n",
    "\n",
    "#         log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
