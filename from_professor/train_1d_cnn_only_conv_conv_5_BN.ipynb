{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path as path\n",
    "import itertools\n",
    "from sklearn.preprocessing import maxabs_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Input,InputLayer, Dense, Activation, BatchNormalization, Flatten, Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D, Dropout\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler, \\\n",
    "                                        EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = path.join('..', 'data')\n",
    "data_dir = path.join(base_dir, 'data_speech_commands_v0.02')\n",
    " \n",
    "train_txt = path.join(data_dir, 'wav_train_16words.txt')\n",
    "val_txt = path.join(data_dir, 'wav_validation_16words.txt')\n",
    "test_txt = path.join(data_dir, 'wav_test_16words.txt')\n",
    "\n",
    "train_data = np.load(path.join(data_dir, 'wav_train_data.npz'))\n",
    "val_data = np.load(path.join(data_dir, 'wav_validation_data.npz'))\n",
    "test_data = np.load(path.join(data_dir, 'wav_test_data.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36805, 16000, 1),\n",
       " (36805,),\n",
       " (4293, 16000, 1),\n",
       " (4293,),\n",
       " (4815, 16000, 1),\n",
       " (4815,),\n",
       " (16, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = train_data['x_train']\n",
    "y_train = train_data['y_train']\n",
    "x_val = val_data['x_val']\n",
    "y_val = val_data['y_val']\n",
    "x_test = test_data['x_test']\n",
    "y_test = test_data['y_test']\n",
    "y_table = test_data['table']\n",
    "\n",
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape, y_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_test[0].shape\n",
    "output_size = y_table.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_abs = np.asarray([maxabs_scale(wav) for wav in x_train])\n",
    "y_train_onehot = np.asarray([to_categorical(label, output_size) for label in y_train])\n",
    "del x_train, y_train\n",
    "\n",
    "x_val_abs = np.asarray([maxabs_scale(wav) for wav in x_val])\n",
    "y_val_onehot = np.asarray([to_categorical(label, output_size) for label in y_val])\n",
    "del x_val, y_val\n",
    "\n",
    "x_test_abs = np.asarray([maxabs_scale(wav) for wav in x_test])\n",
    "y_test_onehot = np.asarray([to_categorical(label, output_size) for label in y_test])\n",
    "del x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_1d_cnn_only_conv_conv_5_BN(conv_num=1):\n",
    "    model=Sequential()\n",
    "    model.add(Conv1D (kernel_size=5, filters=8, strides=1, padding='valid', input_shape=input_shape)) \n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling1D(pool_size=3, strides=3, padding='same'))\n",
    "    \n",
    "    for i in range(conv_num-1):\n",
    "        model.add(Conv1D (kernel_size=5, filters=8*(2**(i+1)), strides=1, padding='valid'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(MaxPooling1D(pool_size=3, strides=3, padding='same'))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(BatchNormalization())\n",
    "#     model.add(Dropout(0.5))\n",
    "    model.add(Dense(output_size, activation='softmax' ))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 15976, 8)          208       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1 (Batc (None, 15976, 8)          32        \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 15976, 8)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 3994, 8)           0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 31952)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                511248    \n",
      "=================================================================\n",
      "Total params: 511,488\n",
      "Trainable params: 511,472\n",
      "Non-trainable params: 16\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 15976, 8)          208       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_1 (Ba (None, 15976, 8)          32        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 15976, 8)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 3994, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 3970, 16)          3216      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_2 (Ba (None, 3970, 16)          64        \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 3970, 16)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 993, 16)           0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 15888)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                254224    \n",
      "=================================================================\n",
      "Total params: 257,744\n",
      "Trainable params: 257,696\n",
      "Non-trainable params: 48\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_3 (Conv1D)            (None, 15976, 8)          208       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_3 (Ba (None, 15976, 8)          32        \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 15976, 8)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 3994, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 3970, 16)          3216      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_4 (Ba (None, 3970, 16)          64        \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 3970, 16)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 993, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 969, 32)           12832     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_5 (Ba (None, 969, 32)           128       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 969, 32)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 243, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 7776)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                124432    \n",
      "=================================================================\n",
      "Total params: 140,912\n",
      "Trainable params: 140,800\n",
      "Non-trainable params: 112\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_6 (Conv1D)            (None, 15976, 8)          208       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_6 (Ba (None, 15976, 8)          32        \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 15976, 8)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 3994, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 3970, 16)          3216      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_7 (Ba (None, 3970, 16)          64        \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 3970, 16)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 993, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 969, 32)           12832     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_8 (Ba (None, 969, 32)           128       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 969, 32)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 243, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 219, 64)           51264     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_9 (Ba (None, 219, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 219, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1 (None, 55, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 3520)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                56336     \n",
      "=================================================================\n",
      "Total params: 124,336\n",
      "Trainable params: 124,096\n",
      "Non-trainable params: 240\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_10 (Conv1D)           (None, 15976, 8)          208       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_10 (B (None, 15976, 8)          32        \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 15976, 8)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling (None, 3994, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 3970, 16)          3216      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_11 (B (None, 3970, 16)          64        \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 3970, 16)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling (None, 993, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_12 (Conv1D)           (None, 969, 32)           12832     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_12 (B (None, 969, 32)           128       \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 969, 32)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling (None, 243, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 219, 64)           51264     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_13 (B (None, 219, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 219, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling (None, 55, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 31, 128)           204928    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_14 (B (None, 31, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 31, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling (None, 8, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                16400     \n",
      "=================================================================\n",
      "Total params: 289,840\n",
      "Trainable params: 289,344\n",
      "Non-trainable params: 496\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 10):\n",
    "    model = build_1d_cnn_only_conv_conv_5_BN(conv_num=i)\n",
    "    model.summary()\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36805 samples, validate on 4293 samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.3810 - acc: 0.2583\n",
      "Epoch 00001: val_loss improved from inf to 2.00441, saving model to model/checkpoint/1D_CNN_BN_1_only_conv_checkpoint/001-2.0044.hdf5\n",
      "36805/36805 [==============================] - 9s 255us/sample - loss: 2.3811 - acc: 0.2583 - val_loss: 2.0044 - val_acc: 0.3522\n",
      "Epoch 2/500\n",
      "36544/36805 [============================>.] - ETA: 0s - loss: 1.6398 - acc: 0.4739\n",
      "Epoch 00002: val_loss improved from 2.00441 to 1.86095, saving model to model/checkpoint/1D_CNN_BN_1_only_conv_checkpoint/002-1.8609.hdf5\n",
      "36805/36805 [==============================] - 7s 198us/sample - loss: 1.6390 - acc: 0.4742 - val_loss: 1.8609 - val_acc: 0.4034\n",
      "Epoch 3/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3685 - acc: 0.5638\n",
      "Epoch 00003: val_loss improved from 1.86095 to 1.75336, saving model to model/checkpoint/1D_CNN_BN_1_only_conv_checkpoint/003-1.7534.hdf5\n",
      "36805/36805 [==============================] - 7s 199us/sample - loss: 1.3685 - acc: 0.5638 - val_loss: 1.7534 - val_acc: 0.4440\n",
      "Epoch 4/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1932 - acc: 0.6253\n",
      "Epoch 00004: val_loss did not improve from 1.75336\n",
      "36805/36805 [==============================] - 8s 206us/sample - loss: 1.1930 - acc: 0.6254 - val_loss: 1.7696 - val_acc: 0.4512\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0502 - acc: 0.6719\n",
      "Epoch 00005: val_loss improved from 1.75336 to 1.74330, saving model to model/checkpoint/1D_CNN_BN_1_only_conv_checkpoint/005-1.7433.hdf5\n",
      "36805/36805 [==============================] - 7s 202us/sample - loss: 1.0502 - acc: 0.6719 - val_loss: 1.7433 - val_acc: 0.4642\n",
      "Epoch 6/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.9543 - acc: 0.7058\n",
      "Epoch 00006: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 207us/sample - loss: 0.9538 - acc: 0.7059 - val_loss: 1.7782 - val_acc: 0.4610\n",
      "Epoch 7/500\n",
      "36544/36805 [============================>.] - ETA: 0s - loss: 0.8577 - acc: 0.7375\n",
      "Epoch 00007: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 7s 203us/sample - loss: 0.8586 - acc: 0.7373 - val_loss: 1.8031 - val_acc: 0.4684\n",
      "Epoch 8/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.7841 - acc: 0.7634\n",
      "Epoch 00008: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 205us/sample - loss: 0.7844 - acc: 0.7632 - val_loss: 1.8632 - val_acc: 0.4652\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7233 - acc: 0.7829\n",
      "Epoch 00009: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 205us/sample - loss: 0.7234 - acc: 0.7829 - val_loss: 1.9623 - val_acc: 0.4403\n",
      "Epoch 10/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.6597 - acc: 0.8071\n",
      "Epoch 00010: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 205us/sample - loss: 0.6600 - acc: 0.8068 - val_loss: 1.9007 - val_acc: 0.4577\n",
      "Epoch 11/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.6115 - acc: 0.8222\n",
      "Epoch 00011: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 204us/sample - loss: 0.6119 - acc: 0.8220 - val_loss: 1.9441 - val_acc: 0.4617\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5679 - acc: 0.8412\n",
      "Epoch 00012: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 7s 202us/sample - loss: 0.5679 - acc: 0.8412 - val_loss: 2.0906 - val_acc: 0.4340\n",
      "Epoch 13/500\n",
      "36544/36805 [============================>.] - ETA: 0s - loss: 0.5165 - acc: 0.8605\n",
      "Epoch 00013: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 206us/sample - loss: 0.5168 - acc: 0.8604 - val_loss: 2.0346 - val_acc: 0.4612\n",
      "Epoch 14/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4805 - acc: 0.8716\n",
      "Epoch 00014: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 7s 204us/sample - loss: 0.4809 - acc: 0.8715 - val_loss: 2.0934 - val_acc: 0.4561\n",
      "Epoch 15/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.4537 - acc: 0.8800\n",
      "Epoch 00015: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 205us/sample - loss: 0.4539 - acc: 0.8799 - val_loss: 2.1286 - val_acc: 0.4619\n",
      "Epoch 16/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4265 - acc: 0.8904\n",
      "Epoch 00016: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 206us/sample - loss: 0.4266 - acc: 0.8903 - val_loss: 2.1804 - val_acc: 0.4514\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3842 - acc: 0.9070\n",
      "Epoch 00017: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 206us/sample - loss: 0.3842 - acc: 0.9070 - val_loss: 2.3505 - val_acc: 0.4363\n",
      "Epoch 18/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3592 - acc: 0.9148\n",
      "Epoch 00018: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 7s 198us/sample - loss: 0.3602 - acc: 0.9143 - val_loss: 2.3500 - val_acc: 0.4391\n",
      "Epoch 19/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3386 - acc: 0.9191\n",
      "Epoch 00019: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 7s 197us/sample - loss: 0.3386 - acc: 0.9190 - val_loss: 2.2984 - val_acc: 0.4512\n",
      "Epoch 20/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3158 - acc: 0.9280\n",
      "Epoch 00020: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 204us/sample - loss: 0.3160 - acc: 0.9279 - val_loss: 2.3504 - val_acc: 0.4570\n",
      "Epoch 21/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2982 - acc: 0.9327\n",
      "Epoch 00021: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 208us/sample - loss: 0.2983 - acc: 0.9327 - val_loss: 2.4080 - val_acc: 0.4547\n",
      "Epoch 22/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2819 - acc: 0.9377\n",
      "Epoch 00022: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 211us/sample - loss: 0.2821 - acc: 0.9375 - val_loss: 2.4392 - val_acc: 0.4526\n",
      "Epoch 23/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2591 - acc: 0.9459\n",
      "Epoch 00023: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 211us/sample - loss: 0.2591 - acc: 0.9459 - val_loss: 2.4672 - val_acc: 0.4559\n",
      "Epoch 24/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.2410 - acc: 0.9521\n",
      "Epoch 00024: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 212us/sample - loss: 0.2411 - acc: 0.9520 - val_loss: 2.4629 - val_acc: 0.4605\n",
      "Epoch 25/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.2298 - acc: 0.9553\n",
      "Epoch 00025: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 209us/sample - loss: 0.2297 - acc: 0.9553 - val_loss: 2.5833 - val_acc: 0.4479\n",
      "Epoch 26/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2202 - acc: 0.9579\n",
      "Epoch 00026: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 208us/sample - loss: 0.2203 - acc: 0.9579 - val_loss: 2.6370 - val_acc: 0.4470\n",
      "Epoch 27/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.2016 - acc: 0.9641\n",
      "Epoch 00027: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 215us/sample - loss: 0.2019 - acc: 0.9640 - val_loss: 2.6467 - val_acc: 0.4486\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1969 - acc: 0.9639\n",
      "Epoch 00028: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 209us/sample - loss: 0.1969 - acc: 0.9639 - val_loss: 2.6891 - val_acc: 0.4440\n",
      "Epoch 29/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1827 - acc: 0.9683\n",
      "Epoch 00029: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 209us/sample - loss: 0.1828 - acc: 0.9683 - val_loss: 2.7005 - val_acc: 0.4477\n",
      "Epoch 30/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1717 - acc: 0.9732\n",
      "Epoch 00030: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 212us/sample - loss: 0.1716 - acc: 0.9732 - val_loss: 2.7663 - val_acc: 0.4475\n",
      "Epoch 31/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1625 - acc: 0.9761\n",
      "Epoch 00031: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 218us/sample - loss: 0.1625 - acc: 0.9761 - val_loss: 2.7977 - val_acc: 0.4533\n",
      "Epoch 32/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1614 - acc: 0.9746\n",
      "Epoch 00032: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 210us/sample - loss: 0.1616 - acc: 0.9745 - val_loss: 2.8492 - val_acc: 0.4526\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1608 - acc: 0.9736\n",
      "Epoch 00033: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 210us/sample - loss: 0.1608 - acc: 0.9736 - val_loss: 2.9018 - val_acc: 0.4400\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1367 - acc: 0.9813\n",
      "Epoch 00034: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 211us/sample - loss: 0.1367 - acc: 0.9813 - val_loss: 2.9822 - val_acc: 0.4382\n",
      "Epoch 35/500\n",
      "36544/36805 [============================>.] - ETA: 0s - loss: 0.1320 - acc: 0.9822\n",
      "Epoch 00035: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 211us/sample - loss: 0.1323 - acc: 0.9822 - val_loss: 2.8762 - val_acc: 0.4540\n",
      "Epoch 36/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1258 - acc: 0.9827\n",
      "Epoch 00036: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 211us/sample - loss: 0.1258 - acc: 0.9827 - val_loss: 2.9582 - val_acc: 0.4528\n",
      "Epoch 37/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.1229 - acc: 0.9827\n",
      "Epoch 00037: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 214us/sample - loss: 0.1227 - acc: 0.9827 - val_loss: 3.0319 - val_acc: 0.4407\n",
      "Epoch 38/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.1153 - acc: 0.9847\n",
      "Epoch 00038: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 214us/sample - loss: 0.1153 - acc: 0.9846 - val_loss: 3.0553 - val_acc: 0.4340\n",
      "Epoch 39/500\n",
      "36544/36805 [============================>.] - ETA: 0s - loss: 0.1153 - acc: 0.9835\n",
      "Epoch 00039: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 209us/sample - loss: 0.1152 - acc: 0.9836 - val_loss: 3.0715 - val_acc: 0.4503\n",
      "Epoch 40/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0976 - acc: 0.9893\n",
      "Epoch 00040: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 209us/sample - loss: 0.0977 - acc: 0.9892 - val_loss: 3.1611 - val_acc: 0.4393\n",
      "Epoch 41/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1029 - acc: 0.9877\n",
      "Epoch 00041: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 210us/sample - loss: 0.1029 - acc: 0.9877 - val_loss: 3.2423 - val_acc: 0.4305\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1023 - acc: 0.9871\n",
      "Epoch 00042: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 211us/sample - loss: 0.1023 - acc: 0.9871 - val_loss: 3.1800 - val_acc: 0.4391\n",
      "Epoch 43/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0934 - acc: 0.9891\n",
      "Epoch 00043: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 212us/sample - loss: 0.0934 - acc: 0.9891 - val_loss: 3.2613 - val_acc: 0.4430\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1006 - acc: 0.9862\n",
      "Epoch 00044: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 210us/sample - loss: 0.1006 - acc: 0.9863 - val_loss: 3.2189 - val_acc: 0.4414\n",
      "Epoch 45/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0885 - acc: 0.9900\n",
      "Epoch 00045: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 211us/sample - loss: 0.0885 - acc: 0.9899 - val_loss: 3.2533 - val_acc: 0.4396\n",
      "Epoch 46/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0834 - acc: 0.9906\n",
      "Epoch 00046: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 210us/sample - loss: 0.0835 - acc: 0.9905 - val_loss: 3.2941 - val_acc: 0.4391\n",
      "Epoch 47/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.0849 - acc: 0.9898\n",
      "Epoch 00047: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 210us/sample - loss: 0.0847 - acc: 0.9898 - val_loss: 3.3165 - val_acc: 0.4365\n",
      "Epoch 48/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0715 - acc: 0.9937\n",
      "Epoch 00048: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 209us/sample - loss: 0.0715 - acc: 0.9937 - val_loss: 3.4186 - val_acc: 0.4370\n",
      "Epoch 49/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0706 - acc: 0.9931\n",
      "Epoch 00049: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 210us/sample - loss: 0.0706 - acc: 0.9931 - val_loss: 3.3855 - val_acc: 0.4463\n",
      "Epoch 50/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0791 - acc: 0.9901\n",
      "Epoch 00050: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 214us/sample - loss: 0.0801 - acc: 0.9899 - val_loss: 3.4524 - val_acc: 0.4335\n",
      "Epoch 51/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0808 - acc: 0.9891\n",
      "Epoch 00051: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 208us/sample - loss: 0.0812 - acc: 0.9890 - val_loss: 3.5019 - val_acc: 0.4356\n",
      "Epoch 52/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.0685 - acc: 0.9927\n",
      "Epoch 00052: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 209us/sample - loss: 0.0684 - acc: 0.9927 - val_loss: 3.5044 - val_acc: 0.4349\n",
      "Epoch 53/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0641 - acc: 0.9936\n",
      "Epoch 00053: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 211us/sample - loss: 0.0641 - acc: 0.9936 - val_loss: 3.4434 - val_acc: 0.4477\n",
      "Epoch 54/500\n",
      "36544/36805 [============================>.] - ETA: 0s - loss: 0.0629 - acc: 0.9938\n",
      "Epoch 00054: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 210us/sample - loss: 0.0630 - acc: 0.9938 - val_loss: 3.5209 - val_acc: 0.4349\n",
      "Epoch 55/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.0630 - acc: 0.9939\n",
      "Epoch 00055: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 211us/sample - loss: 0.0629 - acc: 0.9939 - val_loss: 3.5095 - val_acc: 0.4421\n",
      "Epoch 56/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.0623 - acc: 0.9932\n",
      "Epoch 00056: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 212us/sample - loss: 0.0623 - acc: 0.9933 - val_loss: 3.5274 - val_acc: 0.4403\n",
      "Epoch 57/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0602 - acc: 0.9939\n",
      "Epoch 00057: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 209us/sample - loss: 0.0601 - acc: 0.9939 - val_loss: 3.6125 - val_acc: 0.4444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0560 - acc: 0.9940\n",
      "Epoch 00058: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 209us/sample - loss: 0.0560 - acc: 0.9940 - val_loss: 3.5909 - val_acc: 0.4391\n",
      "Epoch 59/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0603 - acc: 0.9929\n",
      "Epoch 00059: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 210us/sample - loss: 0.0602 - acc: 0.9930 - val_loss: 3.7059 - val_acc: 0.4279\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0524 - acc: 0.9945\n",
      "Epoch 00060: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 212us/sample - loss: 0.0524 - acc: 0.9945 - val_loss: 3.6987 - val_acc: 0.4358\n",
      "Epoch 61/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0532 - acc: 0.9954\n",
      "Epoch 00061: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 217us/sample - loss: 0.0531 - acc: 0.9954 - val_loss: 3.6980 - val_acc: 0.4426\n",
      "Epoch 62/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0496 - acc: 0.9951\n",
      "Epoch 00062: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 209us/sample - loss: 0.0498 - acc: 0.9950 - val_loss: 3.8046 - val_acc: 0.4153\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0573 - acc: 0.9923\n",
      "Epoch 00063: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 209us/sample - loss: 0.0573 - acc: 0.9923 - val_loss: 3.7156 - val_acc: 0.4354\n",
      "Epoch 64/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.0459 - acc: 0.9967\n",
      "Epoch 00064: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 211us/sample - loss: 0.0462 - acc: 0.9965 - val_loss: 3.7836 - val_acc: 0.4270\n",
      "Epoch 65/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.0521 - acc: 0.9939\n",
      "Epoch 00065: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 212us/sample - loss: 0.0521 - acc: 0.9939 - val_loss: 3.8697 - val_acc: 0.4293\n",
      "Epoch 66/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.0502 - acc: 0.9945\n",
      "Epoch 00066: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 211us/sample - loss: 0.0501 - acc: 0.9945 - val_loss: 3.7861 - val_acc: 0.4393\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0475 - acc: 0.9948\n",
      "Epoch 00067: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 214us/sample - loss: 0.0476 - acc: 0.9947 - val_loss: 3.8647 - val_acc: 0.4286\n",
      "Epoch 68/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.0508 - acc: 0.9942\n",
      "Epoch 00068: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 210us/sample - loss: 0.0507 - acc: 0.9942 - val_loss: 3.8078 - val_acc: 0.4300\n",
      "Epoch 69/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0434 - acc: 0.9961\n",
      "Epoch 00069: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 213us/sample - loss: 0.0434 - acc: 0.9960 - val_loss: 3.8106 - val_acc: 0.4361\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0456 - acc: 0.9945\n",
      "Epoch 00070: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 211us/sample - loss: 0.0456 - acc: 0.9945 - val_loss: 3.9774 - val_acc: 0.4253\n",
      "Epoch 71/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0455 - acc: 0.9952\n",
      "Epoch 00071: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 212us/sample - loss: 0.0454 - acc: 0.9952 - val_loss: 3.8902 - val_acc: 0.4305\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0446 - acc: 0.9951\n",
      "Epoch 00072: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 214us/sample - loss: 0.0446 - acc: 0.9951 - val_loss: 3.9132 - val_acc: 0.4330\n",
      "Epoch 73/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0424 - acc: 0.9953\n",
      "Epoch 00073: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 209us/sample - loss: 0.0424 - acc: 0.9953 - val_loss: 3.9583 - val_acc: 0.4167\n",
      "Epoch 74/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0430 - acc: 0.9949\n",
      "Epoch 00074: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 208us/sample - loss: 0.0431 - acc: 0.9949 - val_loss: 3.9710 - val_acc: 0.4284\n",
      "Epoch 75/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.0467 - acc: 0.9939\n",
      "Epoch 00075: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 214us/sample - loss: 0.0468 - acc: 0.9938 - val_loss: 3.9653 - val_acc: 0.4309\n",
      "Epoch 76/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0413 - acc: 0.9951\n",
      "Epoch 00076: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 211us/sample - loss: 0.0413 - acc: 0.9951 - val_loss: 3.9834 - val_acc: 0.4314\n",
      "Epoch 77/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0377 - acc: 0.9966\n",
      "Epoch 00077: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 211us/sample - loss: 0.0376 - acc: 0.9966 - val_loss: 3.9822 - val_acc: 0.4365\n",
      "Epoch 78/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0374 - acc: 0.9965\n",
      "Epoch 00078: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 212us/sample - loss: 0.0374 - acc: 0.9964 - val_loss: 4.0139 - val_acc: 0.4272\n",
      "Epoch 79/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.0413 - acc: 0.9955\n",
      "Epoch 00079: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 212us/sample - loss: 0.0413 - acc: 0.9955 - val_loss: 4.0330 - val_acc: 0.4337\n",
      "Epoch 80/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0403 - acc: 0.9953\n",
      "Epoch 00080: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 211us/sample - loss: 0.0402 - acc: 0.9953 - val_loss: 4.0650 - val_acc: 0.4356\n",
      "Epoch 81/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.0341 - acc: 0.9970\n",
      "Epoch 00081: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 210us/sample - loss: 0.0346 - acc: 0.9970 - val_loss: 4.1968 - val_acc: 0.4188\n",
      "Epoch 82/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.0344 - acc: 0.9966\n",
      "Epoch 00082: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 211us/sample - loss: 0.0344 - acc: 0.9966 - val_loss: 4.1819 - val_acc: 0.4205\n",
      "Epoch 83/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0334 - acc: 0.9973\n",
      "Epoch 00083: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 209us/sample - loss: 0.0335 - acc: 0.9973 - val_loss: 4.1491 - val_acc: 0.4249\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0384 - acc: 0.9948\n",
      "Epoch 00084: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 213us/sample - loss: 0.0385 - acc: 0.9948 - val_loss: 4.1409 - val_acc: 0.4300\n",
      "Epoch 85/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0353 - acc: 0.9960\n",
      "Epoch 00085: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 210us/sample - loss: 0.0353 - acc: 0.9960 - val_loss: 4.0864 - val_acc: 0.4323\n",
      "Epoch 86/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0290 - acc: 0.9977\n",
      "Epoch 00086: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 211us/sample - loss: 0.0289 - acc: 0.9977 - val_loss: 4.1067 - val_acc: 0.4263\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0366 - acc: 0.9957\n",
      "Epoch 00087: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 210us/sample - loss: 0.0366 - acc: 0.9957 - val_loss: 4.1535 - val_acc: 0.4305\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0323 - acc: 0.9965\n",
      "Epoch 00088: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 210us/sample - loss: 0.0323 - acc: 0.9965 - val_loss: 4.2212 - val_acc: 0.4232\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0393 - acc: 0.9945\n",
      "Epoch 00089: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 213us/sample - loss: 0.0393 - acc: 0.9945 - val_loss: 4.2622 - val_acc: 0.4249\n",
      "Epoch 90/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0344 - acc: 0.9961\n",
      "Epoch 00090: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 210us/sample - loss: 0.0344 - acc: 0.9961 - val_loss: 4.2600 - val_acc: 0.4328\n",
      "Epoch 91/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0325 - acc: 0.9964\n",
      "Epoch 00091: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 213us/sample - loss: 0.0325 - acc: 0.9964 - val_loss: 4.2250 - val_acc: 0.4239\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0311 - acc: 0.9965\n",
      "Epoch 00092: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 212us/sample - loss: 0.0312 - acc: 0.9964 - val_loss: 4.2324 - val_acc: 0.4246\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0332 - acc: 0.9964\n",
      "Epoch 00093: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 212us/sample - loss: 0.0333 - acc: 0.9964 - val_loss: 4.2677 - val_acc: 0.4249\n",
      "Epoch 94/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.0356 - acc: 0.9951\n",
      "Epoch 00094: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 209us/sample - loss: 0.0355 - acc: 0.9951 - val_loss: 4.3352 - val_acc: 0.4205\n",
      "Epoch 95/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.0341 - acc: 0.9964\n",
      "Epoch 00095: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 210us/sample - loss: 0.0340 - acc: 0.9964 - val_loss: 4.2784 - val_acc: 0.4265\n",
      "Epoch 96/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0256 - acc: 0.9976\n",
      "Epoch 00096: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 211us/sample - loss: 0.0256 - acc: 0.9976 - val_loss: 4.3100 - val_acc: 0.4205\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0375 - acc: 0.9948\n",
      "Epoch 00097: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 209us/sample - loss: 0.0376 - acc: 0.9948 - val_loss: 4.2977 - val_acc: 0.4319\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0310 - acc: 0.9961\n",
      "Epoch 00098: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 209us/sample - loss: 0.0312 - acc: 0.9961 - val_loss: 4.3322 - val_acc: 0.4293\n",
      "Epoch 99/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0368 - acc: 0.9949\n",
      "Epoch 00099: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 211us/sample - loss: 0.0367 - acc: 0.9949 - val_loss: 4.3412 - val_acc: 0.4200\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0249 - acc: 0.9980\n",
      "Epoch 00100: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 210us/sample - loss: 0.0249 - acc: 0.9980 - val_loss: 4.3381 - val_acc: 0.4309\n",
      "Epoch 101/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0289 - acc: 0.9968\n",
      "Epoch 00101: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 211us/sample - loss: 0.0288 - acc: 0.9968 - val_loss: 4.4187 - val_acc: 0.4107\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0254 - acc: 0.9975\n",
      "Epoch 00102: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 210us/sample - loss: 0.0255 - acc: 0.9974 - val_loss: 4.4709 - val_acc: 0.4111\n",
      "Epoch 103/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0425 - acc: 0.9926\n",
      "Epoch 00103: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 209us/sample - loss: 0.0426 - acc: 0.9926 - val_loss: 4.4257 - val_acc: 0.4149\n",
      "Epoch 104/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.0298 - acc: 0.9961\n",
      "Epoch 00104: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 208us/sample - loss: 0.0297 - acc: 0.9961 - val_loss: 4.4351 - val_acc: 0.4186\n",
      "Epoch 105/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0290 - acc: 0.9966\n",
      "Epoch 00105: val_loss did not improve from 1.74330\n",
      "36805/36805 [==============================] - 8s 210us/sample - loss: 0.0290 - acc: 0.9966 - val_loss: 4.3743 - val_acc: 0.4258\n",
      "\n",
      "1D_CNN_BN_1_only_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEKCAYAAAARnO4WAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzt3Xd81dX9+PHXuTs7ISRhBAwIsiEsiyKg4kBR6qhi667F2tpaR62oHXx/1lqrVmuLgzrq3qh1Yh2AW4YgU9kQRvbO3ff8/jj3JkECJJCbm9z7fj4e95Hcez/j/bk3ed9zz+d83kdprRFCCBH/LLEOQAghRMeQhC+EEAlCEr4QQiQISfhCCJEgJOELIUSCkIQvhBAJQhK+EEIkCEn4QgiRICThCyFEgrDFOoDmunfvrgsKCmIdhhBCdBnLli0r01rntGbZTpXwCwoKWLp0aazDEEKILkMpta21y0qXjhBCJAhJ+EIIkSAk4QshRILoVH34LfH7/RQVFeHxeGIdSpfkcrnIz8/HbrfHOhQhRIx1+oRfVFREWloaBQUFKKViHU6XorWmvLycoqIi+vXrF+twhBAx1um7dDweD9nZ2ZLsD4FSiuzsbPl2JIQAukDCByTZHwZ57YQQEV0i4QshRJdUVARz50J9fawjASThH1RVVRUPPPDAIa17+umnU1VV1erl58yZw913331I+xJCdDKLFsGYMfCrX0FhIXz2WawjkoR/MAdK+IFA4IDrvv3222RmZkYjLCFEZ6U13HcfTJ0K3brBk0+C3w+TJsEll8A558DIkXDccbB1a4eGJgn/IGbPns2mTZsoLCzkxhtvZOHChUyaNIkZM2YwdOhQAM466yzGjh3LsGHDmDdvXuO6BQUFlJWVsXXrVoYMGcKsWbMYNmwYp5xyCm63+4D7XbFiBRMmTGDkyJGcffbZVFZWAnD//fczdOhQRo4cyQUXXADAokWLKCwspLCwkNGjR1NbWxulV0MIcVB/+xtcdx2ceSZ89RVcfDF88w1cfjnMnw/r1sERR8CaNSbpr13bYaEprXWH7exgxo0bp79fS2fdunUMGTIEgA0brqWubkW77jM1tZCBA+/b7/Nbt27ljDPOYPXq1QAsXLiQ6dOns3r16sahjhUVFXTr1g2328348eNZtGgR2dnZjbWB6urqGDBgAEuXLqWwsJDzzz+fGTNmcNFFF+21rzlz5pCamspvf/tbRo4cyT//+U+mTJnCH//4R2pqarjvvvvo1asXW7Zswel0UlVVRWZmJmeeeSazZ89m4sSJ1NXV4XK5sNmaRtw2fw2FEFH05pswYwacfz48+yxYDtCmXrUKTj0VvF54+234wQ8OaZdKqWVa63GtWVZa+Ifg6KOP3mtc+/3338+oUaOYMGECO3bsYMOGDfus069fPwoLCwEYO3YsWw/wVa66upqqqiqmTJkCwKWXXsrixYsBGDlyJBdeeCFPP/10Y1KfOHEi119/Pffffz9VVVV7JXshRAdZtw5+8hMYPRoee+zAyR5gxAj45BPIyoIzzoC6uqiH2KUyw4Fa4h0pJSWl8feFCxfy/vvv8/nnn5OcnMzxxx/f4rh3p9PZ+LvVaj1ol87+vPXWWyxevJg33niD22+/nVWrVjF79mymT5/O22+/zcSJE1mwYAGDBw8+pO0LkXAWLQKnEyZMOPiyoRC8+67potm0CbZvB7sdUlPhiy8gKQleew2Sk1u37/79TdJfvdpsI8q6VMKPhbS0tAP2iVdXV5OVlUVycjLr16/niy++OOx9ZmRkkJWVxccff8ykSZN46qmnmDJlCqFQiB07dnDCCSdw3HHH8fzzz1NXV0d5eTkjRoxgxIgRLFmyhPXr10vCF6I1XnkFZs40vz/0EPzsZ+Z3rU0i1hpGjYL0dHjjDfj9701XDEBODhQUmA+B2lpISTHdOH36tC2GHj3MrQNIwj+I7OxsJk6cyPDhwznttNOYPn36Xs9PmzaNhx56iCFDhjBo0CAmtKaV0ApPPPEEV111FQ0NDfTv35/HH3+cYDDIRRddRHV1NVprrrnmGjIzM/nDH/7ARx99hMViYdiwYZx22mntEoMQce2//4ULLjB95+npMGuWabWPHQt/+Qt8/XXTst27Q1kZDBwIzz0Hp59u1uliutRJW3Fo5DUUcc/vhxUrwGYz3Sq5uWZIZEu0hpdegosuMv3t771nWue/+hU8/LBZZuBAmD0bevaElSvNSJrJk+HSS00XTifSlpO20sIXQnRtn34KP/+5GeYYYbfDVVfBrbdCXl7T4198ATfdBIsXw7hxpj8+I8M89+CDcOyx5gPjnHPAajWPx9E3Zkn4QoiuqbYWbrgB/v1v6NsX/vMfyMwEtxs+/BAeeMCMljnzTNMds2MHfPut+QCYO9f01zscTdtTylwYFcck4QshOsaGDVBZacoMNE+0h6K+HqZPN+UKfvtbmDPHdMtEXHCBefwPfzAnX3v3NsMgr7gCfvGLDhkR0xlJwhdCRF99vbmqtKQEXC4YP95cnPSzn5n7beF2m4ubPv3UnEA9//yWlzvqKHjhhcOPPY7IhVdCiOh78EGT7O++G375S3OR0a9/DUceCfffb8azB4MH305Dg+lf/+gjeOKJ/Sd70SJJ+EKI6KqvN/VlTjnF9Lnfcw8sX26S9oAB8JvfmNoyLpf5AHjkkZa3s2JF04nWefPMKBvRJpLwoyB1P/2D+3tciLj24INQWgp/+tPejx9/vLnK9csvTQL/3e/MBUizZsE110CkGm1NDdx5Jxx9NFRXw//+13SBlGgT6cMXQkRP89b9sce2vMzRR5sbmCT/u9/Bvfea8gUulxlx4/ebrpx58yA7u+PijzPSwj+I2bNnM3fu3Mb7kUlK6urqmDp1KmPGjGHEiBG8/vrrrd6m1pobb7yR4cOHM2LECF4In1javXs3kydPprCwkOHDh/Pxxx8TDAa57LLLGpe999572/0YhYiKUAj++teWW/f7Y7PB3/8Ojz5qWv6bN5sun08+gZdflmR/mKLewldKWYGlwE6t9RmHtbFrrzX9eO2psNBMVrAfM2fO5Nprr+Xqq68G4MUXX2TBggW4XC5effVV0tPTKSsrY8KECcyYMaNVc8jOnz+fFStWsHLlSsrKyhg/fjyTJ0/m2Wef5dRTT+XWW28lGAzS0NDAihUr2LlzZ2N55rbMoCVETASDpkbNn/9s6s7MmLH/1v3+/PSnZky81WrGx4t20RFdOr8B1gFdr/AEMHr0aEpKSti1axelpaVkZWXRp08f/H4/t9xyC4sXL8ZisbBz506Ki4vp0YoiSJ988gk//vGPsVqt5OXlMWXKFJYsWcL48eP56U9/it/v56yzzqKwsJD+/fuzefNmfv3rXzN9+nROOeWUDjhqIdqovh7eecfUdX/nHdizBwYPhqeeMmPiD4WU+W53UX1FlVL5wHTgduD6w97gAVri0XTeeefx8ssvs2fPHmaGK+s988wzlJaWsmzZMux2OwUFBS2WRW6LyZMns3jxYt566y0uu+wyrr/+ei655BJWrlzJggULeOihh3jxxRd57LHH2uOwhDh8VVXwz3+a/82KCnOl67Rp8KMfwVlnNZUnEJ1CtD9C7wN+B6TtbwGl1JXAlQB9+/aNcjiHZubMmcyaNYuysjIWLVoEmLLIubm52O12PvroI7Zt29bq7U2aNImHH36YSy+9lIqKChYvXsxdd93Ftm3byM/PZ9asWXi9XpYvX87pp5+Ow+Hg3HPPZdCgQfvMkiVEq2htSg9MmmSGQh5s2e3bYeNGMzlHbq557MMPTaGxb74xidxmg+++MyUOzjgDrr/ebF9a5p1W1N4ZpdQZQInWeplS6vj9Lae1ngfMA1MtM1rxHI5hw4ZRW1tL79696dmzJwAXXnghZ555JiNGjGDcuHFtqj9/9tln8/nnnzNq1CiUUvztb3+jR48ePPHEE9x1113Y7XZSU1N58skn2blzJ5dffjmhUAiAO+64IyrHKOLckiWmX3zAAFi2rOXSvkuWwB//aOZhrahoeTt5eaacsFJm5MyIEeakang2N9G5Ra08slLqDuBiIAC4MH3487XW+22iSnnk6JDXUDBrlulPDwTg3HPh+eebTob6/XDbbaYGfF6eqVEzejQMGmTGwJeUmHlXJ082Cf5gU/eJDtUpyiNrrW8Gbg4HdDzw2wMleyFElNTWmpozP/mJqS9z880wZYq5P3++6YNfscKMivnHP0w/vIhL0tkmRLx7/nkzimbWLNMd8/HHZojzddeBz2fKGcyfD2efHetIRZR1SMLXWi8EFnbEvoSIa/PmmZOpd9659/j0114zE3eceuq+6/z73zBsmJmkWyl48knTmj/qKPjxj03lShnrnhCkhS9EV/G//5lZnLQ2V5zedJN5/I03TNkBrU0Zg9/+timBr1xpTsbed1/TY9nZ8NZbsTkGEVOS8IXoCoqKTJ/70KHmgqZbbjGTbefmmsfHjoV+/Uwdmk2b4I47wOOBf/0LnE64+OJYH4HoBCThC9FZvP22qSPz3HOQk9P0uN8PM2eaBP7KK2b2pvXrzRWsKSlmiOXrr5tKk0ceaerXRCbjBrjwwv1P6C0SioyvOoiqqioeeOCBQ1r39NNPl9o3onWqqsz0ex98AJdeagqPgemm+dWvzFR+jzxihkqmppqTrH6/KUz23/9Cr15muOQdd8Cbb5qa8w8+aIZixugKddEJaa07zW3s2LH6+9auXbvPYx1py5YtetiwYS0+5/f7OziaQxPr11C0wtVXa22xaP3LX2oNWt91l9ahkNbXXmvu33zzvuusXKn1smUdH6voVIClupU5Vlr4BzF79mw2bdpEYWEhN954IwsXLmTSpEnMmDGDoUOHAnDWWWcxduxYhg0bxrx58xrXLSgooKysjK1btzJkyBBmzZrFsGHDOOWUU3C73fvs64033uAHP/gBo0eP5qSTTqK4uBiAuro6Lr/8ckaMGMHIkSN55ZVXAHj33XcZM2YMo0aNYurUqR3waoioWLIEHngArr7a9Lmfe64ZK3/RRaZ1/pvfwO2377veyJEwZkzHxyu6rKhdaXsoDnalbQyqI7N161bOOOOMxvLECxcuZPr06axevZp+/foBUFFRQbdu3XC73YwfP55FixaRnZ1NQUEBS5cupa6ujgEDBrB06VIKCws5//zzmTFjxj51cSorK8nMzEQpxSOPPMK6deu45557uOmmm/B6vdwXDrSyspJAIMCYMWNYvHgx/fr1a4yhJXKlbSfg9ZouluRkyM+Hnj3B4TA1aS68EHbvhnXrICPDdO+MHg1bt8LPf27Wk2GTYj86xZW28ezoo49uTPYA999/P6+++ioAO3bsYMOGDWR/b6KGfv36URiuNzJ27Fi2bt26z3aLioqYOXMmu3fvxufzNe7j/fff5/nnn29cLisrizfeeIPJkyc3LrO/ZC86ib//3Yys2Z8XXjDJHsyVrm+9ZYqV/fKXkuxFu+lSCb+znHtKSUlp/H3hwoW8//77fP755yQnJ3P88ce3WCbZ6XQ2/m61Wlvs0vn1r3/N9ddfz4wZM1i4cCFz5syJSvyig+3ebbpkZswwXTZFRVBcbE66BoNmdM2UKXuvM3SouQnRjrpUwo+FtLQ0amtr9/t8dXU1WVlZJCcns379er744otD3ld1dTW9e/cG4Iknnmh8/OSTT2bu3Ll7delMmDCBX/7yl2zZsuWgXToixm691ZQwuOce6NPH3ISIATlpexDZ2dlMnDiR4cOHc+ONN+7z/LRp0wgEAgwZMoTZs2czYcKEQ97XnDlzOO+88xg7dizdu3dvfPz3v/89lZWVDB8+nFGjRvHRRx+Rk5PDvHnzOOeccxg1alTjxCyiE/jkE1NPHkwp4v/8x5yAOlgdeiGirEudtBWHRl7DDvT886Y+DcDRR5uiZaWlsGFDyzXohThMbTlpKy18IdrL2rXws5+ZCbv/+ldz8dSaNabQmSR70QlIH74Q7aG21oyfT0mBl14yV77edJN5PG2/M3wK0aGkhS9EW5SUmDldvV5z3+eDhQtNrZvvvjPDK3v1alpekr3oRKSFL0TEunXw7rvQ0GBuRx8NP/xh0/Nr18Ixx5hp/2w2U6isqMj001utcO+9cPzxMQtfiIORhC+E1maSkGuuaWq5K2Ue/+tfTddMSYmZ6zUpCe6/35yEXbsWpk6FU04xiT5y4ZQQnZQkfJHY6upM+YJnn4WTTzYVKXv2NMn+sstg9mwzyuazz2DPHli0yLT8heiCJOFHQWpqKnV1dbEOQxxMTY1pnS9ZAn/+sylYZml2WuvppyEry1wwBeZkrCR70YVJwheJYcUKU8/m5z+HiRPN6Jlp08yFUa+8Amedte86FosphTBokBlW+aMfdXzcQrQjGaVzELNnz2bu3LmN9+fMmcPdd99NXV0dU6dOZcyYMYwYMYLXX3/9oNvaXxnllsoc768ksjgEXq+ZBvCpp+C440yrfto0+Oorc6FUS8k+QinTt3/ZZR0WrhDR0qVa+Ne+ey0r9rRvfeTCHoXcN23/VdlmzpzJtddey9VXXw3Aiy++yIIFC3C5XLz66qukp6dTVlbGhAkTmDFjBuoAlQ0fe+yxvcoon3vuuYRCIWbNmrVXmWOA2267jYyMDFatWgWY+jniEP3lL2YEzssvm5LDf/sblJebfvtzz411dEJ0mC6V8GNh9OjRlJSUsGvXLkpLS8nKyqJPnz74/X5uueUWFi9ejMViYefOnRQXF9OjR4/9bqulMsqlpaUtljluqSSyOASrV5tp/y68sCm5X3WVOQF75JGxjU2IDtalEv6BWuLRdN555/Hyyy+zZ8+exiJlzzzzDKWlpSxbtgy73U5BQUGLZZEjWltGWbSjQMCUOsjI2Lu2dkqKJHuRkKQPvxVmzpzJ888/z8svv8x5550HmFLGubm52O12PvroI7Zt23bAbeyvjPKECRNYvHgxW7ZsAWjs0omURI6QLp1W0ho+/RR+/Wszs9SXX8I//gHNqo8Kkagk4bfCsGHDqK2tpXfv3vTs2ROACy+8kKVLlzJixAiefPJJBg8efMBt7K+M8v7KHLdUElkcxObN5uKo444z4+mPOw5ef72peqUQCU7KIyeAuHkNGxrguutg0iTTJx85Qe7xmLIG/+//mZIH//d/MGuW1LERCUHKI4uuzeOBs882M0VFGiRam/74efPg4ovhtNNg0yZTEmHgQDNf7Omnm9E4118vyV6IFnSpk7YiAWhtLo567TVzKymBhx4yF0099xzcdpuZ5Hv27KYZpCZMgCeegBNPjG3sQnRyXSLha60POL5d7F9n6rJrlX/8A558EubMMRN833abKVT28cdw3nmm1a8UnHkmzJ1runfOOKOpe0cIsV+dPuG7XC7Ky8vJzs6WpN9GWmvKy8txuVyxDqV13n8fbrjBdOf84Q+mtEFaGvzudzByJDz+eFNiP+IIcwGVEKLVOv1JW7/fT1FRkYxZP0Qul4v8/HzsdnusQzmwnTth1Cjo0QM+/3zvPviPP4bBgyEnJ3bxCdFJteWkbadv4dvt9sarUEWcCgbNqBuPxxQy+/4J10mTYhOXEHGm0yd8kQBuv93Umf/Pf0xlSiFEVEjCF7ETDMLbb5tx8xddBJdcEuuIhIhrUUv4SikXsBhwhvfzstb6T9Han+giSkvNRCJvvw2ffALV1WYc/QMPyEgbIaIsmi18L3Ci1rpOKWUHPlFKvaO1/iKK+xSd1ZIlZqjle++ZomYDB8L558OUKeaCKblQSoioi1rC12b4T2SeP3v41nmGBImOs2qVmS/W5TJXwV54oRlmKYToUFHtw1dKWYFlwABgrtb6y2juT8SIzwfr17ecxHfsMGUQUlLMcMu+fTs+PiEEEOVaOlrroNa6EMgHjlZKDf/+MkqpK5VSS5VSS0tLS6MZjogGrc3J1lGj4M03936ustJMJVhbC++8I8leiBjrkOJpWusq4CNgWgvPzdNaj9Naj8uRC2u6njvugBdeMH3wV18N9fXm8UDA9NFv3Ghq4kgXjhAxF7WEr5TKUUplhn9PAk4G1kdrfyIG/vtfU9vmJz8xrfvt280QSzDFzd5/3xQ+O+GE2MYphACi24ffE3gi3I9vAV7UWr95kHVEV/Hpp+bk67hxZrKRpCS44gpT1dLphHvuMS3+yy+PdaRCiLBOX0tHdDJaw8MPwzXXmAJmCxdC797mufJyU/OmrAwmTzYt/M5ew0eILk4mQBHR4fGYmaR+8QszzHLJkqZkD5CdbVr7xx8PL74oyV6ITkYSvmid7dtNEbNHH4Xf/x7eeMNMRPJ9P/whfPQR5OV1fIxCiAOSWjri4N5/Hy64APx+M+Lmhz+MdURCiEMgLXyxf0uWmMlITj7Z1KlfskSSvRBdmCR8sa+KClPf5uijzUnZP/wBvvgCjjoq1pEJIQ6DdOmIvVVUwEknwdq1cOed5gStFDYTIi5IwhdNmif7114zZRGEEHFDEn6iW7nSzBm7apU5ObtzpyR7IeKUJPxEVVMDN91kSh8AZGXBiBHmoqqTToptbEKIqJCEn0iqquC770yr/v/+D3bvhuuugxtugF69ZMYpIeKcJPxEsHMnzJxp6t9EDB8O8+ebkThCiIQgCT/effEFnHOOqUl/222m22bgQBg0CKzWWEcnhOhAkvDj2UsvwUUXQX6+mUt2+D7zzwghEogk/HhVXm4KnY0ZY2rVZ2fHOiIhRIzJlbbx6vbbTTfOI49IshdCAJLw49OWLfCvf8FPfwrDhsU6GiFEJyEJPx7deivYbE3TDQohBNKHHx+WLoXlyyEjA9xueO45k/R79Yp1ZEKITkQSflfl88HLL8M//2mGXjaXkwO/+11s4hJCdFqS8Lua+npzIvbuu6GoyJQsvv9+mDHDPFdZaaYdTE+PdaRCiE5GEn5X8sorcNVVTZOEz5sHp54KFjkVI4Q4uFZlCqXUb5RS6cp4VCm1XCl1SrSDaw2tNRs2XENp6fxYhxI9WsPf/w7nnQf9+5vqlosWwWmnSbIXQrRaa1v4P9Va/0MpdSqQBVwMPAW8F7XIWkkpRXHxU4AmJ+ecWIfTdmvXmqJmFgskJ5vSB82LmAWDcP31ptvm3HPhqacgKSl28QohuqzWJvxIBjodeEprvUapzlNa0W7Pw+crjnUYbffNN1BYaFrwEeedB48+amaZqqiAn/wEFiwwSf+uu6RFL4Q4ZK1N+MuUUu8B/YCblVJpQCh6YbWNw9FFE/4jj4DdbvrmbTYzSficObB6Nfz5z/Db35pKlw8/DFdeGetohRBdXGsT/hVAIbBZa92glOoGXB69sNrG4cilvn51rMNoG48Hnn7aVLI84wzz2LRpMHEiXHCB6b7p3RsWL4Yf/CC2sQoh4kJr+weOAb7VWlcppS4Cfg9URy+stumSXTqvvWaGUF5xxd6Pn3giLFtmWvrLlkmyF0K0m9Ym/AeBBqXUKOAGYBPwZNSiaiOHI49AoJJQyBfrUFrv0UehoMAk+O/r0wf+9CfIy+vwsIQQ8au1CT+gtdbAD4F/aa3nAmnRC6ttHA6TGP3+0hhH0kpbt5oJwy+/XE7CCiE6TGuzTa1S6mbMcMy3lFIWwB69sNrGbs8F6DrdOo8/boZeXnZZrCMRQiSQ1ib8mYAXMx5/D5AP3BW1qNoo0sLvEgm/ogIeewxOOQX69o11NEKIBNKqhB9O8s8AGUqpMwCP1rpT9eFDF0j4X38NY8dCSYkUNxNCdLjWllY4H/gKOA84H/hSKfWjaAbWFpEuHb+/JMaR7IfW5iTtscdCIGBKI7R0slYIIaKotePwbwXGa61LAJRSOcD7wMvRCqwtbLZULJbkztnC/+ADmD3b1Kw/4QR4/nnIzY11VEKIBNTaPnxLJNmHlbdh3Q4R06ttn3oKHnxw7xIJNTWmZPFJJ0FxsTlR+7//SbIXQsRMa1v47yqlFgDPhe/PBN4+0ApKqT6Ysfp5gAbmaa3/caiBHozDkRebLp3ycvj5z81MU19+aUoWl5XB6afDmjVw551wzTXgcnV8bEII0UyrEr7W+kal1LnAxPBD87TWrx5ktQBwg9Z6ebj2zjKl1P+01msPI979sttz8Xi2RmPTB/bQQybZX3mlSfabNsGOHSbpv/mmqVcvhBCdQKsnQNFavwK80obldwO7w7/XKqXWAb2B9k/4WuOw5lDj+7LdN31AXi/861+mBs7DD8OUKeZiqsxMU69+7NiOjUcIIQ7ggAlfKVWL6Y7Z5ylAa61bNY+eUqoAGA20f0auroaTT6b7aVnsPqEUrUOY68I6wLPPwp49cMMN5v5PfgLjx5vSxj16dEwMQgjRSgdM+Frrwy6foJRKxXwzuFZrXdPC81cCVwL0PZQLkTIywOkk47GlqMkh/P5yHI6cw4y6FbSGe+6BkSNh6tSmxwcOjP6+hRDiEES1KayUsmOS/TNa6xbnINRaz9Naj9Naj8vJOcREfeON2IoqyFkYhYuv/vY3GDUKNm7c+/EFC8xJ2Rtu2HuGKiGE6KSilvDDM2I9CqzTWv89WvsB4IwzCB7Vlz7Pg9+3p/22u3w53HKLmZnquOPMT4C33zZljXv1MrXrhRCiC4hmC38iptjaiUqpFeHb6VHZk8VC4NpZpG0EPviwfbbp85niZnl58NlnZkaqKVPg/PNh+nTIyjKjcByO9tmfEEJEWdQSvtb6E6210lqP1FoXhm8HHLt/OCwXX4E3G5L+2eqBRAd2++2wapUZannMMfDJJ5CdDfPnw+9/byYnGT26ffYlhBAdoNXDMjs7W0oPtpxrof+870yRssNJxl99BX/5C1xyiWnNg5msZNkyc6FV//7tErMQQnSkTlUe4XAopSg9J49gmsNUotQtjSZthUWL4OSTTf/8vffu/VxGhiR7IUSXFTcJH8Ca3YM9vxpoZpN64YW2b+C118yVsb17my6cbt3aP0ghhIiRuEr4Dkcee85ymStcr7vOXJR1MKEQLFwIl14K554LhYWmfHGfPlGPVwghOlJcJXy7PQ9fsMTUtykuhj/+0XTtrFljrootKmpa2O835RAGDDBli197DX7xC1POODs7dgchhBBREjcnbQEcjlx8vhLttlHYAAAgAElEQVT0hLGoX/zC1Ll57jkoDU9ubrGYk7Annghz55qLqY45Bm67Dc4+G5KTY3sAQggRRXGW8PPQ2kswWIPt9tth82Yzjn7KFBg2DF5/3cwn+8YbMHy4+Tl9ulwpK4RICHGV8O32prltbZlHwTvv7L3A0UfDnDmwbp35ALBaOz5IIYSIkbjqw2+azPwAE6HY7abgmSR7IUSCibOEH5nMvBPObSuEEDEWVwm/eZeOEEKIvcVVwnc4crBYUqivj8osikII0aXFVcJXykpGxrFUV38c61CEEKLTiauED5CZOYX6+lX4/RWxDkUIITqVuEv4GRmTAU119SexDkUIITqVuEv4aWnjUcpJVdXiWIcihBCdStwlfKvVRXr6BKqrF8U6FCGE6FTiLuEDZGZOprZ2OYFAbaxDEUKITiMuE77pxw9RU/NZrEMRQohOI04T/jEoZaOqSrp1hBAiIi4TvtWaQlraODlxK4QQzcRlwgfTrVNb+xXBoDvWoQghRKcQtwk/M3MyWvupqfki1qEIIUSnELcJPyNjEko5KSt7LdahCCFEpxC3Cd9mS6d79x9SXPwMoZAv1uEIIUTMxW3CB+jR4zICgXLKy9+KdShCCBFzcZ3ws7JOxuHoyZ49/4l1KEIIEXNxnfAtFht5eRdTXv6WTIoihEh4cZ3wAXr0uBQIUlz8bKxDEUKImIr7hJ+SMpS0tKPZs+dxtNaxDkcIIWIm7hM+mJO39fWrqKtbHutQhBAiZhIi4efm/hiLJYWiovtiHYoQQsRMQiR8uz2TXr2upLj4OTyebbEORwghYiIhEj5Afv61KKWklS+ESFgJk/Bdrr7k5v6YXbv+LROcCyESUsIkfIA+fW4kFKpn164HYx2KEEJ0uKglfKXUY0qpEqXU6mjto61SU0fQrdtpFBXdL2WThRAJJ5ot/P8A06K4/UPSt+9s/P4Stm27LdahCCFEh7JFa8Na68VKqYJobf9QZWZOpkePn7J9+51kZ59JRsYxsQ5JiAMKhSAQALsdlGr7ukq1fb3WbNfvh2DQ3FcKLBawWs2t+f60Nsv6fE3xWCzm8WDQPOZwQFKSeTyyTihk1vF6zfp2O7hcZtlQyKyrtblvsey9v0DA/IzcWmKx7P2aRo4pEGj6abdDSgrYbHvHZbE0rRcMQn09NDQ0bdNqbTo+iwXS01uOMRQyN60hOfnw35eDiVrC78wGDLiXysoPWL/+UsaN+xqrNSXWIXU6Wps/4Pp6cLvNH7DTaf65bDZzPxSCoiLYvh327GlKAJF/4sg/R0QgYP55vV6zjM1mbm43lJSYW+QfO7KfSBLx+8HjMQnA4TD/hElJZt3qaqit3fsfKLKe1ua56mpzLBaLudlskJoKaWlmO5FjCgahpsbc3G6zzch2I69L85/NE13zBNY8yVitTduPJD6fz2w38no1305ysjk+i8XEXV1ttmexmOciCa95smgei8Vitu3xmJ/Q9Jo2j7Gl9zyynci2Iu9RJPbILbLd/YlsQ6mmD4XWcDqbEm9bRP5eIvG1ReS9a+k1ab79yAdX8/WsVvNeHozFAllZ5n2tqzN/X81fw7w88z8UbTFP+EqpK4ErAfr27dsh+7TZ0hk8+HFWrjyRzZtnM3DgPztkvx0hEIDKSpPkamubWh4NDVBRAcXF5g/L42lap67OrFNZCeXl5lZRceB/gPZksUD37pCTY/7hIwmx+YeH3d70gePzNR1XcjJkZJjkHUlqSjUlQ4DevWHo0KYWVKTlWF9vjr2hwST3SGssIwP69DEfBJHWmtXaFG+kZadUUwKNrNv8Bk2JJPLBEYkxkqAiH2qRlqjf3/R+BYOQmWluTqeJsb7eLGO1NrUyI7fIvoLBvV+vyHa9XrNc83UjMX7/mCKvXzDY9EFutzfdIsfQ/HWJvA6RD8jIMTVvLESONfKhHLn5fOb4mreSbbam9ez2pmPweps+HJUy63o85vnmr23z16albziRY/P7zfPNjy/y3vh8TQ2fyDI2297fBpKTmxoOzd/ryLGFQub/qbzcbCctzbT4k5Ob3ouUDmpzxjzha63nAfMAxo0b12HFbrKyTqB379+wc+c/yM7+Id26ndRRu26zUAiqqpr+aCIJubgYtmyBrVthxw6TyEtL9/8VNiI1tSn5aW3+2LKyzG3kSMjOhm7dzB9laqr5Qw4Gm75eR/6plTLJtG9f6NmzKQFE/oibJyQw910u808c+UeItOibJw4hRHTEPOHHUv/+d1BZuYBvv72cceNWYbdndngM9fWmS2THDti5symZl5aaZL5pk3luf1+LMzOhoMDcjjkGevQwCTstzdxSUswtOdks26NHx7UmDub7LWchRHRFLeErpZ4Djge6K6WKgD9prR+N1v4OhdWaxODBT7J8+TFs2PArhg59Oqr7CwZNa3zNGvj4Y/joI/j66327Tmw2k7QLCuDYY83PnJymlnd2trnl5JgkLoQQrRHNUTo/jta221N6+ngKCv7A1q1z6N59Brm55x/W9rQ2XStr1sDatbBxo0nyW7aY3yN95w4HTJgAt94KgwdDfr65de9uWubtPapCCCESuksnom/fWygvf4vvvruK1NQxJCcPaNP6dXXw/vvw1lvwzjumayYiLQ369YP+/WHaNJPchwyB0aNN37gQQnQUSfiAxWJnyJBnWb58AqtWncbo0Z/hcOTsd/lQyLTe//c/ePttWLzYnNBMT4dTToFJk2D4cBg2DHJzpbUuhOgcJOGHJScPYMSIN1i58kRWrTqDwsIP9xqfX1oKL7xgEvznn5tRM2CG+11zDUyfDhMnmhEnQgjRGUnCbyYj4xiGDHmONWvOZc2amfTs+QrvvuvkpZdMV00gYLpkzjvPJPcpU8wJVSGE6Aok4X+P3X4Wn322kOeeU6xdayMUgl694Lrr4OKLYcSIWEcohBCHRhJ+2Ndfw333wUsvgds9iaFDK7joots56aStnH/+3Tid3WIdohBCHJaEqoffkk8/hdNPhzFjYP58uOQSWLoU1qzpxt13D6dv32f45pspeL0dUOhCCCGiKGET/hdfwMknw3HHwZIlcPvt5orWhx6CsWPNMjk55zBy5Nu43ZtZsWIKHk9RbIMWQojDkHAJf8sWOPNMU4Zg5Uq45x7Ytg1uuaXlq1azsqYycuQCfL7drFgxGbd7S8cHLYQQ7SBhEr7W8PTTMGoULFoEf/kLbN4M119/8DrUmZnHMWrUBwQCVSxffgwVFe93TNBCCNGOEiLh19fDhReaUTajRsE338DNN5tKkK2Vnj6e0aM/xm7vxjffnMLmzbcSCrWx8LYQQsRQ3Cf8ykpz9esLL8Cf/wwLFx762PmUlGGMHbuEnj2vYPv2v/D115Oor1/fnuEKIUTUxHXCLy6G4483J2VffNEUKjvccrxWawqDBv2boUNfwO3ewNKlhWzffhdat2FaHyGEiIG4TfgVFTB5sqlQ+cYbcO657bv93NzzGT9+DdnZp7F58+9YseJ4vN5d7bsTIYRoR3GZ8LWGK64wI3LefRdOPTU6+3E6ezBs2HwGD36K2trlLF06hqqqRdHZmRBCHKa4TPhz58Jrr8Gdd5rKldGklKJHj4sYO/YrbLYMVqyYypYtfyIQqIvujoUQoo3iLuGvWAE33GCqV157bcftN3JCNzd3Jtu2/T+++mogu3bNIxQKHHxlIYToAEofbMbrDjRu3Di9dOnSQ14/GDQ16GtrzUVV3bvv/bwv6GNt6VrWla7DYXWQ5kzDbrGzq3YXO2p2UN5QTrI9mTRnGr3TenPGUWeQ5kxr2n4oyJ66PVR5qqj2VtMrrRcFmQV77WN79Xa2lnxIXclcLJ6lKHsvrOnnQMoJ9MoczIBuA3BYHa0+Jq017oAbp9WJ1bL/M87egJdKTyXlDeU4bU76Z/XHouLu81wI8T1KqWVa63GtWTauiqd99hl8+y08+yykZLh5YMnjrC9bT1FNEduqt7G6ZDW+oG+/67tsLjwBT+P9FHsKPxr6I8b0HMOibYv4cMuHVHmq9lpn2oBp/Gr8rwjpEA8sfYAFGxegaf4hugv4V/gGNouVAd0GMq7XOCb2mciE/AnYLXbq/fVUeapYU7KGlcUrWVu6lt11uympL2mM2W6x47K5SLIn4bK5sCgLdb46ar21eIPeveJKdaQyKm8UBZkF2Cw2bBYbIR3CG/TiC/pIsiXRLakbWa4sAqEANd4aanw1BENNo40q3BUU1xdT1lCG1hqLsuC0ORnSfQije4xmeO5w0pxpOK1O3AE3n27/lMXbF7OudB1WixWbxUamK5MRuSMYmTeSoTlD6Z3Wm15pvajyVLFw60IWb1+MJ+BhaPehDM0ZyqDug+if1Z9MVyZaa4rri9latZWyhjKqPdVUe6tRKBxWB06bE6fV2fjTarFiURYsykKSLYlkezIumwuNRmtNvb+ezZWb2VC+gRpvDSPzRjKm5xj6ZfXDF/ThCXgIhAJYldnOnro9LN+9nK/3fI1CMabnGEb3HI3T6mRHzQ521uwkEAo07r/B39DYGEh1pJKTnENuSi5HZR9Ffno+KjwTTiAUYFftrsabJ+Bh+sDpZLgyGl/7SLypjpYvFqnz1fFd+XcADO4+mGT7vlcPBkIByhvKyXRl4rQ59/t3fygCoQAV7gqCoSB5qXl7NS601viCPrxBL56AhwxnRrvvXxyauGrh33gj3H8/PLfkXW5ceDWbKzeT7kwnPz2f/PR8RuaOZGyvsYzIHUFQB6n11uIL+uiZ1pP89HxSHamEdIh6Xz2rSlbxnxX/4fnVz1Prq6VvRl9O6ncS43uPJ8uVRYYrg692fsVDSx9id91uAHql9eLKMVcyMm8k5e5yyhrKcNlcZNkB96ds3P0OW+rq2eFJZV2tjVJ3VYvH0SO1ByNyR9A7vTd5KXlkujLxBX24/W7cATfegBdP0EMwFCTVkUqqI5V0ZzrZSdl0S+pGna+Or/d8zdd7vmZP3R6CoSD+kB+rsuKwOnBYHXgCHsrd5dR4a1Ao0p3ppDvTsVlsjR9YWa4sclNyyUnJwaqsBHWQel89q0tWs6Fiwz5xW5SFwh6FjO4xGjBJoayhjJXFKymqabkOUa+0XqQ50thYsZFgs6GtWa4svEEvDf6GQ/57OBCbxUagld1tSbakxg/LQ5XuTOfIrCMpd5ezs2bnXscKpnFx0ciLmNpvKh9u+ZA3vnuDnbU7yXRl0i+zH9nJ2XgCHtx+N3vq9rCztmkeTYWiILOAbknd8AV9+II+Kj2VlNaXojEf1EdmHcng7oMJ6RBlDWWUNZRR76+nwd+AL+hjVN4oji84nrE9x7K9ejurSlaxoWIDdb46GvwNuP1uQjrU+Do0b/gk2ZIY0G0A6c50dtftbvwQi3DZXBzb51hOKDiBvhl9zYdBwEuNt4ZKTyWV7srG/5dIXG6/G2/QS6Yrkx6pPeie3L3xA7XeV09eah756fnkpeQRDAXxBr3U+mrZXr2d7dXbqfXWUpBZQP+s/uSl5KHRBENBQjrU+PfttDrJdGWSlZSFP+inuL6Y4vpi7BY7eSl55KbkUlxfzOqS1awtXUtQB0m2J5NsT6ZvRl8GZA2gT0YfdlTvYG3ZWrZXb+eY/GOYMWgGpx55KimOFLTW+EN+ar21VHurqfHWUO0xP8vd5RTVFFFUU4TWmofPfPiQ/rba0sKPq4Q/YHgV9SdcxZ7uLzAoexAPTn+QE/qdcFgxNfgbKK0vpW9G38YWWnP+oJ83v3sTpRTTB07Hbt3/lFfBoJvS0hfZseMe6upW4Uk6hXLX2Tjs3Uixp5DqSGVIzhByU3IPK+a2CIQCja3itqj11vJd+Xc0+BvwBr1YlZWxvcaS7kxvcfkKdwUbyjewq3YXO2t3kmRLYvIRkxnQbQBKKbwBL9+Wf8vGio1srtzM5srNuGwu+mf1p19mP3JTcslwZZDuTEeh8Aa9eAPexpakN+AlpEMEdZBAKIAn4KHB34An4EGhsChL4/aO7HYkTquTdWXrWL57OUU1RbhsLpxWJzaLjaA2iaFbUjfG9BzDoOxBhHSI9WXr+XrP12ityU/Pp3d6bxxWB96AF2/QS7I9mUxXJunOdOp8dZTWl7Knbg/ry9azpnQNGys2kpOSwxEZR9A3oy/56fn0SuuF2+/m38v/zXOrn8MT8JBiT2HagGmM6TmGnTU72Vq9lQp3BUm2JJLsSWQnZTMoexCDug8CYF3pOtaWraXWW4vD6sButZPpzCQv1SStkvoS1pau5dvyb3FYHXRP7k52UjZpjjSS7EkoFF/t+oqvdn7V+CHYI7UHQ7oPIcOVYb4pWV2N36DsFjvdk7vTPdn0mW6u3MzGyo3UemvpkdqDXmm9yHJlkWRPwml1sqlyEx9u+ZCVxSv3+btwWp1kJWU1bi87KZtURypJtiQcVgeVnsrGb5kp9hQyXZkk2ZMoriumqKaI0oZSbBYbTquTZHsyfTL6cETGEaQ6UtlWvY1NFZsobSht/NZmURaUUigUnoCHSk9l4zFnODPITcklEAqwp24P7oCbJFsSQ3PMt0+XzUWDv4E6Xx3bqrexsWIjdb46ku3JDM0ZSq+0Xny87WMqPZVt+l/KSc5hUPdBfHz5x21aLyIhE/7rX6zkrOfOxdJtG/93wp+48dgbO+3XyFDIz44d97B16xwsFhe9es0iL+8SUlNldpVEVuGuYG3pWsb1GofL5urw/df76llXto6CzILGZN6eKtwVVHmqzIeSxU66M50ke1K776ctIl1nVmXdK5bI40m2pP2eO9NaU+WpIsOV0dhg8gf9fLL9Ez7e/jHBUBCLsmC1WEl3ppPhNA2WSMMly5VF7/Teh/1eJ1zCf3Llk/zstavw12Qx/8cvcfa4Y6MQXftraPiOzZtvorz8TbQOkJIyitzcmeTk/Ijk5IGxDk8I0QUkVMKvcFcw8J8DCewaQd8vX2DV53lRii56fL5SSkpeoLj4aWprvwRoTP65uTNJSuof4wiFEJ1VQiV8gE++XcvkYUfxx9/bmDOn/ePqSB7PdkpL51Na+iI1NZ8DkJY2jm7dppGVdTLp6ROwWFo/rFMIEd8Sbljmpi+GooMwY0asIzl8Lldf+vS5lj59rsXj2U5JyYuUlc1n27Y72Lbtz1gsyWRkHEtGxhSysqaSnv4DlIy3F0K0Qly08M85B776ykxR2MJAmrgQCFRTWfkRVVUfUFW1iPr6VQA4nX3Jzb2A3NzzSU0dLclfiASTUC18jwcWLIBLL43fZA9gs2WQk3MWOTlnAeD3l1Ne/g4lJc9RVPR3duz4G3Z7DllZJ5OZOYWUlBGkpAzDZmt5mKQQIvF0+YRvt8N770G3brGOpGPZ7dn06HERPXpchM9XRkXFO1RWvkdFxXuUlDzbuJzTmU9S0kCSkgaQljaWrKxTSUoqiF3gQoiYiYsuHdFE6xAezzbq61dTX7+KhoZvcbs30NDwHYFAOQBJSYNITR2J3d4duz2b5OTBpKX9gKSkI1u8uEwI0XklVJeO2JtSFpKS+pGU1I/u3c9sfFxrTUPDt1RUvEtl5XvU1X1DIFCO318BhACw2bqRmjqKlJThpKQMIynpKJKSBuB09pZzA0LEAUn4CUIpRUrKYFJSBtOnT1Pd6FAoQEPDWmpqvqS29ivq6laxZ8/jBIN1zda1Y7G4UMqGUlaUsgFWrNZUunefQV7exXKVsBBdgHTpiH1oHcLr3YHbvRG3eyMez1ZCIQ9aB9E6EP4ZxOfbRWXl/9A6QFLSIJzOfGy2TOz2bByOnjidPbHbc7Fa07DZ0rDZsnA687Fa963sKIQ4NNKlIw6LUhZcriNwuY4gK2vqAZeNXCVcWfkefn85DQ278PvL8PtL97uOzdYNmy0rfL5A4XTmk5U1laysk7Db88Lrl6G1P9yVZMXh6EFSUn9stnRTgdBfhs+3B6ezD3Z7Zvu+AELEKWnhi6gIhfz4fMX4/aUEg3UEg3X4/WV4vUV4vTsIBKoBDWgaGtZTV7eiVdu12bIIBuvRumleA6fzCFJTR+Bw9MJuz8ZmywJChEJ+lLKRkjKctLRxOJ099tpWIFCN270Rr3c3WvvROoDNlk5q6mgcjo6rWCrE4eg0LXyl1DTgH4AVeERr/ddo7k90HhaLHZcrH5crv1XL+3ylVFUtJBisaxw9ZLE4G7uRvN5deDyb8Xi2YrWm4nT2weHIxe3eQn39SurrV1NT8xV+fzkQbHEfNlu38LkIK6GQG7+/bL/xOBy9SE4+CpstC5stC6s1GaXsjecxmh1p47cQi8WBxZKExZKEzZaOzZaB1ZpGMNhAIFBJMFiPw5GHy3UEDkcvlFJobU6YW62pWCymaqLfX4rXu4Ng0E1y8mAcjqbKlVoHCYV8WCxOOZEu2ixqCV+Z/4q5wMlAEbBEKfVfrfXaaO1TdF0ORw65uecd9na0DhEM1odPLtsJhbzU1a2gtnYpbve3hEJ+IIhSDpKSjgyPQspHKQcWix2fr5S6uq+pq1uOx7MNt3sDfn8loVADWvvD60e+FZtvKFoHmz12OKwoZd3r2wuA3Z6Hw5GLz1cS7iozHxLmZLr5gLFak8PHbr5N2WxpOJ354WOzEQp5wudhQo3bNa+ROQlvsbjCH4bO8LGE0DpAIFBDIFCN1l5stmwcjhzs9u5YrRnYbOlYLMmNXXNa+wkEqsPL+8Ifjjas1jTs9lwcjhwslpTwvhXmw9J8gAaDteGuvEosFjtWa2p4vRwcjp7Y7d3x+0vwenfg8xVjtaZgs2Vhsbjw+fbg9e4kEKjCZssKf8vLDB+Ts/G9VcrMVWFeCy+hkJtQyE0w6AbAYnGglAO7vTsuVwE2WyqhkJf6+nU0NKxBKQcORw8cjjyCwXr8/hICgSpcrgJSUoZjtaY0+zvUjUOctQ5RX7+aysoPcLs3hb9xjiE5eVC4EWHFNBii38MezT0cDWzUWm8GUEo9D/wQkIQvokYpCzZb0zzEFoudzMzjyMw8rlXrp6RAVtbxbd6v1hqtfQSDkSRSE05+NeHklInVmozPV4zHsx2fb1c4Xita63CirkXrQDhR98FicdLQsI76+jX4/eWkp0/A4cjDYklBa294X57w/syHnEmUKQSDtXg8O/B4dgCh8AeDs/HbiYnXH/4Q8BMK+Ro/FMw3B0t4e+nhxJ6G31+O2/0tfn/ZXqO49n0PHM2+nfnR2t/m17MzsNmyCQar0bo1M6MpnM6+4Q+9KkKhhvAHcVr4MTMpisWSTCi07yxudnseEyfuaecj2Fc0E35vYEez+0XAD6K4PyFiRimFUk4sFieQCfRscTmX6wjS049u9Xazs09rnwDbmdbB8IdUA5FvOqY1n4HVuveEHsGgB7+/FJ+vmFDIQ9M3oxBguuys1lTs9u7YbFnhbdcSCNTg95fg8+3G7y/Dbs/F5eqL3Z5HKBTpJnPjcOThdPbGZssiEKjC7y8nEKhCa29ja77p2xlYrUmN32gi35DMNxQfoZA3/KG8BY9nG3Z7NqmpI0lJGY7WIXy+3fh8JVitKTgcuVit6bjdm6iv/4aGhu+wWFyNH+6hkJtAoBbQZGQcS2bmiTid+Xg826irW47bvTl8/MFwDNEX81E6SqkrgSsB+vbtG+NohBCtoZQVmy0Dmy3joMtarS6s1j64XH3asIeWPzAPxmZLa+N+2mrf601SU0c01rhqjaSkgpiVN4nmWZ+dQPNXPj/82F601vO01uO01uNycnKiGI4QQiS2aCb8JcBApVQ/pZQDuAD4bxT3J4QQ4gCi1qWjtQ4opX4FLMAMy3xMa70mWvsTQghxYFHtw9davw28Hc19CCGEaB25ckMIIRKEJHwhhEgQkvCFECJBSMIXQogE0amqZSqlSoFth7h6d2D/1bDiR6IcJyTOsSbKcULiHGtHHucRWutWXcTUqRL+4VBKLW1tidCuLFGOExLnWBPlOCFxjrWzHqd06QghRIKQhC+EEAkinhL+vFgH0EES5TghcY41UY4TEudYO+Vxxk0fvhBCiAOLpxa+EEKIA+jyCV8pNU0p9a1SaqNSanas42lPSqk+SqmPlFJrlVJrlFK/CT/eTSn1P6XUhvDPrFjH2h6UUlal1NdKqTfD9/sppb4Mv7cvhKuudnlKqUyl1MtKqfVKqXVKqWPi8T1VSl0X/rtdrZR6Tinlipf3VCn1mFKqRCm1utljLb6Hyrg/fMzfKKXGxCruLp3wm82bexowFPixUmpobKNqVwHgBq31UGACcHX4+GYDH2itBwIfhO/Hg98A65rdvxO4V2s9AKgErohJVO3vH8C7WuvBwCjMMcfVe6qU6g1cA4zTWg/HVMy9gPh5T/8DTPveY/t7D08DBoZvVwIPdlCM++jSCZ9m8+ZqM/NzZN7cuKC13q21Xh7+vRaTGHpjjvGJ8GJPAK2fbqeTUkrlA9OBR8L3FXAi8HJ4kXg5zgxgMvAogNbap7WuIg7fU0w13iSllA1IBnYTJ++p1noxUPG9h/f3Hv4QeFIbXwCZSqlDm9LrMHX1hN/SvLm9YxRLVCmlCoDRwJdAntZ6d/ipPUBejMJqT/cBvwNC4fvZQJVumkE6Xt7bfkAp8Hi4++oRpVQKcfaeaq13AncD2zGJvhpYRny+pxH7ew87TZ7q6gk/ISilUoFXgGu11jXNn9NmmFWXHmqllDoDKNFaL4t1LB3ABowBHtRajwbq+V73TZy8p1mYlm0/oBeQwr5dIHGrs76HXT3ht2re3K5MKWXHJPtntNbzww8XR74Shn+WxCq+djIRmKGU2orpljsR08+dGe4OgPh5b4uAIq31l+H7L2M+AOLtPT0J2KK1LtVa+4H5mPc5Ht/TiP29h50mT3X1hB/X8+aG+7EfBdZprf/e7Kn/ApeGf78UeL2jY2tPWuubtdb5WusCzHv4odb6QuAj4Efhxbr8cQJorfcAO5RSg8IPTQXWEmfvKaYrZ4JSKjn8dxw5zntI5s4AAAJzSURBVLh7T5vZ33v4X+CS8GidCUB1s66fjqW17tI34HTgO2ATcGus42nnYzsO87XwG2BF+HY6pn/7A2AD8D7QLdaxtuMxHw+8Gf69P/AVsBF4CXDGOr52OsZCYGn4fX0NyIrH9xT4P2A9sBp4CnDGy3sKPIc5N+HHfGu7Yn/vIaAwowk3AaswI5diErdcaSuEEAmiq3fpCCGEaCVJ+EIIkSAk4QshRIKQhC+EEAlCEr4QQiQISfhCtAOl1PGRKp9CdFaS8IUQIkFIwhcJRSl1kVLqK6XUCqXUw+Ea/HVKqXvDtds/UErlhJctVEp9Ea5h/mqz+uYDlFLvK6VWKqWWK6WODG8+tVmd+2fCV5gK0WlIwhcJQyk1BJgJTNRaFwJB4EJMYa+lWuthwCLgT+FVngRu0lqPxFwhGXn8GWCu1noUcCzmiksw1UyvxczN0B9TO0aITsN28EWEiBtTgbHAknDjOwlT4CoEvBBe5mlgfrhufabWelH48SeAl5RSaUBvrfWrAFprD0B4e19prYvC91cABcAn0T8sIVpHEr5IJAp4Qmt9814PKvWH7y13qPVGvM1+DyL/X6KTkS4dkUg+AH6klMqFxjlIj8D8H0QqOP4E+ERrXQ1UKqUmhR+/GFikzcxjRUqps8LbcCqlkjv0KIQ4RNICEQlDa71WKfV74D2llAVT6fBqzCQkR4efK8H084MpcftQOKFvBi4PP34x8LBS6v+Ft3FeBx6GEIdMqmWKhKeUqtNap8Y6DiGiTbp0hBAiQUgLXwghEoS08IUQIkFIwhdCiAQhCV8IIRKEJHwhhEgQkvCFECJBSMIXQogE8f8BSydigenbwVYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 1s 191us/sample - loss: 1.8429 - acc: 0.4289\n",
      "Loss: 1.8428832955325751 Accuracy: 0.42886811\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 2.1350 - acc: 0.3176\n",
      "Epoch 00001: val_loss improved from inf to 1.79621, saving model to model/checkpoint/1D_CNN_BN_2_only_conv_checkpoint/001-1.7962.hdf5\n",
      "36805/36805 [==============================] - 13s 342us/sample - loss: 2.1341 - acc: 0.3178 - val_loss: 1.7962 - val_acc: 0.4067\n",
      "Epoch 2/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.5566 - acc: 0.5060\n",
      "Epoch 00002: val_loss improved from 1.79621 to 1.46232, saving model to model/checkpoint/1D_CNN_BN_2_only_conv_checkpoint/002-1.4623.hdf5\n",
      "36805/36805 [==============================] - 11s 307us/sample - loss: 1.5562 - acc: 0.5062 - val_loss: 1.4623 - val_acc: 0.5423\n",
      "Epoch 3/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3504 - acc: 0.5758\n",
      "Epoch 00003: val_loss improved from 1.46232 to 1.41693, saving model to model/checkpoint/1D_CNN_BN_2_only_conv_checkpoint/003-1.4169.hdf5\n",
      "36805/36805 [==============================] - 11s 308us/sample - loss: 1.3507 - acc: 0.5756 - val_loss: 1.4169 - val_acc: 0.5639\n",
      "Epoch 4/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2052 - acc: 0.6292\n",
      "Epoch 00004: val_loss improved from 1.41693 to 1.31060, saving model to model/checkpoint/1D_CNN_BN_2_only_conv_checkpoint/004-1.3106.hdf5\n",
      "36805/36805 [==============================] - 11s 303us/sample - loss: 1.2053 - acc: 0.6293 - val_loss: 1.3106 - val_acc: 0.5980\n",
      "Epoch 5/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1066 - acc: 0.6590\n",
      "Epoch 00005: val_loss did not improve from 1.31060\n",
      "36805/36805 [==============================] - 11s 305us/sample - loss: 1.1066 - acc: 0.6590 - val_loss: 1.3107 - val_acc: 0.5926\n",
      "Epoch 6/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0206 - acc: 0.6885\n",
      "Epoch 00006: val_loss improved from 1.31060 to 1.29083, saving model to model/checkpoint/1D_CNN_BN_2_only_conv_checkpoint/006-1.2908.hdf5\n",
      "36805/36805 [==============================] - 11s 309us/sample - loss: 1.0207 - acc: 0.6885 - val_loss: 1.2908 - val_acc: 0.5973\n",
      "Epoch 7/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9469 - acc: 0.7098\n",
      "Epoch 00007: val_loss improved from 1.29083 to 1.26857, saving model to model/checkpoint/1D_CNN_BN_2_only_conv_checkpoint/007-1.2686.hdf5\n",
      "36805/36805 [==============================] - 11s 308us/sample - loss: 0.9471 - acc: 0.7100 - val_loss: 1.2686 - val_acc: 0.6061\n",
      "Epoch 8/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8790 - acc: 0.7327\n",
      "Epoch 00008: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 304us/sample - loss: 0.8790 - acc: 0.7326 - val_loss: 1.3056 - val_acc: 0.6066\n",
      "Epoch 9/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8207 - acc: 0.7587\n",
      "Epoch 00009: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 306us/sample - loss: 0.8208 - acc: 0.7586 - val_loss: 1.3215 - val_acc: 0.5924\n",
      "Epoch 10/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7727 - acc: 0.7682\n",
      "Epoch 00010: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 304us/sample - loss: 0.7724 - acc: 0.7682 - val_loss: 1.2896 - val_acc: 0.6198\n",
      "Epoch 11/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7215 - acc: 0.7855\n",
      "Epoch 00011: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 309us/sample - loss: 0.7212 - acc: 0.7857 - val_loss: 1.3149 - val_acc: 0.6129\n",
      "Epoch 12/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6746 - acc: 0.8024\n",
      "Epoch 00012: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 303us/sample - loss: 0.6747 - acc: 0.8024 - val_loss: 1.3599 - val_acc: 0.5996\n",
      "Epoch 13/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6378 - acc: 0.8151\n",
      "Epoch 00013: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 308us/sample - loss: 0.6380 - acc: 0.8151 - val_loss: 1.3545 - val_acc: 0.5991\n",
      "Epoch 14/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6060 - acc: 0.8239\n",
      "Epoch 00014: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 303us/sample - loss: 0.6061 - acc: 0.8238 - val_loss: 1.3659 - val_acc: 0.5996\n",
      "Epoch 15/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5701 - acc: 0.8372\n",
      "Epoch 00015: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 304us/sample - loss: 0.5702 - acc: 0.8372 - val_loss: 1.3470 - val_acc: 0.6138\n",
      "Epoch 16/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5369 - acc: 0.8502\n",
      "Epoch 00016: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 305us/sample - loss: 0.5369 - acc: 0.8502 - val_loss: 1.4343 - val_acc: 0.5977\n",
      "Epoch 17/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5066 - acc: 0.8588\n",
      "Epoch 00017: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 303us/sample - loss: 0.5066 - acc: 0.8588 - val_loss: 1.3813 - val_acc: 0.6026\n",
      "Epoch 18/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4783 - acc: 0.8678\n",
      "Epoch 00018: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 303us/sample - loss: 0.4785 - acc: 0.8677 - val_loss: 1.3761 - val_acc: 0.6143\n",
      "Epoch 19/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4563 - acc: 0.8757\n",
      "Epoch 00019: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 306us/sample - loss: 0.4565 - acc: 0.8756 - val_loss: 1.4718 - val_acc: 0.5949\n",
      "Epoch 20/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4269 - acc: 0.8870\n",
      "Epoch 00020: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 302us/sample - loss: 0.4274 - acc: 0.8869 - val_loss: 1.4679 - val_acc: 0.5931\n",
      "Epoch 21/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4055 - acc: 0.8929\n",
      "Epoch 00021: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 305us/sample - loss: 0.4056 - acc: 0.8929 - val_loss: 1.4163 - val_acc: 0.6145\n",
      "Epoch 22/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3846 - acc: 0.9003\n",
      "Epoch 00022: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 306us/sample - loss: 0.3845 - acc: 0.9004 - val_loss: 1.5123 - val_acc: 0.5926\n",
      "Epoch 23/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3661 - acc: 0.9053\n",
      "Epoch 00023: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 305us/sample - loss: 0.3667 - acc: 0.9052 - val_loss: 1.5105 - val_acc: 0.6019\n",
      "Epoch 24/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3431 - acc: 0.9141\n",
      "Epoch 00024: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 305us/sample - loss: 0.3430 - acc: 0.9141 - val_loss: 1.5595 - val_acc: 0.5912\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3293 - acc: 0.9181\n",
      "Epoch 00025: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 295us/sample - loss: 0.3293 - acc: 0.9181 - val_loss: 1.5455 - val_acc: 0.5954\n",
      "Epoch 26/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3118 - acc: 0.9243\n",
      "Epoch 00026: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 297us/sample - loss: 0.3118 - acc: 0.9243 - val_loss: 1.5494 - val_acc: 0.5968\n",
      "Epoch 27/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2944 - acc: 0.9298\n",
      "Epoch 00027: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 294us/sample - loss: 0.2948 - acc: 0.9297 - val_loss: 1.5659 - val_acc: 0.6010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2787 - acc: 0.9345\n",
      "Epoch 00028: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 293us/sample - loss: 0.2791 - acc: 0.9344 - val_loss: 1.6914 - val_acc: 0.5749\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2748 - acc: 0.9354\n",
      "Epoch 00029: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 298us/sample - loss: 0.2748 - acc: 0.9354 - val_loss: 1.6060 - val_acc: 0.5947\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2535 - acc: 0.9430\n",
      "Epoch 00030: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 296us/sample - loss: 0.2535 - acc: 0.9430 - val_loss: 1.5833 - val_acc: 0.5980\n",
      "Epoch 31/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2389 - acc: 0.9490\n",
      "Epoch 00031: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 299us/sample - loss: 0.2389 - acc: 0.9491 - val_loss: 1.7322 - val_acc: 0.5768\n",
      "Epoch 32/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2306 - acc: 0.9496\n",
      "Epoch 00032: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 300us/sample - loss: 0.2314 - acc: 0.9495 - val_loss: 1.6307 - val_acc: 0.5952\n",
      "Epoch 33/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2210 - acc: 0.9533\n",
      "Epoch 00033: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 302us/sample - loss: 0.2209 - acc: 0.9534 - val_loss: 1.7373 - val_acc: 0.5702\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2074 - acc: 0.9573\n",
      "Epoch 00034: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 299us/sample - loss: 0.2074 - acc: 0.9573 - val_loss: 1.7112 - val_acc: 0.5842\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1983 - acc: 0.9607\n",
      "Epoch 00035: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 298us/sample - loss: 0.1983 - acc: 0.9607 - val_loss: 1.6949 - val_acc: 0.5949\n",
      "Epoch 36/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1853 - acc: 0.9645\n",
      "Epoch 00036: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 304us/sample - loss: 0.1853 - acc: 0.9645 - val_loss: 1.7366 - val_acc: 0.5926\n",
      "Epoch 37/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1806 - acc: 0.9657\n",
      "Epoch 00037: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 305us/sample - loss: 0.1806 - acc: 0.9657 - val_loss: 1.7994 - val_acc: 0.5754\n",
      "Epoch 38/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1723 - acc: 0.9675\n",
      "Epoch 00038: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 302us/sample - loss: 0.1724 - acc: 0.9675 - val_loss: 1.7806 - val_acc: 0.5884\n",
      "Epoch 39/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1637 - acc: 0.9704\n",
      "Epoch 00039: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 303us/sample - loss: 0.1637 - acc: 0.9704 - val_loss: 1.8547 - val_acc: 0.5749\n",
      "Epoch 40/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1537 - acc: 0.9735\n",
      "Epoch 00040: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 305us/sample - loss: 0.1537 - acc: 0.9735 - val_loss: 1.9483 - val_acc: 0.5747\n",
      "Epoch 41/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1478 - acc: 0.9744\n",
      "Epoch 00041: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 305us/sample - loss: 0.1478 - acc: 0.9744 - val_loss: 1.8714 - val_acc: 0.5765\n",
      "Epoch 42/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1402 - acc: 0.9771\n",
      "Epoch 00042: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 304us/sample - loss: 0.1403 - acc: 0.9770 - val_loss: 1.9207 - val_acc: 0.5691\n",
      "Epoch 43/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1401 - acc: 0.9772\n",
      "Epoch 00043: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 303us/sample - loss: 0.1401 - acc: 0.9772 - val_loss: 1.8807 - val_acc: 0.5772\n",
      "Epoch 44/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1349 - acc: 0.9778\n",
      "Epoch 00044: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 303us/sample - loss: 0.1350 - acc: 0.9779 - val_loss: 1.9234 - val_acc: 0.5772\n",
      "Epoch 45/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1230 - acc: 0.9818\n",
      "Epoch 00045: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 301us/sample - loss: 0.1229 - acc: 0.9819 - val_loss: 1.9042 - val_acc: 0.5893\n",
      "Epoch 46/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1161 - acc: 0.9834\n",
      "Epoch 00046: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 302us/sample - loss: 0.1161 - acc: 0.9834 - val_loss: 1.9735 - val_acc: 0.5798\n",
      "Epoch 47/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1135 - acc: 0.9834\n",
      "Epoch 00047: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 305us/sample - loss: 0.1135 - acc: 0.9833 - val_loss: 2.1074 - val_acc: 0.5607\n",
      "Epoch 48/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1166 - acc: 0.9818\n",
      "Epoch 00048: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 295us/sample - loss: 0.1165 - acc: 0.9819 - val_loss: 1.9288 - val_acc: 0.5840\n",
      "Epoch 49/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1013 - acc: 0.9864\n",
      "Epoch 00049: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 295us/sample - loss: 0.1018 - acc: 0.9863 - val_loss: 2.0568 - val_acc: 0.5721\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1008 - acc: 0.9864\n",
      "Epoch 00050: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 294us/sample - loss: 0.1009 - acc: 0.9864 - val_loss: 2.0320 - val_acc: 0.5788\n",
      "Epoch 51/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1006 - acc: 0.9854\n",
      "Epoch 00051: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 292us/sample - loss: 0.1007 - acc: 0.9854 - val_loss: 2.0375 - val_acc: 0.5793\n",
      "Epoch 52/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0955 - acc: 0.9872\n",
      "Epoch 00052: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 298us/sample - loss: 0.0956 - acc: 0.9871 - val_loss: 2.1069 - val_acc: 0.5707\n",
      "Epoch 53/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0928 - acc: 0.9875\n",
      "Epoch 00053: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 302us/sample - loss: 0.0929 - acc: 0.9874 - val_loss: 2.0321 - val_acc: 0.5833\n",
      "Epoch 54/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0858 - acc: 0.9902\n",
      "Epoch 00054: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 296us/sample - loss: 0.0857 - acc: 0.9902 - val_loss: 2.0959 - val_acc: 0.5791\n",
      "Epoch 55/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0804 - acc: 0.9905\n",
      "Epoch 00055: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 302us/sample - loss: 0.0805 - acc: 0.9905 - val_loss: 2.0936 - val_acc: 0.5826\n",
      "Epoch 56/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0850 - acc: 0.9890\n",
      "Epoch 00056: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 302us/sample - loss: 0.0851 - acc: 0.9890 - val_loss: 2.1139 - val_acc: 0.5714\n",
      "Epoch 57/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0748 - acc: 0.9921\n",
      "Epoch 00057: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 301us/sample - loss: 0.0748 - acc: 0.9921 - val_loss: 2.1185 - val_acc: 0.5821\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0756 - acc: 0.9906\n",
      "Epoch 00058: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 294us/sample - loss: 0.0755 - acc: 0.9906 - val_loss: 2.1546 - val_acc: 0.5723\n",
      "Epoch 59/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0689 - acc: 0.9932\n",
      "Epoch 00059: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 292us/sample - loss: 0.0688 - acc: 0.9932 - val_loss: 2.1704 - val_acc: 0.5761\n",
      "Epoch 60/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0692 - acc: 0.9925\n",
      "Epoch 00060: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 295us/sample - loss: 0.0693 - acc: 0.9925 - val_loss: 2.2797 - val_acc: 0.5663\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0698 - acc: 0.9920\n",
      "Epoch 00061: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 292us/sample - loss: 0.0698 - acc: 0.9920 - val_loss: 2.1985 - val_acc: 0.5830\n",
      "Epoch 62/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0690 - acc: 0.9922\n",
      "Epoch 00062: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 295us/sample - loss: 0.0690 - acc: 0.9922 - val_loss: 2.3290 - val_acc: 0.5567\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0596 - acc: 0.9942\n",
      "Epoch 00063: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 286us/sample - loss: 0.0597 - acc: 0.9942 - val_loss: 2.3741 - val_acc: 0.5609\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0616 - acc: 0.9935\n",
      "Epoch 00064: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 287us/sample - loss: 0.0616 - acc: 0.9935 - val_loss: 2.4129 - val_acc: 0.5535\n",
      "Epoch 65/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0603 - acc: 0.9936\n",
      "Epoch 00065: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 10s 285us/sample - loss: 0.0604 - acc: 0.9936 - val_loss: 2.2744 - val_acc: 0.5737\n",
      "Epoch 66/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0623 - acc: 0.9926\n",
      "Epoch 00066: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 288us/sample - loss: 0.0624 - acc: 0.9926 - val_loss: 2.3136 - val_acc: 0.5705\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0585 - acc: 0.9936\n",
      "Epoch 00067: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 286us/sample - loss: 0.0586 - acc: 0.9936 - val_loss: 2.2896 - val_acc: 0.5712\n",
      "Epoch 68/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0617 - acc: 0.9926\n",
      "Epoch 00068: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 287us/sample - loss: 0.0618 - acc: 0.9926 - val_loss: 2.2968 - val_acc: 0.5740\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0490 - acc: 0.9956\n",
      "Epoch 00069: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 10s 285us/sample - loss: 0.0490 - acc: 0.9956 - val_loss: 2.3332 - val_acc: 0.5805\n",
      "Epoch 70/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0508 - acc: 0.9950\n",
      "Epoch 00070: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 286us/sample - loss: 0.0508 - acc: 0.9950 - val_loss: 2.3621 - val_acc: 0.5716\n",
      "Epoch 71/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0506 - acc: 0.9948\n",
      "Epoch 00071: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 290us/sample - loss: 0.0506 - acc: 0.9948 - val_loss: 2.3363 - val_acc: 0.5756\n",
      "Epoch 72/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0497 - acc: 0.9953\n",
      "Epoch 00072: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 286us/sample - loss: 0.0497 - acc: 0.9954 - val_loss: 2.4182 - val_acc: 0.5663\n",
      "Epoch 73/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0562 - acc: 0.9935\n",
      "Epoch 00073: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 287us/sample - loss: 0.0563 - acc: 0.9934 - val_loss: 2.3859 - val_acc: 0.5777\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0431 - acc: 0.9966\n",
      "Epoch 00074: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 292us/sample - loss: 0.0433 - acc: 0.9965 - val_loss: 2.4557 - val_acc: 0.5646\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0579 - acc: 0.9920\n",
      "Epoch 00075: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 289us/sample - loss: 0.0579 - acc: 0.9920 - val_loss: 2.4103 - val_acc: 0.5705\n",
      "Epoch 76/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0417 - acc: 0.9967\n",
      "Epoch 00076: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 288us/sample - loss: 0.0417 - acc: 0.9967 - val_loss: 2.4953 - val_acc: 0.5646\n",
      "Epoch 77/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0412 - acc: 0.9963\n",
      "Epoch 00077: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 288us/sample - loss: 0.0414 - acc: 0.9962 - val_loss: 2.4624 - val_acc: 0.5712\n",
      "Epoch 78/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0440 - acc: 0.9955\n",
      "Epoch 00078: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 296us/sample - loss: 0.0440 - acc: 0.9955 - val_loss: 2.4798 - val_acc: 0.5777\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0439 - acc: 0.9955\n",
      "Epoch 00079: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 291us/sample - loss: 0.0440 - acc: 0.9955 - val_loss: 2.7553 - val_acc: 0.5406\n",
      "Epoch 80/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0434 - acc: 0.9951\n",
      "Epoch 00080: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 292us/sample - loss: 0.0434 - acc: 0.9951 - val_loss: 2.5263 - val_acc: 0.5621\n",
      "Epoch 81/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0334 - acc: 0.9980\n",
      "Epoch 00081: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 292us/sample - loss: 0.0335 - acc: 0.9980 - val_loss: 2.4975 - val_acc: 0.5714\n",
      "Epoch 82/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0391 - acc: 0.9965\n",
      "Epoch 00082: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 292us/sample - loss: 0.0391 - acc: 0.9965 - val_loss: 2.5135 - val_acc: 0.5672\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0389 - acc: 0.9959\n",
      "Epoch 00083: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 295us/sample - loss: 0.0392 - acc: 0.9959 - val_loss: 2.7452 - val_acc: 0.5502\n",
      "Epoch 84/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0398 - acc: 0.9958\n",
      "Epoch 00084: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 291us/sample - loss: 0.0402 - acc: 0.9957 - val_loss: 2.5383 - val_acc: 0.5625\n",
      "Epoch 85/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0387 - acc: 0.9964\n",
      "Epoch 00085: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 293us/sample - loss: 0.0387 - acc: 0.9964 - val_loss: 2.6869 - val_acc: 0.5553\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0338 - acc: 0.9971\n",
      "Epoch 00086: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 292us/sample - loss: 0.0338 - acc: 0.9971 - val_loss: 2.5321 - val_acc: 0.5761\n",
      "Epoch 87/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0357 - acc: 0.9962\n",
      "Epoch 00087: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 291us/sample - loss: 0.0358 - acc: 0.9962 - val_loss: 2.9158 - val_acc: 0.5285\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0387 - acc: 0.9954\n",
      "Epoch 00088: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 291us/sample - loss: 0.0389 - acc: 0.9953 - val_loss: 2.5815 - val_acc: 0.5632\n",
      "Epoch 89/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0385 - acc: 0.9952\n",
      "Epoch 00089: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 293us/sample - loss: 0.0389 - acc: 0.9952 - val_loss: 2.7014 - val_acc: 0.5467\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0334 - acc: 0.9968\n",
      "Epoch 00090: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 290us/sample - loss: 0.0334 - acc: 0.9968 - val_loss: 2.6241 - val_acc: 0.5577\n",
      "Epoch 91/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0295 - acc: 0.9974\n",
      "Epoch 00091: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 291us/sample - loss: 0.0295 - acc: 0.9974 - val_loss: 2.6109 - val_acc: 0.5728\n",
      "Epoch 92/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0291 - acc: 0.9976\n",
      "Epoch 00092: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 286us/sample - loss: 0.0291 - acc: 0.9976 - val_loss: 2.6223 - val_acc: 0.5714\n",
      "Epoch 93/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0351 - acc: 0.9961\n",
      "Epoch 00093: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 10s 284us/sample - loss: 0.0351 - acc: 0.9961 - val_loss: 2.6340 - val_acc: 0.5749\n",
      "Epoch 94/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0299 - acc: 0.9978\n",
      "Epoch 00094: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 286us/sample - loss: 0.0299 - acc: 0.9979 - val_loss: 2.6107 - val_acc: 0.5688\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0289 - acc: 0.9976\n",
      "Epoch 00095: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 289us/sample - loss: 0.0291 - acc: 0.9975 - val_loss: 2.7903 - val_acc: 0.5614\n",
      "Epoch 96/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0407 - acc: 0.9933\n",
      "Epoch 00096: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 293us/sample - loss: 0.0407 - acc: 0.9933 - val_loss: 2.7119 - val_acc: 0.5688\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0291 - acc: 0.9973\n",
      "Epoch 00097: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 287us/sample - loss: 0.0291 - acc: 0.9973 - val_loss: 2.6763 - val_acc: 0.5695\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0301 - acc: 0.9970\n",
      "Epoch 00098: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 289us/sample - loss: 0.0302 - acc: 0.9970 - val_loss: 2.8048 - val_acc: 0.5530\n",
      "Epoch 99/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0349 - acc: 0.9956\n",
      "Epoch 00099: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 293us/sample - loss: 0.0350 - acc: 0.9956 - val_loss: 2.7792 - val_acc: 0.5656\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0341 - acc: 0.9959\n",
      "Epoch 00100: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 293us/sample - loss: 0.0341 - acc: 0.9959 - val_loss: 2.8533 - val_acc: 0.5535\n",
      "Epoch 101/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0246 - acc: 0.9980\n",
      "Epoch 00101: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 294us/sample - loss: 0.0246 - acc: 0.9980 - val_loss: 2.7116 - val_acc: 0.5674\n",
      "Epoch 102/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0262 - acc: 0.9977\n",
      "Epoch 00102: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 293us/sample - loss: 0.0262 - acc: 0.9977 - val_loss: 2.8080 - val_acc: 0.5558\n",
      "Epoch 103/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0301 - acc: 0.9966\n",
      "Epoch 00103: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 293us/sample - loss: 0.0301 - acc: 0.9966 - val_loss: 3.0131 - val_acc: 0.5430\n",
      "Epoch 104/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0244 - acc: 0.9981\n",
      "Epoch 00104: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 290us/sample - loss: 0.0244 - acc: 0.9982 - val_loss: 2.9776 - val_acc: 0.5360\n",
      "Epoch 105/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0264 - acc: 0.9977\n",
      "Epoch 00105: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 292us/sample - loss: 0.0264 - acc: 0.9977 - val_loss: 2.7957 - val_acc: 0.5653\n",
      "Epoch 106/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0280 - acc: 0.9967\n",
      "Epoch 00106: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 289us/sample - loss: 0.0279 - acc: 0.9967 - val_loss: 2.9051 - val_acc: 0.5437\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0255 - acc: 0.9974\n",
      "Epoch 00107: val_loss did not improve from 1.26857\n",
      "36805/36805 [==============================] - 11s 293us/sample - loss: 0.0258 - acc: 0.9974 - val_loss: 2.8489 - val_acc: 0.5621\n",
      "\n",
      "1D_CNN_BN_2_only_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 1s 219us/sample - loss: 1.3207 - acc: 0.5836\n",
      "Loss: 1.3206864595289418 Accuracy: 0.58359295\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.9744 - acc: 0.3749\n",
      "Epoch 00001: val_loss improved from inf to 1.58715, saving model to model/checkpoint/1D_CNN_BN_3_only_conv_checkpoint/001-1.5871.hdf5\n",
      "36805/36805 [==============================] - 15s 414us/sample - loss: 1.9738 - acc: 0.3751 - val_loss: 1.5871 - val_acc: 0.4962\n",
      "Epoch 2/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3655 - acc: 0.5724\n",
      "Epoch 00002: val_loss improved from 1.58715 to 1.24479, saving model to model/checkpoint/1D_CNN_BN_3_only_conv_checkpoint/002-1.2448.hdf5\n",
      "36805/36805 [==============================] - 13s 345us/sample - loss: 1.3649 - acc: 0.5726 - val_loss: 1.2448 - val_acc: 0.6056\n",
      "Epoch 3/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1688 - acc: 0.6452\n",
      "Epoch 00003: val_loss improved from 1.24479 to 1.11074, saving model to model/checkpoint/1D_CNN_BN_3_only_conv_checkpoint/003-1.1107.hdf5\n",
      "36805/36805 [==============================] - 13s 346us/sample - loss: 1.1692 - acc: 0.6451 - val_loss: 1.1107 - val_acc: 0.6622\n",
      "Epoch 4/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0459 - acc: 0.6823\n",
      "Epoch 00004: val_loss improved from 1.11074 to 1.00577, saving model to model/checkpoint/1D_CNN_BN_3_only_conv_checkpoint/004-1.0058.hdf5\n",
      "36805/36805 [==============================] - 13s 342us/sample - loss: 1.0464 - acc: 0.6821 - val_loss: 1.0058 - val_acc: 0.7002\n",
      "Epoch 5/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9496 - acc: 0.7149\n",
      "Epoch 00005: val_loss improved from 1.00577 to 0.92628, saving model to model/checkpoint/1D_CNN_BN_3_only_conv_checkpoint/005-0.9263.hdf5\n",
      "36805/36805 [==============================] - 13s 346us/sample - loss: 0.9499 - acc: 0.7148 - val_loss: 0.9263 - val_acc: 0.7230\n",
      "Epoch 6/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8700 - acc: 0.7428\n",
      "Epoch 00006: val_loss did not improve from 0.92628\n",
      "36805/36805 [==============================] - 13s 344us/sample - loss: 0.8700 - acc: 0.7428 - val_loss: 1.0259 - val_acc: 0.6876\n",
      "Epoch 7/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8067 - acc: 0.7606\n",
      "Epoch 00007: val_loss improved from 0.92628 to 0.85093, saving model to model/checkpoint/1D_CNN_BN_3_only_conv_checkpoint/007-0.8509.hdf5\n",
      "36805/36805 [==============================] - 13s 342us/sample - loss: 0.8066 - acc: 0.7607 - val_loss: 0.8509 - val_acc: 0.7508\n",
      "Epoch 8/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7473 - acc: 0.7801\n",
      "Epoch 00008: val_loss improved from 0.85093 to 0.81987, saving model to model/checkpoint/1D_CNN_BN_3_only_conv_checkpoint/008-0.8199.hdf5\n",
      "36805/36805 [==============================] - 13s 345us/sample - loss: 0.7475 - acc: 0.7801 - val_loss: 0.8199 - val_acc: 0.7671\n",
      "Epoch 9/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7001 - acc: 0.7962\n",
      "Epoch 00009: val_loss did not improve from 0.81987\n",
      "36805/36805 [==============================] - 12s 339us/sample - loss: 0.7004 - acc: 0.7962 - val_loss: 0.8648 - val_acc: 0.7475\n",
      "Epoch 10/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6542 - acc: 0.8114\n",
      "Epoch 00010: val_loss improved from 0.81987 to 0.80093, saving model to model/checkpoint/1D_CNN_BN_3_only_conv_checkpoint/010-0.8009.hdf5\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.6545 - acc: 0.8112 - val_loss: 0.8009 - val_acc: 0.7696\n",
      "Epoch 11/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6143 - acc: 0.8249\n",
      "Epoch 00011: val_loss improved from 0.80093 to 0.79592, saving model to model/checkpoint/1D_CNN_BN_3_only_conv_checkpoint/011-0.7959.hdf5\n",
      "36805/36805 [==============================] - 13s 343us/sample - loss: 0.6142 - acc: 0.8248 - val_loss: 0.7959 - val_acc: 0.7596\n",
      "Epoch 12/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5760 - acc: 0.8356\n",
      "Epoch 00012: val_loss did not improve from 0.79592\n",
      "36805/36805 [==============================] - 12s 338us/sample - loss: 0.5756 - acc: 0.8356 - val_loss: 0.8011 - val_acc: 0.7603\n",
      "Epoch 13/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5458 - acc: 0.8445\n",
      "Epoch 00013: val_loss improved from 0.79592 to 0.72948, saving model to model/checkpoint/1D_CNN_BN_3_only_conv_checkpoint/013-0.7295.hdf5\n",
      "36805/36805 [==============================] - 13s 346us/sample - loss: 0.5460 - acc: 0.8444 - val_loss: 0.7295 - val_acc: 0.7880\n",
      "Epoch 14/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5133 - acc: 0.8554\n",
      "Epoch 00014: val_loss did not improve from 0.72948\n",
      "36805/36805 [==============================] - 13s 340us/sample - loss: 0.5132 - acc: 0.8555 - val_loss: 0.7412 - val_acc: 0.7876\n",
      "Epoch 15/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4891 - acc: 0.8619\n",
      "Epoch 00015: val_loss did not improve from 0.72948\n",
      "36805/36805 [==============================] - 13s 340us/sample - loss: 0.4894 - acc: 0.8619 - val_loss: 0.7335 - val_acc: 0.7887\n",
      "Epoch 16/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4624 - acc: 0.8709\n",
      "Epoch 00016: val_loss improved from 0.72948 to 0.69788, saving model to model/checkpoint/1D_CNN_BN_3_only_conv_checkpoint/016-0.6979.hdf5\n",
      "36805/36805 [==============================] - 13s 347us/sample - loss: 0.4630 - acc: 0.8709 - val_loss: 0.6979 - val_acc: 0.7976\n",
      "Epoch 17/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4363 - acc: 0.8786\n",
      "Epoch 00017: val_loss improved from 0.69788 to 0.68028, saving model to model/checkpoint/1D_CNN_BN_3_only_conv_checkpoint/017-0.6803.hdf5\n",
      "36805/36805 [==============================] - 13s 345us/sample - loss: 0.4359 - acc: 0.8787 - val_loss: 0.6803 - val_acc: 0.8057\n",
      "Epoch 18/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4135 - acc: 0.8852\n",
      "Epoch 00018: val_loss did not improve from 0.68028\n",
      "36805/36805 [==============================] - 13s 342us/sample - loss: 0.4135 - acc: 0.8852 - val_loss: 0.7813 - val_acc: 0.7766\n",
      "Epoch 19/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3968 - acc: 0.8904\n",
      "Epoch 00019: val_loss did not improve from 0.68028\n",
      "36805/36805 [==============================] - 13s 343us/sample - loss: 0.3967 - acc: 0.8904 - val_loss: 0.8332 - val_acc: 0.7543\n",
      "Epoch 20/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3741 - acc: 0.8981\n",
      "Epoch 00020: val_loss did not improve from 0.68028\n",
      "36805/36805 [==============================] - 12s 338us/sample - loss: 0.3742 - acc: 0.8981 - val_loss: 0.6815 - val_acc: 0.8036\n",
      "Epoch 21/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3538 - acc: 0.9049\n",
      "Epoch 00021: val_loss did not improve from 0.68028\n",
      "36805/36805 [==============================] - 13s 341us/sample - loss: 0.3542 - acc: 0.9049 - val_loss: 0.7289 - val_acc: 0.7939\n",
      "Epoch 22/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3371 - acc: 0.9109\n",
      "Epoch 00022: val_loss did not improve from 0.68028\n",
      "36805/36805 [==============================] - 13s 343us/sample - loss: 0.3370 - acc: 0.9109 - val_loss: 0.7830 - val_acc: 0.7717\n",
      "Epoch 23/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3168 - acc: 0.9165\n",
      "Epoch 00023: val_loss did not improve from 0.68028\n",
      "36805/36805 [==============================] - 13s 342us/sample - loss: 0.3168 - acc: 0.9165 - val_loss: 0.7678 - val_acc: 0.7864\n",
      "Epoch 24/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3023 - acc: 0.9210\n",
      "Epoch 00024: val_loss did not improve from 0.68028\n",
      "36805/36805 [==============================] - 12s 336us/sample - loss: 0.3024 - acc: 0.9210 - val_loss: 0.7412 - val_acc: 0.7911\n",
      "Epoch 25/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2896 - acc: 0.9250\n",
      "Epoch 00025: val_loss did not improve from 0.68028\n",
      "36805/36805 [==============================] - 12s 337us/sample - loss: 0.2894 - acc: 0.9251 - val_loss: 0.6844 - val_acc: 0.8006\n",
      "Epoch 26/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2700 - acc: 0.9317\n",
      "Epoch 00026: val_loss did not improve from 0.68028\n",
      "36805/36805 [==============================] - 12s 336us/sample - loss: 0.2701 - acc: 0.9316 - val_loss: 0.7507 - val_acc: 0.7894\n",
      "Epoch 27/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2602 - acc: 0.9338\n",
      "Epoch 00027: val_loss did not improve from 0.68028\n",
      "36805/36805 [==============================] - 13s 342us/sample - loss: 0.2603 - acc: 0.9337 - val_loss: 0.7253 - val_acc: 0.7964\n",
      "Epoch 28/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2473 - acc: 0.9395\n",
      "Epoch 00028: val_loss did not improve from 0.68028\n",
      "36805/36805 [==============================] - 12s 339us/sample - loss: 0.2475 - acc: 0.9394 - val_loss: 0.7571 - val_acc: 0.7894\n",
      "Epoch 29/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2377 - acc: 0.9418\n",
      "Epoch 00029: val_loss did not improve from 0.68028\n",
      "36805/36805 [==============================] - 12s 339us/sample - loss: 0.2379 - acc: 0.9416 - val_loss: 0.6999 - val_acc: 0.7962\n",
      "Epoch 30/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2192 - acc: 0.9481\n",
      "Epoch 00030: val_loss did not improve from 0.68028\n",
      "36805/36805 [==============================] - 12s 336us/sample - loss: 0.2191 - acc: 0.9481 - val_loss: 0.6982 - val_acc: 0.8137\n",
      "Epoch 31/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2100 - acc: 0.9517\n",
      "Epoch 00031: val_loss did not improve from 0.68028\n",
      "36805/36805 [==============================] - 13s 341us/sample - loss: 0.2100 - acc: 0.9517 - val_loss: 0.6972 - val_acc: 0.8048\n",
      "Epoch 32/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2049 - acc: 0.9517\n",
      "Epoch 00032: val_loss improved from 0.68028 to 0.66477, saving model to model/checkpoint/1D_CNN_BN_3_only_conv_checkpoint/032-0.6648.hdf5\n",
      "36805/36805 [==============================] - 13s 341us/sample - loss: 0.2048 - acc: 0.9517 - val_loss: 0.6648 - val_acc: 0.8169\n",
      "Epoch 33/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1890 - acc: 0.9559\n",
      "Epoch 00033: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 12s 339us/sample - loss: 0.1889 - acc: 0.9559 - val_loss: 0.8468 - val_acc: 0.7570\n",
      "Epoch 34/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1814 - acc: 0.9603\n",
      "Epoch 00034: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 12s 339us/sample - loss: 0.1812 - acc: 0.9604 - val_loss: 0.7747 - val_acc: 0.7878\n",
      "Epoch 35/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1743 - acc: 0.9625\n",
      "Epoch 00035: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 13s 341us/sample - loss: 0.1742 - acc: 0.9625 - val_loss: 0.7415 - val_acc: 0.7873\n",
      "Epoch 36/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1628 - acc: 0.9646\n",
      "Epoch 00036: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 13s 345us/sample - loss: 0.1628 - acc: 0.9646 - val_loss: 0.8604 - val_acc: 0.7680\n",
      "Epoch 37/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1572 - acc: 0.9662\n",
      "Epoch 00037: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 13s 341us/sample - loss: 0.1570 - acc: 0.9663 - val_loss: 0.7752 - val_acc: 0.7913\n",
      "Epoch 38/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1476 - acc: 0.9691\n",
      "Epoch 00038: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 13s 341us/sample - loss: 0.1478 - acc: 0.9690 - val_loss: 0.7509 - val_acc: 0.7978\n",
      "Epoch 39/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1460 - acc: 0.9694\n",
      "Epoch 00039: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 13s 342us/sample - loss: 0.1461 - acc: 0.9694 - val_loss: 0.7331 - val_acc: 0.8067\n",
      "Epoch 40/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1372 - acc: 0.9717\n",
      "Epoch 00040: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 12s 339us/sample - loss: 0.1372 - acc: 0.9717 - val_loss: 0.8117 - val_acc: 0.7713\n",
      "Epoch 41/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1263 - acc: 0.9765\n",
      "Epoch 00041: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 13s 341us/sample - loss: 0.1263 - acc: 0.9765 - val_loss: 0.7304 - val_acc: 0.8109\n",
      "Epoch 42/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1264 - acc: 0.9760\n",
      "Epoch 00042: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 13s 342us/sample - loss: 0.1267 - acc: 0.9760 - val_loss: 0.7428 - val_acc: 0.7999\n",
      "Epoch 43/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1147 - acc: 0.9801\n",
      "Epoch 00043: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 13s 341us/sample - loss: 0.1146 - acc: 0.9801 - val_loss: 0.9331 - val_acc: 0.7587\n",
      "Epoch 44/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1119 - acc: 0.9802\n",
      "Epoch 00044: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 12s 338us/sample - loss: 0.1120 - acc: 0.9802 - val_loss: 0.7338 - val_acc: 0.8048\n",
      "Epoch 45/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1057 - acc: 0.9815\n",
      "Epoch 00045: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 13s 343us/sample - loss: 0.1057 - acc: 0.9815 - val_loss: 0.8283 - val_acc: 0.7843\n",
      "Epoch 46/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1029 - acc: 0.9821\n",
      "Epoch 00046: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 12s 338us/sample - loss: 0.1028 - acc: 0.9822 - val_loss: 0.8057 - val_acc: 0.7897\n",
      "Epoch 47/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0960 - acc: 0.9846\n",
      "Epoch 00047: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 13s 340us/sample - loss: 0.0963 - acc: 0.9845 - val_loss: 0.7914 - val_acc: 0.7934\n",
      "Epoch 48/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0972 - acc: 0.9831\n",
      "Epoch 00048: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 13s 342us/sample - loss: 0.0972 - acc: 0.9831 - val_loss: 0.7703 - val_acc: 0.7978\n",
      "Epoch 49/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0865 - acc: 0.9861\n",
      "Epoch 00049: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 12s 340us/sample - loss: 0.0865 - acc: 0.9861 - val_loss: 0.9617 - val_acc: 0.7596\n",
      "Epoch 50/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0838 - acc: 0.9873\n",
      "Epoch 00050: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 12s 336us/sample - loss: 0.0840 - acc: 0.9873 - val_loss: 0.8427 - val_acc: 0.7850\n",
      "Epoch 51/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0794 - acc: 0.9887\n",
      "Epoch 00051: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 12s 338us/sample - loss: 0.0794 - acc: 0.9887 - val_loss: 0.7953 - val_acc: 0.7973\n",
      "Epoch 52/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0771 - acc: 0.9886\n",
      "Epoch 00052: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 12s 337us/sample - loss: 0.0771 - acc: 0.9885 - val_loss: 0.8110 - val_acc: 0.7999\n",
      "Epoch 53/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0752 - acc: 0.9887\n",
      "Epoch 00053: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 12s 338us/sample - loss: 0.0752 - acc: 0.9887 - val_loss: 0.8018 - val_acc: 0.8043\n",
      "Epoch 54/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0748 - acc: 0.9894\n",
      "Epoch 00054: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 13s 343us/sample - loss: 0.0748 - acc: 0.9894 - val_loss: 0.8765 - val_acc: 0.7801\n",
      "Epoch 55/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0666 - acc: 0.9904\n",
      "Epoch 00055: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 13s 342us/sample - loss: 0.0670 - acc: 0.9904 - val_loss: 0.8312 - val_acc: 0.7980\n",
      "Epoch 56/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0630 - acc: 0.9922\n",
      "Epoch 00056: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 12s 339us/sample - loss: 0.0631 - acc: 0.9921 - val_loss: 0.9565 - val_acc: 0.7710\n",
      "Epoch 57/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0713 - acc: 0.9888\n",
      "Epoch 00057: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 13s 342us/sample - loss: 0.0714 - acc: 0.9888 - val_loss: 0.9491 - val_acc: 0.7761\n",
      "Epoch 58/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0566 - acc: 0.9938\n",
      "Epoch 00058: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 13s 340us/sample - loss: 0.0567 - acc: 0.9938 - val_loss: 0.8767 - val_acc: 0.7973\n",
      "Epoch 59/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0612 - acc: 0.9913\n",
      "Epoch 00059: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 13s 342us/sample - loss: 0.0612 - acc: 0.9913 - val_loss: 0.8559 - val_acc: 0.7971\n",
      "Epoch 60/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0539 - acc: 0.9936\n",
      "Epoch 00060: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 13s 341us/sample - loss: 0.0538 - acc: 0.9936 - val_loss: 0.9201 - val_acc: 0.7745\n",
      "Epoch 61/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0537 - acc: 0.9935\n",
      "Epoch 00061: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 13s 340us/sample - loss: 0.0538 - acc: 0.9935 - val_loss: 0.8126 - val_acc: 0.8050\n",
      "Epoch 62/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0554 - acc: 0.9927\n",
      "Epoch 00062: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 12s 336us/sample - loss: 0.0554 - acc: 0.9926 - val_loss: 0.8875 - val_acc: 0.7799\n",
      "Epoch 63/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0460 - acc: 0.9960\n",
      "Epoch 00063: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 13s 341us/sample - loss: 0.0460 - acc: 0.9959 - val_loss: 1.0305 - val_acc: 0.7561\n",
      "Epoch 64/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0586 - acc: 0.9908\n",
      "Epoch 00064: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 13s 341us/sample - loss: 0.0586 - acc: 0.9908 - val_loss: 0.8197 - val_acc: 0.8053\n",
      "Epoch 65/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0445 - acc: 0.9954\n",
      "Epoch 00065: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 13s 341us/sample - loss: 0.0446 - acc: 0.9954 - val_loss: 0.8641 - val_acc: 0.8046\n",
      "Epoch 66/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0475 - acc: 0.9947\n",
      "Epoch 00066: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 13s 340us/sample - loss: 0.0474 - acc: 0.9947 - val_loss: 0.8840 - val_acc: 0.7927\n",
      "Epoch 67/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0436 - acc: 0.9951\n",
      "Epoch 00067: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 12s 339us/sample - loss: 0.0436 - acc: 0.9951 - val_loss: 0.9204 - val_acc: 0.7894\n",
      "Epoch 68/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0422 - acc: 0.9958\n",
      "Epoch 00068: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 12s 337us/sample - loss: 0.0422 - acc: 0.9958 - val_loss: 1.0181 - val_acc: 0.7729\n",
      "Epoch 69/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0456 - acc: 0.9943\n",
      "Epoch 00069: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 13s 340us/sample - loss: 0.0456 - acc: 0.9943 - val_loss: 0.9992 - val_acc: 0.7675\n",
      "Epoch 70/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0405 - acc: 0.9959\n",
      "Epoch 00070: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 13s 340us/sample - loss: 0.0406 - acc: 0.9959 - val_loss: 0.8884 - val_acc: 0.7952\n",
      "Epoch 71/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0412 - acc: 0.9953\n",
      "Epoch 00071: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 12s 339us/sample - loss: 0.0412 - acc: 0.9954 - val_loss: 0.9482 - val_acc: 0.7820\n",
      "Epoch 72/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0349 - acc: 0.9963\n",
      "Epoch 00072: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 12s 338us/sample - loss: 0.0349 - acc: 0.9963 - val_loss: 0.9452 - val_acc: 0.7734\n",
      "Epoch 73/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0372 - acc: 0.9961\n",
      "Epoch 00073: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 13s 341us/sample - loss: 0.0373 - acc: 0.9961 - val_loss: 0.9465 - val_acc: 0.7899\n",
      "Epoch 74/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0422 - acc: 0.9945\n",
      "Epoch 00074: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 13s 341us/sample - loss: 0.0422 - acc: 0.9945 - val_loss: 0.8982 - val_acc: 0.7920\n",
      "Epoch 75/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0351 - acc: 0.9967\n",
      "Epoch 00075: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 13s 341us/sample - loss: 0.0353 - acc: 0.9966 - val_loss: 0.9884 - val_acc: 0.7773\n",
      "Epoch 76/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0502 - acc: 0.9924\n",
      "Epoch 00076: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 13s 343us/sample - loss: 0.0502 - acc: 0.9924 - val_loss: 0.8880 - val_acc: 0.8006\n",
      "Epoch 77/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0328 - acc: 0.9969\n",
      "Epoch 00077: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 13s 340us/sample - loss: 0.0329 - acc: 0.9969 - val_loss: 0.9128 - val_acc: 0.7897\n",
      "Epoch 78/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0362 - acc: 0.9954\n",
      "Epoch 00078: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 12s 339us/sample - loss: 0.0362 - acc: 0.9954 - val_loss: 0.8962 - val_acc: 0.7994\n",
      "Epoch 79/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0324 - acc: 0.9970\n",
      "Epoch 00079: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 13s 340us/sample - loss: 0.0324 - acc: 0.9970 - val_loss: 1.1286 - val_acc: 0.7661\n",
      "Epoch 80/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0307 - acc: 0.9973\n",
      "Epoch 00080: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 12s 339us/sample - loss: 0.0307 - acc: 0.9973 - val_loss: 0.9891 - val_acc: 0.7885\n",
      "Epoch 81/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0318 - acc: 0.9969\n",
      "Epoch 00081: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 13s 344us/sample - loss: 0.0317 - acc: 0.9969 - val_loss: 0.9794 - val_acc: 0.7904\n",
      "Epoch 82/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0258 - acc: 0.9978\n",
      "Epoch 00082: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 12s 340us/sample - loss: 0.0258 - acc: 0.9979 - val_loss: 1.0307 - val_acc: 0.7859\n",
      "Epoch 83/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0304 - acc: 0.9964\n",
      "Epoch 00083: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 13s 340us/sample - loss: 0.0304 - acc: 0.9964 - val_loss: 0.9535 - val_acc: 0.7918\n",
      "Epoch 84/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0294 - acc: 0.9973\n",
      "Epoch 00084: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 12s 339us/sample - loss: 0.0295 - acc: 0.9973 - val_loss: 0.8958 - val_acc: 0.8015\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0301 - acc: 0.9967\n",
      "Epoch 00085: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 12s 339us/sample - loss: 0.0301 - acc: 0.9967 - val_loss: 1.1645 - val_acc: 0.7624\n",
      "Epoch 86/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0383 - acc: 0.9945\n",
      "Epoch 00086: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 13s 342us/sample - loss: 0.0383 - acc: 0.9945 - val_loss: 1.0364 - val_acc: 0.7803\n",
      "Epoch 87/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0253 - acc: 0.9978\n",
      "Epoch 00087: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 13s 340us/sample - loss: 0.0259 - acc: 0.9978 - val_loss: 0.9002 - val_acc: 0.7985\n",
      "Epoch 88/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0324 - acc: 0.9956\n",
      "Epoch 00088: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 13s 343us/sample - loss: 0.0324 - acc: 0.9956 - val_loss: 0.9141 - val_acc: 0.7994\n",
      "Epoch 89/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0224 - acc: 0.9984\n",
      "Epoch 00089: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 13s 341us/sample - loss: 0.0224 - acc: 0.9984 - val_loss: 0.9100 - val_acc: 0.8039\n",
      "Epoch 90/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0255 - acc: 0.9972\n",
      "Epoch 00090: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 12s 339us/sample - loss: 0.0256 - acc: 0.9972 - val_loss: 1.0264 - val_acc: 0.7876\n",
      "Epoch 91/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0328 - acc: 0.9954\n",
      "Epoch 00091: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 13s 342us/sample - loss: 0.0328 - acc: 0.9954 - val_loss: 0.8741 - val_acc: 0.8104\n",
      "Epoch 92/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0204 - acc: 0.9986\n",
      "Epoch 00092: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 13s 342us/sample - loss: 0.0204 - acc: 0.9986 - val_loss: 1.1427 - val_acc: 0.7601\n",
      "Epoch 93/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0270 - acc: 0.9965\n",
      "Epoch 00093: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 13s 342us/sample - loss: 0.0270 - acc: 0.9965 - val_loss: 0.9316 - val_acc: 0.8025\n",
      "Epoch 94/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0271 - acc: 0.9965\n",
      "Epoch 00094: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 13s 341us/sample - loss: 0.0271 - acc: 0.9965 - val_loss: 0.9275 - val_acc: 0.8048\n",
      "Epoch 95/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0242 - acc: 0.9976\n",
      "Epoch 00095: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 13s 341us/sample - loss: 0.0242 - acc: 0.9976 - val_loss: 0.9845 - val_acc: 0.7929\n",
      "Epoch 96/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0269 - acc: 0.9969\n",
      "Epoch 00096: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 13s 341us/sample - loss: 0.0269 - acc: 0.9969 - val_loss: 1.2078 - val_acc: 0.7582\n",
      "Epoch 97/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0237 - acc: 0.9974\n",
      "Epoch 00097: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 13s 343us/sample - loss: 0.0237 - acc: 0.9974 - val_loss: 0.8993 - val_acc: 0.8146\n",
      "Epoch 98/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0182 - acc: 0.9987\n",
      "Epoch 00098: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 13s 340us/sample - loss: 0.0183 - acc: 0.9987 - val_loss: 1.0594 - val_acc: 0.7848\n",
      "Epoch 99/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0319 - acc: 0.9945\n",
      "Epoch 00099: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 13s 344us/sample - loss: 0.0319 - acc: 0.9945 - val_loss: 1.0541 - val_acc: 0.7822\n",
      "Epoch 100/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0160 - acc: 0.9993\n",
      "Epoch 00100: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 13s 344us/sample - loss: 0.0160 - acc: 0.9993 - val_loss: 0.9642 - val_acc: 0.8034\n",
      "Epoch 101/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0256 - acc: 0.9970\n",
      "Epoch 00101: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 13s 342us/sample - loss: 0.0255 - acc: 0.9970 - val_loss: 0.9944 - val_acc: 0.7920\n",
      "Epoch 102/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0256 - acc: 0.9968\n",
      "Epoch 00102: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 13s 340us/sample - loss: 0.0256 - acc: 0.9968 - val_loss: 0.9446 - val_acc: 0.8015\n",
      "Epoch 103/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0159 - acc: 0.9991\n",
      "Epoch 00103: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 13s 340us/sample - loss: 0.0159 - acc: 0.9991 - val_loss: 1.0336 - val_acc: 0.7894\n",
      "Epoch 104/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0191 - acc: 0.9983\n",
      "Epoch 00104: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 12s 339us/sample - loss: 0.0191 - acc: 0.9983 - val_loss: 0.9957 - val_acc: 0.7980\n",
      "Epoch 105/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0204 - acc: 0.9979\n",
      "Epoch 00105: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 13s 343us/sample - loss: 0.0204 - acc: 0.9979 - val_loss: 1.4740 - val_acc: 0.7177\n",
      "Epoch 106/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0197 - acc: 0.9983\n",
      "Epoch 00106: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 13s 342us/sample - loss: 0.0197 - acc: 0.9983 - val_loss: 1.0934 - val_acc: 0.7754\n",
      "Epoch 107/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0253 - acc: 0.9965\n",
      "Epoch 00107: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 13s 341us/sample - loss: 0.0253 - acc: 0.9964 - val_loss: 1.0364 - val_acc: 0.7904\n",
      "Epoch 108/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0219 - acc: 0.9973\n",
      "Epoch 00108: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 13s 342us/sample - loss: 0.0219 - acc: 0.9973 - val_loss: 1.1627 - val_acc: 0.7796\n",
      "Epoch 109/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0225 - acc: 0.9969\n",
      "Epoch 00109: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 12s 339us/sample - loss: 0.0225 - acc: 0.9969 - val_loss: 0.9169 - val_acc: 0.8153\n",
      "Epoch 110/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0171 - acc: 0.9984\n",
      "Epoch 00110: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 13s 345us/sample - loss: 0.0170 - acc: 0.9985 - val_loss: 1.0546 - val_acc: 0.7913\n",
      "Epoch 111/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0167 - acc: 0.9985\n",
      "Epoch 00111: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 12s 339us/sample - loss: 0.0167 - acc: 0.9985 - val_loss: 1.3170 - val_acc: 0.7582\n",
      "Epoch 112/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0193 - acc: 0.9982\n",
      "Epoch 00112: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 12s 339us/sample - loss: 0.0192 - acc: 0.9982 - val_loss: 0.9477 - val_acc: 0.8069\n",
      "Epoch 113/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0197 - acc: 0.9974\n",
      "Epoch 00113: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 13s 340us/sample - loss: 0.0198 - acc: 0.9974 - val_loss: 1.1765 - val_acc: 0.7766\n",
      "Epoch 114/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0249 - acc: 0.9965\n",
      "Epoch 00114: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 13s 340us/sample - loss: 0.0249 - acc: 0.9965 - val_loss: 0.9813 - val_acc: 0.7990\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 115/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0181 - acc: 0.9981\n",
      "Epoch 00115: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 13s 340us/sample - loss: 0.0182 - acc: 0.9980 - val_loss: 1.0355 - val_acc: 0.7918\n",
      "Epoch 116/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0152 - acc: 0.9988\n",
      "Epoch 00116: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 13s 343us/sample - loss: 0.0153 - acc: 0.9988 - val_loss: 1.3192 - val_acc: 0.7487\n",
      "Epoch 117/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0307 - acc: 0.9947\n",
      "Epoch 00117: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 12s 339us/sample - loss: 0.0307 - acc: 0.9948 - val_loss: 0.9366 - val_acc: 0.8088\n",
      "Epoch 118/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0148 - acc: 0.9988\n",
      "Epoch 00118: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 12s 338us/sample - loss: 0.0148 - acc: 0.9988 - val_loss: 0.9721 - val_acc: 0.8076\n",
      "Epoch 119/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0146 - acc: 0.9989\n",
      "Epoch 00119: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 13s 341us/sample - loss: 0.0146 - acc: 0.9989 - val_loss: 0.9807 - val_acc: 0.8097\n",
      "Epoch 120/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0191 - acc: 0.9974\n",
      "Epoch 00120: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 13s 340us/sample - loss: 0.0191 - acc: 0.9974 - val_loss: 1.2052 - val_acc: 0.7706\n",
      "Epoch 121/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0193 - acc: 0.9976\n",
      "Epoch 00121: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 13s 341us/sample - loss: 0.0193 - acc: 0.9976 - val_loss: 1.0935 - val_acc: 0.7855\n",
      "Epoch 122/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0188 - acc: 0.9976\n",
      "Epoch 00122: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 13s 340us/sample - loss: 0.0189 - acc: 0.9976 - val_loss: 1.1995 - val_acc: 0.7736\n",
      "Epoch 123/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0164 - acc: 0.9982\n",
      "Epoch 00123: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 13s 344us/sample - loss: 0.0168 - acc: 0.9982 - val_loss: 1.0604 - val_acc: 0.7973\n",
      "Epoch 124/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0157 - acc: 0.9985\n",
      "Epoch 00124: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 13s 341us/sample - loss: 0.0160 - acc: 0.9984 - val_loss: 1.0581 - val_acc: 0.7899\n",
      "Epoch 125/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0256 - acc: 0.9955\n",
      "Epoch 00125: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 13s 340us/sample - loss: 0.0256 - acc: 0.9955 - val_loss: 0.9700 - val_acc: 0.8041\n",
      "Epoch 126/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0124 - acc: 0.9991\n",
      "Epoch 00126: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 13s 340us/sample - loss: 0.0125 - acc: 0.9991 - val_loss: 1.0052 - val_acc: 0.8062\n",
      "Epoch 127/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0143 - acc: 0.9985\n",
      "Epoch 00127: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 13s 341us/sample - loss: 0.0144 - acc: 0.9985 - val_loss: 1.0677 - val_acc: 0.7941\n",
      "Epoch 128/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0250 - acc: 0.9959\n",
      "Epoch 00128: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 13s 342us/sample - loss: 0.0250 - acc: 0.9959 - val_loss: 1.0036 - val_acc: 0.8057\n",
      "Epoch 129/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0146 - acc: 0.9986\n",
      "Epoch 00129: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 13s 340us/sample - loss: 0.0146 - acc: 0.9986 - val_loss: 1.0918 - val_acc: 0.7959\n",
      "Epoch 130/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0135 - acc: 0.9988\n",
      "Epoch 00130: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 13s 340us/sample - loss: 0.0135 - acc: 0.9988 - val_loss: 1.1004 - val_acc: 0.7929\n",
      "Epoch 131/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0115 - acc: 0.9993\n",
      "Epoch 00131: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 13s 341us/sample - loss: 0.0115 - acc: 0.9993 - val_loss: 1.1086 - val_acc: 0.7843\n",
      "Epoch 132/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0210 - acc: 0.9970\n",
      "Epoch 00132: val_loss did not improve from 0.66477\n",
      "36805/36805 [==============================] - 13s 341us/sample - loss: 0.0210 - acc: 0.9970 - val_loss: 1.2484 - val_acc: 0.7603\n",
      "\n",
      "1D_CNN_BN_3_only_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 1s 258us/sample - loss: 0.7525 - acc: 0.7890\n",
      "Loss: 0.7524969367594734 Accuracy: 0.7889927\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9704 - acc: 0.3779\n",
      "Epoch 00001: val_loss improved from inf to 1.66222, saving model to model/checkpoint/1D_CNN_BN_4_only_conv_checkpoint/001-1.6622.hdf5\n",
      "36805/36805 [==============================] - 18s 498us/sample - loss: 1.9704 - acc: 0.3779 - val_loss: 1.6622 - val_acc: 0.4677\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2920 - acc: 0.6000\n",
      "Epoch 00002: val_loss improved from 1.66222 to 1.18882, saving model to model/checkpoint/1D_CNN_BN_4_only_conv_checkpoint/002-1.1888.hdf5\n",
      "36805/36805 [==============================] - 14s 378us/sample - loss: 1.2921 - acc: 0.6000 - val_loss: 1.1888 - val_acc: 0.6161\n",
      "Epoch 3/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0185 - acc: 0.6920\n",
      "Epoch 00003: val_loss improved from 1.18882 to 0.89441, saving model to model/checkpoint/1D_CNN_BN_4_only_conv_checkpoint/003-0.8944.hdf5\n",
      "36805/36805 [==============================] - 14s 377us/sample - loss: 1.0177 - acc: 0.6924 - val_loss: 0.8944 - val_acc: 0.7333\n",
      "Epoch 4/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8372 - acc: 0.7514\n",
      "Epoch 00004: val_loss improved from 0.89441 to 0.78422, saving model to model/checkpoint/1D_CNN_BN_4_only_conv_checkpoint/004-0.7842.hdf5\n",
      "36805/36805 [==============================] - 14s 376us/sample - loss: 0.8375 - acc: 0.7514 - val_loss: 0.7842 - val_acc: 0.7622\n",
      "Epoch 5/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7102 - acc: 0.7940\n",
      "Epoch 00005: val_loss improved from 0.78422 to 0.73763, saving model to model/checkpoint/1D_CNN_BN_4_only_conv_checkpoint/005-0.7376.hdf5\n",
      "36805/36805 [==============================] - 14s 377us/sample - loss: 0.7097 - acc: 0.7942 - val_loss: 0.7376 - val_acc: 0.7808\n",
      "Epoch 6/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.6149 - acc: 0.8237\n",
      "Epoch 00006: val_loss improved from 0.73763 to 0.63869, saving model to model/checkpoint/1D_CNN_BN_4_only_conv_checkpoint/006-0.6387.hdf5\n",
      "36805/36805 [==============================] - 14s 375us/sample - loss: 0.6146 - acc: 0.8238 - val_loss: 0.6387 - val_acc: 0.8164\n",
      "Epoch 7/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5419 - acc: 0.8447\n",
      "Epoch 00007: val_loss improved from 0.63869 to 0.57049, saving model to model/checkpoint/1D_CNN_BN_4_only_conv_checkpoint/007-0.5705.hdf5\n",
      "36805/36805 [==============================] - 14s 377us/sample - loss: 0.5416 - acc: 0.8447 - val_loss: 0.5705 - val_acc: 0.8318\n",
      "Epoch 8/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4849 - acc: 0.8612\n",
      "Epoch 00008: val_loss improved from 0.57049 to 0.55116, saving model to model/checkpoint/1D_CNN_BN_4_only_conv_checkpoint/008-0.5512.hdf5\n",
      "36805/36805 [==============================] - 14s 375us/sample - loss: 0.4851 - acc: 0.8612 - val_loss: 0.5512 - val_acc: 0.8397\n",
      "Epoch 9/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.4416 - acc: 0.8743\n",
      "Epoch 00009: val_loss improved from 0.55116 to 0.47891, saving model to model/checkpoint/1D_CNN_BN_4_only_conv_checkpoint/009-0.4789.hdf5\n",
      "36805/36805 [==============================] - 14s 380us/sample - loss: 0.4415 - acc: 0.8743 - val_loss: 0.4789 - val_acc: 0.8630\n",
      "Epoch 10/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4067 - acc: 0.8860\n",
      "Epoch 00010: val_loss improved from 0.47891 to 0.47874, saving model to model/checkpoint/1D_CNN_BN_4_only_conv_checkpoint/010-0.4787.hdf5\n",
      "36805/36805 [==============================] - 14s 374us/sample - loss: 0.4066 - acc: 0.8860 - val_loss: 0.4787 - val_acc: 0.8651\n",
      "Epoch 11/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3710 - acc: 0.8968\n",
      "Epoch 00011: val_loss improved from 0.47874 to 0.41844, saving model to model/checkpoint/1D_CNN_BN_4_only_conv_checkpoint/011-0.4184.hdf5\n",
      "36805/36805 [==============================] - 14s 377us/sample - loss: 0.3710 - acc: 0.8969 - val_loss: 0.4184 - val_acc: 0.8807\n",
      "Epoch 12/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3426 - acc: 0.9047\n",
      "Epoch 00012: val_loss did not improve from 0.41844\n",
      "36805/36805 [==============================] - 14s 375us/sample - loss: 0.3427 - acc: 0.9047 - val_loss: 0.4547 - val_acc: 0.8728\n",
      "Epoch 13/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3249 - acc: 0.9090\n",
      "Epoch 00013: val_loss did not improve from 0.41844\n",
      "36805/36805 [==============================] - 14s 373us/sample - loss: 0.3248 - acc: 0.9091 - val_loss: 0.4430 - val_acc: 0.8691\n",
      "Epoch 14/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2998 - acc: 0.9168\n",
      "Epoch 00014: val_loss improved from 0.41844 to 0.38272, saving model to model/checkpoint/1D_CNN_BN_4_only_conv_checkpoint/014-0.3827.hdf5\n",
      "36805/36805 [==============================] - 14s 376us/sample - loss: 0.2998 - acc: 0.9168 - val_loss: 0.3827 - val_acc: 0.8959\n",
      "Epoch 15/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2800 - acc: 0.9226\n",
      "Epoch 00015: val_loss did not improve from 0.38272\n",
      "36805/36805 [==============================] - 14s 372us/sample - loss: 0.2797 - acc: 0.9227 - val_loss: 0.4275 - val_acc: 0.8807\n",
      "Epoch 16/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2637 - acc: 0.9259\n",
      "Epoch 00016: val_loss improved from 0.38272 to 0.36469, saving model to model/checkpoint/1D_CNN_BN_4_only_conv_checkpoint/016-0.3647.hdf5\n",
      "36805/36805 [==============================] - 14s 375us/sample - loss: 0.2635 - acc: 0.9260 - val_loss: 0.3647 - val_acc: 0.8915\n",
      "Epoch 17/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2457 - acc: 0.9327\n",
      "Epoch 00017: val_loss improved from 0.36469 to 0.33632, saving model to model/checkpoint/1D_CNN_BN_4_only_conv_checkpoint/017-0.3363.hdf5\n",
      "36805/36805 [==============================] - 14s 378us/sample - loss: 0.2458 - acc: 0.9327 - val_loss: 0.3363 - val_acc: 0.9066\n",
      "Epoch 18/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2340 - acc: 0.9357\n",
      "Epoch 00018: val_loss did not improve from 0.33632\n",
      "36805/36805 [==============================] - 14s 371us/sample - loss: 0.2340 - acc: 0.9357 - val_loss: 0.3778 - val_acc: 0.8912\n",
      "Epoch 19/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2189 - acc: 0.9397\n",
      "Epoch 00019: val_loss did not improve from 0.33632\n",
      "36805/36805 [==============================] - 14s 373us/sample - loss: 0.2194 - acc: 0.9395 - val_loss: 0.4364 - val_acc: 0.8779\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2146 - acc: 0.9410\n",
      "Epoch 00020: val_loss did not improve from 0.33632\n",
      "36805/36805 [==============================] - 14s 377us/sample - loss: 0.2146 - acc: 0.9410 - val_loss: 0.3655 - val_acc: 0.8968\n",
      "Epoch 21/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1949 - acc: 0.9486\n",
      "Epoch 00021: val_loss did not improve from 0.33632\n",
      "36805/36805 [==============================] - 14s 373us/sample - loss: 0.1951 - acc: 0.9486 - val_loss: 0.3462 - val_acc: 0.9029\n",
      "Epoch 22/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1899 - acc: 0.9492\n",
      "Epoch 00022: val_loss did not improve from 0.33632\n",
      "36805/36805 [==============================] - 14s 372us/sample - loss: 0.1902 - acc: 0.9492 - val_loss: 0.3439 - val_acc: 0.9052\n",
      "Epoch 23/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1778 - acc: 0.9523\n",
      "Epoch 00023: val_loss improved from 0.33632 to 0.33480, saving model to model/checkpoint/1D_CNN_BN_4_only_conv_checkpoint/023-0.3348.hdf5\n",
      "36805/36805 [==============================] - 14s 378us/sample - loss: 0.1782 - acc: 0.9522 - val_loss: 0.3348 - val_acc: 0.9001\n",
      "Epoch 24/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1723 - acc: 0.9549\n",
      "Epoch 00024: val_loss did not improve from 0.33480\n",
      "36805/36805 [==============================] - 14s 373us/sample - loss: 0.1722 - acc: 0.9550 - val_loss: 0.3944 - val_acc: 0.8887\n",
      "Epoch 25/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1599 - acc: 0.9579\n",
      "Epoch 00025: val_loss did not improve from 0.33480\n",
      "36805/36805 [==============================] - 14s 372us/sample - loss: 0.1599 - acc: 0.9579 - val_loss: 0.3477 - val_acc: 0.8996\n",
      "Epoch 26/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1500 - acc: 0.9618\n",
      "Epoch 00026: val_loss improved from 0.33480 to 0.31781, saving model to model/checkpoint/1D_CNN_BN_4_only_conv_checkpoint/026-0.3178.hdf5\n",
      "36805/36805 [==============================] - 14s 375us/sample - loss: 0.1502 - acc: 0.9617 - val_loss: 0.3178 - val_acc: 0.9082\n",
      "Epoch 27/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1464 - acc: 0.9616\n",
      "Epoch 00027: val_loss did not improve from 0.31781\n",
      "36805/36805 [==============================] - 14s 372us/sample - loss: 0.1464 - acc: 0.9616 - val_loss: 0.3947 - val_acc: 0.8842\n",
      "Epoch 28/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1362 - acc: 0.9653\n",
      "Epoch 00028: val_loss improved from 0.31781 to 0.31460, saving model to model/checkpoint/1D_CNN_BN_4_only_conv_checkpoint/028-0.3146.hdf5\n",
      "36805/36805 [==============================] - 14s 375us/sample - loss: 0.1362 - acc: 0.9653 - val_loss: 0.3146 - val_acc: 0.9124\n",
      "Epoch 29/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1299 - acc: 0.9684\n",
      "Epoch 00029: val_loss did not improve from 0.31460\n",
      "36805/36805 [==============================] - 14s 373us/sample - loss: 0.1299 - acc: 0.9683 - val_loss: 0.3226 - val_acc: 0.9113\n",
      "Epoch 30/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1226 - acc: 0.9707\n",
      "Epoch 00030: val_loss improved from 0.31460 to 0.30026, saving model to model/checkpoint/1D_CNN_BN_4_only_conv_checkpoint/030-0.3003.hdf5\n",
      "36805/36805 [==============================] - 14s 374us/sample - loss: 0.1225 - acc: 0.9707 - val_loss: 0.3003 - val_acc: 0.9175\n",
      "Epoch 31/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1230 - acc: 0.9702\n",
      "Epoch 00031: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 370us/sample - loss: 0.1231 - acc: 0.9702 - val_loss: 0.3797 - val_acc: 0.8908\n",
      "Epoch 32/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1115 - acc: 0.9736\n",
      "Epoch 00032: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 373us/sample - loss: 0.1115 - acc: 0.9736 - val_loss: 0.3376 - val_acc: 0.9033\n",
      "Epoch 33/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1044 - acc: 0.9757\n",
      "Epoch 00033: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 372us/sample - loss: 0.1049 - acc: 0.9757 - val_loss: 0.4060 - val_acc: 0.8901\n",
      "Epoch 34/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1031 - acc: 0.9756\n",
      "Epoch 00034: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 371us/sample - loss: 0.1032 - acc: 0.9757 - val_loss: 0.3388 - val_acc: 0.9103\n",
      "Epoch 35/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0954 - acc: 0.9777\n",
      "Epoch 00035: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 371us/sample - loss: 0.0956 - acc: 0.9777 - val_loss: 0.3897 - val_acc: 0.8968\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0919 - acc: 0.9795\n",
      "Epoch 00036: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 373us/sample - loss: 0.0923 - acc: 0.9794 - val_loss: 0.3909 - val_acc: 0.8921\n",
      "Epoch 37/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0911 - acc: 0.9791\n",
      "Epoch 00037: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 374us/sample - loss: 0.0911 - acc: 0.9791 - val_loss: 0.3289 - val_acc: 0.9094\n",
      "Epoch 38/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0813 - acc: 0.9826\n",
      "Epoch 00038: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 374us/sample - loss: 0.0813 - acc: 0.9827 - val_loss: 0.3228 - val_acc: 0.9133\n",
      "Epoch 39/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0770 - acc: 0.9831\n",
      "Epoch 00039: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 370us/sample - loss: 0.0770 - acc: 0.9832 - val_loss: 0.3279 - val_acc: 0.9089\n",
      "Epoch 40/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0762 - acc: 0.9836\n",
      "Epoch 00040: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 370us/sample - loss: 0.0763 - acc: 0.9836 - val_loss: 0.3016 - val_acc: 0.9168\n",
      "Epoch 41/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0756 - acc: 0.9830\n",
      "Epoch 00041: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 374us/sample - loss: 0.0757 - acc: 0.9829 - val_loss: 0.3249 - val_acc: 0.9094\n",
      "Epoch 42/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0669 - acc: 0.9865\n",
      "Epoch 00042: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 371us/sample - loss: 0.0670 - acc: 0.9865 - val_loss: 0.3424 - val_acc: 0.9061\n",
      "Epoch 43/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0624 - acc: 0.9883\n",
      "Epoch 00043: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 372us/sample - loss: 0.0625 - acc: 0.9883 - val_loss: 0.4314 - val_acc: 0.8873\n",
      "Epoch 44/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0641 - acc: 0.9875\n",
      "Epoch 00044: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 13s 367us/sample - loss: 0.0641 - acc: 0.9875 - val_loss: 0.3904 - val_acc: 0.8966\n",
      "Epoch 45/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0624 - acc: 0.9872\n",
      "Epoch 00045: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 372us/sample - loss: 0.0625 - acc: 0.9871 - val_loss: 0.4127 - val_acc: 0.8942\n",
      "Epoch 46/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0631 - acc: 0.9874\n",
      "Epoch 00046: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 374us/sample - loss: 0.0630 - acc: 0.9874 - val_loss: 0.4276 - val_acc: 0.8880\n",
      "Epoch 47/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0555 - acc: 0.9897\n",
      "Epoch 00047: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 375us/sample - loss: 0.0555 - acc: 0.9898 - val_loss: 0.3268 - val_acc: 0.9136\n",
      "Epoch 48/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0524 - acc: 0.9905\n",
      "Epoch 00048: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 371us/sample - loss: 0.0528 - acc: 0.9905 - val_loss: 0.3800 - val_acc: 0.8977\n",
      "Epoch 49/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0680 - acc: 0.9854\n",
      "Epoch 00049: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 371us/sample - loss: 0.0680 - acc: 0.9854 - val_loss: 0.3076 - val_acc: 0.9182\n",
      "Epoch 50/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0419 - acc: 0.9944\n",
      "Epoch 00050: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 370us/sample - loss: 0.0419 - acc: 0.9944 - val_loss: 0.3494 - val_acc: 0.9029\n",
      "Epoch 51/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0511 - acc: 0.9907\n",
      "Epoch 00051: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 369us/sample - loss: 0.0510 - acc: 0.9907 - val_loss: 0.3110 - val_acc: 0.9154\n",
      "Epoch 52/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0420 - acc: 0.9927\n",
      "Epoch 00052: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 373us/sample - loss: 0.0419 - acc: 0.9927 - val_loss: 0.3772 - val_acc: 0.9005\n",
      "Epoch 53/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0415 - acc: 0.9927\n",
      "Epoch 00053: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 372us/sample - loss: 0.0416 - acc: 0.9927 - val_loss: 0.4013 - val_acc: 0.9022\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0508 - acc: 0.9899\n",
      "Epoch 00054: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 370us/sample - loss: 0.0508 - acc: 0.9899 - val_loss: 0.3319 - val_acc: 0.9113\n",
      "Epoch 55/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0367 - acc: 0.9942\n",
      "Epoch 00055: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 370us/sample - loss: 0.0368 - acc: 0.9942 - val_loss: 0.3332 - val_acc: 0.9185\n",
      "Epoch 56/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0402 - acc: 0.9936\n",
      "Epoch 00056: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 371us/sample - loss: 0.0403 - acc: 0.9936 - val_loss: 0.3815 - val_acc: 0.9036\n",
      "Epoch 57/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0321 - acc: 0.9961\n",
      "Epoch 00057: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 372us/sample - loss: 0.0322 - acc: 0.9960 - val_loss: 0.4263 - val_acc: 0.8933\n",
      "Epoch 58/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0395 - acc: 0.9933\n",
      "Epoch 00058: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 375us/sample - loss: 0.0397 - acc: 0.9933 - val_loss: 0.3392 - val_acc: 0.9126\n",
      "Epoch 59/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0367 - acc: 0.9942\n",
      "Epoch 00059: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 373us/sample - loss: 0.0366 - acc: 0.9942 - val_loss: 0.3440 - val_acc: 0.9103\n",
      "Epoch 60/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0302 - acc: 0.9959\n",
      "Epoch 00060: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 373us/sample - loss: 0.0302 - acc: 0.9959 - val_loss: 0.3248 - val_acc: 0.9147\n",
      "Epoch 61/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0279 - acc: 0.9968\n",
      "Epoch 00061: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 373us/sample - loss: 0.0279 - acc: 0.9967 - val_loss: 0.4463 - val_acc: 0.8926\n",
      "Epoch 62/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0347 - acc: 0.9944\n",
      "Epoch 00062: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 372us/sample - loss: 0.0347 - acc: 0.9944 - val_loss: 0.3600 - val_acc: 0.9012\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0287 - acc: 0.9960\n",
      "Epoch 00063: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 378us/sample - loss: 0.0289 - acc: 0.9959 - val_loss: 0.6083 - val_acc: 0.8595\n",
      "Epoch 64/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0437 - acc: 0.9900\n",
      "Epoch 00064: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 372us/sample - loss: 0.0436 - acc: 0.9900 - val_loss: 0.3837 - val_acc: 0.9052\n",
      "Epoch 65/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0225 - acc: 0.9975\n",
      "Epoch 00065: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 372us/sample - loss: 0.0224 - acc: 0.9975 - val_loss: 0.3960 - val_acc: 0.9010\n",
      "Epoch 66/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0228 - acc: 0.9974\n",
      "Epoch 00066: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 373us/sample - loss: 0.0229 - acc: 0.9973 - val_loss: 0.3495 - val_acc: 0.9092\n",
      "Epoch 67/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0300 - acc: 0.9952\n",
      "Epoch 00067: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 372us/sample - loss: 0.0299 - acc: 0.9952 - val_loss: 0.4071 - val_acc: 0.9040\n",
      "Epoch 68/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0270 - acc: 0.9956\n",
      "Epoch 00068: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 374us/sample - loss: 0.0270 - acc: 0.9957 - val_loss: 0.3214 - val_acc: 0.9206\n",
      "Epoch 69/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0235 - acc: 0.9963\n",
      "Epoch 00069: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 373us/sample - loss: 0.0235 - acc: 0.9963 - val_loss: 0.4092 - val_acc: 0.9057\n",
      "Epoch 70/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0260 - acc: 0.9959\n",
      "Epoch 00070: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 373us/sample - loss: 0.0260 - acc: 0.9959 - val_loss: 0.3889 - val_acc: 0.9059\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0229 - acc: 0.9967\n",
      "Epoch 00071: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 372us/sample - loss: 0.0229 - acc: 0.9967 - val_loss: 0.3683 - val_acc: 0.9110\n",
      "Epoch 72/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0276 - acc: 0.9949\n",
      "Epoch 00072: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 372us/sample - loss: 0.0276 - acc: 0.9949 - val_loss: 0.4122 - val_acc: 0.9043\n",
      "Epoch 73/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0245 - acc: 0.9961\n",
      "Epoch 00073: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 372us/sample - loss: 0.0244 - acc: 0.9961 - val_loss: 0.3973 - val_acc: 0.9026\n",
      "Epoch 74/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0153 - acc: 0.9987\n",
      "Epoch 00074: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 373us/sample - loss: 0.0155 - acc: 0.9987 - val_loss: 0.4097 - val_acc: 0.9064\n",
      "Epoch 75/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0328 - acc: 0.9933\n",
      "Epoch 00075: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 371us/sample - loss: 0.0328 - acc: 0.9933 - val_loss: 0.3244 - val_acc: 0.9194\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0154 - acc: 0.9985\n",
      "Epoch 00076: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 372us/sample - loss: 0.0154 - acc: 0.9985 - val_loss: 0.3583 - val_acc: 0.9119\n",
      "Epoch 77/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0254 - acc: 0.9958\n",
      "Epoch 00077: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 373us/sample - loss: 0.0255 - acc: 0.9958 - val_loss: 0.5033 - val_acc: 0.8754\n",
      "Epoch 78/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0185 - acc: 0.9977\n",
      "Epoch 00078: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 372us/sample - loss: 0.0185 - acc: 0.9977 - val_loss: 0.3190 - val_acc: 0.9243\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0144 - acc: 0.9986\n",
      "Epoch 00079: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 373us/sample - loss: 0.0144 - acc: 0.9986 - val_loss: 0.3344 - val_acc: 0.9199\n",
      "Epoch 80/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0190 - acc: 0.9973\n",
      "Epoch 00080: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 374us/sample - loss: 0.0190 - acc: 0.9973 - val_loss: 0.3561 - val_acc: 0.9110\n",
      "Epoch 81/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0183 - acc: 0.9975\n",
      "Epoch 00081: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 371us/sample - loss: 0.0185 - acc: 0.9974 - val_loss: 0.6020 - val_acc: 0.8600\n",
      "Epoch 82/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0270 - acc: 0.9944\n",
      "Epoch 00082: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 372us/sample - loss: 0.0270 - acc: 0.9944 - val_loss: 0.4358 - val_acc: 0.8975\n",
      "Epoch 83/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0161 - acc: 0.9979\n",
      "Epoch 00083: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 371us/sample - loss: 0.0160 - acc: 0.9979 - val_loss: 0.3500 - val_acc: 0.9145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0115 - acc: 0.9990\n",
      "Epoch 00084: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 372us/sample - loss: 0.0116 - acc: 0.9990 - val_loss: 0.3881 - val_acc: 0.9047\n",
      "Epoch 85/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0272 - acc: 0.9938\n",
      "Epoch 00085: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 373us/sample - loss: 0.0271 - acc: 0.9938 - val_loss: 0.3182 - val_acc: 0.9201\n",
      "Epoch 86/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0110 - acc: 0.9992\n",
      "Epoch 00086: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 372us/sample - loss: 0.0110 - acc: 0.9992 - val_loss: 0.3221 - val_acc: 0.9196\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0168 - acc: 0.9973\n",
      "Epoch 00087: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 370us/sample - loss: 0.0168 - acc: 0.9973 - val_loss: 0.3507 - val_acc: 0.9166\n",
      "Epoch 88/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0162 - acc: 0.9977\n",
      "Epoch 00088: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 371us/sample - loss: 0.0162 - acc: 0.9977 - val_loss: 0.3556 - val_acc: 0.9171\n",
      "Epoch 89/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0180 - acc: 0.9972\n",
      "Epoch 00089: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 373us/sample - loss: 0.0180 - acc: 0.9972 - val_loss: 0.3564 - val_acc: 0.9122\n",
      "Epoch 90/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0168 - acc: 0.9977\n",
      "Epoch 00090: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 375us/sample - loss: 0.0171 - acc: 0.9977 - val_loss: 0.3529 - val_acc: 0.9157\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0236 - acc: 0.9951\n",
      "Epoch 00091: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 373us/sample - loss: 0.0236 - acc: 0.9951 - val_loss: 0.3661 - val_acc: 0.9110\n",
      "Epoch 92/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0094 - acc: 0.9994\n",
      "Epoch 00092: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 372us/sample - loss: 0.0094 - acc: 0.9994 - val_loss: 0.3301 - val_acc: 0.9215\n",
      "Epoch 93/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0137 - acc: 0.9982\n",
      "Epoch 00093: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 371us/sample - loss: 0.0137 - acc: 0.9982 - val_loss: 0.3893 - val_acc: 0.9064\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0119 - acc: 0.9987\n",
      "Epoch 00094: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 371us/sample - loss: 0.0120 - acc: 0.9986 - val_loss: 0.3894 - val_acc: 0.9085\n",
      "Epoch 95/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0285 - acc: 0.9941\n",
      "Epoch 00095: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 374us/sample - loss: 0.0285 - acc: 0.9942 - val_loss: 0.3142 - val_acc: 0.9227\n",
      "Epoch 96/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0072 - acc: 0.9997\n",
      "Epoch 00096: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 372us/sample - loss: 0.0072 - acc: 0.9997 - val_loss: 0.3269 - val_acc: 0.9241\n",
      "Epoch 97/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0108 - acc: 0.9986\n",
      "Epoch 00097: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 378us/sample - loss: 0.0109 - acc: 0.9986 - val_loss: 0.4857 - val_acc: 0.8928\n",
      "Epoch 98/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0210 - acc: 0.9958\n",
      "Epoch 00098: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 372us/sample - loss: 0.0209 - acc: 0.9958 - val_loss: 0.3435 - val_acc: 0.9217\n",
      "Epoch 99/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0096 - acc: 0.9989\n",
      "Epoch 00099: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 371us/sample - loss: 0.0096 - acc: 0.9989 - val_loss: 0.4267 - val_acc: 0.9061\n",
      "Epoch 100/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0248 - acc: 0.9945\n",
      "Epoch 00100: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 375us/sample - loss: 0.0248 - acc: 0.9945 - val_loss: 0.3477 - val_acc: 0.9168\n",
      "Epoch 101/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0085 - acc: 0.9992\n",
      "Epoch 00101: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 371us/sample - loss: 0.0087 - acc: 0.9992 - val_loss: 0.3418 - val_acc: 0.9178\n",
      "Epoch 102/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0197 - acc: 0.9962\n",
      "Epoch 00102: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 369us/sample - loss: 0.0198 - acc: 0.9961 - val_loss: 0.3486 - val_acc: 0.9210\n",
      "Epoch 103/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0150 - acc: 0.9972\n",
      "Epoch 00103: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 374us/sample - loss: 0.0151 - acc: 0.9972 - val_loss: 0.3531 - val_acc: 0.9201\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0222 - acc: 0.9956\n",
      "Epoch 00104: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 371us/sample - loss: 0.0227 - acc: 0.9955 - val_loss: 0.4104 - val_acc: 0.9040\n",
      "Epoch 105/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0200 - acc: 0.9960\n",
      "Epoch 00105: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 371us/sample - loss: 0.0200 - acc: 0.9960 - val_loss: 0.3381 - val_acc: 0.9189\n",
      "Epoch 106/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0067 - acc: 0.9996\n",
      "Epoch 00106: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 373us/sample - loss: 0.0067 - acc: 0.9996 - val_loss: 0.3216 - val_acc: 0.9229\n",
      "Epoch 107/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0059 - acc: 0.9996\n",
      "Epoch 00107: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 372us/sample - loss: 0.0059 - acc: 0.9996 - val_loss: 0.3302 - val_acc: 0.9220\n",
      "Epoch 108/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0129 - acc: 0.9973\n",
      "Epoch 00108: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 370us/sample - loss: 0.0129 - acc: 0.9973 - val_loss: 0.5717 - val_acc: 0.8763\n",
      "Epoch 109/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0136 - acc: 0.9977\n",
      "Epoch 00109: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 369us/sample - loss: 0.0137 - acc: 0.9976 - val_loss: 0.7455 - val_acc: 0.8484\n",
      "Epoch 110/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0103 - acc: 0.9985\n",
      "Epoch 00110: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 371us/sample - loss: 0.0103 - acc: 0.9985 - val_loss: 0.3444 - val_acc: 0.9224\n",
      "Epoch 111/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0124 - acc: 0.9979\n",
      "Epoch 00111: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 371us/sample - loss: 0.0124 - acc: 0.9979 - val_loss: 0.5252 - val_acc: 0.8866\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0165 - acc: 0.9971\n",
      "Epoch 00112: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 374us/sample - loss: 0.0165 - acc: 0.9971 - val_loss: 0.4405 - val_acc: 0.8956\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0087 - acc: 0.9986\n",
      "Epoch 00113: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 371us/sample - loss: 0.0087 - acc: 0.9986 - val_loss: 0.3690 - val_acc: 0.9173\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 114/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0064 - acc: 0.9992\n",
      "Epoch 00114: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 372us/sample - loss: 0.0064 - acc: 0.9992 - val_loss: 0.3512 - val_acc: 0.9243\n",
      "Epoch 115/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0053 - acc: 0.9996\n",
      "Epoch 00115: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 373us/sample - loss: 0.0053 - acc: 0.9996 - val_loss: 0.3536 - val_acc: 0.9229\n",
      "Epoch 116/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0171 - acc: 0.9961\n",
      "Epoch 00116: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 372us/sample - loss: 0.0171 - acc: 0.9961 - val_loss: 0.3295 - val_acc: 0.9269\n",
      "Epoch 117/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0070 - acc: 0.9991\n",
      "Epoch 00117: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 374us/sample - loss: 0.0070 - acc: 0.9991 - val_loss: 0.7607 - val_acc: 0.8435\n",
      "Epoch 118/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0150 - acc: 0.9969\n",
      "Epoch 00118: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 373us/sample - loss: 0.0150 - acc: 0.9969 - val_loss: 0.3301 - val_acc: 0.9206\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0067 - acc: 0.9993\n",
      "Epoch 00119: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 371us/sample - loss: 0.0067 - acc: 0.9993 - val_loss: 0.3458 - val_acc: 0.9231\n",
      "Epoch 120/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0052 - acc: 0.9996\n",
      "Epoch 00120: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 373us/sample - loss: 0.0052 - acc: 0.9996 - val_loss: 0.3438 - val_acc: 0.9238\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0165 - acc: 0.9964\n",
      "Epoch 00121: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 373us/sample - loss: 0.0165 - acc: 0.9964 - val_loss: 0.7777 - val_acc: 0.8553\n",
      "Epoch 122/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0110 - acc: 0.9980\n",
      "Epoch 00122: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 370us/sample - loss: 0.0110 - acc: 0.9980 - val_loss: 0.3668 - val_acc: 0.9145\n",
      "Epoch 123/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0134 - acc: 0.9975\n",
      "Epoch 00123: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 376us/sample - loss: 0.0134 - acc: 0.9974 - val_loss: 0.3998 - val_acc: 0.9159\n",
      "Epoch 124/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0126 - acc: 0.9976\n",
      "Epoch 00124: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 371us/sample - loss: 0.0126 - acc: 0.9976 - val_loss: 0.3414 - val_acc: 0.9220\n",
      "Epoch 125/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0105 - acc: 0.9984\n",
      "Epoch 00125: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 370us/sample - loss: 0.0105 - acc: 0.9985 - val_loss: 0.3431 - val_acc: 0.9234\n",
      "Epoch 126/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0049 - acc: 0.9996\n",
      "Epoch 00126: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 371us/sample - loss: 0.0049 - acc: 0.9996 - val_loss: 0.3614 - val_acc: 0.9234\n",
      "Epoch 127/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0135 - acc: 0.9973\n",
      "Epoch 00127: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 371us/sample - loss: 0.0135 - acc: 0.9973 - val_loss: 0.3454 - val_acc: 0.9238\n",
      "Epoch 128/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0043 - acc: 0.9997\n",
      "Epoch 00128: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 373us/sample - loss: 0.0043 - acc: 0.9997 - val_loss: 0.3375 - val_acc: 0.9257\n",
      "Epoch 129/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0080 - acc: 0.9988\n",
      "Epoch 00129: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 370us/sample - loss: 0.0080 - acc: 0.9988 - val_loss: 0.6902 - val_acc: 0.8614\n",
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0156 - acc: 0.9963\n",
      "Epoch 00130: val_loss did not improve from 0.30026\n",
      "36805/36805 [==============================] - 14s 370us/sample - loss: 0.0156 - acc: 0.9963 - val_loss: 0.3410 - val_acc: 0.9210\n",
      "\n",
      "1D_CNN_BN_4_only_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 1s 289us/sample - loss: 0.4007 - acc: 0.8804\n",
      "Loss: 0.4007288916583868 Accuracy: 0.88037384\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.6549 - acc: 0.4812\n",
      "Epoch 00001: val_loss improved from inf to 1.26308, saving model to model/checkpoint/1D_CNN_BN_5_only_conv_checkpoint/001-1.2631.hdf5\n",
      "36805/36805 [==============================] - 22s 587us/sample - loss: 1.6540 - acc: 0.4815 - val_loss: 1.2631 - val_acc: 0.5956\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9233 - acc: 0.7261\n",
      "Epoch 00002: val_loss improved from 1.26308 to 0.75987, saving model to model/checkpoint/1D_CNN_BN_5_only_conv_checkpoint/002-0.7599.hdf5\n",
      "36805/36805 [==============================] - 15s 407us/sample - loss: 0.9232 - acc: 0.7261 - val_loss: 0.7599 - val_acc: 0.7680\n",
      "Epoch 3/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6496 - acc: 0.8097\n",
      "Epoch 00003: val_loss improved from 0.75987 to 0.55025, saving model to model/checkpoint/1D_CNN_BN_5_only_conv_checkpoint/003-0.5502.hdf5\n",
      "36805/36805 [==============================] - 15s 405us/sample - loss: 0.6498 - acc: 0.8096 - val_loss: 0.5502 - val_acc: 0.8328\n",
      "Epoch 4/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5069 - acc: 0.8511\n",
      "Epoch 00004: val_loss improved from 0.55025 to 0.45722, saving model to model/checkpoint/1D_CNN_BN_5_only_conv_checkpoint/004-0.4572.hdf5\n",
      "36805/36805 [==============================] - 15s 405us/sample - loss: 0.5071 - acc: 0.8510 - val_loss: 0.4572 - val_acc: 0.8607\n",
      "Epoch 5/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.4154 - acc: 0.8797\n",
      "Epoch 00005: val_loss improved from 0.45722 to 0.40577, saving model to model/checkpoint/1D_CNN_BN_5_only_conv_checkpoint/005-0.4058.hdf5\n",
      "36805/36805 [==============================] - 15s 405us/sample - loss: 0.4153 - acc: 0.8798 - val_loss: 0.4058 - val_acc: 0.8749\n",
      "Epoch 6/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3524 - acc: 0.8980\n",
      "Epoch 00006: val_loss improved from 0.40577 to 0.39029, saving model to model/checkpoint/1D_CNN_BN_5_only_conv_checkpoint/006-0.3903.hdf5\n",
      "36805/36805 [==============================] - 15s 409us/sample - loss: 0.3524 - acc: 0.8979 - val_loss: 0.3903 - val_acc: 0.8765\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3088 - acc: 0.9099\n",
      "Epoch 00007: val_loss improved from 0.39029 to 0.33065, saving model to model/checkpoint/1D_CNN_BN_5_only_conv_checkpoint/007-0.3307.hdf5\n",
      "36805/36805 [==============================] - 15s 405us/sample - loss: 0.3090 - acc: 0.9099 - val_loss: 0.3307 - val_acc: 0.8963\n",
      "Epoch 8/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2687 - acc: 0.9222\n",
      "Epoch 00008: val_loss did not improve from 0.33065\n",
      "36805/36805 [==============================] - 15s 405us/sample - loss: 0.2685 - acc: 0.9223 - val_loss: 0.3310 - val_acc: 0.8980\n",
      "Epoch 9/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2459 - acc: 0.9290\n",
      "Epoch 00009: val_loss improved from 0.33065 to 0.27651, saving model to model/checkpoint/1D_CNN_BN_5_only_conv_checkpoint/009-0.2765.hdf5\n",
      "36805/36805 [==============================] - 15s 405us/sample - loss: 0.2460 - acc: 0.9289 - val_loss: 0.2765 - val_acc: 0.9168\n",
      "Epoch 10/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2220 - acc: 0.9374\n",
      "Epoch 00010: val_loss did not improve from 0.27651\n",
      "36805/36805 [==============================] - 15s 404us/sample - loss: 0.2219 - acc: 0.9374 - val_loss: 0.2823 - val_acc: 0.9094\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1977 - acc: 0.9440\n",
      "Epoch 00011: val_loss did not improve from 0.27651\n",
      "36805/36805 [==============================] - 15s 407us/sample - loss: 0.1977 - acc: 0.9440 - val_loss: 0.3614 - val_acc: 0.8891\n",
      "Epoch 12/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1817 - acc: 0.9480\n",
      "Epoch 00012: val_loss improved from 0.27651 to 0.24893, saving model to model/checkpoint/1D_CNN_BN_5_only_conv_checkpoint/012-0.2489.hdf5\n",
      "36805/36805 [==============================] - 15s 406us/sample - loss: 0.1818 - acc: 0.9480 - val_loss: 0.2489 - val_acc: 0.9180\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1698 - acc: 0.9524\n",
      "Epoch 00013: val_loss improved from 0.24893 to 0.24780, saving model to model/checkpoint/1D_CNN_BN_5_only_conv_checkpoint/013-0.2478.hdf5\n",
      "36805/36805 [==============================] - 15s 406us/sample - loss: 0.1699 - acc: 0.9523 - val_loss: 0.2478 - val_acc: 0.9217\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1583 - acc: 0.9551\n",
      "Epoch 00014: val_loss did not improve from 0.24780\n",
      "36805/36805 [==============================] - 15s 402us/sample - loss: 0.1583 - acc: 0.9551 - val_loss: 0.2487 - val_acc: 0.9238\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1387 - acc: 0.9617\n",
      "Epoch 00015: val_loss did not improve from 0.24780\n",
      "36805/36805 [==============================] - 15s 402us/sample - loss: 0.1387 - acc: 0.9617 - val_loss: 0.2548 - val_acc: 0.9231\n",
      "Epoch 16/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1246 - acc: 0.9670\n",
      "Epoch 00016: val_loss improved from 0.24780 to 0.23983, saving model to model/checkpoint/1D_CNN_BN_5_only_conv_checkpoint/016-0.2398.hdf5\n",
      "36805/36805 [==============================] - 15s 409us/sample - loss: 0.1246 - acc: 0.9670 - val_loss: 0.2398 - val_acc: 0.9271\n",
      "Epoch 17/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1180 - acc: 0.9682\n",
      "Epoch 00017: val_loss did not improve from 0.23983\n",
      "36805/36805 [==============================] - 15s 403us/sample - loss: 0.1181 - acc: 0.9681 - val_loss: 0.2862 - val_acc: 0.9113\n",
      "Epoch 18/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1114 - acc: 0.9701\n",
      "Epoch 00018: val_loss did not improve from 0.23983\n",
      "36805/36805 [==============================] - 15s 405us/sample - loss: 0.1117 - acc: 0.9700 - val_loss: 0.2666 - val_acc: 0.9199\n",
      "Epoch 19/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1079 - acc: 0.9712\n",
      "Epoch 00019: val_loss did not improve from 0.23983\n",
      "36805/36805 [==============================] - 15s 403us/sample - loss: 0.1081 - acc: 0.9712 - val_loss: 0.2541 - val_acc: 0.9248\n",
      "Epoch 20/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0923 - acc: 0.9765\n",
      "Epoch 00020: val_loss did not improve from 0.23983\n",
      "36805/36805 [==============================] - 15s 404us/sample - loss: 0.0924 - acc: 0.9764 - val_loss: 0.2418 - val_acc: 0.9290\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0932 - acc: 0.9758\n",
      "Epoch 00021: val_loss did not improve from 0.23983\n",
      "36805/36805 [==============================] - 15s 402us/sample - loss: 0.0932 - acc: 0.9758 - val_loss: 0.2402 - val_acc: 0.9280\n",
      "Epoch 22/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0764 - acc: 0.9816\n",
      "Epoch 00022: val_loss improved from 0.23983 to 0.23914, saving model to model/checkpoint/1D_CNN_BN_5_only_conv_checkpoint/022-0.2391.hdf5\n",
      "36805/36805 [==============================] - 15s 406us/sample - loss: 0.0765 - acc: 0.9816 - val_loss: 0.2391 - val_acc: 0.9250\n",
      "Epoch 23/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0724 - acc: 0.9824\n",
      "Epoch 00023: val_loss did not improve from 0.23914\n",
      "36805/36805 [==============================] - 15s 402us/sample - loss: 0.0724 - acc: 0.9824 - val_loss: 0.2468 - val_acc: 0.9250\n",
      "Epoch 24/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0673 - acc: 0.9842\n",
      "Epoch 00024: val_loss did not improve from 0.23914\n",
      "36805/36805 [==============================] - 15s 404us/sample - loss: 0.0673 - acc: 0.9842 - val_loss: 0.2698 - val_acc: 0.9206\n",
      "Epoch 25/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0639 - acc: 0.9849\n",
      "Epoch 00025: val_loss did not improve from 0.23914\n",
      "36805/36805 [==============================] - 15s 402us/sample - loss: 0.0639 - acc: 0.9849 - val_loss: 0.2704 - val_acc: 0.9238\n",
      "Epoch 26/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0595 - acc: 0.9862\n",
      "Epoch 00026: val_loss did not improve from 0.23914\n",
      "36805/36805 [==============================] - 15s 401us/sample - loss: 0.0596 - acc: 0.9861 - val_loss: 0.2822 - val_acc: 0.9189\n",
      "Epoch 27/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0584 - acc: 0.9864\n",
      "Epoch 00027: val_loss did not improve from 0.23914\n",
      "36805/36805 [==============================] - 15s 402us/sample - loss: 0.0583 - acc: 0.9864 - val_loss: 0.2520 - val_acc: 0.9266\n",
      "Epoch 28/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0482 - acc: 0.9900\n",
      "Epoch 00028: val_loss improved from 0.23914 to 0.23319, saving model to model/checkpoint/1D_CNN_BN_5_only_conv_checkpoint/028-0.2332.hdf5\n",
      "36805/36805 [==============================] - 15s 406us/sample - loss: 0.0481 - acc: 0.9901 - val_loss: 0.2332 - val_acc: 0.9334\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0423 - acc: 0.9918\n",
      "Epoch 00029: val_loss did not improve from 0.23319\n",
      "36805/36805 [==============================] - 15s 404us/sample - loss: 0.0423 - acc: 0.9917 - val_loss: 0.2842 - val_acc: 0.9173\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0504 - acc: 0.9886\n",
      "Epoch 00030: val_loss did not improve from 0.23319\n",
      "36805/36805 [==============================] - 15s 403us/sample - loss: 0.0504 - acc: 0.9886 - val_loss: 0.2481 - val_acc: 0.9269\n",
      "Epoch 31/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0386 - acc: 0.9927\n",
      "Epoch 00031: val_loss did not improve from 0.23319\n",
      "36805/36805 [==============================] - 15s 403us/sample - loss: 0.0386 - acc: 0.9927 - val_loss: 0.2831 - val_acc: 0.9180\n",
      "Epoch 32/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0445 - acc: 0.9906\n",
      "Epoch 00032: val_loss did not improve from 0.23319\n",
      "36805/36805 [==============================] - 15s 402us/sample - loss: 0.0445 - acc: 0.9906 - val_loss: 0.2365 - val_acc: 0.9331\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0336 - acc: 0.9941\n",
      "Epoch 00033: val_loss did not improve from 0.23319\n",
      "36805/36805 [==============================] - 15s 404us/sample - loss: 0.0337 - acc: 0.9940 - val_loss: 0.2600 - val_acc: 0.9271\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0407 - acc: 0.9916\n",
      "Epoch 00034: val_loss did not improve from 0.23319\n",
      "36805/36805 [==============================] - 15s 403us/sample - loss: 0.0408 - acc: 0.9916 - val_loss: 0.2989 - val_acc: 0.9231\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0563 - acc: 0.9865\n",
      "Epoch 00035: val_loss improved from 0.23319 to 0.22666, saving model to model/checkpoint/1D_CNN_BN_5_only_conv_checkpoint/035-0.2267.hdf5\n",
      "36805/36805 [==============================] - 15s 406us/sample - loss: 0.0563 - acc: 0.9865 - val_loss: 0.2267 - val_acc: 0.9369\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0276 - acc: 0.9957\n",
      "Epoch 00036: val_loss improved from 0.22666 to 0.21759, saving model to model/checkpoint/1D_CNN_BN_5_only_conv_checkpoint/036-0.2176.hdf5\n",
      "36805/36805 [==============================] - 15s 406us/sample - loss: 0.0280 - acc: 0.9956 - val_loss: 0.2176 - val_acc: 0.9390\n",
      "Epoch 37/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0405 - acc: 0.9916\n",
      "Epoch 00037: val_loss did not improve from 0.21759\n",
      "36805/36805 [==============================] - 15s 402us/sample - loss: 0.0405 - acc: 0.9916 - val_loss: 0.2308 - val_acc: 0.9359\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0231 - acc: 0.9966\n",
      "Epoch 00038: val_loss did not improve from 0.21759\n",
      "36805/36805 [==============================] - 15s 401us/sample - loss: 0.0232 - acc: 0.9965 - val_loss: 0.2276 - val_acc: 0.9352\n",
      "Epoch 39/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0341 - acc: 0.9929\n",
      "Epoch 00039: val_loss did not improve from 0.21759\n",
      "36805/36805 [==============================] - 15s 405us/sample - loss: 0.0342 - acc: 0.9929 - val_loss: 0.2268 - val_acc: 0.9343\n",
      "Epoch 40/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0229 - acc: 0.9965\n",
      "Epoch 00040: val_loss did not improve from 0.21759\n",
      "36805/36805 [==============================] - 15s 400us/sample - loss: 0.0229 - acc: 0.9965 - val_loss: 0.2540 - val_acc: 0.9287\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0212 - acc: 0.9967\n",
      "Epoch 00041: val_loss did not improve from 0.21759\n",
      "36805/36805 [==============================] - 15s 400us/sample - loss: 0.0212 - acc: 0.9967 - val_loss: 0.2726 - val_acc: 0.9255\n",
      "Epoch 42/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0286 - acc: 0.9938\n",
      "Epoch 00042: val_loss did not improve from 0.21759\n",
      "36805/36805 [==============================] - 15s 402us/sample - loss: 0.0286 - acc: 0.9938 - val_loss: 0.2773 - val_acc: 0.9194\n",
      "Epoch 43/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0206 - acc: 0.9964\n",
      "Epoch 00043: val_loss did not improve from 0.21759\n",
      "36805/36805 [==============================] - 15s 400us/sample - loss: 0.0206 - acc: 0.9964 - val_loss: 0.2493 - val_acc: 0.9357\n",
      "Epoch 44/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0237 - acc: 0.9952\n",
      "Epoch 00044: val_loss did not improve from 0.21759\n",
      "36805/36805 [==============================] - 15s 399us/sample - loss: 0.0238 - acc: 0.9952 - val_loss: 0.2810 - val_acc: 0.9285\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0373 - acc: 0.9909\n",
      "Epoch 00045: val_loss improved from 0.21759 to 0.21400, saving model to model/checkpoint/1D_CNN_BN_5_only_conv_checkpoint/045-0.2140.hdf5\n",
      "36805/36805 [==============================] - 15s 403us/sample - loss: 0.0373 - acc: 0.9909 - val_loss: 0.2140 - val_acc: 0.9390\n",
      "Epoch 46/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0166 - acc: 0.9975\n",
      "Epoch 00046: val_loss did not improve from 0.21400\n",
      "36805/36805 [==============================] - 15s 399us/sample - loss: 0.0166 - acc: 0.9975 - val_loss: 0.2470 - val_acc: 0.9364\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0143 - acc: 0.9982\n",
      "Epoch 00047: val_loss did not improve from 0.21400\n",
      "36805/36805 [==============================] - 15s 403us/sample - loss: 0.0143 - acc: 0.9982 - val_loss: 0.2803 - val_acc: 0.9283\n",
      "Epoch 48/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0167 - acc: 0.9972\n",
      "Epoch 00048: val_loss did not improve from 0.21400\n",
      "36805/36805 [==============================] - 15s 401us/sample - loss: 0.0167 - acc: 0.9972 - val_loss: 0.3721 - val_acc: 0.9073\n",
      "Epoch 49/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0255 - acc: 0.9938\n",
      "Epoch 00049: val_loss did not improve from 0.21400\n",
      "36805/36805 [==============================] - 15s 402us/sample - loss: 0.0255 - acc: 0.9938 - val_loss: 0.2573 - val_acc: 0.9331\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0129 - acc: 0.9984\n",
      "Epoch 00050: val_loss did not improve from 0.21400\n",
      "36805/36805 [==============================] - 15s 401us/sample - loss: 0.0130 - acc: 0.9984 - val_loss: 0.2452 - val_acc: 0.9329\n",
      "Epoch 51/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0275 - acc: 0.9939\n",
      "Epoch 00051: val_loss did not improve from 0.21400\n",
      "36805/36805 [==============================] - 15s 397us/sample - loss: 0.0274 - acc: 0.9939 - val_loss: 0.2256 - val_acc: 0.9385\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0105 - acc: 0.9988\n",
      "Epoch 00052: val_loss did not improve from 0.21400\n",
      "36805/36805 [==============================] - 15s 401us/sample - loss: 0.0105 - acc: 0.9988 - val_loss: 0.2269 - val_acc: 0.9411\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0177 - acc: 0.9969\n",
      "Epoch 00053: val_loss did not improve from 0.21400\n",
      "36805/36805 [==============================] - 15s 397us/sample - loss: 0.0177 - acc: 0.9969 - val_loss: 0.3754 - val_acc: 0.9103\n",
      "Epoch 54/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0170 - acc: 0.9970\n",
      "Epoch 00054: val_loss did not improve from 0.21400\n",
      "36805/36805 [==============================] - 15s 400us/sample - loss: 0.0171 - acc: 0.9970 - val_loss: 0.2628 - val_acc: 0.9341\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0135 - acc: 0.9979\n",
      "Epoch 00055: val_loss did not improve from 0.21400\n",
      "36805/36805 [==============================] - 15s 401us/sample - loss: 0.0135 - acc: 0.9979 - val_loss: 0.2284 - val_acc: 0.9406\n",
      "Epoch 56/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0214 - acc: 0.9954\n",
      "Epoch 00056: val_loss did not improve from 0.21400\n",
      "36805/36805 [==============================] - 15s 400us/sample - loss: 0.0214 - acc: 0.9954 - val_loss: 0.2570 - val_acc: 0.9324\n",
      "Epoch 57/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0121 - acc: 0.9979\n",
      "Epoch 00057: val_loss did not improve from 0.21400\n",
      "36805/36805 [==============================] - 15s 400us/sample - loss: 0.0122 - acc: 0.9979 - val_loss: 0.2877 - val_acc: 0.9304\n",
      "Epoch 58/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0297 - acc: 0.9928\n",
      "Epoch 00058: val_loss did not improve from 0.21400\n",
      "36805/36805 [==============================] - 15s 399us/sample - loss: 0.0298 - acc: 0.9928 - val_loss: 0.2360 - val_acc: 0.9378\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0133 - acc: 0.9979\n",
      "Epoch 00059: val_loss improved from 0.21400 to 0.21339, saving model to model/checkpoint/1D_CNN_BN_5_only_conv_checkpoint/059-0.2134.hdf5\n",
      "36805/36805 [==============================] - 15s 403us/sample - loss: 0.0133 - acc: 0.9979 - val_loss: 0.2134 - val_acc: 0.9434\n",
      "Epoch 60/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0074 - acc: 0.9993\n",
      "Epoch 00060: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 398us/sample - loss: 0.0074 - acc: 0.9993 - val_loss: 0.2337 - val_acc: 0.9441\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0173 - acc: 0.9964\n",
      "Epoch 00061: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 399us/sample - loss: 0.0173 - acc: 0.9964 - val_loss: 0.2237 - val_acc: 0.9411\n",
      "Epoch 62/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0088 - acc: 0.9987\n",
      "Epoch 00062: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 399us/sample - loss: 0.0088 - acc: 0.9987 - val_loss: 0.2897 - val_acc: 0.9285\n",
      "Epoch 63/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0170 - acc: 0.9963\n",
      "Epoch 00063: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 402us/sample - loss: 0.0170 - acc: 0.9962 - val_loss: 0.2867 - val_acc: 0.9290\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0119 - acc: 0.9980\n",
      "Epoch 00064: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 398us/sample - loss: 0.0119 - acc: 0.9980 - val_loss: 0.2679 - val_acc: 0.9373\n",
      "Epoch 65/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0076 - acc: 0.9989\n",
      "Epoch 00065: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 400us/sample - loss: 0.0075 - acc: 0.9989 - val_loss: 0.2206 - val_acc: 0.9425\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0129 - acc: 0.9974\n",
      "Epoch 00066: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 403us/sample - loss: 0.0129 - acc: 0.9974 - val_loss: 0.3316 - val_acc: 0.9252\n",
      "Epoch 67/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0096 - acc: 0.9984\n",
      "Epoch 00067: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 400us/sample - loss: 0.0096 - acc: 0.9985 - val_loss: 0.2341 - val_acc: 0.9418\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0178 - acc: 0.9954\n",
      "Epoch 00068: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 397us/sample - loss: 0.0178 - acc: 0.9954 - val_loss: 0.2860 - val_acc: 0.9299\n",
      "Epoch 69/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0105 - acc: 0.9984\n",
      "Epoch 00069: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 401us/sample - loss: 0.0105 - acc: 0.9985 - val_loss: 0.2418 - val_acc: 0.9385\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0074 - acc: 0.9990\n",
      "Epoch 00070: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 398us/sample - loss: 0.0074 - acc: 0.9990 - val_loss: 0.2695 - val_acc: 0.9343\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0145 - acc: 0.9970\n",
      "Epoch 00071: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 404us/sample - loss: 0.0145 - acc: 0.9970 - val_loss: 0.2384 - val_acc: 0.9413\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0052 - acc: 0.9995\n",
      "Epoch 00072: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 400us/sample - loss: 0.0052 - acc: 0.9995 - val_loss: 0.2289 - val_acc: 0.9434\n",
      "Epoch 73/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0113 - acc: 0.9977\n",
      "Epoch 00073: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 401us/sample - loss: 0.0113 - acc: 0.9977 - val_loss: 0.2802 - val_acc: 0.9317\n",
      "Epoch 74/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0199 - acc: 0.9953\n",
      "Epoch 00074: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 397us/sample - loss: 0.0199 - acc: 0.9953 - val_loss: 0.2526 - val_acc: 0.9359\n",
      "Epoch 75/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0067 - acc: 0.9990\n",
      "Epoch 00075: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 400us/sample - loss: 0.0067 - acc: 0.9990 - val_loss: 0.2255 - val_acc: 0.9455\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0067 - acc: 0.9990\n",
      "Epoch 00076: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 401us/sample - loss: 0.0067 - acc: 0.9990 - val_loss: 0.2883 - val_acc: 0.9357\n",
      "Epoch 77/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0103 - acc: 0.9979\n",
      "Epoch 00077: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 400us/sample - loss: 0.0103 - acc: 0.9979 - val_loss: 0.2572 - val_acc: 0.9327\n",
      "Epoch 78/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0048 - acc: 0.9995\n",
      "Epoch 00078: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 402us/sample - loss: 0.0048 - acc: 0.9995 - val_loss: 0.2526 - val_acc: 0.9397\n",
      "Epoch 79/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0139 - acc: 0.9969\n",
      "Epoch 00079: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 403us/sample - loss: 0.0139 - acc: 0.9969 - val_loss: 0.2298 - val_acc: 0.9404\n",
      "Epoch 80/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0077 - acc: 0.9987\n",
      "Epoch 00080: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 402us/sample - loss: 0.0078 - acc: 0.9987 - val_loss: 0.2332 - val_acc: 0.9387\n",
      "Epoch 81/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0190 - acc: 0.9957\n",
      "Epoch 00081: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 399us/sample - loss: 0.0190 - acc: 0.9958 - val_loss: 0.2457 - val_acc: 0.9380\n",
      "Epoch 82/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0039 - acc: 0.9997\n",
      "Epoch 00082: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 398us/sample - loss: 0.0039 - acc: 0.9997 - val_loss: 0.2309 - val_acc: 0.9443\n",
      "Epoch 83/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0059 - acc: 0.9989\n",
      "Epoch 00083: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 398us/sample - loss: 0.0059 - acc: 0.9989 - val_loss: 0.3538 - val_acc: 0.9143\n",
      "Epoch 84/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0045 - acc: 0.9995\n",
      "Epoch 00084: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 399us/sample - loss: 0.0045 - acc: 0.9995 - val_loss: 0.2478 - val_acc: 0.9413\n",
      "Epoch 85/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0169 - acc: 0.9958\n",
      "Epoch 00085: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 400us/sample - loss: 0.0169 - acc: 0.9958 - val_loss: 0.3002 - val_acc: 0.9313\n",
      "Epoch 86/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0055 - acc: 0.9992\n",
      "Epoch 00086: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 402us/sample - loss: 0.0055 - acc: 0.9992 - val_loss: 0.2411 - val_acc: 0.9415\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0144 - acc: 0.9968\n",
      "Epoch 00087: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 403us/sample - loss: 0.0144 - acc: 0.9968 - val_loss: 0.2285 - val_acc: 0.9418\n",
      "Epoch 88/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0069 - acc: 0.9988\n",
      "Epoch 00088: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 400us/sample - loss: 0.0068 - acc: 0.9988 - val_loss: 0.2391 - val_acc: 0.9427\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0044 - acc: 0.9993\n",
      "Epoch 00089: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 400us/sample - loss: 0.0044 - acc: 0.9993 - val_loss: 0.2624 - val_acc: 0.9380\n",
      "Epoch 90/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0038 - acc: 0.9996\n",
      "Epoch 00090: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 399us/sample - loss: 0.0038 - acc: 0.9996 - val_loss: 0.2600 - val_acc: 0.9411\n",
      "Epoch 91/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0045 - acc: 0.9994\n",
      "Epoch 00091: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 400us/sample - loss: 0.0048 - acc: 0.9993 - val_loss: 0.3319 - val_acc: 0.9245\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0237 - acc: 0.9936\n",
      "Epoch 00092: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 400us/sample - loss: 0.0237 - acc: 0.9936 - val_loss: 0.2347 - val_acc: 0.9425\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0058 - acc: 0.9991\n",
      "Epoch 00093: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 400us/sample - loss: 0.0058 - acc: 0.9991 - val_loss: 0.2427 - val_acc: 0.9432\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0039 - acc: 0.9996\n",
      "Epoch 00094: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 400us/sample - loss: 0.0039 - acc: 0.9996 - val_loss: 0.2428 - val_acc: 0.9425\n",
      "Epoch 95/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0027 - acc: 0.9997\n",
      "Epoch 00095: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 404us/sample - loss: 0.0027 - acc: 0.9997 - val_loss: 0.2691 - val_acc: 0.9376\n",
      "Epoch 96/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0098 - acc: 0.9981\n",
      "Epoch 00096: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 400us/sample - loss: 0.0098 - acc: 0.9981 - val_loss: 0.3094 - val_acc: 0.9306\n",
      "Epoch 97/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0080 - acc: 0.9985\n",
      "Epoch 00097: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 402us/sample - loss: 0.0080 - acc: 0.9985 - val_loss: 0.4262 - val_acc: 0.9150\n",
      "Epoch 98/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0063 - acc: 0.9989\n",
      "Epoch 00098: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 400us/sample - loss: 0.0063 - acc: 0.9989 - val_loss: 0.2960 - val_acc: 0.9320\n",
      "Epoch 99/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0073 - acc: 0.9982\n",
      "Epoch 00099: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 399us/sample - loss: 0.0073 - acc: 0.9982 - val_loss: 0.3279 - val_acc: 0.9322\n",
      "Epoch 100/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0080 - acc: 0.9985\n",
      "Epoch 00100: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 401us/sample - loss: 0.0080 - acc: 0.9985 - val_loss: 0.2944 - val_acc: 0.9378\n",
      "Epoch 101/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0080 - acc: 0.9984\n",
      "Epoch 00101: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 400us/sample - loss: 0.0080 - acc: 0.9984 - val_loss: 0.2528 - val_acc: 0.9422\n",
      "Epoch 102/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0111 - acc: 0.9977\n",
      "Epoch 00102: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 400us/sample - loss: 0.0113 - acc: 0.9977 - val_loss: 0.2578 - val_acc: 0.9380\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0191 - acc: 0.9949\n",
      "Epoch 00103: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 403us/sample - loss: 0.0191 - acc: 0.9949 - val_loss: 0.2496 - val_acc: 0.9432\n",
      "Epoch 104/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0105 - acc: 0.9979\n",
      "Epoch 00104: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 401us/sample - loss: 0.0105 - acc: 0.9979 - val_loss: 0.2481 - val_acc: 0.9390\n",
      "Epoch 105/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0047 - acc: 0.9992\n",
      "Epoch 00105: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 402us/sample - loss: 0.0048 - acc: 0.9992 - val_loss: 0.2348 - val_acc: 0.9446\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0105 - acc: 0.9975\n",
      "Epoch 00106: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 402us/sample - loss: 0.0105 - acc: 0.9975 - val_loss: 0.2422 - val_acc: 0.9474\n",
      "Epoch 107/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0034 - acc: 0.9996\n",
      "Epoch 00107: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 403us/sample - loss: 0.0034 - acc: 0.9996 - val_loss: 0.2526 - val_acc: 0.9453\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0026 - acc: 0.9997\n",
      "Epoch 00108: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 402us/sample - loss: 0.0027 - acc: 0.9997 - val_loss: 0.2413 - val_acc: 0.9446\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0132 - acc: 0.9966\n",
      "Epoch 00109: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 400us/sample - loss: 0.0132 - acc: 0.9966 - val_loss: 0.2364 - val_acc: 0.9436\n",
      "Epoch 110/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0040 - acc: 0.9994\n",
      "Epoch 00110: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 401us/sample - loss: 0.0044 - acc: 0.9993 - val_loss: 0.2573 - val_acc: 0.9406\n",
      "Epoch 111/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0128 - acc: 0.9972\n",
      "Epoch 00111: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 403us/sample - loss: 0.0128 - acc: 0.9972 - val_loss: 0.2379 - val_acc: 0.9448\n",
      "Epoch 112/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0024 - acc: 0.9998\n",
      "Epoch 00112: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 402us/sample - loss: 0.0024 - acc: 0.9998 - val_loss: 0.2271 - val_acc: 0.9476\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0029 - acc: 0.9996\n",
      "Epoch 00113: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 400us/sample - loss: 0.0029 - acc: 0.9996 - val_loss: 0.2798 - val_acc: 0.9387\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0075 - acc: 0.9982\n",
      "Epoch 00114: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 401us/sample - loss: 0.0075 - acc: 0.9982 - val_loss: 0.2333 - val_acc: 0.9429\n",
      "Epoch 115/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0039 - acc: 0.9993\n",
      "Epoch 00115: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 402us/sample - loss: 0.0040 - acc: 0.9993 - val_loss: 0.2275 - val_acc: 0.9441\n",
      "Epoch 116/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0055 - acc: 0.9989\n",
      "Epoch 00116: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 403us/sample - loss: 0.0055 - acc: 0.9989 - val_loss: 0.2910 - val_acc: 0.9322\n",
      "Epoch 117/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0138 - acc: 0.9966\n",
      "Epoch 00117: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 401us/sample - loss: 0.0138 - acc: 0.9966 - val_loss: 0.2563 - val_acc: 0.9411\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0026 - acc: 0.9997\n",
      "Epoch 00118: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 403us/sample - loss: 0.0027 - acc: 0.9996 - val_loss: 0.2868 - val_acc: 0.9399\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0153 - acc: 0.9959\n",
      "Epoch 00119: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 401us/sample - loss: 0.0154 - acc: 0.9958 - val_loss: 0.2286 - val_acc: 0.9446\n",
      "Epoch 120/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0128 - acc: 0.9968\n",
      "Epoch 00120: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 401us/sample - loss: 0.0128 - acc: 0.9968 - val_loss: 0.2403 - val_acc: 0.9436\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0029 - acc: 0.9996\n",
      "Epoch 00121: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 402us/sample - loss: 0.0029 - acc: 0.9996 - val_loss: 0.2618 - val_acc: 0.9436\n",
      "Epoch 122/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0082 - acc: 0.9984\n",
      "Epoch 00122: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 402us/sample - loss: 0.0083 - acc: 0.9983 - val_loss: 0.2474 - val_acc: 0.9448\n",
      "Epoch 123/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0113 - acc: 0.9972\n",
      "Epoch 00123: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 400us/sample - loss: 0.0113 - acc: 0.9972 - val_loss: 0.2412 - val_acc: 0.9478\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0031 - acc: 0.9996\n",
      "Epoch 00124: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 401us/sample - loss: 0.0031 - acc: 0.9996 - val_loss: 0.2283 - val_acc: 0.9490\n",
      "Epoch 125/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0019 - acc: 0.9998\n",
      "Epoch 00125: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 400us/sample - loss: 0.0019 - acc: 0.9998 - val_loss: 0.2509 - val_acc: 0.9422\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0065 - acc: 0.9986\n",
      "Epoch 00126: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 403us/sample - loss: 0.0065 - acc: 0.9986 - val_loss: 0.2427 - val_acc: 0.9434\n",
      "Epoch 127/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0028 - acc: 0.9997\n",
      "Epoch 00127: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 400us/sample - loss: 0.0028 - acc: 0.9997 - val_loss: 0.2435 - val_acc: 0.9464\n",
      "Epoch 128/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0046 - acc: 0.9989\n",
      "Epoch 00128: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 400us/sample - loss: 0.0047 - acc: 0.9989 - val_loss: 0.3639 - val_acc: 0.9164\n",
      "Epoch 129/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0103 - acc: 0.9971\n",
      "Epoch 00129: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 403us/sample - loss: 0.0103 - acc: 0.9971 - val_loss: 0.2438 - val_acc: 0.9457\n",
      "Epoch 130/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0062 - acc: 0.9988\n",
      "Epoch 00130: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 400us/sample - loss: 0.0062 - acc: 0.9988 - val_loss: 0.2382 - val_acc: 0.9499\n",
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0044 - acc: 0.9992\n",
      "Epoch 00131: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 399us/sample - loss: 0.0044 - acc: 0.9992 - val_loss: 0.2996 - val_acc: 0.9320\n",
      "Epoch 132/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0037 - acc: 0.9994\n",
      "Epoch 00132: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 402us/sample - loss: 0.0037 - acc: 0.9994 - val_loss: 0.2525 - val_acc: 0.9448\n",
      "Epoch 133/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0025 - acc: 0.9996\n",
      "Epoch 00133: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 401us/sample - loss: 0.0025 - acc: 0.9996 - val_loss: 0.2658 - val_acc: 0.9401\n",
      "Epoch 134/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0101 - acc: 0.9972\n",
      "Epoch 00134: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 404us/sample - loss: 0.0104 - acc: 0.9972 - val_loss: 0.3416 - val_acc: 0.9292\n",
      "Epoch 135/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0138 - acc: 0.9969\n",
      "Epoch 00135: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 401us/sample - loss: 0.0138 - acc: 0.9969 - val_loss: 0.2381 - val_acc: 0.9443\n",
      "Epoch 136/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0027 - acc: 0.9996\n",
      "Epoch 00136: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 400us/sample - loss: 0.0027 - acc: 0.9996 - val_loss: 0.2546 - val_acc: 0.9422\n",
      "Epoch 137/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0017 - acc: 0.9999\n",
      "Epoch 00137: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 402us/sample - loss: 0.0017 - acc: 0.9999 - val_loss: 0.2533 - val_acc: 0.9408\n",
      "Epoch 138/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0019 - acc: 0.9997\n",
      "Epoch 00138: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 400us/sample - loss: 0.0019 - acc: 0.9997 - val_loss: 0.2688 - val_acc: 0.9383\n",
      "Epoch 139/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9997\n",
      "Epoch 00139: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 400us/sample - loss: 0.0022 - acc: 0.9997 - val_loss: 0.2932 - val_acc: 0.9380\n",
      "Epoch 140/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0105 - acc: 0.9974\n",
      "Epoch 00140: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 400us/sample - loss: 0.0105 - acc: 0.9974 - val_loss: 0.2431 - val_acc: 0.9443\n",
      "Epoch 141/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0071 - acc: 0.9984\n",
      "Epoch 00141: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 403us/sample - loss: 0.0071 - acc: 0.9984 - val_loss: 0.2921 - val_acc: 0.9336\n",
      "Epoch 142/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0023 - acc: 0.9997\n",
      "Epoch 00142: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 402us/sample - loss: 0.0023 - acc: 0.9997 - val_loss: 0.2428 - val_acc: 0.9460\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 143/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0043 - acc: 0.9993\n",
      "Epoch 00143: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 399us/sample - loss: 0.0044 - acc: 0.9992 - val_loss: 0.2442 - val_acc: 0.9422\n",
      "Epoch 144/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0104 - acc: 0.9973\n",
      "Epoch 00144: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 400us/sample - loss: 0.0103 - acc: 0.9973 - val_loss: 0.2374 - val_acc: 0.9427\n",
      "Epoch 145/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0017 - acc: 0.9999\n",
      "Epoch 00145: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 400us/sample - loss: 0.0017 - acc: 0.9999 - val_loss: 0.2553 - val_acc: 0.9376\n",
      "Epoch 146/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0035 - acc: 0.9993\n",
      "Epoch 00146: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 401us/sample - loss: 0.0036 - acc: 0.9992 - val_loss: 0.2517 - val_acc: 0.9415\n",
      "Epoch 147/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0142 - acc: 0.9964\n",
      "Epoch 00147: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 403us/sample - loss: 0.0142 - acc: 0.9964 - val_loss: 0.2424 - val_acc: 0.9443\n",
      "Epoch 148/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0024 - acc: 0.9997\n",
      "Epoch 00148: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 401us/sample - loss: 0.0025 - acc: 0.9997 - val_loss: 0.2505 - val_acc: 0.9448\n",
      "Epoch 149/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0067 - acc: 0.9984\n",
      "Epoch 00149: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 400us/sample - loss: 0.0068 - acc: 0.9984 - val_loss: 0.2954 - val_acc: 0.9362\n",
      "Epoch 150/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0080 - acc: 0.9983\n",
      "Epoch 00150: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 403us/sample - loss: 0.0080 - acc: 0.9983 - val_loss: 0.2442 - val_acc: 0.9455\n",
      "Epoch 151/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0018 - acc: 0.9999\n",
      "Epoch 00151: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 401us/sample - loss: 0.0018 - acc: 0.9999 - val_loss: 0.2492 - val_acc: 0.9434\n",
      "Epoch 152/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0053 - acc: 0.9990\n",
      "Epoch 00152: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 403us/sample - loss: 0.0053 - acc: 0.9990 - val_loss: 0.2459 - val_acc: 0.9434\n",
      "Epoch 153/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0036 - acc: 0.9993\n",
      "Epoch 00153: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 400us/sample - loss: 0.0038 - acc: 0.9993 - val_loss: 0.2421 - val_acc: 0.9474\n",
      "Epoch 154/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0144 - acc: 0.9965\n",
      "Epoch 00154: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 399us/sample - loss: 0.0143 - acc: 0.9965 - val_loss: 0.2340 - val_acc: 0.9469\n",
      "Epoch 155/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0019 - acc: 0.9997\n",
      "Epoch 00155: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 400us/sample - loss: 0.0019 - acc: 0.9997 - val_loss: 0.2309 - val_acc: 0.9490\n",
      "Epoch 156/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0041 - acc: 0.9990\n",
      "Epoch 00156: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 399us/sample - loss: 0.0041 - acc: 0.9990 - val_loss: 0.2417 - val_acc: 0.9485\n",
      "Epoch 157/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0014 - acc: 0.9999\n",
      "Epoch 00157: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 399us/sample - loss: 0.0014 - acc: 0.9999 - val_loss: 0.2450 - val_acc: 0.9492\n",
      "Epoch 158/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0067 - acc: 0.9984\n",
      "Epoch 00158: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 403us/sample - loss: 0.0067 - acc: 0.9984 - val_loss: 0.2788 - val_acc: 0.9390\n",
      "Epoch 159/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0017 - acc: 0.9998\n",
      "Epoch 00159: val_loss did not improve from 0.21339\n",
      "36805/36805 [==============================] - 15s 401us/sample - loss: 0.0018 - acc: 0.9998 - val_loss: 0.2686 - val_acc: 0.9455\n",
      "\n",
      "1D_CNN_BN_5_only_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 1s 298us/sample - loss: 0.2892 - acc: 0.9288\n",
      "Loss: 0.2892399005359764 Accuracy: 0.9287643\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 10):\n",
    "    base = '1D_CNN_only_conv_conv_5_BN'\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    model = build_1d_cnn_only_conv_conv_5_BN(conv_num=i)\n",
    "#         model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4),\n",
    "          metrics=['accuracy'])\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model_filename = model_path+'{epoch:03d}-{val_loss:.4f}.hdf5'\n",
    "    checkpointer = ModelCheckpoint(filepath = model_filename, monitor = \"val_loss\", \n",
    "                                   verbose=1, save_best_only=True)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=100)\n",
    "    hist = model.fit(x_train_abs, y_train_onehot, batch_size=64, epochs=500, \n",
    "                     validation_data=[x_val_abs, y_val_onehot], shuffle=True, \n",
    "                     callbacks = [checkpointer, early_stopping])\n",
    "\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "    ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "    ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "    ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    png_path = 'visualization/learning_curve/'\n",
    "    filename = model_name+'.png'\n",
    "    os.makedirs(png_path, exist_ok=True)\n",
    "    fig.savefig(png_path+filename, transparent=True)\n",
    "\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "    model = load_model(model_filename)\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "    print()\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_BN_1_only_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_15 (Conv1D)           (None, 15976, 8)          208       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_15 (B (None, 15976, 8)          32        \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 15976, 8)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling (None, 3994, 8)           0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 31952)             0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 16)                511248    \n",
      "=================================================================\n",
      "Total params: 511,488\n",
      "Trainable params: 511,472\n",
      "Non-trainable params: 16\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 1s 215us/sample - loss: 1.8429 - acc: 0.4289\n",
      "Loss: 1.8428832955325751 Accuracy: 0.42886811\n",
      "\n",
      "1D_CNN_BN_2_only_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_16 (Conv1D)           (None, 15976, 8)          208       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_16 (B (None, 15976, 8)          32        \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 15976, 8)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling (None, 3994, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_17 (Conv1D)           (None, 3970, 16)          3216      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_17 (B (None, 3970, 16)          64        \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 3970, 16)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling (None, 993, 16)           0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 15888)             0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 16)                254224    \n",
      "=================================================================\n",
      "Total params: 257,744\n",
      "Trainable params: 257,696\n",
      "Non-trainable params: 48\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 1s 274us/sample - loss: 1.3207 - acc: 0.5836\n",
      "Loss: 1.3206864595289418 Accuracy: 0.58359295\n",
      "\n",
      "1D_CNN_BN_3_only_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_18 (Conv1D)           (None, 15976, 8)          208       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_18 (B (None, 15976, 8)          32        \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 15976, 8)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling (None, 3994, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_19 (Conv1D)           (None, 3970, 16)          3216      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_19 (B (None, 3970, 16)          64        \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 3970, 16)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling (None, 993, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_20 (Conv1D)           (None, 969, 32)           12832     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_20 (B (None, 969, 32)           128       \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 969, 32)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling (None, 243, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 7776)              0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 16)                124432    \n",
      "=================================================================\n",
      "Total params: 140,912\n",
      "Trainable params: 140,800\n",
      "Non-trainable params: 112\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 1s 266us/sample - loss: 0.7525 - acc: 0.7890\n",
      "Loss: 0.7524969367594734 Accuracy: 0.7889927\n",
      "\n",
      "1D_CNN_BN_4_only_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_21 (Conv1D)           (None, 15976, 8)          208       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_21 (B (None, 15976, 8)          32        \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 15976, 8)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling (None, 3994, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_22 (Conv1D)           (None, 3970, 16)          3216      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_22 (B (None, 3970, 16)          64        \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 3970, 16)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling (None, 993, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_23 (Conv1D)           (None, 969, 32)           12832     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_23 (B (None, 969, 32)           128       \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 969, 32)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling (None, 243, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_24 (Conv1D)           (None, 219, 64)           51264     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_24 (B (None, 219, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 219, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling (None, 55, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 3520)              0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 16)                56336     \n",
      "=================================================================\n",
      "Total params: 124,336\n",
      "Trainable params: 124,096\n",
      "Non-trainable params: 240\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 317us/sample - loss: 0.4007 - acc: 0.8804\n",
      "Loss: 0.4007288916583868 Accuracy: 0.88037384\n",
      "\n",
      "1D_CNN_BN_5_only_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_25 (Conv1D)           (None, 15976, 8)          208       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_25 (B (None, 15976, 8)          32        \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 15976, 8)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling (None, 3994, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_26 (Conv1D)           (None, 3970, 16)          3216      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_26 (B (None, 3970, 16)          64        \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 3970, 16)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_26 (MaxPooling (None, 993, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_27 (Conv1D)           (None, 969, 32)           12832     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_27 (B (None, 969, 32)           128       \n",
      "_________________________________________________________________\n",
      "activation_27 (Activation)   (None, 969, 32)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_27 (MaxPooling (None, 243, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_28 (Conv1D)           (None, 219, 64)           51264     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_28 (B (None, 219, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_28 (Activation)   (None, 219, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_28 (MaxPooling (None, 55, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_29 (Conv1D)           (None, 31, 128)           204928    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_29 (B (None, 31, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_29 (Activation)   (None, 31, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_29 (MaxPooling (None, 8, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 16)                16400     \n",
      "=================================================================\n",
      "Total params: 289,840\n",
      "Trainable params: 289,344\n",
      "Non-trainable params: 496\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 2s 328us/sample - loss: 0.2892 - acc: 0.9288\n",
      "Loss: 0.2892399005359764 Accuracy: 0.9287643\n"
     ]
    }
   ],
   "source": [
    "log_dir = 'log'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "base = '1D_CNN_only_conv_conv_5_BN'\n",
    "\n",
    "with open(path.join(log_dir, base)) as log_file:\n",
    "    for i in range(1, 10):\n",
    "        model_name = base+'_{}_conv'.format(i)\n",
    "        print()\n",
    "        print(model_name, 'Model')\n",
    "        model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "        model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "\n",
    "        model = load_model(model_filename)\n",
    "        model.summary()\n",
    "\n",
    "        [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "        print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "        del(model)\n",
    "\n",
    "        log_file.write('\\t'.join([model_name, accuracy, loss])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
