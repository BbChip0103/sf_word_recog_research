{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path as path\n",
    "import itertools\n",
    "from sklearn.preprocessing import maxabs_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Input,InputLayer, Dense, Activation, BatchNormalization, Flatten, Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D, Dropout\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler, \\\n",
    "                                        EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = path.join('..', 'data')\n",
    "data_dir = path.join(base_dir, 'data_speech_commands_v0.02')\n",
    " \n",
    "train_txt = path.join(data_dir, 'wav_train_16words.txt')\n",
    "val_txt = path.join(data_dir, 'wav_validation_16words.txt')\n",
    "test_txt = path.join(data_dir, 'wav_test_16words.txt')\n",
    "\n",
    "train_data = np.load(path.join(data_dir, 'wav_train_data.npz'))\n",
    "val_data = np.load(path.join(data_dir, 'wav_validation_data.npz'))\n",
    "test_data = np.load(path.join(data_dir, 'wav_test_data.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36805, 16000, 1),\n",
       " (36805,),\n",
       " (4293, 16000, 1),\n",
       " (4293,),\n",
       " (4815, 16000, 1),\n",
       " (4815,),\n",
       " (16, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = train_data['x_train']\n",
    "y_train = train_data['y_train']\n",
    "x_val = val_data['x_val']\n",
    "y_val = val_data['y_val']\n",
    "x_test = test_data['x_test']\n",
    "y_test = test_data['y_test']\n",
    "y_table = test_data['table']\n",
    "\n",
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape, y_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_test[0].shape\n",
    "output_size = y_table.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_abs = np.asarray([maxabs_scale(wav) for wav in x_train])\n",
    "y_train_onehot = np.asarray([to_categorical(label, output_size) for label in y_train])\n",
    "del x_train, y_train\n",
    "\n",
    "x_val_abs = np.asarray([maxabs_scale(wav) for wav in x_val])\n",
    "y_val_onehot = np.asarray([to_categorical(label, output_size) for label in y_val])\n",
    "del x_val, y_val\n",
    "\n",
    "x_test_abs = np.asarray([maxabs_scale(wav) for wav in x_test])\n",
    "y_test_onehot = np.asarray([to_categorical(label, output_size) for label in y_test])\n",
    "del x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_1d_cnn_only_conv_conv_5_BN(conv_num=1):\n",
    "    model=Sequential()\n",
    "    model.add(Conv1D (kernel_size=5, filters=8, strides=1, padding='valid', input_shape=input_shape)) \n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling1D(pool_size=3, strides=3, padding='same'))\n",
    "    \n",
    "    for i in range(conv_num-1):\n",
    "        model.add(Conv1D (kernel_size=5, filters=8*(2**(i+1)), strides=1, padding='valid'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(MaxPooling1D(pool_size=3, strides=3, padding='same'))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(BatchNormalization())\n",
    "#     model.add(Dropout(0.5))\n",
    "    model.add(Dense(output_size, activation='softmax' ))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 15996, 8)          48        \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1 (Batc (None, 15996, 8)          32        \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 15996, 8)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 5332, 8)           0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 42656)             0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_1 (Ba (None, 42656)             170624    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                682512    \n",
      "=================================================================\n",
      "Total params: 853,216\n",
      "Trainable params: 767,888\n",
      "Non-trainable params: 85,328\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 15996, 8)          48        \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_2 (Ba (None, 15996, 8)          32        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 15996, 8)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 5332, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 5328, 16)          656       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_3 (Ba (None, 5328, 16)          64        \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 5328, 16)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 1776, 16)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 28416)             0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_4 (Ba (None, 28416)             113664    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                454672    \n",
      "=================================================================\n",
      "Total params: 569,136\n",
      "Trainable params: 512,256\n",
      "Non-trainable params: 56,880\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_3 (Conv1D)            (None, 15996, 8)          48        \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_5 (Ba (None, 15996, 8)          32        \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 15996, 8)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 5332, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 5328, 16)          656       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_6 (Ba (None, 5328, 16)          64        \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 5328, 16)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 1776, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 1772, 32)          2592      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_7 (Ba (None, 1772, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 1772, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 591, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 18912)             0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_8 (Ba (None, 18912)             75648     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                302608    \n",
      "=================================================================\n",
      "Total params: 381,776\n",
      "Trainable params: 343,840\n",
      "Non-trainable params: 37,936\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_6 (Conv1D)            (None, 15996, 8)          48        \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_9 (Ba (None, 15996, 8)          32        \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 15996, 8)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 5332, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 5328, 16)          656       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_10 (B (None, 5328, 16)          64        \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 5328, 16)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 1776, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 1772, 32)          2592      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_11 (B (None, 1772, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 1772, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 591, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 587, 64)           10304     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_12 (B (None, 587, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 587, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1 (None, 196, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 12544)             0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_13 (B (None, 12544)             50176     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                200720    \n",
      "=================================================================\n",
      "Total params: 264,976\n",
      "Trainable params: 239,648\n",
      "Non-trainable params: 25,328\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_10 (Conv1D)           (None, 15996, 8)          48        \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_14 (B (None, 15996, 8)          32        \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 15996, 8)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling (None, 5332, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 5328, 16)          656       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_15 (B (None, 5328, 16)          64        \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 5328, 16)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling (None, 1776, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_12 (Conv1D)           (None, 1772, 32)          2592      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_16 (B (None, 1772, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 1772, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling (None, 591, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 587, 64)           10304     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_17 (B (None, 587, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 587, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling (None, 196, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 192, 128)          41088     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_18 (B (None, 192, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 192, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling (None, 64, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_19 (B (None, 8192)              32768     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                131088    \n",
      "=================================================================\n",
      "Total params: 219,536\n",
      "Trainable params: 202,656\n",
      "Non-trainable params: 16,880\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_15 (Conv1D)           (None, 15996, 8)          48        \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_20 (B (None, 15996, 8)          32        \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 15996, 8)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling (None, 5332, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_16 (Conv1D)           (None, 5328, 16)          656       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_21 (B (None, 5328, 16)          64        \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 5328, 16)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling (None, 1776, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_17 (Conv1D)           (None, 1772, 32)          2592      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_22 (B (None, 1772, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 1772, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling (None, 591, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_18 (Conv1D)           (None, 587, 64)           10304     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_23 (B (None, 587, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 587, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling (None, 196, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_19 (Conv1D)           (None, 192, 128)          41088     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_24 (B (None, 192, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 192, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling (None, 64, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_20 (Conv1D)           (None, 60, 256)           164096    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_25 (B (None, 60, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 60, 256)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling (None, 20, 256)           0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 5120)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_26 (B (None, 5120)              20480     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 16)                81936     \n",
      "=================================================================\n",
      "Total params: 323,216\n",
      "Trainable params: 311,968\n",
      "Non-trainable params: 11,248\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_21 (Conv1D)           (None, 15996, 8)          48        \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_27 (B (None, 15996, 8)          32        \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 15996, 8)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling (None, 5332, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_22 (Conv1D)           (None, 5328, 16)          656       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_28 (B (None, 5328, 16)          64        \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 5328, 16)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling (None, 1776, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_23 (Conv1D)           (None, 1772, 32)          2592      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_29 (B (None, 1772, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 1772, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling (None, 591, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_24 (Conv1D)           (None, 587, 64)           10304     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_30 (B (None, 587, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 587, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling (None, 196, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_25 (Conv1D)           (None, 192, 128)          41088     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_31 (B (None, 192, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 192, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling (None, 64, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_26 (Conv1D)           (None, 60, 256)           164096    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_32 (B (None, 60, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 60, 256)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_26 (MaxPooling (None, 20, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_27 (Conv1D)           (None, 16, 512)           655872    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_33 (B (None, 16, 512)           2048      \n",
      "_________________________________________________________________\n",
      "activation_27 (Activation)   (None, 16, 512)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_27 (MaxPooling (None, 6, 512)            0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 3072)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_34 (B (None, 3072)              12288     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 16)                49168     \n",
      "=================================================================\n",
      "Total params: 940,176\n",
      "Trainable params: 932,000\n",
      "Non-trainable params: 8,176\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_28 (Conv1D)           (None, 15996, 8)          48        \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_35 (B (None, 15996, 8)          32        \n",
      "_________________________________________________________________\n",
      "activation_28 (Activation)   (None, 15996, 8)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_28 (MaxPooling (None, 5332, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_29 (Conv1D)           (None, 5328, 16)          656       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_36 (B (None, 5328, 16)          64        \n",
      "_________________________________________________________________\n",
      "activation_29 (Activation)   (None, 5328, 16)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_29 (MaxPooling (None, 1776, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_30 (Conv1D)           (None, 1772, 32)          2592      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_37 (B (None, 1772, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_30 (Activation)   (None, 1772, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_30 (MaxPooling (None, 591, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_31 (Conv1D)           (None, 587, 64)           10304     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_38 (B (None, 587, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_31 (Activation)   (None, 587, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_31 (MaxPooling (None, 196, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_32 (Conv1D)           (None, 192, 128)          41088     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_39 (B (None, 192, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_32 (Activation)   (None, 192, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_32 (MaxPooling (None, 64, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_33 (Conv1D)           (None, 60, 256)           164096    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_40 (B (None, 60, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_33 (Activation)   (None, 60, 256)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling (None, 20, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_34 (Conv1D)           (None, 16, 512)           655872    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_41 (B (None, 16, 512)           2048      \n",
      "_________________________________________________________________\n",
      "activation_34 (Activation)   (None, 16, 512)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling (None, 6, 512)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_35 (Conv1D)           (None, 2, 1024)           2622464   \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_42 (B (None, 2, 1024)           4096      \n",
      "_________________________________________________________________\n",
      "activation_35 (Activation)   (None, 2, 1024)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling (None, 1, 1024)           0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_43 (B (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 16)                16400     \n",
      "=================================================================\n",
      "Total params: 3,525,776\n",
      "Trainable params: 3,519,648\n",
      "Non-trainable params: 6,128\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 9):\n",
    "    model = build_1d_cnn_only_conv_conv_5_BN(conv_num=i)\n",
    "    model.summary()\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36805 samples, validate on 4293 samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 2.7839 - acc: 0.2244\n",
      "Epoch 00001: val_loss improved from inf to 2.51169, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_BN_1_conv_checkpoint/001-2.5117.hdf5\n",
      "36805/36805 [==============================] - 11s 290us/sample - loss: 2.7835 - acc: 0.2245 - val_loss: 2.5117 - val_acc: 0.2339\n",
      "Epoch 2/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 2.1164 - acc: 0.3710\n",
      "Epoch 00002: val_loss did not improve from 2.51169\n",
      "36805/36805 [==============================] - 9s 235us/sample - loss: 2.1168 - acc: 0.3710 - val_loss: 2.6079 - val_acc: 0.2648\n",
      "Epoch 3/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.7943 - acc: 0.4532\n",
      "Epoch 00003: val_loss did not improve from 2.51169\n",
      "36805/36805 [==============================] - 8s 229us/sample - loss: 1.7953 - acc: 0.4530 - val_loss: 2.5810 - val_acc: 0.3038\n",
      "Epoch 4/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.5896 - acc: 0.5120\n",
      "Epoch 00004: val_loss did not improve from 2.51169\n",
      "36805/36805 [==============================] - 9s 233us/sample - loss: 1.5888 - acc: 0.5120 - val_loss: 2.6342 - val_acc: 0.2989\n",
      "Epoch 5/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 1.4007 - acc: 0.5682\n",
      "Epoch 00005: val_loss did not improve from 2.51169\n",
      "36805/36805 [==============================] - 8s 229us/sample - loss: 1.4025 - acc: 0.5677 - val_loss: 2.5886 - val_acc: 0.3044\n",
      "Epoch 6/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.2751 - acc: 0.6069\n",
      "Epoch 00006: val_loss did not improve from 2.51169\n",
      "36805/36805 [==============================] - 8s 228us/sample - loss: 1.2748 - acc: 0.6071 - val_loss: 2.7057 - val_acc: 0.2977\n",
      "Epoch 7/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.1651 - acc: 0.6398\n",
      "Epoch 00007: val_loss did not improve from 2.51169\n",
      "36805/36805 [==============================] - 9s 231us/sample - loss: 1.1658 - acc: 0.6394 - val_loss: 2.7665 - val_acc: 0.3049\n",
      "Epoch 8/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.0630 - acc: 0.6719\n",
      "Epoch 00008: val_loss did not improve from 2.51169\n",
      "36805/36805 [==============================] - 8s 230us/sample - loss: 1.0638 - acc: 0.6716 - val_loss: 2.8550 - val_acc: 0.3000\n",
      "Epoch 9/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.9817 - acc: 0.6967\n",
      "Epoch 00009: val_loss did not improve from 2.51169\n",
      "36805/36805 [==============================] - 8s 226us/sample - loss: 0.9818 - acc: 0.6967 - val_loss: 2.9166 - val_acc: 0.2879\n",
      "Epoch 10/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.9118 - acc: 0.7200\n",
      "Epoch 00010: val_loss did not improve from 2.51169\n",
      "36805/36805 [==============================] - 8s 228us/sample - loss: 0.9125 - acc: 0.7198 - val_loss: 2.9506 - val_acc: 0.3082\n",
      "Epoch 11/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.8451 - acc: 0.7424\n",
      "Epoch 00011: val_loss did not improve from 2.51169\n",
      "36805/36805 [==============================] - 9s 234us/sample - loss: 0.8458 - acc: 0.7422 - val_loss: 3.1271 - val_acc: 0.2809\n",
      "Epoch 12/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.7916 - acc: 0.7613\n",
      "Epoch 00012: val_loss did not improve from 2.51169\n",
      "36805/36805 [==============================] - 8s 226us/sample - loss: 0.7923 - acc: 0.7611 - val_loss: 3.0566 - val_acc: 0.2835\n",
      "Epoch 13/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.7378 - acc: 0.7784\n",
      "Epoch 00013: val_loss did not improve from 2.51169\n",
      "36805/36805 [==============================] - 8s 231us/sample - loss: 0.7383 - acc: 0.7783 - val_loss: 3.1168 - val_acc: 0.2928\n",
      "Epoch 14/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7068 - acc: 0.7863\n",
      "Epoch 00014: val_loss did not improve from 2.51169\n",
      "36805/36805 [==============================] - 8s 226us/sample - loss: 0.7068 - acc: 0.7862 - val_loss: 3.1444 - val_acc: 0.2933\n",
      "Epoch 15/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.6635 - acc: 0.8004\n",
      "Epoch 00015: val_loss did not improve from 2.51169\n",
      "36805/36805 [==============================] - 8s 228us/sample - loss: 0.6638 - acc: 0.8002 - val_loss: 3.3304 - val_acc: 0.2800\n",
      "Epoch 16/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.6220 - acc: 0.8153\n",
      "Epoch 00016: val_loss did not improve from 2.51169\n",
      "36805/36805 [==============================] - 9s 233us/sample - loss: 0.6233 - acc: 0.8149 - val_loss: 3.2405 - val_acc: 0.2844\n",
      "Epoch 17/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5899 - acc: 0.8254\n",
      "Epoch 00017: val_loss did not improve from 2.51169\n",
      "36805/36805 [==============================] - 8s 226us/sample - loss: 0.5901 - acc: 0.8252 - val_loss: 3.4284 - val_acc: 0.2921\n",
      "Epoch 18/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5595 - acc: 0.8340\n",
      "Epoch 00018: val_loss did not improve from 2.51169\n",
      "36805/36805 [==============================] - 8s 226us/sample - loss: 0.5595 - acc: 0.8339 - val_loss: 3.3735 - val_acc: 0.2777\n",
      "Epoch 19/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.5304 - acc: 0.8454\n",
      "Epoch 00019: val_loss did not improve from 2.51169\n",
      "36805/36805 [==============================] - 8s 227us/sample - loss: 0.5313 - acc: 0.8451 - val_loss: 3.7949 - val_acc: 0.2676\n",
      "Epoch 20/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5130 - acc: 0.8509\n",
      "Epoch 00020: val_loss did not improve from 2.51169\n",
      "36805/36805 [==============================] - 9s 232us/sample - loss: 0.5134 - acc: 0.8508 - val_loss: 3.5221 - val_acc: 0.2837\n",
      "Epoch 21/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.4748 - acc: 0.8626\n",
      "Epoch 00021: val_loss did not improve from 2.51169\n",
      "36805/36805 [==============================] - 8s 229us/sample - loss: 0.4752 - acc: 0.8624 - val_loss: 3.6414 - val_acc: 0.2774\n",
      "Epoch 22/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.4560 - acc: 0.8706\n",
      "Epoch 00022: val_loss did not improve from 2.51169\n",
      "36805/36805 [==============================] - 8s 229us/sample - loss: 0.4565 - acc: 0.8705 - val_loss: 3.6644 - val_acc: 0.2926\n",
      "Epoch 23/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.4331 - acc: 0.8791\n",
      "Epoch 00023: val_loss did not improve from 2.51169\n",
      "36805/36805 [==============================] - 9s 231us/sample - loss: 0.4338 - acc: 0.8789 - val_loss: 3.7016 - val_acc: 0.2833\n",
      "Epoch 24/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.4203 - acc: 0.8821\n",
      "Epoch 00024: val_loss did not improve from 2.51169\n",
      "36805/36805 [==============================] - 8s 229us/sample - loss: 0.4214 - acc: 0.8817 - val_loss: 3.6626 - val_acc: 0.2886\n",
      "Epoch 25/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3944 - acc: 0.8888\n",
      "Epoch 00025: val_loss did not improve from 2.51169\n",
      "36805/36805 [==============================] - 8s 227us/sample - loss: 0.3952 - acc: 0.8887 - val_loss: 3.7458 - val_acc: 0.2900\n",
      "Epoch 26/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3856 - acc: 0.8920\n",
      "Epoch 00026: val_loss did not improve from 2.51169\n",
      "36805/36805 [==============================] - 9s 231us/sample - loss: 0.3858 - acc: 0.8918 - val_loss: 3.8104 - val_acc: 0.2702\n",
      "Epoch 27/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3771 - acc: 0.8945\n",
      "Epoch 00027: val_loss did not improve from 2.51169\n",
      "36805/36805 [==============================] - 8s 229us/sample - loss: 0.3777 - acc: 0.8942 - val_loss: 3.8406 - val_acc: 0.2833\n",
      "Epoch 28/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3534 - acc: 0.9022\n",
      "Epoch 00028: val_loss did not improve from 2.51169\n",
      "36805/36805 [==============================] - 8s 230us/sample - loss: 0.3538 - acc: 0.9021 - val_loss: 3.8482 - val_acc: 0.2900\n",
      "Epoch 29/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3466 - acc: 0.9075\n",
      "Epoch 00029: val_loss did not improve from 2.51169\n",
      "36805/36805 [==============================] - 8s 228us/sample - loss: 0.3470 - acc: 0.9073 - val_loss: 4.2474 - val_acc: 0.2541\n",
      "Epoch 30/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3405 - acc: 0.9054\n",
      "Epoch 00030: val_loss did not improve from 2.51169\n",
      "36805/36805 [==============================] - 8s 229us/sample - loss: 0.3409 - acc: 0.9054 - val_loss: 3.9107 - val_acc: 0.2765\n",
      "Epoch 31/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3249 - acc: 0.9122\n",
      "Epoch 00031: val_loss did not improve from 2.51169\n",
      "36805/36805 [==============================] - 8s 228us/sample - loss: 0.3254 - acc: 0.9120 - val_loss: 3.9511 - val_acc: 0.2756\n",
      "Epoch 32/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3144 - acc: 0.9153\n",
      "Epoch 00032: val_loss did not improve from 2.51169\n",
      "36805/36805 [==============================] - 8s 230us/sample - loss: 0.3148 - acc: 0.9151 - val_loss: 4.2113 - val_acc: 0.2567\n",
      "Epoch 33/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.3072 - acc: 0.9175\n",
      "Epoch 00033: val_loss did not improve from 2.51169\n",
      "36805/36805 [==============================] - 8s 227us/sample - loss: 0.3076 - acc: 0.9173 - val_loss: 4.1728 - val_acc: 0.2751\n",
      "Epoch 34/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2918 - acc: 0.9224\n",
      "Epoch 00034: val_loss did not improve from 2.51169\n",
      "36805/36805 [==============================] - 8s 226us/sample - loss: 0.2922 - acc: 0.9223 - val_loss: 4.2906 - val_acc: 0.2690\n",
      "Epoch 35/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2873 - acc: 0.9242\n",
      "Epoch 00035: val_loss did not improve from 2.51169\n",
      "36805/36805 [==============================] - 8s 228us/sample - loss: 0.2877 - acc: 0.9242 - val_loss: 4.3013 - val_acc: 0.2646\n",
      "Epoch 36/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2825 - acc: 0.9260\n",
      "Epoch 00036: val_loss did not improve from 2.51169\n",
      "36805/36805 [==============================] - 9s 231us/sample - loss: 0.2825 - acc: 0.9259 - val_loss: 4.2061 - val_acc: 0.2742\n",
      "Epoch 37/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2594 - acc: 0.9330\n",
      "Epoch 00037: val_loss did not improve from 2.51169\n",
      "36805/36805 [==============================] - 8s 226us/sample - loss: 0.2597 - acc: 0.9329 - val_loss: 4.2117 - val_acc: 0.2798\n",
      "Epoch 38/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2583 - acc: 0.9323\n",
      "Epoch 00038: val_loss did not improve from 2.51169\n",
      "36805/36805 [==============================] - 8s 226us/sample - loss: 0.2589 - acc: 0.9320 - val_loss: 4.2364 - val_acc: 0.2721\n",
      "Epoch 39/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2570 - acc: 0.9337\n",
      "Epoch 00039: val_loss did not improve from 2.51169\n",
      "36805/36805 [==============================] - 8s 226us/sample - loss: 0.2571 - acc: 0.9337 - val_loss: 4.2751 - val_acc: 0.2784\n",
      "Epoch 40/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2501 - acc: 0.9334\n",
      "Epoch 00040: val_loss did not improve from 2.51169\n",
      "36805/36805 [==============================] - 8s 227us/sample - loss: 0.2503 - acc: 0.9334 - val_loss: 4.5246 - val_acc: 0.2569\n",
      "Epoch 41/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2390 - acc: 0.9368\n",
      "Epoch 00041: val_loss did not improve from 2.51169\n",
      "36805/36805 [==============================] - 8s 226us/sample - loss: 0.2396 - acc: 0.9367 - val_loss: 4.3660 - val_acc: 0.2725\n",
      "Epoch 42/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2369 - acc: 0.9382\n",
      "Epoch 00042: val_loss did not improve from 2.51169\n",
      "36805/36805 [==============================] - 8s 229us/sample - loss: 0.2378 - acc: 0.9380 - val_loss: 4.4254 - val_acc: 0.2744\n",
      "Epoch 43/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2407 - acc: 0.9388\n",
      "Epoch 00043: val_loss did not improve from 2.51169\n",
      "36805/36805 [==============================] - 8s 228us/sample - loss: 0.2408 - acc: 0.9387 - val_loss: 4.4046 - val_acc: 0.2765\n",
      "Epoch 44/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.2242 - acc: 0.9418\n",
      "Epoch 00044: val_loss did not improve from 2.51169\n",
      "36805/36805 [==============================] - 8s 219us/sample - loss: 0.2248 - acc: 0.9417 - val_loss: 4.4513 - val_acc: 0.2711\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2260 - acc: 0.9411\n",
      "Epoch 00045: val_loss did not improve from 2.51169\n",
      "36805/36805 [==============================] - 8s 220us/sample - loss: 0.2261 - acc: 0.9410 - val_loss: 4.4847 - val_acc: 0.2739\n",
      "Epoch 46/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.2154 - acc: 0.9448\n",
      "Epoch 00046: val_loss did not improve from 2.51169\n",
      "36805/36805 [==============================] - 8s 224us/sample - loss: 0.2159 - acc: 0.9448 - val_loss: 4.8159 - val_acc: 0.2721\n",
      "Epoch 47/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.2072 - acc: 0.9449\n",
      "Epoch 00047: val_loss did not improve from 2.51169\n",
      "36805/36805 [==============================] - 8s 223us/sample - loss: 0.2074 - acc: 0.9447 - val_loss: 4.5096 - val_acc: 0.2730\n",
      "Epoch 48/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2003 - acc: 0.9494\n",
      "Epoch 00048: val_loss did not improve from 2.51169\n",
      "36805/36805 [==============================] - 8s 220us/sample - loss: 0.2006 - acc: 0.9494 - val_loss: 4.5391 - val_acc: 0.2819\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2002 - acc: 0.9491\n",
      "Epoch 00049: val_loss did not improve from 2.51169\n",
      "36805/36805 [==============================] - 8s 221us/sample - loss: 0.2005 - acc: 0.9490 - val_loss: 4.6657 - val_acc: 0.2579\n",
      "Epoch 50/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.1971 - acc: 0.9497\n",
      "Epoch 00050: val_loss did not improve from 2.51169\n",
      "36805/36805 [==============================] - 8s 223us/sample - loss: 0.1971 - acc: 0.9497 - val_loss: 4.5808 - val_acc: 0.2805\n",
      "Epoch 51/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1887 - acc: 0.9529\n",
      "Epoch 00051: val_loss did not improve from 2.51169\n",
      "36805/36805 [==============================] - 8s 219us/sample - loss: 0.1890 - acc: 0.9528 - val_loss: 4.6624 - val_acc: 0.2590\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1922 - acc: 0.9520\n",
      "Epoch 00052: val_loss did not improve from 2.51169\n",
      "36805/36805 [==============================] - 8s 219us/sample - loss: 0.1926 - acc: 0.9520 - val_loss: 4.7355 - val_acc: 0.2756\n",
      "Epoch 53/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1710 - acc: 0.9596\n",
      "Epoch 00053: val_loss did not improve from 2.51169\n",
      "36805/36805 [==============================] - 8s 226us/sample - loss: 0.1712 - acc: 0.9596 - val_loss: 4.7491 - val_acc: 0.2707\n",
      "Epoch 54/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.1849 - acc: 0.9526\n",
      "Epoch 00054: val_loss did not improve from 2.51169\n",
      "36805/36805 [==============================] - 8s 216us/sample - loss: 0.1852 - acc: 0.9526 - val_loss: 4.7625 - val_acc: 0.2807\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1858 - acc: 0.9539\n",
      "Epoch 00055: val_loss did not improve from 2.51169\n",
      "36805/36805 [==============================] - 8s 220us/sample - loss: 0.1862 - acc: 0.9538 - val_loss: 4.7689 - val_acc: 0.2669\n",
      "Epoch 56/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1771 - acc: 0.9559\n",
      "Epoch 00056: val_loss did not improve from 2.51169\n",
      "36805/36805 [==============================] - 8s 219us/sample - loss: 0.1770 - acc: 0.9559 - val_loss: 4.7368 - val_acc: 0.2819\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1626 - acc: 0.9600\n",
      "Epoch 00057: val_loss did not improve from 2.51169\n",
      "36805/36805 [==============================] - 8s 221us/sample - loss: 0.1628 - acc: 0.9600 - val_loss: 4.8107 - val_acc: 0.2798\n",
      "Epoch 58/500\n",
      "36608/36805 [============================>.] - ETA: 0s - loss: 0.1747 - acc: 0.9553\n",
      "Epoch 00058: val_loss did not improve from 2.51169\n",
      "36805/36805 [==============================] - 8s 228us/sample - loss: 0.1747 - acc: 0.9553 - val_loss: 4.8039 - val_acc: 0.2732\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1674 - acc: 0.9580\n",
      "Epoch 00059: val_loss did not improve from 2.51169\n",
      "36805/36805 [==============================] - 8s 229us/sample - loss: 0.1676 - acc: 0.9579 - val_loss: 5.1339 - val_acc: 0.2567\n",
      "Epoch 60/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1720 - acc: 0.9566\n",
      "Epoch 00060: val_loss did not improve from 2.51169\n",
      "36805/36805 [==============================] - 9s 232us/sample - loss: 0.1722 - acc: 0.9564 - val_loss: 4.8375 - val_acc: 0.2802\n",
      "Epoch 61/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1630 - acc: 0.9599\n",
      "Epoch 00061: val_loss did not improve from 2.51169\n",
      "36805/36805 [==============================] - 8s 227us/sample - loss: 0.1633 - acc: 0.9598 - val_loss: 4.8350 - val_acc: 0.2763\n",
      "Epoch 62/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1578 - acc: 0.9599\n",
      "Epoch 00062: val_loss did not improve from 2.51169\n",
      "36805/36805 [==============================] - 8s 227us/sample - loss: 0.1579 - acc: 0.9599 - val_loss: 4.9265 - val_acc: 0.2809\n",
      "Epoch 63/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1603 - acc: 0.9588\n",
      "Epoch 00063: val_loss did not improve from 2.51169\n",
      "36805/36805 [==============================] - 8s 229us/sample - loss: 0.1608 - acc: 0.9587 - val_loss: 4.8458 - val_acc: 0.2746\n",
      "Epoch 64/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1553 - acc: 0.9592\n",
      "Epoch 00064: val_loss did not improve from 2.51169\n",
      "36805/36805 [==============================] - 8s 229us/sample - loss: 0.1557 - acc: 0.9591 - val_loss: 4.9416 - val_acc: 0.2676\n",
      "Epoch 65/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1577 - acc: 0.9598\n",
      "Epoch 00065: val_loss did not improve from 2.51169\n",
      "36805/36805 [==============================] - 8s 229us/sample - loss: 0.1576 - acc: 0.9599 - val_loss: 5.0605 - val_acc: 0.2774\n",
      "Epoch 66/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1447 - acc: 0.9631\n",
      "Epoch 00066: val_loss did not improve from 2.51169\n",
      "36805/36805 [==============================] - 8s 229us/sample - loss: 0.1453 - acc: 0.9630 - val_loss: 4.9377 - val_acc: 0.2730\n",
      "Epoch 67/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1462 - acc: 0.9626\n",
      "Epoch 00067: val_loss did not improve from 2.51169\n",
      "36805/36805 [==============================] - 8s 230us/sample - loss: 0.1465 - acc: 0.9625 - val_loss: 4.9560 - val_acc: 0.2760\n",
      "Epoch 68/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1421 - acc: 0.9650\n",
      "Epoch 00068: val_loss did not improve from 2.51169\n",
      "36805/36805 [==============================] - 8s 229us/sample - loss: 0.1425 - acc: 0.9649 - val_loss: 5.0183 - val_acc: 0.2688\n",
      "Epoch 69/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1429 - acc: 0.9652\n",
      "Epoch 00069: val_loss did not improve from 2.51169\n",
      "36805/36805 [==============================] - 8s 227us/sample - loss: 0.1433 - acc: 0.9651 - val_loss: 5.0167 - val_acc: 0.2814\n",
      "Epoch 70/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1464 - acc: 0.9609\n",
      "Epoch 00070: val_loss did not improve from 2.51169\n",
      "36805/36805 [==============================] - 8s 227us/sample - loss: 0.1464 - acc: 0.9608 - val_loss: 5.0901 - val_acc: 0.2667\n",
      "Epoch 71/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1433 - acc: 0.9649\n",
      "Epoch 00071: val_loss did not improve from 2.51169\n",
      "36805/36805 [==============================] - 8s 227us/sample - loss: 0.1436 - acc: 0.9648 - val_loss: 5.0386 - val_acc: 0.2730\n",
      "Epoch 72/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1342 - acc: 0.9680\n",
      "Epoch 00072: val_loss did not improve from 2.51169\n",
      "36805/36805 [==============================] - 8s 226us/sample - loss: 0.1343 - acc: 0.9680 - val_loss: 5.0682 - val_acc: 0.2751\n",
      "Epoch 73/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1347 - acc: 0.9664\n",
      "Epoch 00073: val_loss did not improve from 2.51169\n",
      "36805/36805 [==============================] - 8s 227us/sample - loss: 0.1355 - acc: 0.9663 - val_loss: 5.2212 - val_acc: 0.2546\n",
      "Epoch 74/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1356 - acc: 0.9650\n",
      "Epoch 00074: val_loss did not improve from 2.51169\n",
      "36805/36805 [==============================] - 9s 234us/sample - loss: 0.1356 - acc: 0.9650 - val_loss: 5.2015 - val_acc: 0.2616\n",
      "Epoch 75/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1320 - acc: 0.9669\n",
      "Epoch 00075: val_loss did not improve from 2.51169\n",
      "36805/36805 [==============================] - 8s 229us/sample - loss: 0.1322 - acc: 0.9669 - val_loss: 5.1448 - val_acc: 0.2700\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1337 - acc: 0.9661\n",
      "Epoch 00076: val_loss did not improve from 2.51169\n",
      "36805/36805 [==============================] - 8s 220us/sample - loss: 0.1339 - acc: 0.9660 - val_loss: 5.1829 - val_acc: 0.2756\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1316 - acc: 0.9677\n",
      "Epoch 00077: val_loss did not improve from 2.51169\n",
      "36805/36805 [==============================] - 8s 221us/sample - loss: 0.1320 - acc: 0.9676 - val_loss: 5.1496 - val_acc: 0.2693\n",
      "Epoch 78/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1341 - acc: 0.9666\n",
      "Epoch 00078: val_loss did not improve from 2.51169\n",
      "36805/36805 [==============================] - 8s 222us/sample - loss: 0.1344 - acc: 0.9666 - val_loss: 5.1544 - val_acc: 0.2704\n",
      "Epoch 79/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1210 - acc: 0.9698\n",
      "Epoch 00079: val_loss did not improve from 2.51169\n",
      "36805/36805 [==============================] - 8s 225us/sample - loss: 0.1212 - acc: 0.9697 - val_loss: 5.2973 - val_acc: 0.2618\n",
      "Epoch 80/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1196 - acc: 0.9697\n",
      "Epoch 00080: val_loss did not improve from 2.51169\n",
      "36805/36805 [==============================] - 8s 230us/sample - loss: 0.1201 - acc: 0.9696 - val_loss: 5.2075 - val_acc: 0.2676\n",
      "Epoch 81/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1263 - acc: 0.9689\n",
      "Epoch 00081: val_loss did not improve from 2.51169\n",
      "36805/36805 [==============================] - 8s 229us/sample - loss: 0.1266 - acc: 0.9688 - val_loss: 5.1655 - val_acc: 0.2742\n",
      "Epoch 82/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1246 - acc: 0.9676\n",
      "Epoch 00082: val_loss did not improve from 2.51169\n",
      "36805/36805 [==============================] - 8s 230us/sample - loss: 0.1259 - acc: 0.9674 - val_loss: 5.3275 - val_acc: 0.2672\n",
      "Epoch 83/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1299 - acc: 0.9674\n",
      "Epoch 00083: val_loss did not improve from 2.51169\n",
      "36805/36805 [==============================] - 8s 228us/sample - loss: 0.1298 - acc: 0.9673 - val_loss: 5.2769 - val_acc: 0.2644\n",
      "Epoch 84/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1211 - acc: 0.9705\n",
      "Epoch 00084: val_loss did not improve from 2.51169\n",
      "36805/36805 [==============================] - 9s 233us/sample - loss: 0.1219 - acc: 0.9705 - val_loss: 5.2680 - val_acc: 0.2767\n",
      "Epoch 85/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1237 - acc: 0.9699\n",
      "Epoch 00085: val_loss did not improve from 2.51169\n",
      "36805/36805 [==============================] - 8s 227us/sample - loss: 0.1242 - acc: 0.9697 - val_loss: 5.2779 - val_acc: 0.2660\n",
      "Epoch 86/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1117 - acc: 0.9726\n",
      "Epoch 00086: val_loss did not improve from 2.51169\n",
      "36805/36805 [==============================] - 9s 233us/sample - loss: 0.1119 - acc: 0.9726 - val_loss: 5.3577 - val_acc: 0.2714\n",
      "Epoch 87/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1142 - acc: 0.9709\n",
      "Epoch 00087: val_loss did not improve from 2.51169\n",
      "36805/36805 [==============================] - 9s 234us/sample - loss: 0.1143 - acc: 0.9708 - val_loss: 5.6239 - val_acc: 0.2621\n",
      "Epoch 88/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1140 - acc: 0.9720\n",
      "Epoch 00088: val_loss did not improve from 2.51169\n",
      "36805/36805 [==============================] - 8s 225us/sample - loss: 0.1147 - acc: 0.9718 - val_loss: 5.3695 - val_acc: 0.2702\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 89/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1124 - acc: 0.9711\n",
      "Epoch 00089: val_loss did not improve from 2.51169\n",
      "36805/36805 [==============================] - 8s 230us/sample - loss: 0.1124 - acc: 0.9712 - val_loss: 5.4321 - val_acc: 0.2746\n",
      "Epoch 90/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1148 - acc: 0.9707\n",
      "Epoch 00090: val_loss did not improve from 2.51169\n",
      "36805/36805 [==============================] - 8s 229us/sample - loss: 0.1148 - acc: 0.9707 - val_loss: 5.4045 - val_acc: 0.2590\n",
      "Epoch 91/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1112 - acc: 0.9725\n",
      "Epoch 00091: val_loss did not improve from 2.51169\n",
      "36805/36805 [==============================] - 8s 227us/sample - loss: 0.1115 - acc: 0.9725 - val_loss: 5.4382 - val_acc: 0.2621\n",
      "Epoch 92/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1035 - acc: 0.9747\n",
      "Epoch 00092: val_loss did not improve from 2.51169\n",
      "36805/36805 [==============================] - 8s 229us/sample - loss: 0.1039 - acc: 0.9746 - val_loss: 5.4169 - val_acc: 0.2669\n",
      "Epoch 93/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1078 - acc: 0.9734\n",
      "Epoch 00093: val_loss did not improve from 2.51169\n",
      "36805/36805 [==============================] - 9s 232us/sample - loss: 0.1089 - acc: 0.9733 - val_loss: 5.5745 - val_acc: 0.2525\n",
      "Epoch 94/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1193 - acc: 0.9704\n",
      "Epoch 00094: val_loss did not improve from 2.51169\n",
      "36805/36805 [==============================] - 8s 230us/sample - loss: 0.1194 - acc: 0.9702 - val_loss: 5.5724 - val_acc: 0.2590\n",
      "Epoch 95/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1081 - acc: 0.9728\n",
      "Epoch 00095: val_loss did not improve from 2.51169\n",
      "36805/36805 [==============================] - 9s 234us/sample - loss: 0.1084 - acc: 0.9727 - val_loss: 5.3836 - val_acc: 0.2716\n",
      "Epoch 96/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1063 - acc: 0.9745\n",
      "Epoch 00096: val_loss did not improve from 2.51169\n",
      "36805/36805 [==============================] - 8s 230us/sample - loss: 0.1063 - acc: 0.9745 - val_loss: 5.5632 - val_acc: 0.2681\n",
      "Epoch 97/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1053 - acc: 0.9752\n",
      "Epoch 00097: val_loss did not improve from 2.51169\n",
      "36805/36805 [==============================] - 9s 233us/sample - loss: 0.1058 - acc: 0.9751 - val_loss: 5.6078 - val_acc: 0.2630\n",
      "Epoch 98/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0966 - acc: 0.9773\n",
      "Epoch 00098: val_loss did not improve from 2.51169\n",
      "36805/36805 [==============================] - 8s 230us/sample - loss: 0.0967 - acc: 0.9773 - val_loss: 5.5132 - val_acc: 0.2632\n",
      "Epoch 99/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0993 - acc: 0.9749\n",
      "Epoch 00099: val_loss did not improve from 2.51169\n",
      "36805/36805 [==============================] - 8s 228us/sample - loss: 0.0995 - acc: 0.9749 - val_loss: 5.5077 - val_acc: 0.2602\n",
      "Epoch 100/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0986 - acc: 0.9752\n",
      "Epoch 00100: val_loss did not improve from 2.51169\n",
      "36805/36805 [==============================] - 8s 230us/sample - loss: 0.0994 - acc: 0.9751 - val_loss: 5.5400 - val_acc: 0.2658\n",
      "Epoch 101/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.1052 - acc: 0.9734\n",
      "Epoch 00101: val_loss did not improve from 2.51169\n",
      "36805/36805 [==============================] - 8s 228us/sample - loss: 0.1050 - acc: 0.9734 - val_loss: 5.4471 - val_acc: 0.2665\n",
      "\n",
      "1D_CNN_only_conv_conv_5_BN_1_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 1s 194us/sample - loss: 2.5298 - acc: 0.2301\n",
      "Loss: 2.5297898799088143 Accuracy: 0.23011422\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 2.3514 - acc: 0.3178\n",
      "Epoch 00001: val_loss improved from inf to 2.09724, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_BN_2_conv_checkpoint/001-2.0972.hdf5\n",
      "36805/36805 [==============================] - 14s 376us/sample - loss: 2.3512 - acc: 0.3178 - val_loss: 2.0972 - val_acc: 0.3645\n",
      "Epoch 2/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.6364 - acc: 0.5045\n",
      "Epoch 00002: val_loss improved from 2.09724 to 2.02854, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_BN_2_conv_checkpoint/002-2.0285.hdf5\n",
      "36805/36805 [==============================] - 12s 313us/sample - loss: 1.6369 - acc: 0.5044 - val_loss: 2.0285 - val_acc: 0.4000\n",
      "Epoch 3/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3144 - acc: 0.5948\n",
      "Epoch 00003: val_loss improved from 2.02854 to 2.01233, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_BN_2_conv_checkpoint/003-2.0123.hdf5\n",
      "36805/36805 [==============================] - 12s 315us/sample - loss: 1.3146 - acc: 0.5948 - val_loss: 2.0123 - val_acc: 0.4260\n",
      "Epoch 4/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0799 - acc: 0.6672\n",
      "Epoch 00004: val_loss improved from 2.01233 to 1.91282, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_BN_2_conv_checkpoint/004-1.9128.hdf5\n",
      "36805/36805 [==============================] - 12s 313us/sample - loss: 1.0806 - acc: 0.6669 - val_loss: 1.9128 - val_acc: 0.4589\n",
      "Epoch 5/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9089 - acc: 0.7229\n",
      "Epoch 00005: val_loss did not improve from 1.91282\n",
      "36805/36805 [==============================] - 12s 313us/sample - loss: 0.9091 - acc: 0.7229 - val_loss: 1.9650 - val_acc: 0.4421\n",
      "Epoch 6/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7648 - acc: 0.7704\n",
      "Epoch 00006: val_loss did not improve from 1.91282\n",
      "36805/36805 [==============================] - 11s 312us/sample - loss: 0.7648 - acc: 0.7703 - val_loss: 2.0250 - val_acc: 0.4463\n",
      "Epoch 7/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6536 - acc: 0.8092\n",
      "Epoch 00007: val_loss did not improve from 1.91282\n",
      "36805/36805 [==============================] - 11s 310us/sample - loss: 0.6534 - acc: 0.8093 - val_loss: 2.0009 - val_acc: 0.4563\n",
      "Epoch 8/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5576 - acc: 0.8417\n",
      "Epoch 00008: val_loss did not improve from 1.91282\n",
      "36805/36805 [==============================] - 12s 313us/sample - loss: 0.5583 - acc: 0.8416 - val_loss: 2.4585 - val_acc: 0.3983\n",
      "Epoch 9/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4906 - acc: 0.8658\n",
      "Epoch 00009: val_loss did not improve from 1.91282\n",
      "36805/36805 [==============================] - 12s 314us/sample - loss: 0.4908 - acc: 0.8657 - val_loss: 2.0734 - val_acc: 0.4649\n",
      "Epoch 10/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4275 - acc: 0.8875\n",
      "Epoch 00010: val_loss did not improve from 1.91282\n",
      "36805/36805 [==============================] - 12s 312us/sample - loss: 0.4276 - acc: 0.8875 - val_loss: 2.1867 - val_acc: 0.4414\n",
      "Epoch 11/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3672 - acc: 0.9093\n",
      "Epoch 00011: val_loss did not improve from 1.91282\n",
      "36805/36805 [==============================] - 11s 312us/sample - loss: 0.3675 - acc: 0.9092 - val_loss: 2.2436 - val_acc: 0.4426\n",
      "Epoch 12/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3289 - acc: 0.9201\n",
      "Epoch 00012: val_loss did not improve from 1.91282\n",
      "36805/36805 [==============================] - 11s 311us/sample - loss: 0.3291 - acc: 0.9201 - val_loss: 2.5003 - val_acc: 0.4191\n",
      "Epoch 13/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2956 - acc: 0.9308\n",
      "Epoch 00013: val_loss did not improve from 1.91282\n",
      "36805/36805 [==============================] - 11s 312us/sample - loss: 0.2959 - acc: 0.9308 - val_loss: 2.2157 - val_acc: 0.4610\n",
      "Epoch 14/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2614 - acc: 0.9418\n",
      "Epoch 00014: val_loss did not improve from 1.91282\n",
      "36805/36805 [==============================] - 11s 312us/sample - loss: 0.2617 - acc: 0.9417 - val_loss: 2.3761 - val_acc: 0.4468\n",
      "Epoch 15/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2393 - acc: 0.9512\n",
      "Epoch 00015: val_loss did not improve from 1.91282\n",
      "36805/36805 [==============================] - 11s 303us/sample - loss: 0.2395 - acc: 0.9511 - val_loss: 2.3549 - val_acc: 0.4710\n",
      "Epoch 16/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2134 - acc: 0.9557\n",
      "Epoch 00016: val_loss did not improve from 1.91282\n",
      "36805/36805 [==============================] - 11s 310us/sample - loss: 0.2134 - acc: 0.9557 - val_loss: 2.4508 - val_acc: 0.4545\n",
      "Epoch 17/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1945 - acc: 0.9607\n",
      "Epoch 00017: val_loss did not improve from 1.91282\n",
      "36805/36805 [==============================] - 11s 310us/sample - loss: 0.1946 - acc: 0.9607 - val_loss: 2.3873 - val_acc: 0.4612\n",
      "Epoch 18/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1753 - acc: 0.9669\n",
      "Epoch 00018: val_loss did not improve from 1.91282\n",
      "36805/36805 [==============================] - 11s 312us/sample - loss: 0.1756 - acc: 0.9669 - val_loss: 2.4440 - val_acc: 0.4638\n",
      "Epoch 19/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1620 - acc: 0.9694\n",
      "Epoch 00019: val_loss did not improve from 1.91282\n",
      "36805/36805 [==============================] - 11s 307us/sample - loss: 0.1623 - acc: 0.9693 - val_loss: 2.8239 - val_acc: 0.4195\n",
      "Epoch 20/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1462 - acc: 0.9729\n",
      "Epoch 00020: val_loss did not improve from 1.91282\n",
      "36805/36805 [==============================] - 11s 310us/sample - loss: 0.1465 - acc: 0.9728 - val_loss: 2.7168 - val_acc: 0.4423\n",
      "Epoch 21/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1362 - acc: 0.9754\n",
      "Epoch 00021: val_loss did not improve from 1.91282\n",
      "36805/36805 [==============================] - 11s 309us/sample - loss: 0.1363 - acc: 0.9754 - val_loss: 2.8000 - val_acc: 0.4391\n",
      "Epoch 22/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1267 - acc: 0.9780\n",
      "Epoch 00022: val_loss did not improve from 1.91282\n",
      "36805/36805 [==============================] - 11s 310us/sample - loss: 0.1268 - acc: 0.9779 - val_loss: 2.9832 - val_acc: 0.4032\n",
      "Epoch 23/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1206 - acc: 0.9791\n",
      "Epoch 00023: val_loss did not improve from 1.91282\n",
      "36805/36805 [==============================] - 11s 308us/sample - loss: 0.1212 - acc: 0.9789 - val_loss: 2.6484 - val_acc: 0.4568\n",
      "Epoch 24/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1090 - acc: 0.9814\n",
      "Epoch 00024: val_loss did not improve from 1.91282\n",
      "36805/36805 [==============================] - 11s 309us/sample - loss: 0.1091 - acc: 0.9814 - val_loss: 2.6860 - val_acc: 0.4642\n",
      "Epoch 25/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1071 - acc: 0.9818\n",
      "Epoch 00025: val_loss did not improve from 1.91282\n",
      "36805/36805 [==============================] - 11s 311us/sample - loss: 0.1072 - acc: 0.9819 - val_loss: 3.5582 - val_acc: 0.3760\n",
      "Epoch 26/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0990 - acc: 0.9845\n",
      "Epoch 00026: val_loss did not improve from 1.91282\n",
      "36805/36805 [==============================] - 11s 309us/sample - loss: 0.0991 - acc: 0.9844 - val_loss: 2.8834 - val_acc: 0.4340\n",
      "Epoch 27/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0889 - acc: 0.9858\n",
      "Epoch 00027: val_loss did not improve from 1.91282\n",
      "36805/36805 [==============================] - 12s 314us/sample - loss: 0.0895 - acc: 0.9857 - val_loss: 3.1963 - val_acc: 0.4095\n",
      "Epoch 28/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0934 - acc: 0.9845\n",
      "Epoch 00028: val_loss did not improve from 1.91282\n",
      "36805/36805 [==============================] - 11s 310us/sample - loss: 0.0938 - acc: 0.9844 - val_loss: 3.1148 - val_acc: 0.4258\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0832 - acc: 0.9860\n",
      "Epoch 00029: val_loss did not improve from 1.91282\n",
      "36805/36805 [==============================] - 11s 310us/sample - loss: 0.0832 - acc: 0.9861 - val_loss: 2.8642 - val_acc: 0.4482\n",
      "Epoch 30/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0792 - acc: 0.9888\n",
      "Epoch 00030: val_loss did not improve from 1.91282\n",
      "36805/36805 [==============================] - 11s 308us/sample - loss: 0.0791 - acc: 0.9888 - val_loss: 2.8880 - val_acc: 0.4517\n",
      "Epoch 31/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0725 - acc: 0.9886\n",
      "Epoch 00031: val_loss did not improve from 1.91282\n",
      "36805/36805 [==============================] - 12s 313us/sample - loss: 0.0725 - acc: 0.9886 - val_loss: 2.9244 - val_acc: 0.4547\n",
      "Epoch 32/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0700 - acc: 0.9891\n",
      "Epoch 00032: val_loss did not improve from 1.91282\n",
      "36805/36805 [==============================] - 11s 310us/sample - loss: 0.0701 - acc: 0.9891 - val_loss: 3.0151 - val_acc: 0.4475\n",
      "Epoch 33/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0761 - acc: 0.9873\n",
      "Epoch 00033: val_loss did not improve from 1.91282\n",
      "36805/36805 [==============================] - 11s 307us/sample - loss: 0.0760 - acc: 0.9873 - val_loss: 3.0251 - val_acc: 0.4535\n",
      "Epoch 34/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0679 - acc: 0.9889\n",
      "Epoch 00034: val_loss did not improve from 1.91282\n",
      "36805/36805 [==============================] - 11s 308us/sample - loss: 0.0681 - acc: 0.9888 - val_loss: 3.2061 - val_acc: 0.4382\n",
      "Epoch 35/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0678 - acc: 0.9899\n",
      "Epoch 00035: val_loss did not improve from 1.91282\n",
      "36805/36805 [==============================] - 11s 301us/sample - loss: 0.0679 - acc: 0.9899 - val_loss: 3.0162 - val_acc: 0.4594\n",
      "Epoch 36/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0542 - acc: 0.9927\n",
      "Epoch 00036: val_loss did not improve from 1.91282\n",
      "36805/36805 [==============================] - 11s 302us/sample - loss: 0.0545 - acc: 0.9927 - val_loss: 3.0025 - val_acc: 0.4561\n",
      "Epoch 37/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0597 - acc: 0.9899\n",
      "Epoch 00037: val_loss did not improve from 1.91282\n",
      "36805/36805 [==============================] - 11s 303us/sample - loss: 0.0599 - acc: 0.9898 - val_loss: 3.0537 - val_acc: 0.4468\n",
      "Epoch 38/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0579 - acc: 0.9904\n",
      "Epoch 00038: val_loss did not improve from 1.91282\n",
      "36805/36805 [==============================] - 11s 302us/sample - loss: 0.0584 - acc: 0.9904 - val_loss: 3.2424 - val_acc: 0.4423\n",
      "Epoch 39/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0596 - acc: 0.9898\n",
      "Epoch 00039: val_loss did not improve from 1.91282\n",
      "36805/36805 [==============================] - 11s 304us/sample - loss: 0.0600 - acc: 0.9898 - val_loss: 3.1628 - val_acc: 0.4428\n",
      "Epoch 40/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0575 - acc: 0.9906\n",
      "Epoch 00040: val_loss did not improve from 1.91282\n",
      "36805/36805 [==============================] - 11s 299us/sample - loss: 0.0575 - acc: 0.9905 - val_loss: 3.7087 - val_acc: 0.3960\n",
      "Epoch 41/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0552 - acc: 0.9906\n",
      "Epoch 00041: val_loss did not improve from 1.91282\n",
      "36805/36805 [==============================] - 11s 307us/sample - loss: 0.0554 - acc: 0.9905 - val_loss: 3.1726 - val_acc: 0.4528\n",
      "Epoch 42/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0507 - acc: 0.9922\n",
      "Epoch 00042: val_loss did not improve from 1.91282\n",
      "36805/36805 [==============================] - 11s 301us/sample - loss: 0.0507 - acc: 0.9922 - val_loss: 3.2643 - val_acc: 0.4410\n",
      "Epoch 43/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0464 - acc: 0.9930\n",
      "Epoch 00043: val_loss did not improve from 1.91282\n",
      "36805/36805 [==============================] - 11s 312us/sample - loss: 0.0466 - acc: 0.9930 - val_loss: 3.3362 - val_acc: 0.4405\n",
      "Epoch 44/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0458 - acc: 0.9929\n",
      "Epoch 00044: val_loss did not improve from 1.91282\n",
      "36805/36805 [==============================] - 11s 311us/sample - loss: 0.0462 - acc: 0.9928 - val_loss: 3.2645 - val_acc: 0.4498\n",
      "Epoch 45/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0494 - acc: 0.9919\n",
      "Epoch 00045: val_loss did not improve from 1.91282\n",
      "36805/36805 [==============================] - 11s 308us/sample - loss: 0.0497 - acc: 0.9919 - val_loss: 3.3536 - val_acc: 0.4386\n",
      "Epoch 46/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0473 - acc: 0.9923\n",
      "Epoch 00046: val_loss did not improve from 1.91282\n",
      "36805/36805 [==============================] - 11s 310us/sample - loss: 0.0474 - acc: 0.9923 - val_loss: 3.9036 - val_acc: 0.3748\n",
      "Epoch 47/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0413 - acc: 0.9942\n",
      "Epoch 00047: val_loss did not improve from 1.91282\n",
      "36805/36805 [==============================] - 11s 311us/sample - loss: 0.0413 - acc: 0.9941 - val_loss: 3.4110 - val_acc: 0.4375\n",
      "Epoch 48/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0476 - acc: 0.9924\n",
      "Epoch 00048: val_loss did not improve from 1.91282\n",
      "36805/36805 [==============================] - 11s 310us/sample - loss: 0.0477 - acc: 0.9924 - val_loss: 3.2996 - val_acc: 0.4500\n",
      "Epoch 49/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0408 - acc: 0.9940\n",
      "Epoch 00049: val_loss did not improve from 1.91282\n",
      "36805/36805 [==============================] - 11s 309us/sample - loss: 0.0410 - acc: 0.9940 - val_loss: 3.3234 - val_acc: 0.4535\n",
      "Epoch 50/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0422 - acc: 0.9925\n",
      "Epoch 00050: val_loss did not improve from 1.91282\n",
      "36805/36805 [==============================] - 11s 308us/sample - loss: 0.0424 - acc: 0.9924 - val_loss: 3.3615 - val_acc: 0.4496\n",
      "Epoch 51/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0391 - acc: 0.9938\n",
      "Epoch 00051: val_loss did not improve from 1.91282\n",
      "36805/36805 [==============================] - 11s 310us/sample - loss: 0.0391 - acc: 0.9938 - val_loss: 3.5269 - val_acc: 0.4302\n",
      "Epoch 52/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0333 - acc: 0.9960\n",
      "Epoch 00052: val_loss did not improve from 1.91282\n",
      "36805/36805 [==============================] - 11s 308us/sample - loss: 0.0340 - acc: 0.9959 - val_loss: 3.5087 - val_acc: 0.4349\n",
      "Epoch 53/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0437 - acc: 0.9920\n",
      "Epoch 00053: val_loss did not improve from 1.91282\n",
      "36805/36805 [==============================] - 12s 313us/sample - loss: 0.0441 - acc: 0.9920 - val_loss: 3.5494 - val_acc: 0.4344\n",
      "Epoch 54/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0439 - acc: 0.9923\n",
      "Epoch 00054: val_loss did not improve from 1.91282\n",
      "36805/36805 [==============================] - 11s 310us/sample - loss: 0.0442 - acc: 0.9923 - val_loss: 3.4572 - val_acc: 0.4458\n",
      "Epoch 55/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0424 - acc: 0.9931\n",
      "Epoch 00055: val_loss did not improve from 1.91282\n",
      "36805/36805 [==============================] - 11s 308us/sample - loss: 0.0428 - acc: 0.9930 - val_loss: 3.7027 - val_acc: 0.4137\n",
      "Epoch 56/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0353 - acc: 0.9946\n",
      "Epoch 00056: val_loss did not improve from 1.91282\n",
      "36805/36805 [==============================] - 11s 311us/sample - loss: 0.0354 - acc: 0.9946 - val_loss: 3.6138 - val_acc: 0.4305\n",
      "Epoch 57/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0337 - acc: 0.9951\n",
      "Epoch 00057: val_loss did not improve from 1.91282\n",
      "36805/36805 [==============================] - 11s 308us/sample - loss: 0.0339 - acc: 0.9950 - val_loss: 3.6567 - val_acc: 0.4330\n",
      "Epoch 58/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0336 - acc: 0.9948\n",
      "Epoch 00058: val_loss did not improve from 1.91282\n",
      "36805/36805 [==============================] - 11s 310us/sample - loss: 0.0336 - acc: 0.9948 - val_loss: 3.4618 - val_acc: 0.4465\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0310 - acc: 0.9953\n",
      "Epoch 00059: val_loss did not improve from 1.91282\n",
      "36805/36805 [==============================] - 11s 310us/sample - loss: 0.0312 - acc: 0.9952 - val_loss: 3.4876 - val_acc: 0.4454\n",
      "Epoch 60/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0354 - acc: 0.9939\n",
      "Epoch 00060: val_loss did not improve from 1.91282\n",
      "36805/36805 [==============================] - 11s 308us/sample - loss: 0.0354 - acc: 0.9939 - val_loss: 3.7002 - val_acc: 0.4368\n",
      "Epoch 61/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0330 - acc: 0.9947\n",
      "Epoch 00061: val_loss did not improve from 1.91282\n",
      "36805/36805 [==============================] - 11s 309us/sample - loss: 0.0330 - acc: 0.9947 - val_loss: 3.6913 - val_acc: 0.4354\n",
      "Epoch 62/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0326 - acc: 0.9942\n",
      "Epoch 00062: val_loss did not improve from 1.91282\n",
      "36805/36805 [==============================] - 11s 310us/sample - loss: 0.0329 - acc: 0.9941 - val_loss: 3.5913 - val_acc: 0.4398\n",
      "Epoch 63/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0322 - acc: 0.9943\n",
      "Epoch 00063: val_loss did not improve from 1.91282\n",
      "36805/36805 [==============================] - 12s 314us/sample - loss: 0.0325 - acc: 0.9942 - val_loss: 3.6141 - val_acc: 0.4454\n",
      "Epoch 64/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0348 - acc: 0.9941\n",
      "Epoch 00064: val_loss did not improve from 1.91282\n",
      "36805/36805 [==============================] - 11s 309us/sample - loss: 0.0349 - acc: 0.9940 - val_loss: 3.5790 - val_acc: 0.4472\n",
      "Epoch 65/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0332 - acc: 0.9944\n",
      "Epoch 00065: val_loss did not improve from 1.91282\n",
      "36805/36805 [==============================] - 11s 310us/sample - loss: 0.0334 - acc: 0.9943 - val_loss: 3.7060 - val_acc: 0.4342\n",
      "Epoch 66/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0301 - acc: 0.9950\n",
      "Epoch 00066: val_loss did not improve from 1.91282\n",
      "36805/36805 [==============================] - 11s 309us/sample - loss: 0.0302 - acc: 0.9950 - val_loss: 3.9646 - val_acc: 0.4167\n",
      "Epoch 67/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0318 - acc: 0.9949\n",
      "Epoch 00067: val_loss did not improve from 1.91282\n",
      "36805/36805 [==============================] - 11s 309us/sample - loss: 0.0318 - acc: 0.9949 - val_loss: 3.6332 - val_acc: 0.4403\n",
      "Epoch 68/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0284 - acc: 0.9955\n",
      "Epoch 00068: val_loss did not improve from 1.91282\n",
      "36805/36805 [==============================] - 11s 311us/sample - loss: 0.0285 - acc: 0.9954 - val_loss: 3.6254 - val_acc: 0.4437\n",
      "Epoch 69/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0277 - acc: 0.9957\n",
      "Epoch 00069: val_loss did not improve from 1.91282\n",
      "36805/36805 [==============================] - 11s 311us/sample - loss: 0.0277 - acc: 0.9957 - val_loss: 3.6135 - val_acc: 0.4475\n",
      "Epoch 70/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0297 - acc: 0.9952\n",
      "Epoch 00070: val_loss did not improve from 1.91282\n",
      "36805/36805 [==============================] - 11s 308us/sample - loss: 0.0298 - acc: 0.9952 - val_loss: 4.1134 - val_acc: 0.4223\n",
      "Epoch 71/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0283 - acc: 0.9959\n",
      "Epoch 00071: val_loss did not improve from 1.91282\n",
      "36805/36805 [==============================] - 11s 310us/sample - loss: 0.0284 - acc: 0.9959 - val_loss: 3.7585 - val_acc: 0.4407\n",
      "Epoch 72/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0295 - acc: 0.9950\n",
      "Epoch 00072: val_loss did not improve from 1.91282\n",
      "36805/36805 [==============================] - 12s 313us/sample - loss: 0.0297 - acc: 0.9949 - val_loss: 3.6519 - val_acc: 0.4386\n",
      "Epoch 73/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0313 - acc: 0.9946\n",
      "Epoch 00073: val_loss did not improve from 1.91282\n",
      "36805/36805 [==============================] - 11s 311us/sample - loss: 0.0313 - acc: 0.9946 - val_loss: 3.9374 - val_acc: 0.4326\n",
      "Epoch 74/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0243 - acc: 0.9961\n",
      "Epoch 00074: val_loss did not improve from 1.91282\n",
      "36805/36805 [==============================] - 12s 313us/sample - loss: 0.0244 - acc: 0.9960 - val_loss: 3.7138 - val_acc: 0.4505\n",
      "Epoch 75/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0285 - acc: 0.9955\n",
      "Epoch 00075: val_loss did not improve from 1.91282\n",
      "36805/36805 [==============================] - 11s 311us/sample - loss: 0.0292 - acc: 0.9954 - val_loss: 3.9240 - val_acc: 0.4330\n",
      "Epoch 76/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0295 - acc: 0.9946\n",
      "Epoch 00076: val_loss did not improve from 1.91282\n",
      "36805/36805 [==============================] - 11s 309us/sample - loss: 0.0310 - acc: 0.9945 - val_loss: 4.0283 - val_acc: 0.4216\n",
      "Epoch 77/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0318 - acc: 0.9943\n",
      "Epoch 00077: val_loss did not improve from 1.91282\n",
      "36805/36805 [==============================] - 11s 307us/sample - loss: 0.0317 - acc: 0.9943 - val_loss: 3.8134 - val_acc: 0.4342\n",
      "Epoch 78/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0229 - acc: 0.9971\n",
      "Epoch 00078: val_loss did not improve from 1.91282\n",
      "36805/36805 [==============================] - 11s 299us/sample - loss: 0.0231 - acc: 0.9970 - val_loss: 3.8211 - val_acc: 0.4377\n",
      "Epoch 79/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0271 - acc: 0.9953\n",
      "Epoch 00079: val_loss did not improve from 1.91282\n",
      "36805/36805 [==============================] - 11s 301us/sample - loss: 0.0274 - acc: 0.9953 - val_loss: 3.8264 - val_acc: 0.4328\n",
      "Epoch 80/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0244 - acc: 0.9967\n",
      "Epoch 00080: val_loss did not improve from 1.91282\n",
      "36805/36805 [==============================] - 11s 302us/sample - loss: 0.0246 - acc: 0.9966 - val_loss: 3.7078 - val_acc: 0.4486\n",
      "Epoch 81/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0226 - acc: 0.9967\n",
      "Epoch 00081: val_loss did not improve from 1.91282\n",
      "36805/36805 [==============================] - 11s 298us/sample - loss: 0.0227 - acc: 0.9966 - val_loss: 4.1472 - val_acc: 0.4100\n",
      "Epoch 82/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0249 - acc: 0.9956\n",
      "Epoch 00082: val_loss did not improve from 1.91282\n",
      "36805/36805 [==============================] - 11s 299us/sample - loss: 0.0255 - acc: 0.9955 - val_loss: 3.9366 - val_acc: 0.4344\n",
      "Epoch 83/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0263 - acc: 0.9952\n",
      "Epoch 00083: val_loss did not improve from 1.91282\n",
      "36805/36805 [==============================] - 11s 297us/sample - loss: 0.0264 - acc: 0.9952 - val_loss: 3.8820 - val_acc: 0.4372\n",
      "Epoch 84/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0256 - acc: 0.9954\n",
      "Epoch 00084: val_loss did not improve from 1.91282\n",
      "36805/36805 [==============================] - 11s 302us/sample - loss: 0.0259 - acc: 0.9954 - val_loss: 3.9708 - val_acc: 0.4326\n",
      "Epoch 85/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0257 - acc: 0.9956\n",
      "Epoch 00085: val_loss did not improve from 1.91282\n",
      "36805/36805 [==============================] - 11s 306us/sample - loss: 0.0260 - acc: 0.9955 - val_loss: 4.2011 - val_acc: 0.4202\n",
      "Epoch 86/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0278 - acc: 0.9951\n",
      "Epoch 00086: val_loss did not improve from 1.91282\n",
      "36805/36805 [==============================] - 11s 311us/sample - loss: 0.0280 - acc: 0.9950 - val_loss: 3.9907 - val_acc: 0.4295\n",
      "Epoch 87/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0314 - acc: 0.9940\n",
      "Epoch 00087: val_loss did not improve from 1.91282\n",
      "36805/36805 [==============================] - 12s 313us/sample - loss: 0.0314 - acc: 0.9940 - val_loss: 3.9815 - val_acc: 0.4396\n",
      "Epoch 88/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0220 - acc: 0.9968\n",
      "Epoch 00088: val_loss did not improve from 1.91282\n",
      "36805/36805 [==============================] - 11s 309us/sample - loss: 0.0221 - acc: 0.9968 - val_loss: 3.9704 - val_acc: 0.4300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 89/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0273 - acc: 0.9956\n",
      "Epoch 00089: val_loss did not improve from 1.91282\n",
      "36805/36805 [==============================] - 11s 310us/sample - loss: 0.0275 - acc: 0.9955 - val_loss: 4.0833 - val_acc: 0.4300\n",
      "Epoch 90/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0276 - acc: 0.9944\n",
      "Epoch 00090: val_loss did not improve from 1.91282\n",
      "36805/36805 [==============================] - 11s 310us/sample - loss: 0.0276 - acc: 0.9943 - val_loss: 4.0419 - val_acc: 0.4284\n",
      "Epoch 91/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0226 - acc: 0.9964\n",
      "Epoch 00091: val_loss did not improve from 1.91282\n",
      "36805/36805 [==============================] - 11s 308us/sample - loss: 0.0226 - acc: 0.9964 - val_loss: 4.0974 - val_acc: 0.4277\n",
      "Epoch 92/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0201 - acc: 0.9972\n",
      "Epoch 00092: val_loss did not improve from 1.91282\n",
      "36805/36805 [==============================] - 11s 309us/sample - loss: 0.0203 - acc: 0.9972 - val_loss: 3.9861 - val_acc: 0.4326\n",
      "Epoch 93/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0267 - acc: 0.9954\n",
      "Epoch 00093: val_loss did not improve from 1.91282\n",
      "36805/36805 [==============================] - 11s 311us/sample - loss: 0.0268 - acc: 0.9954 - val_loss: 4.2873 - val_acc: 0.4107\n",
      "Epoch 94/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0254 - acc: 0.9958\n",
      "Epoch 00094: val_loss did not improve from 1.91282\n",
      "36805/36805 [==============================] - 11s 309us/sample - loss: 0.0255 - acc: 0.9958 - val_loss: 4.0841 - val_acc: 0.4354\n",
      "Epoch 95/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0237 - acc: 0.9959\n",
      "Epoch 00095: val_loss did not improve from 1.91282\n",
      "36805/36805 [==============================] - 11s 309us/sample - loss: 0.0239 - acc: 0.9959 - val_loss: 4.3735 - val_acc: 0.4149\n",
      "Epoch 96/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0236 - acc: 0.9959\n",
      "Epoch 00096: val_loss did not improve from 1.91282\n",
      "36805/36805 [==============================] - 11s 312us/sample - loss: 0.0238 - acc: 0.9959 - val_loss: 4.3084 - val_acc: 0.4321\n",
      "Epoch 97/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0265 - acc: 0.9956\n",
      "Epoch 00097: val_loss did not improve from 1.91282\n",
      "36805/36805 [==============================] - 11s 312us/sample - loss: 0.0265 - acc: 0.9956 - val_loss: 4.1863 - val_acc: 0.4249\n",
      "Epoch 98/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0223 - acc: 0.9965\n",
      "Epoch 00098: val_loss did not improve from 1.91282\n",
      "36805/36805 [==============================] - 12s 314us/sample - loss: 0.0224 - acc: 0.9964 - val_loss: 4.1768 - val_acc: 0.4239\n",
      "Epoch 99/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0200 - acc: 0.9970\n",
      "Epoch 00099: val_loss did not improve from 1.91282\n",
      "36805/36805 [==============================] - 11s 309us/sample - loss: 0.0202 - acc: 0.9970 - val_loss: 4.0536 - val_acc: 0.4391\n",
      "Epoch 100/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0234 - acc: 0.9958\n",
      "Epoch 00100: val_loss did not improve from 1.91282\n",
      "36805/36805 [==============================] - 11s 312us/sample - loss: 0.0236 - acc: 0.9958 - val_loss: 4.0710 - val_acc: 0.4347\n",
      "Epoch 101/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0259 - acc: 0.9953\n",
      "Epoch 00101: val_loss did not improve from 1.91282\n",
      "36805/36805 [==============================] - 11s 307us/sample - loss: 0.0264 - acc: 0.9953 - val_loss: 4.2172 - val_acc: 0.4272\n",
      "Epoch 102/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0267 - acc: 0.9943\n",
      "Epoch 00102: val_loss did not improve from 1.91282\n",
      "36805/36805 [==============================] - 11s 309us/sample - loss: 0.0272 - acc: 0.9943 - val_loss: 4.0815 - val_acc: 0.4340\n",
      "Epoch 103/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0259 - acc: 0.9950\n",
      "Epoch 00103: val_loss did not improve from 1.91282\n",
      "36805/36805 [==============================] - 11s 310us/sample - loss: 0.0259 - acc: 0.9951 - val_loss: 4.2959 - val_acc: 0.4291\n",
      "Epoch 104/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0202 - acc: 0.9972\n",
      "Epoch 00104: val_loss did not improve from 1.91282\n",
      "36805/36805 [==============================] - 12s 318us/sample - loss: 0.0206 - acc: 0.9971 - val_loss: 4.2421 - val_acc: 0.4184\n",
      "\n",
      "1D_CNN_only_conv_conv_5_BN_2_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 1s 229us/sample - loss: 1.9663 - acc: 0.4320\n",
      "Loss: 1.9662681654358702 Accuracy: 0.43198338\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.1152 - acc: 0.3607\n",
      "Epoch 00001: val_loss improved from inf to 1.72444, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_BN_3_conv_checkpoint/001-1.7244.hdf5\n",
      "36805/36805 [==============================] - 17s 470us/sample - loss: 2.1151 - acc: 0.3608 - val_loss: 1.7244 - val_acc: 0.4458\n",
      "Epoch 2/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.4620 - acc: 0.5500\n",
      "Epoch 00002: val_loss improved from 1.72444 to 1.48501, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_BN_3_conv_checkpoint/002-1.4850.hdf5\n",
      "36805/36805 [==============================] - 13s 364us/sample - loss: 1.4616 - acc: 0.5501 - val_loss: 1.4850 - val_acc: 0.5469\n",
      "Epoch 3/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1996 - acc: 0.6327\n",
      "Epoch 00003: val_loss improved from 1.48501 to 1.42210, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_BN_3_conv_checkpoint/003-1.4221.hdf5\n",
      "36805/36805 [==============================] - 13s 363us/sample - loss: 1.1996 - acc: 0.6326 - val_loss: 1.4221 - val_acc: 0.5763\n",
      "Epoch 4/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0212 - acc: 0.6863\n",
      "Epoch 00004: val_loss improved from 1.42210 to 1.33706, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_BN_3_conv_checkpoint/004-1.3371.hdf5\n",
      "36805/36805 [==============================] - 13s 363us/sample - loss: 1.0212 - acc: 0.6864 - val_loss: 1.3371 - val_acc: 0.5996\n",
      "Epoch 5/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8656 - acc: 0.7364\n",
      "Epoch 00005: val_loss did not improve from 1.33706\n",
      "36805/36805 [==============================] - 13s 361us/sample - loss: 0.8657 - acc: 0.7364 - val_loss: 1.3440 - val_acc: 0.6040\n",
      "Epoch 6/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7425 - acc: 0.7748\n",
      "Epoch 00006: val_loss did not improve from 1.33706\n",
      "36805/36805 [==============================] - 13s 359us/sample - loss: 0.7427 - acc: 0.7748 - val_loss: 1.3423 - val_acc: 0.6082\n",
      "Epoch 7/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6381 - acc: 0.8094\n",
      "Epoch 00007: val_loss did not improve from 1.33706\n",
      "36805/36805 [==============================] - 13s 362us/sample - loss: 0.6382 - acc: 0.8093 - val_loss: 1.4004 - val_acc: 0.6028\n",
      "Epoch 8/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5464 - acc: 0.8433\n",
      "Epoch 00008: val_loss did not improve from 1.33706\n",
      "36805/36805 [==============================] - 13s 358us/sample - loss: 0.5466 - acc: 0.8431 - val_loss: 1.3804 - val_acc: 0.6077\n",
      "Epoch 9/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4715 - acc: 0.8681\n",
      "Epoch 00009: val_loss did not improve from 1.33706\n",
      "36805/36805 [==============================] - 13s 356us/sample - loss: 0.4718 - acc: 0.8680 - val_loss: 1.4351 - val_acc: 0.6019\n",
      "Epoch 10/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4041 - acc: 0.8938\n",
      "Epoch 00010: val_loss did not improve from 1.33706\n",
      "36805/36805 [==============================] - 13s 361us/sample - loss: 0.4042 - acc: 0.8937 - val_loss: 1.3849 - val_acc: 0.6182\n",
      "Epoch 11/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3530 - acc: 0.9089\n",
      "Epoch 00011: val_loss did not improve from 1.33706\n",
      "36805/36805 [==============================] - 13s 363us/sample - loss: 0.3532 - acc: 0.9088 - val_loss: 1.4020 - val_acc: 0.6226\n",
      "Epoch 12/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3066 - acc: 0.9238\n",
      "Epoch 00012: val_loss did not improve from 1.33706\n",
      "36805/36805 [==============================] - 13s 363us/sample - loss: 0.3067 - acc: 0.9238 - val_loss: 1.4422 - val_acc: 0.6112\n",
      "Epoch 13/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2667 - acc: 0.9359\n",
      "Epoch 00013: val_loss did not improve from 1.33706\n",
      "36805/36805 [==============================] - 13s 360us/sample - loss: 0.2668 - acc: 0.9358 - val_loss: 1.4369 - val_acc: 0.6315\n",
      "Epoch 14/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2325 - acc: 0.9490\n",
      "Epoch 00014: val_loss did not improve from 1.33706\n",
      "36805/36805 [==============================] - 13s 358us/sample - loss: 0.2327 - acc: 0.9488 - val_loss: 1.4596 - val_acc: 0.6194\n",
      "Epoch 15/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2082 - acc: 0.9556\n",
      "Epoch 00015: val_loss did not improve from 1.33706\n",
      "36805/36805 [==============================] - 13s 360us/sample - loss: 0.2084 - acc: 0.9555 - val_loss: 1.4781 - val_acc: 0.6147\n",
      "Epoch 16/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1819 - acc: 0.9641\n",
      "Epoch 00016: val_loss did not improve from 1.33706\n",
      "36805/36805 [==============================] - 13s 360us/sample - loss: 0.1823 - acc: 0.9640 - val_loss: 1.5903 - val_acc: 0.6000\n",
      "Epoch 17/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1715 - acc: 0.9674\n",
      "Epoch 00017: val_loss did not improve from 1.33706\n",
      "36805/36805 [==============================] - 13s 360us/sample - loss: 0.1716 - acc: 0.9673 - val_loss: 1.6017 - val_acc: 0.6014\n",
      "Epoch 18/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1457 - acc: 0.9737\n",
      "Epoch 00018: val_loss did not improve from 1.33706\n",
      "36805/36805 [==============================] - 13s 357us/sample - loss: 0.1458 - acc: 0.9737 - val_loss: 1.6607 - val_acc: 0.5982\n",
      "Epoch 19/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1315 - acc: 0.9788\n",
      "Epoch 00019: val_loss did not improve from 1.33706\n",
      "36805/36805 [==============================] - 13s 361us/sample - loss: 0.1317 - acc: 0.9787 - val_loss: 1.6269 - val_acc: 0.6103\n",
      "Epoch 20/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1237 - acc: 0.9788\n",
      "Epoch 00020: val_loss did not improve from 1.33706\n",
      "36805/36805 [==============================] - 13s 358us/sample - loss: 0.1240 - acc: 0.9787 - val_loss: 1.6799 - val_acc: 0.6014\n",
      "Epoch 21/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1111 - acc: 0.9825\n",
      "Epoch 00021: val_loss did not improve from 1.33706\n",
      "36805/36805 [==============================] - 13s 360us/sample - loss: 0.1111 - acc: 0.9825 - val_loss: 1.6165 - val_acc: 0.6205\n",
      "Epoch 22/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1020 - acc: 0.9845\n",
      "Epoch 00022: val_loss did not improve from 1.33706\n",
      "36805/36805 [==============================] - 13s 360us/sample - loss: 0.1020 - acc: 0.9845 - val_loss: 1.7123 - val_acc: 0.6045\n",
      "Epoch 23/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0923 - acc: 0.9865\n",
      "Epoch 00023: val_loss did not improve from 1.33706\n",
      "36805/36805 [==============================] - 13s 359us/sample - loss: 0.0928 - acc: 0.9864 - val_loss: 1.6634 - val_acc: 0.6231\n",
      "Epoch 24/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0881 - acc: 0.9867\n",
      "Epoch 00024: val_loss did not improve from 1.33706\n",
      "36805/36805 [==============================] - 13s 360us/sample - loss: 0.0881 - acc: 0.9867 - val_loss: 1.7038 - val_acc: 0.6159\n",
      "Epoch 25/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0813 - acc: 0.9878\n",
      "Epoch 00025: val_loss did not improve from 1.33706\n",
      "36805/36805 [==============================] - 13s 360us/sample - loss: 0.0814 - acc: 0.9877 - val_loss: 1.8033 - val_acc: 0.6012\n",
      "Epoch 26/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0705 - acc: 0.9915\n",
      "Epoch 00026: val_loss did not improve from 1.33706\n",
      "36805/36805 [==============================] - 13s 361us/sample - loss: 0.0707 - acc: 0.9914 - val_loss: 1.7881 - val_acc: 0.6159\n",
      "Epoch 27/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0685 - acc: 0.9908\n",
      "Epoch 00027: val_loss did not improve from 1.33706\n",
      "36805/36805 [==============================] - 13s 357us/sample - loss: 0.0688 - acc: 0.9907 - val_loss: 1.7938 - val_acc: 0.6105\n",
      "Epoch 28/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0692 - acc: 0.9897\n",
      "Epoch 00028: val_loss did not improve from 1.33706\n",
      "36805/36805 [==============================] - 13s 359us/sample - loss: 0.0696 - acc: 0.9897 - val_loss: 1.7801 - val_acc: 0.6182\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0629 - acc: 0.9915\n",
      "Epoch 00029: val_loss did not improve from 1.33706\n",
      "36805/36805 [==============================] - 13s 359us/sample - loss: 0.0632 - acc: 0.9914 - val_loss: 1.9584 - val_acc: 0.5919\n",
      "Epoch 30/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0792 - acc: 0.9875\n",
      "Epoch 00030: val_loss did not improve from 1.33706\n",
      "36805/36805 [==============================] - 13s 361us/sample - loss: 0.0792 - acc: 0.9876 - val_loss: 1.8543 - val_acc: 0.6066\n",
      "Epoch 31/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0496 - acc: 0.9951\n",
      "Epoch 00031: val_loss did not improve from 1.33706\n",
      "36805/36805 [==============================] - 13s 357us/sample - loss: 0.0496 - acc: 0.9950 - val_loss: 1.8672 - val_acc: 0.6108\n",
      "Epoch 32/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0485 - acc: 0.9949\n",
      "Epoch 00032: val_loss did not improve from 1.33706\n",
      "36805/36805 [==============================] - 13s 359us/sample - loss: 0.0486 - acc: 0.9949 - val_loss: 1.9063 - val_acc: 0.6087\n",
      "Epoch 33/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0516 - acc: 0.9939\n",
      "Epoch 00033: val_loss did not improve from 1.33706\n",
      "36805/36805 [==============================] - 13s 358us/sample - loss: 0.0516 - acc: 0.9939 - val_loss: 2.2000 - val_acc: 0.5544\n",
      "Epoch 34/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0484 - acc: 0.9935\n",
      "Epoch 00034: val_loss did not improve from 1.33706\n",
      "36805/36805 [==============================] - 13s 360us/sample - loss: 0.0484 - acc: 0.9935 - val_loss: 1.9357 - val_acc: 0.6075\n",
      "Epoch 35/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0469 - acc: 0.9937\n",
      "Epoch 00035: val_loss did not improve from 1.33706\n",
      "36805/36805 [==============================] - 13s 361us/sample - loss: 0.0470 - acc: 0.9936 - val_loss: 2.0367 - val_acc: 0.5926\n",
      "Epoch 36/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0425 - acc: 0.9945\n",
      "Epoch 00036: val_loss did not improve from 1.33706\n",
      "36805/36805 [==============================] - 13s 360us/sample - loss: 0.0426 - acc: 0.9945 - val_loss: 1.9843 - val_acc: 0.6000\n",
      "Epoch 37/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0443 - acc: 0.9937\n",
      "Epoch 00037: val_loss did not improve from 1.33706\n",
      "36805/36805 [==============================] - 13s 361us/sample - loss: 0.0443 - acc: 0.9937 - val_loss: 1.9237 - val_acc: 0.6177\n",
      "Epoch 38/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0386 - acc: 0.9953\n",
      "Epoch 00038: val_loss did not improve from 1.33706\n",
      "36805/36805 [==============================] - 13s 362us/sample - loss: 0.0391 - acc: 0.9952 - val_loss: 2.0204 - val_acc: 0.5928\n",
      "Epoch 39/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0375 - acc: 0.9952\n",
      "Epoch 00039: val_loss did not improve from 1.33706\n",
      "36805/36805 [==============================] - 13s 360us/sample - loss: 0.0377 - acc: 0.9951 - val_loss: 2.0100 - val_acc: 0.6019\n",
      "Epoch 40/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0358 - acc: 0.9959\n",
      "Epoch 00040: val_loss did not improve from 1.33706\n",
      "36805/36805 [==============================] - 13s 361us/sample - loss: 0.0365 - acc: 0.9959 - val_loss: 2.0578 - val_acc: 0.6012\n",
      "Epoch 41/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0378 - acc: 0.9949\n",
      "Epoch 00041: val_loss did not improve from 1.33706\n",
      "36805/36805 [==============================] - 13s 362us/sample - loss: 0.0383 - acc: 0.9949 - val_loss: 2.0436 - val_acc: 0.6042\n",
      "Epoch 42/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0332 - acc: 0.9962\n",
      "Epoch 00042: val_loss did not improve from 1.33706\n",
      "36805/36805 [==============================] - 13s 359us/sample - loss: 0.0333 - acc: 0.9961 - val_loss: 2.1468 - val_acc: 0.5954\n",
      "Epoch 43/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0332 - acc: 0.9959\n",
      "Epoch 00043: val_loss did not improve from 1.33706\n",
      "36805/36805 [==============================] - 13s 361us/sample - loss: 0.0332 - acc: 0.9959 - val_loss: 2.0880 - val_acc: 0.6070\n",
      "Epoch 44/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0319 - acc: 0.9961\n",
      "Epoch 00044: val_loss did not improve from 1.33706\n",
      "36805/36805 [==============================] - 13s 359us/sample - loss: 0.0322 - acc: 0.9960 - val_loss: 2.1196 - val_acc: 0.6026\n",
      "Epoch 45/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0344 - acc: 0.9955\n",
      "Epoch 00045: val_loss did not improve from 1.33706\n",
      "36805/36805 [==============================] - 13s 361us/sample - loss: 0.0345 - acc: 0.9954 - val_loss: 2.0968 - val_acc: 0.6070\n",
      "Epoch 46/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0311 - acc: 0.9967\n",
      "Epoch 00046: val_loss did not improve from 1.33706\n",
      "36805/36805 [==============================] - 13s 362us/sample - loss: 0.0313 - acc: 0.9966 - val_loss: 2.2593 - val_acc: 0.5858\n",
      "Epoch 47/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0324 - acc: 0.9965\n",
      "Epoch 00047: val_loss did not improve from 1.33706\n",
      "36805/36805 [==============================] - 13s 359us/sample - loss: 0.0325 - acc: 0.9964 - val_loss: 2.1017 - val_acc: 0.6061\n",
      "Epoch 48/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0311 - acc: 0.9961\n",
      "Epoch 00048: val_loss did not improve from 1.33706\n",
      "36805/36805 [==============================] - 13s 359us/sample - loss: 0.0312 - acc: 0.9961 - val_loss: 2.2273 - val_acc: 0.5917\n",
      "Epoch 49/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0325 - acc: 0.9961\n",
      "Epoch 00049: val_loss did not improve from 1.33706\n",
      "36805/36805 [==============================] - 13s 359us/sample - loss: 0.0325 - acc: 0.9961 - val_loss: 2.1096 - val_acc: 0.6082\n",
      "Epoch 50/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0277 - acc: 0.9964\n",
      "Epoch 00050: val_loss did not improve from 1.33706\n",
      "36805/36805 [==============================] - 13s 360us/sample - loss: 0.0277 - acc: 0.9964 - val_loss: 2.2108 - val_acc: 0.5945\n",
      "Epoch 51/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0263 - acc: 0.9970\n",
      "Epoch 00051: val_loss did not improve from 1.33706\n",
      "36805/36805 [==============================] - 13s 362us/sample - loss: 0.0263 - acc: 0.9970 - val_loss: 2.2925 - val_acc: 0.5889\n",
      "Epoch 52/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0234 - acc: 0.9973\n",
      "Epoch 00052: val_loss did not improve from 1.33706\n",
      "36805/36805 [==============================] - 13s 358us/sample - loss: 0.0239 - acc: 0.9973 - val_loss: 2.1482 - val_acc: 0.6031\n",
      "Epoch 53/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0275 - acc: 0.9962\n",
      "Epoch 00053: val_loss did not improve from 1.33706\n",
      "36805/36805 [==============================] - 13s 361us/sample - loss: 0.0277 - acc: 0.9962 - val_loss: 2.2704 - val_acc: 0.5924\n",
      "Epoch 54/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0310 - acc: 0.9954\n",
      "Epoch 00054: val_loss did not improve from 1.33706\n",
      "36805/36805 [==============================] - 13s 360us/sample - loss: 0.0311 - acc: 0.9953 - val_loss: 2.1519 - val_acc: 0.6021\n",
      "Epoch 55/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0267 - acc: 0.9970\n",
      "Epoch 00055: val_loss did not improve from 1.33706\n",
      "36805/36805 [==============================] - 13s 359us/sample - loss: 0.0268 - acc: 0.9970 - val_loss: 2.2251 - val_acc: 0.6012\n",
      "Epoch 56/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0253 - acc: 0.9971\n",
      "Epoch 00056: val_loss did not improve from 1.33706\n",
      "36805/36805 [==============================] - 13s 362us/sample - loss: 0.0259 - acc: 0.9970 - val_loss: 2.2019 - val_acc: 0.6068\n",
      "Epoch 57/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0288 - acc: 0.9954\n",
      "Epoch 00057: val_loss did not improve from 1.33706\n",
      "36805/36805 [==============================] - 13s 362us/sample - loss: 0.0291 - acc: 0.9953 - val_loss: 2.3109 - val_acc: 0.5954\n",
      "Epoch 58/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0270 - acc: 0.9961\n",
      "Epoch 00058: val_loss did not improve from 1.33706\n",
      "36805/36805 [==============================] - 13s 361us/sample - loss: 0.0270 - acc: 0.9961 - val_loss: 2.2902 - val_acc: 0.5931\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0237 - acc: 0.9973\n",
      "Epoch 00059: val_loss did not improve from 1.33706\n",
      "36805/36805 [==============================] - 13s 359us/sample - loss: 0.0240 - acc: 0.9973 - val_loss: 2.2426 - val_acc: 0.6005\n",
      "Epoch 60/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0253 - acc: 0.9968\n",
      "Epoch 00060: val_loss did not improve from 1.33706\n",
      "36805/36805 [==============================] - 13s 360us/sample - loss: 0.0252 - acc: 0.9968 - val_loss: 2.2319 - val_acc: 0.6061\n",
      "Epoch 61/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0207 - acc: 0.9977\n",
      "Epoch 00061: val_loss did not improve from 1.33706\n",
      "36805/36805 [==============================] - 13s 359us/sample - loss: 0.0208 - acc: 0.9976 - val_loss: 2.5021 - val_acc: 0.5751\n",
      "Epoch 62/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0244 - acc: 0.9966\n",
      "Epoch 00062: val_loss did not improve from 1.33706\n",
      "36805/36805 [==============================] - 13s 360us/sample - loss: 0.0244 - acc: 0.9966 - val_loss: 2.2574 - val_acc: 0.6084\n",
      "Epoch 63/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0202 - acc: 0.9976\n",
      "Epoch 00063: val_loss did not improve from 1.33706\n",
      "36805/36805 [==============================] - 13s 364us/sample - loss: 0.0206 - acc: 0.9976 - val_loss: 2.2438 - val_acc: 0.6045\n",
      "Epoch 64/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0239 - acc: 0.9966\n",
      "Epoch 00064: val_loss did not improve from 1.33706\n",
      "36805/36805 [==============================] - 13s 361us/sample - loss: 0.0241 - acc: 0.9966 - val_loss: 2.3434 - val_acc: 0.5935\n",
      "Epoch 65/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0210 - acc: 0.9973\n",
      "Epoch 00065: val_loss did not improve from 1.33706\n",
      "36805/36805 [==============================] - 13s 361us/sample - loss: 0.0210 - acc: 0.9973 - val_loss: 2.2882 - val_acc: 0.6117\n",
      "Epoch 66/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0227 - acc: 0.9969\n",
      "Epoch 00066: val_loss did not improve from 1.33706\n",
      "36805/36805 [==============================] - 13s 359us/sample - loss: 0.0228 - acc: 0.9969 - val_loss: 2.3764 - val_acc: 0.5919\n",
      "Epoch 67/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0222 - acc: 0.9972\n",
      "Epoch 00067: val_loss did not improve from 1.33706\n",
      "36805/36805 [==============================] - 13s 357us/sample - loss: 0.0225 - acc: 0.9971 - val_loss: 2.3449 - val_acc: 0.5954\n",
      "Epoch 68/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0242 - acc: 0.9962\n",
      "Epoch 00068: val_loss did not improve from 1.33706\n",
      "36805/36805 [==============================] - 13s 361us/sample - loss: 0.0242 - acc: 0.9962 - val_loss: 2.3304 - val_acc: 0.6010\n",
      "Epoch 69/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0229 - acc: 0.9967\n",
      "Epoch 00069: val_loss did not improve from 1.33706\n",
      "36805/36805 [==============================] - 13s 361us/sample - loss: 0.0230 - acc: 0.9966 - val_loss: 2.3242 - val_acc: 0.6014\n",
      "Epoch 70/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0211 - acc: 0.9971\n",
      "Epoch 00070: val_loss did not improve from 1.33706\n",
      "36805/36805 [==============================] - 13s 360us/sample - loss: 0.0211 - acc: 0.9970 - val_loss: 2.2939 - val_acc: 0.5996\n",
      "Epoch 71/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0194 - acc: 0.9976\n",
      "Epoch 00071: val_loss did not improve from 1.33706\n",
      "36805/36805 [==============================] - 13s 362us/sample - loss: 0.0194 - acc: 0.9976 - val_loss: 2.3311 - val_acc: 0.6017\n",
      "Epoch 72/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0185 - acc: 0.9976\n",
      "Epoch 00072: val_loss did not improve from 1.33706\n",
      "36805/36805 [==============================] - 13s 360us/sample - loss: 0.0186 - acc: 0.9976 - val_loss: 2.4123 - val_acc: 0.5989\n",
      "Epoch 73/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0223 - acc: 0.9968\n",
      "Epoch 00073: val_loss did not improve from 1.33706\n",
      "36805/36805 [==============================] - 13s 359us/sample - loss: 0.0225 - acc: 0.9967 - val_loss: 2.3396 - val_acc: 0.6024\n",
      "Epoch 74/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0229 - acc: 0.9966\n",
      "Epoch 00074: val_loss did not improve from 1.33706\n",
      "36805/36805 [==============================] - 13s 360us/sample - loss: 0.0232 - acc: 0.9965 - val_loss: 2.3771 - val_acc: 0.6061\n",
      "Epoch 75/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0205 - acc: 0.9971\n",
      "Epoch 00075: val_loss did not improve from 1.33706\n",
      "36805/36805 [==============================] - 13s 359us/sample - loss: 0.0212 - acc: 0.9970 - val_loss: 2.3847 - val_acc: 0.5942\n",
      "Epoch 76/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0196 - acc: 0.9972\n",
      "Epoch 00076: val_loss did not improve from 1.33706\n",
      "36805/36805 [==============================] - 13s 365us/sample - loss: 0.0196 - acc: 0.9972 - val_loss: 2.3416 - val_acc: 0.6000\n",
      "Epoch 77/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0155 - acc: 0.9986\n",
      "Epoch 00077: val_loss did not improve from 1.33706\n",
      "36805/36805 [==============================] - 13s 360us/sample - loss: 0.0158 - acc: 0.9985 - val_loss: 2.4782 - val_acc: 0.5926\n",
      "Epoch 78/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0198 - acc: 0.9973\n",
      "Epoch 00078: val_loss did not improve from 1.33706\n",
      "36805/36805 [==============================] - 13s 361us/sample - loss: 0.0207 - acc: 0.9971 - val_loss: 2.4216 - val_acc: 0.5989\n",
      "Epoch 79/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0264 - acc: 0.9957\n",
      "Epoch 00079: val_loss did not improve from 1.33706\n",
      "36805/36805 [==============================] - 13s 361us/sample - loss: 0.0265 - acc: 0.9957 - val_loss: 2.5517 - val_acc: 0.5765\n",
      "Epoch 80/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0173 - acc: 0.9979\n",
      "Epoch 00080: val_loss did not improve from 1.33706\n",
      "36805/36805 [==============================] - 13s 360us/sample - loss: 0.0175 - acc: 0.9979 - val_loss: 2.5144 - val_acc: 0.5875\n",
      "Epoch 81/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0166 - acc: 0.9977\n",
      "Epoch 00081: val_loss did not improve from 1.33706\n",
      "36805/36805 [==============================] - 13s 357us/sample - loss: 0.0169 - acc: 0.9977 - val_loss: 2.4222 - val_acc: 0.5961\n",
      "Epoch 82/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0180 - acc: 0.9977\n",
      "Epoch 00082: val_loss did not improve from 1.33706\n",
      "36805/36805 [==============================] - 13s 361us/sample - loss: 0.0183 - acc: 0.9976 - val_loss: 2.5419 - val_acc: 0.5782\n",
      "Epoch 83/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0312 - acc: 0.9950\n",
      "Epoch 00083: val_loss did not improve from 1.33706\n",
      "36805/36805 [==============================] - 13s 359us/sample - loss: 0.0312 - acc: 0.9950 - val_loss: 2.4068 - val_acc: 0.5998\n",
      "Epoch 84/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0166 - acc: 0.9979\n",
      "Epoch 00084: val_loss did not improve from 1.33706\n",
      "36805/36805 [==============================] - 13s 362us/sample - loss: 0.0166 - acc: 0.9979 - val_loss: 2.4278 - val_acc: 0.6014\n",
      "Epoch 85/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0161 - acc: 0.9980\n",
      "Epoch 00085: val_loss did not improve from 1.33706\n",
      "36805/36805 [==============================] - 13s 360us/sample - loss: 0.0162 - acc: 0.9980 - val_loss: 2.4515 - val_acc: 0.5989\n",
      "Epoch 86/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0134 - acc: 0.9989\n",
      "Epoch 00086: val_loss did not improve from 1.33706\n",
      "36805/36805 [==============================] - 13s 357us/sample - loss: 0.0136 - acc: 0.9989 - val_loss: 2.4103 - val_acc: 0.6033\n",
      "Epoch 87/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0182 - acc: 0.9973\n",
      "Epoch 00087: val_loss did not improve from 1.33706\n",
      "36805/36805 [==============================] - 13s 358us/sample - loss: 0.0183 - acc: 0.9973 - val_loss: 2.4310 - val_acc: 0.6005\n",
      "Epoch 88/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0171 - acc: 0.9976\n",
      "Epoch 00088: val_loss did not improve from 1.33706\n",
      "36805/36805 [==============================] - 13s 359us/sample - loss: 0.0173 - acc: 0.9975 - val_loss: 2.4456 - val_acc: 0.6052\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 89/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0166 - acc: 0.9973\n",
      "Epoch 00089: val_loss did not improve from 1.33706\n",
      "36805/36805 [==============================] - 13s 362us/sample - loss: 0.0166 - acc: 0.9973 - val_loss: 2.4795 - val_acc: 0.6026\n",
      "Epoch 90/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0121 - acc: 0.9992\n",
      "Epoch 00090: val_loss did not improve from 1.33706\n",
      "36805/36805 [==============================] - 13s 361us/sample - loss: 0.0122 - acc: 0.9992 - val_loss: 2.6332 - val_acc: 0.5823\n",
      "Epoch 91/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0140 - acc: 0.9981\n",
      "Epoch 00091: val_loss did not improve from 1.33706\n",
      "36805/36805 [==============================] - 13s 362us/sample - loss: 0.0140 - acc: 0.9982 - val_loss: 2.4534 - val_acc: 0.6028\n",
      "Epoch 92/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0163 - acc: 0.9978\n",
      "Epoch 00092: val_loss did not improve from 1.33706\n",
      "36805/36805 [==============================] - 13s 359us/sample - loss: 0.0163 - acc: 0.9978 - val_loss: 2.4161 - val_acc: 0.6007\n",
      "Epoch 93/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0140 - acc: 0.9983\n",
      "Epoch 00093: val_loss did not improve from 1.33706\n",
      "36805/36805 [==============================] - 13s 360us/sample - loss: 0.0141 - acc: 0.9983 - val_loss: 2.6711 - val_acc: 0.5823\n",
      "Epoch 94/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0186 - acc: 0.9973\n",
      "Epoch 00094: val_loss did not improve from 1.33706\n",
      "36805/36805 [==============================] - 13s 359us/sample - loss: 0.0186 - acc: 0.9973 - val_loss: 2.6366 - val_acc: 0.5800\n",
      "Epoch 95/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0163 - acc: 0.9979\n",
      "Epoch 00095: val_loss did not improve from 1.33706\n",
      "36805/36805 [==============================] - 13s 360us/sample - loss: 0.0163 - acc: 0.9979 - val_loss: 2.5122 - val_acc: 0.5961\n",
      "Epoch 96/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0170 - acc: 0.9976\n",
      "Epoch 00096: val_loss did not improve from 1.33706\n",
      "36805/36805 [==============================] - 13s 361us/sample - loss: 0.0171 - acc: 0.9976 - val_loss: 2.5214 - val_acc: 0.5963\n",
      "Epoch 97/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0204 - acc: 0.9969- ETA: 0s - loss: 0.0206 \n",
      "Epoch 00097: val_loss did not improve from 1.33706\n",
      "36805/36805 [==============================] - 13s 361us/sample - loss: 0.0205 - acc: 0.9968 - val_loss: 2.4789 - val_acc: 0.6087\n",
      "Epoch 98/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0176 - acc: 0.9974\n",
      "Epoch 00098: val_loss did not improve from 1.33706\n",
      "36805/36805 [==============================] - 13s 359us/sample - loss: 0.0176 - acc: 0.9974 - val_loss: 2.5163 - val_acc: 0.5963\n",
      "Epoch 99/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0153 - acc: 0.9980\n",
      "Epoch 00099: val_loss did not improve from 1.33706\n",
      "36805/36805 [==============================] - 13s 361us/sample - loss: 0.0154 - acc: 0.9980 - val_loss: 2.7465 - val_acc: 0.5784\n",
      "Epoch 100/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0127 - acc: 0.9986\n",
      "Epoch 00100: val_loss did not improve from 1.33706\n",
      "36805/36805 [==============================] - 13s 359us/sample - loss: 0.0128 - acc: 0.9985 - val_loss: 2.5269 - val_acc: 0.5945\n",
      "Epoch 101/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0157 - acc: 0.9980\n",
      "Epoch 00101: val_loss did not improve from 1.33706\n",
      "36805/36805 [==============================] - 13s 358us/sample - loss: 0.0158 - acc: 0.9980 - val_loss: 2.5213 - val_acc: 0.5952\n",
      "Epoch 102/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0151 - acc: 0.9980\n",
      "Epoch 00102: val_loss did not improve from 1.33706\n",
      "36805/36805 [==============================] - 13s 360us/sample - loss: 0.0151 - acc: 0.9979 - val_loss: 2.6084 - val_acc: 0.5886\n",
      "Epoch 103/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0157 - acc: 0.9976\n",
      "Epoch 00103: val_loss did not improve from 1.33706\n",
      "36805/36805 [==============================] - 13s 360us/sample - loss: 0.0158 - acc: 0.9976 - val_loss: 2.5450 - val_acc: 0.5961\n",
      "Epoch 104/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0156 - acc: 0.9979\n",
      "Epoch 00104: val_loss did not improve from 1.33706\n",
      "36805/36805 [==============================] - 13s 363us/sample - loss: 0.0156 - acc: 0.9979 - val_loss: 2.5425 - val_acc: 0.6010\n",
      "\n",
      "1D_CNN_only_conv_conv_5_BN_3_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 1s 265us/sample - loss: 1.4184 - acc: 0.5705\n",
      "Loss: 1.4183883005956253 Accuracy: 0.57050884\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.9357 - acc: 0.4126\n",
      "Epoch 00001: val_loss improved from inf to 1.57671, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_BN_4_conv_checkpoint/001-1.5767.hdf5\n",
      "36805/36805 [==============================] - 20s 545us/sample - loss: 1.9337 - acc: 0.4132 - val_loss: 1.5767 - val_acc: 0.4978\n",
      "Epoch 2/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3166 - acc: 0.5986\n",
      "Epoch 00002: val_loss improved from 1.57671 to 1.27661, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_BN_4_conv_checkpoint/002-1.2766.hdf5\n",
      "36805/36805 [==============================] - 15s 415us/sample - loss: 1.3165 - acc: 0.5988 - val_loss: 1.2766 - val_acc: 0.6166\n",
      "Epoch 3/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0822 - acc: 0.6713\n",
      "Epoch 00003: val_loss improved from 1.27661 to 1.14669, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_BN_4_conv_checkpoint/003-1.1467.hdf5\n",
      "36805/36805 [==============================] - 15s 417us/sample - loss: 1.0821 - acc: 0.6712 - val_loss: 1.1467 - val_acc: 0.6546\n",
      "Epoch 4/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9355 - acc: 0.7143\n",
      "Epoch 00004: val_loss improved from 1.14669 to 1.14289, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_BN_4_conv_checkpoint/004-1.1429.hdf5\n",
      "36805/36805 [==============================] - 15s 410us/sample - loss: 0.9347 - acc: 0.7145 - val_loss: 1.1429 - val_acc: 0.6608\n",
      "Epoch 5/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.8110 - acc: 0.7566\n",
      "Epoch 00005: val_loss did not improve from 1.14289\n",
      "36805/36805 [==============================] - 15s 415us/sample - loss: 0.8110 - acc: 0.7567 - val_loss: 1.1483 - val_acc: 0.6655\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7191 - acc: 0.7863\n",
      "Epoch 00006: val_loss improved from 1.14289 to 1.07866, saving model to model/checkpoint/1D_CNN_only_conv_conv_5_BN_4_conv_checkpoint/006-1.0787.hdf5\n",
      "36805/36805 [==============================] - 15s 409us/sample - loss: 0.7191 - acc: 0.7862 - val_loss: 1.0787 - val_acc: 0.6760\n",
      "Epoch 7/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.6241 - acc: 0.8150\n",
      "Epoch 00007: val_loss did not improve from 1.07866\n",
      "36805/36805 [==============================] - 15s 415us/sample - loss: 0.6238 - acc: 0.8151 - val_loss: 1.1215 - val_acc: 0.6718\n",
      "Epoch 8/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5492 - acc: 0.8408\n",
      "Epoch 00008: val_loss did not improve from 1.07866\n",
      "36805/36805 [==============================] - 15s 410us/sample - loss: 0.5493 - acc: 0.8407 - val_loss: 1.1477 - val_acc: 0.6688\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4884 - acc: 0.8591\n",
      "Epoch 00009: val_loss did not improve from 1.07866\n",
      "36805/36805 [==============================] - 15s 415us/sample - loss: 0.4884 - acc: 0.8591 - val_loss: 1.1297 - val_acc: 0.6869\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4255 - acc: 0.8798\n",
      "Epoch 00010: val_loss did not improve from 1.07866\n",
      "36805/36805 [==============================] - 15s 415us/sample - loss: 0.4255 - acc: 0.8798 - val_loss: 1.1964 - val_acc: 0.6730\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3721 - acc: 0.9002\n",
      "Epoch 00011: val_loss did not improve from 1.07866\n",
      "36805/36805 [==============================] - 15s 416us/sample - loss: 0.3721 - acc: 0.9002 - val_loss: 1.1904 - val_acc: 0.6653\n",
      "Epoch 12/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3250 - acc: 0.9156\n",
      "Epoch 00012: val_loss did not improve from 1.07866\n",
      "36805/36805 [==============================] - 15s 413us/sample - loss: 0.3252 - acc: 0.9156 - val_loss: 1.1419 - val_acc: 0.6893\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2911 - acc: 0.9258\n",
      "Epoch 00013: val_loss did not improve from 1.07866\n",
      "36805/36805 [==============================] - 15s 413us/sample - loss: 0.2912 - acc: 0.9257 - val_loss: 1.1149 - val_acc: 0.6997\n",
      "Epoch 14/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2540 - acc: 0.9380\n",
      "Epoch 00014: val_loss did not improve from 1.07866\n",
      "36805/36805 [==============================] - 15s 415us/sample - loss: 0.2542 - acc: 0.9380 - val_loss: 1.1751 - val_acc: 0.6893\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2296 - acc: 0.9464\n",
      "Epoch 00015: val_loss did not improve from 1.07866\n",
      "36805/36805 [==============================] - 15s 412us/sample - loss: 0.2296 - acc: 0.9464 - val_loss: 1.1933 - val_acc: 0.6932\n",
      "Epoch 16/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2023 - acc: 0.9548\n",
      "Epoch 00016: val_loss did not improve from 1.07866\n",
      "36805/36805 [==============================] - 15s 411us/sample - loss: 0.2029 - acc: 0.9546 - val_loss: 1.1625 - val_acc: 0.6932\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1883 - acc: 0.9598\n",
      "Epoch 00017: val_loss did not improve from 1.07866\n",
      "36805/36805 [==============================] - 15s 415us/sample - loss: 0.1886 - acc: 0.9597 - val_loss: 1.1975 - val_acc: 0.6895\n",
      "Epoch 18/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1654 - acc: 0.9666\n",
      "Epoch 00018: val_loss did not improve from 1.07866\n",
      "36805/36805 [==============================] - 15s 409us/sample - loss: 0.1655 - acc: 0.9666 - val_loss: 1.1915 - val_acc: 0.6932\n",
      "Epoch 19/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1473 - acc: 0.9715\n",
      "Epoch 00019: val_loss did not improve from 1.07866\n",
      "36805/36805 [==============================] - 15s 413us/sample - loss: 0.1473 - acc: 0.9716 - val_loss: 1.1659 - val_acc: 0.7063\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1348 - acc: 0.9753\n",
      "Epoch 00020: val_loss did not improve from 1.07866\n",
      "36805/36805 [==============================] - 15s 414us/sample - loss: 0.1349 - acc: 0.9753 - val_loss: 1.2608 - val_acc: 0.6939\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1218 - acc: 0.9786\n",
      "Epoch 00021: val_loss did not improve from 1.07866\n",
      "36805/36805 [==============================] - 15s 412us/sample - loss: 0.1219 - acc: 0.9786 - val_loss: 1.2967 - val_acc: 0.6900\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1146 - acc: 0.9798\n",
      "Epoch 00022: val_loss did not improve from 1.07866\n",
      "36805/36805 [==============================] - 15s 413us/sample - loss: 0.1147 - acc: 0.9797 - val_loss: 1.4197 - val_acc: 0.6604\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1075 - acc: 0.9814\n",
      "Epoch 00023: val_loss did not improve from 1.07866\n",
      "36805/36805 [==============================] - 15s 409us/sample - loss: 0.1075 - acc: 0.9814 - val_loss: 1.3541 - val_acc: 0.6844\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0950 - acc: 0.9854\n",
      "Epoch 00024: val_loss did not improve from 1.07866\n",
      "36805/36805 [==============================] - 15s 416us/sample - loss: 0.0951 - acc: 0.9854 - val_loss: 1.3128 - val_acc: 0.6897\n",
      "Epoch 25/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0878 - acc: 0.9868\n",
      "Epoch 00025: val_loss did not improve from 1.07866\n",
      "36805/36805 [==============================] - 15s 412us/sample - loss: 0.0877 - acc: 0.9867 - val_loss: 1.4444 - val_acc: 0.6590\n",
      "Epoch 26/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0805 - acc: 0.9888\n",
      "Epoch 00026: val_loss did not improve from 1.07866\n",
      "36805/36805 [==============================] - 15s 410us/sample - loss: 0.0810 - acc: 0.9887 - val_loss: 1.4071 - val_acc: 0.6834\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0776 - acc: 0.9890\n",
      "Epoch 00027: val_loss did not improve from 1.07866\n",
      "36805/36805 [==============================] - 15s 410us/sample - loss: 0.0776 - acc: 0.9890 - val_loss: 1.4599 - val_acc: 0.6678\n",
      "Epoch 28/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0739 - acc: 0.9891\n",
      "Epoch 00028: val_loss did not improve from 1.07866\n",
      "36805/36805 [==============================] - 15s 412us/sample - loss: 0.0739 - acc: 0.9890 - val_loss: 1.6367 - val_acc: 0.6587\n",
      "Epoch 29/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0717 - acc: 0.9891\n",
      "Epoch 00029: val_loss did not improve from 1.07866\n",
      "36805/36805 [==============================] - 15s 410us/sample - loss: 0.0718 - acc: 0.9891 - val_loss: 1.3650 - val_acc: 0.6893\n",
      "Epoch 30/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0676 - acc: 0.9907\n",
      "Epoch 00030: val_loss did not improve from 1.07866\n",
      "36805/36805 [==============================] - 15s 413us/sample - loss: 0.0677 - acc: 0.9907 - val_loss: 1.4215 - val_acc: 0.6876\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0624 - acc: 0.9911\n",
      "Epoch 00031: val_loss did not improve from 1.07866\n",
      "36805/36805 [==============================] - 15s 413us/sample - loss: 0.0626 - acc: 0.9910 - val_loss: 1.6389 - val_acc: 0.6499\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0598 - acc: 0.9914\n",
      "Epoch 00032: val_loss did not improve from 1.07866\n",
      "36805/36805 [==============================] - 15s 413us/sample - loss: 0.0598 - acc: 0.9914 - val_loss: 1.4151 - val_acc: 0.6869\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0529 - acc: 0.9931\n",
      "Epoch 00033: val_loss did not improve from 1.07866\n",
      "36805/36805 [==============================] - 15s 417us/sample - loss: 0.0534 - acc: 0.9930 - val_loss: 1.3963 - val_acc: 0.6993\n",
      "Epoch 34/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0568 - acc: 0.9917\n",
      "Epoch 00034: val_loss did not improve from 1.07866\n",
      "36805/36805 [==============================] - 15s 411us/sample - loss: 0.0570 - acc: 0.9917 - val_loss: 1.6998 - val_acc: 0.6362\n",
      "Epoch 35/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0572 - acc: 0.9911\n",
      "Epoch 00035: val_loss did not improve from 1.07866\n",
      "36805/36805 [==============================] - 15s 411us/sample - loss: 0.0573 - acc: 0.9911 - val_loss: 1.5669 - val_acc: 0.6783\n",
      "Epoch 36/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0517 - acc: 0.9929\n",
      "Epoch 00036: val_loss did not improve from 1.07866\n",
      "36805/36805 [==============================] - 15s 409us/sample - loss: 0.0523 - acc: 0.9928 - val_loss: 1.4740 - val_acc: 0.6869\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0515 - acc: 0.9922\n",
      "Epoch 00037: val_loss did not improve from 1.07866\n",
      "36805/36805 [==============================] - 15s 411us/sample - loss: 0.0516 - acc: 0.9921 - val_loss: 1.5049 - val_acc: 0.6781\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0441 - acc: 0.9947\n",
      "Epoch 00038: val_loss did not improve from 1.07866\n",
      "36805/36805 [==============================] - 15s 411us/sample - loss: 0.0441 - acc: 0.9946 - val_loss: 1.5490 - val_acc: 0.6809\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0433 - acc: 0.9941\n",
      "Epoch 00039: val_loss did not improve from 1.07866\n",
      "36805/36805 [==============================] - 15s 416us/sample - loss: 0.0436 - acc: 0.9940 - val_loss: 1.6624 - val_acc: 0.6711\n",
      "Epoch 40/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0507 - acc: 0.9922\n",
      "Epoch 00040: val_loss did not improve from 1.07866\n",
      "36805/36805 [==============================] - 15s 415us/sample - loss: 0.0507 - acc: 0.9923 - val_loss: 1.4862 - val_acc: 0.6909\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0386 - acc: 0.9955\n",
      "Epoch 00041: val_loss did not improve from 1.07866\n",
      "36805/36805 [==============================] - 15s 405us/sample - loss: 0.0386 - acc: 0.9955 - val_loss: 1.6294 - val_acc: 0.6601\n",
      "Epoch 42/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0377 - acc: 0.9955\n",
      "Epoch 00042: val_loss did not improve from 1.07866\n",
      "36805/36805 [==============================] - 15s 411us/sample - loss: 0.0380 - acc: 0.9954 - val_loss: 1.6154 - val_acc: 0.6739\n",
      "Epoch 43/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0391 - acc: 0.9950\n",
      "Epoch 00043: val_loss did not improve from 1.07866\n",
      "36805/36805 [==============================] - 15s 412us/sample - loss: 0.0391 - acc: 0.9950 - val_loss: 1.5734 - val_acc: 0.6799\n",
      "Epoch 44/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0363 - acc: 0.9958\n",
      "Epoch 00044: val_loss did not improve from 1.07866\n",
      "36805/36805 [==============================] - 15s 410us/sample - loss: 0.0363 - acc: 0.9958 - val_loss: 1.7395 - val_acc: 0.6487\n",
      "Epoch 45/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0365 - acc: 0.9953\n",
      "Epoch 00045: val_loss did not improve from 1.07866\n",
      "36805/36805 [==============================] - 15s 417us/sample - loss: 0.0366 - acc: 0.9953 - val_loss: 1.7252 - val_acc: 0.6580\n",
      "Epoch 46/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0378 - acc: 0.9951\n",
      "Epoch 00046: val_loss did not improve from 1.07866\n",
      "36805/36805 [==============================] - 15s 413us/sample - loss: 0.0378 - acc: 0.9951 - val_loss: 1.5387 - val_acc: 0.6806\n",
      "Epoch 47/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0309 - acc: 0.9966\n",
      "Epoch 00047: val_loss did not improve from 1.07866\n",
      "36805/36805 [==============================] - 15s 411us/sample - loss: 0.0319 - acc: 0.9965 - val_loss: 1.5839 - val_acc: 0.6837\n",
      "Epoch 48/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0401 - acc: 0.9941\n",
      "Epoch 00048: val_loss did not improve from 1.07866\n",
      "36805/36805 [==============================] - 15s 411us/sample - loss: 0.0401 - acc: 0.9941 - val_loss: 1.6631 - val_acc: 0.6753\n",
      "Epoch 49/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0333 - acc: 0.9960\n",
      "Epoch 00049: val_loss did not improve from 1.07866\n",
      "36805/36805 [==============================] - 15s 414us/sample - loss: 0.0334 - acc: 0.9959 - val_loss: 1.6305 - val_acc: 0.6809\n",
      "Epoch 50/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0344 - acc: 0.9952\n",
      "Epoch 00050: val_loss did not improve from 1.07866\n",
      "36805/36805 [==============================] - 15s 410us/sample - loss: 0.0345 - acc: 0.9952 - val_loss: 1.9549 - val_acc: 0.6466\n",
      "Epoch 51/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0332 - acc: 0.9954\n",
      "Epoch 00051: val_loss did not improve from 1.07866\n",
      "36805/36805 [==============================] - 15s 413us/sample - loss: 0.0335 - acc: 0.9954 - val_loss: 1.6405 - val_acc: 0.6744\n",
      "Epoch 52/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0344 - acc: 0.9951\n",
      "Epoch 00052: val_loss did not improve from 1.07866\n",
      "36805/36805 [==============================] - 15s 413us/sample - loss: 0.0344 - acc: 0.9951 - val_loss: 1.6651 - val_acc: 0.6774\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0295 - acc: 0.9965\n",
      "Epoch 00053: val_loss did not improve from 1.07866\n",
      "36805/36805 [==============================] - 15s 413us/sample - loss: 0.0296 - acc: 0.9965 - val_loss: 1.6347 - val_acc: 0.6879\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0308 - acc: 0.9957\n",
      "Epoch 00054: val_loss did not improve from 1.07866\n",
      "36805/36805 [==============================] - 15s 416us/sample - loss: 0.0309 - acc: 0.9957 - val_loss: 1.7959 - val_acc: 0.6597\n",
      "Epoch 55/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0291 - acc: 0.9960\n",
      "Epoch 00055: val_loss did not improve from 1.07866\n",
      "36805/36805 [==============================] - 15s 409us/sample - loss: 0.0290 - acc: 0.9960 - val_loss: 1.7134 - val_acc: 0.6713\n",
      "Epoch 56/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0284 - acc: 0.9968\n",
      "Epoch 00056: val_loss did not improve from 1.07866\n",
      "36805/36805 [==============================] - 15s 408us/sample - loss: 0.0286 - acc: 0.9968 - val_loss: 1.8779 - val_acc: 0.6604\n",
      "Epoch 57/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0343 - acc: 0.9942\n",
      "Epoch 00057: val_loss did not improve from 1.07866\n",
      "36805/36805 [==============================] - 15s 412us/sample - loss: 0.0344 - acc: 0.9942 - val_loss: 1.7253 - val_acc: 0.6732\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0311 - acc: 0.9953\n",
      "Epoch 00058: val_loss did not improve from 1.07866\n",
      "36805/36805 [==============================] - 15s 409us/sample - loss: 0.0311 - acc: 0.9953 - val_loss: 2.0466 - val_acc: 0.6366\n",
      "Epoch 59/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0255 - acc: 0.9970\n",
      "Epoch 00059: val_loss did not improve from 1.07866\n",
      "36805/36805 [==============================] - 15s 409us/sample - loss: 0.0255 - acc: 0.9970 - val_loss: 1.7276 - val_acc: 0.6895\n",
      "Epoch 60/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0268 - acc: 0.9964\n",
      "Epoch 00060: val_loss did not improve from 1.07866\n",
      "36805/36805 [==============================] - 15s 417us/sample - loss: 0.0268 - acc: 0.9963 - val_loss: 2.0544 - val_acc: 0.6355\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0257 - acc: 0.9966\n",
      "Epoch 00061: val_loss did not improve from 1.07866\n",
      "36805/36805 [==============================] - 15s 415us/sample - loss: 0.0257 - acc: 0.9966 - val_loss: 1.7327 - val_acc: 0.6806\n",
      "Epoch 62/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0233 - acc: 0.9974\n",
      "Epoch 00062: val_loss did not improve from 1.07866\n",
      "36805/36805 [==============================] - 15s 408us/sample - loss: 0.0236 - acc: 0.9974 - val_loss: 1.7104 - val_acc: 0.6783\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0273 - acc: 0.9957\n",
      "Epoch 00063: val_loss did not improve from 1.07866\n",
      "36805/36805 [==============================] - 15s 408us/sample - loss: 0.0274 - acc: 0.9957 - val_loss: 1.7912 - val_acc: 0.6639\n",
      "Epoch 64/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0259 - acc: 0.9965\n",
      "Epoch 00064: val_loss did not improve from 1.07866\n",
      "36805/36805 [==============================] - 15s 410us/sample - loss: 0.0263 - acc: 0.9965 - val_loss: 1.8968 - val_acc: 0.6532\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0254 - acc: 0.9967\n",
      "Epoch 00065: val_loss did not improve from 1.07866\n",
      "36805/36805 [==============================] - 15s 410us/sample - loss: 0.0255 - acc: 0.9967 - val_loss: 1.7507 - val_acc: 0.6760\n",
      "Epoch 66/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0276 - acc: 0.9959\n",
      "Epoch 00066: val_loss did not improve from 1.07866\n",
      "36805/36805 [==============================] - 15s 413us/sample - loss: 0.0276 - acc: 0.9959 - val_loss: 1.8545 - val_acc: 0.6587\n",
      "Epoch 67/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0220 - acc: 0.9974\n",
      "Epoch 00067: val_loss did not improve from 1.07866\n",
      "36805/36805 [==============================] - 15s 415us/sample - loss: 0.0223 - acc: 0.9974 - val_loss: 2.0098 - val_acc: 0.6560\n",
      "Epoch 68/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0316 - acc: 0.9946\n",
      "Epoch 00068: val_loss did not improve from 1.07866\n",
      "36805/36805 [==============================] - 15s 413us/sample - loss: 0.0316 - acc: 0.9946 - val_loss: 1.7476 - val_acc: 0.6767\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0239 - acc: 0.9967\n",
      "Epoch 00069: val_loss did not improve from 1.07866\n",
      "36805/36805 [==============================] - 15s 411us/sample - loss: 0.0240 - acc: 0.9966 - val_loss: 1.8064 - val_acc: 0.6795\n",
      "Epoch 70/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0264 - acc: 0.9958\n",
      "Epoch 00070: val_loss did not improve from 1.07866\n",
      "36805/36805 [==============================] - 15s 413us/sample - loss: 0.0266 - acc: 0.9958 - val_loss: 1.7784 - val_acc: 0.6816\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0241 - acc: 0.9967\n",
      "Epoch 00071: val_loss did not improve from 1.07866\n",
      "36805/36805 [==============================] - 15s 417us/sample - loss: 0.0242 - acc: 0.9966 - val_loss: 1.8549 - val_acc: 0.6667\n",
      "Epoch 72/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 0.0287 - acc: 0.9954\n",
      "Epoch 00072: val_loss did not improve from 1.07866\n",
      "36805/36805 [==============================] - 15s 413us/sample - loss: 0.0286 - acc: 0.9954 - val_loss: 1.9584 - val_acc: 0.6445\n",
      "Epoch 73/500\n",
      "28352/36805 [======================>.......] - ETA: 3s - loss: 0.0191 - acc: 0.9978"
     ]
    }
   ],
   "source": [
    "for i in range(1, 9):\n",
    "    base = '1D_CNN_only_conv_conv_5_BN'\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    model = build_1d_cnn_only_conv_conv_5_BN(conv_num=i)\n",
    "#         model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4),\n",
    "          metrics=['accuracy'])\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model_filename = model_path+'{epoch:03d}-{val_loss:.4f}.hdf5'\n",
    "    checkpointer = ModelCheckpoint(filepath = model_filename, monitor = \"val_loss\", \n",
    "                                   verbose=1, save_best_only=True)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=100)\n",
    "    hist = model.fit(x_train_abs, y_train_onehot, batch_size=64, epochs=500, \n",
    "                     validation_data=[x_val_abs, y_val_onehot], shuffle=True, \n",
    "                     callbacks = [checkpointer, early_stopping])\n",
    "\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "    ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "    ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "    ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    png_path = 'visualization/learning_curve/'\n",
    "    filename = model_name+'.png'\n",
    "    os.makedirs(png_path, exist_ok=True)\n",
    "    fig.savefig(png_path+filename, transparent=True)\n",
    "\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "    model = load_model(model_filename)\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "    print()\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = 'log'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "base = '1D_CNN_only_conv_conv_5_BN'\n",
    "\n",
    "with open(path.join(log_dir, base), 'w') as log_file:\n",
    "    for i in range(1, 9):\n",
    "        model_name = base+'_{}_conv'.format(i)\n",
    "        print()\n",
    "        print(model_name, 'Model')\n",
    "        model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "        model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "\n",
    "        model = load_model(model_filename)\n",
    "        model.summary()\n",
    "\n",
    "        [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "        print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "        del(model)\n",
    "\n",
    "        log_file.write('\\t'.join([model_name, accuracy, loss])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
