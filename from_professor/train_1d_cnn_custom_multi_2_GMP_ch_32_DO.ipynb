{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path as path\n",
    "import itertools\n",
    "from sklearn.preprocessing import maxabs_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Input, Dense, Activation, BatchNormalization, \\\n",
    "                                    Flatten, Conv1D, MaxPooling1D, Dropout, \\\n",
    "                                    Concatenate, GlobalMaxPool1D, GlobalAvgPool1D\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler, \\\n",
    "                                        EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = path.join('..', 'data')\n",
    "data_dir = path.join(base_dir, 'data_speech_commands_v0.02')\n",
    " \n",
    "train_txt = path.join(data_dir, 'wav_train_16words.txt')\n",
    "val_txt = path.join(data_dir, 'wav_validation_16words.txt')\n",
    "test_txt = path.join(data_dir, 'wav_test_16words.txt')\n",
    "\n",
    "train_data = np.load(path.join(data_dir, 'wav_train_data.npz'))\n",
    "val_data = np.load(path.join(data_dir, 'wav_validation_data.npz'))\n",
    "test_data = np.load(path.join(data_dir, 'wav_test_data.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36805, 16000, 1),\n",
       " (36805,),\n",
       " (4293, 16000, 1),\n",
       " (4293,),\n",
       " (4815, 16000, 1),\n",
       " (4815,),\n",
       " (16, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = train_data['x_train']\n",
    "y_train = train_data['y_train']\n",
    "x_val = val_data['x_val']\n",
    "y_val = val_data['y_val']\n",
    "x_test = test_data['x_test']\n",
    "y_test = test_data['y_test']\n",
    "y_table = test_data['table']\n",
    "\n",
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape, y_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_test[0].shape\n",
    "output_size = y_table.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_abs = np.asarray([maxabs_scale(wav) for wav in x_train])\n",
    "y_train_onehot = np.asarray([to_categorical(label, output_size) for label in y_train])\n",
    "del x_train, y_train\n",
    "\n",
    "x_val_abs = np.asarray([maxabs_scale(wav) for wav in x_val])\n",
    "y_val_onehot = np.asarray([to_categorical(label, output_size) for label in y_val])\n",
    "del x_val, y_val\n",
    "\n",
    "x_test_abs = np.asarray([maxabs_scale(wav) for wav in x_test])\n",
    "y_test_onehot = np.asarray([to_categorical(label, output_size) for label in y_test])\n",
    "del x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn(conv_num=1):\n",
    "    filter_size = 32\n",
    "\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    x = input_layer\n",
    "\n",
    "    layer_outputs = []\n",
    "    for i in range(conv_num):\n",
    "        x = Conv1D (kernel_size=5, filters=filter_size*(2**(i//4)), \n",
    "                          strides=1, padding='same')(x)\n",
    "        x = Activation('relu')(x)\n",
    "        x = MaxPooling1D(pool_size=3, strides=3)(x)\n",
    "        layer_outputs.append(x)    \n",
    "    \n",
    "    x = Concatenate()([GlobalMaxPool1D()(output) for output in layer_outputs[-2:]])\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(output_size, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 16000, 32)    192         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 16000, 32)    0           conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D)    (None, 5333, 32)     0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 5333, 32)     5152        max_pooling1d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 5333, 32)     0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 1777, 32)     0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 1777, 32)     5152        max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 1777, 32)     0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 592, 32)      0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d (GlobalMax (None, 32)           0           max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 32)           0           max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 64)           0           global_max_pooling1d[0][0]       \n",
      "                                                                 global_max_pooling1d_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 64)           0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 16)           1040        dropout[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 11,536\n",
      "Trainable params: 11,536\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 16000, 32)    192         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 16000, 32)    0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 5333, 32)     0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 5333, 32)     5152        max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 5333, 32)     0           conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 1777, 32)     0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 1777, 32)     5152        max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 1777, 32)     0           conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 592, 32)      0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 592, 32)      5152        max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 592, 32)      0           conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1D)  (None, 197, 32)      0           activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_2 (GlobalM (None, 32)           0           max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_3 (GlobalM (None, 32)           0           max_pooling1d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 64)           0           global_max_pooling1d_2[0][0]     \n",
      "                                                                 global_max_pooling1d_3[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 64)           0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 16)           1040        dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 16,688\n",
      "Trainable params: 16,688\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 16000, 32)    192         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 16000, 32)    0           conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1D)  (None, 5333, 32)     0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 5333, 32)     5152        max_pooling1d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 5333, 32)     0           conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1D)  (None, 1777, 32)     0           activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 1777, 32)     5152        max_pooling1d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 1777, 32)     0           conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1D)  (None, 592, 32)      0           activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 592, 32)      0           conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling1D) (None, 197, 32)      0           activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 197, 64)      0           conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling1D) (None, 65, 64)       0           activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_4 (GlobalM (None, 32)           0           max_pooling1d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_5 (GlobalM (None, 64)           0           max_pooling1d_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 96)           0           global_max_pooling1d_4[0][0]     \n",
      "                                                                 global_max_pooling1d_5[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 96)           0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 16)           1552        dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 27,504\n",
      "Trainable params: 27,504\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 16000, 32)    192         input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 16000, 32)    0           conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling1D) (None, 5333, 32)     0           activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 5333, 32)     0           conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling1D) (None, 1777, 32)     0           activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_13[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 1777, 32)     0           conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling1D) (None, 592, 32)      0           activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_14[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 592, 32)      0           conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling1D) (None, 197, 32)      0           activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_15[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 197, 64)      0           conv1d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling1D) (None, 65, 64)       0           activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_17 (Conv1D)              (None, 65, 64)       20544       max_pooling1d_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 65, 64)       0           conv1d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling1D) (None, 21, 64)       0           activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_6 (GlobalM (None, 64)           0           max_pooling1d_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_7 (GlobalM (None, 64)           0           max_pooling1d_17[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 128)          0           global_max_pooling1d_6[0][0]     \n",
      "                                                                 global_max_pooling1d_7[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 128)          0           concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 16)           2064        dropout_3[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 48,560\n",
      "Trainable params: 48,560\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_18 (Conv1D)              (None, 16000, 32)    192         input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 16000, 32)    0           conv1d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling1D) (None, 5333, 32)     0           activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_19 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_18[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 5333, 32)     0           conv1d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling1D) (None, 1777, 32)     0           activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_20 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_19[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 1777, 32)     0           conv1d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling1D) (None, 592, 32)      0           activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_21 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_20[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 592, 32)      0           conv1d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling1D) (None, 197, 32)      0           activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_22 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_21[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 197, 64)      0           conv1d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling1D) (None, 65, 64)       0           activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_23 (Conv1D)              (None, 65, 64)       20544       max_pooling1d_22[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 65, 64)       0           conv1d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling1D) (None, 21, 64)       0           activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_24 (Conv1D)              (None, 21, 64)       20544       max_pooling1d_23[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 21, 64)       0           conv1d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling1D) (None, 7, 64)        0           activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_8 (GlobalM (None, 64)           0           max_pooling1d_23[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_9 (GlobalM (None, 64)           0           max_pooling1d_24[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 128)          0           global_max_pooling1d_8[0][0]     \n",
      "                                                                 global_max_pooling1d_9[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 128)          0           concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 16)           2064        dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 69,104\n",
      "Trainable params: 69,104\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_25 (Conv1D)              (None, 16000, 32)    192         input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 16000, 32)    0           conv1d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling1D) (None, 5333, 32)     0           activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_26 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_25[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 5333, 32)     0           conv1d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_26 (MaxPooling1D) (None, 1777, 32)     0           activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_27 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_26[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 1777, 32)     0           conv1d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_27 (MaxPooling1D) (None, 592, 32)      0           activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_28 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_27[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 592, 32)      0           conv1d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_28 (MaxPooling1D) (None, 197, 32)      0           activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_29 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_28[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 197, 64)      0           conv1d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_29 (MaxPooling1D) (None, 65, 64)       0           activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_30 (Conv1D)              (None, 65, 64)       20544       max_pooling1d_29[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 65, 64)       0           conv1d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_30 (MaxPooling1D) (None, 21, 64)       0           activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_31 (Conv1D)              (None, 21, 64)       20544       max_pooling1d_30[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 21, 64)       0           conv1d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_31 (MaxPooling1D) (None, 7, 64)        0           activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_32 (Conv1D)              (None, 7, 64)        20544       max_pooling1d_31[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 7, 64)        0           conv1d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_32 (MaxPooling1D) (None, 2, 64)        0           activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_10 (Global (None, 64)           0           max_pooling1d_31[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_11 (Global (None, 64)           0           max_pooling1d_32[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 128)          0           global_max_pooling1d_10[0][0]    \n",
      "                                                                 global_max_pooling1d_11[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 128)          0           concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 16)           2064        dropout_5[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 89,648\n",
      "Trainable params: 89,648\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in range(3, 9):\n",
    "    model = build_cnn(conv_num=i)\n",
    "    model.summary()\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36805 samples, validate on 4293 samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7152 - acc: 0.1041\n",
      "Epoch 00001: val_loss improved from inf to 2.63212, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv_checkpoint/001-2.6321.hdf5\n",
      "36805/36805 [==============================] - 18s 500us/sample - loss: 2.7152 - acc: 0.1041 - val_loss: 2.6321 - val_acc: 0.1766\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.5896 - acc: 0.1563\n",
      "Epoch 00002: val_loss improved from 2.63212 to 2.44033, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv_checkpoint/002-2.4403.hdf5\n",
      "36805/36805 [==============================] - 16s 435us/sample - loss: 2.5896 - acc: 0.1563 - val_loss: 2.4403 - val_acc: 0.2553\n",
      "Epoch 3/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 2.4395 - acc: 0.1922\n",
      "Epoch 00003: val_loss improved from 2.44033 to 2.26473, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv_checkpoint/003-2.2647.hdf5\n",
      "36805/36805 [==============================] - 16s 432us/sample - loss: 2.4395 - acc: 0.1921 - val_loss: 2.2647 - val_acc: 0.3028\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.3286 - acc: 0.2187\n",
      "Epoch 00004: val_loss improved from 2.26473 to 2.14759, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv_checkpoint/004-2.1476.hdf5\n",
      "36805/36805 [==============================] - 16s 428us/sample - loss: 2.3285 - acc: 0.2187 - val_loss: 2.1476 - val_acc: 0.3196\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.2531 - acc: 0.2416\n",
      "Epoch 00005: val_loss improved from 2.14759 to 2.06402, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv_checkpoint/005-2.0640.hdf5\n",
      "36805/36805 [==============================] - 16s 428us/sample - loss: 2.2532 - acc: 0.2416 - val_loss: 2.0640 - val_acc: 0.3522\n",
      "Epoch 6/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 2.1957 - acc: 0.2568\n",
      "Epoch 00006: val_loss improved from 2.06402 to 1.99113, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv_checkpoint/006-1.9911.hdf5\n",
      "36805/36805 [==============================] - 16s 429us/sample - loss: 2.1958 - acc: 0.2569 - val_loss: 1.9911 - val_acc: 0.3736\n",
      "Epoch 7/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 2.1396 - acc: 0.2757\n",
      "Epoch 00007: val_loss improved from 1.99113 to 1.92411, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv_checkpoint/007-1.9241.hdf5\n",
      "36805/36805 [==============================] - 16s 428us/sample - loss: 2.1397 - acc: 0.2756 - val_loss: 1.9241 - val_acc: 0.3990\n",
      "Epoch 8/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 2.0864 - acc: 0.2902\n",
      "Epoch 00008: val_loss improved from 1.92411 to 1.86315, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv_checkpoint/008-1.8631.hdf5\n",
      "36805/36805 [==============================] - 16s 429us/sample - loss: 2.0862 - acc: 0.2902 - val_loss: 1.8631 - val_acc: 0.4191\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.0406 - acc: 0.3082\n",
      "Epoch 00009: val_loss improved from 1.86315 to 1.80749, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv_checkpoint/009-1.8075.hdf5\n",
      "36805/36805 [==============================] - 16s 425us/sample - loss: 2.0406 - acc: 0.3082 - val_loss: 1.8075 - val_acc: 0.4368\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9987 - acc: 0.3202\n",
      "Epoch 00010: val_loss improved from 1.80749 to 1.75444, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv_checkpoint/010-1.7544.hdf5\n",
      "36805/36805 [==============================] - 16s 432us/sample - loss: 1.9987 - acc: 0.3201 - val_loss: 1.7544 - val_acc: 0.4528\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9639 - acc: 0.3349\n",
      "Epoch 00011: val_loss improved from 1.75444 to 1.70462, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv_checkpoint/011-1.7046.hdf5\n",
      "36805/36805 [==============================] - 16s 425us/sample - loss: 1.9639 - acc: 0.3350 - val_loss: 1.7046 - val_acc: 0.4729\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9210 - acc: 0.3502\n",
      "Epoch 00012: val_loss improved from 1.70462 to 1.66320, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv_checkpoint/012-1.6632.hdf5\n",
      "36805/36805 [==============================] - 16s 424us/sample - loss: 1.9210 - acc: 0.3501 - val_loss: 1.6632 - val_acc: 0.4906\n",
      "Epoch 13/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.8898 - acc: 0.3659\n",
      "Epoch 00013: val_loss improved from 1.66320 to 1.62219, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv_checkpoint/013-1.6222.hdf5\n",
      "36805/36805 [==============================] - 16s 427us/sample - loss: 1.8897 - acc: 0.3658 - val_loss: 1.6222 - val_acc: 0.5064\n",
      "Epoch 14/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.8588 - acc: 0.3742\n",
      "Epoch 00014: val_loss improved from 1.62219 to 1.58382, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv_checkpoint/014-1.5838.hdf5\n",
      "36805/36805 [==============================] - 16s 422us/sample - loss: 1.8584 - acc: 0.3743 - val_loss: 1.5838 - val_acc: 0.5195\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.8306 - acc: 0.3838\n",
      "Epoch 00015: val_loss improved from 1.58382 to 1.55465, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv_checkpoint/015-1.5546.hdf5\n",
      "36805/36805 [==============================] - 15s 420us/sample - loss: 1.8306 - acc: 0.3838 - val_loss: 1.5546 - val_acc: 0.5274\n",
      "Epoch 16/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.8094 - acc: 0.3894\n",
      "Epoch 00016: val_loss improved from 1.55465 to 1.52302, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv_checkpoint/016-1.5230.hdf5\n",
      "36805/36805 [==============================] - 16s 429us/sample - loss: 1.8093 - acc: 0.3894 - val_loss: 1.5230 - val_acc: 0.5390\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.7777 - acc: 0.4054\n",
      "Epoch 00017: val_loss improved from 1.52302 to 1.49453, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv_checkpoint/017-1.4945.hdf5\n",
      "36805/36805 [==============================] - 16s 425us/sample - loss: 1.7776 - acc: 0.4054 - val_loss: 1.4945 - val_acc: 0.5462\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.7579 - acc: 0.4112\n",
      "Epoch 00018: val_loss improved from 1.49453 to 1.47103, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv_checkpoint/018-1.4710.hdf5\n",
      "36805/36805 [==============================] - 16s 424us/sample - loss: 1.7579 - acc: 0.4112 - val_loss: 1.4710 - val_acc: 0.5539\n",
      "Epoch 19/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.7448 - acc: 0.4168\n",
      "Epoch 00019: val_loss improved from 1.47103 to 1.44250, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv_checkpoint/019-1.4425.hdf5\n",
      "36805/36805 [==============================] - 16s 425us/sample - loss: 1.7451 - acc: 0.4168 - val_loss: 1.4425 - val_acc: 0.5630\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.7231 - acc: 0.4264\n",
      "Epoch 00020: val_loss improved from 1.44250 to 1.42107, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv_checkpoint/020-1.4211.hdf5\n",
      "36805/36805 [==============================] - 16s 430us/sample - loss: 1.7230 - acc: 0.4264 - val_loss: 1.4211 - val_acc: 0.5712\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.6950 - acc: 0.4348\n",
      "Epoch 00021: val_loss improved from 1.42107 to 1.39757, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv_checkpoint/021-1.3976.hdf5\n",
      "36805/36805 [==============================] - 16s 426us/sample - loss: 1.6949 - acc: 0.4350 - val_loss: 1.3976 - val_acc: 0.5775\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6833 - acc: 0.4412\n",
      "Epoch 00022: val_loss improved from 1.39757 to 1.38363, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv_checkpoint/022-1.3836.hdf5\n",
      "36805/36805 [==============================] - 16s 426us/sample - loss: 1.6833 - acc: 0.4412 - val_loss: 1.3836 - val_acc: 0.5886\n",
      "Epoch 23/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.6651 - acc: 0.4441\n",
      "Epoch 00023: val_loss improved from 1.38363 to 1.36035, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv_checkpoint/023-1.3604.hdf5\n",
      "36805/36805 [==============================] - 16s 426us/sample - loss: 1.6657 - acc: 0.4441 - val_loss: 1.3604 - val_acc: 0.5851\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6577 - acc: 0.4480\n",
      "Epoch 00024: val_loss improved from 1.36035 to 1.34474, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv_checkpoint/024-1.3447.hdf5\n",
      "36805/36805 [==============================] - 16s 427us/sample - loss: 1.6578 - acc: 0.4480 - val_loss: 1.3447 - val_acc: 0.5900\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6355 - acc: 0.4542\n",
      "Epoch 00025: val_loss improved from 1.34474 to 1.32687, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv_checkpoint/025-1.3269.hdf5\n",
      "36805/36805 [==============================] - 16s 425us/sample - loss: 1.6356 - acc: 0.4542 - val_loss: 1.3269 - val_acc: 0.5938\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6251 - acc: 0.4610\n",
      "Epoch 00026: val_loss improved from 1.32687 to 1.31108, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv_checkpoint/026-1.3111.hdf5\n",
      "36805/36805 [==============================] - 16s 427us/sample - loss: 1.6249 - acc: 0.4611 - val_loss: 1.3111 - val_acc: 0.6014\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6143 - acc: 0.4627\n",
      "Epoch 00027: val_loss improved from 1.31108 to 1.30312, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv_checkpoint/027-1.3031.hdf5\n",
      "36805/36805 [==============================] - 16s 424us/sample - loss: 1.6142 - acc: 0.4628 - val_loss: 1.3031 - val_acc: 0.6096\n",
      "Epoch 28/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.5950 - acc: 0.4711\n",
      "Epoch 00028: val_loss improved from 1.30312 to 1.28449, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv_checkpoint/028-1.2845.hdf5\n",
      "36805/36805 [==============================] - 16s 427us/sample - loss: 1.5950 - acc: 0.4711 - val_loss: 1.2845 - val_acc: 0.6108\n",
      "Epoch 29/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.5872 - acc: 0.4695\n",
      "Epoch 00029: val_loss did not improve from 1.28449\n",
      "36805/36805 [==============================] - 16s 427us/sample - loss: 1.5871 - acc: 0.4695 - val_loss: 1.2872 - val_acc: 0.6126\n",
      "Epoch 30/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.5726 - acc: 0.4788\n",
      "Epoch 00030: val_loss improved from 1.28449 to 1.25876, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv_checkpoint/030-1.2588.hdf5\n",
      "36805/36805 [==============================] - 16s 426us/sample - loss: 1.5726 - acc: 0.4789 - val_loss: 1.2588 - val_acc: 0.6147\n",
      "Epoch 31/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.5672 - acc: 0.4792\n",
      "Epoch 00031: val_loss improved from 1.25876 to 1.24609, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv_checkpoint/031-1.2461.hdf5\n",
      "36805/36805 [==============================] - 16s 425us/sample - loss: 1.5672 - acc: 0.4791 - val_loss: 1.2461 - val_acc: 0.6222\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5630 - acc: 0.4843\n",
      "Epoch 00032: val_loss improved from 1.24609 to 1.23624, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv_checkpoint/032-1.2362.hdf5\n",
      "36805/36805 [==============================] - 16s 425us/sample - loss: 1.5630 - acc: 0.4843 - val_loss: 1.2362 - val_acc: 0.6247\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5480 - acc: 0.4868\n",
      "Epoch 00033: val_loss improved from 1.23624 to 1.22555, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv_checkpoint/033-1.2256.hdf5\n",
      "36805/36805 [==============================] - 16s 424us/sample - loss: 1.5480 - acc: 0.4869 - val_loss: 1.2256 - val_acc: 0.6313\n",
      "Epoch 34/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.5378 - acc: 0.4910\n",
      "Epoch 00034: val_loss improved from 1.22555 to 1.20899, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv_checkpoint/034-1.2090.hdf5\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 1.5379 - acc: 0.4910 - val_loss: 1.2090 - val_acc: 0.6329\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5349 - acc: 0.4929\n",
      "Epoch 00035: val_loss improved from 1.20899 to 1.20799, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv_checkpoint/035-1.2080.hdf5\n",
      "36805/36805 [==============================] - 16s 426us/sample - loss: 1.5348 - acc: 0.4929 - val_loss: 1.2080 - val_acc: 0.6403\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5176 - acc: 0.4990\n",
      "Epoch 00036: val_loss improved from 1.20799 to 1.19920, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv_checkpoint/036-1.1992.hdf5\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 1.5177 - acc: 0.4990 - val_loss: 1.1992 - val_acc: 0.6431\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5191 - acc: 0.4992\n",
      "Epoch 00037: val_loss improved from 1.19920 to 1.19421, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv_checkpoint/037-1.1942.hdf5\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 1.5192 - acc: 0.4992 - val_loss: 1.1942 - val_acc: 0.6445\n",
      "Epoch 38/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.5057 - acc: 0.5029\n",
      "Epoch 00038: val_loss improved from 1.19421 to 1.18191, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv_checkpoint/038-1.1819.hdf5\n",
      "36805/36805 [==============================] - 16s 424us/sample - loss: 1.5052 - acc: 0.5030 - val_loss: 1.1819 - val_acc: 0.6483\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5020 - acc: 0.5048\n",
      "Epoch 00039: val_loss improved from 1.18191 to 1.16738, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv_checkpoint/039-1.1674.hdf5\n",
      "36805/36805 [==============================] - 16s 425us/sample - loss: 1.5021 - acc: 0.5047 - val_loss: 1.1674 - val_acc: 0.6483\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4918 - acc: 0.5080\n",
      "Epoch 00040: val_loss improved from 1.16738 to 1.15358, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv_checkpoint/040-1.1536.hdf5\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 1.4918 - acc: 0.5080 - val_loss: 1.1536 - val_acc: 0.6550\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4886 - acc: 0.5071\n",
      "Epoch 00041: val_loss did not improve from 1.15358\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 1.4887 - acc: 0.5071 - val_loss: 1.1617 - val_acc: 0.6527\n",
      "Epoch 42/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.4856 - acc: 0.5101\n",
      "Epoch 00042: val_loss improved from 1.15358 to 1.15126, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv_checkpoint/042-1.1513.hdf5\n",
      "36805/36805 [==============================] - 16s 422us/sample - loss: 1.4852 - acc: 0.5102 - val_loss: 1.1513 - val_acc: 0.6564\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.4804 - acc: 0.5103\n",
      "Epoch 00043: val_loss did not improve from 1.15126\n",
      "36805/36805 [==============================] - 16s 427us/sample - loss: 1.4800 - acc: 0.5103 - val_loss: 1.1553 - val_acc: 0.6564\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4731 - acc: 0.5151\n",
      "Epoch 00044: val_loss improved from 1.15126 to 1.14545, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv_checkpoint/044-1.1455.hdf5\n",
      "36805/36805 [==============================] - 16s 425us/sample - loss: 1.4731 - acc: 0.5150 - val_loss: 1.1455 - val_acc: 0.6583\n",
      "Epoch 45/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.4636 - acc: 0.5181\n",
      "Epoch 00045: val_loss improved from 1.14545 to 1.13203, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv_checkpoint/045-1.1320.hdf5\n",
      "36805/36805 [==============================] - 16s 425us/sample - loss: 1.4635 - acc: 0.5179 - val_loss: 1.1320 - val_acc: 0.6636\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4568 - acc: 0.5199\n",
      "Epoch 00046: val_loss improved from 1.13203 to 1.12589, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv_checkpoint/046-1.1259.hdf5\n",
      "36805/36805 [==============================] - 16s 426us/sample - loss: 1.4568 - acc: 0.5199 - val_loss: 1.1259 - val_acc: 0.6650\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4514 - acc: 0.5251\n",
      "Epoch 00047: val_loss did not improve from 1.12589\n",
      "36805/36805 [==============================] - 16s 424us/sample - loss: 1.4514 - acc: 0.5250 - val_loss: 1.1263 - val_acc: 0.6634\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4441 - acc: 0.5242\n",
      "Epoch 00048: val_loss improved from 1.12589 to 1.12223, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv_checkpoint/048-1.1222.hdf5\n",
      "36805/36805 [==============================] - 16s 427us/sample - loss: 1.4442 - acc: 0.5242 - val_loss: 1.1222 - val_acc: 0.6653\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4439 - acc: 0.5257\n",
      "Epoch 00049: val_loss improved from 1.12223 to 1.10402, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv_checkpoint/049-1.1040.hdf5\n",
      "36805/36805 [==============================] - 16s 424us/sample - loss: 1.4438 - acc: 0.5258 - val_loss: 1.1040 - val_acc: 0.6709\n",
      "Epoch 50/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.4383 - acc: 0.5274\n",
      "Epoch 00050: val_loss did not improve from 1.10402\n",
      "36805/36805 [==============================] - 16s 425us/sample - loss: 1.4384 - acc: 0.5275 - val_loss: 1.1096 - val_acc: 0.6725\n",
      "Epoch 51/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.4404 - acc: 0.5236\n",
      "Epoch 00051: val_loss improved from 1.10402 to 1.09909, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv_checkpoint/051-1.0991.hdf5\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 1.4401 - acc: 0.5237 - val_loss: 1.0991 - val_acc: 0.6702\n",
      "Epoch 52/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.4268 - acc: 0.5327\n",
      "Epoch 00052: val_loss improved from 1.09909 to 1.09062, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv_checkpoint/052-1.0906.hdf5\n",
      "36805/36805 [==============================] - 16s 424us/sample - loss: 1.4272 - acc: 0.5328 - val_loss: 1.0906 - val_acc: 0.6774\n",
      "Epoch 53/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.4255 - acc: 0.5305\n",
      "Epoch 00053: val_loss improved from 1.09062 to 1.08766, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv_checkpoint/053-1.0877.hdf5\n",
      "36805/36805 [==============================] - 16s 428us/sample - loss: 1.4251 - acc: 0.5306 - val_loss: 1.0877 - val_acc: 0.6760\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4184 - acc: 0.5333\n",
      "Epoch 00054: val_loss improved from 1.08766 to 1.08472, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv_checkpoint/054-1.0847.hdf5\n",
      "36805/36805 [==============================] - 16s 422us/sample - loss: 1.4184 - acc: 0.5334 - val_loss: 1.0847 - val_acc: 0.6820\n",
      "Epoch 55/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.4194 - acc: 0.5376\n",
      "Epoch 00055: val_loss improved from 1.08472 to 1.08249, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv_checkpoint/055-1.0825.hdf5\n",
      "36805/36805 [==============================] - 16s 424us/sample - loss: 1.4193 - acc: 0.5375 - val_loss: 1.0825 - val_acc: 0.6804\n",
      "Epoch 56/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.4135 - acc: 0.5318\n",
      "Epoch 00056: val_loss improved from 1.08249 to 1.07718, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv_checkpoint/056-1.0772.hdf5\n",
      "36805/36805 [==============================] - 16s 426us/sample - loss: 1.4140 - acc: 0.5315 - val_loss: 1.0772 - val_acc: 0.6809\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4120 - acc: 0.5368\n",
      "Epoch 00057: val_loss did not improve from 1.07718\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 1.4120 - acc: 0.5368 - val_loss: 1.0929 - val_acc: 0.6774\n",
      "Epoch 58/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3995 - acc: 0.5416\n",
      "Epoch 00058: val_loss improved from 1.07718 to 1.06194, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv_checkpoint/058-1.0619.hdf5\n",
      "36805/36805 [==============================] - 16s 425us/sample - loss: 1.3994 - acc: 0.5417 - val_loss: 1.0619 - val_acc: 0.6804\n",
      "Epoch 59/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3973 - acc: 0.5439\n",
      "Epoch 00059: val_loss improved from 1.06194 to 1.05855, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv_checkpoint/059-1.0586.hdf5\n",
      "36805/36805 [==============================] - 16s 425us/sample - loss: 1.3975 - acc: 0.5438 - val_loss: 1.0586 - val_acc: 0.6837\n",
      "Epoch 60/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.4025 - acc: 0.5435\n",
      "Epoch 00060: val_loss did not improve from 1.05855\n",
      "36805/36805 [==============================] - 16s 422us/sample - loss: 1.4027 - acc: 0.5433 - val_loss: 1.0616 - val_acc: 0.6846\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3940 - acc: 0.5413\n",
      "Epoch 00061: val_loss improved from 1.05855 to 1.05395, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv_checkpoint/061-1.0540.hdf5\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 1.3940 - acc: 0.5414 - val_loss: 1.0540 - val_acc: 0.6911\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3919 - acc: 0.5458\n",
      "Epoch 00062: val_loss improved from 1.05395 to 1.05372, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv_checkpoint/062-1.0537.hdf5\n",
      "36805/36805 [==============================] - 16s 422us/sample - loss: 1.3919 - acc: 0.5458 - val_loss: 1.0537 - val_acc: 0.6874\n",
      "Epoch 63/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3902 - acc: 0.5427\n",
      "Epoch 00063: val_loss improved from 1.05372 to 1.04871, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv_checkpoint/063-1.0487.hdf5\n",
      "36805/36805 [==============================] - 16s 430us/sample - loss: 1.3901 - acc: 0.5428 - val_loss: 1.0487 - val_acc: 0.6921\n",
      "Epoch 64/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3849 - acc: 0.5458\n",
      "Epoch 00064: val_loss did not improve from 1.04871\n",
      "36805/36805 [==============================] - 16s 424us/sample - loss: 1.3848 - acc: 0.5459 - val_loss: 1.0514 - val_acc: 0.6839\n",
      "Epoch 65/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3775 - acc: 0.5498\n",
      "Epoch 00065: val_loss improved from 1.04871 to 1.03476, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv_checkpoint/065-1.0348.hdf5\n",
      "36805/36805 [==============================] - 16s 425us/sample - loss: 1.3772 - acc: 0.5500 - val_loss: 1.0348 - val_acc: 0.6916\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3783 - acc: 0.5501\n",
      "Epoch 00066: val_loss did not improve from 1.03476\n",
      "36805/36805 [==============================] - 16s 424us/sample - loss: 1.3783 - acc: 0.5501 - val_loss: 1.0393 - val_acc: 0.6923\n",
      "Epoch 67/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3712 - acc: 0.5522\n",
      "Epoch 00067: val_loss improved from 1.03476 to 1.03305, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv_checkpoint/067-1.0330.hdf5\n",
      "36805/36805 [==============================] - 16s 422us/sample - loss: 1.3715 - acc: 0.5523 - val_loss: 1.0330 - val_acc: 0.6937\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3754 - acc: 0.5474\n",
      "Epoch 00068: val_loss improved from 1.03305 to 1.03110, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv_checkpoint/068-1.0311.hdf5\n",
      "36805/36805 [==============================] - 16s 424us/sample - loss: 1.3754 - acc: 0.5474 - val_loss: 1.0311 - val_acc: 0.6981\n",
      "Epoch 69/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.3783 - acc: 0.5487\n",
      "Epoch 00069: val_loss improved from 1.03110 to 1.02955, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv_checkpoint/069-1.0296.hdf5\n",
      "36805/36805 [==============================] - 16s 428us/sample - loss: 1.3786 - acc: 0.5487 - val_loss: 1.0296 - val_acc: 0.6969\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3671 - acc: 0.5564\n",
      "Epoch 00070: val_loss improved from 1.02955 to 1.02347, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv_checkpoint/070-1.0235.hdf5\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 1.3672 - acc: 0.5564 - val_loss: 1.0235 - val_acc: 0.6986\n",
      "Epoch 71/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3620 - acc: 0.5557\n",
      "Epoch 00071: val_loss improved from 1.02347 to 1.02173, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv_checkpoint/071-1.0217.hdf5\n",
      "36805/36805 [==============================] - 16s 428us/sample - loss: 1.3620 - acc: 0.5557 - val_loss: 1.0217 - val_acc: 0.6974\n",
      "Epoch 72/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3585 - acc: 0.5547\n",
      "Epoch 00072: val_loss improved from 1.02173 to 1.02067, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv_checkpoint/072-1.0207.hdf5\n",
      "36805/36805 [==============================] - 16s 427us/sample - loss: 1.3591 - acc: 0.5548 - val_loss: 1.0207 - val_acc: 0.6979\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3581 - acc: 0.5546\n",
      "Epoch 00073: val_loss improved from 1.02067 to 1.01410, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv_checkpoint/073-1.0141.hdf5\n",
      "36805/36805 [==============================] - 16s 428us/sample - loss: 1.3581 - acc: 0.5546 - val_loss: 1.0141 - val_acc: 0.7037\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3535 - acc: 0.5589\n",
      "Epoch 00074: val_loss improved from 1.01410 to 1.01389, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv_checkpoint/074-1.0139.hdf5\n",
      "36805/36805 [==============================] - 16s 426us/sample - loss: 1.3534 - acc: 0.5589 - val_loss: 1.0139 - val_acc: 0.7011\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3553 - acc: 0.5606\n",
      "Epoch 00075: val_loss improved from 1.01389 to 1.01146, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv_checkpoint/075-1.0115.hdf5\n",
      "36805/36805 [==============================] - 16s 426us/sample - loss: 1.3554 - acc: 0.5605 - val_loss: 1.0115 - val_acc: 0.7011\n",
      "Epoch 76/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3496 - acc: 0.5611\n",
      "Epoch 00076: val_loss improved from 1.01146 to 1.00887, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv_checkpoint/076-1.0089.hdf5\n",
      "36805/36805 [==============================] - 16s 425us/sample - loss: 1.3497 - acc: 0.5610 - val_loss: 1.0089 - val_acc: 0.7021\n",
      "Epoch 77/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3482 - acc: 0.5618\n",
      "Epoch 00077: val_loss improved from 1.00887 to 1.00693, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv_checkpoint/077-1.0069.hdf5\n",
      "36805/36805 [==============================] - 16s 429us/sample - loss: 1.3482 - acc: 0.5617 - val_loss: 1.0069 - val_acc: 0.7046\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3451 - acc: 0.5646\n",
      "Epoch 00078: val_loss improved from 1.00693 to 1.00332, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv_checkpoint/078-1.0033.hdf5\n",
      "36805/36805 [==============================] - 16s 424us/sample - loss: 1.3451 - acc: 0.5647 - val_loss: 1.0033 - val_acc: 0.7039\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3372 - acc: 0.5641\n",
      "Epoch 00079: val_loss improved from 1.00332 to 0.99513, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv_checkpoint/079-0.9951.hdf5\n",
      "36805/36805 [==============================] - 16s 425us/sample - loss: 1.3372 - acc: 0.5641 - val_loss: 0.9951 - val_acc: 0.7046\n",
      "Epoch 80/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3495 - acc: 0.5617\n",
      "Epoch 00080: val_loss improved from 0.99513 to 0.99193, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv_checkpoint/080-0.9919.hdf5\n",
      "36805/36805 [==============================] - 16s 429us/sample - loss: 1.3491 - acc: 0.5618 - val_loss: 0.9919 - val_acc: 0.7074\n",
      "Epoch 81/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3389 - acc: 0.5643\n",
      "Epoch 00081: val_loss improved from 0.99193 to 0.99189, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv_checkpoint/081-0.9919.hdf5\n",
      "36805/36805 [==============================] - 16s 426us/sample - loss: 1.3388 - acc: 0.5644 - val_loss: 0.9919 - val_acc: 0.7056\n",
      "Epoch 82/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3323 - acc: 0.5668\n",
      "Epoch 00082: val_loss improved from 0.99189 to 0.98008, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv_checkpoint/082-0.9801.hdf5\n",
      "36805/36805 [==============================] - 16s 428us/sample - loss: 1.3319 - acc: 0.5667 - val_loss: 0.9801 - val_acc: 0.7079\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3363 - acc: 0.5625\n",
      "Epoch 00083: val_loss did not improve from 0.98008\n",
      "36805/36805 [==============================] - 16s 426us/sample - loss: 1.3364 - acc: 0.5625 - val_loss: 0.9834 - val_acc: 0.7098\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3371 - acc: 0.5632\n",
      "Epoch 00084: val_loss did not improve from 0.98008\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 1.3372 - acc: 0.5632 - val_loss: 0.9829 - val_acc: 0.7100\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3272 - acc: 0.5668\n",
      "Epoch 00085: val_loss did not improve from 0.98008\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 1.3272 - acc: 0.5668 - val_loss: 0.9873 - val_acc: 0.7119\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3341 - acc: 0.5631\n",
      "Epoch 00086: val_loss improved from 0.98008 to 0.97763, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv_checkpoint/086-0.9776.hdf5\n",
      "36805/36805 [==============================] - 16s 429us/sample - loss: 1.3340 - acc: 0.5631 - val_loss: 0.9776 - val_acc: 0.7077\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3334 - acc: 0.5642\n",
      "Epoch 00087: val_loss did not improve from 0.97763\n",
      "36805/36805 [==============================] - 16s 425us/sample - loss: 1.3335 - acc: 0.5642 - val_loss: 0.9784 - val_acc: 0.7079\n",
      "Epoch 88/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3278 - acc: 0.5707\n",
      "Epoch 00088: val_loss did not improve from 0.97763\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 1.3278 - acc: 0.5707 - val_loss: 0.9839 - val_acc: 0.7100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3295 - acc: 0.5688\n",
      "Epoch 00089: val_loss did not improve from 0.97763\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 1.3295 - acc: 0.5688 - val_loss: 0.9836 - val_acc: 0.7114\n",
      "Epoch 90/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3219 - acc: 0.5692\n",
      "Epoch 00090: val_loss improved from 0.97763 to 0.97545, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv_checkpoint/090-0.9755.hdf5\n",
      "36805/36805 [==============================] - 16s 426us/sample - loss: 1.3218 - acc: 0.5692 - val_loss: 0.9755 - val_acc: 0.7128\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3250 - acc: 0.5708\n",
      "Epoch 00091: val_loss improved from 0.97545 to 0.97374, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv_checkpoint/091-0.9737.hdf5\n",
      "36805/36805 [==============================] - 16s 426us/sample - loss: 1.3249 - acc: 0.5709 - val_loss: 0.9737 - val_acc: 0.7128\n",
      "Epoch 92/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3223 - acc: 0.5692\n",
      "Epoch 00092: val_loss improved from 0.97374 to 0.96907, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv_checkpoint/092-0.9691.hdf5\n",
      "36805/36805 [==============================] - 16s 422us/sample - loss: 1.3221 - acc: 0.5693 - val_loss: 0.9691 - val_acc: 0.7091\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3194 - acc: 0.5698\n",
      "Epoch 00093: val_loss did not improve from 0.96907\n",
      "36805/36805 [==============================] - 16s 424us/sample - loss: 1.3193 - acc: 0.5698 - val_loss: 0.9704 - val_acc: 0.7121\n",
      "Epoch 94/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3189 - acc: 0.5704\n",
      "Epoch 00094: val_loss improved from 0.96907 to 0.96453, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv_checkpoint/094-0.9645.hdf5\n",
      "36805/36805 [==============================] - 16s 429us/sample - loss: 1.3190 - acc: 0.5704 - val_loss: 0.9645 - val_acc: 0.7140\n",
      "Epoch 95/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3152 - acc: 0.5723\n",
      "Epoch 00095: val_loss improved from 0.96453 to 0.95950, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv_checkpoint/095-0.9595.hdf5\n",
      "36805/36805 [==============================] - 16s 425us/sample - loss: 1.3147 - acc: 0.5724 - val_loss: 0.9595 - val_acc: 0.7151\n",
      "Epoch 96/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3065 - acc: 0.5765\n",
      "Epoch 00096: val_loss did not improve from 0.95950\n",
      "36805/36805 [==============================] - 16s 425us/sample - loss: 1.3063 - acc: 0.5766 - val_loss: 0.9643 - val_acc: 0.7151\n",
      "Epoch 97/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3073 - acc: 0.5747\n",
      "Epoch 00097: val_loss did not improve from 0.95950\n",
      "36805/36805 [==============================] - 16s 424us/sample - loss: 1.3077 - acc: 0.5746 - val_loss: 0.9619 - val_acc: 0.7207\n",
      "Epoch 98/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3128 - acc: 0.5738\n",
      "Epoch 00098: val_loss improved from 0.95950 to 0.95179, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv_checkpoint/098-0.9518.hdf5\n",
      "36805/36805 [==============================] - 16s 425us/sample - loss: 1.3124 - acc: 0.5739 - val_loss: 0.9518 - val_acc: 0.7170\n",
      "Epoch 99/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3046 - acc: 0.5754\n",
      "Epoch 00099: val_loss improved from 0.95179 to 0.95061, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv_checkpoint/099-0.9506.hdf5\n",
      "36805/36805 [==============================] - 16s 430us/sample - loss: 1.3050 - acc: 0.5751 - val_loss: 0.9506 - val_acc: 0.7207\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3011 - acc: 0.5773\n",
      "Epoch 00100: val_loss improved from 0.95061 to 0.94415, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv_checkpoint/100-0.9442.hdf5\n",
      "36805/36805 [==============================] - 16s 427us/sample - loss: 1.3012 - acc: 0.5772 - val_loss: 0.9442 - val_acc: 0.7216\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2994 - acc: 0.5780\n",
      "Epoch 00101: val_loss did not improve from 0.94415\n",
      "36805/36805 [==============================] - 16s 425us/sample - loss: 1.2994 - acc: 0.5780 - val_loss: 0.9449 - val_acc: 0.7202\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3032 - acc: 0.5779\n",
      "Epoch 00102: val_loss did not improve from 0.94415\n",
      "36805/36805 [==============================] - 16s 428us/sample - loss: 1.3032 - acc: 0.5779 - val_loss: 0.9485 - val_acc: 0.7202\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3015 - acc: 0.5773\n",
      "Epoch 00103: val_loss did not improve from 0.94415\n",
      "36805/36805 [==============================] - 16s 428us/sample - loss: 1.3015 - acc: 0.5773 - val_loss: 0.9454 - val_acc: 0.7235\n",
      "Epoch 104/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3043 - acc: 0.5756\n",
      "Epoch 00104: val_loss did not improve from 0.94415\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 1.3042 - acc: 0.5756 - val_loss: 0.9513 - val_acc: 0.7184\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2919 - acc: 0.5792\n",
      "Epoch 00105: val_loss did not improve from 0.94415\n",
      "36805/36805 [==============================] - 16s 425us/sample - loss: 1.2921 - acc: 0.5792 - val_loss: 0.9461 - val_acc: 0.7258\n",
      "Epoch 106/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2903 - acc: 0.5765\n",
      "Epoch 00106: val_loss did not improve from 0.94415\n",
      "36805/36805 [==============================] - 16s 424us/sample - loss: 1.2902 - acc: 0.5765 - val_loss: 0.9527 - val_acc: 0.7207\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2944 - acc: 0.5787\n",
      "Epoch 00107: val_loss improved from 0.94415 to 0.94212, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv_checkpoint/107-0.9421.hdf5\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 1.2943 - acc: 0.5787 - val_loss: 0.9421 - val_acc: 0.7263\n",
      "Epoch 108/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2923 - acc: 0.5784\n",
      "Epoch 00108: val_loss improved from 0.94212 to 0.93953, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv_checkpoint/108-0.9395.hdf5\n",
      "36805/36805 [==============================] - 16s 424us/sample - loss: 1.2920 - acc: 0.5785 - val_loss: 0.9395 - val_acc: 0.7258\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2905 - acc: 0.5785\n",
      "Epoch 00109: val_loss improved from 0.93953 to 0.93832, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv_checkpoint/109-0.9383.hdf5\n",
      "36805/36805 [==============================] - 16s 425us/sample - loss: 1.2906 - acc: 0.5784 - val_loss: 0.9383 - val_acc: 0.7251\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2963 - acc: 0.5772\n",
      "Epoch 00110: val_loss improved from 0.93832 to 0.93201, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv_checkpoint/110-0.9320.hdf5\n",
      "36805/36805 [==============================] - 16s 425us/sample - loss: 1.2962 - acc: 0.5773 - val_loss: 0.9320 - val_acc: 0.7284\n",
      "Epoch 111/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.2872 - acc: 0.5804\n",
      "Epoch 00111: val_loss did not improve from 0.93201\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 1.2870 - acc: 0.5804 - val_loss: 0.9365 - val_acc: 0.7256\n",
      "Epoch 112/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2877 - acc: 0.5801\n",
      "Epoch 00112: val_loss did not improve from 0.93201\n",
      "36805/36805 [==============================] - 16s 427us/sample - loss: 1.2873 - acc: 0.5804 - val_loss: 0.9352 - val_acc: 0.7216\n",
      "Epoch 113/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2796 - acc: 0.5867\n",
      "Epoch 00113: val_loss did not improve from 0.93201\n",
      "36805/36805 [==============================] - 16s 422us/sample - loss: 1.2794 - acc: 0.5869 - val_loss: 0.9410 - val_acc: 0.7240\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 114/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2827 - acc: 0.5831\n",
      "Epoch 00114: val_loss did not improve from 0.93201\n",
      "36805/36805 [==============================] - 16s 421us/sample - loss: 1.2830 - acc: 0.5829 - val_loss: 0.9429 - val_acc: 0.7265\n",
      "Epoch 115/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2860 - acc: 0.5830\n",
      "Epoch 00115: val_loss did not improve from 0.93201\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 1.2866 - acc: 0.5829 - val_loss: 0.9398 - val_acc: 0.7214\n",
      "Epoch 116/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2866 - acc: 0.5810\n",
      "Epoch 00116: val_loss did not improve from 0.93201\n",
      "36805/36805 [==============================] - 16s 424us/sample - loss: 1.2874 - acc: 0.5808 - val_loss: 0.9343 - val_acc: 0.7277\n",
      "Epoch 117/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2821 - acc: 0.5863\n",
      "Epoch 00117: val_loss improved from 0.93201 to 0.92678, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv_checkpoint/117-0.9268.hdf5\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 1.2823 - acc: 0.5863 - val_loss: 0.9268 - val_acc: 0.7305\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2741 - acc: 0.5881\n",
      "Epoch 00118: val_loss improved from 0.92678 to 0.92166, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv_checkpoint/118-0.9217.hdf5\n",
      "36805/36805 [==============================] - 16s 424us/sample - loss: 1.2740 - acc: 0.5881 - val_loss: 0.9217 - val_acc: 0.7284\n",
      "Epoch 119/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2745 - acc: 0.5856\n",
      "Epoch 00119: val_loss improved from 0.92166 to 0.91963, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv_checkpoint/119-0.9196.hdf5\n",
      "36805/36805 [==============================] - 16s 422us/sample - loss: 1.2738 - acc: 0.5858 - val_loss: 0.9196 - val_acc: 0.7263\n",
      "Epoch 120/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2777 - acc: 0.5868\n",
      "Epoch 00120: val_loss did not improve from 0.91963\n",
      "36805/36805 [==============================] - 16s 425us/sample - loss: 1.2774 - acc: 0.5868 - val_loss: 0.9206 - val_acc: 0.7321\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2666 - acc: 0.5872\n",
      "Epoch 00121: val_loss did not improve from 0.91963\n",
      "36805/36805 [==============================] - 16s 425us/sample - loss: 1.2666 - acc: 0.5872 - val_loss: 0.9320 - val_acc: 0.7312\n",
      "Epoch 122/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2781 - acc: 0.5866\n",
      "Epoch 00122: val_loss did not improve from 0.91963\n",
      "36805/36805 [==============================] - 16s 424us/sample - loss: 1.2787 - acc: 0.5864 - val_loss: 0.9281 - val_acc: 0.7282\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2727 - acc: 0.5855\n",
      "Epoch 00123: val_loss improved from 0.91963 to 0.91126, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv_checkpoint/123-0.9113.hdf5\n",
      "36805/36805 [==============================] - 16s 426us/sample - loss: 1.2727 - acc: 0.5855 - val_loss: 0.9113 - val_acc: 0.7317\n",
      "Epoch 124/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2604 - acc: 0.5910\n",
      "Epoch 00124: val_loss did not improve from 0.91126\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 1.2603 - acc: 0.5910 - val_loss: 0.9261 - val_acc: 0.7310\n",
      "Epoch 125/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2734 - acc: 0.5843\n",
      "Epoch 00125: val_loss did not improve from 0.91126\n",
      "36805/36805 [==============================] - 16s 424us/sample - loss: 1.2738 - acc: 0.5841 - val_loss: 0.9177 - val_acc: 0.7293\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2702 - acc: 0.5889\n",
      "Epoch 00126: val_loss did not improve from 0.91126\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 1.2702 - acc: 0.5890 - val_loss: 0.9246 - val_acc: 0.7310\n",
      "Epoch 127/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2692 - acc: 0.5871\n",
      "Epoch 00127: val_loss did not improve from 0.91126\n",
      "36805/36805 [==============================] - 16s 427us/sample - loss: 1.2689 - acc: 0.5872 - val_loss: 0.9131 - val_acc: 0.7314\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2742 - acc: 0.5851\n",
      "Epoch 00128: val_loss did not improve from 0.91126\n",
      "36805/36805 [==============================] - 16s 427us/sample - loss: 1.2742 - acc: 0.5851 - val_loss: 0.9220 - val_acc: 0.7331\n",
      "Epoch 129/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2640 - acc: 0.5890\n",
      "Epoch 00129: val_loss did not improve from 0.91126\n",
      "36805/36805 [==============================] - 16s 425us/sample - loss: 1.2638 - acc: 0.5889 - val_loss: 0.9125 - val_acc: 0.7300\n",
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2711 - acc: 0.5888\n",
      "Epoch 00130: val_loss did not improve from 0.91126\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 1.2711 - acc: 0.5888 - val_loss: 0.9205 - val_acc: 0.7333\n",
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2579 - acc: 0.5916\n",
      "Epoch 00131: val_loss improved from 0.91126 to 0.90486, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv_checkpoint/131-0.9049.hdf5\n",
      "36805/36805 [==============================] - 16s 425us/sample - loss: 1.2578 - acc: 0.5916 - val_loss: 0.9049 - val_acc: 0.7340\n",
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2601 - acc: 0.5922\n",
      "Epoch 00132: val_loss improved from 0.90486 to 0.90406, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv_checkpoint/132-0.9041.hdf5\n",
      "36805/36805 [==============================] - 16s 426us/sample - loss: 1.2601 - acc: 0.5922 - val_loss: 0.9041 - val_acc: 0.7310\n",
      "Epoch 133/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2622 - acc: 0.5878\n",
      "Epoch 00133: val_loss did not improve from 0.90406\n",
      "36805/36805 [==============================] - 16s 427us/sample - loss: 1.2623 - acc: 0.5878 - val_loss: 0.9082 - val_acc: 0.7331\n",
      "Epoch 134/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2594 - acc: 0.5905\n",
      "Epoch 00134: val_loss improved from 0.90406 to 0.90110, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv_checkpoint/134-0.9011.hdf5\n",
      "36805/36805 [==============================] - 16s 424us/sample - loss: 1.2594 - acc: 0.5905 - val_loss: 0.9011 - val_acc: 0.7317\n",
      "Epoch 135/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2567 - acc: 0.5917\n",
      "Epoch 00135: val_loss did not improve from 0.90110\n",
      "36805/36805 [==============================] - 16s 426us/sample - loss: 1.2567 - acc: 0.5917 - val_loss: 0.9109 - val_acc: 0.7303\n",
      "Epoch 136/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2537 - acc: 0.5926\n",
      "Epoch 00136: val_loss did not improve from 0.90110\n",
      "36805/36805 [==============================] - 16s 424us/sample - loss: 1.2536 - acc: 0.5926 - val_loss: 0.9048 - val_acc: 0.7310\n",
      "Epoch 137/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2563 - acc: 0.5917\n",
      "Epoch 00137: val_loss did not improve from 0.90110\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 1.2563 - acc: 0.5917 - val_loss: 0.9024 - val_acc: 0.7275\n",
      "Epoch 138/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2642 - acc: 0.5882\n",
      "Epoch 00138: val_loss improved from 0.90110 to 0.89449, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv_checkpoint/138-0.8945.hdf5\n",
      "36805/36805 [==============================] - 16s 426us/sample - loss: 1.2643 - acc: 0.5882 - val_loss: 0.8945 - val_acc: 0.7407\n",
      "Epoch 139/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2499 - acc: 0.5928\n",
      "Epoch 00139: val_loss did not improve from 0.89449\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 1.2500 - acc: 0.5928 - val_loss: 0.8949 - val_acc: 0.7358\n",
      "Epoch 140/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2466 - acc: 0.5933\n",
      "Epoch 00140: val_loss did not improve from 0.89449\n",
      "36805/36805 [==============================] - 16s 425us/sample - loss: 1.2467 - acc: 0.5932 - val_loss: 0.8969 - val_acc: 0.7333\n",
      "Epoch 141/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2534 - acc: 0.5933\n",
      "Epoch 00141: val_loss did not improve from 0.89449\n",
      "36805/36805 [==============================] - 16s 430us/sample - loss: 1.2534 - acc: 0.5933 - val_loss: 0.9035 - val_acc: 0.7361\n",
      "Epoch 142/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2517 - acc: 0.5931\n",
      "Epoch 00142: val_loss improved from 0.89449 to 0.88808, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv_checkpoint/142-0.8881.hdf5\n",
      "36805/36805 [==============================] - 16s 426us/sample - loss: 1.2514 - acc: 0.5931 - val_loss: 0.8881 - val_acc: 0.7361\n",
      "Epoch 143/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2524 - acc: 0.5960\n",
      "Epoch 00143: val_loss did not improve from 0.88808\n",
      "36805/36805 [==============================] - 16s 426us/sample - loss: 1.2521 - acc: 0.5960 - val_loss: 0.8918 - val_acc: 0.7393\n",
      "Epoch 144/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2414 - acc: 0.5951\n",
      "Epoch 00144: val_loss did not improve from 0.88808\n",
      "36805/36805 [==============================] - 16s 428us/sample - loss: 1.2415 - acc: 0.5949 - val_loss: 0.9018 - val_acc: 0.7338\n",
      "Epoch 145/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2451 - acc: 0.5981\n",
      "Epoch 00145: val_loss did not improve from 0.88808\n",
      "36805/36805 [==============================] - 16s 428us/sample - loss: 1.2448 - acc: 0.5982 - val_loss: 0.8923 - val_acc: 0.7352\n",
      "Epoch 146/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2511 - acc: 0.5943\n",
      "Epoch 00146: val_loss did not improve from 0.88808\n",
      "36805/36805 [==============================] - 16s 425us/sample - loss: 1.2510 - acc: 0.5943 - val_loss: 0.8889 - val_acc: 0.7342\n",
      "Epoch 147/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2384 - acc: 0.5968\n",
      "Epoch 00147: val_loss improved from 0.88808 to 0.88603, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv_checkpoint/147-0.8860.hdf5\n",
      "36805/36805 [==============================] - 16s 427us/sample - loss: 1.2381 - acc: 0.5969 - val_loss: 0.8860 - val_acc: 0.7414\n",
      "Epoch 148/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2513 - acc: 0.5932\n",
      "Epoch 00148: val_loss did not improve from 0.88603\n",
      "36805/36805 [==============================] - 16s 427us/sample - loss: 1.2505 - acc: 0.5935 - val_loss: 0.8914 - val_acc: 0.7349\n",
      "Epoch 149/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2375 - acc: 0.5979\n",
      "Epoch 00149: val_loss did not improve from 0.88603\n",
      "36805/36805 [==============================] - 16s 425us/sample - loss: 1.2378 - acc: 0.5978 - val_loss: 0.8900 - val_acc: 0.7379\n",
      "Epoch 150/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2416 - acc: 0.5958\n",
      "Epoch 00150: val_loss did not improve from 0.88603\n",
      "36805/36805 [==============================] - 16s 425us/sample - loss: 1.2419 - acc: 0.5958 - val_loss: 0.8905 - val_acc: 0.7342\n",
      "Epoch 151/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2437 - acc: 0.5940\n",
      "Epoch 00151: val_loss did not improve from 0.88603\n",
      "36805/36805 [==============================] - 16s 431us/sample - loss: 1.2436 - acc: 0.5941 - val_loss: 0.8966 - val_acc: 0.7368\n",
      "Epoch 152/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2385 - acc: 0.5999\n",
      "Epoch 00152: val_loss did not improve from 0.88603\n",
      "36805/36805 [==============================] - 16s 426us/sample - loss: 1.2386 - acc: 0.5999 - val_loss: 0.8897 - val_acc: 0.7412\n",
      "Epoch 153/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2390 - acc: 0.5988\n",
      "Epoch 00153: val_loss did not improve from 0.88603\n",
      "36805/36805 [==============================] - 16s 429us/sample - loss: 1.2390 - acc: 0.5988 - val_loss: 0.8976 - val_acc: 0.7417\n",
      "Epoch 154/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2385 - acc: 0.5979\n",
      "Epoch 00154: val_loss improved from 0.88603 to 0.88553, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv_checkpoint/154-0.8855.hdf5\n",
      "36805/36805 [==============================] - 16s 427us/sample - loss: 1.2384 - acc: 0.5979 - val_loss: 0.8855 - val_acc: 0.7435\n",
      "Epoch 155/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2343 - acc: 0.5979\n",
      "Epoch 00155: val_loss improved from 0.88553 to 0.88107, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv_checkpoint/155-0.8811.hdf5\n",
      "36805/36805 [==============================] - 16s 429us/sample - loss: 1.2346 - acc: 0.5977 - val_loss: 0.8811 - val_acc: 0.7452\n",
      "Epoch 156/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2281 - acc: 0.6040\n",
      "Epoch 00156: val_loss did not improve from 0.88107\n",
      "36805/36805 [==============================] - 16s 426us/sample - loss: 1.2281 - acc: 0.6040 - val_loss: 0.8841 - val_acc: 0.7445\n",
      "Epoch 157/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2322 - acc: 0.6020\n",
      "Epoch 00157: val_loss did not improve from 0.88107\n",
      "36805/36805 [==============================] - 16s 425us/sample - loss: 1.2323 - acc: 0.6020 - val_loss: 0.8843 - val_acc: 0.7438\n",
      "Epoch 158/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2316 - acc: 0.6015\n",
      "Epoch 00158: val_loss did not improve from 0.88107\n",
      "36805/36805 [==============================] - 16s 425us/sample - loss: 1.2318 - acc: 0.6014 - val_loss: 0.8839 - val_acc: 0.7410\n",
      "Epoch 159/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2297 - acc: 0.6011\n",
      "Epoch 00159: val_loss did not improve from 0.88107\n",
      "36805/36805 [==============================] - 16s 425us/sample - loss: 1.2291 - acc: 0.6013 - val_loss: 0.8832 - val_acc: 0.7433\n",
      "Epoch 160/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2328 - acc: 0.6016\n",
      "Epoch 00160: val_loss improved from 0.88107 to 0.87465, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv_checkpoint/160-0.8747.hdf5\n",
      "36805/36805 [==============================] - 16s 430us/sample - loss: 1.2327 - acc: 0.6017 - val_loss: 0.8747 - val_acc: 0.7454\n",
      "Epoch 161/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2280 - acc: 0.6000\n",
      "Epoch 00161: val_loss improved from 0.87465 to 0.87305, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv_checkpoint/161-0.8731.hdf5\n",
      "36805/36805 [==============================] - 16s 425us/sample - loss: 1.2280 - acc: 0.6000 - val_loss: 0.8731 - val_acc: 0.7410\n",
      "Epoch 162/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2264 - acc: 0.6035\n",
      "Epoch 00162: val_loss did not improve from 0.87305\n",
      "36805/36805 [==============================] - 16s 425us/sample - loss: 1.2263 - acc: 0.6035 - val_loss: 0.8762 - val_acc: 0.7375\n",
      "Epoch 163/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.2316 - acc: 0.5997\n",
      "Epoch 00163: val_loss did not improve from 0.87305\n",
      "36805/36805 [==============================] - 16s 426us/sample - loss: 1.2316 - acc: 0.5998 - val_loss: 0.8796 - val_acc: 0.7440\n",
      "Epoch 164/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2379 - acc: 0.6012\n",
      "Epoch 00164: val_loss did not improve from 0.87305\n",
      "36805/36805 [==============================] - 16s 428us/sample - loss: 1.2379 - acc: 0.6012 - val_loss: 0.8828 - val_acc: 0.7442\n",
      "Epoch 165/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2278 - acc: 0.6009\n",
      "Epoch 00165: val_loss improved from 0.87305 to 0.86925, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv_checkpoint/165-0.8692.hdf5\n",
      "36805/36805 [==============================] - 16s 425us/sample - loss: 1.2281 - acc: 0.6007 - val_loss: 0.8692 - val_acc: 0.7421\n",
      "Epoch 166/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2156 - acc: 0.6044\n",
      "Epoch 00166: val_loss improved from 0.86925 to 0.85971, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv_checkpoint/166-0.8597.hdf5\n",
      "36805/36805 [==============================] - 16s 424us/sample - loss: 1.2159 - acc: 0.6042 - val_loss: 0.8597 - val_acc: 0.7477\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 167/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2246 - acc: 0.6032\n",
      "Epoch 00167: val_loss did not improve from 0.85971\n",
      "36805/36805 [==============================] - 16s 424us/sample - loss: 1.2248 - acc: 0.6031 - val_loss: 0.8708 - val_acc: 0.7470\n",
      "Epoch 168/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2261 - acc: 0.6028\n",
      "Epoch 00168: val_loss did not improve from 0.85971\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 1.2262 - acc: 0.6028 - val_loss: 0.8658 - val_acc: 0.7442\n",
      "Epoch 169/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2238 - acc: 0.6051\n",
      "Epoch 00169: val_loss did not improve from 0.85971\n",
      "36805/36805 [==============================] - 16s 427us/sample - loss: 1.2239 - acc: 0.6051 - val_loss: 0.8817 - val_acc: 0.7435\n",
      "Epoch 170/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2290 - acc: 0.6039\n",
      "Epoch 00170: val_loss did not improve from 0.85971\n",
      "36805/36805 [==============================] - 16s 422us/sample - loss: 1.2291 - acc: 0.6039 - val_loss: 0.8707 - val_acc: 0.7440\n",
      "Epoch 171/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2225 - acc: 0.6033\n",
      "Epoch 00171: val_loss did not improve from 0.85971\n",
      "36805/36805 [==============================] - 16s 427us/sample - loss: 1.2226 - acc: 0.6033 - val_loss: 0.8707 - val_acc: 0.7440\n",
      "Epoch 172/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2203 - acc: 0.6027\n",
      "Epoch 00172: val_loss did not improve from 0.85971\n",
      "36805/36805 [==============================] - 16s 424us/sample - loss: 1.2201 - acc: 0.6027 - val_loss: 0.8687 - val_acc: 0.7473\n",
      "Epoch 173/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2208 - acc: 0.6040\n",
      "Epoch 00173: val_loss did not improve from 0.85971\n",
      "36805/36805 [==============================] - 16s 424us/sample - loss: 1.2208 - acc: 0.6041 - val_loss: 0.8647 - val_acc: 0.7435\n",
      "Epoch 174/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2144 - acc: 0.6053\n",
      "Epoch 00174: val_loss did not improve from 0.85971\n",
      "36805/36805 [==============================] - 16s 426us/sample - loss: 1.2144 - acc: 0.6053 - val_loss: 0.8637 - val_acc: 0.7433\n",
      "Epoch 175/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2173 - acc: 0.6076\n",
      "Epoch 00175: val_loss did not improve from 0.85971\n",
      "36805/36805 [==============================] - 16s 422us/sample - loss: 1.2173 - acc: 0.6075 - val_loss: 0.8629 - val_acc: 0.7433\n",
      "Epoch 176/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2148 - acc: 0.6055\n",
      "Epoch 00176: val_loss improved from 0.85971 to 0.85504, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv_checkpoint/176-0.8550.hdf5\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 1.2147 - acc: 0.6055 - val_loss: 0.8550 - val_acc: 0.7482\n",
      "Epoch 177/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.2171 - acc: 0.6067\n",
      "Epoch 00177: val_loss did not improve from 0.85504\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 1.2169 - acc: 0.6066 - val_loss: 0.8568 - val_acc: 0.7410\n",
      "Epoch 178/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2179 - acc: 0.6057\n",
      "Epoch 00178: val_loss did not improve from 0.85504\n",
      "36805/36805 [==============================] - 16s 422us/sample - loss: 1.2178 - acc: 0.6057 - val_loss: 0.8681 - val_acc: 0.7431\n",
      "Epoch 179/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2193 - acc: 0.6033\n",
      "Epoch 00179: val_loss did not improve from 0.85504\n",
      "36805/36805 [==============================] - 16s 424us/sample - loss: 1.2193 - acc: 0.6033 - val_loss: 0.8685 - val_acc: 0.7482\n",
      "Epoch 180/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2208 - acc: 0.6049\n",
      "Epoch 00180: val_loss did not improve from 0.85504\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 1.2207 - acc: 0.6051 - val_loss: 0.8552 - val_acc: 0.7445\n",
      "Epoch 181/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2135 - acc: 0.6068\n",
      "Epoch 00181: val_loss did not improve from 0.85504\n",
      "36805/36805 [==============================] - 16s 426us/sample - loss: 1.2135 - acc: 0.6068 - val_loss: 0.8656 - val_acc: 0.7445\n",
      "Epoch 182/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2119 - acc: 0.6072\n",
      "Epoch 00182: val_loss improved from 0.85504 to 0.84978, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv_checkpoint/182-0.8498.hdf5\n",
      "36805/36805 [==============================] - 16s 424us/sample - loss: 1.2120 - acc: 0.6072 - val_loss: 0.8498 - val_acc: 0.7496\n",
      "Epoch 183/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2191 - acc: 0.6039\n",
      "Epoch 00183: val_loss did not improve from 0.84978\n",
      "36805/36805 [==============================] - 16s 425us/sample - loss: 1.2192 - acc: 0.6040 - val_loss: 0.8577 - val_acc: 0.7475\n",
      "Epoch 184/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2125 - acc: 0.6066\n",
      "Epoch 00184: val_loss did not improve from 0.84978\n",
      "36805/36805 [==============================] - 16s 424us/sample - loss: 1.2125 - acc: 0.6066 - val_loss: 0.8563 - val_acc: 0.7491\n",
      "Epoch 185/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2079 - acc: 0.6061\n",
      "Epoch 00185: val_loss improved from 0.84978 to 0.84915, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv_checkpoint/185-0.8492.hdf5\n",
      "36805/36805 [==============================] - 16s 424us/sample - loss: 1.2080 - acc: 0.6062 - val_loss: 0.8492 - val_acc: 0.7463\n",
      "Epoch 186/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2143 - acc: 0.6068\n",
      "Epoch 00186: val_loss did not improve from 0.84915\n",
      "36805/36805 [==============================] - 16s 422us/sample - loss: 1.2145 - acc: 0.6068 - val_loss: 0.8574 - val_acc: 0.7473\n",
      "Epoch 187/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2180 - acc: 0.6045\n",
      "Epoch 00187: val_loss did not improve from 0.84915\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 1.2180 - acc: 0.6045 - val_loss: 0.8587 - val_acc: 0.7498\n",
      "Epoch 188/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2122 - acc: 0.6111\n",
      "Epoch 00188: val_loss did not improve from 0.84915\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 1.2121 - acc: 0.6112 - val_loss: 0.8568 - val_acc: 0.7484\n",
      "Epoch 189/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2149 - acc: 0.6055\n",
      "Epoch 00189: val_loss did not improve from 0.84915\n",
      "36805/36805 [==============================] - 16s 422us/sample - loss: 1.2151 - acc: 0.6055 - val_loss: 0.8506 - val_acc: 0.7489\n",
      "Epoch 190/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2129 - acc: 0.6076\n",
      "Epoch 00190: val_loss did not improve from 0.84915\n",
      "36805/36805 [==============================] - 16s 422us/sample - loss: 1.2128 - acc: 0.6077 - val_loss: 0.8575 - val_acc: 0.7407\n",
      "Epoch 191/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2077 - acc: 0.6089\n",
      "Epoch 00191: val_loss did not improve from 0.84915\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 1.2074 - acc: 0.6090 - val_loss: 0.8495 - val_acc: 0.7482\n",
      "Epoch 192/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2136 - acc: 0.6075\n",
      "Epoch 00192: val_loss did not improve from 0.84915\n",
      "36805/36805 [==============================] - 15s 421us/sample - loss: 1.2131 - acc: 0.6076 - val_loss: 0.8606 - val_acc: 0.7515\n",
      "Epoch 193/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2020 - acc: 0.6106\n",
      "Epoch 00193: val_loss did not improve from 0.84915\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 1.2020 - acc: 0.6107 - val_loss: 0.8502 - val_acc: 0.7480\n",
      "Epoch 194/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2134 - acc: 0.6073\n",
      "Epoch 00194: val_loss did not improve from 0.84915\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 1.2133 - acc: 0.6074 - val_loss: 0.8536 - val_acc: 0.7529\n",
      "Epoch 195/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2069 - acc: 0.6111\n",
      "Epoch 00195: val_loss did not improve from 0.84915\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 1.2069 - acc: 0.6112 - val_loss: 0.8533 - val_acc: 0.7480\n",
      "Epoch 196/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1993 - acc: 0.6080\n",
      "Epoch 00196: val_loss improved from 0.84915 to 0.84554, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv_checkpoint/196-0.8455.hdf5\n",
      "36805/36805 [==============================] - 16s 424us/sample - loss: 1.1992 - acc: 0.6080 - val_loss: 0.8455 - val_acc: 0.7501\n",
      "Epoch 197/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2062 - acc: 0.6101\n",
      "Epoch 00197: val_loss did not improve from 0.84554\n",
      "36805/36805 [==============================] - 16s 421us/sample - loss: 1.2061 - acc: 0.6102 - val_loss: 0.8475 - val_acc: 0.7491\n",
      "Epoch 198/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2094 - acc: 0.6078\n",
      "Epoch 00198: val_loss did not improve from 0.84554\n",
      "36805/36805 [==============================] - 16s 425us/sample - loss: 1.2093 - acc: 0.6079 - val_loss: 0.8523 - val_acc: 0.7505\n",
      "Epoch 199/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1971 - acc: 0.6139\n",
      "Epoch 00199: val_loss did not improve from 0.84554\n",
      "36805/36805 [==============================] - 16s 421us/sample - loss: 1.1978 - acc: 0.6138 - val_loss: 0.8544 - val_acc: 0.7505\n",
      "Epoch 200/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1972 - acc: 0.6109\n",
      "Epoch 00200: val_loss improved from 0.84554 to 0.83621, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv_checkpoint/200-0.8362.hdf5\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 1.1970 - acc: 0.6108 - val_loss: 0.8362 - val_acc: 0.7512\n",
      "Epoch 201/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.1984 - acc: 0.6080\n",
      "Epoch 00201: val_loss did not improve from 0.83621\n",
      "36805/36805 [==============================] - 16s 422us/sample - loss: 1.1985 - acc: 0.6082 - val_loss: 0.8461 - val_acc: 0.7473\n",
      "Epoch 202/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1928 - acc: 0.6128\n",
      "Epoch 00202: val_loss did not improve from 0.83621\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 1.1931 - acc: 0.6127 - val_loss: 0.8368 - val_acc: 0.7508\n",
      "Epoch 203/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1992 - acc: 0.6138\n",
      "Epoch 00203: val_loss did not improve from 0.83621\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 1.1992 - acc: 0.6138 - val_loss: 0.8522 - val_acc: 0.7515\n",
      "Epoch 204/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.2038 - acc: 0.6083\n",
      "Epoch 00204: val_loss did not improve from 0.83621\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 1.2037 - acc: 0.6083 - val_loss: 0.8441 - val_acc: 0.7540\n",
      "Epoch 205/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2054 - acc: 0.6119\n",
      "Epoch 00205: val_loss did not improve from 0.83621\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 1.2054 - acc: 0.6118 - val_loss: 0.8451 - val_acc: 0.7510\n",
      "Epoch 206/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1920 - acc: 0.6145\n",
      "Epoch 00206: val_loss did not improve from 0.83621\n",
      "36805/36805 [==============================] - 16s 424us/sample - loss: 1.1921 - acc: 0.6147 - val_loss: 0.8415 - val_acc: 0.7515\n",
      "Epoch 207/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.2007 - acc: 0.6134\n",
      "Epoch 00207: val_loss did not improve from 0.83621\n",
      "36805/36805 [==============================] - 15s 421us/sample - loss: 1.2004 - acc: 0.6137 - val_loss: 0.8417 - val_acc: 0.7515\n",
      "Epoch 208/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1937 - acc: 0.6129\n",
      "Epoch 00208: val_loss did not improve from 0.83621\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 1.1937 - acc: 0.6129 - val_loss: 0.8430 - val_acc: 0.7517\n",
      "Epoch 209/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1899 - acc: 0.6141\n",
      "Epoch 00209: val_loss did not improve from 0.83621\n",
      "36805/36805 [==============================] - 16s 424us/sample - loss: 1.1896 - acc: 0.6141 - val_loss: 0.8401 - val_acc: 0.7505\n",
      "Epoch 210/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1938 - acc: 0.6122\n",
      "Epoch 00210: val_loss did not improve from 0.83621\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 1.1937 - acc: 0.6123 - val_loss: 0.8453 - val_acc: 0.7556\n",
      "Epoch 211/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1881 - acc: 0.6140\n",
      "Epoch 00211: val_loss did not improve from 0.83621\n",
      "36805/36805 [==============================] - 16s 424us/sample - loss: 1.1877 - acc: 0.6141 - val_loss: 0.8400 - val_acc: 0.7512\n",
      "Epoch 212/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1980 - acc: 0.6126\n",
      "Epoch 00212: val_loss did not improve from 0.83621\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 1.1979 - acc: 0.6126 - val_loss: 0.8423 - val_acc: 0.7538\n",
      "Epoch 213/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.1940 - acc: 0.6117\n",
      "Epoch 00213: val_loss did not improve from 0.83621\n",
      "36805/36805 [==============================] - 16s 424us/sample - loss: 1.1944 - acc: 0.6116 - val_loss: 0.8374 - val_acc: 0.7531\n",
      "Epoch 214/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1844 - acc: 0.6160\n",
      "Epoch 00214: val_loss did not improve from 0.83621\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 1.1842 - acc: 0.6161 - val_loss: 0.8389 - val_acc: 0.7529\n",
      "Epoch 215/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1944 - acc: 0.6110\n",
      "Epoch 00215: val_loss improved from 0.83621 to 0.83544, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv_checkpoint/215-0.8354.hdf5\n",
      "36805/36805 [==============================] - 16s 425us/sample - loss: 1.1947 - acc: 0.6110 - val_loss: 0.8354 - val_acc: 0.7524\n",
      "Epoch 216/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1858 - acc: 0.6158\n",
      "Epoch 00216: val_loss improved from 0.83544 to 0.83394, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv_checkpoint/216-0.8339.hdf5\n",
      "36805/36805 [==============================] - 16s 424us/sample - loss: 1.1859 - acc: 0.6158 - val_loss: 0.8339 - val_acc: 0.7547\n",
      "Epoch 217/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1888 - acc: 0.6130\n",
      "Epoch 00217: val_loss did not improve from 0.83394\n",
      "36805/36805 [==============================] - 16s 424us/sample - loss: 1.1887 - acc: 0.6130 - val_loss: 0.8373 - val_acc: 0.7522\n",
      "Epoch 218/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1891 - acc: 0.6133\n",
      "Epoch 00218: val_loss did not improve from 0.83394\n",
      "36805/36805 [==============================] - 16s 425us/sample - loss: 1.1891 - acc: 0.6133 - val_loss: 0.8401 - val_acc: 0.7547\n",
      "Epoch 219/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1967 - acc: 0.6125\n",
      "Epoch 00219: val_loss did not improve from 0.83394\n",
      "36805/36805 [==============================] - 16s 425us/sample - loss: 1.1968 - acc: 0.6124 - val_loss: 0.8401 - val_acc: 0.7573\n",
      "Epoch 220/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1920 - acc: 0.6124\n",
      "Epoch 00220: val_loss did not improve from 0.83394\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 1.1920 - acc: 0.6124 - val_loss: 0.8405 - val_acc: 0.7552\n",
      "Epoch 221/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1876 - acc: 0.6160\n",
      "Epoch 00221: val_loss improved from 0.83394 to 0.83378, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv_checkpoint/221-0.8338.hdf5\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 1.1876 - acc: 0.6160 - val_loss: 0.8338 - val_acc: 0.7563\n",
      "Epoch 222/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1917 - acc: 0.6136\n",
      "Epoch 00222: val_loss improved from 0.83378 to 0.82735, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv_checkpoint/222-0.8273.hdf5\n",
      "36805/36805 [==============================] - 16s 425us/sample - loss: 1.1918 - acc: 0.6136 - val_loss: 0.8273 - val_acc: 0.7563\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 223/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1858 - acc: 0.6146\n",
      "Epoch 00223: val_loss did not improve from 0.82735\n",
      "36805/36805 [==============================] - 16s 424us/sample - loss: 1.1861 - acc: 0.6145 - val_loss: 0.8315 - val_acc: 0.7559\n",
      "Epoch 224/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1888 - acc: 0.6177\n",
      "Epoch 00224: val_loss did not improve from 0.82735\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 1.1888 - acc: 0.6176 - val_loss: 0.8495 - val_acc: 0.7494\n",
      "Epoch 225/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1856 - acc: 0.6151\n",
      "Epoch 00225: val_loss did not improve from 0.82735\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 1.1859 - acc: 0.6150 - val_loss: 0.8311 - val_acc: 0.7559\n",
      "Epoch 226/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1820 - acc: 0.6147\n",
      "Epoch 00226: val_loss did not improve from 0.82735\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 1.1820 - acc: 0.6145 - val_loss: 0.8339 - val_acc: 0.7543\n",
      "Epoch 227/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1792 - acc: 0.6185\n",
      "Epoch 00227: val_loss did not improve from 0.82735\n",
      "36805/36805 [==============================] - 16s 424us/sample - loss: 1.1791 - acc: 0.6186 - val_loss: 0.8313 - val_acc: 0.7561\n",
      "Epoch 228/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1807 - acc: 0.6162\n",
      "Epoch 00228: val_loss did not improve from 0.82735\n",
      "36805/36805 [==============================] - 16s 425us/sample - loss: 1.1804 - acc: 0.6162 - val_loss: 0.8432 - val_acc: 0.7531\n",
      "Epoch 229/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1852 - acc: 0.6174\n",
      "Epoch 00229: val_loss did not improve from 0.82735\n",
      "36805/36805 [==============================] - 16s 422us/sample - loss: 1.1853 - acc: 0.6173 - val_loss: 0.8289 - val_acc: 0.7545\n",
      "Epoch 230/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1894 - acc: 0.6150\n",
      "Epoch 00230: val_loss did not improve from 0.82735\n",
      "36805/36805 [==============================] - 16s 424us/sample - loss: 1.1894 - acc: 0.6151 - val_loss: 0.8394 - val_acc: 0.7522\n",
      "Epoch 231/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1828 - acc: 0.6183\n",
      "Epoch 00231: val_loss did not improve from 0.82735\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 1.1828 - acc: 0.6183 - val_loss: 0.8343 - val_acc: 0.7568\n",
      "Epoch 232/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1827 - acc: 0.6166\n",
      "Epoch 00232: val_loss did not improve from 0.82735\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 1.1826 - acc: 0.6168 - val_loss: 0.8298 - val_acc: 0.7531\n",
      "Epoch 233/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1807 - acc: 0.6210\n",
      "Epoch 00233: val_loss did not improve from 0.82735\n",
      "36805/36805 [==============================] - 16s 424us/sample - loss: 1.1806 - acc: 0.6210 - val_loss: 0.8328 - val_acc: 0.7573\n",
      "Epoch 234/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1814 - acc: 0.6143\n",
      "Epoch 00234: val_loss improved from 0.82735 to 0.81756, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv_checkpoint/234-0.8176.hdf5\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 1.1816 - acc: 0.6142 - val_loss: 0.8176 - val_acc: 0.7533\n",
      "Epoch 235/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1812 - acc: 0.6158\n",
      "Epoch 00235: val_loss did not improve from 0.81756\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 1.1809 - acc: 0.6159 - val_loss: 0.8191 - val_acc: 0.7554\n",
      "Epoch 236/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1818 - acc: 0.6186\n",
      "Epoch 00236: val_loss did not improve from 0.81756\n",
      "36805/36805 [==============================] - 16s 422us/sample - loss: 1.1818 - acc: 0.6186 - val_loss: 0.8190 - val_acc: 0.7568\n",
      "Epoch 237/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1813 - acc: 0.6172\n",
      "Epoch 00237: val_loss did not improve from 0.81756\n",
      "36805/36805 [==============================] - 16s 424us/sample - loss: 1.1814 - acc: 0.6171 - val_loss: 0.8229 - val_acc: 0.7561\n",
      "Epoch 238/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1751 - acc: 0.6213\n",
      "Epoch 00238: val_loss did not improve from 0.81756\n",
      "36805/36805 [==============================] - 16s 422us/sample - loss: 1.1748 - acc: 0.6213 - val_loss: 0.8192 - val_acc: 0.7573\n",
      "Epoch 239/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.1781 - acc: 0.6157\n",
      "Epoch 00239: val_loss did not improve from 0.81756\n",
      "36805/36805 [==============================] - 16s 424us/sample - loss: 1.1785 - acc: 0.6157 - val_loss: 0.8251 - val_acc: 0.7563\n",
      "Epoch 240/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1815 - acc: 0.6147\n",
      "Epoch 00240: val_loss did not improve from 0.81756\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 1.1814 - acc: 0.6147 - val_loss: 0.8336 - val_acc: 0.7589\n",
      "Epoch 241/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1727 - acc: 0.6190\n",
      "Epoch 00241: val_loss did not improve from 0.81756\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 1.1727 - acc: 0.6191 - val_loss: 0.8195 - val_acc: 0.7547\n",
      "Epoch 242/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1776 - acc: 0.6187\n",
      "Epoch 00242: val_loss did not improve from 0.81756\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 1.1778 - acc: 0.6187 - val_loss: 0.8248 - val_acc: 0.7570\n",
      "Epoch 243/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1701 - acc: 0.6216\n",
      "Epoch 00243: val_loss improved from 0.81756 to 0.81562, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv_checkpoint/243-0.8156.hdf5\n",
      "36805/36805 [==============================] - 16s 426us/sample - loss: 1.1701 - acc: 0.6216 - val_loss: 0.8156 - val_acc: 0.7584\n",
      "Epoch 244/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1771 - acc: 0.6173\n",
      "Epoch 00244: val_loss did not improve from 0.81562\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 1.1772 - acc: 0.6173 - val_loss: 0.8174 - val_acc: 0.7568\n",
      "Epoch 245/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1795 - acc: 0.6206\n",
      "Epoch 00245: val_loss did not improve from 0.81562\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 1.1793 - acc: 0.6206 - val_loss: 0.8190 - val_acc: 0.7556\n",
      "Epoch 246/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1812 - acc: 0.6153\n",
      "Epoch 00246: val_loss did not improve from 0.81562\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 1.1812 - acc: 0.6153 - val_loss: 0.8240 - val_acc: 0.7601\n",
      "Epoch 247/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1807 - acc: 0.6168\n",
      "Epoch 00247: val_loss improved from 0.81562 to 0.81270, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv_checkpoint/247-0.8127.hdf5\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 1.1807 - acc: 0.6168 - val_loss: 0.8127 - val_acc: 0.7594\n",
      "Epoch 248/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1738 - acc: 0.6175\n",
      "Epoch 00248: val_loss did not improve from 0.81270\n",
      "36805/36805 [==============================] - 16s 424us/sample - loss: 1.1740 - acc: 0.6175 - val_loss: 0.8145 - val_acc: 0.7552\n",
      "Epoch 249/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1733 - acc: 0.6198\n",
      "Epoch 00249: val_loss did not improve from 0.81270\n",
      "36805/36805 [==============================] - 16s 425us/sample - loss: 1.1736 - acc: 0.6197 - val_loss: 0.8254 - val_acc: 0.7545\n",
      "Epoch 250/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1721 - acc: 0.6230\n",
      "Epoch 00250: val_loss did not improve from 0.81270\n",
      "36805/36805 [==============================] - 16s 422us/sample - loss: 1.1721 - acc: 0.6230 - val_loss: 0.8223 - val_acc: 0.7587\n",
      "Epoch 251/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1734 - acc: 0.6249\n",
      "Epoch 00251: val_loss did not improve from 0.81270\n",
      "36805/36805 [==============================] - 16s 424us/sample - loss: 1.1733 - acc: 0.6250 - val_loss: 0.8161 - val_acc: 0.7580\n",
      "Epoch 252/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1751 - acc: 0.6224\n",
      "Epoch 00252: val_loss improved from 0.81270 to 0.81175, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv_checkpoint/252-0.8118.hdf5\n",
      "36805/36805 [==============================] - 16s 424us/sample - loss: 1.1751 - acc: 0.6224 - val_loss: 0.8118 - val_acc: 0.7561\n",
      "Epoch 253/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1707 - acc: 0.6214\n",
      "Epoch 00253: val_loss did not improve from 0.81175\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 1.1707 - acc: 0.6213 - val_loss: 0.8222 - val_acc: 0.7559\n",
      "Epoch 254/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1697 - acc: 0.6212\n",
      "Epoch 00254: val_loss did not improve from 0.81175\n",
      "36805/36805 [==============================] - 16s 422us/sample - loss: 1.1703 - acc: 0.6212 - val_loss: 0.8191 - val_acc: 0.7610\n",
      "Epoch 255/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1664 - acc: 0.6234\n",
      "Epoch 00255: val_loss did not improve from 0.81175\n",
      "36805/36805 [==============================] - 16s 421us/sample - loss: 1.1664 - acc: 0.6234 - val_loss: 0.8150 - val_acc: 0.7601\n",
      "Epoch 256/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1646 - acc: 0.6237\n",
      "Epoch 00256: val_loss improved from 0.81175 to 0.80784, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv_checkpoint/256-0.8078.hdf5\n",
      "36805/36805 [==============================] - 16s 422us/sample - loss: 1.1646 - acc: 0.6237 - val_loss: 0.8078 - val_acc: 0.7573\n",
      "Epoch 257/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1662 - acc: 0.6243\n",
      "Epoch 00257: val_loss did not improve from 0.80784\n",
      "36805/36805 [==============================] - 16s 422us/sample - loss: 1.1660 - acc: 0.6245 - val_loss: 0.8216 - val_acc: 0.7603\n",
      "Epoch 258/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1637 - acc: 0.6230\n",
      "Epoch 00258: val_loss did not improve from 0.80784\n",
      "36805/36805 [==============================] - 16s 422us/sample - loss: 1.1636 - acc: 0.6230 - val_loss: 0.8220 - val_acc: 0.7570\n",
      "Epoch 259/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1626 - acc: 0.6220\n",
      "Epoch 00259: val_loss did not improve from 0.80784\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 1.1626 - acc: 0.6218 - val_loss: 0.8087 - val_acc: 0.7631\n",
      "Epoch 260/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1687 - acc: 0.6224\n",
      "Epoch 00260: val_loss did not improve from 0.80784\n",
      "36805/36805 [==============================] - 16s 435us/sample - loss: 1.1687 - acc: 0.6223 - val_loss: 0.8128 - val_acc: 0.7594\n",
      "Epoch 261/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1640 - acc: 0.6201\n",
      "Epoch 00261: val_loss did not improve from 0.80784\n",
      "36805/36805 [==============================] - 16s 421us/sample - loss: 1.1640 - acc: 0.6202 - val_loss: 0.8184 - val_acc: 0.7573\n",
      "Epoch 262/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1687 - acc: 0.6220\n",
      "Epoch 00262: val_loss did not improve from 0.80784\n",
      "36805/36805 [==============================] - 15s 420us/sample - loss: 1.1684 - acc: 0.6221 - val_loss: 0.8082 - val_acc: 0.7601\n",
      "Epoch 263/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1635 - acc: 0.6240\n",
      "Epoch 00263: val_loss did not improve from 0.80784\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 1.1634 - acc: 0.6241 - val_loss: 0.8163 - val_acc: 0.7561\n",
      "Epoch 264/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1645 - acc: 0.6236\n",
      "Epoch 00264: val_loss did not improve from 0.80784\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 1.1644 - acc: 0.6235 - val_loss: 0.8172 - val_acc: 0.7619\n",
      "Epoch 265/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1696 - acc: 0.6207\n",
      "Epoch 00265: val_loss did not improve from 0.80784\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 1.1696 - acc: 0.6206 - val_loss: 0.8144 - val_acc: 0.7577\n",
      "Epoch 266/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1742 - acc: 0.6233\n",
      "Epoch 00266: val_loss improved from 0.80784 to 0.80616, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv_checkpoint/266-0.8062.hdf5\n",
      "36805/36805 [==============================] - 16s 422us/sample - loss: 1.1743 - acc: 0.6232 - val_loss: 0.8062 - val_acc: 0.7580\n",
      "Epoch 267/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1622 - acc: 0.6208\n",
      "Epoch 00267: val_loss did not improve from 0.80616\n",
      "36805/36805 [==============================] - 16s 422us/sample - loss: 1.1620 - acc: 0.6209 - val_loss: 0.8122 - val_acc: 0.7573\n",
      "Epoch 268/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1629 - acc: 0.6220\n",
      "Epoch 00268: val_loss did not improve from 0.80616\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 1.1628 - acc: 0.6220 - val_loss: 0.8155 - val_acc: 0.7568\n",
      "Epoch 269/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1623 - acc: 0.6217\n",
      "Epoch 00269: val_loss did not improve from 0.80616\n",
      "36805/36805 [==============================] - 16s 421us/sample - loss: 1.1623 - acc: 0.6218 - val_loss: 0.8135 - val_acc: 0.7577\n",
      "Epoch 270/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1628 - acc: 0.6211\n",
      "Epoch 00270: val_loss did not improve from 0.80616\n",
      "36805/36805 [==============================] - 16s 421us/sample - loss: 1.1628 - acc: 0.6211 - val_loss: 0.8171 - val_acc: 0.7570\n",
      "Epoch 271/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1595 - acc: 0.6231\n",
      "Epoch 00271: val_loss did not improve from 0.80616\n",
      "36805/36805 [==============================] - 16s 422us/sample - loss: 1.1597 - acc: 0.6231 - val_loss: 0.8092 - val_acc: 0.7596\n",
      "Epoch 272/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1590 - acc: 0.6238\n",
      "Epoch 00272: val_loss improved from 0.80616 to 0.80355, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv_checkpoint/272-0.8036.hdf5\n",
      "36805/36805 [==============================] - 16s 424us/sample - loss: 1.1588 - acc: 0.6238 - val_loss: 0.8036 - val_acc: 0.7633\n",
      "Epoch 273/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1598 - acc: 0.6247\n",
      "Epoch 00273: val_loss did not improve from 0.80355\n",
      "36805/36805 [==============================] - 16s 424us/sample - loss: 1.1599 - acc: 0.6247 - val_loss: 0.8132 - val_acc: 0.7549\n",
      "Epoch 274/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1572 - acc: 0.6261\n",
      "Epoch 00274: val_loss did not improve from 0.80355\n",
      "36805/36805 [==============================] - 16s 422us/sample - loss: 1.1572 - acc: 0.6261 - val_loss: 0.8070 - val_acc: 0.7587\n",
      "Epoch 275/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1538 - acc: 0.6280\n",
      "Epoch 00275: val_loss did not improve from 0.80355\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 1.1538 - acc: 0.6280 - val_loss: 0.8140 - val_acc: 0.7587\n",
      "Epoch 276/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1609 - acc: 0.6230\n",
      "Epoch 00276: val_loss did not improve from 0.80355\n",
      "36805/36805 [==============================] - 16s 424us/sample - loss: 1.1608 - acc: 0.6230 - val_loss: 0.8109 - val_acc: 0.7601\n",
      "Epoch 277/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1569 - acc: 0.6250\n",
      "Epoch 00277: val_loss did not improve from 0.80355\n",
      "36805/36805 [==============================] - 16s 422us/sample - loss: 1.1568 - acc: 0.6251 - val_loss: 0.8150 - val_acc: 0.7608\n",
      "Epoch 278/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1606 - acc: 0.6250\n",
      "Epoch 00278: val_loss improved from 0.80355 to 0.80141, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv_checkpoint/278-0.8014.hdf5\n",
      "36805/36805 [==============================] - 16s 427us/sample - loss: 1.1604 - acc: 0.6252 - val_loss: 0.8014 - val_acc: 0.7629\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 279/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1606 - acc: 0.6224\n",
      "Epoch 00279: val_loss improved from 0.80141 to 0.80043, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv_checkpoint/279-0.8004.hdf5\n",
      "36805/36805 [==============================] - 16s 425us/sample - loss: 1.1605 - acc: 0.6224 - val_loss: 0.8004 - val_acc: 0.7643\n",
      "Epoch 280/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1542 - acc: 0.6243\n",
      "Epoch 00280: val_loss did not improve from 0.80043\n",
      "36805/36805 [==============================] - 15s 421us/sample - loss: 1.1545 - acc: 0.6243 - val_loss: 0.8092 - val_acc: 0.7624\n",
      "Epoch 281/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1553 - acc: 0.6248\n",
      "Epoch 00281: val_loss did not improve from 0.80043\n",
      "36805/36805 [==============================] - 16s 422us/sample - loss: 1.1553 - acc: 0.6248 - val_loss: 0.8080 - val_acc: 0.7668\n",
      "Epoch 282/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1530 - acc: 0.6268\n",
      "Epoch 00282: val_loss did not improve from 0.80043\n",
      "36805/36805 [==============================] - 16s 424us/sample - loss: 1.1531 - acc: 0.6268 - val_loss: 0.8052 - val_acc: 0.7615\n",
      "Epoch 283/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1621 - acc: 0.6235\n",
      "Epoch 00283: val_loss did not improve from 0.80043\n",
      "36805/36805 [==============================] - 16s 424us/sample - loss: 1.1623 - acc: 0.6235 - val_loss: 0.8095 - val_acc: 0.7626\n",
      "Epoch 284/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1559 - acc: 0.6245\n",
      "Epoch 00284: val_loss did not improve from 0.80043\n",
      "36805/36805 [==============================] - 16s 422us/sample - loss: 1.1559 - acc: 0.6245 - val_loss: 0.8075 - val_acc: 0.7624\n",
      "Epoch 285/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1518 - acc: 0.6260\n",
      "Epoch 00285: val_loss did not improve from 0.80043\n",
      "36805/36805 [==============================] - 16s 425us/sample - loss: 1.1515 - acc: 0.6260 - val_loss: 0.8082 - val_acc: 0.7591\n",
      "Epoch 286/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1474 - acc: 0.6258\n",
      "Epoch 00286: val_loss did not improve from 0.80043\n",
      "36805/36805 [==============================] - 16s 426us/sample - loss: 1.1474 - acc: 0.6258 - val_loss: 0.8027 - val_acc: 0.7622\n",
      "Epoch 287/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1545 - acc: 0.6281\n",
      "Epoch 00287: val_loss did not improve from 0.80043\n",
      "36805/36805 [==============================] - 16s 425us/sample - loss: 1.1545 - acc: 0.6282 - val_loss: 0.8052 - val_acc: 0.7589\n",
      "Epoch 288/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1536 - acc: 0.6258\n",
      "Epoch 00288: val_loss did not improve from 0.80043\n",
      "36805/36805 [==============================] - 16s 424us/sample - loss: 1.1536 - acc: 0.6258 - val_loss: 0.8128 - val_acc: 0.7617\n",
      "Epoch 289/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1548 - acc: 0.6263\n",
      "Epoch 00289: val_loss did not improve from 0.80043\n",
      "36805/36805 [==============================] - 16s 424us/sample - loss: 1.1543 - acc: 0.6264 - val_loss: 0.8162 - val_acc: 0.7626\n",
      "Epoch 290/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1483 - acc: 0.6306\n",
      "Epoch 00290: val_loss did not improve from 0.80043\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 1.1481 - acc: 0.6306 - val_loss: 0.8037 - val_acc: 0.7591\n",
      "Epoch 291/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1522 - acc: 0.6290\n",
      "Epoch 00291: val_loss did not improve from 0.80043\n",
      "36805/36805 [==============================] - 16s 425us/sample - loss: 1.1528 - acc: 0.6289 - val_loss: 0.8114 - val_acc: 0.7629\n",
      "Epoch 292/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1499 - acc: 0.6280\n",
      "Epoch 00292: val_loss did not improve from 0.80043\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 1.1498 - acc: 0.6280 - val_loss: 0.8025 - val_acc: 0.7650\n",
      "Epoch 293/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1582 - acc: 0.6246\n",
      "Epoch 00293: val_loss did not improve from 0.80043\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 1.1582 - acc: 0.6246 - val_loss: 0.8032 - val_acc: 0.7610\n",
      "Epoch 294/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1509 - acc: 0.6287\n",
      "Epoch 00294: val_loss improved from 0.80043 to 0.79996, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv_checkpoint/294-0.8000.hdf5\n",
      "36805/36805 [==============================] - 16s 425us/sample - loss: 1.1508 - acc: 0.6287 - val_loss: 0.8000 - val_acc: 0.7640\n",
      "Epoch 295/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1502 - acc: 0.6276\n",
      "Epoch 00295: val_loss improved from 0.79996 to 0.79953, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv_checkpoint/295-0.7995.hdf5\n",
      "36805/36805 [==============================] - 16s 424us/sample - loss: 1.1504 - acc: 0.6276 - val_loss: 0.7995 - val_acc: 0.7615\n",
      "Epoch 296/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1556 - acc: 0.6275\n",
      "Epoch 00296: val_loss did not improve from 0.79953\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 1.1557 - acc: 0.6274 - val_loss: 0.8018 - val_acc: 0.7624\n",
      "Epoch 297/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1539 - acc: 0.6249\n",
      "Epoch 00297: val_loss did not improve from 0.79953\n",
      "36805/36805 [==============================] - 16s 424us/sample - loss: 1.1539 - acc: 0.6250 - val_loss: 0.8152 - val_acc: 0.7605\n",
      "Epoch 298/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1463 - acc: 0.6264\n",
      "Epoch 00298: val_loss improved from 0.79953 to 0.79263, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv_checkpoint/298-0.7926.hdf5\n",
      "36805/36805 [==============================] - 16s 425us/sample - loss: 1.1463 - acc: 0.6264 - val_loss: 0.7926 - val_acc: 0.7650\n",
      "Epoch 299/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1531 - acc: 0.6254\n",
      "Epoch 00299: val_loss did not improve from 0.79263\n",
      "36805/36805 [==============================] - 16s 424us/sample - loss: 1.1532 - acc: 0.6253 - val_loss: 0.8105 - val_acc: 0.7615\n",
      "Epoch 300/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1485 - acc: 0.6264\n",
      "Epoch 00300: val_loss did not improve from 0.79263\n",
      "36805/36805 [==============================] - 16s 422us/sample - loss: 1.1487 - acc: 0.6265 - val_loss: 0.7965 - val_acc: 0.7622\n",
      "Epoch 301/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1555 - acc: 0.6275\n",
      "Epoch 00301: val_loss did not improve from 0.79263\n",
      "36805/36805 [==============================] - 15s 420us/sample - loss: 1.1556 - acc: 0.6275 - val_loss: 0.7982 - val_acc: 0.7626\n",
      "Epoch 302/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1489 - acc: 0.6259\n",
      "Epoch 00302: val_loss did not improve from 0.79263\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 1.1492 - acc: 0.6258 - val_loss: 0.8061 - val_acc: 0.7612\n",
      "Epoch 303/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1534 - acc: 0.6252\n",
      "Epoch 00303: val_loss did not improve from 0.79263\n",
      "36805/36805 [==============================] - 15s 421us/sample - loss: 1.1534 - acc: 0.6252 - val_loss: 0.7940 - val_acc: 0.7587\n",
      "Epoch 304/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1479 - acc: 0.6274\n",
      "Epoch 00304: val_loss did not improve from 0.79263\n",
      "36805/36805 [==============================] - 16s 422us/sample - loss: 1.1479 - acc: 0.6274 - val_loss: 0.7990 - val_acc: 0.7661\n",
      "Epoch 305/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1522 - acc: 0.6279\n",
      "Epoch 00305: val_loss did not improve from 0.79263\n",
      "36805/36805 [==============================] - 16s 425us/sample - loss: 1.1521 - acc: 0.6280 - val_loss: 0.7968 - val_acc: 0.7640\n",
      "Epoch 306/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1466 - acc: 0.6305\n",
      "Epoch 00306: val_loss did not improve from 0.79263\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 1.1468 - acc: 0.6305 - val_loss: 0.8053 - val_acc: 0.7636\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 307/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1569 - acc: 0.6267\n",
      "Epoch 00307: val_loss did not improve from 0.79263\n",
      "36805/36805 [==============================] - 16s 422us/sample - loss: 1.1569 - acc: 0.6268 - val_loss: 0.8039 - val_acc: 0.7624\n",
      "Epoch 308/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1449 - acc: 0.6299\n",
      "Epoch 00308: val_loss did not improve from 0.79263\n",
      "36805/36805 [==============================] - 16s 425us/sample - loss: 1.1451 - acc: 0.6299 - val_loss: 0.7949 - val_acc: 0.7638\n",
      "Epoch 309/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1400 - acc: 0.6280\n",
      "Epoch 00309: val_loss did not improve from 0.79263\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 1.1401 - acc: 0.6280 - val_loss: 0.7935 - val_acc: 0.7643\n",
      "Epoch 310/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1465 - acc: 0.6306\n",
      "Epoch 00310: val_loss did not improve from 0.79263\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 1.1466 - acc: 0.6306 - val_loss: 0.8048 - val_acc: 0.7629\n",
      "Epoch 311/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1464 - acc: 0.6276\n",
      "Epoch 00311: val_loss did not improve from 0.79263\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 1.1462 - acc: 0.6278 - val_loss: 0.8088 - val_acc: 0.7615\n",
      "Epoch 312/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1474 - acc: 0.6296\n",
      "Epoch 00312: val_loss did not improve from 0.79263\n",
      "36805/36805 [==============================] - 15s 421us/sample - loss: 1.1473 - acc: 0.6297 - val_loss: 0.8035 - val_acc: 0.7640\n",
      "Epoch 313/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1453 - acc: 0.6275\n",
      "Epoch 00313: val_loss improved from 0.79263 to 0.79154, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv_checkpoint/313-0.7915.hdf5\n",
      "36805/36805 [==============================] - 16s 424us/sample - loss: 1.1454 - acc: 0.6274 - val_loss: 0.7915 - val_acc: 0.7615\n",
      "Epoch 314/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1545 - acc: 0.6254\n",
      "Epoch 00314: val_loss did not improve from 0.79154\n",
      "36805/36805 [==============================] - 16s 422us/sample - loss: 1.1540 - acc: 0.6256 - val_loss: 0.7954 - val_acc: 0.7654\n",
      "Epoch 315/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1485 - acc: 0.6279\n",
      "Epoch 00315: val_loss did not improve from 0.79154\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 1.1485 - acc: 0.6279 - val_loss: 0.8020 - val_acc: 0.7615\n",
      "Epoch 316/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.1449 - acc: 0.6281\n",
      "Epoch 00316: val_loss did not improve from 0.79154\n",
      "36805/36805 [==============================] - 16s 425us/sample - loss: 1.1449 - acc: 0.6280 - val_loss: 0.7944 - val_acc: 0.7617\n",
      "Epoch 317/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1406 - acc: 0.6303\n",
      "Epoch 00317: val_loss improved from 0.79154 to 0.78975, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv_checkpoint/317-0.7898.hdf5\n",
      "36805/36805 [==============================] - 16s 425us/sample - loss: 1.1409 - acc: 0.6301 - val_loss: 0.7898 - val_acc: 0.7638\n",
      "Epoch 318/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1457 - acc: 0.6284\n",
      "Epoch 00318: val_loss did not improve from 0.78975\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 1.1457 - acc: 0.6284 - val_loss: 0.7980 - val_acc: 0.7643\n",
      "Epoch 319/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1446 - acc: 0.6292\n",
      "Epoch 00319: val_loss did not improve from 0.78975\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 1.1445 - acc: 0.6292 - val_loss: 0.7954 - val_acc: 0.7640\n",
      "Epoch 320/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1387 - acc: 0.6290\n",
      "Epoch 00320: val_loss did not improve from 0.78975\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 1.1385 - acc: 0.6291 - val_loss: 0.7968 - val_acc: 0.7617\n",
      "Epoch 321/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1325 - acc: 0.6305\n",
      "Epoch 00321: val_loss did not improve from 0.78975\n",
      "36805/36805 [==============================] - 16s 426us/sample - loss: 1.1330 - acc: 0.6304 - val_loss: 0.8037 - val_acc: 0.7612\n",
      "Epoch 322/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1376 - acc: 0.6299\n",
      "Epoch 00322: val_loss did not improve from 0.78975\n",
      "36805/36805 [==============================] - 16s 424us/sample - loss: 1.1375 - acc: 0.6299 - val_loss: 0.7907 - val_acc: 0.7603\n",
      "Epoch 323/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1447 - acc: 0.6277\n",
      "Epoch 00323: val_loss did not improve from 0.78975\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 1.1447 - acc: 0.6277 - val_loss: 0.8005 - val_acc: 0.7617\n",
      "Epoch 324/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1409 - acc: 0.6288\n",
      "Epoch 00324: val_loss did not improve from 0.78975\n",
      "36805/36805 [==============================] - 16s 424us/sample - loss: 1.1407 - acc: 0.6287 - val_loss: 0.7926 - val_acc: 0.7652\n",
      "Epoch 325/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1429 - acc: 0.6283\n",
      "Epoch 00325: val_loss did not improve from 0.78975\n",
      "36805/36805 [==============================] - 16s 425us/sample - loss: 1.1434 - acc: 0.6281 - val_loss: 0.8021 - val_acc: 0.7624\n",
      "Epoch 326/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1375 - acc: 0.6289\n",
      "Epoch 00326: val_loss improved from 0.78975 to 0.78943, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv_checkpoint/326-0.7894.hdf5\n",
      "36805/36805 [==============================] - 16s 425us/sample - loss: 1.1375 - acc: 0.6289 - val_loss: 0.7894 - val_acc: 0.7575\n",
      "Epoch 327/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1377 - acc: 0.6324\n",
      "Epoch 00327: val_loss did not improve from 0.78943\n",
      "36805/36805 [==============================] - 16s 422us/sample - loss: 1.1375 - acc: 0.6326 - val_loss: 0.7908 - val_acc: 0.7619\n",
      "Epoch 328/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1366 - acc: 0.6308\n",
      "Epoch 00328: val_loss did not improve from 0.78943\n",
      "36805/36805 [==============================] - 16s 424us/sample - loss: 1.1367 - acc: 0.6308 - val_loss: 0.7917 - val_acc: 0.7654\n",
      "Epoch 329/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1400 - acc: 0.6308\n",
      "Epoch 00329: val_loss did not improve from 0.78943\n",
      "36805/36805 [==============================] - 16s 422us/sample - loss: 1.1405 - acc: 0.6306 - val_loss: 0.8027 - val_acc: 0.7636\n",
      "Epoch 330/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1355 - acc: 0.6312\n",
      "Epoch 00330: val_loss did not improve from 0.78943\n",
      "36805/36805 [==============================] - 16s 421us/sample - loss: 1.1356 - acc: 0.6312 - val_loss: 0.7902 - val_acc: 0.7659\n",
      "Epoch 331/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1388 - acc: 0.6305\n",
      "Epoch 00331: val_loss did not improve from 0.78943\n",
      "36805/36805 [==============================] - 16s 424us/sample - loss: 1.1386 - acc: 0.6305 - val_loss: 0.7901 - val_acc: 0.7617\n",
      "Epoch 332/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1424 - acc: 0.6322\n",
      "Epoch 00332: val_loss improved from 0.78943 to 0.78507, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv_checkpoint/332-0.7851.hdf5\n",
      "36805/36805 [==============================] - 16s 425us/sample - loss: 1.1423 - acc: 0.6322 - val_loss: 0.7851 - val_acc: 0.7629\n",
      "Epoch 333/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1381 - acc: 0.6302\n",
      "Epoch 00333: val_loss did not improve from 0.78507\n",
      "36805/36805 [==============================] - 16s 421us/sample - loss: 1.1381 - acc: 0.6302 - val_loss: 0.7948 - val_acc: 0.7650\n",
      "Epoch 334/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1370 - acc: 0.6325\n",
      "Epoch 00334: val_loss did not improve from 0.78507\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 1.1370 - acc: 0.6326 - val_loss: 0.7955 - val_acc: 0.7675\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 335/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1346 - acc: 0.6309\n",
      "Epoch 00335: val_loss did not improve from 0.78507\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 1.1347 - acc: 0.6309 - val_loss: 0.7903 - val_acc: 0.7657\n",
      "Epoch 336/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1394 - acc: 0.6311\n",
      "Epoch 00336: val_loss did not improve from 0.78507\n",
      "36805/36805 [==============================] - 16s 422us/sample - loss: 1.1393 - acc: 0.6311 - val_loss: 0.7904 - val_acc: 0.7664\n",
      "Epoch 337/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1307 - acc: 0.6348\n",
      "Epoch 00337: val_loss did not improve from 0.78507\n",
      "36805/36805 [==============================] - 16s 424us/sample - loss: 1.1307 - acc: 0.6348 - val_loss: 0.7900 - val_acc: 0.7624\n",
      "Epoch 338/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1355 - acc: 0.6333\n",
      "Epoch 00338: val_loss did not improve from 0.78507\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 1.1355 - acc: 0.6333 - val_loss: 0.7868 - val_acc: 0.7659\n",
      "Epoch 339/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1347 - acc: 0.6331\n",
      "Epoch 00339: val_loss did not improve from 0.78507\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 1.1346 - acc: 0.6331 - val_loss: 0.7892 - val_acc: 0.7661\n",
      "Epoch 340/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.1347 - acc: 0.6333\n",
      "Epoch 00340: val_loss improved from 0.78507 to 0.78231, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv_checkpoint/340-0.7823.hdf5\n",
      "36805/36805 [==============================] - 16s 427us/sample - loss: 1.1345 - acc: 0.6335 - val_loss: 0.7823 - val_acc: 0.7638\n",
      "Epoch 341/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1338 - acc: 0.6345\n",
      "Epoch 00341: val_loss did not improve from 0.78231\n",
      "36805/36805 [==============================] - 15s 421us/sample - loss: 1.1337 - acc: 0.6346 - val_loss: 0.7897 - val_acc: 0.7668\n",
      "Epoch 342/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1413 - acc: 0.6300\n",
      "Epoch 00342: val_loss did not improve from 0.78231\n",
      "36805/36805 [==============================] - 16s 422us/sample - loss: 1.1415 - acc: 0.6299 - val_loss: 0.7858 - val_acc: 0.7680\n",
      "Epoch 343/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1278 - acc: 0.6339\n",
      "Epoch 00343: val_loss did not improve from 0.78231\n",
      "36805/36805 [==============================] - 15s 421us/sample - loss: 1.1276 - acc: 0.6339 - val_loss: 0.7831 - val_acc: 0.7675\n",
      "Epoch 344/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1415 - acc: 0.6309\n",
      "Epoch 00344: val_loss improved from 0.78231 to 0.77855, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv_checkpoint/344-0.7786.hdf5\n",
      "36805/36805 [==============================] - 16s 425us/sample - loss: 1.1414 - acc: 0.6309 - val_loss: 0.7786 - val_acc: 0.7680\n",
      "Epoch 345/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1374 - acc: 0.6315\n",
      "Epoch 00345: val_loss did not improve from 0.77855\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 1.1376 - acc: 0.6314 - val_loss: 0.7824 - val_acc: 0.7643\n",
      "Epoch 346/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1324 - acc: 0.6326\n",
      "Epoch 00346: val_loss did not improve from 0.77855\n",
      "36805/36805 [==============================] - 15s 420us/sample - loss: 1.1322 - acc: 0.6326 - val_loss: 0.7928 - val_acc: 0.7661\n",
      "Epoch 347/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1327 - acc: 0.6309\n",
      "Epoch 00347: val_loss did not improve from 0.77855\n",
      "36805/36805 [==============================] - 16s 424us/sample - loss: 1.1326 - acc: 0.6311 - val_loss: 0.7992 - val_acc: 0.7640\n",
      "Epoch 348/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1343 - acc: 0.6325\n",
      "Epoch 00348: val_loss did not improve from 0.77855\n",
      "36805/36805 [==============================] - 15s 420us/sample - loss: 1.1343 - acc: 0.6325 - val_loss: 0.7859 - val_acc: 0.7671\n",
      "Epoch 349/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1378 - acc: 0.6301\n",
      "Epoch 00349: val_loss did not improve from 0.77855\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 1.1378 - acc: 0.6301 - val_loss: 0.7969 - val_acc: 0.7624\n",
      "Epoch 350/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1318 - acc: 0.6306\n",
      "Epoch 00350: val_loss did not improve from 0.77855\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 1.1313 - acc: 0.6308 - val_loss: 0.7869 - val_acc: 0.7666\n",
      "Epoch 351/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1280 - acc: 0.6351\n",
      "Epoch 00351: val_loss did not improve from 0.77855\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 1.1280 - acc: 0.6350 - val_loss: 0.7797 - val_acc: 0.7706\n",
      "Epoch 352/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1313 - acc: 0.6364\n",
      "Epoch 00352: val_loss did not improve from 0.77855\n",
      "36805/36805 [==============================] - 15s 420us/sample - loss: 1.1318 - acc: 0.6362 - val_loss: 0.7869 - val_acc: 0.7668\n",
      "Epoch 353/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1300 - acc: 0.6337\n",
      "Epoch 00353: val_loss did not improve from 0.77855\n",
      "36805/36805 [==============================] - 15s 420us/sample - loss: 1.1300 - acc: 0.6337 - val_loss: 0.7807 - val_acc: 0.7717\n",
      "Epoch 354/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1346 - acc: 0.6305\n",
      "Epoch 00354: val_loss did not improve from 0.77855\n",
      "36805/36805 [==============================] - 15s 421us/sample - loss: 1.1347 - acc: 0.6306 - val_loss: 0.7913 - val_acc: 0.7629\n",
      "Epoch 355/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1319 - acc: 0.6324\n",
      "Epoch 00355: val_loss did not improve from 0.77855\n",
      "36805/36805 [==============================] - 15s 421us/sample - loss: 1.1319 - acc: 0.6324 - val_loss: 0.7795 - val_acc: 0.7638\n",
      "Epoch 356/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1262 - acc: 0.6344\n",
      "Epoch 00356: val_loss did not improve from 0.77855\n",
      "36805/36805 [==============================] - 16s 422us/sample - loss: 1.1260 - acc: 0.6345 - val_loss: 0.7884 - val_acc: 0.7675\n",
      "Epoch 357/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1304 - acc: 0.6344\n",
      "Epoch 00357: val_loss improved from 0.77855 to 0.77739, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv_checkpoint/357-0.7774.hdf5\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 1.1304 - acc: 0.6344 - val_loss: 0.7774 - val_acc: 0.7652\n",
      "Epoch 358/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1296 - acc: 0.6333\n",
      "Epoch 00358: val_loss improved from 0.77739 to 0.77737, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv_checkpoint/358-0.7774.hdf5\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 1.1293 - acc: 0.6334 - val_loss: 0.7774 - val_acc: 0.7675\n",
      "Epoch 359/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1295 - acc: 0.6361\n",
      "Epoch 00359: val_loss improved from 0.77737 to 0.77305, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv_checkpoint/359-0.7731.hdf5\n",
      "36805/36805 [==============================] - 16s 424us/sample - loss: 1.1294 - acc: 0.6361 - val_loss: 0.7731 - val_acc: 0.7680\n",
      "Epoch 360/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1297 - acc: 0.6341\n",
      "Epoch 00360: val_loss did not improve from 0.77305\n",
      "36805/36805 [==============================] - 15s 420us/sample - loss: 1.1290 - acc: 0.6343 - val_loss: 0.7770 - val_acc: 0.7675\n",
      "Epoch 361/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1281 - acc: 0.6344\n",
      "Epoch 00361: val_loss did not improve from 0.77305\n",
      "36805/36805 [==============================] - 16s 424us/sample - loss: 1.1280 - acc: 0.6345 - val_loss: 0.7840 - val_acc: 0.7643\n",
      "Epoch 362/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1347 - acc: 0.6320\n",
      "Epoch 00362: val_loss did not improve from 0.77305\n",
      "36805/36805 [==============================] - 16s 424us/sample - loss: 1.1347 - acc: 0.6320 - val_loss: 0.7853 - val_acc: 0.7678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 363/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1325 - acc: 0.6368\n",
      "Epoch 00363: val_loss did not improve from 0.77305\n",
      "36805/36805 [==============================] - 16s 422us/sample - loss: 1.1322 - acc: 0.6369 - val_loss: 0.7778 - val_acc: 0.7701\n",
      "Epoch 364/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1357 - acc: 0.6338\n",
      "Epoch 00364: val_loss did not improve from 0.77305\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 1.1357 - acc: 0.6339 - val_loss: 0.7930 - val_acc: 0.7615\n",
      "Epoch 365/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1325 - acc: 0.6323\n",
      "Epoch 00365: val_loss did not improve from 0.77305\n",
      "36805/36805 [==============================] - 16s 422us/sample - loss: 1.1326 - acc: 0.6323 - val_loss: 0.7810 - val_acc: 0.7678\n",
      "Epoch 366/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1278 - acc: 0.6341\n",
      "Epoch 00366: val_loss did not improve from 0.77305\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 1.1277 - acc: 0.6341 - val_loss: 0.7774 - val_acc: 0.7673\n",
      "Epoch 367/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1313 - acc: 0.6323\n",
      "Epoch 00367: val_loss did not improve from 0.77305\n",
      "36805/36805 [==============================] - 15s 421us/sample - loss: 1.1310 - acc: 0.6324 - val_loss: 0.7790 - val_acc: 0.7673\n",
      "Epoch 368/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1248 - acc: 0.6371\n",
      "Epoch 00368: val_loss did not improve from 0.77305\n",
      "36805/36805 [==============================] - 15s 420us/sample - loss: 1.1247 - acc: 0.6371 - val_loss: 0.7845 - val_acc: 0.7689\n",
      "Epoch 369/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1253 - acc: 0.6350\n",
      "Epoch 00369: val_loss did not improve from 0.77305\n",
      "36805/36805 [==============================] - 16s 422us/sample - loss: 1.1250 - acc: 0.6351 - val_loss: 0.7741 - val_acc: 0.7722\n",
      "Epoch 370/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1253 - acc: 0.6354\n",
      "Epoch 00370: val_loss did not improve from 0.77305\n",
      "36805/36805 [==============================] - 15s 421us/sample - loss: 1.1252 - acc: 0.6353 - val_loss: 0.7929 - val_acc: 0.7638\n",
      "Epoch 371/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1257 - acc: 0.6346\n",
      "Epoch 00371: val_loss did not improve from 0.77305\n",
      "36805/36805 [==============================] - 16s 424us/sample - loss: 1.1255 - acc: 0.6347 - val_loss: 0.7825 - val_acc: 0.7675\n",
      "Epoch 372/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1235 - acc: 0.6353\n",
      "Epoch 00372: val_loss did not improve from 0.77305\n",
      "36805/36805 [==============================] - 16s 421us/sample - loss: 1.1237 - acc: 0.6352 - val_loss: 0.7812 - val_acc: 0.7666\n",
      "Epoch 373/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1297 - acc: 0.6319\n",
      "Epoch 00373: val_loss did not improve from 0.77305\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 1.1297 - acc: 0.6320 - val_loss: 0.7762 - val_acc: 0.7629\n",
      "Epoch 374/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1186 - acc: 0.6369\n",
      "Epoch 00374: val_loss did not improve from 0.77305\n",
      "36805/36805 [==============================] - 16s 424us/sample - loss: 1.1187 - acc: 0.6368 - val_loss: 0.7771 - val_acc: 0.7696\n",
      "Epoch 375/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1219 - acc: 0.6368\n",
      "Epoch 00375: val_loss did not improve from 0.77305\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 1.1222 - acc: 0.6367 - val_loss: 0.7814 - val_acc: 0.7664\n",
      "Epoch 376/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1251 - acc: 0.6352\n",
      "Epoch 00376: val_loss did not improve from 0.77305\n",
      "36805/36805 [==============================] - 15s 421us/sample - loss: 1.1251 - acc: 0.6352 - val_loss: 0.7737 - val_acc: 0.7706\n",
      "Epoch 377/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1302 - acc: 0.6311\n",
      "Epoch 00377: val_loss did not improve from 0.77305\n",
      "36805/36805 [==============================] - 15s 420us/sample - loss: 1.1300 - acc: 0.6312 - val_loss: 0.7741 - val_acc: 0.7668\n",
      "Epoch 378/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1293 - acc: 0.6329\n",
      "Epoch 00378: val_loss did not improve from 0.77305\n",
      "36805/36805 [==============================] - 16s 422us/sample - loss: 1.1294 - acc: 0.6328 - val_loss: 0.7899 - val_acc: 0.7659\n",
      "Epoch 379/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1219 - acc: 0.6351\n",
      "Epoch 00379: val_loss did not improve from 0.77305\n",
      "36805/36805 [==============================] - 16s 422us/sample - loss: 1.1220 - acc: 0.6350 - val_loss: 0.7791 - val_acc: 0.7715\n",
      "Epoch 380/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1252 - acc: 0.6368\n",
      "Epoch 00380: val_loss did not improve from 0.77305\n",
      "36805/36805 [==============================] - 15s 421us/sample - loss: 1.1252 - acc: 0.6368 - val_loss: 0.7910 - val_acc: 0.7673\n",
      "Epoch 381/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1213 - acc: 0.6356\n",
      "Epoch 00381: val_loss did not improve from 0.77305\n",
      "36805/36805 [==============================] - 16s 422us/sample - loss: 1.1209 - acc: 0.6357 - val_loss: 0.7899 - val_acc: 0.7645\n",
      "Epoch 382/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1240 - acc: 0.6336\n",
      "Epoch 00382: val_loss did not improve from 0.77305\n",
      "36805/36805 [==============================] - 15s 417us/sample - loss: 1.1240 - acc: 0.6337 - val_loss: 0.7806 - val_acc: 0.7680\n",
      "Epoch 383/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1227 - acc: 0.6384\n",
      "Epoch 00383: val_loss did not improve from 0.77305\n",
      "36805/36805 [==============================] - 16s 425us/sample - loss: 1.1225 - acc: 0.6385 - val_loss: 0.7775 - val_acc: 0.7715\n",
      "Epoch 384/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1163 - acc: 0.6379\n",
      "Epoch 00384: val_loss did not improve from 0.77305\n",
      "36805/36805 [==============================] - 16s 426us/sample - loss: 1.1165 - acc: 0.6379 - val_loss: 0.7749 - val_acc: 0.7696\n",
      "Epoch 385/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1247 - acc: 0.6356\n",
      "Epoch 00385: val_loss did not improve from 0.77305\n",
      "36805/36805 [==============================] - 16s 425us/sample - loss: 1.1248 - acc: 0.6356 - val_loss: 0.7735 - val_acc: 0.7706\n",
      "Epoch 386/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1260 - acc: 0.6333\n",
      "Epoch 00386: val_loss did not improve from 0.77305\n",
      "36805/36805 [==============================] - 16s 425us/sample - loss: 1.1256 - acc: 0.6334 - val_loss: 0.7777 - val_acc: 0.7661\n",
      "Epoch 387/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1187 - acc: 0.6377\n",
      "Epoch 00387: val_loss improved from 0.77305 to 0.77033, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv_checkpoint/387-0.7703.hdf5\n",
      "36805/36805 [==============================] - 16s 424us/sample - loss: 1.1187 - acc: 0.6376 - val_loss: 0.7703 - val_acc: 0.7722\n",
      "Epoch 388/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1232 - acc: 0.6365\n",
      "Epoch 00388: val_loss did not improve from 0.77033\n",
      "36805/36805 [==============================] - 16s 421us/sample - loss: 1.1232 - acc: 0.6363 - val_loss: 0.7779 - val_acc: 0.7696\n",
      "Epoch 389/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1201 - acc: 0.6344\n",
      "Epoch 00389: val_loss did not improve from 0.77033\n",
      "36805/36805 [==============================] - 16s 422us/sample - loss: 1.1202 - acc: 0.6344 - val_loss: 0.7887 - val_acc: 0.7671\n",
      "Epoch 390/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1257 - acc: 0.6370\n",
      "Epoch 00390: val_loss did not improve from 0.77033\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 1.1257 - acc: 0.6370 - val_loss: 0.7710 - val_acc: 0.7743\n",
      "Epoch 391/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1180 - acc: 0.6355\n",
      "Epoch 00391: val_loss did not improve from 0.77033\n",
      "36805/36805 [==============================] - 16s 426us/sample - loss: 1.1180 - acc: 0.6355 - val_loss: 0.7800 - val_acc: 0.7680\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 392/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1169 - acc: 0.6355\n",
      "Epoch 00392: val_loss did not improve from 0.77033\n",
      "36805/36805 [==============================] - 16s 424us/sample - loss: 1.1169 - acc: 0.6355 - val_loss: 0.7928 - val_acc: 0.7643\n",
      "Epoch 393/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1167 - acc: 0.6364\n",
      "Epoch 00393: val_loss improved from 0.77033 to 0.76757, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv_checkpoint/393-0.7676.hdf5\n",
      "36805/36805 [==============================] - 16s 425us/sample - loss: 1.1167 - acc: 0.6364 - val_loss: 0.7676 - val_acc: 0.7696\n",
      "Epoch 394/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1244 - acc: 0.6356\n",
      "Epoch 00394: val_loss did not improve from 0.76757\n",
      "36805/36805 [==============================] - 16s 426us/sample - loss: 1.1244 - acc: 0.6356 - val_loss: 0.7721 - val_acc: 0.7689\n",
      "Epoch 395/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1151 - acc: 0.6377\n",
      "Epoch 00395: val_loss did not improve from 0.76757\n",
      "36805/36805 [==============================] - 16s 424us/sample - loss: 1.1153 - acc: 0.6377 - val_loss: 0.7759 - val_acc: 0.7694\n",
      "Epoch 396/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1180 - acc: 0.6377\n",
      "Epoch 00396: val_loss did not improve from 0.76757\n",
      "36805/36805 [==============================] - 16s 427us/sample - loss: 1.1180 - acc: 0.6377 - val_loss: 0.7681 - val_acc: 0.7699\n",
      "Epoch 397/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1158 - acc: 0.6381\n",
      "Epoch 00397: val_loss did not improve from 0.76757\n",
      "36805/36805 [==============================] - 16s 424us/sample - loss: 1.1164 - acc: 0.6380 - val_loss: 0.7823 - val_acc: 0.7678\n",
      "Epoch 398/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1218 - acc: 0.6385\n",
      "Epoch 00398: val_loss did not improve from 0.76757\n",
      "36805/36805 [==============================] - 16s 422us/sample - loss: 1.1217 - acc: 0.6385 - val_loss: 0.7765 - val_acc: 0.7699\n",
      "Epoch 399/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1154 - acc: 0.6386\n",
      "Epoch 00399: val_loss did not improve from 0.76757\n",
      "36805/36805 [==============================] - 16s 425us/sample - loss: 1.1153 - acc: 0.6387 - val_loss: 0.7718 - val_acc: 0.7703\n",
      "Epoch 400/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1175 - acc: 0.6395\n",
      "Epoch 00400: val_loss did not improve from 0.76757\n",
      "36805/36805 [==============================] - 16s 426us/sample - loss: 1.1178 - acc: 0.6393 - val_loss: 0.7680 - val_acc: 0.7724\n",
      "Epoch 401/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1200 - acc: 0.6351\n",
      "Epoch 00401: val_loss did not improve from 0.76757\n",
      "36805/36805 [==============================] - 16s 425us/sample - loss: 1.1201 - acc: 0.6350 - val_loss: 0.7802 - val_acc: 0.7715\n",
      "Epoch 402/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1246 - acc: 0.6377\n",
      "Epoch 00402: val_loss did not improve from 0.76757\n",
      "36805/36805 [==============================] - 16s 426us/sample - loss: 1.1246 - acc: 0.6377 - val_loss: 0.7795 - val_acc: 0.7661\n",
      "Epoch 403/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1255 - acc: 0.6340\n",
      "Epoch 00403: val_loss did not improve from 0.76757\n",
      "36805/36805 [==============================] - 16s 424us/sample - loss: 1.1255 - acc: 0.6341 - val_loss: 0.7721 - val_acc: 0.7675\n",
      "Epoch 404/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1208 - acc: 0.6363\n",
      "Epoch 00404: val_loss did not improve from 0.76757\n",
      "36805/36805 [==============================] - 16s 426us/sample - loss: 1.1208 - acc: 0.6361 - val_loss: 0.7733 - val_acc: 0.7694\n",
      "Epoch 405/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1047 - acc: 0.6427\n",
      "Epoch 00405: val_loss did not improve from 0.76757\n",
      "36805/36805 [==============================] - 16s 426us/sample - loss: 1.1047 - acc: 0.6427 - val_loss: 0.7846 - val_acc: 0.7657\n",
      "Epoch 406/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1135 - acc: 0.6402\n",
      "Epoch 00406: val_loss did not improve from 0.76757\n",
      "36805/36805 [==============================] - 16s 425us/sample - loss: 1.1134 - acc: 0.6402 - val_loss: 0.7688 - val_acc: 0.7692\n",
      "Epoch 407/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1134 - acc: 0.6389\n",
      "Epoch 00407: val_loss did not improve from 0.76757\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 1.1132 - acc: 0.6390 - val_loss: 0.7727 - val_acc: 0.7699\n",
      "Epoch 408/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1196 - acc: 0.6374\n",
      "Epoch 00408: val_loss did not improve from 0.76757\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 1.1196 - acc: 0.6374 - val_loss: 0.7750 - val_acc: 0.7703\n",
      "Epoch 409/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1164 - acc: 0.6378\n",
      "Epoch 00409: val_loss did not improve from 0.76757\n",
      "36805/36805 [==============================] - 16s 425us/sample - loss: 1.1166 - acc: 0.6377 - val_loss: 0.7783 - val_acc: 0.7650\n",
      "Epoch 410/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1208 - acc: 0.6375\n",
      "Epoch 00410: val_loss did not improve from 0.76757\n",
      "36805/36805 [==============================] - 16s 424us/sample - loss: 1.1201 - acc: 0.6377 - val_loss: 0.7771 - val_acc: 0.7694\n",
      "Epoch 411/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1110 - acc: 0.6389\n",
      "Epoch 00411: val_loss did not improve from 0.76757\n",
      "36805/36805 [==============================] - 16s 424us/sample - loss: 1.1110 - acc: 0.6389 - val_loss: 0.7723 - val_acc: 0.7729\n",
      "Epoch 412/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1122 - acc: 0.6386\n",
      "Epoch 00412: val_loss did not improve from 0.76757\n",
      "36805/36805 [==============================] - 16s 425us/sample - loss: 1.1116 - acc: 0.6388 - val_loss: 0.7735 - val_acc: 0.7687\n",
      "Epoch 413/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1171 - acc: 0.6386\n",
      "Epoch 00413: val_loss improved from 0.76757 to 0.76433, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv_checkpoint/413-0.7643.hdf5\n",
      "36805/36805 [==============================] - 16s 426us/sample - loss: 1.1167 - acc: 0.6388 - val_loss: 0.7643 - val_acc: 0.7727\n",
      "Epoch 414/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1086 - acc: 0.6392\n",
      "Epoch 00414: val_loss improved from 0.76433 to 0.76371, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv_checkpoint/414-0.7637.hdf5\n",
      "36805/36805 [==============================] - 16s 425us/sample - loss: 1.1084 - acc: 0.6393 - val_loss: 0.7637 - val_acc: 0.7687\n",
      "Epoch 415/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1189 - acc: 0.6358\n",
      "Epoch 00415: val_loss did not improve from 0.76371\n",
      "36805/36805 [==============================] - 16s 424us/sample - loss: 1.1187 - acc: 0.6359 - val_loss: 0.7680 - val_acc: 0.7741\n",
      "Epoch 416/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1175 - acc: 0.6364\n",
      "Epoch 00416: val_loss did not improve from 0.76371\n",
      "36805/36805 [==============================] - 16s 427us/sample - loss: 1.1173 - acc: 0.6364 - val_loss: 0.7706 - val_acc: 0.7706\n",
      "Epoch 417/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1120 - acc: 0.6370\n",
      "Epoch 00417: val_loss did not improve from 0.76371\n",
      "36805/36805 [==============================] - 16s 424us/sample - loss: 1.1119 - acc: 0.6371 - val_loss: 0.7714 - val_acc: 0.7722\n",
      "Epoch 418/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1135 - acc: 0.6346\n",
      "Epoch 00418: val_loss did not improve from 0.76371\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 1.1134 - acc: 0.6346 - val_loss: 0.7706 - val_acc: 0.7706\n",
      "Epoch 419/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1257 - acc: 0.6352\n",
      "Epoch 00419: val_loss did not improve from 0.76371\n",
      "36805/36805 [==============================] - 16s 425us/sample - loss: 1.1260 - acc: 0.6352 - val_loss: 0.7675 - val_acc: 0.7734\n",
      "Epoch 420/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1081 - acc: 0.6408\n",
      "Epoch 00420: val_loss did not improve from 0.76371\n",
      "36805/36805 [==============================] - 16s 426us/sample - loss: 1.1081 - acc: 0.6408 - val_loss: 0.7719 - val_acc: 0.7706\n",
      "Epoch 421/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.1181 - acc: 0.6379\n",
      "Epoch 00421: val_loss did not improve from 0.76371\n",
      "36805/36805 [==============================] - 16s 426us/sample - loss: 1.1173 - acc: 0.6380 - val_loss: 0.7732 - val_acc: 0.7717\n",
      "Epoch 422/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1062 - acc: 0.6389\n",
      "Epoch 00422: val_loss did not improve from 0.76371\n",
      "36805/36805 [==============================] - 16s 426us/sample - loss: 1.1062 - acc: 0.6389 - val_loss: 0.7693 - val_acc: 0.7727\n",
      "Epoch 423/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1013 - acc: 0.6452\n",
      "Epoch 00423: val_loss did not improve from 0.76371\n",
      "36805/36805 [==============================] - 16s 426us/sample - loss: 1.1012 - acc: 0.6453 - val_loss: 0.7672 - val_acc: 0.7710\n",
      "Epoch 424/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1160 - acc: 0.6379\n",
      "Epoch 00424: val_loss did not improve from 0.76371\n",
      "36805/36805 [==============================] - 16s 425us/sample - loss: 1.1159 - acc: 0.6379 - val_loss: 0.7684 - val_acc: 0.7722\n",
      "Epoch 425/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1075 - acc: 0.6424\n",
      "Epoch 00425: val_loss did not improve from 0.76371\n",
      "36805/36805 [==============================] - 16s 425us/sample - loss: 1.1078 - acc: 0.6423 - val_loss: 0.7696 - val_acc: 0.7715\n",
      "Epoch 426/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.1128 - acc: 0.6386\n",
      "Epoch 00426: val_loss improved from 0.76371 to 0.76306, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv_checkpoint/426-0.7631.hdf5\n",
      "36805/36805 [==============================] - 16s 426us/sample - loss: 1.1129 - acc: 0.6384 - val_loss: 0.7631 - val_acc: 0.7734\n",
      "Epoch 427/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1119 - acc: 0.6370\n",
      "Epoch 00427: val_loss improved from 0.76306 to 0.75747, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv_checkpoint/427-0.7575.hdf5\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 1.1118 - acc: 0.6371 - val_loss: 0.7575 - val_acc: 0.7759\n",
      "Epoch 428/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1101 - acc: 0.6366\n",
      "Epoch 00428: val_loss did not improve from 0.75747\n",
      "36805/36805 [==============================] - 16s 424us/sample - loss: 1.1102 - acc: 0.6365 - val_loss: 0.7764 - val_acc: 0.7671\n",
      "Epoch 429/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1087 - acc: 0.6407\n",
      "Epoch 00429: val_loss did not improve from 0.75747\n",
      "36805/36805 [==============================] - 16s 426us/sample - loss: 1.1085 - acc: 0.6409 - val_loss: 0.7627 - val_acc: 0.7731\n",
      "Epoch 430/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1103 - acc: 0.6398\n",
      "Epoch 00430: val_loss did not improve from 0.75747\n",
      "36805/36805 [==============================] - 16s 424us/sample - loss: 1.1103 - acc: 0.6398 - val_loss: 0.7661 - val_acc: 0.7717\n",
      "Epoch 431/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1142 - acc: 0.6386\n",
      "Epoch 00431: val_loss did not improve from 0.75747\n",
      "36805/36805 [==============================] - 16s 425us/sample - loss: 1.1138 - acc: 0.6389 - val_loss: 0.7748 - val_acc: 0.7715\n",
      "Epoch 432/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1002 - acc: 0.6424\n",
      "Epoch 00432: val_loss did not improve from 0.75747\n",
      "36805/36805 [==============================] - 16s 424us/sample - loss: 1.1004 - acc: 0.6424 - val_loss: 0.7708 - val_acc: 0.7727\n",
      "Epoch 433/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1060 - acc: 0.6411\n",
      "Epoch 00433: val_loss did not improve from 0.75747\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 1.1061 - acc: 0.6411 - val_loss: 0.7738 - val_acc: 0.7671\n",
      "Epoch 434/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1046 - acc: 0.6407\n",
      "Epoch 00434: val_loss did not improve from 0.75747\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 1.1049 - acc: 0.6407 - val_loss: 0.7667 - val_acc: 0.7738\n",
      "Epoch 435/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1055 - acc: 0.6426\n",
      "Epoch 00435: val_loss did not improve from 0.75747\n",
      "36805/36805 [==============================] - 16s 425us/sample - loss: 1.1055 - acc: 0.6426 - val_loss: 0.7594 - val_acc: 0.7734\n",
      "Epoch 436/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1116 - acc: 0.6378\n",
      "Epoch 00436: val_loss did not improve from 0.75747\n",
      "36805/36805 [==============================] - 16s 425us/sample - loss: 1.1121 - acc: 0.6378 - val_loss: 0.7615 - val_acc: 0.7773\n",
      "Epoch 437/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1130 - acc: 0.6383\n",
      "Epoch 00437: val_loss did not improve from 0.75747\n",
      "36805/36805 [==============================] - 16s 424us/sample - loss: 1.1127 - acc: 0.6383 - val_loss: 0.7580 - val_acc: 0.7710\n",
      "Epoch 438/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1133 - acc: 0.6390\n",
      "Epoch 00438: val_loss did not improve from 0.75747\n",
      "36805/36805 [==============================] - 16s 425us/sample - loss: 1.1135 - acc: 0.6390 - val_loss: 0.7708 - val_acc: 0.7727\n",
      "Epoch 439/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1102 - acc: 0.6376\n",
      "Epoch 00439: val_loss did not improve from 0.75747\n",
      "36805/36805 [==============================] - 16s 426us/sample - loss: 1.1103 - acc: 0.6376 - val_loss: 0.7664 - val_acc: 0.7727\n",
      "Epoch 440/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1008 - acc: 0.6440\n",
      "Epoch 00440: val_loss did not improve from 0.75747\n",
      "36805/36805 [==============================] - 16s 424us/sample - loss: 1.1008 - acc: 0.6440 - val_loss: 0.7615 - val_acc: 0.7759\n",
      "Epoch 441/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1114 - acc: 0.6419\n",
      "Epoch 00441: val_loss did not improve from 0.75747\n",
      "36805/36805 [==============================] - 16s 424us/sample - loss: 1.1114 - acc: 0.6419 - val_loss: 0.7744 - val_acc: 0.7678\n",
      "Epoch 442/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1085 - acc: 0.6407\n",
      "Epoch 00442: val_loss did not improve from 0.75747\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 1.1085 - acc: 0.6405 - val_loss: 0.7657 - val_acc: 0.7720\n",
      "Epoch 443/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1057 - acc: 0.6418\n",
      "Epoch 00443: val_loss did not improve from 0.75747\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 1.1058 - acc: 0.6418 - val_loss: 0.7626 - val_acc: 0.7738\n",
      "Epoch 444/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.1077 - acc: 0.6364\n",
      "Epoch 00444: val_loss did not improve from 0.75747\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 1.1080 - acc: 0.6364 - val_loss: 0.7580 - val_acc: 0.7738\n",
      "Epoch 445/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1098 - acc: 0.6380\n",
      "Epoch 00445: val_loss did not improve from 0.75747\n",
      "36805/36805 [==============================] - 16s 424us/sample - loss: 1.1098 - acc: 0.6380 - val_loss: 0.7613 - val_acc: 0.7729\n",
      "Epoch 446/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1013 - acc: 0.6450\n",
      "Epoch 00446: val_loss did not improve from 0.75747\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 1.1012 - acc: 0.6449 - val_loss: 0.7653 - val_acc: 0.7713\n",
      "Epoch 447/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1084 - acc: 0.6419\n",
      "Epoch 00447: val_loss did not improve from 0.75747\n",
      "36805/36805 [==============================] - 16s 426us/sample - loss: 1.1085 - acc: 0.6419 - val_loss: 0.7631 - val_acc: 0.7757\n",
      "Epoch 448/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1024 - acc: 0.6451\n",
      "Epoch 00448: val_loss did not improve from 0.75747\n",
      "36805/36805 [==============================] - 16s 424us/sample - loss: 1.1029 - acc: 0.6450 - val_loss: 0.7732 - val_acc: 0.7699\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 449/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1062 - acc: 0.6410\n",
      "Epoch 00449: val_loss did not improve from 0.75747\n",
      "36805/36805 [==============================] - 15s 421us/sample - loss: 1.1060 - acc: 0.6409 - val_loss: 0.7595 - val_acc: 0.7745\n",
      "Epoch 450/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1091 - acc: 0.6387\n",
      "Epoch 00450: val_loss did not improve from 0.75747\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 1.1090 - acc: 0.6388 - val_loss: 0.7654 - val_acc: 0.7708\n",
      "Epoch 451/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1004 - acc: 0.6432\n",
      "Epoch 00451: val_loss did not improve from 0.75747\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 1.1006 - acc: 0.6433 - val_loss: 0.7679 - val_acc: 0.7717\n",
      "Epoch 452/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1062 - acc: 0.6396\n",
      "Epoch 00452: val_loss did not improve from 0.75747\n",
      "36805/36805 [==============================] - 16s 426us/sample - loss: 1.1058 - acc: 0.6397 - val_loss: 0.7592 - val_acc: 0.7752\n",
      "Epoch 453/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1082 - acc: 0.6419\n",
      "Epoch 00453: val_loss did not improve from 0.75747\n",
      "36805/36805 [==============================] - 16s 424us/sample - loss: 1.1083 - acc: 0.6419 - val_loss: 0.7655 - val_acc: 0.7720\n",
      "Epoch 454/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1027 - acc: 0.6416\n",
      "Epoch 00454: val_loss did not improve from 0.75747\n",
      "36805/36805 [==============================] - 16s 424us/sample - loss: 1.1025 - acc: 0.6416 - val_loss: 0.7710 - val_acc: 0.7720\n",
      "Epoch 455/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.1089 - acc: 0.6398\n",
      "Epoch 00455: val_loss improved from 0.75747 to 0.75746, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv_checkpoint/455-0.7575.hdf5\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 1.1094 - acc: 0.6396 - val_loss: 0.7575 - val_acc: 0.7743\n",
      "Epoch 456/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0995 - acc: 0.6414\n",
      "Epoch 00456: val_loss improved from 0.75746 to 0.75540, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv_checkpoint/456-0.7554.hdf5\n",
      "36805/36805 [==============================] - 16s 425us/sample - loss: 1.0993 - acc: 0.6414 - val_loss: 0.7554 - val_acc: 0.7757\n",
      "Epoch 457/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1038 - acc: 0.6437\n",
      "Epoch 00457: val_loss did not improve from 0.75540\n",
      "36805/36805 [==============================] - 16s 424us/sample - loss: 1.1040 - acc: 0.6437 - val_loss: 0.7696 - val_acc: 0.7727\n",
      "Epoch 458/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0968 - acc: 0.6438\n",
      "Epoch 00458: val_loss did not improve from 0.75540\n",
      "36805/36805 [==============================] - 15s 421us/sample - loss: 1.0965 - acc: 0.6439 - val_loss: 0.7619 - val_acc: 0.7729\n",
      "Epoch 459/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1045 - acc: 0.6450\n",
      "Epoch 00459: val_loss did not improve from 0.75540\n",
      "36805/36805 [==============================] - 16s 422us/sample - loss: 1.1045 - acc: 0.6449 - val_loss: 0.7698 - val_acc: 0.7764\n",
      "Epoch 460/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1026 - acc: 0.6414\n",
      "Epoch 00460: val_loss did not improve from 0.75540\n",
      "36805/36805 [==============================] - 16s 424us/sample - loss: 1.1027 - acc: 0.6413 - val_loss: 0.7641 - val_acc: 0.7731\n",
      "Epoch 461/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.1035 - acc: 0.6422\n",
      "Epoch 00461: val_loss did not improve from 0.75540\n",
      "36805/36805 [==============================] - 16s 425us/sample - loss: 1.1030 - acc: 0.6422 - val_loss: 0.7584 - val_acc: 0.7757\n",
      "Epoch 462/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1018 - acc: 0.6429\n",
      "Epoch 00462: val_loss did not improve from 0.75540\n",
      "36805/36805 [==============================] - 16s 426us/sample - loss: 1.1019 - acc: 0.6429 - val_loss: 0.7617 - val_acc: 0.7741\n",
      "Epoch 463/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1024 - acc: 0.6407\n",
      "Epoch 00463: val_loss did not improve from 0.75540\n",
      "36805/36805 [==============================] - 16s 425us/sample - loss: 1.1027 - acc: 0.6406 - val_loss: 0.7752 - val_acc: 0.7727\n",
      "Epoch 464/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1051 - acc: 0.6425\n",
      "Epoch 00464: val_loss did not improve from 0.75540\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 1.1051 - acc: 0.6425 - val_loss: 0.7653 - val_acc: 0.7722\n",
      "Epoch 465/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0967 - acc: 0.6447\n",
      "Epoch 00465: val_loss did not improve from 0.75540\n",
      "36805/36805 [==============================] - 16s 425us/sample - loss: 1.0968 - acc: 0.6447 - val_loss: 0.7563 - val_acc: 0.7764\n",
      "Epoch 466/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0950 - acc: 0.6471\n",
      "Epoch 00466: val_loss did not improve from 0.75540\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 1.0953 - acc: 0.6470 - val_loss: 0.7668 - val_acc: 0.7761\n",
      "Epoch 467/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1018 - acc: 0.6415\n",
      "Epoch 00467: val_loss improved from 0.75540 to 0.75298, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv_checkpoint/467-0.7530.hdf5\n",
      "36805/36805 [==============================] - 16s 424us/sample - loss: 1.1017 - acc: 0.6415 - val_loss: 0.7530 - val_acc: 0.7766\n",
      "Epoch 468/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1003 - acc: 0.6404\n",
      "Epoch 00468: val_loss did not improve from 0.75298\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 1.1004 - acc: 0.6403 - val_loss: 0.7583 - val_acc: 0.7771\n",
      "Epoch 469/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1029 - acc: 0.6438\n",
      "Epoch 00469: val_loss improved from 0.75298 to 0.75173, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv_checkpoint/469-0.7517.hdf5\n",
      "36805/36805 [==============================] - 16s 425us/sample - loss: 1.1025 - acc: 0.6439 - val_loss: 0.7517 - val_acc: 0.7775\n",
      "Epoch 470/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0996 - acc: 0.6443\n",
      "Epoch 00470: val_loss did not improve from 0.75173\n",
      "36805/36805 [==============================] - 16s 424us/sample - loss: 1.0997 - acc: 0.6443 - val_loss: 0.7612 - val_acc: 0.7745\n",
      "Epoch 471/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1024 - acc: 0.6413\n",
      "Epoch 00471: val_loss did not improve from 0.75173\n",
      "36805/36805 [==============================] - 16s 425us/sample - loss: 1.1022 - acc: 0.6414 - val_loss: 0.7566 - val_acc: 0.7773\n",
      "Epoch 472/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0970 - acc: 0.6435\n",
      "Epoch 00472: val_loss improved from 0.75173 to 0.74680, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv_checkpoint/472-0.7468.hdf5\n",
      "36805/36805 [==============================] - 16s 422us/sample - loss: 1.0971 - acc: 0.6435 - val_loss: 0.7468 - val_acc: 0.7757\n",
      "Epoch 473/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0972 - acc: 0.6450\n",
      "Epoch 00473: val_loss improved from 0.74680 to 0.74646, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv_checkpoint/473-0.7465.hdf5\n",
      "36805/36805 [==============================] - 16s 425us/sample - loss: 1.0970 - acc: 0.6450 - val_loss: 0.7465 - val_acc: 0.7780\n",
      "Epoch 474/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0943 - acc: 0.6426\n",
      "Epoch 00474: val_loss did not improve from 0.74646\n",
      "36805/36805 [==============================] - 16s 425us/sample - loss: 1.0943 - acc: 0.6427 - val_loss: 0.7537 - val_acc: 0.7787\n",
      "Epoch 475/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1086 - acc: 0.6411\n",
      "Epoch 00475: val_loss did not improve from 0.74646\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 1.1086 - acc: 0.6411 - val_loss: 0.7578 - val_acc: 0.7775\n",
      "Epoch 476/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1026 - acc: 0.6430\n",
      "Epoch 00476: val_loss did not improve from 0.74646\n",
      "36805/36805 [==============================] - 16s 421us/sample - loss: 1.1027 - acc: 0.6430 - val_loss: 0.7554 - val_acc: 0.7773\n",
      "Epoch 477/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1030 - acc: 0.6394\n",
      "Epoch 00477: val_loss did not improve from 0.74646\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 1.1031 - acc: 0.6394 - val_loss: 0.7600 - val_acc: 0.7750\n",
      "Epoch 478/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0931 - acc: 0.6453\n",
      "Epoch 00478: val_loss did not improve from 0.74646\n",
      "36805/36805 [==============================] - 16s 424us/sample - loss: 1.0932 - acc: 0.6453 - val_loss: 0.7566 - val_acc: 0.7789\n",
      "Epoch 479/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0972 - acc: 0.6427\n",
      "Epoch 00479: val_loss did not improve from 0.74646\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 1.0972 - acc: 0.6427 - val_loss: 0.7635 - val_acc: 0.7761\n",
      "Epoch 480/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1056 - acc: 0.6446\n",
      "Epoch 00480: val_loss did not improve from 0.74646\n",
      "36805/36805 [==============================] - 16s 422us/sample - loss: 1.1056 - acc: 0.6447 - val_loss: 0.7539 - val_acc: 0.7787\n",
      "Epoch 481/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0949 - acc: 0.6416\n",
      "Epoch 00481: val_loss did not improve from 0.74646\n",
      "36805/36805 [==============================] - 16s 424us/sample - loss: 1.0948 - acc: 0.6417 - val_loss: 0.7536 - val_acc: 0.7771\n",
      "Epoch 482/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0992 - acc: 0.6430\n",
      "Epoch 00482: val_loss did not improve from 0.74646\n",
      "36805/36805 [==============================] - 16s 421us/sample - loss: 1.0986 - acc: 0.6432 - val_loss: 0.7577 - val_acc: 0.7752\n",
      "Epoch 483/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1040 - acc: 0.6423\n",
      "Epoch 00483: val_loss did not improve from 0.74646\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 1.1041 - acc: 0.6424 - val_loss: 0.7613 - val_acc: 0.7771\n",
      "Epoch 484/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1040 - acc: 0.6442\n",
      "Epoch 00484: val_loss did not improve from 0.74646\n",
      "36805/36805 [==============================] - 15s 420us/sample - loss: 1.1039 - acc: 0.6442 - val_loss: 0.7565 - val_acc: 0.7780\n",
      "Epoch 485/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0952 - acc: 0.6439\n",
      "Epoch 00485: val_loss did not improve from 0.74646\n",
      "36805/36805 [==============================] - 15s 421us/sample - loss: 1.0952 - acc: 0.6440 - val_loss: 0.7533 - val_acc: 0.7801\n",
      "Epoch 486/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0957 - acc: 0.6414\n",
      "Epoch 00486: val_loss did not improve from 0.74646\n",
      "36805/36805 [==============================] - 16s 424us/sample - loss: 1.0956 - acc: 0.6415 - val_loss: 0.7522 - val_acc: 0.7810\n",
      "Epoch 487/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0949 - acc: 0.6449\n",
      "Epoch 00487: val_loss did not improve from 0.74646\n",
      "36805/36805 [==============================] - 16s 424us/sample - loss: 1.0948 - acc: 0.6449 - val_loss: 0.7562 - val_acc: 0.7761\n",
      "Epoch 488/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0971 - acc: 0.6439\n",
      "Epoch 00488: val_loss did not improve from 0.74646\n",
      "36805/36805 [==============================] - 16s 424us/sample - loss: 1.0967 - acc: 0.6440 - val_loss: 0.7581 - val_acc: 0.7764\n",
      "Epoch 489/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0926 - acc: 0.6500\n",
      "Epoch 00489: val_loss did not improve from 0.74646\n",
      "36805/36805 [==============================] - 16s 425us/sample - loss: 1.0925 - acc: 0.6500 - val_loss: 0.7542 - val_acc: 0.7796\n",
      "Epoch 490/500\n",
      "36672/36805 [============================>.] - ETA: 0s - loss: 1.1006 - acc: 0.6420\n",
      "Epoch 00490: val_loss did not improve from 0.74646\n",
      "36805/36805 [==============================] - 16s 424us/sample - loss: 1.1007 - acc: 0.6421 - val_loss: 0.7540 - val_acc: 0.7782\n",
      "Epoch 491/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0996 - acc: 0.6410\n",
      "Epoch 00491: val_loss did not improve from 0.74646\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 1.0991 - acc: 0.6411 - val_loss: 0.7541 - val_acc: 0.7766\n",
      "Epoch 492/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0958 - acc: 0.6442\n",
      "Epoch 00492: val_loss did not improve from 0.74646\n",
      "36805/36805 [==============================] - 16s 422us/sample - loss: 1.0958 - acc: 0.6442 - val_loss: 0.7571 - val_acc: 0.7738\n",
      "Epoch 493/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0985 - acc: 0.6415\n",
      "Epoch 00493: val_loss did not improve from 0.74646\n",
      "36805/36805 [==============================] - 16s 426us/sample - loss: 1.0984 - acc: 0.6415 - val_loss: 0.7466 - val_acc: 0.7843\n",
      "Epoch 494/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1014 - acc: 0.6463\n",
      "Epoch 00494: val_loss did not improve from 0.74646\n",
      "36805/36805 [==============================] - 16s 423us/sample - loss: 1.1013 - acc: 0.6464 - val_loss: 0.7580 - val_acc: 0.7764\n",
      "Epoch 495/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0965 - acc: 0.6444\n",
      "Epoch 00495: val_loss did not improve from 0.74646\n",
      "36805/36805 [==============================] - 16s 425us/sample - loss: 1.0965 - acc: 0.6444 - val_loss: 0.7520 - val_acc: 0.7775\n",
      "Epoch 496/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0927 - acc: 0.6454\n",
      "Epoch 00496: val_loss improved from 0.74646 to 0.74619, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv_checkpoint/496-0.7462.hdf5\n",
      "36805/36805 [==============================] - 16s 425us/sample - loss: 1.0930 - acc: 0.6453 - val_loss: 0.7462 - val_acc: 0.7801\n",
      "Epoch 497/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0974 - acc: 0.6454\n",
      "Epoch 00497: val_loss did not improve from 0.74619\n",
      "36805/36805 [==============================] - 16s 424us/sample - loss: 1.0973 - acc: 0.6455 - val_loss: 0.7621 - val_acc: 0.7736\n",
      "Epoch 498/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0965 - acc: 0.6454\n",
      "Epoch 00498: val_loss did not improve from 0.74619\n",
      "36805/36805 [==============================] - 16s 425us/sample - loss: 1.0966 - acc: 0.6454 - val_loss: 0.7585 - val_acc: 0.7782\n",
      "Epoch 499/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0956 - acc: 0.6438\n",
      "Epoch 00499: val_loss did not improve from 0.74619\n",
      "36805/36805 [==============================] - 16s 422us/sample - loss: 1.0957 - acc: 0.6436 - val_loss: 0.7480 - val_acc: 0.7778\n",
      "Epoch 500/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0958 - acc: 0.6430\n",
      "Epoch 00500: val_loss did not improve from 0.74619\n",
      "36805/36805 [==============================] - 16s 425us/sample - loss: 1.0958 - acc: 0.6430 - val_loss: 0.7555 - val_acc: 0.7782\n",
      "\n",
      "1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 1s 237us/sample - loss: 0.8415 - acc: 0.7468\n",
      "Loss: 0.8415316059829537 Accuracy: 0.7468328\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7293 - acc: 0.0962\n",
      "Epoch 00001: val_loss improved from inf to 2.64463, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_4_conv_checkpoint/001-2.6446.hdf5\n",
      "36805/36805 [==============================] - 18s 476us/sample - loss: 2.7294 - acc: 0.0962 - val_loss: 2.6446 - val_acc: 0.1242\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.5410 - acc: 0.1700\n",
      "Epoch 00002: val_loss improved from 2.64463 to 2.32217, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_4_conv_checkpoint/002-2.3222.hdf5\n",
      "36805/36805 [==============================] - 16s 435us/sample - loss: 2.5409 - acc: 0.1700 - val_loss: 2.3222 - val_acc: 0.2895\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.3228 - acc: 0.2317\n",
      "Epoch 00003: val_loss improved from 2.32217 to 2.10849, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_4_conv_checkpoint/003-2.1085.hdf5\n",
      "36805/36805 [==============================] - 16s 436us/sample - loss: 2.3229 - acc: 0.2317 - val_loss: 2.1085 - val_acc: 0.3592\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.1975 - acc: 0.2654\n",
      "Epoch 00004: val_loss improved from 2.10849 to 1.98227, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_4_conv_checkpoint/004-1.9823.hdf5\n",
      "36805/36805 [==============================] - 16s 435us/sample - loss: 2.1976 - acc: 0.2654 - val_loss: 1.9823 - val_acc: 0.3990\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.1035 - acc: 0.2933\n",
      "Epoch 00005: val_loss improved from 1.98227 to 1.87204, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_4_conv_checkpoint/005-1.8720.hdf5\n",
      "36805/36805 [==============================] - 16s 438us/sample - loss: 2.1035 - acc: 0.2933 - val_loss: 1.8720 - val_acc: 0.4326\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.0275 - acc: 0.3181\n",
      "Epoch 00006: val_loss improved from 1.87204 to 1.79220, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_4_conv_checkpoint/006-1.7922.hdf5\n",
      "36805/36805 [==============================] - 16s 436us/sample - loss: 2.0275 - acc: 0.3181 - val_loss: 1.7922 - val_acc: 0.4540\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9589 - acc: 0.3399\n",
      "Epoch 00007: val_loss improved from 1.79220 to 1.72065, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_4_conv_checkpoint/007-1.7207.hdf5\n",
      "36805/36805 [==============================] - 16s 437us/sample - loss: 1.9589 - acc: 0.3398 - val_loss: 1.7207 - val_acc: 0.4831\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9040 - acc: 0.3593\n",
      "Epoch 00008: val_loss improved from 1.72065 to 1.65300, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_4_conv_checkpoint/008-1.6530.hdf5\n",
      "36805/36805 [==============================] - 16s 436us/sample - loss: 1.9039 - acc: 0.3594 - val_loss: 1.6530 - val_acc: 0.4934\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.8480 - acc: 0.3813\n",
      "Epoch 00009: val_loss improved from 1.65300 to 1.59134, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_4_conv_checkpoint/009-1.5913.hdf5\n",
      "36805/36805 [==============================] - 16s 435us/sample - loss: 1.8480 - acc: 0.3813 - val_loss: 1.5913 - val_acc: 0.5141\n",
      "Epoch 10/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.7967 - acc: 0.3989\n",
      "Epoch 00010: val_loss improved from 1.59134 to 1.53192, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_4_conv_checkpoint/010-1.5319.hdf5\n",
      "36805/36805 [==============================] - 16s 437us/sample - loss: 1.7965 - acc: 0.3991 - val_loss: 1.5319 - val_acc: 0.5392\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.7522 - acc: 0.4160\n",
      "Epoch 00011: val_loss improved from 1.53192 to 1.47865, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_4_conv_checkpoint/011-1.4787.hdf5\n",
      "36805/36805 [==============================] - 16s 437us/sample - loss: 1.7522 - acc: 0.4160 - val_loss: 1.4787 - val_acc: 0.5521\n",
      "Epoch 12/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.7086 - acc: 0.4312\n",
      "Epoch 00012: val_loss improved from 1.47865 to 1.42147, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_4_conv_checkpoint/012-1.4215.hdf5\n",
      "36805/36805 [==============================] - 16s 438us/sample - loss: 1.7083 - acc: 0.4313 - val_loss: 1.4215 - val_acc: 0.5679\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6593 - acc: 0.4501\n",
      "Epoch 00013: val_loss improved from 1.42147 to 1.38262, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_4_conv_checkpoint/013-1.3826.hdf5\n",
      "36805/36805 [==============================] - 16s 434us/sample - loss: 1.6594 - acc: 0.4501 - val_loss: 1.3826 - val_acc: 0.5870\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6257 - acc: 0.4635\n",
      "Epoch 00014: val_loss improved from 1.38262 to 1.33365, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_4_conv_checkpoint/014-1.3336.hdf5\n",
      "36805/36805 [==============================] - 16s 437us/sample - loss: 1.6258 - acc: 0.4634 - val_loss: 1.3336 - val_acc: 0.5949\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5913 - acc: 0.4786\n",
      "Epoch 00015: val_loss improved from 1.33365 to 1.30791, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_4_conv_checkpoint/015-1.3079.hdf5\n",
      "36805/36805 [==============================] - 16s 436us/sample - loss: 1.5912 - acc: 0.4786 - val_loss: 1.3079 - val_acc: 0.6103\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5623 - acc: 0.4917\n",
      "Epoch 00016: val_loss improved from 1.30791 to 1.26111, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_4_conv_checkpoint/016-1.2611.hdf5\n",
      "36805/36805 [==============================] - 16s 436us/sample - loss: 1.5622 - acc: 0.4917 - val_loss: 1.2611 - val_acc: 0.6215\n",
      "Epoch 17/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.5319 - acc: 0.5028\n",
      "Epoch 00017: val_loss improved from 1.26111 to 1.23176, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_4_conv_checkpoint/017-1.2318.hdf5\n",
      "36805/36805 [==============================] - 16s 433us/sample - loss: 1.5322 - acc: 0.5025 - val_loss: 1.2318 - val_acc: 0.6375\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5047 - acc: 0.5079\n",
      "Epoch 00018: val_loss improved from 1.23176 to 1.21707, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_4_conv_checkpoint/018-1.2171.hdf5\n",
      "36805/36805 [==============================] - 16s 435us/sample - loss: 1.5046 - acc: 0.5079 - val_loss: 1.2171 - val_acc: 0.6406\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4748 - acc: 0.5186\n",
      "Epoch 00019: val_loss improved from 1.21707 to 1.17772, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_4_conv_checkpoint/019-1.1777.hdf5\n",
      "36805/36805 [==============================] - 16s 435us/sample - loss: 1.4747 - acc: 0.5186 - val_loss: 1.1777 - val_acc: 0.6522\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4533 - acc: 0.5267\n",
      "Epoch 00020: val_loss improved from 1.17772 to 1.14878, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_4_conv_checkpoint/020-1.1488.hdf5\n",
      "36805/36805 [==============================] - 16s 440us/sample - loss: 1.4533 - acc: 0.5267 - val_loss: 1.1488 - val_acc: 0.6580\n",
      "Epoch 21/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.4307 - acc: 0.5343\n",
      "Epoch 00021: val_loss improved from 1.14878 to 1.13333, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_4_conv_checkpoint/021-1.1333.hdf5\n",
      "36805/36805 [==============================] - 16s 437us/sample - loss: 1.4306 - acc: 0.5344 - val_loss: 1.1333 - val_acc: 0.6639\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4079 - acc: 0.5432\n",
      "Epoch 00022: val_loss improved from 1.13333 to 1.11042, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_4_conv_checkpoint/022-1.1104.hdf5\n",
      "36805/36805 [==============================] - 16s 438us/sample - loss: 1.4080 - acc: 0.5432 - val_loss: 1.1104 - val_acc: 0.6695\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3934 - acc: 0.5515\n",
      "Epoch 00023: val_loss improved from 1.11042 to 1.08074, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_4_conv_checkpoint/023-1.0807.hdf5\n",
      "36805/36805 [==============================] - 16s 438us/sample - loss: 1.3934 - acc: 0.5514 - val_loss: 1.0807 - val_acc: 0.6820\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3698 - acc: 0.5598\n",
      "Epoch 00024: val_loss improved from 1.08074 to 1.05326, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_4_conv_checkpoint/024-1.0533.hdf5\n",
      "36805/36805 [==============================] - 16s 437us/sample - loss: 1.3698 - acc: 0.5598 - val_loss: 1.0533 - val_acc: 0.6886\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3611 - acc: 0.5601\n",
      "Epoch 00025: val_loss did not improve from 1.05326\n",
      "36805/36805 [==============================] - 16s 434us/sample - loss: 1.3611 - acc: 0.5601 - val_loss: 1.0555 - val_acc: 0.6907\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3305 - acc: 0.5727\n",
      "Epoch 00026: val_loss improved from 1.05326 to 1.02992, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_4_conv_checkpoint/026-1.0299.hdf5\n",
      "36805/36805 [==============================] - 16s 435us/sample - loss: 1.3304 - acc: 0.5727 - val_loss: 1.0299 - val_acc: 0.6993\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3212 - acc: 0.5790\n",
      "Epoch 00027: val_loss improved from 1.02992 to 1.00500, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_4_conv_checkpoint/027-1.0050.hdf5\n",
      "36805/36805 [==============================] - 16s 438us/sample - loss: 1.3212 - acc: 0.5789 - val_loss: 1.0050 - val_acc: 0.7074\n",
      "Epoch 28/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3101 - acc: 0.5782\n",
      "Epoch 00028: val_loss improved from 1.00500 to 0.98737, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_4_conv_checkpoint/028-0.9874.hdf5\n",
      "36805/36805 [==============================] - 16s 439us/sample - loss: 1.3105 - acc: 0.5782 - val_loss: 0.9874 - val_acc: 0.7093\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2919 - acc: 0.5872\n",
      "Epoch 00029: val_loss improved from 0.98737 to 0.97122, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_4_conv_checkpoint/029-0.9712.hdf5\n",
      "36805/36805 [==============================] - 16s 437us/sample - loss: 1.2920 - acc: 0.5872 - val_loss: 0.9712 - val_acc: 0.7188\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2820 - acc: 0.5892\n",
      "Epoch 00030: val_loss did not improve from 0.97122\n",
      "36805/36805 [==============================] - 16s 439us/sample - loss: 1.2819 - acc: 0.5892 - val_loss: 0.9727 - val_acc: 0.7147\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2686 - acc: 0.5941\n",
      "Epoch 00031: val_loss improved from 0.97122 to 0.95286, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_4_conv_checkpoint/031-0.9529.hdf5\n",
      "36805/36805 [==============================] - 16s 437us/sample - loss: 1.2686 - acc: 0.5941 - val_loss: 0.9529 - val_acc: 0.7198\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2413 - acc: 0.6016\n",
      "Epoch 00032: val_loss improved from 0.95286 to 0.92666, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_4_conv_checkpoint/032-0.9267.hdf5\n",
      "36805/36805 [==============================] - 16s 437us/sample - loss: 1.2413 - acc: 0.6016 - val_loss: 0.9267 - val_acc: 0.7340\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2384 - acc: 0.6025\n",
      "Epoch 00033: val_loss did not improve from 0.92666\n",
      "36805/36805 [==============================] - 16s 437us/sample - loss: 1.2384 - acc: 0.6025 - val_loss: 0.9398 - val_acc: 0.7230\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2310 - acc: 0.6035\n",
      "Epoch 00034: val_loss improved from 0.92666 to 0.90946, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_4_conv_checkpoint/034-0.9095.hdf5\n",
      "36805/36805 [==============================] - 16s 440us/sample - loss: 1.2309 - acc: 0.6036 - val_loss: 0.9095 - val_acc: 0.7354\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2262 - acc: 0.6095\n",
      "Epoch 00035: val_loss improved from 0.90946 to 0.90443, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_4_conv_checkpoint/035-0.9044.hdf5\n",
      "36805/36805 [==============================] - 16s 442us/sample - loss: 1.2263 - acc: 0.6095 - val_loss: 0.9044 - val_acc: 0.7333\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2082 - acc: 0.6128\n",
      "Epoch 00036: val_loss improved from 0.90443 to 0.90197, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_4_conv_checkpoint/036-0.9020.hdf5\n",
      "36805/36805 [==============================] - 16s 438us/sample - loss: 1.2082 - acc: 0.6129 - val_loss: 0.9020 - val_acc: 0.7370\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2010 - acc: 0.6181\n",
      "Epoch 00037: val_loss improved from 0.90197 to 0.88165, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_4_conv_checkpoint/037-0.8816.hdf5\n",
      "36805/36805 [==============================] - 16s 435us/sample - loss: 1.2010 - acc: 0.6181 - val_loss: 0.8816 - val_acc: 0.7417\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1988 - acc: 0.6176\n",
      "Epoch 00038: val_loss improved from 0.88165 to 0.87865, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_4_conv_checkpoint/038-0.8787.hdf5\n",
      "36805/36805 [==============================] - 16s 440us/sample - loss: 1.1989 - acc: 0.6176 - val_loss: 0.8787 - val_acc: 0.7445\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1824 - acc: 0.6237\n",
      "Epoch 00039: val_loss improved from 0.87865 to 0.85968, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_4_conv_checkpoint/039-0.8597.hdf5\n",
      "36805/36805 [==============================] - 16s 438us/sample - loss: 1.1823 - acc: 0.6237 - val_loss: 0.8597 - val_acc: 0.7442\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1778 - acc: 0.6256\n",
      "Epoch 00040: val_loss improved from 0.85968 to 0.85378, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_4_conv_checkpoint/040-0.8538.hdf5\n",
      "36805/36805 [==============================] - 16s 438us/sample - loss: 1.1778 - acc: 0.6256 - val_loss: 0.8538 - val_acc: 0.7526\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1719 - acc: 0.6248\n",
      "Epoch 00041: val_loss improved from 0.85378 to 0.84577, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_4_conv_checkpoint/041-0.8458.hdf5\n",
      "36805/36805 [==============================] - 16s 441us/sample - loss: 1.1720 - acc: 0.6248 - val_loss: 0.8458 - val_acc: 0.7568\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1595 - acc: 0.6308\n",
      "Epoch 00042: val_loss improved from 0.84577 to 0.84477, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_4_conv_checkpoint/042-0.8448.hdf5\n",
      "36805/36805 [==============================] - 16s 440us/sample - loss: 1.1594 - acc: 0.6308 - val_loss: 0.8448 - val_acc: 0.7536\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1595 - acc: 0.6312\n",
      "Epoch 00043: val_loss improved from 0.84477 to 0.84094, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_4_conv_checkpoint/043-0.8409.hdf5\n",
      "36805/36805 [==============================] - 16s 440us/sample - loss: 1.1594 - acc: 0.6312 - val_loss: 0.8409 - val_acc: 0.7522\n",
      "Epoch 44/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1502 - acc: 0.6322\n",
      "Epoch 00044: val_loss did not improve from 0.84094\n",
      "36805/36805 [==============================] - 16s 437us/sample - loss: 1.1503 - acc: 0.6322 - val_loss: 0.8423 - val_acc: 0.7549\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1448 - acc: 0.6365\n",
      "Epoch 00045: val_loss improved from 0.84094 to 0.82839, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_4_conv_checkpoint/045-0.8284.hdf5\n",
      "36805/36805 [==============================] - 16s 436us/sample - loss: 1.1448 - acc: 0.6365 - val_loss: 0.8284 - val_acc: 0.7605\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1407 - acc: 0.6374\n",
      "Epoch 00046: val_loss improved from 0.82839 to 0.80938, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_4_conv_checkpoint/046-0.8094.hdf5\n",
      "36805/36805 [==============================] - 16s 438us/sample - loss: 1.1407 - acc: 0.6373 - val_loss: 0.8094 - val_acc: 0.7659\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1264 - acc: 0.6411\n",
      "Epoch 00047: val_loss improved from 0.80938 to 0.80580, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_4_conv_checkpoint/047-0.8058.hdf5\n",
      "36805/36805 [==============================] - 16s 436us/sample - loss: 1.1265 - acc: 0.6411 - val_loss: 0.8058 - val_acc: 0.7650\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1224 - acc: 0.6420\n",
      "Epoch 00048: val_loss improved from 0.80580 to 0.80292, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_4_conv_checkpoint/048-0.8029.hdf5\n",
      "36805/36805 [==============================] - 16s 437us/sample - loss: 1.1226 - acc: 0.6419 - val_loss: 0.8029 - val_acc: 0.7657\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1236 - acc: 0.6430\n",
      "Epoch 00049: val_loss improved from 0.80292 to 0.79754, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_4_conv_checkpoint/049-0.7975.hdf5\n",
      "36805/36805 [==============================] - 16s 438us/sample - loss: 1.1236 - acc: 0.6430 - val_loss: 0.7975 - val_acc: 0.7661\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1097 - acc: 0.6457\n",
      "Epoch 00050: val_loss improved from 0.79754 to 0.79171, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_4_conv_checkpoint/050-0.7917.hdf5\n",
      "36805/36805 [==============================] - 16s 437us/sample - loss: 1.1097 - acc: 0.6456 - val_loss: 0.7917 - val_acc: 0.7694\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1078 - acc: 0.6470\n",
      "Epoch 00051: val_loss improved from 0.79171 to 0.78323, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_4_conv_checkpoint/051-0.7832.hdf5\n",
      "36805/36805 [==============================] - 16s 437us/sample - loss: 1.1078 - acc: 0.6471 - val_loss: 0.7832 - val_acc: 0.7687\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1028 - acc: 0.6499\n",
      "Epoch 00052: val_loss improved from 0.78323 to 0.77885, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_4_conv_checkpoint/052-0.7788.hdf5\n",
      "36805/36805 [==============================] - 16s 436us/sample - loss: 1.1029 - acc: 0.6498 - val_loss: 0.7788 - val_acc: 0.7729\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0918 - acc: 0.6503\n",
      "Epoch 00053: val_loss improved from 0.77885 to 0.77304, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_4_conv_checkpoint/053-0.7730.hdf5\n",
      "36805/36805 [==============================] - 16s 437us/sample - loss: 1.0917 - acc: 0.6503 - val_loss: 0.7730 - val_acc: 0.7727\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0928 - acc: 0.6521\n",
      "Epoch 00054: val_loss improved from 0.77304 to 0.77062, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_4_conv_checkpoint/054-0.7706.hdf5\n",
      "36805/36805 [==============================] - 16s 435us/sample - loss: 1.0928 - acc: 0.6521 - val_loss: 0.7706 - val_acc: 0.7734\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0863 - acc: 0.6559\n",
      "Epoch 00055: val_loss improved from 0.77062 to 0.76244, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_4_conv_checkpoint/055-0.7624.hdf5\n",
      "36805/36805 [==============================] - 16s 436us/sample - loss: 1.0862 - acc: 0.6560 - val_loss: 0.7624 - val_acc: 0.7757\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0851 - acc: 0.6532\n",
      "Epoch 00056: val_loss did not improve from 0.76244\n",
      "36805/36805 [==============================] - 16s 436us/sample - loss: 1.0850 - acc: 0.6533 - val_loss: 0.7627 - val_acc: 0.7773\n",
      "Epoch 57/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0857 - acc: 0.6565\n",
      "Epoch 00057: val_loss improved from 0.76244 to 0.75945, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_4_conv_checkpoint/057-0.7594.hdf5\n",
      "36805/36805 [==============================] - 16s 439us/sample - loss: 1.0855 - acc: 0.6566 - val_loss: 0.7594 - val_acc: 0.7759\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0800 - acc: 0.6551\n",
      "Epoch 00058: val_loss improved from 0.75945 to 0.75394, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_4_conv_checkpoint/058-0.7539.hdf5\n",
      "36805/36805 [==============================] - 16s 441us/sample - loss: 1.0800 - acc: 0.6551 - val_loss: 0.7539 - val_acc: 0.7836\n",
      "Epoch 59/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0710 - acc: 0.6616\n",
      "Epoch 00059: val_loss did not improve from 0.75394\n",
      "36805/36805 [==============================] - 16s 435us/sample - loss: 1.0711 - acc: 0.6615 - val_loss: 0.7540 - val_acc: 0.7838\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0715 - acc: 0.6590\n",
      "Epoch 00060: val_loss did not improve from 0.75394\n",
      "36805/36805 [==============================] - 16s 439us/sample - loss: 1.0715 - acc: 0.6590 - val_loss: 0.7695 - val_acc: 0.7727\n",
      "Epoch 61/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0670 - acc: 0.6606\n",
      "Epoch 00061: val_loss improved from 0.75394 to 0.75263, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_4_conv_checkpoint/061-0.7526.hdf5\n",
      "36805/36805 [==============================] - 16s 436us/sample - loss: 1.0670 - acc: 0.6608 - val_loss: 0.7526 - val_acc: 0.7785\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0626 - acc: 0.6615\n",
      "Epoch 00062: val_loss improved from 0.75263 to 0.72822, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_4_conv_checkpoint/062-0.7282.hdf5\n",
      "36805/36805 [==============================] - 16s 435us/sample - loss: 1.0625 - acc: 0.6615 - val_loss: 0.7282 - val_acc: 0.7904\n",
      "Epoch 63/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0581 - acc: 0.6643\n",
      "Epoch 00063: val_loss did not improve from 0.72822\n",
      "36805/36805 [==============================] - 16s 437us/sample - loss: 1.0581 - acc: 0.6643 - val_loss: 0.7362 - val_acc: 0.7848\n",
      "Epoch 64/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0534 - acc: 0.6665\n",
      "Epoch 00064: val_loss did not improve from 0.72822\n",
      "36805/36805 [==============================] - 16s 435us/sample - loss: 1.0530 - acc: 0.6666 - val_loss: 0.7420 - val_acc: 0.7841\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0522 - acc: 0.6660\n",
      "Epoch 00065: val_loss did not improve from 0.72822\n",
      "36805/36805 [==============================] - 16s 435us/sample - loss: 1.0521 - acc: 0.6660 - val_loss: 0.7313 - val_acc: 0.7824\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0409 - acc: 0.6690\n",
      "Epoch 00066: val_loss improved from 0.72822 to 0.72573, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_4_conv_checkpoint/066-0.7257.hdf5\n",
      "36805/36805 [==============================] - 16s 436us/sample - loss: 1.0409 - acc: 0.6690 - val_loss: 0.7257 - val_acc: 0.7866\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0440 - acc: 0.6674\n",
      "Epoch 00067: val_loss improved from 0.72573 to 0.72556, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_4_conv_checkpoint/067-0.7256.hdf5\n",
      "36805/36805 [==============================] - 16s 436us/sample - loss: 1.0440 - acc: 0.6674 - val_loss: 0.7256 - val_acc: 0.7899\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0394 - acc: 0.6676\n",
      "Epoch 00068: val_loss improved from 0.72556 to 0.71960, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_4_conv_checkpoint/068-0.7196.hdf5\n",
      "36805/36805 [==============================] - 16s 437us/sample - loss: 1.0394 - acc: 0.6676 - val_loss: 0.7196 - val_acc: 0.7929\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0299 - acc: 0.6722\n",
      "Epoch 00069: val_loss did not improve from 0.71960\n",
      "36805/36805 [==============================] - 16s 436us/sample - loss: 1.0300 - acc: 0.6722 - val_loss: 0.7261 - val_acc: 0.7873\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0383 - acc: 0.6703\n",
      "Epoch 00070: val_loss improved from 0.71960 to 0.71543, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_4_conv_checkpoint/070-0.7154.hdf5\n",
      "36805/36805 [==============================] - 16s 436us/sample - loss: 1.0384 - acc: 0.6703 - val_loss: 0.7154 - val_acc: 0.7936\n",
      "Epoch 71/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0325 - acc: 0.6723\n",
      "Epoch 00071: val_loss improved from 0.71543 to 0.71383, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_4_conv_checkpoint/071-0.7138.hdf5\n",
      "36805/36805 [==============================] - 16s 438us/sample - loss: 1.0325 - acc: 0.6723 - val_loss: 0.7138 - val_acc: 0.7948\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0274 - acc: 0.6723\n",
      "Epoch 00072: val_loss improved from 0.71383 to 0.71188, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_4_conv_checkpoint/072-0.7119.hdf5\n",
      "36805/36805 [==============================] - 16s 436us/sample - loss: 1.0274 - acc: 0.6723 - val_loss: 0.7119 - val_acc: 0.7915\n",
      "Epoch 73/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0285 - acc: 0.6743\n",
      "Epoch 00073: val_loss improved from 0.71188 to 0.70650, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_4_conv_checkpoint/073-0.7065.hdf5\n",
      "36805/36805 [==============================] - 16s 435us/sample - loss: 1.0284 - acc: 0.6743 - val_loss: 0.7065 - val_acc: 0.7955\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0242 - acc: 0.6730\n",
      "Epoch 00074: val_loss did not improve from 0.70650\n",
      "36805/36805 [==============================] - 16s 434us/sample - loss: 1.0242 - acc: 0.6730 - val_loss: 0.7068 - val_acc: 0.7948\n",
      "Epoch 75/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0200 - acc: 0.6764\n",
      "Epoch 00075: val_loss did not improve from 0.70650\n",
      "36805/36805 [==============================] - 16s 434us/sample - loss: 1.0200 - acc: 0.6763 - val_loss: 0.7104 - val_acc: 0.7955\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0165 - acc: 0.6788\n",
      "Epoch 00076: val_loss improved from 0.70650 to 0.70084, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_4_conv_checkpoint/076-0.7008.hdf5\n",
      "36805/36805 [==============================] - 16s 439us/sample - loss: 1.0166 - acc: 0.6788 - val_loss: 0.7008 - val_acc: 0.7978\n",
      "Epoch 77/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0146 - acc: 0.6767\n",
      "Epoch 00077: val_loss did not improve from 0.70084\n",
      "36805/36805 [==============================] - 16s 437us/sample - loss: 1.0147 - acc: 0.6766 - val_loss: 0.7043 - val_acc: 0.7922\n",
      "Epoch 78/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0173 - acc: 0.6769\n",
      "Epoch 00078: val_loss improved from 0.70084 to 0.69283, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_4_conv_checkpoint/078-0.6928.hdf5\n",
      "36805/36805 [==============================] - 16s 434us/sample - loss: 1.0172 - acc: 0.6768 - val_loss: 0.6928 - val_acc: 0.8001\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0051 - acc: 0.6797\n",
      "Epoch 00079: val_loss improved from 0.69283 to 0.69036, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_4_conv_checkpoint/079-0.6904.hdf5\n",
      "36805/36805 [==============================] - 16s 436us/sample - loss: 1.0050 - acc: 0.6797 - val_loss: 0.6904 - val_acc: 0.8001\n",
      "Epoch 80/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0118 - acc: 0.6794\n",
      "Epoch 00080: val_loss did not improve from 0.69036\n",
      "36805/36805 [==============================] - 16s 435us/sample - loss: 1.0119 - acc: 0.6794 - val_loss: 0.6949 - val_acc: 0.7985\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0100 - acc: 0.6785\n",
      "Epoch 00081: val_loss did not improve from 0.69036\n",
      "36805/36805 [==============================] - 16s 436us/sample - loss: 1.0100 - acc: 0.6785 - val_loss: 0.6912 - val_acc: 0.7973\n",
      "Epoch 82/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9981 - acc: 0.6807\n",
      "Epoch 00082: val_loss did not improve from 0.69036\n",
      "36805/36805 [==============================] - 16s 433us/sample - loss: 0.9985 - acc: 0.6805 - val_loss: 0.6927 - val_acc: 0.7969\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0073 - acc: 0.6808\n",
      "Epoch 00083: val_loss improved from 0.69036 to 0.68542, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_4_conv_checkpoint/083-0.6854.hdf5\n",
      "36805/36805 [==============================] - 16s 439us/sample - loss: 1.0072 - acc: 0.6807 - val_loss: 0.6854 - val_acc: 0.8015\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9984 - acc: 0.6829\n",
      "Epoch 00084: val_loss did not improve from 0.68542\n",
      "36805/36805 [==============================] - 16s 436us/sample - loss: 0.9984 - acc: 0.6830 - val_loss: 0.6968 - val_acc: 0.7992\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0001 - acc: 0.6845\n",
      "Epoch 00085: val_loss improved from 0.68542 to 0.68124, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_4_conv_checkpoint/085-0.6812.hdf5\n",
      "36805/36805 [==============================] - 16s 438us/sample - loss: 1.0001 - acc: 0.6845 - val_loss: 0.6812 - val_acc: 0.8006\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9905 - acc: 0.6879\n",
      "Epoch 00086: val_loss improved from 0.68124 to 0.66844, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_4_conv_checkpoint/086-0.6684.hdf5\n",
      "36805/36805 [==============================] - 16s 437us/sample - loss: 0.9904 - acc: 0.6879 - val_loss: 0.6684 - val_acc: 0.8043\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9961 - acc: 0.6862\n",
      "Epoch 00087: val_loss did not improve from 0.66844\n",
      "36805/36805 [==============================] - 16s 437us/sample - loss: 0.9960 - acc: 0.6862 - val_loss: 0.6785 - val_acc: 0.8029\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9930 - acc: 0.6845\n",
      "Epoch 00088: val_loss did not improve from 0.66844\n",
      "36805/36805 [==============================] - 16s 435us/sample - loss: 0.9930 - acc: 0.6845 - val_loss: 0.6799 - val_acc: 0.7983\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9893 - acc: 0.6879\n",
      "Epoch 00089: val_loss did not improve from 0.66844\n",
      "36805/36805 [==============================] - 16s 435us/sample - loss: 0.9894 - acc: 0.6879 - val_loss: 0.6731 - val_acc: 0.8027\n",
      "Epoch 90/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9803 - acc: 0.6893\n",
      "Epoch 00090: val_loss did not improve from 0.66844\n",
      "36805/36805 [==============================] - 16s 438us/sample - loss: 0.9801 - acc: 0.6893 - val_loss: 0.6687 - val_acc: 0.8057\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9794 - acc: 0.6933\n",
      "Epoch 00091: val_loss did not improve from 0.66844\n",
      "36805/36805 [==============================] - 16s 437us/sample - loss: 0.9794 - acc: 0.6934 - val_loss: 0.6734 - val_acc: 0.8071\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9807 - acc: 0.6881\n",
      "Epoch 00092: val_loss improved from 0.66844 to 0.66404, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_4_conv_checkpoint/092-0.6640.hdf5\n",
      "36805/36805 [==============================] - 16s 439us/sample - loss: 0.9807 - acc: 0.6881 - val_loss: 0.6640 - val_acc: 0.8088\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9809 - acc: 0.6881\n",
      "Epoch 00093: val_loss improved from 0.66404 to 0.66279, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_4_conv_checkpoint/093-0.6628.hdf5\n",
      "36805/36805 [==============================] - 16s 438us/sample - loss: 0.9809 - acc: 0.6882 - val_loss: 0.6628 - val_acc: 0.8076\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9720 - acc: 0.6895\n",
      "Epoch 00094: val_loss improved from 0.66279 to 0.65345, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_4_conv_checkpoint/094-0.6535.hdf5\n",
      "36805/36805 [==============================] - 16s 438us/sample - loss: 0.9720 - acc: 0.6895 - val_loss: 0.6535 - val_acc: 0.8085\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9814 - acc: 0.6895\n",
      "Epoch 00095: val_loss did not improve from 0.65345\n",
      "36805/36805 [==============================] - 16s 434us/sample - loss: 0.9814 - acc: 0.6895 - val_loss: 0.6559 - val_acc: 0.8039\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9779 - acc: 0.6893\n",
      "Epoch 00096: val_loss did not improve from 0.65345\n",
      "36805/36805 [==============================] - 16s 437us/sample - loss: 0.9779 - acc: 0.6893 - val_loss: 0.6783 - val_acc: 0.8027\n",
      "Epoch 97/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9678 - acc: 0.6920\n",
      "Epoch 00097: val_loss did not improve from 0.65345\n",
      "36805/36805 [==============================] - 16s 436us/sample - loss: 0.9676 - acc: 0.6921 - val_loss: 0.6547 - val_acc: 0.8053\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9749 - acc: 0.6918\n",
      "Epoch 00098: val_loss improved from 0.65345 to 0.65251, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_4_conv_checkpoint/098-0.6525.hdf5\n",
      "36805/36805 [==============================] - 16s 437us/sample - loss: 0.9750 - acc: 0.6918 - val_loss: 0.6525 - val_acc: 0.8090\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9651 - acc: 0.6934\n",
      "Epoch 00099: val_loss did not improve from 0.65251\n",
      "36805/36805 [==============================] - 16s 434us/sample - loss: 0.9651 - acc: 0.6934 - val_loss: 0.6534 - val_acc: 0.8060\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9721 - acc: 0.6926\n",
      "Epoch 00100: val_loss did not improve from 0.65251\n",
      "36805/36805 [==============================] - 16s 435us/sample - loss: 0.9721 - acc: 0.6926 - val_loss: 0.6560 - val_acc: 0.8078\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9645 - acc: 0.6932\n",
      "Epoch 00101: val_loss did not improve from 0.65251\n",
      "36805/36805 [==============================] - 16s 436us/sample - loss: 0.9644 - acc: 0.6932 - val_loss: 0.6531 - val_acc: 0.8102\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9639 - acc: 0.6952\n",
      "Epoch 00102: val_loss improved from 0.65251 to 0.64967, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_4_conv_checkpoint/102-0.6497.hdf5\n",
      "36805/36805 [==============================] - 16s 434us/sample - loss: 0.9640 - acc: 0.6952 - val_loss: 0.6497 - val_acc: 0.8097\n",
      "Epoch 103/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9578 - acc: 0.6954\n",
      "Epoch 00103: val_loss did not improve from 0.64967\n",
      "36805/36805 [==============================] - 16s 432us/sample - loss: 0.9577 - acc: 0.6955 - val_loss: 0.6518 - val_acc: 0.8029\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9552 - acc: 0.6974\n",
      "Epoch 00104: val_loss improved from 0.64967 to 0.64867, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_4_conv_checkpoint/104-0.6487.hdf5\n",
      "36805/36805 [==============================] - 16s 438us/sample - loss: 0.9552 - acc: 0.6974 - val_loss: 0.6487 - val_acc: 0.8113\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9525 - acc: 0.6971\n",
      "Epoch 00105: val_loss improved from 0.64867 to 0.63726, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_4_conv_checkpoint/105-0.6373.hdf5\n",
      "36805/36805 [==============================] - 16s 437us/sample - loss: 0.9525 - acc: 0.6971 - val_loss: 0.6373 - val_acc: 0.8104\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9586 - acc: 0.6951\n",
      "Epoch 00106: val_loss did not improve from 0.63726\n",
      "36805/36805 [==============================] - 16s 438us/sample - loss: 0.9586 - acc: 0.6952 - val_loss: 0.6519 - val_acc: 0.8081\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9569 - acc: 0.6983\n",
      "Epoch 00107: val_loss did not improve from 0.63726\n",
      "36805/36805 [==============================] - 16s 438us/sample - loss: 0.9569 - acc: 0.6983 - val_loss: 0.6413 - val_acc: 0.8127\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9536 - acc: 0.6992\n",
      "Epoch 00108: val_loss did not improve from 0.63726\n",
      "36805/36805 [==============================] - 16s 438us/sample - loss: 0.9537 - acc: 0.6991 - val_loss: 0.6489 - val_acc: 0.8123\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9537 - acc: 0.6962\n",
      "Epoch 00109: val_loss did not improve from 0.63726\n",
      "36805/36805 [==============================] - 16s 436us/sample - loss: 0.9537 - acc: 0.6961 - val_loss: 0.6438 - val_acc: 0.8099\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9539 - acc: 0.6981\n",
      "Epoch 00110: val_loss improved from 0.63726 to 0.63414, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_4_conv_checkpoint/110-0.6341.hdf5\n",
      "36805/36805 [==============================] - 16s 439us/sample - loss: 0.9539 - acc: 0.6981 - val_loss: 0.6341 - val_acc: 0.8134\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9516 - acc: 0.6982\n",
      "Epoch 00111: val_loss improved from 0.63414 to 0.63352, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_4_conv_checkpoint/111-0.6335.hdf5\n",
      "36805/36805 [==============================] - 16s 438us/sample - loss: 0.9515 - acc: 0.6982 - val_loss: 0.6335 - val_acc: 0.8109\n",
      "Epoch 112/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9445 - acc: 0.7014\n",
      "Epoch 00112: val_loss did not improve from 0.63352\n",
      "36805/36805 [==============================] - 16s 436us/sample - loss: 0.9452 - acc: 0.7013 - val_loss: 0.6365 - val_acc: 0.8116\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9430 - acc: 0.6986\n",
      "Epoch 00113: val_loss did not improve from 0.63352\n",
      "36805/36805 [==============================] - 16s 436us/sample - loss: 0.9430 - acc: 0.6986 - val_loss: 0.6411 - val_acc: 0.8109\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9415 - acc: 0.7021\n",
      "Epoch 00114: val_loss did not improve from 0.63352\n",
      "36805/36805 [==============================] - 16s 436us/sample - loss: 0.9415 - acc: 0.7021 - val_loss: 0.6450 - val_acc: 0.8109\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9395 - acc: 0.7020\n",
      "Epoch 00115: val_loss improved from 0.63352 to 0.63100, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_4_conv_checkpoint/115-0.6310.hdf5\n",
      "36805/36805 [==============================] - 16s 437us/sample - loss: 0.9395 - acc: 0.7021 - val_loss: 0.6310 - val_acc: 0.8162\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9355 - acc: 0.7037\n",
      "Epoch 00116: val_loss improved from 0.63100 to 0.62605, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_4_conv_checkpoint/116-0.6261.hdf5\n",
      "36805/36805 [==============================] - 16s 436us/sample - loss: 0.9354 - acc: 0.7037 - val_loss: 0.6261 - val_acc: 0.8150\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9399 - acc: 0.7025\n",
      "Epoch 00117: val_loss did not improve from 0.62605\n",
      "36805/36805 [==============================] - 16s 435us/sample - loss: 0.9398 - acc: 0.7025 - val_loss: 0.6291 - val_acc: 0.8169\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9399 - acc: 0.7015\n",
      "Epoch 00118: val_loss improved from 0.62605 to 0.62439, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_4_conv_checkpoint/118-0.6244.hdf5\n",
      "36805/36805 [==============================] - 16s 437us/sample - loss: 0.9398 - acc: 0.7015 - val_loss: 0.6244 - val_acc: 0.8190\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9356 - acc: 0.7035\n",
      "Epoch 00119: val_loss did not improve from 0.62439\n",
      "36805/36805 [==============================] - 16s 435us/sample - loss: 0.9358 - acc: 0.7034 - val_loss: 0.6328 - val_acc: 0.8146\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9451 - acc: 0.7021\n",
      "Epoch 00120: val_loss did not improve from 0.62439\n",
      "36805/36805 [==============================] - 16s 437us/sample - loss: 0.9452 - acc: 0.7020 - val_loss: 0.6319 - val_acc: 0.8153\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9387 - acc: 0.7011\n",
      "Epoch 00121: val_loss did not improve from 0.62439\n",
      "36805/36805 [==============================] - 16s 435us/sample - loss: 0.9386 - acc: 0.7011 - val_loss: 0.6361 - val_acc: 0.8097\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9315 - acc: 0.7044\n",
      "Epoch 00122: val_loss improved from 0.62439 to 0.62345, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_4_conv_checkpoint/122-0.6235.hdf5\n",
      "36805/36805 [==============================] - 16s 436us/sample - loss: 0.9315 - acc: 0.7044 - val_loss: 0.6235 - val_acc: 0.8155\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9309 - acc: 0.7041\n",
      "Epoch 00123: val_loss did not improve from 0.62345\n",
      "36805/36805 [==============================] - 16s 436us/sample - loss: 0.9309 - acc: 0.7041 - val_loss: 0.6244 - val_acc: 0.8171\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9306 - acc: 0.7028\n",
      "Epoch 00124: val_loss improved from 0.62345 to 0.61443, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_4_conv_checkpoint/124-0.6144.hdf5\n",
      "36805/36805 [==============================] - 16s 439us/sample - loss: 0.9307 - acc: 0.7028 - val_loss: 0.6144 - val_acc: 0.8155\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9285 - acc: 0.7058\n",
      "Epoch 00125: val_loss did not improve from 0.61443\n",
      "36805/36805 [==============================] - 16s 438us/sample - loss: 0.9286 - acc: 0.7058 - val_loss: 0.6170 - val_acc: 0.8185\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9315 - acc: 0.7041\n",
      "Epoch 00126: val_loss did not improve from 0.61443\n",
      "36805/36805 [==============================] - 16s 437us/sample - loss: 0.9316 - acc: 0.7041 - val_loss: 0.6160 - val_acc: 0.8188\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9231 - acc: 0.7097\n",
      "Epoch 00127: val_loss did not improve from 0.61443\n",
      "36805/36805 [==============================] - 16s 438us/sample - loss: 0.9230 - acc: 0.7097 - val_loss: 0.6193 - val_acc: 0.8188\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9157 - acc: 0.7098\n",
      "Epoch 00128: val_loss improved from 0.61443 to 0.60602, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_4_conv_checkpoint/128-0.6060.hdf5\n",
      "36805/36805 [==============================] - 16s 439us/sample - loss: 0.9158 - acc: 0.7097 - val_loss: 0.6060 - val_acc: 0.8237\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9203 - acc: 0.7083\n",
      "Epoch 00129: val_loss did not improve from 0.60602\n",
      "36805/36805 [==============================] - 16s 437us/sample - loss: 0.9202 - acc: 0.7082 - val_loss: 0.6102 - val_acc: 0.8216\n",
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9231 - acc: 0.7069\n",
      "Epoch 00130: val_loss did not improve from 0.60602\n",
      "36805/36805 [==============================] - 16s 439us/sample - loss: 0.9231 - acc: 0.7069 - val_loss: 0.6147 - val_acc: 0.8192\n",
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9176 - acc: 0.7073\n",
      "Epoch 00131: val_loss did not improve from 0.60602\n",
      "36805/36805 [==============================] - 16s 439us/sample - loss: 0.9177 - acc: 0.7073 - val_loss: 0.6117 - val_acc: 0.8218\n",
      "Epoch 132/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9158 - acc: 0.7104\n",
      "Epoch 00132: val_loss did not improve from 0.60602\n",
      "36805/36805 [==============================] - 16s 434us/sample - loss: 0.9161 - acc: 0.7103 - val_loss: 0.6114 - val_acc: 0.8171\n",
      "Epoch 133/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9184 - acc: 0.7063\n",
      "Epoch 00133: val_loss improved from 0.60602 to 0.60565, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_4_conv_checkpoint/133-0.6057.hdf5\n",
      "36805/36805 [==============================] - 16s 436us/sample - loss: 0.9184 - acc: 0.7063 - val_loss: 0.6057 - val_acc: 0.8234\n",
      "Epoch 134/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9221 - acc: 0.7095\n",
      "Epoch 00134: val_loss did not improve from 0.60565\n",
      "36805/36805 [==============================] - 16s 436us/sample - loss: 0.9221 - acc: 0.7095 - val_loss: 0.6106 - val_acc: 0.8199\n",
      "Epoch 135/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9152 - acc: 0.7093\n",
      "Epoch 00135: val_loss improved from 0.60565 to 0.60203, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_4_conv_checkpoint/135-0.6020.hdf5\n",
      "36805/36805 [==============================] - 16s 437us/sample - loss: 0.9153 - acc: 0.7093 - val_loss: 0.6020 - val_acc: 0.8218\n",
      "Epoch 136/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9187 - acc: 0.7089\n",
      "Epoch 00136: val_loss did not improve from 0.60203\n",
      "36805/36805 [==============================] - 16s 439us/sample - loss: 0.9187 - acc: 0.7088 - val_loss: 0.6100 - val_acc: 0.8211\n",
      "Epoch 137/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9118 - acc: 0.7098\n",
      "Epoch 00137: val_loss did not improve from 0.60203\n",
      "36805/36805 [==============================] - 16s 437us/sample - loss: 0.9120 - acc: 0.7097 - val_loss: 0.6170 - val_acc: 0.8153\n",
      "Epoch 138/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9071 - acc: 0.7105\n",
      "Epoch 00138: val_loss did not improve from 0.60203\n",
      "36805/36805 [==============================] - 16s 436us/sample - loss: 0.9071 - acc: 0.7106 - val_loss: 0.6174 - val_acc: 0.8143\n",
      "Epoch 139/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9107 - acc: 0.7093\n",
      "Epoch 00139: val_loss did not improve from 0.60203\n",
      "36805/36805 [==============================] - 16s 438us/sample - loss: 0.9107 - acc: 0.7094 - val_loss: 0.6134 - val_acc: 0.8197\n",
      "Epoch 140/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9058 - acc: 0.7124\n",
      "Epoch 00140: val_loss did not improve from 0.60203\n",
      "36805/36805 [==============================] - 16s 434us/sample - loss: 0.9058 - acc: 0.7124 - val_loss: 0.6084 - val_acc: 0.8204\n",
      "Epoch 141/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9004 - acc: 0.7154\n",
      "Epoch 00141: val_loss did not improve from 0.60203\n",
      "36805/36805 [==============================] - 16s 433us/sample - loss: 0.9004 - acc: 0.7154 - val_loss: 0.6053 - val_acc: 0.8232\n",
      "Epoch 142/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9037 - acc: 0.7136\n",
      "Epoch 00142: val_loss improved from 0.60203 to 0.59994, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_4_conv_checkpoint/142-0.5999.hdf5\n",
      "36805/36805 [==============================] - 16s 436us/sample - loss: 0.9040 - acc: 0.7135 - val_loss: 0.5999 - val_acc: 0.8213\n",
      "Epoch 143/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9040 - acc: 0.7149\n",
      "Epoch 00143: val_loss improved from 0.59994 to 0.59691, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_4_conv_checkpoint/143-0.5969.hdf5\n",
      "36805/36805 [==============================] - 16s 440us/sample - loss: 0.9040 - acc: 0.7149 - val_loss: 0.5969 - val_acc: 0.8204\n",
      "Epoch 144/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9026 - acc: 0.7136\n",
      "Epoch 00144: val_loss improved from 0.59691 to 0.59306, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_4_conv_checkpoint/144-0.5931.hdf5\n",
      "36805/36805 [==============================] - 16s 436us/sample - loss: 0.9026 - acc: 0.7136 - val_loss: 0.5931 - val_acc: 0.8253\n",
      "Epoch 145/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9007 - acc: 0.7128\n",
      "Epoch 00145: val_loss did not improve from 0.59306\n",
      "36805/36805 [==============================] - 16s 435us/sample - loss: 0.9008 - acc: 0.7127 - val_loss: 0.6099 - val_acc: 0.8197\n",
      "Epoch 146/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8982 - acc: 0.7135\n",
      "Epoch 00146: val_loss did not improve from 0.59306\n",
      "36805/36805 [==============================] - 16s 434us/sample - loss: 0.8982 - acc: 0.7135 - val_loss: 0.5978 - val_acc: 0.8195\n",
      "Epoch 147/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9026 - acc: 0.7145\n",
      "Epoch 00147: val_loss did not improve from 0.59306\n",
      "36805/36805 [==============================] - 16s 437us/sample - loss: 0.9026 - acc: 0.7145 - val_loss: 0.5960 - val_acc: 0.8239\n",
      "Epoch 148/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9008 - acc: 0.7131\n",
      "Epoch 00148: val_loss did not improve from 0.59306\n",
      "36805/36805 [==============================] - 16s 436us/sample - loss: 0.9007 - acc: 0.7132 - val_loss: 0.5933 - val_acc: 0.8253\n",
      "Epoch 149/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8936 - acc: 0.7160\n",
      "Epoch 00149: val_loss did not improve from 0.59306\n",
      "36805/36805 [==============================] - 16s 436us/sample - loss: 0.8935 - acc: 0.7160 - val_loss: 0.5954 - val_acc: 0.8267\n",
      "Epoch 150/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8975 - acc: 0.7137\n",
      "Epoch 00150: val_loss improved from 0.59306 to 0.59008, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_4_conv_checkpoint/150-0.5901.hdf5\n",
      "36805/36805 [==============================] - 16s 445us/sample - loss: 0.8975 - acc: 0.7137 - val_loss: 0.5901 - val_acc: 0.8251\n",
      "Epoch 151/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9005 - acc: 0.7125\n",
      "Epoch 00151: val_loss improved from 0.59008 to 0.58701, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_4_conv_checkpoint/151-0.5870.hdf5\n",
      "36805/36805 [==============================] - 16s 442us/sample - loss: 0.9004 - acc: 0.7125 - val_loss: 0.5870 - val_acc: 0.8288\n",
      "Epoch 152/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8916 - acc: 0.7168\n",
      "Epoch 00152: val_loss did not improve from 0.58701\n",
      "36805/36805 [==============================] - 16s 444us/sample - loss: 0.8916 - acc: 0.7168 - val_loss: 0.5976 - val_acc: 0.8255\n",
      "Epoch 153/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8880 - acc: 0.7179\n",
      "Epoch 00153: val_loss did not improve from 0.58701\n",
      "36805/36805 [==============================] - 16s 446us/sample - loss: 0.8879 - acc: 0.7179 - val_loss: 0.5927 - val_acc: 0.8267\n",
      "Epoch 154/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8901 - acc: 0.7189\n",
      "Epoch 00154: val_loss improved from 0.58701 to 0.58617, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_4_conv_checkpoint/154-0.5862.hdf5\n",
      "36805/36805 [==============================] - 16s 445us/sample - loss: 0.8901 - acc: 0.7188 - val_loss: 0.5862 - val_acc: 0.8293\n",
      "Epoch 155/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8966 - acc: 0.7170\n",
      "Epoch 00155: val_loss did not improve from 0.58617\n",
      "36805/36805 [==============================] - 16s 443us/sample - loss: 0.8965 - acc: 0.7170 - val_loss: 0.6067 - val_acc: 0.8223\n",
      "Epoch 156/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8923 - acc: 0.7181\n",
      "Epoch 00156: val_loss did not improve from 0.58617\n",
      "36805/36805 [==============================] - 16s 440us/sample - loss: 0.8923 - acc: 0.7181 - val_loss: 0.5872 - val_acc: 0.8267\n",
      "Epoch 157/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8927 - acc: 0.7163\n",
      "Epoch 00157: val_loss did not improve from 0.58617\n",
      "36805/36805 [==============================] - 16s 444us/sample - loss: 0.8927 - acc: 0.7163 - val_loss: 0.5921 - val_acc: 0.8281\n",
      "Epoch 158/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8899 - acc: 0.7171\n",
      "Epoch 00158: val_loss improved from 0.58617 to 0.57911, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_4_conv_checkpoint/158-0.5791.hdf5\n",
      "36805/36805 [==============================] - 16s 442us/sample - loss: 0.8898 - acc: 0.7172 - val_loss: 0.5791 - val_acc: 0.8297\n",
      "Epoch 159/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8809 - acc: 0.7219\n",
      "Epoch 00159: val_loss improved from 0.57911 to 0.57613, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_4_conv_checkpoint/159-0.5761.hdf5\n",
      "36805/36805 [==============================] - 16s 441us/sample - loss: 0.8808 - acc: 0.7219 - val_loss: 0.5761 - val_acc: 0.8323\n",
      "Epoch 160/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8907 - acc: 0.7178\n",
      "Epoch 00160: val_loss did not improve from 0.57613\n",
      "36805/36805 [==============================] - 16s 440us/sample - loss: 0.8906 - acc: 0.7178 - val_loss: 0.5851 - val_acc: 0.8281\n",
      "Epoch 161/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8898 - acc: 0.7184\n",
      "Epoch 00161: val_loss did not improve from 0.57613\n",
      "36805/36805 [==============================] - 16s 443us/sample - loss: 0.8898 - acc: 0.7184 - val_loss: 0.5866 - val_acc: 0.8309\n",
      "Epoch 162/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8818 - acc: 0.7228\n",
      "Epoch 00162: val_loss did not improve from 0.57613\n",
      "36805/36805 [==============================] - 16s 445us/sample - loss: 0.8818 - acc: 0.7229 - val_loss: 0.5799 - val_acc: 0.8279\n",
      "Epoch 163/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8785 - acc: 0.7201\n",
      "Epoch 00163: val_loss did not improve from 0.57613\n",
      "36805/36805 [==============================] - 16s 441us/sample - loss: 0.8786 - acc: 0.7201 - val_loss: 0.5805 - val_acc: 0.8309\n",
      "Epoch 164/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8886 - acc: 0.7177\n",
      "Epoch 00164: val_loss did not improve from 0.57613\n",
      "36805/36805 [==============================] - 16s 442us/sample - loss: 0.8886 - acc: 0.7178 - val_loss: 0.5917 - val_acc: 0.8260\n",
      "Epoch 165/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8844 - acc: 0.7201\n",
      "Epoch 00165: val_loss did not improve from 0.57613\n",
      "36805/36805 [==============================] - 16s 442us/sample - loss: 0.8846 - acc: 0.7200 - val_loss: 0.5810 - val_acc: 0.8318\n",
      "Epoch 166/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8794 - acc: 0.7217\n",
      "Epoch 00166: val_loss did not improve from 0.57613\n",
      "36805/36805 [==============================] - 16s 442us/sample - loss: 0.8794 - acc: 0.7217 - val_loss: 0.5764 - val_acc: 0.8267\n",
      "Epoch 167/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8811 - acc: 0.7211\n",
      "Epoch 00167: val_loss did not improve from 0.57613\n",
      "36805/36805 [==============================] - 16s 443us/sample - loss: 0.8810 - acc: 0.7211 - val_loss: 0.5806 - val_acc: 0.8288\n",
      "Epoch 168/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8779 - acc: 0.7218\n",
      "Epoch 00168: val_loss did not improve from 0.57613\n",
      "36805/36805 [==============================] - 16s 441us/sample - loss: 0.8778 - acc: 0.7219 - val_loss: 0.5793 - val_acc: 0.8309\n",
      "Epoch 169/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8758 - acc: 0.7225\n",
      "Epoch 00169: val_loss did not improve from 0.57613\n",
      "36805/36805 [==============================] - 16s 444us/sample - loss: 0.8757 - acc: 0.7226 - val_loss: 0.5810 - val_acc: 0.8297\n",
      "Epoch 170/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8852 - acc: 0.7225\n",
      "Epoch 00170: val_loss improved from 0.57613 to 0.57231, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_4_conv_checkpoint/170-0.5723.hdf5\n",
      "36805/36805 [==============================] - 16s 443us/sample - loss: 0.8852 - acc: 0.7225 - val_loss: 0.5723 - val_acc: 0.8253\n",
      "Epoch 171/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8748 - acc: 0.7223\n",
      "Epoch 00171: val_loss did not improve from 0.57231\n",
      "36805/36805 [==============================] - 16s 437us/sample - loss: 0.8749 - acc: 0.7222 - val_loss: 0.5837 - val_acc: 0.8323\n",
      "Epoch 172/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8771 - acc: 0.7218\n",
      "Epoch 00172: val_loss did not improve from 0.57231\n",
      "36805/36805 [==============================] - 16s 439us/sample - loss: 0.8770 - acc: 0.7219 - val_loss: 0.5855 - val_acc: 0.8265\n",
      "Epoch 173/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8702 - acc: 0.7224\n",
      "Epoch 00173: val_loss did not improve from 0.57231\n",
      "36805/36805 [==============================] - 16s 436us/sample - loss: 0.8702 - acc: 0.7223 - val_loss: 0.5800 - val_acc: 0.8293\n",
      "Epoch 174/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8699 - acc: 0.7248\n",
      "Epoch 00174: val_loss improved from 0.57231 to 0.57009, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_4_conv_checkpoint/174-0.5701.hdf5\n",
      "36805/36805 [==============================] - 16s 437us/sample - loss: 0.8699 - acc: 0.7248 - val_loss: 0.5701 - val_acc: 0.8300\n",
      "Epoch 175/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8774 - acc: 0.7217\n",
      "Epoch 00175: val_loss did not improve from 0.57009\n",
      "36805/36805 [==============================] - 16s 438us/sample - loss: 0.8776 - acc: 0.7216 - val_loss: 0.5743 - val_acc: 0.8323\n",
      "Epoch 176/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8776 - acc: 0.7183\n",
      "Epoch 00176: val_loss did not improve from 0.57009\n",
      "36805/36805 [==============================] - 16s 438us/sample - loss: 0.8776 - acc: 0.7184 - val_loss: 0.5787 - val_acc: 0.8316\n",
      "Epoch 177/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8681 - acc: 0.7251\n",
      "Epoch 00177: val_loss improved from 0.57009 to 0.56562, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_4_conv_checkpoint/177-0.5656.hdf5\n",
      "36805/36805 [==============================] - 16s 437us/sample - loss: 0.8680 - acc: 0.7251 - val_loss: 0.5656 - val_acc: 0.8311\n",
      "Epoch 178/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8634 - acc: 0.7252\n",
      "Epoch 00178: val_loss did not improve from 0.56562\n",
      "36805/36805 [==============================] - 16s 435us/sample - loss: 0.8633 - acc: 0.7252 - val_loss: 0.5805 - val_acc: 0.8311\n",
      "Epoch 179/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8629 - acc: 0.7247\n",
      "Epoch 00179: val_loss did not improve from 0.56562\n",
      "36805/36805 [==============================] - 16s 437us/sample - loss: 0.8628 - acc: 0.7247 - val_loss: 0.5767 - val_acc: 0.8281\n",
      "Epoch 180/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8703 - acc: 0.7232\n",
      "Epoch 00180: val_loss did not improve from 0.56562\n",
      "36805/36805 [==============================] - 16s 436us/sample - loss: 0.8704 - acc: 0.7232 - val_loss: 0.5701 - val_acc: 0.8323\n",
      "Epoch 181/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8710 - acc: 0.7240\n",
      "Epoch 00181: val_loss did not improve from 0.56562\n",
      "36805/36805 [==============================] - 16s 436us/sample - loss: 0.8710 - acc: 0.7240 - val_loss: 0.5754 - val_acc: 0.8262\n",
      "Epoch 182/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8702 - acc: 0.7243\n",
      "Epoch 00182: val_loss did not improve from 0.56562\n",
      "36805/36805 [==============================] - 16s 435us/sample - loss: 0.8702 - acc: 0.7243 - val_loss: 0.5819 - val_acc: 0.8325\n",
      "Epoch 183/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8652 - acc: 0.7263\n",
      "Epoch 00183: val_loss did not improve from 0.56562\n",
      "36805/36805 [==============================] - 16s 436us/sample - loss: 0.8652 - acc: 0.7263 - val_loss: 0.5791 - val_acc: 0.8290\n",
      "Epoch 184/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8641 - acc: 0.7247\n",
      "Epoch 00184: val_loss improved from 0.56562 to 0.56528, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_4_conv_checkpoint/184-0.5653.hdf5\n",
      "36805/36805 [==============================] - 16s 436us/sample - loss: 0.8642 - acc: 0.7247 - val_loss: 0.5653 - val_acc: 0.8341\n",
      "Epoch 185/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8702 - acc: 0.7244\n",
      "Epoch 00185: val_loss did not improve from 0.56528\n",
      "36805/36805 [==============================] - 16s 436us/sample - loss: 0.8704 - acc: 0.7243 - val_loss: 0.5699 - val_acc: 0.8332\n",
      "Epoch 186/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8593 - acc: 0.7281\n",
      "Epoch 00186: val_loss did not improve from 0.56528\n",
      "36805/36805 [==============================] - 16s 438us/sample - loss: 0.8593 - acc: 0.7281 - val_loss: 0.5666 - val_acc: 0.8334\n",
      "Epoch 187/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8566 - acc: 0.7291\n",
      "Epoch 00187: val_loss did not improve from 0.56528\n",
      "36805/36805 [==============================] - 16s 446us/sample - loss: 0.8566 - acc: 0.7291 - val_loss: 0.5736 - val_acc: 0.8348\n",
      "Epoch 188/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8666 - acc: 0.7236\n",
      "Epoch 00188: val_loss improved from 0.56528 to 0.56509, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_4_conv_checkpoint/188-0.5651.hdf5\n",
      "36805/36805 [==============================] - 16s 445us/sample - loss: 0.8666 - acc: 0.7237 - val_loss: 0.5651 - val_acc: 0.8318\n",
      "Epoch 189/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8646 - acc: 0.7257\n",
      "Epoch 00189: val_loss did not improve from 0.56509\n",
      "36805/36805 [==============================] - 16s 444us/sample - loss: 0.8646 - acc: 0.7257 - val_loss: 0.5772 - val_acc: 0.8334\n",
      "Epoch 190/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8571 - acc: 0.7271\n",
      "Epoch 00190: val_loss did not improve from 0.56509\n",
      "36805/36805 [==============================] - 16s 446us/sample - loss: 0.8572 - acc: 0.7271 - val_loss: 0.5665 - val_acc: 0.8374\n",
      "Epoch 191/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8595 - acc: 0.7263\n",
      "Epoch 00191: val_loss did not improve from 0.56509\n",
      "36805/36805 [==============================] - 16s 446us/sample - loss: 0.8596 - acc: 0.7263 - val_loss: 0.5724 - val_acc: 0.8321\n",
      "Epoch 192/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8607 - acc: 0.7292\n",
      "Epoch 00192: val_loss improved from 0.56509 to 0.56179, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_4_conv_checkpoint/192-0.5618.hdf5\n",
      "36805/36805 [==============================] - 16s 444us/sample - loss: 0.8606 - acc: 0.7292 - val_loss: 0.5618 - val_acc: 0.8344\n",
      "Epoch 193/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8614 - acc: 0.7279\n",
      "Epoch 00193: val_loss improved from 0.56179 to 0.55757, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_4_conv_checkpoint/193-0.5576.hdf5\n",
      "36805/36805 [==============================] - 16s 446us/sample - loss: 0.8613 - acc: 0.7279 - val_loss: 0.5576 - val_acc: 0.8372\n",
      "Epoch 194/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8624 - acc: 0.7270\n",
      "Epoch 00194: val_loss did not improve from 0.55757\n",
      "36805/36805 [==============================] - 16s 445us/sample - loss: 0.8623 - acc: 0.7270 - val_loss: 0.5596 - val_acc: 0.8346\n",
      "Epoch 195/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8596 - acc: 0.7275\n",
      "Epoch 00195: val_loss did not improve from 0.55757\n",
      "36805/36805 [==============================] - 16s 443us/sample - loss: 0.8596 - acc: 0.7275 - val_loss: 0.5603 - val_acc: 0.8309\n",
      "Epoch 196/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8610 - acc: 0.7264\n",
      "Epoch 00196: val_loss did not improve from 0.55757\n",
      "36805/36805 [==============================] - 16s 442us/sample - loss: 0.8611 - acc: 0.7264 - val_loss: 0.5624 - val_acc: 0.8374\n",
      "Epoch 197/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8589 - acc: 0.7252\n",
      "Epoch 00197: val_loss did not improve from 0.55757\n",
      "36805/36805 [==============================] - 16s 440us/sample - loss: 0.8589 - acc: 0.7252 - val_loss: 0.5664 - val_acc: 0.8397\n",
      "Epoch 198/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8586 - acc: 0.7252\n",
      "Epoch 00198: val_loss did not improve from 0.55757\n",
      "36805/36805 [==============================] - 16s 443us/sample - loss: 0.8585 - acc: 0.7252 - val_loss: 0.5721 - val_acc: 0.8341\n",
      "Epoch 199/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8580 - acc: 0.7286\n",
      "Epoch 00199: val_loss did not improve from 0.55757\n",
      "36805/36805 [==============================] - 16s 441us/sample - loss: 0.8579 - acc: 0.7287 - val_loss: 0.5613 - val_acc: 0.8351\n",
      "Epoch 200/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8571 - acc: 0.7257\n",
      "Epoch 00200: val_loss did not improve from 0.55757\n",
      "36805/36805 [==============================] - 16s 444us/sample - loss: 0.8574 - acc: 0.7257 - val_loss: 0.5630 - val_acc: 0.8367\n",
      "Epoch 201/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8540 - acc: 0.7298\n",
      "Epoch 00201: val_loss improved from 0.55757 to 0.55719, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_4_conv_checkpoint/201-0.5572.hdf5\n",
      "36805/36805 [==============================] - 16s 442us/sample - loss: 0.8539 - acc: 0.7299 - val_loss: 0.5572 - val_acc: 0.8332\n",
      "Epoch 202/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8536 - acc: 0.7294\n",
      "Epoch 00202: val_loss did not improve from 0.55719\n",
      "36805/36805 [==============================] - 16s 436us/sample - loss: 0.8538 - acc: 0.7294 - val_loss: 0.5572 - val_acc: 0.8360\n",
      "Epoch 203/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8549 - acc: 0.7293\n",
      "Epoch 00203: val_loss improved from 0.55719 to 0.55101, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_4_conv_checkpoint/203-0.5510.hdf5\n",
      "36805/36805 [==============================] - 16s 440us/sample - loss: 0.8549 - acc: 0.7293 - val_loss: 0.5510 - val_acc: 0.8400\n",
      "Epoch 204/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8520 - acc: 0.7301\n",
      "Epoch 00204: val_loss did not improve from 0.55101\n",
      "36805/36805 [==============================] - 16s 440us/sample - loss: 0.8520 - acc: 0.7301 - val_loss: 0.5647 - val_acc: 0.8323\n",
      "Epoch 205/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8483 - acc: 0.7300\n",
      "Epoch 00205: val_loss did not improve from 0.55101\n",
      "36805/36805 [==============================] - 16s 439us/sample - loss: 0.8483 - acc: 0.7300 - val_loss: 0.5625 - val_acc: 0.8316\n",
      "Epoch 206/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8466 - acc: 0.7302\n",
      "Epoch 00206: val_loss did not improve from 0.55101\n",
      "36805/36805 [==============================] - 16s 442us/sample - loss: 0.8465 - acc: 0.7303 - val_loss: 0.5567 - val_acc: 0.8369\n",
      "Epoch 207/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8474 - acc: 0.7312\n",
      "Epoch 00207: val_loss improved from 0.55101 to 0.54949, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_4_conv_checkpoint/207-0.5495.hdf5\n",
      "36805/36805 [==============================] - 16s 445us/sample - loss: 0.8473 - acc: 0.7312 - val_loss: 0.5495 - val_acc: 0.8397\n",
      "Epoch 208/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8441 - acc: 0.7313\n",
      "Epoch 00208: val_loss did not improve from 0.54949\n",
      "36805/36805 [==============================] - 16s 436us/sample - loss: 0.8437 - acc: 0.7314 - val_loss: 0.5512 - val_acc: 0.8383\n",
      "Epoch 209/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8485 - acc: 0.7333\n",
      "Epoch 00209: val_loss did not improve from 0.54949\n",
      "36805/36805 [==============================] - 16s 435us/sample - loss: 0.8485 - acc: 0.7332 - val_loss: 0.5539 - val_acc: 0.8339\n",
      "Epoch 210/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8423 - acc: 0.7328\n",
      "Epoch 00210: val_loss did not improve from 0.54949\n",
      "36805/36805 [==============================] - 16s 436us/sample - loss: 0.8424 - acc: 0.7326 - val_loss: 0.5504 - val_acc: 0.8365\n",
      "Epoch 211/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8493 - acc: 0.7305\n",
      "Epoch 00211: val_loss did not improve from 0.54949\n",
      "36805/36805 [==============================] - 16s 436us/sample - loss: 0.8493 - acc: 0.7305 - val_loss: 0.5605 - val_acc: 0.8421\n",
      "Epoch 212/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8441 - acc: 0.7317\n",
      "Epoch 00212: val_loss did not improve from 0.54949\n",
      "36805/36805 [==============================] - 16s 437us/sample - loss: 0.8442 - acc: 0.7317 - val_loss: 0.5511 - val_acc: 0.8362\n",
      "Epoch 213/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8444 - acc: 0.7302\n",
      "Epoch 00213: val_loss did not improve from 0.54949\n",
      "36805/36805 [==============================] - 16s 436us/sample - loss: 0.8444 - acc: 0.7301 - val_loss: 0.5531 - val_acc: 0.8376\n",
      "Epoch 214/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8492 - acc: 0.7302\n",
      "Epoch 00214: val_loss did not improve from 0.54949\n",
      "36805/36805 [==============================] - 16s 437us/sample - loss: 0.8491 - acc: 0.7303 - val_loss: 0.5536 - val_acc: 0.8421\n",
      "Epoch 215/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8485 - acc: 0.7324\n",
      "Epoch 00215: val_loss did not improve from 0.54949\n",
      "36805/36805 [==============================] - 16s 436us/sample - loss: 0.8486 - acc: 0.7324 - val_loss: 0.5525 - val_acc: 0.8372\n",
      "Epoch 216/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8443 - acc: 0.7349\n",
      "Epoch 00216: val_loss did not improve from 0.54949\n",
      "36805/36805 [==============================] - 16s 435us/sample - loss: 0.8442 - acc: 0.7349 - val_loss: 0.5552 - val_acc: 0.8390\n",
      "Epoch 217/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8476 - acc: 0.7340\n",
      "Epoch 00217: val_loss improved from 0.54949 to 0.54882, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_4_conv_checkpoint/217-0.5488.hdf5\n",
      "36805/36805 [==============================] - 16s 439us/sample - loss: 0.8475 - acc: 0.7340 - val_loss: 0.5488 - val_acc: 0.8393\n",
      "Epoch 218/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8501 - acc: 0.7290\n",
      "Epoch 00218: val_loss improved from 0.54882 to 0.54616, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_4_conv_checkpoint/218-0.5462.hdf5\n",
      "36805/36805 [==============================] - 16s 439us/sample - loss: 0.8501 - acc: 0.7291 - val_loss: 0.5462 - val_acc: 0.8383\n",
      "Epoch 219/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8397 - acc: 0.7323\n",
      "Epoch 00219: val_loss did not improve from 0.54616\n",
      "36805/36805 [==============================] - 16s 438us/sample - loss: 0.8398 - acc: 0.7323 - val_loss: 0.5614 - val_acc: 0.8332\n",
      "Epoch 220/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8391 - acc: 0.7320\n",
      "Epoch 00220: val_loss did not improve from 0.54616\n",
      "36805/36805 [==============================] - 16s 436us/sample - loss: 0.8390 - acc: 0.7321 - val_loss: 0.5513 - val_acc: 0.8393\n",
      "Epoch 221/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8396 - acc: 0.7348\n",
      "Epoch 00221: val_loss did not improve from 0.54616\n",
      "36805/36805 [==============================] - 16s 437us/sample - loss: 0.8396 - acc: 0.7348 - val_loss: 0.5476 - val_acc: 0.8365\n",
      "Epoch 222/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8407 - acc: 0.7293\n",
      "Epoch 00222: val_loss improved from 0.54616 to 0.54334, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_4_conv_checkpoint/222-0.5433.hdf5\n",
      "36805/36805 [==============================] - 16s 440us/sample - loss: 0.8408 - acc: 0.7292 - val_loss: 0.5433 - val_acc: 0.8397\n",
      "Epoch 223/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8398 - acc: 0.7352\n",
      "Epoch 00223: val_loss did not improve from 0.54334\n",
      "36805/36805 [==============================] - 16s 436us/sample - loss: 0.8397 - acc: 0.7352 - val_loss: 0.5485 - val_acc: 0.8414\n",
      "Epoch 224/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8406 - acc: 0.7342\n",
      "Epoch 00224: val_loss did not improve from 0.54334\n",
      "36805/36805 [==============================] - 16s 445us/sample - loss: 0.8407 - acc: 0.7341 - val_loss: 0.5580 - val_acc: 0.8409\n",
      "Epoch 225/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8443 - acc: 0.7326\n",
      "Epoch 00225: val_loss did not improve from 0.54334\n",
      "36805/36805 [==============================] - 16s 442us/sample - loss: 0.8443 - acc: 0.7326 - val_loss: 0.5515 - val_acc: 0.8388\n",
      "Epoch 226/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8364 - acc: 0.7329\n",
      "Epoch 00226: val_loss did not improve from 0.54334\n",
      "36805/36805 [==============================] - 16s 440us/sample - loss: 0.8363 - acc: 0.7329 - val_loss: 0.5488 - val_acc: 0.8423\n",
      "Epoch 227/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8449 - acc: 0.7328\n",
      "Epoch 00227: val_loss did not improve from 0.54334\n",
      "36805/36805 [==============================] - 16s 436us/sample - loss: 0.8451 - acc: 0.7327 - val_loss: 0.5443 - val_acc: 0.8449\n",
      "Epoch 228/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8320 - acc: 0.7335\n",
      "Epoch 00228: val_loss did not improve from 0.54334\n",
      "36805/36805 [==============================] - 16s 436us/sample - loss: 0.8320 - acc: 0.7335 - val_loss: 0.5563 - val_acc: 0.8409\n",
      "Epoch 229/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8415 - acc: 0.7328\n",
      "Epoch 00229: val_loss did not improve from 0.54334\n",
      "36805/36805 [==============================] - 16s 436us/sample - loss: 0.8416 - acc: 0.7328 - val_loss: 0.5568 - val_acc: 0.8397\n",
      "Epoch 230/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8408 - acc: 0.7312\n",
      "Epoch 00230: val_loss did not improve from 0.54334\n",
      "36805/36805 [==============================] - 16s 435us/sample - loss: 0.8412 - acc: 0.7311 - val_loss: 0.5617 - val_acc: 0.8318\n",
      "Epoch 231/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8301 - acc: 0.7347\n",
      "Epoch 00231: val_loss did not improve from 0.54334\n",
      "36805/36805 [==============================] - 16s 435us/sample - loss: 0.8301 - acc: 0.7347 - val_loss: 0.5474 - val_acc: 0.8411\n",
      "Epoch 232/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8319 - acc: 0.7359\n",
      "Epoch 00232: val_loss improved from 0.54334 to 0.54009, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_4_conv_checkpoint/232-0.5401.hdf5\n",
      "36805/36805 [==============================] - 16s 437us/sample - loss: 0.8319 - acc: 0.7359 - val_loss: 0.5401 - val_acc: 0.8411\n",
      "Epoch 233/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8295 - acc: 0.7368\n",
      "Epoch 00233: val_loss did not improve from 0.54009\n",
      "36805/36805 [==============================] - 16s 435us/sample - loss: 0.8294 - acc: 0.7368 - val_loss: 0.5454 - val_acc: 0.8381\n",
      "Epoch 234/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8379 - acc: 0.7332\n",
      "Epoch 00234: val_loss did not improve from 0.54009\n",
      "36805/36805 [==============================] - 16s 435us/sample - loss: 0.8379 - acc: 0.7332 - val_loss: 0.5478 - val_acc: 0.8435\n",
      "Epoch 235/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8360 - acc: 0.7349\n",
      "Epoch 00235: val_loss did not improve from 0.54009\n",
      "36805/36805 [==============================] - 16s 435us/sample - loss: 0.8360 - acc: 0.7349 - val_loss: 0.5452 - val_acc: 0.8444\n",
      "Epoch 236/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8362 - acc: 0.7352\n",
      "Epoch 00236: val_loss did not improve from 0.54009\n",
      "36805/36805 [==============================] - 16s 437us/sample - loss: 0.8362 - acc: 0.7352 - val_loss: 0.5503 - val_acc: 0.8423\n",
      "Epoch 237/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8272 - acc: 0.7380\n",
      "Epoch 00237: val_loss did not improve from 0.54009\n",
      "36805/36805 [==============================] - 16s 436us/sample - loss: 0.8271 - acc: 0.7380 - val_loss: 0.5459 - val_acc: 0.8421\n",
      "Epoch 238/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8394 - acc: 0.7323\n",
      "Epoch 00238: val_loss did not improve from 0.54009\n",
      "36805/36805 [==============================] - 16s 435us/sample - loss: 0.8395 - acc: 0.7323 - val_loss: 0.5432 - val_acc: 0.8453\n",
      "Epoch 239/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8368 - acc: 0.7356\n",
      "Epoch 00239: val_loss did not improve from 0.54009\n",
      "36805/36805 [==============================] - 16s 438us/sample - loss: 0.8369 - acc: 0.7356 - val_loss: 0.5520 - val_acc: 0.8360\n",
      "Epoch 240/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8244 - acc: 0.7391\n",
      "Epoch 00240: val_loss did not improve from 0.54009\n",
      "36805/36805 [==============================] - 16s 436us/sample - loss: 0.8246 - acc: 0.7391 - val_loss: 0.5582 - val_acc: 0.8404\n",
      "Epoch 241/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8297 - acc: 0.7369\n",
      "Epoch 00241: val_loss improved from 0.54009 to 0.53076, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_4_conv_checkpoint/241-0.5308.hdf5\n",
      "36805/36805 [==============================] - 16s 438us/sample - loss: 0.8298 - acc: 0.7369 - val_loss: 0.5308 - val_acc: 0.8477\n",
      "Epoch 242/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8231 - acc: 0.7375\n",
      "Epoch 00242: val_loss did not improve from 0.53076\n",
      "36805/36805 [==============================] - 16s 436us/sample - loss: 0.8230 - acc: 0.7375 - val_loss: 0.5428 - val_acc: 0.8439\n",
      "Epoch 243/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8266 - acc: 0.7370\n",
      "Epoch 00243: val_loss did not improve from 0.53076\n",
      "36805/36805 [==============================] - 16s 437us/sample - loss: 0.8266 - acc: 0.7370 - val_loss: 0.5466 - val_acc: 0.8393\n",
      "Epoch 244/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8258 - acc: 0.7394\n",
      "Epoch 00244: val_loss did not improve from 0.53076\n",
      "36805/36805 [==============================] - 16s 443us/sample - loss: 0.8259 - acc: 0.7394 - val_loss: 0.5457 - val_acc: 0.8404\n",
      "Epoch 245/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8293 - acc: 0.7354\n",
      "Epoch 00245: val_loss did not improve from 0.53076\n",
      "36805/36805 [==============================] - 16s 441us/sample - loss: 0.8294 - acc: 0.7354 - val_loss: 0.5436 - val_acc: 0.8442\n",
      "Epoch 246/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8372 - acc: 0.7322\n",
      "Epoch 00246: val_loss did not improve from 0.53076\n",
      "36805/36805 [==============================] - 16s 437us/sample - loss: 0.8373 - acc: 0.7321 - val_loss: 0.5418 - val_acc: 0.8418\n",
      "Epoch 247/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8285 - acc: 0.7349\n",
      "Epoch 00247: val_loss did not improve from 0.53076\n",
      "36805/36805 [==============================] - 16s 438us/sample - loss: 0.8286 - acc: 0.7348 - val_loss: 0.5505 - val_acc: 0.8439\n",
      "Epoch 248/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8256 - acc: 0.7354\n",
      "Epoch 00248: val_loss did not improve from 0.53076\n",
      "36805/36805 [==============================] - 16s 436us/sample - loss: 0.8255 - acc: 0.7354 - val_loss: 0.5353 - val_acc: 0.8432\n",
      "Epoch 249/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8235 - acc: 0.7369\n",
      "Epoch 00249: val_loss did not improve from 0.53076\n",
      "36805/36805 [==============================] - 16s 437us/sample - loss: 0.8235 - acc: 0.7369 - val_loss: 0.5471 - val_acc: 0.8393\n",
      "Epoch 250/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8316 - acc: 0.7349\n",
      "Epoch 00250: val_loss did not improve from 0.53076\n",
      "36805/36805 [==============================] - 16s 436us/sample - loss: 0.8315 - acc: 0.7349 - val_loss: 0.5454 - val_acc: 0.8425\n",
      "Epoch 251/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8292 - acc: 0.7363\n",
      "Epoch 00251: val_loss did not improve from 0.53076\n",
      "36805/36805 [==============================] - 16s 437us/sample - loss: 0.8292 - acc: 0.7363 - val_loss: 0.5327 - val_acc: 0.8423\n",
      "Epoch 252/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8229 - acc: 0.7360\n",
      "Epoch 00252: val_loss did not improve from 0.53076\n",
      "36805/36805 [==============================] - 16s 434us/sample - loss: 0.8229 - acc: 0.7360 - val_loss: 0.5443 - val_acc: 0.8442\n",
      "Epoch 253/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8211 - acc: 0.7404\n",
      "Epoch 00253: val_loss did not improve from 0.53076\n",
      "36805/36805 [==============================] - 16s 438us/sample - loss: 0.8210 - acc: 0.7404 - val_loss: 0.5436 - val_acc: 0.8416\n",
      "Epoch 254/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8180 - acc: 0.7361\n",
      "Epoch 00254: val_loss did not improve from 0.53076\n",
      "36805/36805 [==============================] - 16s 435us/sample - loss: 0.8180 - acc: 0.7361 - val_loss: 0.5349 - val_acc: 0.8435\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 255/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8173 - acc: 0.7416\n",
      "Epoch 00255: val_loss did not improve from 0.53076\n",
      "36805/36805 [==============================] - 16s 436us/sample - loss: 0.8172 - acc: 0.7416 - val_loss: 0.5324 - val_acc: 0.8444\n",
      "Epoch 256/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8215 - acc: 0.7390\n",
      "Epoch 00256: val_loss did not improve from 0.53076\n",
      "36805/36805 [==============================] - 16s 437us/sample - loss: 0.8214 - acc: 0.7391 - val_loss: 0.5417 - val_acc: 0.8425\n",
      "Epoch 257/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8199 - acc: 0.7391\n",
      "Epoch 00257: val_loss did not improve from 0.53076\n",
      "36805/36805 [==============================] - 16s 435us/sample - loss: 0.8198 - acc: 0.7391 - val_loss: 0.5350 - val_acc: 0.8472\n",
      "Epoch 258/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8175 - acc: 0.7402\n",
      "Epoch 00258: val_loss did not improve from 0.53076\n",
      "36805/36805 [==============================] - 16s 436us/sample - loss: 0.8176 - acc: 0.7402 - val_loss: 0.5381 - val_acc: 0.8458\n",
      "Epoch 259/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8187 - acc: 0.7393\n",
      "Epoch 00259: val_loss did not improve from 0.53076\n",
      "36805/36805 [==============================] - 16s 436us/sample - loss: 0.8188 - acc: 0.7392 - val_loss: 0.5377 - val_acc: 0.8465\n",
      "Epoch 260/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8196 - acc: 0.7404\n",
      "Epoch 00260: val_loss did not improve from 0.53076\n",
      "36805/36805 [==============================] - 16s 440us/sample - loss: 0.8195 - acc: 0.7404 - val_loss: 0.5313 - val_acc: 0.8463\n",
      "Epoch 261/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8199 - acc: 0.7407\n",
      "Epoch 00261: val_loss improved from 0.53076 to 0.53031, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_4_conv_checkpoint/261-0.5303.hdf5\n",
      "36805/36805 [==============================] - 16s 436us/sample - loss: 0.8199 - acc: 0.7408 - val_loss: 0.5303 - val_acc: 0.8451\n",
      "Epoch 262/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8207 - acc: 0.7387\n",
      "Epoch 00262: val_loss improved from 0.53031 to 0.53013, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_4_conv_checkpoint/262-0.5301.hdf5\n",
      "36805/36805 [==============================] - 16s 436us/sample - loss: 0.8207 - acc: 0.7388 - val_loss: 0.5301 - val_acc: 0.8463\n",
      "Epoch 263/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8158 - acc: 0.7410\n",
      "Epoch 00263: val_loss did not improve from 0.53013\n",
      "36805/36805 [==============================] - 16s 432us/sample - loss: 0.8160 - acc: 0.7409 - val_loss: 0.5328 - val_acc: 0.8421\n",
      "Epoch 264/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8189 - acc: 0.7375\n",
      "Epoch 00264: val_loss did not improve from 0.53013\n",
      "36805/36805 [==============================] - 16s 435us/sample - loss: 0.8188 - acc: 0.7374 - val_loss: 0.5308 - val_acc: 0.8458\n",
      "Epoch 265/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8188 - acc: 0.7411\n",
      "Epoch 00265: val_loss did not improve from 0.53013\n",
      "36805/36805 [==============================] - 16s 443us/sample - loss: 0.8188 - acc: 0.7411 - val_loss: 0.5302 - val_acc: 0.8477\n",
      "Epoch 266/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8185 - acc: 0.7413\n",
      "Epoch 00266: val_loss did not improve from 0.53013\n",
      "36805/36805 [==============================] - 16s 442us/sample - loss: 0.8184 - acc: 0.7413 - val_loss: 0.5431 - val_acc: 0.8432\n",
      "Epoch 267/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8168 - acc: 0.7378\n",
      "Epoch 00267: val_loss did not improve from 0.53013\n",
      "36805/36805 [==============================] - 16s 432us/sample - loss: 0.8168 - acc: 0.7378 - val_loss: 0.5408 - val_acc: 0.8416\n",
      "Epoch 268/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8201 - acc: 0.7393\n",
      "Epoch 00268: val_loss did not improve from 0.53013\n",
      "36805/36805 [==============================] - 16s 435us/sample - loss: 0.8200 - acc: 0.7393 - val_loss: 0.5402 - val_acc: 0.8407\n",
      "Epoch 269/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8265 - acc: 0.7372\n",
      "Epoch 00269: val_loss did not improve from 0.53013\n",
      "36805/36805 [==============================] - 16s 435us/sample - loss: 0.8265 - acc: 0.7372 - val_loss: 0.5337 - val_acc: 0.8479\n",
      "Epoch 270/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8178 - acc: 0.7416\n",
      "Epoch 00270: val_loss did not improve from 0.53013\n",
      "36805/36805 [==============================] - 16s 433us/sample - loss: 0.8178 - acc: 0.7416 - val_loss: 0.5348 - val_acc: 0.8444\n",
      "Epoch 271/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8207 - acc: 0.7395\n",
      "Epoch 00271: val_loss did not improve from 0.53013\n",
      "36805/36805 [==============================] - 16s 434us/sample - loss: 0.8209 - acc: 0.7396 - val_loss: 0.5317 - val_acc: 0.8467\n",
      "Epoch 272/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8061 - acc: 0.7428\n",
      "Epoch 00272: val_loss did not improve from 0.53013\n",
      "36805/36805 [==============================] - 16s 432us/sample - loss: 0.8061 - acc: 0.7428 - val_loss: 0.5323 - val_acc: 0.8453\n",
      "Epoch 273/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8050 - acc: 0.7431\n",
      "Epoch 00273: val_loss improved from 0.53013 to 0.52221, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_4_conv_checkpoint/273-0.5222.hdf5\n",
      "36805/36805 [==============================] - 16s 435us/sample - loss: 0.8050 - acc: 0.7431 - val_loss: 0.5222 - val_acc: 0.8470\n",
      "Epoch 274/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8045 - acc: 0.7429\n",
      "Epoch 00274: val_loss did not improve from 0.52221\n",
      "36805/36805 [==============================] - 16s 435us/sample - loss: 0.8044 - acc: 0.7429 - val_loss: 0.5300 - val_acc: 0.8451\n",
      "Epoch 275/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8111 - acc: 0.7416\n",
      "Epoch 00275: val_loss did not improve from 0.52221\n",
      "36805/36805 [==============================] - 16s 436us/sample - loss: 0.8110 - acc: 0.7416 - val_loss: 0.5280 - val_acc: 0.8481\n",
      "Epoch 276/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8147 - acc: 0.7394\n",
      "Epoch 00276: val_loss did not improve from 0.52221\n",
      "36805/36805 [==============================] - 16s 436us/sample - loss: 0.8147 - acc: 0.7394 - val_loss: 0.5288 - val_acc: 0.8470\n",
      "Epoch 277/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8145 - acc: 0.7403\n",
      "Epoch 00277: val_loss did not improve from 0.52221\n",
      "36805/36805 [==============================] - 16s 434us/sample - loss: 0.8146 - acc: 0.7403 - val_loss: 0.5387 - val_acc: 0.8428\n",
      "Epoch 278/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8109 - acc: 0.7415\n",
      "Epoch 00278: val_loss did not improve from 0.52221\n",
      "36805/36805 [==============================] - 16s 435us/sample - loss: 0.8109 - acc: 0.7414 - val_loss: 0.5383 - val_acc: 0.8435\n",
      "Epoch 279/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8150 - acc: 0.7413\n",
      "Epoch 00279: val_loss did not improve from 0.52221\n",
      "36805/36805 [==============================] - 16s 435us/sample - loss: 0.8149 - acc: 0.7414 - val_loss: 0.5445 - val_acc: 0.8467\n",
      "Epoch 280/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8168 - acc: 0.7396\n",
      "Epoch 00280: val_loss did not improve from 0.52221\n",
      "36805/36805 [==============================] - 16s 435us/sample - loss: 0.8168 - acc: 0.7396 - val_loss: 0.5300 - val_acc: 0.8495\n",
      "Epoch 281/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8131 - acc: 0.7409\n",
      "Epoch 00281: val_loss did not improve from 0.52221\n",
      "36805/36805 [==============================] - 16s 435us/sample - loss: 0.8130 - acc: 0.7409 - val_loss: 0.5267 - val_acc: 0.8453\n",
      "Epoch 282/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8039 - acc: 0.7442\n",
      "Epoch 00282: val_loss did not improve from 0.52221\n",
      "36805/36805 [==============================] - 16s 435us/sample - loss: 0.8038 - acc: 0.7442 - val_loss: 0.5324 - val_acc: 0.8453\n",
      "Epoch 283/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8090 - acc: 0.7427\n",
      "Epoch 00283: val_loss did not improve from 0.52221\n",
      "36805/36805 [==============================] - 16s 435us/sample - loss: 0.8095 - acc: 0.7426 - val_loss: 0.5411 - val_acc: 0.8418\n",
      "Epoch 284/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8130 - acc: 0.7420\n",
      "Epoch 00284: val_loss did not improve from 0.52221\n",
      "36805/36805 [==============================] - 16s 436us/sample - loss: 0.8129 - acc: 0.7420 - val_loss: 0.5416 - val_acc: 0.8383\n",
      "Epoch 285/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8036 - acc: 0.7451\n",
      "Epoch 00285: val_loss improved from 0.52221 to 0.51921, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_4_conv_checkpoint/285-0.5192.hdf5\n",
      "36805/36805 [==============================] - 16s 443us/sample - loss: 0.8035 - acc: 0.7452 - val_loss: 0.5192 - val_acc: 0.8512\n",
      "Epoch 286/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8054 - acc: 0.7447\n",
      "Epoch 00286: val_loss did not improve from 0.51921\n",
      "36805/36805 [==============================] - 16s 441us/sample - loss: 0.8055 - acc: 0.7447 - val_loss: 0.5254 - val_acc: 0.8495\n",
      "Epoch 287/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8048 - acc: 0.7425\n",
      "Epoch 00287: val_loss did not improve from 0.51921\n",
      "36805/36805 [==============================] - 16s 437us/sample - loss: 0.8048 - acc: 0.7425 - val_loss: 0.5247 - val_acc: 0.8507\n",
      "Epoch 288/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8057 - acc: 0.7416\n",
      "Epoch 00288: val_loss improved from 0.51921 to 0.51863, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_4_conv_checkpoint/288-0.5186.hdf5\n",
      "36805/36805 [==============================] - 16s 435us/sample - loss: 0.8056 - acc: 0.7416 - val_loss: 0.5186 - val_acc: 0.8505\n",
      "Epoch 289/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8004 - acc: 0.7461\n",
      "Epoch 00289: val_loss did not improve from 0.51863\n",
      "36805/36805 [==============================] - 16s 433us/sample - loss: 0.8006 - acc: 0.7461 - val_loss: 0.5337 - val_acc: 0.8435\n",
      "Epoch 290/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8098 - acc: 0.7409\n",
      "Epoch 00290: val_loss did not improve from 0.51863\n",
      "36805/36805 [==============================] - 16s 435us/sample - loss: 0.8097 - acc: 0.7409 - val_loss: 0.5290 - val_acc: 0.8491\n",
      "Epoch 291/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8053 - acc: 0.7428\n",
      "Epoch 00291: val_loss did not improve from 0.51863\n",
      "36805/36805 [==============================] - 16s 435us/sample - loss: 0.8051 - acc: 0.7429 - val_loss: 0.5337 - val_acc: 0.8463\n",
      "Epoch 292/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8004 - acc: 0.7453\n",
      "Epoch 00292: val_loss did not improve from 0.51863\n",
      "36805/36805 [==============================] - 16s 437us/sample - loss: 0.8004 - acc: 0.7453 - val_loss: 0.5226 - val_acc: 0.8512\n",
      "Epoch 293/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8019 - acc: 0.7470\n",
      "Epoch 00293: val_loss did not improve from 0.51863\n",
      "36805/36805 [==============================] - 16s 434us/sample - loss: 0.8018 - acc: 0.7470 - val_loss: 0.5342 - val_acc: 0.8425\n",
      "Epoch 294/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8100 - acc: 0.7446\n",
      "Epoch 00294: val_loss did not improve from 0.51863\n",
      "36805/36805 [==============================] - 16s 434us/sample - loss: 0.8100 - acc: 0.7446 - val_loss: 0.5258 - val_acc: 0.8507\n",
      "Epoch 295/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8071 - acc: 0.7428\n",
      "Epoch 00295: val_loss did not improve from 0.51863\n",
      "36805/36805 [==============================] - 16s 435us/sample - loss: 0.8072 - acc: 0.7427 - val_loss: 0.5427 - val_acc: 0.8481\n",
      "Epoch 296/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8073 - acc: 0.7450\n",
      "Epoch 00296: val_loss did not improve from 0.51863\n",
      "36805/36805 [==============================] - 16s 434us/sample - loss: 0.8073 - acc: 0.7449 - val_loss: 0.5244 - val_acc: 0.8479\n",
      "Epoch 297/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7996 - acc: 0.7442\n",
      "Epoch 00297: val_loss did not improve from 0.51863\n",
      "36805/36805 [==============================] - 16s 432us/sample - loss: 0.7996 - acc: 0.7442 - val_loss: 0.5338 - val_acc: 0.8416\n",
      "Epoch 298/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7929 - acc: 0.7460\n",
      "Epoch 00298: val_loss did not improve from 0.51863\n",
      "36805/36805 [==============================] - 16s 435us/sample - loss: 0.7928 - acc: 0.7460 - val_loss: 0.5212 - val_acc: 0.8516\n",
      "Epoch 299/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8018 - acc: 0.7439\n",
      "Epoch 00299: val_loss did not improve from 0.51863\n",
      "36805/36805 [==============================] - 16s 434us/sample - loss: 0.8017 - acc: 0.7439 - val_loss: 0.5268 - val_acc: 0.8442\n",
      "Epoch 300/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8021 - acc: 0.7430\n",
      "Epoch 00300: val_loss did not improve from 0.51863\n",
      "36805/36805 [==============================] - 16s 435us/sample - loss: 0.8021 - acc: 0.7430 - val_loss: 0.5238 - val_acc: 0.8470\n",
      "Epoch 301/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8083 - acc: 0.7433\n",
      "Epoch 00301: val_loss did not improve from 0.51863\n",
      "36805/36805 [==============================] - 16s 433us/sample - loss: 0.8082 - acc: 0.7433 - val_loss: 0.5268 - val_acc: 0.8488\n",
      "Epoch 302/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7989 - acc: 0.7435\n",
      "Epoch 00302: val_loss did not improve from 0.51863\n",
      "36805/36805 [==============================] - 16s 435us/sample - loss: 0.7989 - acc: 0.7435 - val_loss: 0.5401 - val_acc: 0.8463\n",
      "Epoch 303/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8034 - acc: 0.7439\n",
      "Epoch 00303: val_loss did not improve from 0.51863\n",
      "36805/36805 [==============================] - 16s 436us/sample - loss: 0.8034 - acc: 0.7439 - val_loss: 0.5232 - val_acc: 0.8488\n",
      "Epoch 304/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7994 - acc: 0.7431\n",
      "Epoch 00304: val_loss did not improve from 0.51863\n",
      "36805/36805 [==============================] - 16s 435us/sample - loss: 0.7993 - acc: 0.7431 - val_loss: 0.5219 - val_acc: 0.8481\n",
      "Epoch 305/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8014 - acc: 0.7443\n",
      "Epoch 00305: val_loss did not improve from 0.51863\n",
      "36805/36805 [==============================] - 16s 444us/sample - loss: 0.8014 - acc: 0.7443 - val_loss: 0.5355 - val_acc: 0.8474\n",
      "Epoch 306/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8021 - acc: 0.7449\n",
      "Epoch 00306: val_loss did not improve from 0.51863\n",
      "36805/36805 [==============================] - 16s 441us/sample - loss: 0.8021 - acc: 0.7449 - val_loss: 0.5260 - val_acc: 0.8502\n",
      "Epoch 307/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8019 - acc: 0.7429\n",
      "Epoch 00307: val_loss did not improve from 0.51863\n",
      "36805/36805 [==============================] - 16s 438us/sample - loss: 0.8021 - acc: 0.7429 - val_loss: 0.5242 - val_acc: 0.8477\n",
      "Epoch 308/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7973 - acc: 0.7435\n",
      "Epoch 00308: val_loss did not improve from 0.51863\n",
      "36805/36805 [==============================] - 16s 444us/sample - loss: 0.7973 - acc: 0.7435 - val_loss: 0.5239 - val_acc: 0.8488\n",
      "Epoch 309/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8056 - acc: 0.7440\n",
      "Epoch 00309: val_loss did not improve from 0.51863\n",
      "36805/36805 [==============================] - 16s 444us/sample - loss: 0.8056 - acc: 0.7441 - val_loss: 0.5210 - val_acc: 0.8505\n",
      "Epoch 310/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8019 - acc: 0.7417\n",
      "Epoch 00310: val_loss improved from 0.51863 to 0.51541, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_4_conv_checkpoint/310-0.5154.hdf5\n",
      "36805/36805 [==============================] - 16s 437us/sample - loss: 0.8016 - acc: 0.7417 - val_loss: 0.5154 - val_acc: 0.8521\n",
      "Epoch 311/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8070 - acc: 0.7428\n",
      "Epoch 00311: val_loss did not improve from 0.51541\n",
      "36805/36805 [==============================] - 16s 436us/sample - loss: 0.8066 - acc: 0.7429 - val_loss: 0.5244 - val_acc: 0.8514\n",
      "Epoch 312/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7904 - acc: 0.7467\n",
      "Epoch 00312: val_loss did not improve from 0.51541\n",
      "36805/36805 [==============================] - 16s 437us/sample - loss: 0.7905 - acc: 0.7467 - val_loss: 0.5342 - val_acc: 0.8444\n",
      "Epoch 313/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7980 - acc: 0.7461\n",
      "Epoch 00313: val_loss did not improve from 0.51541\n",
      "36805/36805 [==============================] - 16s 435us/sample - loss: 0.7979 - acc: 0.7461 - val_loss: 0.5220 - val_acc: 0.8521\n",
      "Epoch 314/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7974 - acc: 0.7459\n",
      "Epoch 00314: val_loss did not improve from 0.51541\n",
      "36805/36805 [==============================] - 16s 436us/sample - loss: 0.7975 - acc: 0.7459 - val_loss: 0.5207 - val_acc: 0.8465\n",
      "Epoch 315/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7976 - acc: 0.7455\n",
      "Epoch 00315: val_loss did not improve from 0.51541\n",
      "36805/36805 [==============================] - 16s 436us/sample - loss: 0.7981 - acc: 0.7454 - val_loss: 0.5284 - val_acc: 0.8456\n",
      "Epoch 316/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8024 - acc: 0.7464\n",
      "Epoch 00316: val_loss did not improve from 0.51541\n",
      "36805/36805 [==============================] - 16s 435us/sample - loss: 0.8024 - acc: 0.7464 - val_loss: 0.5330 - val_acc: 0.8435\n",
      "Epoch 317/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7973 - acc: 0.7464\n",
      "Epoch 00317: val_loss did not improve from 0.51541\n",
      "36805/36805 [==============================] - 16s 434us/sample - loss: 0.7972 - acc: 0.7464 - val_loss: 0.5183 - val_acc: 0.8509\n",
      "Epoch 318/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7912 - acc: 0.7504\n",
      "Epoch 00318: val_loss did not improve from 0.51541\n",
      "36805/36805 [==============================] - 16s 432us/sample - loss: 0.7914 - acc: 0.7503 - val_loss: 0.5211 - val_acc: 0.8472\n",
      "Epoch 319/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7961 - acc: 0.7460\n",
      "Epoch 00319: val_loss did not improve from 0.51541\n",
      "36805/36805 [==============================] - 16s 435us/sample - loss: 0.7962 - acc: 0.7460 - val_loss: 0.5189 - val_acc: 0.8505\n",
      "Epoch 320/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7922 - acc: 0.7485\n",
      "Epoch 00320: val_loss did not improve from 0.51541\n",
      "36805/36805 [==============================] - 16s 436us/sample - loss: 0.7923 - acc: 0.7485 - val_loss: 0.5288 - val_acc: 0.8500\n",
      "Epoch 321/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7909 - acc: 0.7471\n",
      "Epoch 00321: val_loss did not improve from 0.51541\n",
      "36805/36805 [==============================] - 16s 437us/sample - loss: 0.7909 - acc: 0.7471 - val_loss: 0.5195 - val_acc: 0.8523\n",
      "Epoch 322/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7929 - acc: 0.7476\n",
      "Epoch 00322: val_loss did not improve from 0.51541\n",
      "36805/36805 [==============================] - 16s 435us/sample - loss: 0.7929 - acc: 0.7476 - val_loss: 0.5255 - val_acc: 0.8465\n",
      "Epoch 323/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7944 - acc: 0.7481\n",
      "Epoch 00323: val_loss did not improve from 0.51541\n",
      "36805/36805 [==============================] - 16s 434us/sample - loss: 0.7944 - acc: 0.7481 - val_loss: 0.5173 - val_acc: 0.8474\n",
      "Epoch 324/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7934 - acc: 0.7473\n",
      "Epoch 00324: val_loss did not improve from 0.51541\n",
      "36805/36805 [==============================] - 16s 435us/sample - loss: 0.7933 - acc: 0.7473 - val_loss: 0.5198 - val_acc: 0.8470\n",
      "Epoch 325/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7883 - acc: 0.7504\n",
      "Epoch 00325: val_loss did not improve from 0.51541\n",
      "36805/36805 [==============================] - 16s 435us/sample - loss: 0.7885 - acc: 0.7504 - val_loss: 0.5158 - val_acc: 0.8521\n",
      "Epoch 326/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7937 - acc: 0.7484\n",
      "Epoch 00326: val_loss did not improve from 0.51541\n",
      "36805/36805 [==============================] - 16s 441us/sample - loss: 0.7936 - acc: 0.7484 - val_loss: 0.5158 - val_acc: 0.8516\n",
      "Epoch 327/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7881 - acc: 0.7482\n",
      "Epoch 00327: val_loss did not improve from 0.51541\n",
      "36805/36805 [==============================] - 16s 442us/sample - loss: 0.7882 - acc: 0.7482 - val_loss: 0.5171 - val_acc: 0.8486\n",
      "Epoch 328/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7979 - acc: 0.7458\n",
      "Epoch 00328: val_loss did not improve from 0.51541\n",
      "36805/36805 [==============================] - 16s 437us/sample - loss: 0.7979 - acc: 0.7458 - val_loss: 0.5205 - val_acc: 0.8523\n",
      "Epoch 329/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7902 - acc: 0.7492\n",
      "Epoch 00329: val_loss did not improve from 0.51541\n",
      "36805/36805 [==============================] - 16s 437us/sample - loss: 0.7902 - acc: 0.7492 - val_loss: 0.5244 - val_acc: 0.8463\n",
      "Epoch 330/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7908 - acc: 0.7487\n",
      "Epoch 00330: val_loss did not improve from 0.51541\n",
      "36805/36805 [==============================] - 16s 446us/sample - loss: 0.7909 - acc: 0.7486 - val_loss: 0.5224 - val_acc: 0.8479\n",
      "Epoch 331/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7945 - acc: 0.7452\n",
      "Epoch 00331: val_loss did not improve from 0.51541\n",
      "36805/36805 [==============================] - 16s 435us/sample - loss: 0.7944 - acc: 0.7452 - val_loss: 0.5247 - val_acc: 0.8486\n",
      "Epoch 332/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7874 - acc: 0.7493\n",
      "Epoch 00332: val_loss did not improve from 0.51541\n",
      "36805/36805 [==============================] - 16s 434us/sample - loss: 0.7874 - acc: 0.7493 - val_loss: 0.5184 - val_acc: 0.8526\n",
      "Epoch 333/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7862 - acc: 0.7510\n",
      "Epoch 00333: val_loss did not improve from 0.51541\n",
      "36805/36805 [==============================] - 16s 435us/sample - loss: 0.7862 - acc: 0.7510 - val_loss: 0.5214 - val_acc: 0.8491\n",
      "Epoch 334/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7908 - acc: 0.7465\n",
      "Epoch 00334: val_loss did not improve from 0.51541\n",
      "36805/36805 [==============================] - 16s 434us/sample - loss: 0.7907 - acc: 0.7465 - val_loss: 0.5268 - val_acc: 0.8470\n",
      "Epoch 335/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7877 - acc: 0.7490\n",
      "Epoch 00335: val_loss improved from 0.51541 to 0.50932, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_4_conv_checkpoint/335-0.5093.hdf5\n",
      "36805/36805 [==============================] - 16s 438us/sample - loss: 0.7877 - acc: 0.7490 - val_loss: 0.5093 - val_acc: 0.8537\n",
      "Epoch 336/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7872 - acc: 0.7472\n",
      "Epoch 00336: val_loss did not improve from 0.50932\n",
      "36805/36805 [==============================] - 16s 436us/sample - loss: 0.7868 - acc: 0.7474 - val_loss: 0.5104 - val_acc: 0.8542\n",
      "Epoch 337/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7945 - acc: 0.7460\n",
      "Epoch 00337: val_loss did not improve from 0.50932\n",
      "36805/36805 [==============================] - 16s 439us/sample - loss: 0.7945 - acc: 0.7460 - val_loss: 0.5202 - val_acc: 0.8519\n",
      "Epoch 338/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7853 - acc: 0.7468\n",
      "Epoch 00338: val_loss did not improve from 0.50932\n",
      "36805/36805 [==============================] - 16s 434us/sample - loss: 0.7854 - acc: 0.7467 - val_loss: 0.5171 - val_acc: 0.8537\n",
      "Epoch 339/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7857 - acc: 0.7471\n",
      "Epoch 00339: val_loss did not improve from 0.50932\n",
      "36805/36805 [==============================] - 16s 436us/sample - loss: 0.7858 - acc: 0.7470 - val_loss: 0.5158 - val_acc: 0.8491\n",
      "Epoch 340/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7800 - acc: 0.7514\n",
      "Epoch 00340: val_loss did not improve from 0.50932\n",
      "36805/36805 [==============================] - 16s 438us/sample - loss: 0.7800 - acc: 0.7514 - val_loss: 0.5169 - val_acc: 0.8535\n",
      "Epoch 341/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7834 - acc: 0.7512\n",
      "Epoch 00341: val_loss did not improve from 0.50932\n",
      "36805/36805 [==============================] - 16s 436us/sample - loss: 0.7834 - acc: 0.7511 - val_loss: 0.5126 - val_acc: 0.8500\n",
      "Epoch 342/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7863 - acc: 0.7489\n",
      "Epoch 00342: val_loss did not improve from 0.50932\n",
      "36805/36805 [==============================] - 16s 435us/sample - loss: 0.7862 - acc: 0.7489 - val_loss: 0.5275 - val_acc: 0.8484\n",
      "Epoch 343/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7854 - acc: 0.7456\n",
      "Epoch 00343: val_loss did not improve from 0.50932\n",
      "36805/36805 [==============================] - 16s 434us/sample - loss: 0.7855 - acc: 0.7456 - val_loss: 0.5163 - val_acc: 0.8512\n",
      "Epoch 344/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7868 - acc: 0.7499\n",
      "Epoch 00344: val_loss did not improve from 0.50932\n",
      "36805/36805 [==============================] - 16s 436us/sample - loss: 0.7867 - acc: 0.7500 - val_loss: 0.5175 - val_acc: 0.8481\n",
      "Epoch 345/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7846 - acc: 0.7495\n",
      "Epoch 00345: val_loss did not improve from 0.50932\n",
      "36805/36805 [==============================] - 16s 434us/sample - loss: 0.7847 - acc: 0.7495 - val_loss: 0.5116 - val_acc: 0.8556\n",
      "Epoch 346/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7804 - acc: 0.7524\n",
      "Epoch 00346: val_loss did not improve from 0.50932\n",
      "36805/36805 [==============================] - 16s 435us/sample - loss: 0.7812 - acc: 0.7521 - val_loss: 0.5123 - val_acc: 0.8551\n",
      "Epoch 347/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7856 - acc: 0.7496\n",
      "Epoch 00347: val_loss did not improve from 0.50932\n",
      "36805/36805 [==============================] - 16s 440us/sample - loss: 0.7857 - acc: 0.7496 - val_loss: 0.5221 - val_acc: 0.8516\n",
      "Epoch 348/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7836 - acc: 0.7506\n",
      "Epoch 00348: val_loss did not improve from 0.50932\n",
      "36805/36805 [==============================] - 16s 445us/sample - loss: 0.7835 - acc: 0.7506 - val_loss: 0.5142 - val_acc: 0.8516\n",
      "Epoch 349/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7824 - acc: 0.7507\n",
      "Epoch 00349: val_loss did not improve from 0.50932\n",
      "36805/36805 [==============================] - 16s 440us/sample - loss: 0.7824 - acc: 0.7507 - val_loss: 0.5189 - val_acc: 0.8481\n",
      "Epoch 350/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7864 - acc: 0.7471\n",
      "Epoch 00350: val_loss did not improve from 0.50932\n",
      "36805/36805 [==============================] - 16s 435us/sample - loss: 0.7864 - acc: 0.7471 - val_loss: 0.5127 - val_acc: 0.8526\n",
      "Epoch 351/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7803 - acc: 0.7519\n",
      "Epoch 00351: val_loss did not improve from 0.50932\n",
      "36805/36805 [==============================] - 16s 442us/sample - loss: 0.7802 - acc: 0.7519 - val_loss: 0.5101 - val_acc: 0.8519\n",
      "Epoch 352/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7814 - acc: 0.7483\n",
      "Epoch 00352: val_loss did not improve from 0.50932\n",
      "36805/36805 [==============================] - 16s 436us/sample - loss: 0.7813 - acc: 0.7483 - val_loss: 0.5141 - val_acc: 0.8514\n",
      "Epoch 353/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7850 - acc: 0.7510\n",
      "Epoch 00353: val_loss did not improve from 0.50932\n",
      "36805/36805 [==============================] - 16s 435us/sample - loss: 0.7851 - acc: 0.7510 - val_loss: 0.5200 - val_acc: 0.8546\n",
      "Epoch 354/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7794 - acc: 0.7495\n",
      "Epoch 00354: val_loss did not improve from 0.50932\n",
      "36805/36805 [==============================] - 16s 436us/sample - loss: 0.7793 - acc: 0.7495 - val_loss: 0.5181 - val_acc: 0.8551\n",
      "Epoch 355/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7848 - acc: 0.7509\n",
      "Epoch 00355: val_loss did not improve from 0.50932\n",
      "36805/36805 [==============================] - 16s 437us/sample - loss: 0.7846 - acc: 0.7510 - val_loss: 0.5180 - val_acc: 0.8519\n",
      "Epoch 356/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7812 - acc: 0.7520\n",
      "Epoch 00356: val_loss did not improve from 0.50932\n",
      "36805/36805 [==============================] - 16s 437us/sample - loss: 0.7814 - acc: 0.7520 - val_loss: 0.5107 - val_acc: 0.8500\n",
      "Epoch 357/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7783 - acc: 0.7512\n",
      "Epoch 00357: val_loss did not improve from 0.50932\n",
      "36805/36805 [==============================] - 16s 439us/sample - loss: 0.7784 - acc: 0.7512 - val_loss: 0.5122 - val_acc: 0.8542\n",
      "Epoch 358/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7781 - acc: 0.7508\n",
      "Epoch 00358: val_loss did not improve from 0.50932\n",
      "36805/36805 [==============================] - 16s 437us/sample - loss: 0.7781 - acc: 0.7508 - val_loss: 0.5179 - val_acc: 0.8509\n",
      "Epoch 359/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7859 - acc: 0.7502\n",
      "Epoch 00359: val_loss improved from 0.50932 to 0.50775, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_4_conv_checkpoint/359-0.5077.hdf5\n",
      "36805/36805 [==============================] - 16s 438us/sample - loss: 0.7858 - acc: 0.7502 - val_loss: 0.5077 - val_acc: 0.8551\n",
      "Epoch 360/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7770 - acc: 0.7500\n",
      "Epoch 00360: val_loss improved from 0.50775 to 0.50645, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_4_conv_checkpoint/360-0.5064.hdf5\n",
      "36805/36805 [==============================] - 16s 434us/sample - loss: 0.7769 - acc: 0.7500 - val_loss: 0.5064 - val_acc: 0.8532\n",
      "Epoch 361/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7786 - acc: 0.7542\n",
      "Epoch 00361: val_loss did not improve from 0.50645\n",
      "36805/36805 [==============================] - 16s 434us/sample - loss: 0.7790 - acc: 0.7541 - val_loss: 0.5218 - val_acc: 0.8549\n",
      "Epoch 362/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7791 - acc: 0.7516\n",
      "Epoch 00362: val_loss did not improve from 0.50645\n",
      "36805/36805 [==============================] - 16s 434us/sample - loss: 0.7791 - acc: 0.7516 - val_loss: 0.5077 - val_acc: 0.8519\n",
      "Epoch 363/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7755 - acc: 0.7537\n",
      "Epoch 00363: val_loss improved from 0.50645 to 0.50443, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_4_conv_checkpoint/363-0.5044.hdf5\n",
      "36805/36805 [==============================] - 16s 438us/sample - loss: 0.7755 - acc: 0.7536 - val_loss: 0.5044 - val_acc: 0.8574\n",
      "Epoch 364/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7875 - acc: 0.7486\n",
      "Epoch 00364: val_loss did not improve from 0.50443\n",
      "36805/36805 [==============================] - 16s 435us/sample - loss: 0.7875 - acc: 0.7486 - val_loss: 0.5077 - val_acc: 0.8551\n",
      "Epoch 365/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7871 - acc: 0.7484\n",
      "Epoch 00365: val_loss did not improve from 0.50443\n",
      "36805/36805 [==============================] - 16s 436us/sample - loss: 0.7872 - acc: 0.7483 - val_loss: 0.5190 - val_acc: 0.8500\n",
      "Epoch 366/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7793 - acc: 0.7510\n",
      "Epoch 00366: val_loss did not improve from 0.50443\n",
      "36805/36805 [==============================] - 16s 433us/sample - loss: 0.7797 - acc: 0.7507 - val_loss: 0.5132 - val_acc: 0.8514\n",
      "Epoch 367/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7791 - acc: 0.7507\n",
      "Epoch 00367: val_loss did not improve from 0.50443\n",
      "36805/36805 [==============================] - 16s 435us/sample - loss: 0.7790 - acc: 0.7507 - val_loss: 0.5110 - val_acc: 0.8523\n",
      "Epoch 368/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7743 - acc: 0.7541\n",
      "Epoch 00368: val_loss did not improve from 0.50443\n",
      "36805/36805 [==============================] - 16s 443us/sample - loss: 0.7743 - acc: 0.7541 - val_loss: 0.5128 - val_acc: 0.8505\n",
      "Epoch 369/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7789 - acc: 0.7504\n",
      "Epoch 00369: val_loss did not improve from 0.50443\n",
      "36805/36805 [==============================] - 16s 443us/sample - loss: 0.7789 - acc: 0.7504 - val_loss: 0.5060 - val_acc: 0.8535\n",
      "Epoch 370/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7768 - acc: 0.7511\n",
      "Epoch 00370: val_loss did not improve from 0.50443\n",
      "36805/36805 [==============================] - 16s 437us/sample - loss: 0.7765 - acc: 0.7512 - val_loss: 0.5064 - val_acc: 0.8558\n",
      "Epoch 371/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7729 - acc: 0.7550\n",
      "Epoch 00371: val_loss did not improve from 0.50443\n",
      "36805/36805 [==============================] - 16s 434us/sample - loss: 0.7728 - acc: 0.7550 - val_loss: 0.5182 - val_acc: 0.8509\n",
      "Epoch 372/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7747 - acc: 0.7522\n",
      "Epoch 00372: val_loss did not improve from 0.50443\n",
      "36805/36805 [==============================] - 16s 435us/sample - loss: 0.7746 - acc: 0.7522 - val_loss: 0.5143 - val_acc: 0.8530\n",
      "Epoch 373/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7768 - acc: 0.7531\n",
      "Epoch 00373: val_loss did not improve from 0.50443\n",
      "36805/36805 [==============================] - 16s 434us/sample - loss: 0.7768 - acc: 0.7531 - val_loss: 0.5094 - val_acc: 0.8495\n",
      "Epoch 374/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7708 - acc: 0.7549\n",
      "Epoch 00374: val_loss did not improve from 0.50443\n",
      "36805/36805 [==============================] - 16s 436us/sample - loss: 0.7709 - acc: 0.7549 - val_loss: 0.5087 - val_acc: 0.8565\n",
      "Epoch 375/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7758 - acc: 0.7508\n",
      "Epoch 00375: val_loss improved from 0.50443 to 0.50048, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_4_conv_checkpoint/375-0.5005.hdf5\n",
      "36805/36805 [==============================] - 16s 438us/sample - loss: 0.7758 - acc: 0.7509 - val_loss: 0.5005 - val_acc: 0.8565\n",
      "Epoch 376/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7767 - acc: 0.7529\n",
      "Epoch 00376: val_loss did not improve from 0.50048\n",
      "36805/36805 [==============================] - 16s 435us/sample - loss: 0.7766 - acc: 0.7530 - val_loss: 0.5148 - val_acc: 0.8556\n",
      "Epoch 377/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7759 - acc: 0.7529\n",
      "Epoch 00377: val_loss did not improve from 0.50048\n",
      "36805/36805 [==============================] - 16s 437us/sample - loss: 0.7759 - acc: 0.7529 - val_loss: 0.5163 - val_acc: 0.8528\n",
      "Epoch 378/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7789 - acc: 0.7498\n",
      "Epoch 00378: val_loss did not improve from 0.50048\n",
      "36805/36805 [==============================] - 16s 437us/sample - loss: 0.7788 - acc: 0.7498 - val_loss: 0.5111 - val_acc: 0.8532\n",
      "Epoch 379/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7778 - acc: 0.7516\n",
      "Epoch 00379: val_loss did not improve from 0.50048\n",
      "36805/36805 [==============================] - 16s 437us/sample - loss: 0.7779 - acc: 0.7515 - val_loss: 0.5075 - val_acc: 0.8512\n",
      "Epoch 380/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7732 - acc: 0.7533\n",
      "Epoch 00380: val_loss did not improve from 0.50048\n",
      "36805/36805 [==============================] - 16s 435us/sample - loss: 0.7731 - acc: 0.7533 - val_loss: 0.5030 - val_acc: 0.8544\n",
      "Epoch 381/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7733 - acc: 0.7514\n",
      "Epoch 00381: val_loss did not improve from 0.50048\n",
      "36805/36805 [==============================] - 16s 436us/sample - loss: 0.7734 - acc: 0.7513 - val_loss: 0.5072 - val_acc: 0.8537\n",
      "Epoch 382/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7744 - acc: 0.7526\n",
      "Epoch 00382: val_loss did not improve from 0.50048\n",
      "36805/36805 [==============================] - 16s 434us/sample - loss: 0.7743 - acc: 0.7527 - val_loss: 0.5098 - val_acc: 0.8491\n",
      "Epoch 383/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7669 - acc: 0.7551\n",
      "Epoch 00383: val_loss did not improve from 0.50048\n",
      "36805/36805 [==============================] - 16s 436us/sample - loss: 0.7668 - acc: 0.7551 - val_loss: 0.5030 - val_acc: 0.8570\n",
      "Epoch 384/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7720 - acc: 0.7526\n",
      "Epoch 00384: val_loss did not improve from 0.50048\n",
      "36805/36805 [==============================] - 16s 433us/sample - loss: 0.7719 - acc: 0.7526 - val_loss: 0.5172 - val_acc: 0.8535\n",
      "Epoch 385/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7683 - acc: 0.7540\n",
      "Epoch 00385: val_loss did not improve from 0.50048\n",
      "36805/36805 [==============================] - 16s 434us/sample - loss: 0.7683 - acc: 0.7540 - val_loss: 0.5085 - val_acc: 0.8544\n",
      "Epoch 386/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7777 - acc: 0.7522\n",
      "Epoch 00386: val_loss did not improve from 0.50048\n",
      "36805/36805 [==============================] - 16s 435us/sample - loss: 0.7776 - acc: 0.7522 - val_loss: 0.5225 - val_acc: 0.8523\n",
      "Epoch 387/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7662 - acc: 0.7544\n",
      "Epoch 00387: val_loss improved from 0.50048 to 0.50002, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_4_conv_checkpoint/387-0.5000.hdf5\n",
      "36805/36805 [==============================] - 16s 431us/sample - loss: 0.7661 - acc: 0.7544 - val_loss: 0.5000 - val_acc: 0.8574\n",
      "Epoch 388/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7739 - acc: 0.7533\n",
      "Epoch 00388: val_loss did not improve from 0.50002\n",
      "36805/36805 [==============================] - 16s 446us/sample - loss: 0.7739 - acc: 0.7533 - val_loss: 0.5010 - val_acc: 0.8574\n",
      "Epoch 389/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7654 - acc: 0.7525\n",
      "Epoch 00389: val_loss did not improve from 0.50002\n",
      "36805/36805 [==============================] - 16s 445us/sample - loss: 0.7656 - acc: 0.7524 - val_loss: 0.5180 - val_acc: 0.8514\n",
      "Epoch 390/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7753 - acc: 0.7541\n",
      "Epoch 00390: val_loss did not improve from 0.50002\n",
      "36805/36805 [==============================] - 16s 437us/sample - loss: 0.7754 - acc: 0.7541 - val_loss: 0.5112 - val_acc: 0.8532\n",
      "Epoch 391/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7704 - acc: 0.7564\n",
      "Epoch 00391: val_loss did not improve from 0.50002\n",
      "36805/36805 [==============================] - 16s 433us/sample - loss: 0.7704 - acc: 0.7563 - val_loss: 0.5006 - val_acc: 0.8567\n",
      "Epoch 392/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7726 - acc: 0.7519\n",
      "Epoch 00392: val_loss did not improve from 0.50002\n",
      "36805/36805 [==============================] - 16s 434us/sample - loss: 0.7727 - acc: 0.7519 - val_loss: 0.5132 - val_acc: 0.8523\n",
      "Epoch 393/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7730 - acc: 0.7515\n",
      "Epoch 00393: val_loss did not improve from 0.50002\n",
      "36805/36805 [==============================] - 16s 435us/sample - loss: 0.7731 - acc: 0.7514 - val_loss: 0.5050 - val_acc: 0.8542\n",
      "Epoch 394/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7688 - acc: 0.7539\n",
      "Epoch 00394: val_loss did not improve from 0.50002\n",
      "36805/36805 [==============================] - 16s 433us/sample - loss: 0.7687 - acc: 0.7539 - val_loss: 0.5077 - val_acc: 0.8530\n",
      "Epoch 395/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7703 - acc: 0.7533\n",
      "Epoch 00395: val_loss did not improve from 0.50002\n",
      "36805/36805 [==============================] - 16s 434us/sample - loss: 0.7702 - acc: 0.7533 - val_loss: 0.5117 - val_acc: 0.8516\n",
      "Epoch 396/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7620 - acc: 0.7558\n",
      "Epoch 00396: val_loss did not improve from 0.50002\n",
      "36805/36805 [==============================] - 16s 436us/sample - loss: 0.7620 - acc: 0.7557 - val_loss: 0.5026 - val_acc: 0.8519\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 397/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7699 - acc: 0.7543\n",
      "Epoch 00397: val_loss did not improve from 0.50002\n",
      "36805/36805 [==============================] - 16s 435us/sample - loss: 0.7698 - acc: 0.7543 - val_loss: 0.5082 - val_acc: 0.8535\n",
      "Epoch 398/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7690 - acc: 0.7549\n",
      "Epoch 00398: val_loss did not improve from 0.50002\n",
      "36805/36805 [==============================] - 16s 433us/sample - loss: 0.7689 - acc: 0.7549 - val_loss: 0.5017 - val_acc: 0.8588\n",
      "Epoch 399/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7666 - acc: 0.7561\n",
      "Epoch 00399: val_loss did not improve from 0.50002\n",
      "36805/36805 [==============================] - 16s 435us/sample - loss: 0.7665 - acc: 0.7561 - val_loss: 0.5149 - val_acc: 0.8521\n",
      "Epoch 400/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7675 - acc: 0.7533\n",
      "Epoch 00400: val_loss did not improve from 0.50002\n",
      "36805/36805 [==============================] - 16s 434us/sample - loss: 0.7676 - acc: 0.7533 - val_loss: 0.5036 - val_acc: 0.8570\n",
      "Epoch 401/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7686 - acc: 0.7549\n",
      "Epoch 00401: val_loss improved from 0.50002 to 0.49940, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_4_conv_checkpoint/401-0.4994.hdf5\n",
      "36805/36805 [==============================] - 16s 434us/sample - loss: 0.7688 - acc: 0.7549 - val_loss: 0.4994 - val_acc: 0.8546\n",
      "Epoch 402/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7723 - acc: 0.7530\n",
      "Epoch 00402: val_loss did not improve from 0.49940\n",
      "36805/36805 [==============================] - 16s 432us/sample - loss: 0.7720 - acc: 0.7532 - val_loss: 0.5073 - val_acc: 0.8519\n",
      "Epoch 403/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7700 - acc: 0.7550\n",
      "Epoch 00403: val_loss improved from 0.49940 to 0.49835, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_4_conv_checkpoint/403-0.4983.hdf5\n",
      "36805/36805 [==============================] - 16s 437us/sample - loss: 0.7700 - acc: 0.7550 - val_loss: 0.4983 - val_acc: 0.8565\n",
      "Epoch 404/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7643 - acc: 0.7567\n",
      "Epoch 00404: val_loss did not improve from 0.49835\n",
      "36805/36805 [==============================] - 16s 435us/sample - loss: 0.7644 - acc: 0.7567 - val_loss: 0.5043 - val_acc: 0.8558\n",
      "Epoch 405/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7585 - acc: 0.7556\n",
      "Epoch 00405: val_loss improved from 0.49835 to 0.49401, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_4_conv_checkpoint/405-0.4940.hdf5\n",
      "36805/36805 [==============================] - 16s 436us/sample - loss: 0.7587 - acc: 0.7556 - val_loss: 0.4940 - val_acc: 0.8579\n",
      "Epoch 406/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7764 - acc: 0.7508\n",
      "Epoch 00406: val_loss did not improve from 0.49401\n",
      "36805/36805 [==============================] - 16s 438us/sample - loss: 0.7764 - acc: 0.7508 - val_loss: 0.5037 - val_acc: 0.8539\n",
      "Epoch 407/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7651 - acc: 0.7547\n",
      "Epoch 00407: val_loss did not improve from 0.49401\n",
      "36805/36805 [==============================] - 16s 436us/sample - loss: 0.7650 - acc: 0.7548 - val_loss: 0.5023 - val_acc: 0.8549\n",
      "Epoch 408/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7622 - acc: 0.7565\n",
      "Epoch 00408: val_loss did not improve from 0.49401\n",
      "36805/36805 [==============================] - 16s 443us/sample - loss: 0.7622 - acc: 0.7565 - val_loss: 0.5082 - val_acc: 0.8530\n",
      "Epoch 409/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7654 - acc: 0.7558\n",
      "Epoch 00409: val_loss did not improve from 0.49401\n",
      "36805/36805 [==============================] - 16s 443us/sample - loss: 0.7655 - acc: 0.7558 - val_loss: 0.5059 - val_acc: 0.8530\n",
      "Epoch 410/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7664 - acc: 0.7542\n",
      "Epoch 00410: val_loss did not improve from 0.49401\n",
      "36805/36805 [==============================] - 16s 436us/sample - loss: 0.7664 - acc: 0.7542 - val_loss: 0.4961 - val_acc: 0.8553\n",
      "Epoch 411/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7632 - acc: 0.7560\n",
      "Epoch 00411: val_loss did not improve from 0.49401\n",
      "36805/36805 [==============================] - 16s 436us/sample - loss: 0.7631 - acc: 0.7561 - val_loss: 0.5034 - val_acc: 0.8591\n",
      "Epoch 412/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7719 - acc: 0.7533\n",
      "Epoch 00412: val_loss did not improve from 0.49401\n",
      "36805/36805 [==============================] - 16s 435us/sample - loss: 0.7720 - acc: 0.7533 - val_loss: 0.5044 - val_acc: 0.8542\n",
      "Epoch 413/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7673 - acc: 0.7558\n",
      "Epoch 00413: val_loss did not improve from 0.49401\n",
      "36805/36805 [==============================] - 16s 435us/sample - loss: 0.7674 - acc: 0.7556 - val_loss: 0.5090 - val_acc: 0.8558\n",
      "Epoch 414/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7619 - acc: 0.7583\n",
      "Epoch 00414: val_loss improved from 0.49401 to 0.49055, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_4_conv_checkpoint/414-0.4906.hdf5\n",
      "36805/36805 [==============================] - 16s 433us/sample - loss: 0.7615 - acc: 0.7584 - val_loss: 0.4906 - val_acc: 0.8593\n",
      "Epoch 415/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7647 - acc: 0.7543\n",
      "Epoch 00415: val_loss did not improve from 0.49055\n",
      "36805/36805 [==============================] - 16s 434us/sample - loss: 0.7647 - acc: 0.7544 - val_loss: 0.5049 - val_acc: 0.8560\n",
      "Epoch 416/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7635 - acc: 0.7531\n",
      "Epoch 00416: val_loss did not improve from 0.49055\n",
      "36805/36805 [==============================] - 16s 436us/sample - loss: 0.7636 - acc: 0.7531 - val_loss: 0.5083 - val_acc: 0.8523\n",
      "Epoch 417/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7653 - acc: 0.7555\n",
      "Epoch 00417: val_loss did not improve from 0.49055\n",
      "36805/36805 [==============================] - 16s 435us/sample - loss: 0.7652 - acc: 0.7555 - val_loss: 0.4997 - val_acc: 0.8563\n",
      "Epoch 418/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7651 - acc: 0.7552\n",
      "Epoch 00418: val_loss did not improve from 0.49055\n",
      "36805/36805 [==============================] - 16s 435us/sample - loss: 0.7652 - acc: 0.7551 - val_loss: 0.4968 - val_acc: 0.8558\n",
      "Epoch 419/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7636 - acc: 0.7545\n",
      "Epoch 00419: val_loss did not improve from 0.49055\n",
      "36805/36805 [==============================] - 16s 434us/sample - loss: 0.7637 - acc: 0.7545 - val_loss: 0.4968 - val_acc: 0.8600\n",
      "Epoch 420/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7599 - acc: 0.7569\n",
      "Epoch 00420: val_loss did not improve from 0.49055\n",
      "36805/36805 [==============================] - 16s 434us/sample - loss: 0.7598 - acc: 0.7569 - val_loss: 0.5000 - val_acc: 0.8532\n",
      "Epoch 421/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7578 - acc: 0.7574\n",
      "Epoch 00421: val_loss did not improve from 0.49055\n",
      "36805/36805 [==============================] - 16s 434us/sample - loss: 0.7589 - acc: 0.7573 - val_loss: 0.4999 - val_acc: 0.8549\n",
      "Epoch 422/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7670 - acc: 0.7534\n",
      "Epoch 00422: val_loss did not improve from 0.49055\n",
      "36805/36805 [==============================] - 16s 435us/sample - loss: 0.7671 - acc: 0.7533 - val_loss: 0.5014 - val_acc: 0.8570\n",
      "Epoch 423/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7601 - acc: 0.7570\n",
      "Epoch 00423: val_loss did not improve from 0.49055\n",
      "36805/36805 [==============================] - 16s 436us/sample - loss: 0.7603 - acc: 0.7569 - val_loss: 0.4955 - val_acc: 0.8549\n",
      "Epoch 424/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7581 - acc: 0.7547\n",
      "Epoch 00424: val_loss did not improve from 0.49055\n",
      "36805/36805 [==============================] - 16s 433us/sample - loss: 0.7581 - acc: 0.7547 - val_loss: 0.5022 - val_acc: 0.8570\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 425/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7555 - acc: 0.7598\n",
      "Epoch 00425: val_loss did not improve from 0.49055\n",
      "36805/36805 [==============================] - 16s 437us/sample - loss: 0.7554 - acc: 0.7598 - val_loss: 0.4980 - val_acc: 0.8579\n",
      "Epoch 426/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7604 - acc: 0.7558\n",
      "Epoch 00426: val_loss did not improve from 0.49055\n",
      "36805/36805 [==============================] - 16s 436us/sample - loss: 0.7605 - acc: 0.7558 - val_loss: 0.5009 - val_acc: 0.8574\n",
      "Epoch 427/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7644 - acc: 0.7557\n",
      "Epoch 00427: val_loss did not improve from 0.49055\n",
      "36805/36805 [==============================] - 16s 433us/sample - loss: 0.7644 - acc: 0.7557 - val_loss: 0.5031 - val_acc: 0.8537\n",
      "Epoch 428/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7583 - acc: 0.7572\n",
      "Epoch 00428: val_loss did not improve from 0.49055\n",
      "36805/36805 [==============================] - 16s 441us/sample - loss: 0.7584 - acc: 0.7572 - val_loss: 0.4946 - val_acc: 0.8563\n",
      "Epoch 429/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7646 - acc: 0.7558\n",
      "Epoch 00429: val_loss did not improve from 0.49055\n",
      "36805/36805 [==============================] - 16s 440us/sample - loss: 0.7647 - acc: 0.7557 - val_loss: 0.5105 - val_acc: 0.8528\n",
      "Epoch 430/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7562 - acc: 0.7595\n",
      "Epoch 00430: val_loss improved from 0.49055 to 0.48996, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_4_conv_checkpoint/430-0.4900.hdf5\n",
      "36805/36805 [==============================] - 16s 436us/sample - loss: 0.7561 - acc: 0.7595 - val_loss: 0.4900 - val_acc: 0.8598\n",
      "Epoch 431/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7654 - acc: 0.7519\n",
      "Epoch 00431: val_loss did not improve from 0.48996\n",
      "36805/36805 [==============================] - 16s 434us/sample - loss: 0.7653 - acc: 0.7520 - val_loss: 0.5003 - val_acc: 0.8614\n",
      "Epoch 432/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7551 - acc: 0.7609\n",
      "Epoch 00432: val_loss did not improve from 0.48996\n",
      "36805/36805 [==============================] - 16s 436us/sample - loss: 0.7551 - acc: 0.7609 - val_loss: 0.4907 - val_acc: 0.8588\n",
      "Epoch 433/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7555 - acc: 0.7589\n",
      "Epoch 00433: val_loss did not improve from 0.48996\n",
      "36805/36805 [==============================] - 16s 435us/sample - loss: 0.7552 - acc: 0.7590 - val_loss: 0.4999 - val_acc: 0.8549\n",
      "Epoch 434/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7664 - acc: 0.7536\n",
      "Epoch 00434: val_loss did not improve from 0.48996\n",
      "36805/36805 [==============================] - 16s 434us/sample - loss: 0.7664 - acc: 0.7536 - val_loss: 0.4928 - val_acc: 0.8591\n",
      "Epoch 435/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7618 - acc: 0.7548\n",
      "Epoch 00435: val_loss did not improve from 0.48996\n",
      "36805/36805 [==============================] - 16s 434us/sample - loss: 0.7619 - acc: 0.7547 - val_loss: 0.4957 - val_acc: 0.8563\n",
      "Epoch 436/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7640 - acc: 0.7533\n",
      "Epoch 00436: val_loss did not improve from 0.48996\n",
      "36805/36805 [==============================] - 16s 432us/sample - loss: 0.7640 - acc: 0.7533 - val_loss: 0.5060 - val_acc: 0.8577\n",
      "Epoch 437/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7558 - acc: 0.7602\n",
      "Epoch 00437: val_loss did not improve from 0.48996\n",
      "36805/36805 [==============================] - 16s 433us/sample - loss: 0.7558 - acc: 0.7602 - val_loss: 0.4922 - val_acc: 0.8567\n",
      "Epoch 438/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7626 - acc: 0.7555\n",
      "Epoch 00438: val_loss did not improve from 0.48996\n",
      "36805/36805 [==============================] - 16s 433us/sample - loss: 0.7625 - acc: 0.7555 - val_loss: 0.5087 - val_acc: 0.8500\n",
      "Epoch 439/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7608 - acc: 0.7541\n",
      "Epoch 00439: val_loss did not improve from 0.48996\n",
      "36805/36805 [==============================] - 16s 434us/sample - loss: 0.7607 - acc: 0.7541 - val_loss: 0.5032 - val_acc: 0.8565\n",
      "Epoch 440/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7551 - acc: 0.7576\n",
      "Epoch 00440: val_loss did not improve from 0.48996\n",
      "36805/36805 [==============================] - 16s 437us/sample - loss: 0.7547 - acc: 0.7577 - val_loss: 0.4962 - val_acc: 0.8544\n",
      "Epoch 441/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7567 - acc: 0.7560\n",
      "Epoch 00441: val_loss did not improve from 0.48996\n",
      "36805/36805 [==============================] - 16s 433us/sample - loss: 0.7567 - acc: 0.7560 - val_loss: 0.4962 - val_acc: 0.8586\n",
      "Epoch 442/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7598 - acc: 0.7557\n",
      "Epoch 00442: val_loss did not improve from 0.48996\n",
      "36805/36805 [==============================] - 16s 433us/sample - loss: 0.7600 - acc: 0.7557 - val_loss: 0.4974 - val_acc: 0.8579\n",
      "Epoch 443/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7590 - acc: 0.7591\n",
      "Epoch 00443: val_loss did not improve from 0.48996\n",
      "36805/36805 [==============================] - 16s 435us/sample - loss: 0.7590 - acc: 0.7591 - val_loss: 0.4966 - val_acc: 0.8567\n",
      "Epoch 444/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7531 - acc: 0.7581\n",
      "Epoch 00444: val_loss improved from 0.48996 to 0.48933, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_4_conv_checkpoint/444-0.4893.hdf5\n",
      "36805/36805 [==============================] - 16s 439us/sample - loss: 0.7531 - acc: 0.7581 - val_loss: 0.4893 - val_acc: 0.8567\n",
      "Epoch 445/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7605 - acc: 0.7575\n",
      "Epoch 00445: val_loss did not improve from 0.48933\n",
      "36805/36805 [==============================] - 16s 438us/sample - loss: 0.7606 - acc: 0.7575 - val_loss: 0.4926 - val_acc: 0.8588\n",
      "Epoch 446/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7602 - acc: 0.7581\n",
      "Epoch 00446: val_loss did not improve from 0.48933\n",
      "36805/36805 [==============================] - 16s 436us/sample - loss: 0.7601 - acc: 0.7581 - val_loss: 0.5055 - val_acc: 0.8528\n",
      "Epoch 447/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7567 - acc: 0.7589\n",
      "Epoch 00447: val_loss did not improve from 0.48933\n",
      "36805/36805 [==============================] - 16s 443us/sample - loss: 0.7568 - acc: 0.7589 - val_loss: 0.4909 - val_acc: 0.8558\n",
      "Epoch 448/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7601 - acc: 0.7542\n",
      "Epoch 00448: val_loss improved from 0.48933 to 0.48775, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_4_conv_checkpoint/448-0.4878.hdf5\n",
      "36805/36805 [==============================] - 16s 446us/sample - loss: 0.7601 - acc: 0.7542 - val_loss: 0.4878 - val_acc: 0.8602\n",
      "Epoch 449/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7555 - acc: 0.7587\n",
      "Epoch 00449: val_loss did not improve from 0.48775\n",
      "36805/36805 [==============================] - 16s 445us/sample - loss: 0.7555 - acc: 0.7587 - val_loss: 0.4898 - val_acc: 0.8572\n",
      "Epoch 450/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7518 - acc: 0.7620\n",
      "Epoch 00450: val_loss did not improve from 0.48775\n",
      "36805/36805 [==============================] - 16s 436us/sample - loss: 0.7521 - acc: 0.7619 - val_loss: 0.5000 - val_acc: 0.8514\n",
      "Epoch 451/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7545 - acc: 0.7578\n",
      "Epoch 00451: val_loss did not improve from 0.48775\n",
      "36805/36805 [==============================] - 16s 441us/sample - loss: 0.7546 - acc: 0.7577 - val_loss: 0.4952 - val_acc: 0.8567\n",
      "Epoch 452/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7576 - acc: 0.7569\n",
      "Epoch 00452: val_loss did not improve from 0.48775\n",
      "36805/36805 [==============================] - 16s 441us/sample - loss: 0.7575 - acc: 0.7570 - val_loss: 0.4946 - val_acc: 0.8537\n",
      "Epoch 453/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7553 - acc: 0.7602\n",
      "Epoch 00453: val_loss did not improve from 0.48775\n",
      "36805/36805 [==============================] - 16s 439us/sample - loss: 0.7553 - acc: 0.7602 - val_loss: 0.4967 - val_acc: 0.8581\n",
      "Epoch 454/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7513 - acc: 0.7584\n",
      "Epoch 00454: val_loss did not improve from 0.48775\n",
      "36805/36805 [==============================] - 16s 442us/sample - loss: 0.7514 - acc: 0.7583 - val_loss: 0.5077 - val_acc: 0.8542\n",
      "Epoch 455/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7501 - acc: 0.7589\n",
      "Epoch 00455: val_loss did not improve from 0.48775\n",
      "36805/36805 [==============================] - 16s 442us/sample - loss: 0.7502 - acc: 0.7588 - val_loss: 0.4890 - val_acc: 0.8577\n",
      "Epoch 456/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7446 - acc: 0.7611\n",
      "Epoch 00456: val_loss improved from 0.48775 to 0.48751, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_4_conv_checkpoint/456-0.4875.hdf5\n",
      "36805/36805 [==============================] - 16s 446us/sample - loss: 0.7445 - acc: 0.7611 - val_loss: 0.4875 - val_acc: 0.8588\n",
      "Epoch 457/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7498 - acc: 0.7591\n",
      "Epoch 00457: val_loss did not improve from 0.48751\n",
      "36805/36805 [==============================] - 16s 440us/sample - loss: 0.7498 - acc: 0.7591 - val_loss: 0.4900 - val_acc: 0.8607\n",
      "Epoch 458/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7532 - acc: 0.7592\n",
      "Epoch 00458: val_loss did not improve from 0.48751\n",
      "36805/36805 [==============================] - 16s 440us/sample - loss: 0.7532 - acc: 0.7592 - val_loss: 0.4945 - val_acc: 0.8560\n",
      "Epoch 459/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7481 - acc: 0.7611\n",
      "Epoch 00459: val_loss did not improve from 0.48751\n",
      "36805/36805 [==============================] - 16s 444us/sample - loss: 0.7481 - acc: 0.7610 - val_loss: 0.5030 - val_acc: 0.8558\n",
      "Epoch 460/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7505 - acc: 0.7618\n",
      "Epoch 00460: val_loss did not improve from 0.48751\n",
      "36805/36805 [==============================] - 16s 439us/sample - loss: 0.7504 - acc: 0.7618 - val_loss: 0.4988 - val_acc: 0.8546\n",
      "Epoch 461/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7534 - acc: 0.7591\n",
      "Epoch 00461: val_loss did not improve from 0.48751\n",
      "36805/36805 [==============================] - 16s 439us/sample - loss: 0.7533 - acc: 0.7591 - val_loss: 0.4912 - val_acc: 0.8560\n",
      "Epoch 462/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7563 - acc: 0.7590\n",
      "Epoch 00462: val_loss did not improve from 0.48751\n",
      "36805/36805 [==============================] - 16s 439us/sample - loss: 0.7563 - acc: 0.7590 - val_loss: 0.4965 - val_acc: 0.8563\n",
      "Epoch 463/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7583 - acc: 0.7541\n",
      "Epoch 00463: val_loss did not improve from 0.48751\n",
      "36805/36805 [==============================] - 16s 438us/sample - loss: 0.7579 - acc: 0.7542 - val_loss: 0.4935 - val_acc: 0.8560\n",
      "Epoch 464/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7517 - acc: 0.7596\n",
      "Epoch 00464: val_loss did not improve from 0.48751\n",
      "36805/36805 [==============================] - 16s 428us/sample - loss: 0.7518 - acc: 0.7596 - val_loss: 0.4935 - val_acc: 0.8565\n",
      "Epoch 465/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7533 - acc: 0.7595\n",
      "Epoch 00465: val_loss improved from 0.48751 to 0.48708, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_4_conv_checkpoint/465-0.4871.hdf5\n",
      "36805/36805 [==============================] - 16s 441us/sample - loss: 0.7532 - acc: 0.7595 - val_loss: 0.4871 - val_acc: 0.8591\n",
      "Epoch 466/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7511 - acc: 0.7603\n",
      "Epoch 00466: val_loss did not improve from 0.48708\n",
      "36805/36805 [==============================] - 16s 440us/sample - loss: 0.7510 - acc: 0.7603 - val_loss: 0.4938 - val_acc: 0.8570\n",
      "Epoch 467/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7506 - acc: 0.7580\n",
      "Epoch 00467: val_loss did not improve from 0.48708\n",
      "36805/36805 [==============================] - 16s 439us/sample - loss: 0.7505 - acc: 0.7580 - val_loss: 0.5000 - val_acc: 0.8530\n",
      "Epoch 468/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7535 - acc: 0.7567\n",
      "Epoch 00468: val_loss did not improve from 0.48708\n",
      "36805/36805 [==============================] - 16s 431us/sample - loss: 0.7535 - acc: 0.7567 - val_loss: 0.4947 - val_acc: 0.8544\n",
      "Epoch 469/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7525 - acc: 0.7579\n",
      "Epoch 00469: val_loss improved from 0.48708 to 0.48619, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_4_conv_checkpoint/469-0.4862.hdf5\n",
      "36805/36805 [==============================] - 16s 439us/sample - loss: 0.7524 - acc: 0.7579 - val_loss: 0.4862 - val_acc: 0.8584\n",
      "Epoch 470/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7505 - acc: 0.7600\n",
      "Epoch 00470: val_loss did not improve from 0.48619\n",
      "36805/36805 [==============================] - 16s 435us/sample - loss: 0.7505 - acc: 0.7600 - val_loss: 0.4904 - val_acc: 0.8595\n",
      "Epoch 471/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7497 - acc: 0.7597\n",
      "Epoch 00471: val_loss improved from 0.48619 to 0.48420, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_4_conv_checkpoint/471-0.4842.hdf5\n",
      "36805/36805 [==============================] - 16s 434us/sample - loss: 0.7497 - acc: 0.7597 - val_loss: 0.4842 - val_acc: 0.8609\n",
      "Epoch 472/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7455 - acc: 0.7603\n",
      "Epoch 00472: val_loss did not improve from 0.48420\n",
      "36805/36805 [==============================] - 16s 431us/sample - loss: 0.7455 - acc: 0.7603 - val_loss: 0.4962 - val_acc: 0.8572\n",
      "Epoch 473/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7509 - acc: 0.7636\n",
      "Epoch 00473: val_loss did not improve from 0.48420\n",
      "36805/36805 [==============================] - 16s 432us/sample - loss: 0.7509 - acc: 0.7635 - val_loss: 0.4880 - val_acc: 0.8572\n",
      "Epoch 474/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7548 - acc: 0.7586\n",
      "Epoch 00474: val_loss did not improve from 0.48420\n",
      "36805/36805 [==============================] - 16s 433us/sample - loss: 0.7549 - acc: 0.7586 - val_loss: 0.4856 - val_acc: 0.8598\n",
      "Epoch 475/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7482 - acc: 0.7590\n",
      "Epoch 00475: val_loss did not improve from 0.48420\n",
      "36805/36805 [==============================] - 16s 434us/sample - loss: 0.7482 - acc: 0.7591 - val_loss: 0.4883 - val_acc: 0.8598\n",
      "Epoch 476/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7529 - acc: 0.7574\n",
      "Epoch 00476: val_loss did not improve from 0.48420\n",
      "36805/36805 [==============================] - 16s 434us/sample - loss: 0.7528 - acc: 0.7575 - val_loss: 0.4906 - val_acc: 0.8598\n",
      "Epoch 477/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7517 - acc: 0.7606\n",
      "Epoch 00477: val_loss did not improve from 0.48420\n",
      "36805/36805 [==============================] - 16s 433us/sample - loss: 0.7516 - acc: 0.7607 - val_loss: 0.4909 - val_acc: 0.8570\n",
      "Epoch 478/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7485 - acc: 0.7601\n",
      "Epoch 00478: val_loss did not improve from 0.48420\n",
      "36805/36805 [==============================] - 16s 437us/sample - loss: 0.7485 - acc: 0.7601 - val_loss: 0.4948 - val_acc: 0.8577\n",
      "Epoch 479/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7438 - acc: 0.7599\n",
      "Epoch 00479: val_loss did not improve from 0.48420\n",
      "36805/36805 [==============================] - 16s 432us/sample - loss: 0.7439 - acc: 0.7599 - val_loss: 0.4928 - val_acc: 0.8519\n",
      "Epoch 480/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7511 - acc: 0.7628\n",
      "Epoch 00480: val_loss did not improve from 0.48420\n",
      "36805/36805 [==============================] - 16s 438us/sample - loss: 0.7511 - acc: 0.7627 - val_loss: 0.4872 - val_acc: 0.8595\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 481/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7483 - acc: 0.7608\n",
      "Epoch 00481: val_loss improved from 0.48420 to 0.48366, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_4_conv_checkpoint/481-0.4837.hdf5\n",
      "36805/36805 [==============================] - 16s 439us/sample - loss: 0.7486 - acc: 0.7608 - val_loss: 0.4837 - val_acc: 0.8577\n",
      "Epoch 482/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7514 - acc: 0.7611\n",
      "Epoch 00482: val_loss did not improve from 0.48366\n",
      "36805/36805 [==============================] - 16s 439us/sample - loss: 0.7514 - acc: 0.7611 - val_loss: 0.4936 - val_acc: 0.8567\n",
      "Epoch 483/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7481 - acc: 0.7579\n",
      "Epoch 00483: val_loss did not improve from 0.48366\n",
      "36805/36805 [==============================] - 16s 440us/sample - loss: 0.7481 - acc: 0.7578 - val_loss: 0.4968 - val_acc: 0.8588\n",
      "Epoch 484/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7497 - acc: 0.7614\n",
      "Epoch 00484: val_loss did not improve from 0.48366\n",
      "36805/36805 [==============================] - 16s 440us/sample - loss: 0.7498 - acc: 0.7614 - val_loss: 0.4921 - val_acc: 0.8602\n",
      "Epoch 485/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7448 - acc: 0.7619\n",
      "Epoch 00485: val_loss did not improve from 0.48366\n",
      "36805/36805 [==============================] - 16s 440us/sample - loss: 0.7449 - acc: 0.7619 - val_loss: 0.4858 - val_acc: 0.8628\n",
      "Epoch 486/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7460 - acc: 0.7631\n",
      "Epoch 00486: val_loss did not improve from 0.48366\n",
      "36805/36805 [==============================] - 16s 447us/sample - loss: 0.7459 - acc: 0.7631 - val_loss: 0.4917 - val_acc: 0.8553\n",
      "Epoch 487/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7497 - acc: 0.7612\n",
      "Epoch 00487: val_loss did not improve from 0.48366\n",
      "36805/36805 [==============================] - 16s 445us/sample - loss: 0.7496 - acc: 0.7612 - val_loss: 0.5004 - val_acc: 0.8516\n",
      "Epoch 488/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7514 - acc: 0.7585\n",
      "Epoch 00488: val_loss did not improve from 0.48366\n",
      "36805/36805 [==============================] - 17s 449us/sample - loss: 0.7515 - acc: 0.7584 - val_loss: 0.4839 - val_acc: 0.8602\n",
      "Epoch 489/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7456 - acc: 0.7610\n",
      "Epoch 00489: val_loss did not improve from 0.48366\n",
      "36805/36805 [==============================] - 17s 450us/sample - loss: 0.7456 - acc: 0.7610 - val_loss: 0.4854 - val_acc: 0.8607\n",
      "Epoch 490/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7426 - acc: 0.7611\n",
      "Epoch 00490: val_loss did not improve from 0.48366\n",
      "36805/36805 [==============================] - 16s 440us/sample - loss: 0.7426 - acc: 0.7610 - val_loss: 0.4900 - val_acc: 0.8577\n",
      "Epoch 491/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7441 - acc: 0.7658\n",
      "Epoch 00491: val_loss did not improve from 0.48366\n",
      "36805/36805 [==============================] - 16s 438us/sample - loss: 0.7444 - acc: 0.7658 - val_loss: 0.4938 - val_acc: 0.8593\n",
      "Epoch 492/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7412 - acc: 0.7644\n",
      "Epoch 00492: val_loss did not improve from 0.48366\n",
      "36805/36805 [==============================] - 16s 440us/sample - loss: 0.7412 - acc: 0.7644 - val_loss: 0.4842 - val_acc: 0.8600\n",
      "Epoch 493/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7462 - acc: 0.7620\n",
      "Epoch 00493: val_loss did not improve from 0.48366\n",
      "36805/36805 [==============================] - 16s 439us/sample - loss: 0.7461 - acc: 0.7621 - val_loss: 0.4861 - val_acc: 0.8637\n",
      "Epoch 494/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7418 - acc: 0.7610\n",
      "Epoch 00494: val_loss did not improve from 0.48366\n",
      "36805/36805 [==============================] - 16s 441us/sample - loss: 0.7418 - acc: 0.7610 - val_loss: 0.4881 - val_acc: 0.8633\n",
      "Epoch 495/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7411 - acc: 0.7613\n",
      "Epoch 00495: val_loss did not improve from 0.48366\n",
      "36805/36805 [==============================] - 16s 441us/sample - loss: 0.7411 - acc: 0.7613 - val_loss: 0.4885 - val_acc: 0.8586\n",
      "Epoch 496/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7402 - acc: 0.7659\n",
      "Epoch 00496: val_loss did not improve from 0.48366\n",
      "36805/36805 [==============================] - 16s 441us/sample - loss: 0.7402 - acc: 0.7659 - val_loss: 0.5019 - val_acc: 0.8581\n",
      "Epoch 497/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7467 - acc: 0.7611\n",
      "Epoch 00497: val_loss did not improve from 0.48366\n",
      "36805/36805 [==============================] - 16s 439us/sample - loss: 0.7467 - acc: 0.7611 - val_loss: 0.4953 - val_acc: 0.8577\n",
      "Epoch 498/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7453 - acc: 0.7592\n",
      "Epoch 00498: val_loss did not improve from 0.48366\n",
      "36805/36805 [==============================] - 16s 440us/sample - loss: 0.7453 - acc: 0.7591 - val_loss: 0.4892 - val_acc: 0.8598\n",
      "Epoch 499/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7419 - acc: 0.7608\n",
      "Epoch 00499: val_loss did not improve from 0.48366\n",
      "36805/36805 [==============================] - 16s 442us/sample - loss: 0.7418 - acc: 0.7609 - val_loss: 0.4843 - val_acc: 0.8598\n",
      "Epoch 500/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7468 - acc: 0.7626\n",
      "Epoch 00500: val_loss did not improve from 0.48366\n",
      "36805/36805 [==============================] - 16s 437us/sample - loss: 0.7468 - acc: 0.7626 - val_loss: 0.4873 - val_acc: 0.8593\n",
      "\n",
      "1D_CNN_custom_multi_2_GMP_ch_32_DO_4_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 1s 265us/sample - loss: 0.5608 - acc: 0.8289\n",
      "Loss: 0.5607636648422709 Accuracy: 0.8288681\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.6079 - acc: 0.1335\n",
      "Epoch 00001: val_loss improved from inf to 2.28873, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_5_conv_checkpoint/001-2.2887.hdf5\n",
      "36805/36805 [==============================] - 19s 521us/sample - loss: 2.6079 - acc: 0.1334 - val_loss: 2.2887 - val_acc: 0.2942\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.2680 - acc: 0.2411\n",
      "Epoch 00002: val_loss improved from 2.28873 to 1.99128, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_5_conv_checkpoint/002-1.9913.hdf5\n",
      "36805/36805 [==============================] - 17s 463us/sample - loss: 2.2679 - acc: 0.2411 - val_loss: 1.9913 - val_acc: 0.3965\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.0542 - acc: 0.3127\n",
      "Epoch 00003: val_loss improved from 1.99128 to 1.75154, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_5_conv_checkpoint/003-1.7515.hdf5\n",
      "36805/36805 [==============================] - 17s 465us/sample - loss: 2.0542 - acc: 0.3128 - val_loss: 1.7515 - val_acc: 0.4812\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.8927 - acc: 0.3722\n",
      "Epoch 00004: val_loss improved from 1.75154 to 1.57692, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_5_conv_checkpoint/004-1.5769.hdf5\n",
      "36805/36805 [==============================] - 17s 463us/sample - loss: 1.8927 - acc: 0.3721 - val_loss: 1.5769 - val_acc: 0.5262\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.7580 - acc: 0.4169\n",
      "Epoch 00005: val_loss improved from 1.57692 to 1.43275, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_5_conv_checkpoint/005-1.4327.hdf5\n",
      "36805/36805 [==============================] - 17s 463us/sample - loss: 1.7580 - acc: 0.4169 - val_loss: 1.4327 - val_acc: 0.5847\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6471 - acc: 0.4531\n",
      "Epoch 00006: val_loss improved from 1.43275 to 1.30089, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_5_conv_checkpoint/006-1.3009.hdf5\n",
      "36805/36805 [==============================] - 17s 463us/sample - loss: 1.6472 - acc: 0.4531 - val_loss: 1.3009 - val_acc: 0.6261\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5533 - acc: 0.4836\n",
      "Epoch 00007: val_loss improved from 1.30089 to 1.18944, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_5_conv_checkpoint/007-1.1894.hdf5\n",
      "36805/36805 [==============================] - 17s 460us/sample - loss: 1.5533 - acc: 0.4837 - val_loss: 1.1894 - val_acc: 0.6601\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4613 - acc: 0.5186\n",
      "Epoch 00008: val_loss improved from 1.18944 to 1.10408, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_5_conv_checkpoint/008-1.1041.hdf5\n",
      "36805/36805 [==============================] - 17s 465us/sample - loss: 1.4612 - acc: 0.5186 - val_loss: 1.1041 - val_acc: 0.6865\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3826 - acc: 0.5465\n",
      "Epoch 00009: val_loss improved from 1.10408 to 1.02541, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_5_conv_checkpoint/009-1.0254.hdf5\n",
      "36805/36805 [==============================] - 17s 466us/sample - loss: 1.3825 - acc: 0.5466 - val_loss: 1.0254 - val_acc: 0.7105\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3120 - acc: 0.5731\n",
      "Epoch 00010: val_loss improved from 1.02541 to 0.96263, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_5_conv_checkpoint/010-0.9626.hdf5\n",
      "36805/36805 [==============================] - 17s 465us/sample - loss: 1.3120 - acc: 0.5731 - val_loss: 0.9626 - val_acc: 0.7270\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2593 - acc: 0.5894\n",
      "Epoch 00011: val_loss improved from 0.96263 to 0.90266, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_5_conv_checkpoint/011-0.9027.hdf5\n",
      "36805/36805 [==============================] - 17s 466us/sample - loss: 1.2592 - acc: 0.5894 - val_loss: 0.9027 - val_acc: 0.7491\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2028 - acc: 0.6127\n",
      "Epoch 00012: val_loss improved from 0.90266 to 0.86673, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_5_conv_checkpoint/012-0.8667.hdf5\n",
      "36805/36805 [==============================] - 17s 463us/sample - loss: 1.2029 - acc: 0.6127 - val_loss: 0.8667 - val_acc: 0.7459\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1584 - acc: 0.6310\n",
      "Epoch 00013: val_loss improved from 0.86673 to 0.82502, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_5_conv_checkpoint/013-0.8250.hdf5\n",
      "36805/36805 [==============================] - 17s 465us/sample - loss: 1.1584 - acc: 0.6310 - val_loss: 0.8250 - val_acc: 0.7533\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1103 - acc: 0.6460\n",
      "Epoch 00014: val_loss improved from 0.82502 to 0.76748, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_5_conv_checkpoint/014-0.7675.hdf5\n",
      "36805/36805 [==============================] - 17s 463us/sample - loss: 1.1102 - acc: 0.6460 - val_loss: 0.7675 - val_acc: 0.7764\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0707 - acc: 0.6594\n",
      "Epoch 00015: val_loss improved from 0.76748 to 0.73549, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_5_conv_checkpoint/015-0.7355.hdf5\n",
      "36805/36805 [==============================] - 17s 462us/sample - loss: 1.0707 - acc: 0.6594 - val_loss: 0.7355 - val_acc: 0.7911\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0332 - acc: 0.6734\n",
      "Epoch 00016: val_loss improved from 0.73549 to 0.70640, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_5_conv_checkpoint/016-0.7064.hdf5\n",
      "36805/36805 [==============================] - 17s 460us/sample - loss: 1.0332 - acc: 0.6734 - val_loss: 0.7064 - val_acc: 0.7945\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0147 - acc: 0.6812\n",
      "Epoch 00017: val_loss improved from 0.70640 to 0.68449, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_5_conv_checkpoint/017-0.6845.hdf5\n",
      "36805/36805 [==============================] - 17s 466us/sample - loss: 1.0146 - acc: 0.6813 - val_loss: 0.6845 - val_acc: 0.8025\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9836 - acc: 0.6924\n",
      "Epoch 00018: val_loss improved from 0.68449 to 0.66998, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_5_conv_checkpoint/018-0.6700.hdf5\n",
      "36805/36805 [==============================] - 17s 464us/sample - loss: 0.9837 - acc: 0.6924 - val_loss: 0.6700 - val_acc: 0.8041\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9562 - acc: 0.7006\n",
      "Epoch 00019: val_loss improved from 0.66998 to 0.64124, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_5_conv_checkpoint/019-0.6412.hdf5\n",
      "36805/36805 [==============================] - 17s 462us/sample - loss: 0.9562 - acc: 0.7005 - val_loss: 0.6412 - val_acc: 0.8169\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9373 - acc: 0.7044\n",
      "Epoch 00020: val_loss improved from 0.64124 to 0.63005, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_5_conv_checkpoint/020-0.6300.hdf5\n",
      "36805/36805 [==============================] - 17s 463us/sample - loss: 0.9373 - acc: 0.7044 - val_loss: 0.6300 - val_acc: 0.8167\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9076 - acc: 0.7186\n",
      "Epoch 00021: val_loss improved from 0.63005 to 0.60103, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_5_conv_checkpoint/021-0.6010.hdf5\n",
      "36805/36805 [==============================] - 17s 465us/sample - loss: 0.9077 - acc: 0.7186 - val_loss: 0.6010 - val_acc: 0.8267\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8930 - acc: 0.7226\n",
      "Epoch 00022: val_loss improved from 0.60103 to 0.59074, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_5_conv_checkpoint/022-0.5907.hdf5\n",
      "36805/36805 [==============================] - 17s 462us/sample - loss: 0.8929 - acc: 0.7226 - val_loss: 0.5907 - val_acc: 0.8281\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8806 - acc: 0.7243\n",
      "Epoch 00023: val_loss improved from 0.59074 to 0.58567, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_5_conv_checkpoint/023-0.5857.hdf5\n",
      "36805/36805 [==============================] - 17s 462us/sample - loss: 0.8806 - acc: 0.7243 - val_loss: 0.5857 - val_acc: 0.8311\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8658 - acc: 0.7331\n",
      "Epoch 00024: val_loss improved from 0.58567 to 0.56738, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_5_conv_checkpoint/024-0.5674.hdf5\n",
      "36805/36805 [==============================] - 17s 460us/sample - loss: 0.8658 - acc: 0.7331 - val_loss: 0.5674 - val_acc: 0.8390\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8450 - acc: 0.7374\n",
      "Epoch 00025: val_loss improved from 0.56738 to 0.54865, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_5_conv_checkpoint/025-0.5487.hdf5\n",
      "36805/36805 [==============================] - 17s 463us/sample - loss: 0.8449 - acc: 0.7375 - val_loss: 0.5487 - val_acc: 0.8416\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8417 - acc: 0.7384\n",
      "Epoch 00026: val_loss did not improve from 0.54865\n",
      "36805/36805 [==============================] - 17s 462us/sample - loss: 0.8418 - acc: 0.7383 - val_loss: 0.5581 - val_acc: 0.8409\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8276 - acc: 0.7437\n",
      "Epoch 00027: val_loss improved from 0.54865 to 0.53204, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_5_conv_checkpoint/027-0.5320.hdf5\n",
      "36805/36805 [==============================] - 17s 463us/sample - loss: 0.8277 - acc: 0.7436 - val_loss: 0.5320 - val_acc: 0.8465\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8136 - acc: 0.7500\n",
      "Epoch 00028: val_loss improved from 0.53204 to 0.52361, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_5_conv_checkpoint/028-0.5236.hdf5\n",
      "36805/36805 [==============================] - 17s 461us/sample - loss: 0.8135 - acc: 0.7500 - val_loss: 0.5236 - val_acc: 0.8465\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8021 - acc: 0.7538\n",
      "Epoch 00029: val_loss improved from 0.52361 to 0.51980, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_5_conv_checkpoint/029-0.5198.hdf5\n",
      "36805/36805 [==============================] - 17s 465us/sample - loss: 0.8021 - acc: 0.7538 - val_loss: 0.5198 - val_acc: 0.8498\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7885 - acc: 0.7528\n",
      "Epoch 00030: val_loss improved from 0.51980 to 0.51620, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_5_conv_checkpoint/030-0.5162.hdf5\n",
      "36805/36805 [==============================] - 17s 463us/sample - loss: 0.7885 - acc: 0.7528 - val_loss: 0.5162 - val_acc: 0.8502\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7777 - acc: 0.7614\n",
      "Epoch 00031: val_loss improved from 0.51620 to 0.49607, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_5_conv_checkpoint/031-0.4961.hdf5\n",
      "36805/36805 [==============================] - 17s 464us/sample - loss: 0.7777 - acc: 0.7614 - val_loss: 0.4961 - val_acc: 0.8560\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7746 - acc: 0.7599\n",
      "Epoch 00032: val_loss improved from 0.49607 to 0.48981, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_5_conv_checkpoint/032-0.4898.hdf5\n",
      "36805/36805 [==============================] - 17s 466us/sample - loss: 0.7746 - acc: 0.7599 - val_loss: 0.4898 - val_acc: 0.8577\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7554 - acc: 0.7661\n",
      "Epoch 00033: val_loss did not improve from 0.48981\n",
      "36805/36805 [==============================] - 17s 462us/sample - loss: 0.7553 - acc: 0.7661 - val_loss: 0.4914 - val_acc: 0.8563\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7552 - acc: 0.7667\n",
      "Epoch 00034: val_loss improved from 0.48981 to 0.47973, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_5_conv_checkpoint/034-0.4797.hdf5\n",
      "36805/36805 [==============================] - 17s 467us/sample - loss: 0.7552 - acc: 0.7667 - val_loss: 0.4797 - val_acc: 0.8572\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7432 - acc: 0.7714\n",
      "Epoch 00035: val_loss improved from 0.47973 to 0.46549, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_5_conv_checkpoint/035-0.4655.hdf5\n",
      "36805/36805 [==============================] - 17s 467us/sample - loss: 0.7431 - acc: 0.7714 - val_loss: 0.4655 - val_acc: 0.8623\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7352 - acc: 0.7719\n",
      "Epoch 00036: val_loss did not improve from 0.46549\n",
      "36805/36805 [==============================] - 17s 463us/sample - loss: 0.7352 - acc: 0.7719 - val_loss: 0.4831 - val_acc: 0.8579\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7304 - acc: 0.7749\n",
      "Epoch 00037: val_loss did not improve from 0.46549\n",
      "36805/36805 [==============================] - 17s 462us/sample - loss: 0.7304 - acc: 0.7749 - val_loss: 0.4662 - val_acc: 0.8633\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7212 - acc: 0.7768\n",
      "Epoch 00038: val_loss improved from 0.46549 to 0.44211, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_5_conv_checkpoint/038-0.4421.hdf5\n",
      "36805/36805 [==============================] - 17s 466us/sample - loss: 0.7211 - acc: 0.7768 - val_loss: 0.4421 - val_acc: 0.8675\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7138 - acc: 0.7814\n",
      "Epoch 00039: val_loss did not improve from 0.44211\n",
      "36805/36805 [==============================] - 17s 465us/sample - loss: 0.7137 - acc: 0.7814 - val_loss: 0.4574 - val_acc: 0.8677\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7073 - acc: 0.7818\n",
      "Epoch 00040: val_loss did not improve from 0.44211\n",
      "36805/36805 [==============================] - 17s 464us/sample - loss: 0.7073 - acc: 0.7819 - val_loss: 0.4494 - val_acc: 0.8693\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6985 - acc: 0.7849\n",
      "Epoch 00041: val_loss did not improve from 0.44211\n",
      "36805/36805 [==============================] - 17s 463us/sample - loss: 0.6988 - acc: 0.7848 - val_loss: 0.4485 - val_acc: 0.8689\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7010 - acc: 0.7866\n",
      "Epoch 00042: val_loss improved from 0.44211 to 0.43355, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_5_conv_checkpoint/042-0.4336.hdf5\n",
      "36805/36805 [==============================] - 17s 463us/sample - loss: 0.7011 - acc: 0.7866 - val_loss: 0.4336 - val_acc: 0.8721\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6922 - acc: 0.7863\n",
      "Epoch 00043: val_loss did not improve from 0.43355\n",
      "36805/36805 [==============================] - 17s 462us/sample - loss: 0.6922 - acc: 0.7863 - val_loss: 0.4429 - val_acc: 0.8698\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6824 - acc: 0.7879\n",
      "Epoch 00044: val_loss improved from 0.43355 to 0.42590, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_5_conv_checkpoint/044-0.4259.hdf5\n",
      "36805/36805 [==============================] - 17s 464us/sample - loss: 0.6825 - acc: 0.7879 - val_loss: 0.4259 - val_acc: 0.8742\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6809 - acc: 0.7895\n",
      "Epoch 00045: val_loss did not improve from 0.42590\n",
      "36805/36805 [==============================] - 17s 463us/sample - loss: 0.6810 - acc: 0.7895 - val_loss: 0.4529 - val_acc: 0.8670\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6813 - acc: 0.7911\n",
      "Epoch 00046: val_loss did not improve from 0.42590\n",
      "36805/36805 [==============================] - 17s 461us/sample - loss: 0.6812 - acc: 0.7911 - val_loss: 0.4332 - val_acc: 0.8730\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6721 - acc: 0.7927\n",
      "Epoch 00047: val_loss improved from 0.42590 to 0.41658, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_5_conv_checkpoint/047-0.4166.hdf5\n",
      "36805/36805 [==============================] - 17s 460us/sample - loss: 0.6720 - acc: 0.7927 - val_loss: 0.4166 - val_acc: 0.8793\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6685 - acc: 0.7934\n",
      "Epoch 00048: val_loss improved from 0.41658 to 0.41611, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_5_conv_checkpoint/048-0.4161.hdf5\n",
      "36805/36805 [==============================] - 17s 466us/sample - loss: 0.6686 - acc: 0.7934 - val_loss: 0.4161 - val_acc: 0.8791\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6603 - acc: 0.7982\n",
      "Epoch 00049: val_loss did not improve from 0.41611\n",
      "36805/36805 [==============================] - 17s 464us/sample - loss: 0.6603 - acc: 0.7982 - val_loss: 0.4189 - val_acc: 0.8831\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6552 - acc: 0.7953\n",
      "Epoch 00050: val_loss improved from 0.41611 to 0.40817, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_5_conv_checkpoint/050-0.4082.hdf5\n",
      "36805/36805 [==============================] - 17s 466us/sample - loss: 0.6552 - acc: 0.7953 - val_loss: 0.4082 - val_acc: 0.8796\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6549 - acc: 0.7973\n",
      "Epoch 00051: val_loss did not improve from 0.40817\n",
      "36805/36805 [==============================] - 17s 463us/sample - loss: 0.6549 - acc: 0.7973 - val_loss: 0.4164 - val_acc: 0.8782\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6482 - acc: 0.8012\n",
      "Epoch 00052: val_loss improved from 0.40817 to 0.40199, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_5_conv_checkpoint/052-0.4020.hdf5\n",
      "36805/36805 [==============================] - 17s 465us/sample - loss: 0.6483 - acc: 0.8011 - val_loss: 0.4020 - val_acc: 0.8819\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6482 - acc: 0.7984\n",
      "Epoch 00053: val_loss improved from 0.40199 to 0.39645, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_5_conv_checkpoint/053-0.3964.hdf5\n",
      "36805/36805 [==============================] - 17s 467us/sample - loss: 0.6481 - acc: 0.7985 - val_loss: 0.3964 - val_acc: 0.8852\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6421 - acc: 0.8035\n",
      "Epoch 00054: val_loss did not improve from 0.39645\n",
      "36805/36805 [==============================] - 17s 464us/sample - loss: 0.6420 - acc: 0.8035 - val_loss: 0.4122 - val_acc: 0.8859\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6364 - acc: 0.8033\n",
      "Epoch 00055: val_loss did not improve from 0.39645\n",
      "36805/36805 [==============================] - 17s 462us/sample - loss: 0.6364 - acc: 0.8033 - val_loss: 0.4063 - val_acc: 0.8803\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6286 - acc: 0.8067\n",
      "Epoch 00056: val_loss improved from 0.39645 to 0.38461, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_5_conv_checkpoint/056-0.3846.hdf5\n",
      "36805/36805 [==============================] - 17s 463us/sample - loss: 0.6286 - acc: 0.8067 - val_loss: 0.3846 - val_acc: 0.8919\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6345 - acc: 0.8049\n",
      "Epoch 00057: val_loss did not improve from 0.38461\n",
      "36805/36805 [==============================] - 17s 464us/sample - loss: 0.6345 - acc: 0.8049 - val_loss: 0.3965 - val_acc: 0.8891\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6283 - acc: 0.8071\n",
      "Epoch 00058: val_loss did not improve from 0.38461\n",
      "36805/36805 [==============================] - 17s 464us/sample - loss: 0.6285 - acc: 0.8071 - val_loss: 0.3992 - val_acc: 0.8833\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6196 - acc: 0.8090\n",
      "Epoch 00059: val_loss improved from 0.38461 to 0.37624, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_5_conv_checkpoint/059-0.3762.hdf5\n",
      "36805/36805 [==============================] - 17s 464us/sample - loss: 0.6196 - acc: 0.8090 - val_loss: 0.3762 - val_acc: 0.8891\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6230 - acc: 0.8091\n",
      "Epoch 00060: val_loss improved from 0.37624 to 0.37232, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_5_conv_checkpoint/060-0.3723.hdf5\n",
      "36805/36805 [==============================] - 17s 464us/sample - loss: 0.6230 - acc: 0.8091 - val_loss: 0.3723 - val_acc: 0.8896\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6238 - acc: 0.8107\n",
      "Epoch 00061: val_loss did not improve from 0.37232\n",
      "36805/36805 [==============================] - 17s 462us/sample - loss: 0.6238 - acc: 0.8107 - val_loss: 0.3732 - val_acc: 0.8917\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6171 - acc: 0.8106\n",
      "Epoch 00062: val_loss did not improve from 0.37232\n",
      "36805/36805 [==============================] - 17s 462us/sample - loss: 0.6172 - acc: 0.8105 - val_loss: 0.3992 - val_acc: 0.8803\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6060 - acc: 0.8128\n",
      "Epoch 00063: val_loss improved from 0.37232 to 0.37115, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_5_conv_checkpoint/063-0.3711.hdf5\n",
      "36805/36805 [==============================] - 17s 463us/sample - loss: 0.6059 - acc: 0.8128 - val_loss: 0.3711 - val_acc: 0.8898\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6056 - acc: 0.8135\n",
      "Epoch 00064: val_loss improved from 0.37115 to 0.36605, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_5_conv_checkpoint/064-0.3661.hdf5\n",
      "36805/36805 [==============================] - 17s 465us/sample - loss: 0.6055 - acc: 0.8135 - val_loss: 0.3661 - val_acc: 0.8926\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6080 - acc: 0.8146\n",
      "Epoch 00065: val_loss did not improve from 0.36605\n",
      "36805/36805 [==============================] - 17s 463us/sample - loss: 0.6079 - acc: 0.8146 - val_loss: 0.3692 - val_acc: 0.8940\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6010 - acc: 0.8136\n",
      "Epoch 00066: val_loss did not improve from 0.36605\n",
      "36805/36805 [==============================] - 17s 462us/sample - loss: 0.6010 - acc: 0.8136 - val_loss: 0.3749 - val_acc: 0.8891\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6003 - acc: 0.8142\n",
      "Epoch 00067: val_loss did not improve from 0.36605\n",
      "36805/36805 [==============================] - 17s 465us/sample - loss: 0.6003 - acc: 0.8142 - val_loss: 0.3688 - val_acc: 0.8884\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5955 - acc: 0.8160\n",
      "Epoch 00068: val_loss did not improve from 0.36605\n",
      "36805/36805 [==============================] - 17s 465us/sample - loss: 0.5955 - acc: 0.8160 - val_loss: 0.3753 - val_acc: 0.8870\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5909 - acc: 0.8176\n",
      "Epoch 00069: val_loss improved from 0.36605 to 0.35737, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_5_conv_checkpoint/069-0.3574.hdf5\n",
      "36805/36805 [==============================] - 17s 464us/sample - loss: 0.5909 - acc: 0.8176 - val_loss: 0.3574 - val_acc: 0.8952\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5951 - acc: 0.8177\n",
      "Epoch 00070: val_loss did not improve from 0.35737\n",
      "36805/36805 [==============================] - 17s 465us/sample - loss: 0.5951 - acc: 0.8177 - val_loss: 0.3668 - val_acc: 0.8931\n",
      "Epoch 71/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5943 - acc: 0.8177\n",
      "Epoch 00071: val_loss did not improve from 0.35737\n",
      "36805/36805 [==============================] - 17s 464us/sample - loss: 0.5944 - acc: 0.8177 - val_loss: 0.3590 - val_acc: 0.8921\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5928 - acc: 0.8181\n",
      "Epoch 00072: val_loss did not improve from 0.35737\n",
      "36805/36805 [==============================] - 17s 463us/sample - loss: 0.5927 - acc: 0.8181 - val_loss: 0.3646 - val_acc: 0.8880\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5886 - acc: 0.8188\n",
      "Epoch 00073: val_loss did not improve from 0.35737\n",
      "36805/36805 [==============================] - 17s 461us/sample - loss: 0.5886 - acc: 0.8188 - val_loss: 0.3595 - val_acc: 0.8949\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5793 - acc: 0.8220\n",
      "Epoch 00074: val_loss improved from 0.35737 to 0.35370, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_5_conv_checkpoint/074-0.3537.hdf5\n",
      "36805/36805 [==============================] - 17s 468us/sample - loss: 0.5792 - acc: 0.8220 - val_loss: 0.3537 - val_acc: 0.8984\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5790 - acc: 0.8212\n",
      "Epoch 00075: val_loss did not improve from 0.35370\n",
      "36805/36805 [==============================] - 17s 463us/sample - loss: 0.5791 - acc: 0.8211 - val_loss: 0.3588 - val_acc: 0.8952\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5819 - acc: 0.8201\n",
      "Epoch 00076: val_loss improved from 0.35370 to 0.35000, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_5_conv_checkpoint/076-0.3500.hdf5\n",
      "36805/36805 [==============================] - 17s 462us/sample - loss: 0.5818 - acc: 0.8201 - val_loss: 0.3500 - val_acc: 0.8949\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5781 - acc: 0.8208\n",
      "Epoch 00077: val_loss did not improve from 0.35000\n",
      "36805/36805 [==============================] - 17s 463us/sample - loss: 0.5781 - acc: 0.8207 - val_loss: 0.3584 - val_acc: 0.8935\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5725 - acc: 0.8251\n",
      "Epoch 00078: val_loss improved from 0.35000 to 0.34772, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_5_conv_checkpoint/078-0.3477.hdf5\n",
      "36805/36805 [==============================] - 17s 466us/sample - loss: 0.5725 - acc: 0.8251 - val_loss: 0.3477 - val_acc: 0.8975\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5766 - acc: 0.8238\n",
      "Epoch 00079: val_loss did not improve from 0.34772\n",
      "36805/36805 [==============================] - 17s 463us/sample - loss: 0.5765 - acc: 0.8238 - val_loss: 0.3490 - val_acc: 0.8966\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5748 - acc: 0.8238\n",
      "Epoch 00080: val_loss improved from 0.34772 to 0.34723, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_5_conv_checkpoint/080-0.3472.hdf5\n",
      "36805/36805 [==============================] - 17s 464us/sample - loss: 0.5748 - acc: 0.8238 - val_loss: 0.3472 - val_acc: 0.8952\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5696 - acc: 0.8239\n",
      "Epoch 00081: val_loss improved from 0.34723 to 0.34702, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_5_conv_checkpoint/081-0.3470.hdf5\n",
      "36805/36805 [==============================] - 17s 463us/sample - loss: 0.5696 - acc: 0.8239 - val_loss: 0.3470 - val_acc: 0.8945\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5653 - acc: 0.8267\n",
      "Epoch 00082: val_loss improved from 0.34702 to 0.34587, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_5_conv_checkpoint/082-0.3459.hdf5\n",
      "36805/36805 [==============================] - 17s 464us/sample - loss: 0.5654 - acc: 0.8266 - val_loss: 0.3459 - val_acc: 0.9005\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5629 - acc: 0.8272\n",
      "Epoch 00083: val_loss did not improve from 0.34587\n",
      "36805/36805 [==============================] - 17s 465us/sample - loss: 0.5628 - acc: 0.8273 - val_loss: 0.3472 - val_acc: 0.8961\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5629 - acc: 0.8257\n",
      "Epoch 00084: val_loss improved from 0.34587 to 0.34243, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_5_conv_checkpoint/084-0.3424.hdf5\n",
      "36805/36805 [==============================] - 17s 462us/sample - loss: 0.5629 - acc: 0.8257 - val_loss: 0.3424 - val_acc: 0.8996\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5604 - acc: 0.8254\n",
      "Epoch 00085: val_loss did not improve from 0.34243\n",
      "36805/36805 [==============================] - 17s 463us/sample - loss: 0.5604 - acc: 0.8254 - val_loss: 0.3449 - val_acc: 0.8984\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5590 - acc: 0.8279\n",
      "Epoch 00086: val_loss did not improve from 0.34243\n",
      "36805/36805 [==============================] - 17s 464us/sample - loss: 0.5590 - acc: 0.8279 - val_loss: 0.3559 - val_acc: 0.8947\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5620 - acc: 0.8273\n",
      "Epoch 00087: val_loss did not improve from 0.34243\n",
      "36805/36805 [==============================] - 17s 464us/sample - loss: 0.5619 - acc: 0.8274 - val_loss: 0.3435 - val_acc: 0.9017\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5545 - acc: 0.8297\n",
      "Epoch 00088: val_loss did not improve from 0.34243\n",
      "36805/36805 [==============================] - 17s 462us/sample - loss: 0.5545 - acc: 0.8297 - val_loss: 0.3446 - val_acc: 0.8996\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5479 - acc: 0.8304\n",
      "Epoch 00089: val_loss improved from 0.34243 to 0.34138, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_5_conv_checkpoint/089-0.3414.hdf5\n",
      "36805/36805 [==============================] - 17s 465us/sample - loss: 0.5479 - acc: 0.8304 - val_loss: 0.3414 - val_acc: 0.9005\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5477 - acc: 0.8304\n",
      "Epoch 00090: val_loss improved from 0.34138 to 0.33494, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_5_conv_checkpoint/090-0.3349.hdf5\n",
      "36805/36805 [==============================] - 17s 465us/sample - loss: 0.5477 - acc: 0.8304 - val_loss: 0.3349 - val_acc: 0.9026\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5539 - acc: 0.8285\n",
      "Epoch 00091: val_loss did not improve from 0.33494\n",
      "36805/36805 [==============================] - 17s 465us/sample - loss: 0.5538 - acc: 0.8285 - val_loss: 0.3471 - val_acc: 0.9012\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5407 - acc: 0.8340\n",
      "Epoch 00092: val_loss improved from 0.33494 to 0.32935, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_5_conv_checkpoint/092-0.3294.hdf5\n",
      "36805/36805 [==============================] - 17s 467us/sample - loss: 0.5406 - acc: 0.8341 - val_loss: 0.3294 - val_acc: 0.9036\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5430 - acc: 0.8302\n",
      "Epoch 00093: val_loss did not improve from 0.32935\n",
      "36805/36805 [==============================] - 17s 461us/sample - loss: 0.5430 - acc: 0.8302 - val_loss: 0.3357 - val_acc: 0.9022\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5444 - acc: 0.8314\n",
      "Epoch 00094: val_loss did not improve from 0.32935\n",
      "36805/36805 [==============================] - 17s 465us/sample - loss: 0.5444 - acc: 0.8314 - val_loss: 0.3353 - val_acc: 0.9003\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5447 - acc: 0.8300\n",
      "Epoch 00095: val_loss did not improve from 0.32935\n",
      "36805/36805 [==============================] - 17s 462us/sample - loss: 0.5447 - acc: 0.8299 - val_loss: 0.3301 - val_acc: 0.9043\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5478 - acc: 0.8325\n",
      "Epoch 00096: val_loss improved from 0.32935 to 0.32923, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_5_conv_checkpoint/096-0.3292.hdf5\n",
      "36805/36805 [==============================] - 17s 464us/sample - loss: 0.5479 - acc: 0.8324 - val_loss: 0.3292 - val_acc: 0.9057\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5384 - acc: 0.8327\n",
      "Epoch 00097: val_loss did not improve from 0.32923\n",
      "36805/36805 [==============================] - 17s 463us/sample - loss: 0.5384 - acc: 0.8326 - val_loss: 0.3326 - val_acc: 0.9029\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5383 - acc: 0.8335\n",
      "Epoch 00098: val_loss did not improve from 0.32923\n",
      "36805/36805 [==============================] - 17s 461us/sample - loss: 0.5383 - acc: 0.8336 - val_loss: 0.3321 - val_acc: 0.9008\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5415 - acc: 0.8352\n",
      "Epoch 00099: val_loss improved from 0.32923 to 0.32800, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_5_conv_checkpoint/099-0.3280.hdf5\n",
      "36805/36805 [==============================] - 17s 463us/sample - loss: 0.5415 - acc: 0.8352 - val_loss: 0.3280 - val_acc: 0.9015\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5338 - acc: 0.8361\n",
      "Epoch 00100: val_loss did not improve from 0.32800\n",
      "36805/36805 [==============================] - 17s 463us/sample - loss: 0.5338 - acc: 0.8360 - val_loss: 0.3327 - val_acc: 0.9029\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5354 - acc: 0.8326\n",
      "Epoch 00101: val_loss did not improve from 0.32800\n",
      "36805/36805 [==============================] - 17s 463us/sample - loss: 0.5353 - acc: 0.8326 - val_loss: 0.3335 - val_acc: 0.8996\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5309 - acc: 0.8363\n",
      "Epoch 00102: val_loss improved from 0.32800 to 0.32548, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_5_conv_checkpoint/102-0.3255.hdf5\n",
      "36805/36805 [==============================] - 17s 465us/sample - loss: 0.5309 - acc: 0.8363 - val_loss: 0.3255 - val_acc: 0.9022\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5276 - acc: 0.8383\n",
      "Epoch 00103: val_loss did not improve from 0.32548\n",
      "36805/36805 [==============================] - 17s 467us/sample - loss: 0.5276 - acc: 0.8383 - val_loss: 0.3302 - val_acc: 0.9075\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5277 - acc: 0.8382\n",
      "Epoch 00104: val_loss improved from 0.32548 to 0.32484, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_5_conv_checkpoint/104-0.3248.hdf5\n",
      "36805/36805 [==============================] - 17s 461us/sample - loss: 0.5277 - acc: 0.8382 - val_loss: 0.3248 - val_acc: 0.9033\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5276 - acc: 0.8360\n",
      "Epoch 00105: val_loss improved from 0.32484 to 0.31973, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_5_conv_checkpoint/105-0.3197.hdf5\n",
      "36805/36805 [==============================] - 17s 466us/sample - loss: 0.5276 - acc: 0.8360 - val_loss: 0.3197 - val_acc: 0.9073\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5247 - acc: 0.8376\n",
      "Epoch 00106: val_loss did not improve from 0.31973\n",
      "36805/36805 [==============================] - 17s 460us/sample - loss: 0.5247 - acc: 0.8375 - val_loss: 0.3320 - val_acc: 0.9012\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5207 - acc: 0.8379\n",
      "Epoch 00107: val_loss did not improve from 0.31973\n",
      "36805/36805 [==============================] - 17s 458us/sample - loss: 0.5208 - acc: 0.8379 - val_loss: 0.3326 - val_acc: 0.9003\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5216 - acc: 0.8381\n",
      "Epoch 00108: val_loss did not improve from 0.31973\n",
      "36805/36805 [==============================] - 17s 464us/sample - loss: 0.5216 - acc: 0.8381 - val_loss: 0.3231 - val_acc: 0.9052\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5183 - acc: 0.8382\n",
      "Epoch 00109: val_loss did not improve from 0.31973\n",
      "36805/36805 [==============================] - 17s 462us/sample - loss: 0.5184 - acc: 0.8382 - val_loss: 0.3312 - val_acc: 0.9050\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5205 - acc: 0.8385\n",
      "Epoch 00110: val_loss did not improve from 0.31973\n",
      "36805/36805 [==============================] - 17s 458us/sample - loss: 0.5204 - acc: 0.8385 - val_loss: 0.3228 - val_acc: 0.9045\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5137 - acc: 0.8399\n",
      "Epoch 00111: val_loss did not improve from 0.31973\n",
      "36805/36805 [==============================] - 17s 464us/sample - loss: 0.5138 - acc: 0.8399 - val_loss: 0.3202 - val_acc: 0.9061\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5164 - acc: 0.8422\n",
      "Epoch 00112: val_loss did not improve from 0.31973\n",
      "36805/36805 [==============================] - 17s 463us/sample - loss: 0.5163 - acc: 0.8422 - val_loss: 0.3219 - val_acc: 0.9057\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5110 - acc: 0.8413\n",
      "Epoch 00113: val_loss improved from 0.31973 to 0.31933, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_5_conv_checkpoint/113-0.3193.hdf5\n",
      "36805/36805 [==============================] - 17s 468us/sample - loss: 0.5110 - acc: 0.8413 - val_loss: 0.3193 - val_acc: 0.9043\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5124 - acc: 0.8412\n",
      "Epoch 00114: val_loss improved from 0.31933 to 0.31554, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_5_conv_checkpoint/114-0.3155.hdf5\n",
      "36805/36805 [==============================] - 17s 465us/sample - loss: 0.5123 - acc: 0.8412 - val_loss: 0.3155 - val_acc: 0.9110\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5137 - acc: 0.8402\n",
      "Epoch 00115: val_loss did not improve from 0.31554\n",
      "36805/36805 [==============================] - 17s 463us/sample - loss: 0.5138 - acc: 0.8402 - val_loss: 0.3189 - val_acc: 0.9087\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5101 - acc: 0.8446\n",
      "Epoch 00116: val_loss improved from 0.31554 to 0.31402, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_5_conv_checkpoint/116-0.3140.hdf5\n",
      "36805/36805 [==============================] - 17s 466us/sample - loss: 0.5101 - acc: 0.8446 - val_loss: 0.3140 - val_acc: 0.9080\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5101 - acc: 0.8437\n",
      "Epoch 00117: val_loss did not improve from 0.31402\n",
      "36805/36805 [==============================] - 17s 467us/sample - loss: 0.5101 - acc: 0.8437 - val_loss: 0.3141 - val_acc: 0.9080\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5032 - acc: 0.8438\n",
      "Epoch 00118: val_loss improved from 0.31402 to 0.30899, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_5_conv_checkpoint/118-0.3090.hdf5\n",
      "36805/36805 [==============================] - 17s 464us/sample - loss: 0.5032 - acc: 0.8438 - val_loss: 0.3090 - val_acc: 0.9101\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5044 - acc: 0.8437\n",
      "Epoch 00119: val_loss did not improve from 0.30899\n",
      "36805/36805 [==============================] - 17s 463us/sample - loss: 0.5045 - acc: 0.8437 - val_loss: 0.3140 - val_acc: 0.9106\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5096 - acc: 0.8431\n",
      "Epoch 00120: val_loss improved from 0.30899 to 0.30839, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_5_conv_checkpoint/120-0.3084.hdf5\n",
      "36805/36805 [==============================] - 17s 466us/sample - loss: 0.5096 - acc: 0.8431 - val_loss: 0.3084 - val_acc: 0.9124\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5026 - acc: 0.8457\n",
      "Epoch 00121: val_loss did not improve from 0.30839\n",
      "36805/36805 [==============================] - 17s 463us/sample - loss: 0.5026 - acc: 0.8456 - val_loss: 0.3175 - val_acc: 0.9108\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5022 - acc: 0.8446\n",
      "Epoch 00122: val_loss did not improve from 0.30839\n",
      "36805/36805 [==============================] - 17s 462us/sample - loss: 0.5022 - acc: 0.8446 - val_loss: 0.3101 - val_acc: 0.9078\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4951 - acc: 0.8451\n",
      "Epoch 00123: val_loss did not improve from 0.30839\n",
      "36805/36805 [==============================] - 17s 465us/sample - loss: 0.4953 - acc: 0.8451 - val_loss: 0.3178 - val_acc: 0.9073\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4964 - acc: 0.8457\n",
      "Epoch 00124: val_loss did not improve from 0.30839\n",
      "36805/36805 [==============================] - 17s 464us/sample - loss: 0.4966 - acc: 0.8456 - val_loss: 0.3207 - val_acc: 0.9059\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5022 - acc: 0.8445\n",
      "Epoch 00125: val_loss did not improve from 0.30839\n",
      "36805/36805 [==============================] - 17s 464us/sample - loss: 0.5022 - acc: 0.8445 - val_loss: 0.3116 - val_acc: 0.9068\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5067 - acc: 0.8432\n",
      "Epoch 00126: val_loss did not improve from 0.30839\n",
      "36805/36805 [==============================] - 17s 463us/sample - loss: 0.5067 - acc: 0.8432 - val_loss: 0.3167 - val_acc: 0.9101\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4938 - acc: 0.8473\n",
      "Epoch 00127: val_loss did not improve from 0.30839\n",
      "36805/36805 [==============================] - 17s 462us/sample - loss: 0.4937 - acc: 0.8473 - val_loss: 0.3163 - val_acc: 0.9073\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4994 - acc: 0.8480\n",
      "Epoch 00128: val_loss did not improve from 0.30839\n",
      "36805/36805 [==============================] - 17s 467us/sample - loss: 0.4994 - acc: 0.8480 - val_loss: 0.3141 - val_acc: 0.9094\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4947 - acc: 0.8459\n",
      "Epoch 00129: val_loss improved from 0.30839 to 0.30609, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_5_conv_checkpoint/129-0.3061.hdf5\n",
      "36805/36805 [==============================] - 17s 465us/sample - loss: 0.4946 - acc: 0.8459 - val_loss: 0.3061 - val_acc: 0.9117\n",
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4953 - acc: 0.8468\n",
      "Epoch 00130: val_loss improved from 0.30609 to 0.30539, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_5_conv_checkpoint/130-0.3054.hdf5\n",
      "36805/36805 [==============================] - 17s 465us/sample - loss: 0.4953 - acc: 0.8468 - val_loss: 0.3054 - val_acc: 0.9089\n",
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4898 - acc: 0.8507\n",
      "Epoch 00131: val_loss did not improve from 0.30539\n",
      "36805/36805 [==============================] - 17s 465us/sample - loss: 0.4898 - acc: 0.8507 - val_loss: 0.3075 - val_acc: 0.9092\n",
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4906 - acc: 0.8478\n",
      "Epoch 00132: val_loss improved from 0.30539 to 0.30297, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_5_conv_checkpoint/132-0.3030.hdf5\n",
      "36805/36805 [==============================] - 17s 467us/sample - loss: 0.4908 - acc: 0.8478 - val_loss: 0.3030 - val_acc: 0.9129\n",
      "Epoch 133/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4849 - acc: 0.8490\n",
      "Epoch 00133: val_loss did not improve from 0.30297\n",
      "36805/36805 [==============================] - 17s 465us/sample - loss: 0.4849 - acc: 0.8490 - val_loss: 0.3065 - val_acc: 0.9085\n",
      "Epoch 134/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4917 - acc: 0.8478\n",
      "Epoch 00134: val_loss did not improve from 0.30297\n",
      "36805/36805 [==============================] - 17s 462us/sample - loss: 0.4918 - acc: 0.8478 - val_loss: 0.3218 - val_acc: 0.9054\n",
      "Epoch 135/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4770 - acc: 0.8509\n",
      "Epoch 00135: val_loss did not improve from 0.30297\n",
      "36805/36805 [==============================] - 17s 466us/sample - loss: 0.4769 - acc: 0.8509 - val_loss: 0.3036 - val_acc: 0.9119\n",
      "Epoch 136/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4870 - acc: 0.8502\n",
      "Epoch 00136: val_loss improved from 0.30297 to 0.30252, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_5_conv_checkpoint/136-0.3025.hdf5\n",
      "36805/36805 [==============================] - 17s 466us/sample - loss: 0.4871 - acc: 0.8502 - val_loss: 0.3025 - val_acc: 0.9087\n",
      "Epoch 137/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4828 - acc: 0.8498\n",
      "Epoch 00137: val_loss improved from 0.30252 to 0.29747, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_5_conv_checkpoint/137-0.2975.hdf5\n",
      "36805/36805 [==============================] - 17s 466us/sample - loss: 0.4828 - acc: 0.8498 - val_loss: 0.2975 - val_acc: 0.9140\n",
      "Epoch 138/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4813 - acc: 0.8484\n",
      "Epoch 00138: val_loss did not improve from 0.29747\n",
      "36805/36805 [==============================] - 17s 463us/sample - loss: 0.4813 - acc: 0.8484 - val_loss: 0.3090 - val_acc: 0.9064\n",
      "Epoch 139/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4788 - acc: 0.8521\n",
      "Epoch 00139: val_loss did not improve from 0.29747\n",
      "36805/36805 [==============================] - 17s 465us/sample - loss: 0.4788 - acc: 0.8522 - val_loss: 0.3087 - val_acc: 0.9115\n",
      "Epoch 140/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4832 - acc: 0.8494\n",
      "Epoch 00140: val_loss did not improve from 0.29747\n",
      "36805/36805 [==============================] - 17s 465us/sample - loss: 0.4832 - acc: 0.8494 - val_loss: 0.3205 - val_acc: 0.9050\n",
      "Epoch 141/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4820 - acc: 0.8490\n",
      "Epoch 00141: val_loss did not improve from 0.29747\n",
      "36805/36805 [==============================] - 17s 459us/sample - loss: 0.4819 - acc: 0.8490 - val_loss: 0.3040 - val_acc: 0.9131\n",
      "Epoch 142/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4795 - acc: 0.8528\n",
      "Epoch 00142: val_loss did not improve from 0.29747\n",
      "36805/36805 [==============================] - 17s 463us/sample - loss: 0.4794 - acc: 0.8528 - val_loss: 0.3021 - val_acc: 0.9122\n",
      "Epoch 143/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4766 - acc: 0.8527\n",
      "Epoch 00143: val_loss did not improve from 0.29747\n",
      "36805/36805 [==============================] - 17s 465us/sample - loss: 0.4767 - acc: 0.8527 - val_loss: 0.2993 - val_acc: 0.9113\n",
      "Epoch 144/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4795 - acc: 0.8515\n",
      "Epoch 00144: val_loss did not improve from 0.29747\n",
      "36805/36805 [==============================] - 17s 467us/sample - loss: 0.4795 - acc: 0.8515 - val_loss: 0.3029 - val_acc: 0.9126\n",
      "Epoch 145/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4769 - acc: 0.8506\n",
      "Epoch 00145: val_loss did not improve from 0.29747\n",
      "36805/36805 [==============================] - 17s 462us/sample - loss: 0.4770 - acc: 0.8506 - val_loss: 0.2987 - val_acc: 0.9133\n",
      "Epoch 146/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4745 - acc: 0.8532\n",
      "Epoch 00146: val_loss did not improve from 0.29747\n",
      "36805/36805 [==============================] - 17s 463us/sample - loss: 0.4746 - acc: 0.8531 - val_loss: 0.3060 - val_acc: 0.9103\n",
      "Epoch 147/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4682 - acc: 0.8563\n",
      "Epoch 00147: val_loss did not improve from 0.29747\n",
      "36805/36805 [==============================] - 17s 462us/sample - loss: 0.4681 - acc: 0.8564 - val_loss: 0.3057 - val_acc: 0.9078\n",
      "Epoch 148/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4748 - acc: 0.8526\n",
      "Epoch 00148: val_loss did not improve from 0.29747\n",
      "36805/36805 [==============================] - 17s 465us/sample - loss: 0.4748 - acc: 0.8526 - val_loss: 0.2985 - val_acc: 0.9106\n",
      "Epoch 149/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4702 - acc: 0.8524\n",
      "Epoch 00149: val_loss did not improve from 0.29747\n",
      "36805/36805 [==============================] - 17s 465us/sample - loss: 0.4703 - acc: 0.8524 - val_loss: 0.3118 - val_acc: 0.9059\n",
      "Epoch 150/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4736 - acc: 0.8518\n",
      "Epoch 00150: val_loss improved from 0.29747 to 0.29508, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_5_conv_checkpoint/150-0.2951.hdf5\n",
      "36805/36805 [==============================] - 17s 463us/sample - loss: 0.4736 - acc: 0.8518 - val_loss: 0.2951 - val_acc: 0.9117\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 151/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4710 - acc: 0.8537\n",
      "Epoch 00151: val_loss did not improve from 0.29508\n",
      "36805/36805 [==============================] - 17s 463us/sample - loss: 0.4711 - acc: 0.8537 - val_loss: 0.2951 - val_acc: 0.9122\n",
      "Epoch 152/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4701 - acc: 0.8548\n",
      "Epoch 00152: val_loss did not improve from 0.29508\n",
      "36805/36805 [==============================] - 17s 463us/sample - loss: 0.4700 - acc: 0.8548 - val_loss: 0.3062 - val_acc: 0.9094\n",
      "Epoch 153/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4661 - acc: 0.8554\n",
      "Epoch 00153: val_loss did not improve from 0.29508\n",
      "36805/36805 [==============================] - 17s 462us/sample - loss: 0.4664 - acc: 0.8553 - val_loss: 0.3009 - val_acc: 0.9136\n",
      "Epoch 154/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4729 - acc: 0.8537\n",
      "Epoch 00154: val_loss did not improve from 0.29508\n",
      "36805/36805 [==============================] - 17s 465us/sample - loss: 0.4730 - acc: 0.8537 - val_loss: 0.3037 - val_acc: 0.9126\n",
      "Epoch 155/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4642 - acc: 0.8558\n",
      "Epoch 00155: val_loss did not improve from 0.29508\n",
      "36805/36805 [==============================] - 17s 465us/sample - loss: 0.4642 - acc: 0.8558 - val_loss: 0.3082 - val_acc: 0.9085\n",
      "Epoch 156/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4659 - acc: 0.8512\n",
      "Epoch 00156: val_loss did not improve from 0.29508\n",
      "36805/36805 [==============================] - 17s 463us/sample - loss: 0.4659 - acc: 0.8512 - val_loss: 0.3022 - val_acc: 0.9113\n",
      "Epoch 157/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4650 - acc: 0.8546\n",
      "Epoch 00157: val_loss improved from 0.29508 to 0.29382, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_5_conv_checkpoint/157-0.2938.hdf5\n",
      "36805/36805 [==============================] - 17s 463us/sample - loss: 0.4649 - acc: 0.8546 - val_loss: 0.2938 - val_acc: 0.9131\n",
      "Epoch 158/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4659 - acc: 0.8533\n",
      "Epoch 00158: val_loss improved from 0.29382 to 0.29352, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_5_conv_checkpoint/158-0.2935.hdf5\n",
      "36805/36805 [==============================] - 17s 465us/sample - loss: 0.4660 - acc: 0.8534 - val_loss: 0.2935 - val_acc: 0.9145\n",
      "Epoch 159/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4606 - acc: 0.8582\n",
      "Epoch 00159: val_loss did not improve from 0.29352\n",
      "36805/36805 [==============================] - 17s 464us/sample - loss: 0.4605 - acc: 0.8583 - val_loss: 0.2970 - val_acc: 0.9108\n",
      "Epoch 160/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4614 - acc: 0.8571\n",
      "Epoch 00160: val_loss did not improve from 0.29352\n",
      "36805/36805 [==============================] - 17s 462us/sample - loss: 0.4613 - acc: 0.8572 - val_loss: 0.2976 - val_acc: 0.9138\n",
      "Epoch 161/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4618 - acc: 0.8561\n",
      "Epoch 00161: val_loss did not improve from 0.29352\n",
      "36805/36805 [==============================] - 17s 464us/sample - loss: 0.4618 - acc: 0.8561 - val_loss: 0.2937 - val_acc: 0.9145\n",
      "Epoch 162/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4639 - acc: 0.8554\n",
      "Epoch 00162: val_loss did not improve from 0.29352\n",
      "36805/36805 [==============================] - 17s 462us/sample - loss: 0.4638 - acc: 0.8554 - val_loss: 0.2950 - val_acc: 0.9119\n",
      "Epoch 163/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4549 - acc: 0.8576\n",
      "Epoch 00163: val_loss improved from 0.29352 to 0.28799, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_5_conv_checkpoint/163-0.2880.hdf5\n",
      "36805/36805 [==============================] - 17s 460us/sample - loss: 0.4549 - acc: 0.8577 - val_loss: 0.2880 - val_acc: 0.9164\n",
      "Epoch 164/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4554 - acc: 0.8580\n",
      "Epoch 00164: val_loss did not improve from 0.28799\n",
      "36805/36805 [==============================] - 17s 459us/sample - loss: 0.4553 - acc: 0.8581 - val_loss: 0.2913 - val_acc: 0.9138\n",
      "Epoch 165/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4617 - acc: 0.8574\n",
      "Epoch 00165: val_loss did not improve from 0.28799\n",
      "36805/36805 [==============================] - 17s 457us/sample - loss: 0.4617 - acc: 0.8574 - val_loss: 0.2926 - val_acc: 0.9154\n",
      "Epoch 166/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4636 - acc: 0.8573\n",
      "Epoch 00166: val_loss did not improve from 0.28799\n",
      "36805/36805 [==============================] - 17s 457us/sample - loss: 0.4636 - acc: 0.8573 - val_loss: 0.2917 - val_acc: 0.9154\n",
      "Epoch 167/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4538 - acc: 0.8581\n",
      "Epoch 00167: val_loss did not improve from 0.28799\n",
      "36805/36805 [==============================] - 17s 464us/sample - loss: 0.4537 - acc: 0.8581 - val_loss: 0.2910 - val_acc: 0.9161\n",
      "Epoch 168/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4596 - acc: 0.8565\n",
      "Epoch 00168: val_loss did not improve from 0.28799\n",
      "36805/36805 [==============================] - 17s 457us/sample - loss: 0.4596 - acc: 0.8565 - val_loss: 0.2915 - val_acc: 0.9154\n",
      "Epoch 169/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4537 - acc: 0.8592\n",
      "Epoch 00169: val_loss did not improve from 0.28799\n",
      "36805/36805 [==============================] - 17s 462us/sample - loss: 0.4537 - acc: 0.8593 - val_loss: 0.2882 - val_acc: 0.9157\n",
      "Epoch 170/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4509 - acc: 0.8596\n",
      "Epoch 00170: val_loss did not improve from 0.28799\n",
      "36805/36805 [==============================] - 17s 466us/sample - loss: 0.4509 - acc: 0.8596 - val_loss: 0.2886 - val_acc: 0.9150\n",
      "Epoch 171/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4507 - acc: 0.8598\n",
      "Epoch 00171: val_loss improved from 0.28799 to 0.28328, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_5_conv_checkpoint/171-0.2833.hdf5\n",
      "36805/36805 [==============================] - 17s 465us/sample - loss: 0.4506 - acc: 0.8598 - val_loss: 0.2833 - val_acc: 0.9185\n",
      "Epoch 172/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4528 - acc: 0.8602\n",
      "Epoch 00172: val_loss did not improve from 0.28328\n",
      "36805/36805 [==============================] - 17s 465us/sample - loss: 0.4527 - acc: 0.8602 - val_loss: 0.2950 - val_acc: 0.9161\n",
      "Epoch 173/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4473 - acc: 0.8607\n",
      "Epoch 00173: val_loss did not improve from 0.28328\n",
      "36805/36805 [==============================] - 17s 462us/sample - loss: 0.4474 - acc: 0.8606 - val_loss: 0.2873 - val_acc: 0.9154\n",
      "Epoch 174/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4505 - acc: 0.8605\n",
      "Epoch 00174: val_loss improved from 0.28328 to 0.28229, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_5_conv_checkpoint/174-0.2823.hdf5\n",
      "36805/36805 [==============================] - 17s 464us/sample - loss: 0.4505 - acc: 0.8605 - val_loss: 0.2823 - val_acc: 0.9187\n",
      "Epoch 175/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4497 - acc: 0.8602\n",
      "Epoch 00175: val_loss did not improve from 0.28229\n",
      "36805/36805 [==============================] - 17s 461us/sample - loss: 0.4497 - acc: 0.8602 - val_loss: 0.2899 - val_acc: 0.9187\n",
      "Epoch 176/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4447 - acc: 0.8619\n",
      "Epoch 00176: val_loss did not improve from 0.28229\n",
      "36805/36805 [==============================] - 17s 460us/sample - loss: 0.4447 - acc: 0.8619 - val_loss: 0.2828 - val_acc: 0.9182\n",
      "Epoch 177/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4419 - acc: 0.8612\n",
      "Epoch 00177: val_loss did not improve from 0.28229\n",
      "36805/36805 [==============================] - 17s 466us/sample - loss: 0.4421 - acc: 0.8612 - val_loss: 0.2901 - val_acc: 0.9150\n",
      "Epoch 178/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4463 - acc: 0.8617\n",
      "Epoch 00178: val_loss did not improve from 0.28229\n",
      "36805/36805 [==============================] - 17s 462us/sample - loss: 0.4463 - acc: 0.8617 - val_loss: 0.2913 - val_acc: 0.9119\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 179/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4418 - acc: 0.8622\n",
      "Epoch 00179: val_loss did not improve from 0.28229\n",
      "36805/36805 [==============================] - 17s 462us/sample - loss: 0.4418 - acc: 0.8622 - val_loss: 0.2970 - val_acc: 0.9129\n",
      "Epoch 180/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4424 - acc: 0.8625\n",
      "Epoch 00180: val_loss did not improve from 0.28229\n",
      "36805/36805 [==============================] - 17s 465us/sample - loss: 0.4424 - acc: 0.8625 - val_loss: 0.2904 - val_acc: 0.9180\n",
      "Epoch 181/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4442 - acc: 0.8620\n",
      "Epoch 00181: val_loss did not improve from 0.28229\n",
      "36805/36805 [==============================] - 17s 466us/sample - loss: 0.4442 - acc: 0.8620 - val_loss: 0.3078 - val_acc: 0.9082\n",
      "Epoch 182/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4473 - acc: 0.8602\n",
      "Epoch 00182: val_loss did not improve from 0.28229\n",
      "36805/36805 [==============================] - 17s 462us/sample - loss: 0.4473 - acc: 0.8602 - val_loss: 0.2938 - val_acc: 0.9178\n",
      "Epoch 183/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4456 - acc: 0.8611\n",
      "Epoch 00183: val_loss did not improve from 0.28229\n",
      "36805/36805 [==============================] - 17s 460us/sample - loss: 0.4455 - acc: 0.8611 - val_loss: 0.2875 - val_acc: 0.9180\n",
      "Epoch 184/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4450 - acc: 0.8614\n",
      "Epoch 00184: val_loss did not improve from 0.28229\n",
      "36805/36805 [==============================] - 17s 465us/sample - loss: 0.4450 - acc: 0.8615 - val_loss: 0.2833 - val_acc: 0.9168\n",
      "Epoch 185/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4403 - acc: 0.8626\n",
      "Epoch 00185: val_loss did not improve from 0.28229\n",
      "36805/36805 [==============================] - 17s 463us/sample - loss: 0.4403 - acc: 0.8626 - val_loss: 0.2901 - val_acc: 0.9126\n",
      "Epoch 186/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4378 - acc: 0.8610\n",
      "Epoch 00186: val_loss did not improve from 0.28229\n",
      "36805/36805 [==============================] - 17s 459us/sample - loss: 0.4378 - acc: 0.8610 - val_loss: 0.2899 - val_acc: 0.9159\n",
      "Epoch 187/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4449 - acc: 0.8614\n",
      "Epoch 00187: val_loss did not improve from 0.28229\n",
      "36805/36805 [==============================] - 17s 459us/sample - loss: 0.4449 - acc: 0.8614 - val_loss: 0.2921 - val_acc: 0.9140\n",
      "Epoch 188/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4413 - acc: 0.8616\n",
      "Epoch 00188: val_loss did not improve from 0.28229\n",
      "36805/36805 [==============================] - 17s 461us/sample - loss: 0.4414 - acc: 0.8616 - val_loss: 0.2859 - val_acc: 0.9164\n",
      "Epoch 189/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4382 - acc: 0.8623\n",
      "Epoch 00189: val_loss did not improve from 0.28229\n",
      "36805/36805 [==============================] - 17s 463us/sample - loss: 0.4382 - acc: 0.8623 - val_loss: 0.2888 - val_acc: 0.9150\n",
      "Epoch 190/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4437 - acc: 0.8614\n",
      "Epoch 00190: val_loss improved from 0.28229 to 0.28208, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_5_conv_checkpoint/190-0.2821.hdf5\n",
      "36805/36805 [==============================] - 17s 466us/sample - loss: 0.4437 - acc: 0.8614 - val_loss: 0.2821 - val_acc: 0.9187\n",
      "Epoch 191/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4346 - acc: 0.8650\n",
      "Epoch 00191: val_loss did not improve from 0.28208\n",
      "36805/36805 [==============================] - 17s 463us/sample - loss: 0.4345 - acc: 0.8650 - val_loss: 0.2827 - val_acc: 0.9161\n",
      "Epoch 192/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4384 - acc: 0.8643\n",
      "Epoch 00192: val_loss did not improve from 0.28208\n",
      "36805/36805 [==============================] - 17s 464us/sample - loss: 0.4384 - acc: 0.8644 - val_loss: 0.3001 - val_acc: 0.9122\n",
      "Epoch 193/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4351 - acc: 0.8647\n",
      "Epoch 00193: val_loss improved from 0.28208 to 0.28161, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_5_conv_checkpoint/193-0.2816.hdf5\n",
      "36805/36805 [==============================] - 17s 465us/sample - loss: 0.4351 - acc: 0.8647 - val_loss: 0.2816 - val_acc: 0.9199\n",
      "Epoch 194/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4376 - acc: 0.8638\n",
      "Epoch 00194: val_loss did not improve from 0.28161\n",
      "36805/36805 [==============================] - 17s 463us/sample - loss: 0.4376 - acc: 0.8638 - val_loss: 0.2820 - val_acc: 0.9168\n",
      "Epoch 195/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4349 - acc: 0.8636\n",
      "Epoch 00195: val_loss did not improve from 0.28161\n",
      "36805/36805 [==============================] - 17s 466us/sample - loss: 0.4349 - acc: 0.8636 - val_loss: 0.2833 - val_acc: 0.9164\n",
      "Epoch 196/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4323 - acc: 0.8658\n",
      "Epoch 00196: val_loss did not improve from 0.28161\n",
      "36805/36805 [==============================] - 17s 463us/sample - loss: 0.4323 - acc: 0.8658 - val_loss: 0.2900 - val_acc: 0.9140\n",
      "Epoch 197/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4344 - acc: 0.8664\n",
      "Epoch 00197: val_loss improved from 0.28161 to 0.27810, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_5_conv_checkpoint/197-0.2781.hdf5\n",
      "36805/36805 [==============================] - 17s 468us/sample - loss: 0.4344 - acc: 0.8664 - val_loss: 0.2781 - val_acc: 0.9201\n",
      "Epoch 198/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4340 - acc: 0.8634\n",
      "Epoch 00198: val_loss did not improve from 0.27810\n",
      "36805/36805 [==============================] - 17s 466us/sample - loss: 0.4340 - acc: 0.8634 - val_loss: 0.2832 - val_acc: 0.9206\n",
      "Epoch 199/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4309 - acc: 0.8657\n",
      "Epoch 00199: val_loss did not improve from 0.27810\n",
      "36805/36805 [==============================] - 17s 468us/sample - loss: 0.4309 - acc: 0.8657 - val_loss: 0.2816 - val_acc: 0.9182\n",
      "Epoch 200/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4363 - acc: 0.8639\n",
      "Epoch 00200: val_loss did not improve from 0.27810\n",
      "36805/36805 [==============================] - 17s 463us/sample - loss: 0.4362 - acc: 0.8639 - val_loss: 0.2948 - val_acc: 0.9157\n",
      "Epoch 201/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4326 - acc: 0.8638\n",
      "Epoch 00201: val_loss did not improve from 0.27810\n",
      "36805/36805 [==============================] - 17s 462us/sample - loss: 0.4327 - acc: 0.8638 - val_loss: 0.2841 - val_acc: 0.9189\n",
      "Epoch 202/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4260 - acc: 0.8683\n",
      "Epoch 00202: val_loss did not improve from 0.27810\n",
      "36805/36805 [==============================] - 17s 462us/sample - loss: 0.4260 - acc: 0.8683 - val_loss: 0.2793 - val_acc: 0.9210\n",
      "Epoch 203/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4269 - acc: 0.8673\n",
      "Epoch 00203: val_loss did not improve from 0.27810\n",
      "36805/36805 [==============================] - 17s 461us/sample - loss: 0.4270 - acc: 0.8672 - val_loss: 0.2840 - val_acc: 0.9189\n",
      "Epoch 204/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4277 - acc: 0.8665\n",
      "Epoch 00204: val_loss did not improve from 0.27810\n",
      "36805/36805 [==============================] - 17s 462us/sample - loss: 0.4277 - acc: 0.8665 - val_loss: 0.2922 - val_acc: 0.9117\n",
      "Epoch 205/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4282 - acc: 0.8655\n",
      "Epoch 00205: val_loss did not improve from 0.27810\n",
      "36805/36805 [==============================] - 17s 467us/sample - loss: 0.4282 - acc: 0.8655 - val_loss: 0.2848 - val_acc: 0.9154\n",
      "Epoch 206/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4236 - acc: 0.8672\n",
      "Epoch 00206: val_loss did not improve from 0.27810\n",
      "36805/36805 [==============================] - 17s 464us/sample - loss: 0.4236 - acc: 0.8672 - val_loss: 0.2828 - val_acc: 0.9161\n",
      "Epoch 207/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4258 - acc: 0.8662\n",
      "Epoch 00207: val_loss did not improve from 0.27810\n",
      "36805/36805 [==============================] - 17s 463us/sample - loss: 0.4258 - acc: 0.8662 - val_loss: 0.2863 - val_acc: 0.9189\n",
      "Epoch 208/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4271 - acc: 0.8665\n",
      "Epoch 00208: val_loss did not improve from 0.27810\n",
      "36805/36805 [==============================] - 17s 463us/sample - loss: 0.4271 - acc: 0.8665 - val_loss: 0.2807 - val_acc: 0.9175\n",
      "Epoch 209/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4246 - acc: 0.8660\n",
      "Epoch 00209: val_loss did not improve from 0.27810\n",
      "36805/36805 [==============================] - 17s 465us/sample - loss: 0.4246 - acc: 0.8660 - val_loss: 0.2912 - val_acc: 0.9194\n",
      "Epoch 210/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4251 - acc: 0.8669\n",
      "Epoch 00210: val_loss did not improve from 0.27810\n",
      "36805/36805 [==============================] - 17s 464us/sample - loss: 0.4251 - acc: 0.8669 - val_loss: 0.2933 - val_acc: 0.9166\n",
      "Epoch 211/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4238 - acc: 0.8670\n",
      "Epoch 00211: val_loss did not improve from 0.27810\n",
      "36805/36805 [==============================] - 17s 463us/sample - loss: 0.4237 - acc: 0.8671 - val_loss: 0.2783 - val_acc: 0.9173\n",
      "Epoch 212/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4223 - acc: 0.8667\n",
      "Epoch 00212: val_loss did not improve from 0.27810\n",
      "36805/36805 [==============================] - 17s 464us/sample - loss: 0.4222 - acc: 0.8668 - val_loss: 0.2891 - val_acc: 0.9161\n",
      "Epoch 213/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4191 - acc: 0.8680\n",
      "Epoch 00213: val_loss did not improve from 0.27810\n",
      "36805/36805 [==============================] - 17s 467us/sample - loss: 0.4190 - acc: 0.8680 - val_loss: 0.2844 - val_acc: 0.9150\n",
      "Epoch 214/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4253 - acc: 0.8673\n",
      "Epoch 00214: val_loss did not improve from 0.27810\n",
      "36805/36805 [==============================] - 17s 465us/sample - loss: 0.4252 - acc: 0.8674 - val_loss: 0.2784 - val_acc: 0.9192\n",
      "Epoch 215/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4203 - acc: 0.8679\n",
      "Epoch 00215: val_loss did not improve from 0.27810\n",
      "36805/36805 [==============================] - 17s 465us/sample - loss: 0.4203 - acc: 0.8678 - val_loss: 0.2847 - val_acc: 0.9192\n",
      "Epoch 216/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4222 - acc: 0.8690\n",
      "Epoch 00216: val_loss improved from 0.27810 to 0.27743, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_5_conv_checkpoint/216-0.2774.hdf5\n",
      "36805/36805 [==============================] - 17s 467us/sample - loss: 0.4222 - acc: 0.8690 - val_loss: 0.2774 - val_acc: 0.9210\n",
      "Epoch 217/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4167 - acc: 0.8705\n",
      "Epoch 00217: val_loss did not improve from 0.27743\n",
      "36805/36805 [==============================] - 17s 462us/sample - loss: 0.4166 - acc: 0.8705 - val_loss: 0.2918 - val_acc: 0.9178\n",
      "Epoch 218/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4156 - acc: 0.8704\n",
      "Epoch 00218: val_loss did not improve from 0.27743\n",
      "36805/36805 [==============================] - 17s 461us/sample - loss: 0.4155 - acc: 0.8704 - val_loss: 0.2993 - val_acc: 0.9126\n",
      "Epoch 219/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4189 - acc: 0.8688\n",
      "Epoch 00219: val_loss did not improve from 0.27743\n",
      "36805/36805 [==============================] - 17s 465us/sample - loss: 0.4189 - acc: 0.8688 - val_loss: 0.2795 - val_acc: 0.9173\n",
      "Epoch 220/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4155 - acc: 0.8699\n",
      "Epoch 00220: val_loss did not improve from 0.27743\n",
      "36805/36805 [==============================] - 17s 463us/sample - loss: 0.4155 - acc: 0.8700 - val_loss: 0.2828 - val_acc: 0.9196\n",
      "Epoch 221/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4148 - acc: 0.8702\n",
      "Epoch 00221: val_loss improved from 0.27743 to 0.27607, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_5_conv_checkpoint/221-0.2761.hdf5\n",
      "36805/36805 [==============================] - 17s 465us/sample - loss: 0.4148 - acc: 0.8702 - val_loss: 0.2761 - val_acc: 0.9220\n",
      "Epoch 222/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4172 - acc: 0.8700\n",
      "Epoch 00222: val_loss did not improve from 0.27607\n",
      "36805/36805 [==============================] - 17s 459us/sample - loss: 0.4172 - acc: 0.8700 - val_loss: 0.2863 - val_acc: 0.9173\n",
      "Epoch 223/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4165 - acc: 0.8693\n",
      "Epoch 00223: val_loss did not improve from 0.27607\n",
      "36805/36805 [==============================] - 17s 454us/sample - loss: 0.4165 - acc: 0.8693 - val_loss: 0.2866 - val_acc: 0.9206\n",
      "Epoch 224/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4112 - acc: 0.8716\n",
      "Epoch 00224: val_loss did not improve from 0.27607\n",
      "36805/36805 [==============================] - 17s 455us/sample - loss: 0.4112 - acc: 0.8716 - val_loss: 0.2824 - val_acc: 0.9166\n",
      "Epoch 225/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4177 - acc: 0.8686\n",
      "Epoch 00225: val_loss did not improve from 0.27607\n",
      "36805/36805 [==============================] - 17s 458us/sample - loss: 0.4176 - acc: 0.8686 - val_loss: 0.2926 - val_acc: 0.9147\n",
      "Epoch 226/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4167 - acc: 0.8685\n",
      "Epoch 00226: val_loss did not improve from 0.27607\n",
      "36805/36805 [==============================] - 17s 456us/sample - loss: 0.4167 - acc: 0.8685 - val_loss: 0.2809 - val_acc: 0.9159\n",
      "Epoch 227/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4140 - acc: 0.8713\n",
      "Epoch 00227: val_loss did not improve from 0.27607\n",
      "36805/36805 [==============================] - 17s 454us/sample - loss: 0.4140 - acc: 0.8713 - val_loss: 0.2812 - val_acc: 0.9182\n",
      "Epoch 228/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4086 - acc: 0.8738\n",
      "Epoch 00228: val_loss did not improve from 0.27607\n",
      "36805/36805 [==============================] - 17s 457us/sample - loss: 0.4086 - acc: 0.8738 - val_loss: 0.2811 - val_acc: 0.9187\n",
      "Epoch 229/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4122 - acc: 0.8703\n",
      "Epoch 00229: val_loss did not improve from 0.27607\n",
      "36805/36805 [==============================] - 17s 459us/sample - loss: 0.4122 - acc: 0.8703 - val_loss: 0.2808 - val_acc: 0.9187\n",
      "Epoch 230/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4070 - acc: 0.8722\n",
      "Epoch 00230: val_loss did not improve from 0.27607\n",
      "36805/36805 [==============================] - 17s 455us/sample - loss: 0.4070 - acc: 0.8722 - val_loss: 0.2892 - val_acc: 0.9147\n",
      "Epoch 231/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4130 - acc: 0.8708\n",
      "Epoch 00231: val_loss did not improve from 0.27607\n",
      "36805/36805 [==============================] - 17s 452us/sample - loss: 0.4130 - acc: 0.8709 - val_loss: 0.2797 - val_acc: 0.9192\n",
      "Epoch 232/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4109 - acc: 0.8726\n",
      "Epoch 00232: val_loss did not improve from 0.27607\n",
      "36805/36805 [==============================] - 17s 451us/sample - loss: 0.4108 - acc: 0.8726 - val_loss: 0.2967 - val_acc: 0.9136\n",
      "Epoch 233/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4060 - acc: 0.8716\n",
      "Epoch 00233: val_loss did not improve from 0.27607\n",
      "36805/36805 [==============================] - 16s 446us/sample - loss: 0.4060 - acc: 0.8716 - val_loss: 0.2763 - val_acc: 0.9208\n",
      "Epoch 234/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4108 - acc: 0.8732\n",
      "Epoch 00234: val_loss did not improve from 0.27607\n",
      "36805/36805 [==============================] - 17s 454us/sample - loss: 0.4108 - acc: 0.8732 - val_loss: 0.2845 - val_acc: 0.9171\n",
      "Epoch 235/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4097 - acc: 0.8707\n",
      "Epoch 00235: val_loss improved from 0.27607 to 0.27436, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_5_conv_checkpoint/235-0.2744.hdf5\n",
      "36805/36805 [==============================] - 17s 458us/sample - loss: 0.4097 - acc: 0.8707 - val_loss: 0.2744 - val_acc: 0.9194\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 236/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4051 - acc: 0.8722\n",
      "Epoch 00236: val_loss did not improve from 0.27436\n",
      "36805/36805 [==============================] - 17s 458us/sample - loss: 0.4051 - acc: 0.8722 - val_loss: 0.2839 - val_acc: 0.9182\n",
      "Epoch 237/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4127 - acc: 0.8720\n",
      "Epoch 00237: val_loss did not improve from 0.27436\n",
      "36805/36805 [==============================] - 17s 459us/sample - loss: 0.4127 - acc: 0.8720 - val_loss: 0.2771 - val_acc: 0.9192\n",
      "Epoch 238/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4082 - acc: 0.8710\n",
      "Epoch 00238: val_loss did not improve from 0.27436\n",
      "36805/36805 [==============================] - 17s 458us/sample - loss: 0.4083 - acc: 0.8710 - val_loss: 0.2890 - val_acc: 0.9138\n",
      "Epoch 239/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4073 - acc: 0.8709\n",
      "Epoch 00239: val_loss did not improve from 0.27436\n",
      "36805/36805 [==============================] - 17s 455us/sample - loss: 0.4073 - acc: 0.8709 - val_loss: 0.2801 - val_acc: 0.9180\n",
      "Epoch 240/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4034 - acc: 0.8715\n",
      "Epoch 00240: val_loss did not improve from 0.27436\n",
      "36805/36805 [==============================] - 17s 456us/sample - loss: 0.4033 - acc: 0.8716 - val_loss: 0.2767 - val_acc: 0.9217\n",
      "Epoch 241/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4089 - acc: 0.8718\n",
      "Epoch 00241: val_loss did not improve from 0.27436\n",
      "36805/36805 [==============================] - 17s 453us/sample - loss: 0.4089 - acc: 0.8718 - val_loss: 0.2768 - val_acc: 0.9217\n",
      "Epoch 242/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4022 - acc: 0.8727\n",
      "Epoch 00242: val_loss did not improve from 0.27436\n",
      "36805/36805 [==============================] - 16s 447us/sample - loss: 0.4022 - acc: 0.8727 - val_loss: 0.2808 - val_acc: 0.9168\n",
      "Epoch 243/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4039 - acc: 0.8743\n",
      "Epoch 00243: val_loss did not improve from 0.27436\n",
      "36805/36805 [==============================] - 16s 446us/sample - loss: 0.4039 - acc: 0.8743 - val_loss: 0.2858 - val_acc: 0.9157\n",
      "Epoch 244/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4081 - acc: 0.8712\n",
      "Epoch 00244: val_loss did not improve from 0.27436\n",
      "36805/36805 [==============================] - 17s 452us/sample - loss: 0.4081 - acc: 0.8712 - val_loss: 0.2770 - val_acc: 0.9196\n",
      "Epoch 245/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3975 - acc: 0.8744\n",
      "Epoch 00245: val_loss did not improve from 0.27436\n",
      "36805/36805 [==============================] - 17s 455us/sample - loss: 0.3976 - acc: 0.8744 - val_loss: 0.2862 - val_acc: 0.9164\n",
      "Epoch 246/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4051 - acc: 0.8717\n",
      "Epoch 00246: val_loss did not improve from 0.27436\n",
      "36805/36805 [==============================] - 16s 444us/sample - loss: 0.4051 - acc: 0.8717 - val_loss: 0.2763 - val_acc: 0.9234\n",
      "Epoch 247/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4015 - acc: 0.8758\n",
      "Epoch 00247: val_loss did not improve from 0.27436\n",
      "36805/36805 [==============================] - 17s 451us/sample - loss: 0.4016 - acc: 0.8758 - val_loss: 0.2760 - val_acc: 0.9210\n",
      "Epoch 248/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4047 - acc: 0.8722\n",
      "Epoch 00248: val_loss did not improve from 0.27436\n",
      "36805/36805 [==============================] - 17s 458us/sample - loss: 0.4047 - acc: 0.8722 - val_loss: 0.2750 - val_acc: 0.9208\n",
      "Epoch 249/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3984 - acc: 0.8738\n",
      "Epoch 00249: val_loss did not improve from 0.27436\n",
      "36805/36805 [==============================] - 17s 459us/sample - loss: 0.3984 - acc: 0.8738 - val_loss: 0.2866 - val_acc: 0.9164\n",
      "Epoch 250/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4018 - acc: 0.8735\n",
      "Epoch 00250: val_loss did not improve from 0.27436\n",
      "36805/36805 [==============================] - 17s 459us/sample - loss: 0.4018 - acc: 0.8735 - val_loss: 0.2779 - val_acc: 0.9213\n",
      "Epoch 251/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3986 - acc: 0.8746\n",
      "Epoch 00251: val_loss did not improve from 0.27436\n",
      "36805/36805 [==============================] - 17s 454us/sample - loss: 0.3986 - acc: 0.8746 - val_loss: 0.2761 - val_acc: 0.9206\n",
      "Epoch 252/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4014 - acc: 0.8735\n",
      "Epoch 00252: val_loss did not improve from 0.27436\n",
      "36805/36805 [==============================] - 17s 456us/sample - loss: 0.4014 - acc: 0.8735 - val_loss: 0.2752 - val_acc: 0.9201\n",
      "Epoch 253/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3982 - acc: 0.8735\n",
      "Epoch 00253: val_loss did not improve from 0.27436\n",
      "36805/36805 [==============================] - 17s 457us/sample - loss: 0.3982 - acc: 0.8735 - val_loss: 0.2773 - val_acc: 0.9192\n",
      "Epoch 254/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3965 - acc: 0.8771\n",
      "Epoch 00254: val_loss did not improve from 0.27436\n",
      "36805/36805 [==============================] - 17s 457us/sample - loss: 0.3965 - acc: 0.8771 - val_loss: 0.2765 - val_acc: 0.9210\n",
      "Epoch 255/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4021 - acc: 0.8717\n",
      "Epoch 00255: val_loss did not improve from 0.27436\n",
      "36805/36805 [==============================] - 17s 456us/sample - loss: 0.4021 - acc: 0.8717 - val_loss: 0.2863 - val_acc: 0.9206\n",
      "Epoch 256/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3913 - acc: 0.8766\n",
      "Epoch 00256: val_loss did not improve from 0.27436\n",
      "36805/36805 [==============================] - 17s 455us/sample - loss: 0.3914 - acc: 0.8766 - val_loss: 0.2766 - val_acc: 0.9196\n",
      "Epoch 257/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3958 - acc: 0.8757\n",
      "Epoch 00257: val_loss did not improve from 0.27436\n",
      "36805/36805 [==============================] - 17s 458us/sample - loss: 0.3958 - acc: 0.8757 - val_loss: 0.2798 - val_acc: 0.9217\n",
      "Epoch 258/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3911 - acc: 0.8778\n",
      "Epoch 00258: val_loss did not improve from 0.27436\n",
      "36805/36805 [==============================] - 17s 464us/sample - loss: 0.3910 - acc: 0.8778 - val_loss: 0.2787 - val_acc: 0.9229\n",
      "Epoch 259/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3951 - acc: 0.8751\n",
      "Epoch 00259: val_loss did not improve from 0.27436\n",
      "36805/36805 [==============================] - 17s 463us/sample - loss: 0.3950 - acc: 0.8751 - val_loss: 0.2756 - val_acc: 0.9213\n",
      "Epoch 260/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3967 - acc: 0.8743\n",
      "Epoch 00260: val_loss did not improve from 0.27436\n",
      "36805/36805 [==============================] - 17s 460us/sample - loss: 0.3968 - acc: 0.8743 - val_loss: 0.2832 - val_acc: 0.9171\n",
      "Epoch 261/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3953 - acc: 0.8753\n",
      "Epoch 00261: val_loss did not improve from 0.27436\n",
      "36805/36805 [==============================] - 17s 458us/sample - loss: 0.3953 - acc: 0.8753 - val_loss: 0.2812 - val_acc: 0.9199\n",
      "Epoch 262/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3959 - acc: 0.8741\n",
      "Epoch 00262: val_loss did not improve from 0.27436\n",
      "36805/36805 [==============================] - 17s 457us/sample - loss: 0.3959 - acc: 0.8741 - val_loss: 0.2779 - val_acc: 0.9199\n",
      "Epoch 263/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3928 - acc: 0.8752\n",
      "Epoch 00263: val_loss improved from 0.27436 to 0.27423, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_5_conv_checkpoint/263-0.2742.hdf5\n",
      "36805/36805 [==============================] - 17s 466us/sample - loss: 0.3929 - acc: 0.8751 - val_loss: 0.2742 - val_acc: 0.9217\n",
      "Epoch 264/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3946 - acc: 0.8762\n",
      "Epoch 00264: val_loss improved from 0.27423 to 0.27389, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_5_conv_checkpoint/264-0.2739.hdf5\n",
      "36805/36805 [==============================] - 17s 464us/sample - loss: 0.3946 - acc: 0.8761 - val_loss: 0.2739 - val_acc: 0.9213\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 265/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3930 - acc: 0.8767\n",
      "Epoch 00265: val_loss improved from 0.27389 to 0.27172, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_5_conv_checkpoint/265-0.2717.hdf5\n",
      "36805/36805 [==============================] - 17s 466us/sample - loss: 0.3929 - acc: 0.8767 - val_loss: 0.2717 - val_acc: 0.9217\n",
      "Epoch 266/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3895 - acc: 0.8760\n",
      "Epoch 00266: val_loss did not improve from 0.27172\n",
      "36805/36805 [==============================] - 17s 466us/sample - loss: 0.3894 - acc: 0.8760 - val_loss: 0.2765 - val_acc: 0.9196\n",
      "Epoch 267/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3914 - acc: 0.8739\n",
      "Epoch 00267: val_loss did not improve from 0.27172\n",
      "36805/36805 [==============================] - 17s 464us/sample - loss: 0.3914 - acc: 0.8738 - val_loss: 0.2746 - val_acc: 0.9208\n",
      "Epoch 268/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3918 - acc: 0.8738\n",
      "Epoch 00268: val_loss did not improve from 0.27172\n",
      "36805/36805 [==============================] - 17s 465us/sample - loss: 0.3918 - acc: 0.8738 - val_loss: 0.2867 - val_acc: 0.9173\n",
      "Epoch 269/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3935 - acc: 0.8751\n",
      "Epoch 00269: val_loss did not improve from 0.27172\n",
      "36805/36805 [==============================] - 17s 463us/sample - loss: 0.3934 - acc: 0.8751 - val_loss: 0.2750 - val_acc: 0.9231\n",
      "Epoch 270/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3901 - acc: 0.8773\n",
      "Epoch 00270: val_loss did not improve from 0.27172\n",
      "36805/36805 [==============================] - 17s 460us/sample - loss: 0.3901 - acc: 0.8772 - val_loss: 0.2726 - val_acc: 0.9199\n",
      "Epoch 271/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3879 - acc: 0.8764\n",
      "Epoch 00271: val_loss did not improve from 0.27172\n",
      "36805/36805 [==============================] - 17s 468us/sample - loss: 0.3879 - acc: 0.8764 - val_loss: 0.2743 - val_acc: 0.9231\n",
      "Epoch 272/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3855 - acc: 0.8779\n",
      "Epoch 00272: val_loss did not improve from 0.27172\n",
      "36805/36805 [==============================] - 17s 464us/sample - loss: 0.3855 - acc: 0.8779 - val_loss: 0.2728 - val_acc: 0.9222\n",
      "Epoch 273/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3959 - acc: 0.8771\n",
      "Epoch 00273: val_loss did not improve from 0.27172\n",
      "36805/36805 [==============================] - 17s 462us/sample - loss: 0.3959 - acc: 0.8771 - val_loss: 0.2732 - val_acc: 0.9220\n",
      "Epoch 274/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3847 - acc: 0.8789\n",
      "Epoch 00274: val_loss did not improve from 0.27172\n",
      "36805/36805 [==============================] - 17s 464us/sample - loss: 0.3847 - acc: 0.8789 - val_loss: 0.2783 - val_acc: 0.9178\n",
      "Epoch 275/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3855 - acc: 0.8796\n",
      "Epoch 00275: val_loss did not improve from 0.27172\n",
      "36805/36805 [==============================] - 17s 462us/sample - loss: 0.3856 - acc: 0.8796 - val_loss: 0.2733 - val_acc: 0.9213\n",
      "Epoch 276/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3935 - acc: 0.8760\n",
      "Epoch 00276: val_loss did not improve from 0.27172\n",
      "36805/36805 [==============================] - 17s 462us/sample - loss: 0.3934 - acc: 0.8760 - val_loss: 0.2727 - val_acc: 0.9199\n",
      "Epoch 277/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3878 - acc: 0.8785\n",
      "Epoch 00277: val_loss did not improve from 0.27172\n",
      "36805/36805 [==============================] - 17s 460us/sample - loss: 0.3878 - acc: 0.8785 - val_loss: 0.2768 - val_acc: 0.9215\n",
      "Epoch 278/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3880 - acc: 0.8771\n",
      "Epoch 00278: val_loss did not improve from 0.27172\n",
      "36805/36805 [==============================] - 17s 463us/sample - loss: 0.3880 - acc: 0.8771 - val_loss: 0.2832 - val_acc: 0.9194\n",
      "Epoch 279/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3781 - acc: 0.8810\n",
      "Epoch 00279: val_loss did not improve from 0.27172\n",
      "36805/36805 [==============================] - 17s 461us/sample - loss: 0.3783 - acc: 0.8809 - val_loss: 0.2761 - val_acc: 0.9222\n",
      "Epoch 280/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3887 - acc: 0.8757\n",
      "Epoch 00280: val_loss did not improve from 0.27172\n",
      "36805/36805 [==============================] - 17s 464us/sample - loss: 0.3887 - acc: 0.8757 - val_loss: 0.2781 - val_acc: 0.9196\n",
      "Epoch 281/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3877 - acc: 0.8778\n",
      "Epoch 00281: val_loss did not improve from 0.27172\n",
      "36805/36805 [==============================] - 17s 462us/sample - loss: 0.3877 - acc: 0.8778 - val_loss: 0.2719 - val_acc: 0.9224\n",
      "Epoch 282/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3849 - acc: 0.8787\n",
      "Epoch 00282: val_loss did not improve from 0.27172\n",
      "36805/36805 [==============================] - 17s 462us/sample - loss: 0.3849 - acc: 0.8787 - val_loss: 0.2806 - val_acc: 0.9224\n",
      "Epoch 283/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3800 - acc: 0.8792\n",
      "Epoch 00283: val_loss did not improve from 0.27172\n",
      "36805/36805 [==============================] - 17s 461us/sample - loss: 0.3799 - acc: 0.8792 - val_loss: 0.2832 - val_acc: 0.9201\n",
      "Epoch 284/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3827 - acc: 0.8795\n",
      "Epoch 00284: val_loss did not improve from 0.27172\n",
      "36805/36805 [==============================] - 17s 462us/sample - loss: 0.3826 - acc: 0.8795 - val_loss: 0.2870 - val_acc: 0.9196\n",
      "Epoch 285/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3759 - acc: 0.8820\n",
      "Epoch 00285: val_loss did not improve from 0.27172\n",
      "36805/36805 [==============================] - 17s 464us/sample - loss: 0.3758 - acc: 0.8820 - val_loss: 0.2777 - val_acc: 0.9215\n",
      "Epoch 286/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3794 - acc: 0.8805\n",
      "Epoch 00286: val_loss did not improve from 0.27172\n",
      "36805/36805 [==============================] - 17s 466us/sample - loss: 0.3794 - acc: 0.8805 - val_loss: 0.2814 - val_acc: 0.9178\n",
      "Epoch 287/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3759 - acc: 0.8809\n",
      "Epoch 00287: val_loss did not improve from 0.27172\n",
      "36805/36805 [==============================] - 17s 462us/sample - loss: 0.3759 - acc: 0.8809 - val_loss: 0.2760 - val_acc: 0.9203\n",
      "Epoch 288/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3842 - acc: 0.8783\n",
      "Epoch 00288: val_loss did not improve from 0.27172\n",
      "36805/36805 [==============================] - 17s 463us/sample - loss: 0.3842 - acc: 0.8784 - val_loss: 0.2770 - val_acc: 0.9208\n",
      "Epoch 289/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3796 - acc: 0.8787\n",
      "Epoch 00289: val_loss did not improve from 0.27172\n",
      "36805/36805 [==============================] - 17s 463us/sample - loss: 0.3795 - acc: 0.8787 - val_loss: 0.2767 - val_acc: 0.9222\n",
      "Epoch 290/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3810 - acc: 0.8780\n",
      "Epoch 00290: val_loss did not improve from 0.27172\n",
      "36805/36805 [==============================] - 17s 463us/sample - loss: 0.3809 - acc: 0.8780 - val_loss: 0.2749 - val_acc: 0.9217\n",
      "Epoch 291/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3809 - acc: 0.8799\n",
      "Epoch 00291: val_loss did not improve from 0.27172\n",
      "36805/36805 [==============================] - 17s 464us/sample - loss: 0.3809 - acc: 0.8799 - val_loss: 0.2841 - val_acc: 0.9182\n",
      "Epoch 292/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3870 - acc: 0.8774\n",
      "Epoch 00292: val_loss did not improve from 0.27172\n",
      "36805/36805 [==============================] - 17s 467us/sample - loss: 0.3870 - acc: 0.8774 - val_loss: 0.2776 - val_acc: 0.9224\n",
      "Epoch 293/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3803 - acc: 0.8796\n",
      "Epoch 00293: val_loss did not improve from 0.27172\n",
      "36805/36805 [==============================] - 17s 464us/sample - loss: 0.3802 - acc: 0.8796 - val_loss: 0.2802 - val_acc: 0.9208\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 294/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3768 - acc: 0.8815\n",
      "Epoch 00294: val_loss did not improve from 0.27172\n",
      "36805/36805 [==============================] - 17s 465us/sample - loss: 0.3767 - acc: 0.8815 - val_loss: 0.2777 - val_acc: 0.9222\n",
      "Epoch 295/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3801 - acc: 0.8797\n",
      "Epoch 00295: val_loss did not improve from 0.27172\n",
      "36805/36805 [==============================] - 17s 466us/sample - loss: 0.3801 - acc: 0.8797 - val_loss: 0.2766 - val_acc: 0.9210\n",
      "Epoch 296/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3796 - acc: 0.8790\n",
      "Epoch 00296: val_loss did not improve from 0.27172\n",
      "36805/36805 [==============================] - 17s 464us/sample - loss: 0.3796 - acc: 0.8790 - val_loss: 0.2775 - val_acc: 0.9231\n",
      "Epoch 297/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3774 - acc: 0.8793\n",
      "Epoch 00297: val_loss did not improve from 0.27172\n",
      "36805/36805 [==============================] - 17s 465us/sample - loss: 0.3774 - acc: 0.8793 - val_loss: 0.2730 - val_acc: 0.9241\n",
      "Epoch 298/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3840 - acc: 0.8782\n",
      "Epoch 00298: val_loss did not improve from 0.27172\n",
      "36805/36805 [==============================] - 17s 465us/sample - loss: 0.3840 - acc: 0.8782 - val_loss: 0.2741 - val_acc: 0.9224\n",
      "Epoch 299/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3737 - acc: 0.8813\n",
      "Epoch 00299: val_loss did not improve from 0.27172\n",
      "36805/36805 [==============================] - 17s 466us/sample - loss: 0.3737 - acc: 0.8813 - val_loss: 0.2762 - val_acc: 0.9243\n",
      "Epoch 300/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3758 - acc: 0.8813\n",
      "Epoch 00300: val_loss did not improve from 0.27172\n",
      "36805/36805 [==============================] - 17s 461us/sample - loss: 0.3758 - acc: 0.8813 - val_loss: 0.2723 - val_acc: 0.9229\n",
      "Epoch 301/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3785 - acc: 0.8795\n",
      "Epoch 00301: val_loss did not improve from 0.27172\n",
      "36805/36805 [==============================] - 17s 463us/sample - loss: 0.3785 - acc: 0.8795 - val_loss: 0.2780 - val_acc: 0.9234\n",
      "Epoch 302/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3788 - acc: 0.8809\n",
      "Epoch 00302: val_loss did not improve from 0.27172\n",
      "36805/36805 [==============================] - 17s 463us/sample - loss: 0.3788 - acc: 0.8809 - val_loss: 0.2742 - val_acc: 0.9224\n",
      "Epoch 303/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3713 - acc: 0.8813\n",
      "Epoch 00303: val_loss did not improve from 0.27172\n",
      "36805/36805 [==============================] - 17s 464us/sample - loss: 0.3713 - acc: 0.8813 - val_loss: 0.2764 - val_acc: 0.9185\n",
      "Epoch 304/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3739 - acc: 0.8822\n",
      "Epoch 00304: val_loss did not improve from 0.27172\n",
      "36805/36805 [==============================] - 17s 465us/sample - loss: 0.3738 - acc: 0.8822 - val_loss: 0.2741 - val_acc: 0.9215\n",
      "Epoch 305/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3760 - acc: 0.8815\n",
      "Epoch 00305: val_loss did not improve from 0.27172\n",
      "36805/36805 [==============================] - 17s 463us/sample - loss: 0.3760 - acc: 0.8815 - val_loss: 0.2789 - val_acc: 0.9199\n",
      "Epoch 306/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3719 - acc: 0.8826\n",
      "Epoch 00306: val_loss did not improve from 0.27172\n",
      "36805/36805 [==============================] - 17s 465us/sample - loss: 0.3718 - acc: 0.8826 - val_loss: 0.2732 - val_acc: 0.9213\n",
      "Epoch 307/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3686 - acc: 0.8828\n",
      "Epoch 00307: val_loss did not improve from 0.27172\n",
      "36805/36805 [==============================] - 17s 462us/sample - loss: 0.3687 - acc: 0.8828 - val_loss: 0.2746 - val_acc: 0.9217\n",
      "Epoch 308/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3725 - acc: 0.8811\n",
      "Epoch 00308: val_loss did not improve from 0.27172\n",
      "36805/36805 [==============================] - 17s 465us/sample - loss: 0.3725 - acc: 0.8811 - val_loss: 0.2746 - val_acc: 0.9210\n",
      "Epoch 309/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3705 - acc: 0.8812\n",
      "Epoch 00309: val_loss did not improve from 0.27172\n",
      "36805/36805 [==============================] - 17s 468us/sample - loss: 0.3705 - acc: 0.8812 - val_loss: 0.2720 - val_acc: 0.9238\n",
      "Epoch 310/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3759 - acc: 0.8815\n",
      "Epoch 00310: val_loss improved from 0.27172 to 0.26929, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_5_conv_checkpoint/310-0.2693.hdf5\n",
      "36805/36805 [==============================] - 17s 465us/sample - loss: 0.3758 - acc: 0.8815 - val_loss: 0.2693 - val_acc: 0.9229\n",
      "Epoch 311/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3727 - acc: 0.8829\n",
      "Epoch 00311: val_loss did not improve from 0.26929\n",
      "36805/36805 [==============================] - 17s 460us/sample - loss: 0.3727 - acc: 0.8829 - val_loss: 0.2696 - val_acc: 0.9231\n",
      "Epoch 312/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3719 - acc: 0.8815\n",
      "Epoch 00312: val_loss did not improve from 0.26929\n",
      "36805/36805 [==============================] - 17s 465us/sample - loss: 0.3719 - acc: 0.8815 - val_loss: 0.2775 - val_acc: 0.9210\n",
      "Epoch 313/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3713 - acc: 0.8820\n",
      "Epoch 00313: val_loss did not improve from 0.26929\n",
      "36805/36805 [==============================] - 17s 466us/sample - loss: 0.3712 - acc: 0.8820 - val_loss: 0.2773 - val_acc: 0.9241\n",
      "Epoch 314/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3667 - acc: 0.8831\n",
      "Epoch 00314: val_loss did not improve from 0.26929\n",
      "36805/36805 [==============================] - 17s 464us/sample - loss: 0.3667 - acc: 0.8831 - val_loss: 0.2820 - val_acc: 0.9203\n",
      "Epoch 315/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3697 - acc: 0.8804\n",
      "Epoch 00315: val_loss did not improve from 0.26929\n",
      "36805/36805 [==============================] - 17s 465us/sample - loss: 0.3697 - acc: 0.8804 - val_loss: 0.2827 - val_acc: 0.9185\n",
      "Epoch 316/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3668 - acc: 0.8828\n",
      "Epoch 00316: val_loss did not improve from 0.26929\n",
      "36805/36805 [==============================] - 17s 463us/sample - loss: 0.3669 - acc: 0.8828 - val_loss: 0.2738 - val_acc: 0.9234\n",
      "Epoch 317/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3653 - acc: 0.8839\n",
      "Epoch 00317: val_loss did not improve from 0.26929\n",
      "36805/36805 [==============================] - 17s 463us/sample - loss: 0.3653 - acc: 0.8839 - val_loss: 0.2754 - val_acc: 0.9215\n",
      "Epoch 318/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3645 - acc: 0.8849\n",
      "Epoch 00318: val_loss did not improve from 0.26929\n",
      "36805/36805 [==============================] - 17s 460us/sample - loss: 0.3645 - acc: 0.8850 - val_loss: 0.2849 - val_acc: 0.9192\n",
      "Epoch 319/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3656 - acc: 0.8847\n",
      "Epoch 00319: val_loss improved from 0.26929 to 0.26812, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_5_conv_checkpoint/319-0.2681.hdf5\n",
      "36805/36805 [==============================] - 17s 464us/sample - loss: 0.3655 - acc: 0.8847 - val_loss: 0.2681 - val_acc: 0.9224\n",
      "Epoch 320/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3662 - acc: 0.8827\n",
      "Epoch 00320: val_loss did not improve from 0.26812\n",
      "36805/36805 [==============================] - 17s 463us/sample - loss: 0.3662 - acc: 0.8828 - val_loss: 0.2899 - val_acc: 0.9185\n",
      "Epoch 321/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3727 - acc: 0.8820\n",
      "Epoch 00321: val_loss did not improve from 0.26812\n",
      "36805/36805 [==============================] - 17s 461us/sample - loss: 0.3727 - acc: 0.8819 - val_loss: 0.2721 - val_acc: 0.9208\n",
      "Epoch 322/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3680 - acc: 0.8821\n",
      "Epoch 00322: val_loss did not improve from 0.26812\n",
      "36805/36805 [==============================] - 17s 461us/sample - loss: 0.3680 - acc: 0.8821 - val_loss: 0.2774 - val_acc: 0.9222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 323/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3620 - acc: 0.8835\n",
      "Epoch 00323: val_loss did not improve from 0.26812\n",
      "36805/36805 [==============================] - 17s 461us/sample - loss: 0.3620 - acc: 0.8835 - val_loss: 0.2761 - val_acc: 0.9231\n",
      "Epoch 324/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3682 - acc: 0.8835\n",
      "Epoch 00324: val_loss did not improve from 0.26812\n",
      "36805/36805 [==============================] - 17s 461us/sample - loss: 0.3682 - acc: 0.8835 - val_loss: 0.2738 - val_acc: 0.9203\n",
      "Epoch 325/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3680 - acc: 0.8827\n",
      "Epoch 00325: val_loss did not improve from 0.26812\n",
      "36805/36805 [==============================] - 17s 458us/sample - loss: 0.3681 - acc: 0.8827 - val_loss: 0.2756 - val_acc: 0.9224\n",
      "Epoch 326/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3649 - acc: 0.8840\n",
      "Epoch 00326: val_loss did not improve from 0.26812\n",
      "36805/36805 [==============================] - 17s 459us/sample - loss: 0.3651 - acc: 0.8839 - val_loss: 0.2745 - val_acc: 0.9224\n",
      "Epoch 327/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3681 - acc: 0.8823\n",
      "Epoch 00327: val_loss did not improve from 0.26812\n",
      "36805/36805 [==============================] - 17s 464us/sample - loss: 0.3681 - acc: 0.8823 - val_loss: 0.2868 - val_acc: 0.9189\n",
      "Epoch 328/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3651 - acc: 0.8831\n",
      "Epoch 00328: val_loss did not improve from 0.26812\n",
      "36805/36805 [==============================] - 17s 460us/sample - loss: 0.3651 - acc: 0.8831 - val_loss: 0.2711 - val_acc: 0.9262\n",
      "Epoch 329/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3651 - acc: 0.8830\n",
      "Epoch 00329: val_loss did not improve from 0.26812\n",
      "36805/36805 [==============================] - 17s 458us/sample - loss: 0.3652 - acc: 0.8830 - val_loss: 0.2725 - val_acc: 0.9229\n",
      "Epoch 330/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3630 - acc: 0.8845\n",
      "Epoch 00330: val_loss did not improve from 0.26812\n",
      "36805/36805 [==============================] - 17s 460us/sample - loss: 0.3630 - acc: 0.8845 - val_loss: 0.2747 - val_acc: 0.9215\n",
      "Epoch 331/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3683 - acc: 0.8851\n",
      "Epoch 00331: val_loss did not improve from 0.26812\n",
      "36805/36805 [==============================] - 17s 465us/sample - loss: 0.3683 - acc: 0.8851 - val_loss: 0.2723 - val_acc: 0.9245\n",
      "Epoch 332/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3629 - acc: 0.8866\n",
      "Epoch 00332: val_loss did not improve from 0.26812\n",
      "36805/36805 [==============================] - 17s 461us/sample - loss: 0.3628 - acc: 0.8866 - val_loss: 0.2819 - val_acc: 0.9229\n",
      "Epoch 333/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3623 - acc: 0.8835\n",
      "Epoch 00333: val_loss did not improve from 0.26812\n",
      "36805/36805 [==============================] - 17s 466us/sample - loss: 0.3623 - acc: 0.8835 - val_loss: 0.2738 - val_acc: 0.9234\n",
      "Epoch 334/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3631 - acc: 0.8833\n",
      "Epoch 00334: val_loss did not improve from 0.26812\n",
      "36805/36805 [==============================] - 17s 462us/sample - loss: 0.3631 - acc: 0.8833 - val_loss: 0.2721 - val_acc: 0.9238\n",
      "Epoch 335/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3636 - acc: 0.8843\n",
      "Epoch 00335: val_loss did not improve from 0.26812\n",
      "36805/36805 [==============================] - 17s 466us/sample - loss: 0.3636 - acc: 0.8843 - val_loss: 0.2767 - val_acc: 0.9220\n",
      "Epoch 336/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3645 - acc: 0.8851\n",
      "Epoch 00336: val_loss did not improve from 0.26812\n",
      "36805/36805 [==============================] - 17s 462us/sample - loss: 0.3645 - acc: 0.8851 - val_loss: 0.2718 - val_acc: 0.9252\n",
      "Epoch 337/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3626 - acc: 0.8853\n",
      "Epoch 00337: val_loss did not improve from 0.26812\n",
      "36805/36805 [==============================] - 17s 464us/sample - loss: 0.3625 - acc: 0.8853 - val_loss: 0.2732 - val_acc: 0.9222\n",
      "Epoch 338/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3578 - acc: 0.8878\n",
      "Epoch 00338: val_loss did not improve from 0.26812\n",
      "36805/36805 [==============================] - 17s 464us/sample - loss: 0.3580 - acc: 0.8877 - val_loss: 0.2718 - val_acc: 0.9220\n",
      "Epoch 339/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3557 - acc: 0.8860\n",
      "Epoch 00339: val_loss did not improve from 0.26812\n",
      "36805/36805 [==============================] - 17s 462us/sample - loss: 0.3557 - acc: 0.8860 - val_loss: 0.2728 - val_acc: 0.9266\n",
      "Epoch 340/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3652 - acc: 0.8831\n",
      "Epoch 00340: val_loss did not improve from 0.26812\n",
      "36805/36805 [==============================] - 17s 463us/sample - loss: 0.3652 - acc: 0.8831 - val_loss: 0.2763 - val_acc: 0.9229\n",
      "Epoch 341/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3667 - acc: 0.8836\n",
      "Epoch 00341: val_loss did not improve from 0.26812\n",
      "36805/36805 [==============================] - 17s 461us/sample - loss: 0.3667 - acc: 0.8836 - val_loss: 0.2735 - val_acc: 0.9250\n",
      "Epoch 342/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3600 - acc: 0.8859\n",
      "Epoch 00342: val_loss did not improve from 0.26812\n",
      "36805/36805 [==============================] - 17s 463us/sample - loss: 0.3599 - acc: 0.8859 - val_loss: 0.2726 - val_acc: 0.9215\n",
      "Epoch 343/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3544 - acc: 0.8873\n",
      "Epoch 00343: val_loss did not improve from 0.26812\n",
      "36805/36805 [==============================] - 17s 460us/sample - loss: 0.3544 - acc: 0.8872 - val_loss: 0.2818 - val_acc: 0.9217\n",
      "Epoch 344/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3590 - acc: 0.8845\n",
      "Epoch 00344: val_loss did not improve from 0.26812\n",
      "36805/36805 [==============================] - 17s 461us/sample - loss: 0.3590 - acc: 0.8845 - val_loss: 0.2735 - val_acc: 0.9236\n",
      "Epoch 345/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3593 - acc: 0.8854\n",
      "Epoch 00345: val_loss did not improve from 0.26812\n",
      "36805/36805 [==============================] - 17s 465us/sample - loss: 0.3593 - acc: 0.8854 - val_loss: 0.2814 - val_acc: 0.9206\n",
      "Epoch 346/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3539 - acc: 0.8860\n",
      "Epoch 00346: val_loss did not improve from 0.26812\n",
      "36805/36805 [==============================] - 17s 461us/sample - loss: 0.3539 - acc: 0.8860 - val_loss: 0.2754 - val_acc: 0.9241\n",
      "Epoch 347/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3597 - acc: 0.8848\n",
      "Epoch 00347: val_loss did not improve from 0.26812\n",
      "36805/36805 [==============================] - 17s 465us/sample - loss: 0.3597 - acc: 0.8849 - val_loss: 0.2798 - val_acc: 0.9210\n",
      "Epoch 348/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3559 - acc: 0.8863\n",
      "Epoch 00348: val_loss did not improve from 0.26812\n",
      "36805/36805 [==============================] - 17s 464us/sample - loss: 0.3559 - acc: 0.8863 - val_loss: 0.2799 - val_acc: 0.9213\n",
      "Epoch 349/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3592 - acc: 0.8876\n",
      "Epoch 00349: val_loss did not improve from 0.26812\n",
      "36805/36805 [==============================] - 17s 466us/sample - loss: 0.3592 - acc: 0.8876 - val_loss: 0.2776 - val_acc: 0.9220\n",
      "Epoch 350/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3555 - acc: 0.8866\n",
      "Epoch 00350: val_loss did not improve from 0.26812\n",
      "36805/36805 [==============================] - 17s 464us/sample - loss: 0.3555 - acc: 0.8866 - val_loss: 0.2849 - val_acc: 0.9194\n",
      "Epoch 351/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3514 - acc: 0.8890\n",
      "Epoch 00351: val_loss did not improve from 0.26812\n",
      "36805/36805 [==============================] - 17s 463us/sample - loss: 0.3514 - acc: 0.8890 - val_loss: 0.2768 - val_acc: 0.9215\n",
      "Epoch 352/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3553 - acc: 0.8847\n",
      "Epoch 00352: val_loss did not improve from 0.26812\n",
      "36805/36805 [==============================] - 17s 466us/sample - loss: 0.3553 - acc: 0.8847 - val_loss: 0.2737 - val_acc: 0.9231\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 353/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3544 - acc: 0.8876\n",
      "Epoch 00353: val_loss did not improve from 0.26812\n",
      "36805/36805 [==============================] - 17s 464us/sample - loss: 0.3543 - acc: 0.8876 - val_loss: 0.2736 - val_acc: 0.9236\n",
      "Epoch 354/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3542 - acc: 0.8863\n",
      "Epoch 00354: val_loss did not improve from 0.26812\n",
      "36805/36805 [==============================] - 17s 464us/sample - loss: 0.3542 - acc: 0.8863 - val_loss: 0.2772 - val_acc: 0.9227\n",
      "Epoch 355/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3556 - acc: 0.8864\n",
      "Epoch 00355: val_loss did not improve from 0.26812\n",
      "36805/36805 [==============================] - 17s 462us/sample - loss: 0.3556 - acc: 0.8863 - val_loss: 0.2805 - val_acc: 0.9224\n",
      "Epoch 356/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3508 - acc: 0.8869\n",
      "Epoch 00356: val_loss did not improve from 0.26812\n",
      "36805/36805 [==============================] - 17s 462us/sample - loss: 0.3508 - acc: 0.8869 - val_loss: 0.2861 - val_acc: 0.9208\n",
      "Epoch 357/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3501 - acc: 0.8874\n",
      "Epoch 00357: val_loss did not improve from 0.26812\n",
      "36805/36805 [==============================] - 17s 466us/sample - loss: 0.3501 - acc: 0.8874 - val_loss: 0.2819 - val_acc: 0.9206\n",
      "Epoch 358/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3526 - acc: 0.8857\n",
      "Epoch 00358: val_loss did not improve from 0.26812\n",
      "36805/36805 [==============================] - 17s 462us/sample - loss: 0.3526 - acc: 0.8857 - val_loss: 0.2797 - val_acc: 0.9178\n",
      "Epoch 359/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3581 - acc: 0.8860\n",
      "Epoch 00359: val_loss did not improve from 0.26812\n",
      "36805/36805 [==============================] - 17s 461us/sample - loss: 0.3581 - acc: 0.8860 - val_loss: 0.2785 - val_acc: 0.9199\n",
      "Epoch 360/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3538 - acc: 0.8876\n",
      "Epoch 00360: val_loss did not improve from 0.26812\n",
      "36805/36805 [==============================] - 17s 461us/sample - loss: 0.3538 - acc: 0.8876 - val_loss: 0.2711 - val_acc: 0.9266\n",
      "Epoch 361/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3497 - acc: 0.8893\n",
      "Epoch 00361: val_loss did not improve from 0.26812\n",
      "36805/36805 [==============================] - 17s 463us/sample - loss: 0.3498 - acc: 0.8893 - val_loss: 0.2742 - val_acc: 0.9217\n",
      "Epoch 362/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3544 - acc: 0.8865\n",
      "Epoch 00362: val_loss did not improve from 0.26812\n",
      "36805/36805 [==============================] - 17s 465us/sample - loss: 0.3543 - acc: 0.8865 - val_loss: 0.2754 - val_acc: 0.9234\n",
      "Epoch 363/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3555 - acc: 0.8868\n",
      "Epoch 00363: val_loss did not improve from 0.26812\n",
      "36805/36805 [==============================] - 17s 462us/sample - loss: 0.3555 - acc: 0.8868 - val_loss: 0.2757 - val_acc: 0.9199\n",
      "Epoch 364/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3518 - acc: 0.8865\n",
      "Epoch 00364: val_loss did not improve from 0.26812\n",
      "36805/36805 [==============================] - 17s 461us/sample - loss: 0.3518 - acc: 0.8865 - val_loss: 0.2766 - val_acc: 0.9241\n",
      "Epoch 365/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3524 - acc: 0.8866\n",
      "Epoch 00365: val_loss did not improve from 0.26812\n",
      "36805/36805 [==============================] - 17s 461us/sample - loss: 0.3524 - acc: 0.8866 - val_loss: 0.2797 - val_acc: 0.9199\n",
      "Epoch 366/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3501 - acc: 0.8888\n",
      "Epoch 00366: val_loss did not improve from 0.26812\n",
      "36805/36805 [==============================] - 17s 460us/sample - loss: 0.3501 - acc: 0.8888 - val_loss: 0.2724 - val_acc: 0.9227\n",
      "Epoch 367/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3491 - acc: 0.8893\n",
      "Epoch 00367: val_loss did not improve from 0.26812\n",
      "36805/36805 [==============================] - 17s 464us/sample - loss: 0.3491 - acc: 0.8893 - val_loss: 0.2749 - val_acc: 0.9229\n",
      "Epoch 368/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3517 - acc: 0.8874\n",
      "Epoch 00368: val_loss did not improve from 0.26812\n",
      "36805/36805 [==============================] - 17s 463us/sample - loss: 0.3517 - acc: 0.8874 - val_loss: 0.2756 - val_acc: 0.9213\n",
      "Epoch 369/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3446 - acc: 0.8915\n",
      "Epoch 00369: val_loss did not improve from 0.26812\n",
      "36805/36805 [==============================] - 17s 463us/sample - loss: 0.3447 - acc: 0.8915 - val_loss: 0.2736 - val_acc: 0.9229\n",
      "\n",
      "1D_CNN_custom_multi_2_GMP_ch_32_DO_5_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 1s 291us/sample - loss: 0.3142 - acc: 0.9055\n",
      "Loss: 0.31418762652673454 Accuracy: 0.90550363\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.5581 - acc: 0.1510\n",
      "Epoch 00001: val_loss improved from inf to 2.12097, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_6_conv_checkpoint/001-2.1210.hdf5\n",
      "36805/36805 [==============================] - 21s 562us/sample - loss: 2.5581 - acc: 0.1510 - val_loss: 2.1210 - val_acc: 0.3501\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.0947 - acc: 0.3052\n",
      "Epoch 00002: val_loss improved from 2.12097 to 1.70993, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_6_conv_checkpoint/002-1.7099.hdf5\n",
      "36805/36805 [==============================] - 17s 475us/sample - loss: 2.0946 - acc: 0.3051 - val_loss: 1.7099 - val_acc: 0.4915\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.8129 - acc: 0.4033\n",
      "Epoch 00003: val_loss improved from 1.70993 to 1.40215, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_6_conv_checkpoint/003-1.4022.hdf5\n",
      "36805/36805 [==============================] - 18s 477us/sample - loss: 1.8129 - acc: 0.4033 - val_loss: 1.4022 - val_acc: 0.5777\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5902 - acc: 0.4791\n",
      "Epoch 00004: val_loss improved from 1.40215 to 1.18691, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_6_conv_checkpoint/004-1.1869.hdf5\n",
      "36805/36805 [==============================] - 17s 474us/sample - loss: 1.5902 - acc: 0.4791 - val_loss: 1.1869 - val_acc: 0.6520\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4200 - acc: 0.5315\n",
      "Epoch 00005: val_loss improved from 1.18691 to 1.02162, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_6_conv_checkpoint/005-1.0216.hdf5\n",
      "36805/36805 [==============================] - 17s 475us/sample - loss: 1.4199 - acc: 0.5314 - val_loss: 1.0216 - val_acc: 0.6988\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2908 - acc: 0.5790\n",
      "Epoch 00006: val_loss improved from 1.02162 to 0.90522, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_6_conv_checkpoint/006-0.9052.hdf5\n",
      "36805/36805 [==============================] - 18s 480us/sample - loss: 1.2907 - acc: 0.5790 - val_loss: 0.9052 - val_acc: 0.7421\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1894 - acc: 0.6120\n",
      "Epoch 00007: val_loss improved from 0.90522 to 0.80693, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_6_conv_checkpoint/007-0.8069.hdf5\n",
      "36805/36805 [==============================] - 18s 483us/sample - loss: 1.1893 - acc: 0.6120 - val_loss: 0.8069 - val_acc: 0.7682\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0951 - acc: 0.6474\n",
      "Epoch 00008: val_loss improved from 0.80693 to 0.74960, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_6_conv_checkpoint/008-0.7496.hdf5\n",
      "36805/36805 [==============================] - 17s 472us/sample - loss: 1.0951 - acc: 0.6474 - val_loss: 0.7496 - val_acc: 0.7859\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0269 - acc: 0.6679\n",
      "Epoch 00009: val_loss improved from 0.74960 to 0.69344, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_6_conv_checkpoint/009-0.6934.hdf5\n",
      "36805/36805 [==============================] - 17s 474us/sample - loss: 1.0271 - acc: 0.6678 - val_loss: 0.6934 - val_acc: 0.8062\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9589 - acc: 0.6915\n",
      "Epoch 00010: val_loss improved from 0.69344 to 0.63599, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_6_conv_checkpoint/010-0.6360.hdf5\n",
      "36805/36805 [==============================] - 18s 478us/sample - loss: 0.9590 - acc: 0.6914 - val_loss: 0.6360 - val_acc: 0.8176\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9149 - acc: 0.7068\n",
      "Epoch 00011: val_loss improved from 0.63599 to 0.60413, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_6_conv_checkpoint/011-0.6041.hdf5\n",
      "36805/36805 [==============================] - 17s 470us/sample - loss: 0.9149 - acc: 0.7068 - val_loss: 0.6041 - val_acc: 0.8269\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8698 - acc: 0.7232\n",
      "Epoch 00012: val_loss improved from 0.60413 to 0.56297, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_6_conv_checkpoint/012-0.5630.hdf5\n",
      "36805/36805 [==============================] - 17s 471us/sample - loss: 0.8699 - acc: 0.7232 - val_loss: 0.5630 - val_acc: 0.8355\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8289 - acc: 0.7386\n",
      "Epoch 00013: val_loss improved from 0.56297 to 0.54208, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_6_conv_checkpoint/013-0.5421.hdf5\n",
      "36805/36805 [==============================] - 18s 478us/sample - loss: 0.8290 - acc: 0.7386 - val_loss: 0.5421 - val_acc: 0.8369\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7931 - acc: 0.7493\n",
      "Epoch 00014: val_loss improved from 0.54208 to 0.50727, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_6_conv_checkpoint/014-0.5073.hdf5\n",
      "36805/36805 [==============================] - 17s 474us/sample - loss: 0.7931 - acc: 0.7493 - val_loss: 0.5073 - val_acc: 0.8574\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7479 - acc: 0.7644\n",
      "Epoch 00015: val_loss improved from 0.50727 to 0.46545, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_6_conv_checkpoint/015-0.4654.hdf5\n",
      "36805/36805 [==============================] - 18s 476us/sample - loss: 0.7479 - acc: 0.7644 - val_loss: 0.4654 - val_acc: 0.8600\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7270 - acc: 0.7730\n",
      "Epoch 00016: val_loss improved from 0.46545 to 0.43410, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_6_conv_checkpoint/016-0.4341.hdf5\n",
      "36805/36805 [==============================] - 18s 475us/sample - loss: 0.7269 - acc: 0.7730 - val_loss: 0.4341 - val_acc: 0.8670\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6991 - acc: 0.7795\n",
      "Epoch 00017: val_loss did not improve from 0.43410\n",
      "36805/36805 [==============================] - 17s 474us/sample - loss: 0.6991 - acc: 0.7795 - val_loss: 0.4617 - val_acc: 0.8549\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6706 - acc: 0.7891\n",
      "Epoch 00018: val_loss improved from 0.43410 to 0.41482, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_6_conv_checkpoint/018-0.4148.hdf5\n",
      "36805/36805 [==============================] - 17s 474us/sample - loss: 0.6705 - acc: 0.7892 - val_loss: 0.4148 - val_acc: 0.8747\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6466 - acc: 0.7952\n",
      "Epoch 00019: val_loss improved from 0.41482 to 0.38771, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_6_conv_checkpoint/019-0.3877.hdf5\n",
      "36805/36805 [==============================] - 18s 476us/sample - loss: 0.6466 - acc: 0.7952 - val_loss: 0.3877 - val_acc: 0.8852\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6329 - acc: 0.8013\n",
      "Epoch 00020: val_loss improved from 0.38771 to 0.37282, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_6_conv_checkpoint/020-0.3728.hdf5\n",
      "36805/36805 [==============================] - 18s 481us/sample - loss: 0.6331 - acc: 0.8013 - val_loss: 0.3728 - val_acc: 0.8859\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6122 - acc: 0.8083\n",
      "Epoch 00021: val_loss did not improve from 0.37282\n",
      "36805/36805 [==============================] - 18s 476us/sample - loss: 0.6122 - acc: 0.8083 - val_loss: 0.3782 - val_acc: 0.8875\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5949 - acc: 0.8134\n",
      "Epoch 00022: val_loss improved from 0.37282 to 0.34647, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_6_conv_checkpoint/022-0.3465.hdf5\n",
      "36805/36805 [==============================] - 18s 481us/sample - loss: 0.5949 - acc: 0.8134 - val_loss: 0.3465 - val_acc: 0.8926\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5715 - acc: 0.8211\n",
      "Epoch 00023: val_loss did not improve from 0.34647\n",
      "36805/36805 [==============================] - 18s 479us/sample - loss: 0.5716 - acc: 0.8211 - val_loss: 0.3563 - val_acc: 0.8954\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5593 - acc: 0.8251\n",
      "Epoch 00024: val_loss improved from 0.34647 to 0.33163, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_6_conv_checkpoint/024-0.3316.hdf5\n",
      "36805/36805 [==============================] - 18s 483us/sample - loss: 0.5593 - acc: 0.8251 - val_loss: 0.3316 - val_acc: 0.8996\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5472 - acc: 0.8302\n",
      "Epoch 00025: val_loss improved from 0.33163 to 0.30975, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_6_conv_checkpoint/025-0.3097.hdf5\n",
      "36805/36805 [==============================] - 18s 479us/sample - loss: 0.5471 - acc: 0.8302 - val_loss: 0.3097 - val_acc: 0.9071\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5386 - acc: 0.8318\n",
      "Epoch 00026: val_loss did not improve from 0.30975\n",
      "36805/36805 [==============================] - 17s 475us/sample - loss: 0.5386 - acc: 0.8318 - val_loss: 0.3115 - val_acc: 0.9075\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5302 - acc: 0.8349\n",
      "Epoch 00027: val_loss improved from 0.30975 to 0.29615, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_6_conv_checkpoint/027-0.2962.hdf5\n",
      "36805/36805 [==============================] - 18s 481us/sample - loss: 0.5301 - acc: 0.8349 - val_loss: 0.2962 - val_acc: 0.9124\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5111 - acc: 0.8408\n",
      "Epoch 00028: val_loss improved from 0.29615 to 0.28920, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_6_conv_checkpoint/028-0.2892.hdf5\n",
      "36805/36805 [==============================] - 17s 474us/sample - loss: 0.5112 - acc: 0.8408 - val_loss: 0.2892 - val_acc: 0.9140\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5020 - acc: 0.8448\n",
      "Epoch 00029: val_loss improved from 0.28920 to 0.28445, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_6_conv_checkpoint/029-0.2845.hdf5\n",
      "36805/36805 [==============================] - 18s 477us/sample - loss: 0.5020 - acc: 0.8448 - val_loss: 0.2845 - val_acc: 0.9108\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4955 - acc: 0.8467\n",
      "Epoch 00030: val_loss improved from 0.28445 to 0.27625, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_6_conv_checkpoint/030-0.2763.hdf5\n",
      "36805/36805 [==============================] - 18s 482us/sample - loss: 0.4955 - acc: 0.8468 - val_loss: 0.2763 - val_acc: 0.9157\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4804 - acc: 0.8517\n",
      "Epoch 00031: val_loss improved from 0.27625 to 0.27470, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_6_conv_checkpoint/031-0.2747.hdf5\n",
      "36805/36805 [==============================] - 18s 477us/sample - loss: 0.4804 - acc: 0.8517 - val_loss: 0.2747 - val_acc: 0.9164\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4745 - acc: 0.8525\n",
      "Epoch 00032: val_loss improved from 0.27470 to 0.26887, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_6_conv_checkpoint/032-0.2689.hdf5\n",
      "36805/36805 [==============================] - 18s 481us/sample - loss: 0.4745 - acc: 0.8525 - val_loss: 0.2689 - val_acc: 0.9196\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4701 - acc: 0.8549\n",
      "Epoch 00033: val_loss did not improve from 0.26887\n",
      "36805/36805 [==============================] - 17s 474us/sample - loss: 0.4700 - acc: 0.8550 - val_loss: 0.2689 - val_acc: 0.9208\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4566 - acc: 0.8565\n",
      "Epoch 00034: val_loss improved from 0.26887 to 0.26093, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_6_conv_checkpoint/034-0.2609.hdf5\n",
      "36805/36805 [==============================] - 18s 483us/sample - loss: 0.4566 - acc: 0.8565 - val_loss: 0.2609 - val_acc: 0.9215\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4562 - acc: 0.8603\n",
      "Epoch 00035: val_loss improved from 0.26093 to 0.25955, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_6_conv_checkpoint/035-0.2596.hdf5\n",
      "36805/36805 [==============================] - 18s 476us/sample - loss: 0.4562 - acc: 0.8603 - val_loss: 0.2596 - val_acc: 0.9231\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4456 - acc: 0.8609\n",
      "Epoch 00036: val_loss did not improve from 0.25955\n",
      "36805/36805 [==============================] - 18s 476us/sample - loss: 0.4456 - acc: 0.8609 - val_loss: 0.2695 - val_acc: 0.9203\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4426 - acc: 0.8628\n",
      "Epoch 00037: val_loss improved from 0.25955 to 0.24453, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_6_conv_checkpoint/037-0.2445.hdf5\n",
      "36805/36805 [==============================] - 18s 485us/sample - loss: 0.4426 - acc: 0.8628 - val_loss: 0.2445 - val_acc: 0.9255\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4337 - acc: 0.8645\n",
      "Epoch 00038: val_loss did not improve from 0.24453\n",
      "36805/36805 [==============================] - 18s 476us/sample - loss: 0.4337 - acc: 0.8646 - val_loss: 0.2454 - val_acc: 0.9273\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4312 - acc: 0.8677\n",
      "Epoch 00039: val_loss improved from 0.24453 to 0.23961, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_6_conv_checkpoint/039-0.2396.hdf5\n",
      "36805/36805 [==============================] - 18s 480us/sample - loss: 0.4313 - acc: 0.8677 - val_loss: 0.2396 - val_acc: 0.9276\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4212 - acc: 0.8688\n",
      "Epoch 00040: val_loss did not improve from 0.23961\n",
      "36805/36805 [==============================] - 17s 475us/sample - loss: 0.4211 - acc: 0.8688 - val_loss: 0.2472 - val_acc: 0.9266\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4190 - acc: 0.8706\n",
      "Epoch 00041: val_loss did not improve from 0.23961\n",
      "36805/36805 [==============================] - 18s 477us/sample - loss: 0.4190 - acc: 0.8706 - val_loss: 0.2446 - val_acc: 0.9215\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4125 - acc: 0.8727\n",
      "Epoch 00042: val_loss improved from 0.23961 to 0.23175, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_6_conv_checkpoint/042-0.2317.hdf5\n",
      "36805/36805 [==============================] - 18s 476us/sample - loss: 0.4125 - acc: 0.8727 - val_loss: 0.2317 - val_acc: 0.9301\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4064 - acc: 0.8744\n",
      "Epoch 00043: val_loss did not improve from 0.23175\n",
      "36805/36805 [==============================] - 17s 473us/sample - loss: 0.4066 - acc: 0.8744 - val_loss: 0.2367 - val_acc: 0.9301\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4027 - acc: 0.8749\n",
      "Epoch 00044: val_loss did not improve from 0.23175\n",
      "36805/36805 [==============================] - 17s 475us/sample - loss: 0.4027 - acc: 0.8749 - val_loss: 0.2585 - val_acc: 0.9206\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3961 - acc: 0.8764\n",
      "Epoch 00045: val_loss improved from 0.23175 to 0.22324, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_6_conv_checkpoint/045-0.2232.hdf5\n",
      "36805/36805 [==============================] - 18s 485us/sample - loss: 0.3961 - acc: 0.8764 - val_loss: 0.2232 - val_acc: 0.9359\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3918 - acc: 0.8782\n",
      "Epoch 00046: val_loss did not improve from 0.22324\n",
      "36805/36805 [==============================] - 18s 476us/sample - loss: 0.3918 - acc: 0.8782 - val_loss: 0.2252 - val_acc: 0.9301\n",
      "Epoch 47/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3872 - acc: 0.8789\n",
      "Epoch 00047: val_loss improved from 0.22324 to 0.21823, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_6_conv_checkpoint/047-0.2182.hdf5\n",
      "36805/36805 [==============================] - 18s 476us/sample - loss: 0.3872 - acc: 0.8789 - val_loss: 0.2182 - val_acc: 0.9362\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3838 - acc: 0.8799\n",
      "Epoch 00048: val_loss improved from 0.21823 to 0.21485, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_6_conv_checkpoint/048-0.2148.hdf5\n",
      "36805/36805 [==============================] - 18s 482us/sample - loss: 0.3838 - acc: 0.8799 - val_loss: 0.2148 - val_acc: 0.9341\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3858 - acc: 0.8819\n",
      "Epoch 00049: val_loss did not improve from 0.21485\n",
      "36805/36805 [==============================] - 18s 476us/sample - loss: 0.3858 - acc: 0.8819 - val_loss: 0.2176 - val_acc: 0.9334\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3793 - acc: 0.8822\n",
      "Epoch 00050: val_loss improved from 0.21485 to 0.21040, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_6_conv_checkpoint/050-0.2104.hdf5\n",
      "36805/36805 [==============================] - 18s 477us/sample - loss: 0.3792 - acc: 0.8822 - val_loss: 0.2104 - val_acc: 0.9369\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3763 - acc: 0.8840\n",
      "Epoch 00051: val_loss did not improve from 0.21040\n",
      "36805/36805 [==============================] - 18s 478us/sample - loss: 0.3763 - acc: 0.8840 - val_loss: 0.2106 - val_acc: 0.9371\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3691 - acc: 0.8845\n",
      "Epoch 00052: val_loss did not improve from 0.21040\n",
      "36805/36805 [==============================] - 18s 477us/sample - loss: 0.3692 - acc: 0.8845 - val_loss: 0.2124 - val_acc: 0.9352\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3636 - acc: 0.8855\n",
      "Epoch 00053: val_loss improved from 0.21040 to 0.21028, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_6_conv_checkpoint/053-0.2103.hdf5\n",
      "36805/36805 [==============================] - 17s 474us/sample - loss: 0.3635 - acc: 0.8855 - val_loss: 0.2103 - val_acc: 0.9373\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3674 - acc: 0.8857\n",
      "Epoch 00054: val_loss improved from 0.21028 to 0.20158, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_6_conv_checkpoint/054-0.2016.hdf5\n",
      "36805/36805 [==============================] - 17s 471us/sample - loss: 0.3674 - acc: 0.8857 - val_loss: 0.2016 - val_acc: 0.9397\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3591 - acc: 0.8897\n",
      "Epoch 00055: val_loss improved from 0.20158 to 0.20091, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_6_conv_checkpoint/055-0.2009.hdf5\n",
      "36805/36805 [==============================] - 17s 465us/sample - loss: 0.3592 - acc: 0.8897 - val_loss: 0.2009 - val_acc: 0.9411\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3619 - acc: 0.8873\n",
      "Epoch 00056: val_loss did not improve from 0.20091\n",
      "36805/36805 [==============================] - 17s 465us/sample - loss: 0.3619 - acc: 0.8872 - val_loss: 0.2131 - val_acc: 0.9357\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3519 - acc: 0.8914\n",
      "Epoch 00057: val_loss did not improve from 0.20091\n",
      "36805/36805 [==============================] - 17s 462us/sample - loss: 0.3519 - acc: 0.8914 - val_loss: 0.2026 - val_acc: 0.9404\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3516 - acc: 0.8899\n",
      "Epoch 00058: val_loss improved from 0.20091 to 0.19765, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_6_conv_checkpoint/058-0.1976.hdf5\n",
      "36805/36805 [==============================] - 17s 459us/sample - loss: 0.3517 - acc: 0.8898 - val_loss: 0.1976 - val_acc: 0.9401\n",
      "Epoch 59/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3486 - acc: 0.8913\n",
      "Epoch 00059: val_loss improved from 0.19765 to 0.19478, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_6_conv_checkpoint/059-0.1948.hdf5\n",
      "36805/36805 [==============================] - 17s 454us/sample - loss: 0.3486 - acc: 0.8913 - val_loss: 0.1948 - val_acc: 0.9420\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3434 - acc: 0.8925\n",
      "Epoch 00060: val_loss improved from 0.19478 to 0.18798, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_6_conv_checkpoint/060-0.1880.hdf5\n",
      "36805/36805 [==============================] - 17s 459us/sample - loss: 0.3434 - acc: 0.8925 - val_loss: 0.1880 - val_acc: 0.9441\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3365 - acc: 0.8948\n",
      "Epoch 00061: val_loss did not improve from 0.18798\n",
      "36805/36805 [==============================] - 17s 466us/sample - loss: 0.3365 - acc: 0.8948 - val_loss: 0.2192 - val_acc: 0.9313\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3411 - acc: 0.8926\n",
      "Epoch 00062: val_loss did not improve from 0.18798\n",
      "36805/36805 [==============================] - 17s 470us/sample - loss: 0.3411 - acc: 0.8926 - val_loss: 0.1930 - val_acc: 0.9429\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3349 - acc: 0.8949\n",
      "Epoch 00063: val_loss did not improve from 0.18798\n",
      "36805/36805 [==============================] - 17s 472us/sample - loss: 0.3350 - acc: 0.8949 - val_loss: 0.1908 - val_acc: 0.9429\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3359 - acc: 0.8960\n",
      "Epoch 00064: val_loss improved from 0.18798 to 0.18790, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_6_conv_checkpoint/064-0.1879.hdf5\n",
      "36805/36805 [==============================] - 17s 472us/sample - loss: 0.3360 - acc: 0.8959 - val_loss: 0.1879 - val_acc: 0.9439\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3272 - acc: 0.8963\n",
      "Epoch 00065: val_loss did not improve from 0.18790\n",
      "36805/36805 [==============================] - 17s 468us/sample - loss: 0.3272 - acc: 0.8963 - val_loss: 0.1949 - val_acc: 0.9415\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3284 - acc: 0.8992\n",
      "Epoch 00066: val_loss improved from 0.18790 to 0.18409, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_6_conv_checkpoint/066-0.1841.hdf5\n",
      "36805/36805 [==============================] - 18s 476us/sample - loss: 0.3284 - acc: 0.8993 - val_loss: 0.1841 - val_acc: 0.9441\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3233 - acc: 0.8995\n",
      "Epoch 00067: val_loss did not improve from 0.18409\n",
      "36805/36805 [==============================] - 17s 467us/sample - loss: 0.3233 - acc: 0.8995 - val_loss: 0.1852 - val_acc: 0.9434\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3268 - acc: 0.8992\n",
      "Epoch 00068: val_loss improved from 0.18409 to 0.18390, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_6_conv_checkpoint/068-0.1839.hdf5\n",
      "36805/36805 [==============================] - 17s 472us/sample - loss: 0.3268 - acc: 0.8993 - val_loss: 0.1839 - val_acc: 0.9485\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3216 - acc: 0.8998\n",
      "Epoch 00069: val_loss did not improve from 0.18390\n",
      "36805/36805 [==============================] - 17s 457us/sample - loss: 0.3216 - acc: 0.8997 - val_loss: 0.1871 - val_acc: 0.9427\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3218 - acc: 0.9014\n",
      "Epoch 00070: val_loss did not improve from 0.18390\n",
      "36805/36805 [==============================] - 17s 466us/sample - loss: 0.3218 - acc: 0.9014 - val_loss: 0.1866 - val_acc: 0.9434\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3176 - acc: 0.9022\n",
      "Epoch 00071: val_loss improved from 0.18390 to 0.18282, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_6_conv_checkpoint/071-0.1828.hdf5\n",
      "36805/36805 [==============================] - 17s 471us/sample - loss: 0.3177 - acc: 0.9022 - val_loss: 0.1828 - val_acc: 0.9455\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3107 - acc: 0.9026\n",
      "Epoch 00072: val_loss improved from 0.18282 to 0.18258, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_6_conv_checkpoint/072-0.1826.hdf5\n",
      "36805/36805 [==============================] - 17s 460us/sample - loss: 0.3106 - acc: 0.9026 - val_loss: 0.1826 - val_acc: 0.9439\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3101 - acc: 0.9022\n",
      "Epoch 00073: val_loss improved from 0.18258 to 0.18075, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_6_conv_checkpoint/073-0.1807.hdf5\n",
      "36805/36805 [==============================] - 17s 462us/sample - loss: 0.3101 - acc: 0.9022 - val_loss: 0.1807 - val_acc: 0.9469\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3120 - acc: 0.9017\n",
      "Epoch 00074: val_loss improved from 0.18075 to 0.17529, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_6_conv_checkpoint/074-0.1753.hdf5\n",
      "36805/36805 [==============================] - 17s 472us/sample - loss: 0.3121 - acc: 0.9017 - val_loss: 0.1753 - val_acc: 0.9495\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3068 - acc: 0.9042\n",
      "Epoch 00075: val_loss did not improve from 0.17529\n",
      "36805/36805 [==============================] - 17s 471us/sample - loss: 0.3068 - acc: 0.9042 - val_loss: 0.1793 - val_acc: 0.9474\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3051 - acc: 0.9037\n",
      "Epoch 00076: val_loss improved from 0.17529 to 0.17371, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_6_conv_checkpoint/076-0.1737.hdf5\n",
      "36805/36805 [==============================] - 17s 473us/sample - loss: 0.3052 - acc: 0.9037 - val_loss: 0.1737 - val_acc: 0.9490\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3061 - acc: 0.9056\n",
      "Epoch 00077: val_loss did not improve from 0.17371\n",
      "36805/36805 [==============================] - 17s 463us/sample - loss: 0.3061 - acc: 0.9056 - val_loss: 0.1790 - val_acc: 0.9478\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3033 - acc: 0.9041\n",
      "Epoch 00078: val_loss improved from 0.17371 to 0.17263, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_6_conv_checkpoint/078-0.1726.hdf5\n",
      "36805/36805 [==============================] - 17s 472us/sample - loss: 0.3033 - acc: 0.9041 - val_loss: 0.1726 - val_acc: 0.9499\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2989 - acc: 0.9043\n",
      "Epoch 00079: val_loss did not improve from 0.17263\n",
      "36805/36805 [==============================] - 17s 474us/sample - loss: 0.2989 - acc: 0.9043 - val_loss: 0.1782 - val_acc: 0.9434\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2946 - acc: 0.9076\n",
      "Epoch 00080: val_loss did not improve from 0.17263\n",
      "36805/36805 [==============================] - 17s 470us/sample - loss: 0.2946 - acc: 0.9076 - val_loss: 0.1851 - val_acc: 0.9413\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2957 - acc: 0.9085\n",
      "Epoch 00081: val_loss did not improve from 0.17263\n",
      "36805/36805 [==============================] - 17s 469us/sample - loss: 0.2957 - acc: 0.9085 - val_loss: 0.1797 - val_acc: 0.9471\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2920 - acc: 0.9085\n",
      "Epoch 00082: val_loss did not improve from 0.17263\n",
      "36805/36805 [==============================] - 17s 465us/sample - loss: 0.2919 - acc: 0.9085 - val_loss: 0.1736 - val_acc: 0.9471\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2939 - acc: 0.9062\n",
      "Epoch 00083: val_loss improved from 0.17263 to 0.16834, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_6_conv_checkpoint/083-0.1683.hdf5\n",
      "36805/36805 [==============================] - 18s 481us/sample - loss: 0.2939 - acc: 0.9062 - val_loss: 0.1683 - val_acc: 0.9506\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2870 - acc: 0.9096\n",
      "Epoch 00084: val_loss did not improve from 0.16834\n",
      "36805/36805 [==============================] - 17s 475us/sample - loss: 0.2870 - acc: 0.9096 - val_loss: 0.1703 - val_acc: 0.9469\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2914 - acc: 0.9090\n",
      "Epoch 00085: val_loss improved from 0.16834 to 0.16522, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_6_conv_checkpoint/085-0.1652.hdf5\n",
      "36805/36805 [==============================] - 18s 478us/sample - loss: 0.2914 - acc: 0.9090 - val_loss: 0.1652 - val_acc: 0.9497\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2855 - acc: 0.9106\n",
      "Epoch 00086: val_loss improved from 0.16522 to 0.16383, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_6_conv_checkpoint/086-0.1638.hdf5\n",
      "36805/36805 [==============================] - 18s 476us/sample - loss: 0.2855 - acc: 0.9106 - val_loss: 0.1638 - val_acc: 0.9490\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2859 - acc: 0.9095\n",
      "Epoch 00087: val_loss did not improve from 0.16383\n",
      "36805/36805 [==============================] - 18s 480us/sample - loss: 0.2859 - acc: 0.9095 - val_loss: 0.1791 - val_acc: 0.9467\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2862 - acc: 0.9086\n",
      "Epoch 00088: val_loss did not improve from 0.16383\n",
      "36805/36805 [==============================] - 18s 479us/sample - loss: 0.2862 - acc: 0.9086 - val_loss: 0.1664 - val_acc: 0.9483\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2792 - acc: 0.9134\n",
      "Epoch 00089: val_loss did not improve from 0.16383\n",
      "36805/36805 [==============================] - 18s 480us/sample - loss: 0.2793 - acc: 0.9134 - val_loss: 0.1743 - val_acc: 0.9441\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2866 - acc: 0.9113\n",
      "Epoch 00090: val_loss did not improve from 0.16383\n",
      "36805/36805 [==============================] - 18s 475us/sample - loss: 0.2866 - acc: 0.9113 - val_loss: 0.1679 - val_acc: 0.9502\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2779 - acc: 0.9121\n",
      "Epoch 00091: val_loss improved from 0.16383 to 0.16376, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_6_conv_checkpoint/091-0.1638.hdf5\n",
      "36805/36805 [==============================] - 18s 483us/sample - loss: 0.2780 - acc: 0.9121 - val_loss: 0.1638 - val_acc: 0.9515\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2809 - acc: 0.9122\n",
      "Epoch 00092: val_loss did not improve from 0.16376\n",
      "36805/36805 [==============================] - 17s 472us/sample - loss: 0.2809 - acc: 0.9122 - val_loss: 0.1738 - val_acc: 0.9453\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2781 - acc: 0.9131\n",
      "Epoch 00093: val_loss did not improve from 0.16376\n",
      "36805/36805 [==============================] - 17s 473us/sample - loss: 0.2781 - acc: 0.9131 - val_loss: 0.1648 - val_acc: 0.9504\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2802 - acc: 0.9119\n",
      "Epoch 00094: val_loss improved from 0.16376 to 0.16348, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_6_conv_checkpoint/094-0.1635.hdf5\n",
      "36805/36805 [==============================] - 18s 480us/sample - loss: 0.2802 - acc: 0.9118 - val_loss: 0.1635 - val_acc: 0.9488\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2768 - acc: 0.9123\n",
      "Epoch 00095: val_loss improved from 0.16348 to 0.16161, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_6_conv_checkpoint/095-0.1616.hdf5\n",
      "36805/36805 [==============================] - 18s 478us/sample - loss: 0.2767 - acc: 0.9123 - val_loss: 0.1616 - val_acc: 0.9518\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2730 - acc: 0.9136\n",
      "Epoch 00096: val_loss improved from 0.16161 to 0.16149, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_6_conv_checkpoint/096-0.1615.hdf5\n",
      "36805/36805 [==============================] - 18s 483us/sample - loss: 0.2730 - acc: 0.9135 - val_loss: 0.1615 - val_acc: 0.9509\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2734 - acc: 0.9146\n",
      "Epoch 00097: val_loss improved from 0.16149 to 0.16081, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_6_conv_checkpoint/097-0.1608.hdf5\n",
      "36805/36805 [==============================] - 18s 481us/sample - loss: 0.2735 - acc: 0.9146 - val_loss: 0.1608 - val_acc: 0.9509\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2693 - acc: 0.9158\n",
      "Epoch 00098: val_loss did not improve from 0.16081\n",
      "36805/36805 [==============================] - 17s 475us/sample - loss: 0.2693 - acc: 0.9158 - val_loss: 0.1610 - val_acc: 0.9497\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2725 - acc: 0.9138\n",
      "Epoch 00099: val_loss did not improve from 0.16081\n",
      "36805/36805 [==============================] - 17s 475us/sample - loss: 0.2725 - acc: 0.9138 - val_loss: 0.1708 - val_acc: 0.9481\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2714 - acc: 0.9156\n",
      "Epoch 00100: val_loss did not improve from 0.16081\n",
      "36805/36805 [==============================] - 18s 479us/sample - loss: 0.2714 - acc: 0.9156 - val_loss: 0.1621 - val_acc: 0.9513\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2629 - acc: 0.9170\n",
      "Epoch 00101: val_loss improved from 0.16081 to 0.15909, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_6_conv_checkpoint/101-0.1591.hdf5\n",
      "36805/36805 [==============================] - 18s 479us/sample - loss: 0.2630 - acc: 0.9169 - val_loss: 0.1591 - val_acc: 0.9509\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2698 - acc: 0.9154\n",
      "Epoch 00102: val_loss did not improve from 0.15909\n",
      "36805/36805 [==============================] - 18s 479us/sample - loss: 0.2698 - acc: 0.9154 - val_loss: 0.1765 - val_acc: 0.9439\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2566 - acc: 0.9185\n",
      "Epoch 00103: val_loss did not improve from 0.15909\n",
      "36805/36805 [==============================] - 18s 480us/sample - loss: 0.2569 - acc: 0.9184 - val_loss: 0.1617 - val_acc: 0.9488\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2635 - acc: 0.9154\n",
      "Epoch 00104: val_loss did not improve from 0.15909\n",
      "36805/36805 [==============================] - 17s 475us/sample - loss: 0.2635 - acc: 0.9154 - val_loss: 0.1615 - val_acc: 0.9485\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2623 - acc: 0.9177\n",
      "Epoch 00105: val_loss improved from 0.15909 to 0.15731, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_6_conv_checkpoint/105-0.1573.hdf5\n",
      "36805/36805 [==============================] - 18s 478us/sample - loss: 0.2623 - acc: 0.9177 - val_loss: 0.1573 - val_acc: 0.9495\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2555 - acc: 0.9186\n",
      "Epoch 00106: val_loss improved from 0.15731 to 0.15332, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_6_conv_checkpoint/106-0.1533.hdf5\n",
      "36805/36805 [==============================] - 18s 479us/sample - loss: 0.2555 - acc: 0.9186 - val_loss: 0.1533 - val_acc: 0.9511\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2596 - acc: 0.9169\n",
      "Epoch 00107: val_loss did not improve from 0.15332\n",
      "36805/36805 [==============================] - 17s 474us/sample - loss: 0.2596 - acc: 0.9169 - val_loss: 0.1643 - val_acc: 0.9488\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2544 - acc: 0.9205\n",
      "Epoch 00108: val_loss did not improve from 0.15332\n",
      "36805/36805 [==============================] - 18s 479us/sample - loss: 0.2544 - acc: 0.9205 - val_loss: 0.1647 - val_acc: 0.9467\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2542 - acc: 0.9182\n",
      "Epoch 00109: val_loss did not improve from 0.15332\n",
      "36805/36805 [==============================] - 18s 477us/sample - loss: 0.2541 - acc: 0.9182 - val_loss: 0.1609 - val_acc: 0.9506\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2529 - acc: 0.9194\n",
      "Epoch 00110: val_loss did not improve from 0.15332\n",
      "36805/36805 [==============================] - 18s 478us/sample - loss: 0.2529 - acc: 0.9194 - val_loss: 0.1546 - val_acc: 0.9504\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2541 - acc: 0.9182\n",
      "Epoch 00111: val_loss did not improve from 0.15332\n",
      "36805/36805 [==============================] - 17s 475us/sample - loss: 0.2541 - acc: 0.9182 - val_loss: 0.1554 - val_acc: 0.9518\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2514 - acc: 0.9218\n",
      "Epoch 00112: val_loss did not improve from 0.15332\n",
      "36805/36805 [==============================] - 18s 478us/sample - loss: 0.2514 - acc: 0.9218 - val_loss: 0.1576 - val_acc: 0.9502\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2507 - acc: 0.9206\n",
      "Epoch 00113: val_loss did not improve from 0.15332\n",
      "36805/36805 [==============================] - 18s 479us/sample - loss: 0.2507 - acc: 0.9206 - val_loss: 0.1560 - val_acc: 0.9490\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2476 - acc: 0.9205\n",
      "Epoch 00114: val_loss did not improve from 0.15332\n",
      "36805/36805 [==============================] - 18s 476us/sample - loss: 0.2477 - acc: 0.9204 - val_loss: 0.1562 - val_acc: 0.9483\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2486 - acc: 0.9221\n",
      "Epoch 00115: val_loss did not improve from 0.15332\n",
      "36805/36805 [==============================] - 18s 484us/sample - loss: 0.2487 - acc: 0.9221 - val_loss: 0.1599 - val_acc: 0.9488\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2452 - acc: 0.9217\n",
      "Epoch 00116: val_loss did not improve from 0.15332\n",
      "36805/36805 [==============================] - 18s 479us/sample - loss: 0.2452 - acc: 0.9217 - val_loss: 0.1540 - val_acc: 0.9504\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2470 - acc: 0.9208\n",
      "Epoch 00117: val_loss did not improve from 0.15332\n",
      "36805/36805 [==============================] - 18s 478us/sample - loss: 0.2470 - acc: 0.9208 - val_loss: 0.1554 - val_acc: 0.9506\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2393 - acc: 0.9241\n",
      "Epoch 00118: val_loss did not improve from 0.15332\n",
      "36805/36805 [==============================] - 18s 479us/sample - loss: 0.2393 - acc: 0.9241 - val_loss: 0.1535 - val_acc: 0.9522\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2431 - acc: 0.9230\n",
      "Epoch 00119: val_loss improved from 0.15332 to 0.15161, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_6_conv_checkpoint/119-0.1516.hdf5\n",
      "36805/36805 [==============================] - 17s 473us/sample - loss: 0.2431 - acc: 0.9230 - val_loss: 0.1516 - val_acc: 0.9520\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2425 - acc: 0.9241\n",
      "Epoch 00120: val_loss did not improve from 0.15161\n",
      "36805/36805 [==============================] - 18s 483us/sample - loss: 0.2425 - acc: 0.9241 - val_loss: 0.1518 - val_acc: 0.9525\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2387 - acc: 0.9246\n",
      "Epoch 00121: val_loss did not improve from 0.15161\n",
      "36805/36805 [==============================] - 18s 479us/sample - loss: 0.2387 - acc: 0.9246 - val_loss: 0.1548 - val_acc: 0.9527\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2400 - acc: 0.9223\n",
      "Epoch 00122: val_loss did not improve from 0.15161\n",
      "36805/36805 [==============================] - 18s 477us/sample - loss: 0.2401 - acc: 0.9223 - val_loss: 0.1621 - val_acc: 0.9513\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2414 - acc: 0.9241\n",
      "Epoch 00123: val_loss did not improve from 0.15161\n",
      "36805/36805 [==============================] - 17s 475us/sample - loss: 0.2414 - acc: 0.9241 - val_loss: 0.1685 - val_acc: 0.9469\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2367 - acc: 0.9249\n",
      "Epoch 00124: val_loss did not improve from 0.15161\n",
      "36805/36805 [==============================] - 18s 480us/sample - loss: 0.2367 - acc: 0.9249 - val_loss: 0.1546 - val_acc: 0.9534\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2379 - acc: 0.9246\n",
      "Epoch 00125: val_loss did not improve from 0.15161\n",
      "36805/36805 [==============================] - 18s 480us/sample - loss: 0.2379 - acc: 0.9246 - val_loss: 0.1547 - val_acc: 0.9532\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2303 - acc: 0.9268\n",
      "Epoch 00126: val_loss did not improve from 0.15161\n",
      "36805/36805 [==============================] - 18s 478us/sample - loss: 0.2303 - acc: 0.9268 - val_loss: 0.1531 - val_acc: 0.9515\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2324 - acc: 0.9256\n",
      "Epoch 00127: val_loss did not improve from 0.15161\n",
      "36805/36805 [==============================] - 17s 473us/sample - loss: 0.2324 - acc: 0.9256 - val_loss: 0.1550 - val_acc: 0.9511\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2350 - acc: 0.9237\n",
      "Epoch 00128: val_loss did not improve from 0.15161\n",
      "36805/36805 [==============================] - 17s 474us/sample - loss: 0.2350 - acc: 0.9237 - val_loss: 0.1576 - val_acc: 0.9488\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2290 - acc: 0.9269\n",
      "Epoch 00129: val_loss did not improve from 0.15161\n",
      "36805/36805 [==============================] - 18s 476us/sample - loss: 0.2289 - acc: 0.9269 - val_loss: 0.1520 - val_acc: 0.9541\n",
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2329 - acc: 0.9243\n",
      "Epoch 00130: val_loss did not improve from 0.15161\n",
      "36805/36805 [==============================] - 18s 477us/sample - loss: 0.2330 - acc: 0.9242 - val_loss: 0.1559 - val_acc: 0.9527\n",
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2308 - acc: 0.9267\n",
      "Epoch 00131: val_loss improved from 0.15161 to 0.15034, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_6_conv_checkpoint/131-0.1503.hdf5\n",
      "36805/36805 [==============================] - 18s 476us/sample - loss: 0.2308 - acc: 0.9267 - val_loss: 0.1503 - val_acc: 0.9532\n",
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2263 - acc: 0.9270\n",
      "Epoch 00132: val_loss did not improve from 0.15034\n",
      "36805/36805 [==============================] - 18s 482us/sample - loss: 0.2264 - acc: 0.9269 - val_loss: 0.1534 - val_acc: 0.9522\n",
      "Epoch 133/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2313 - acc: 0.9258\n",
      "Epoch 00133: val_loss did not improve from 0.15034\n",
      "36805/36805 [==============================] - 18s 478us/sample - loss: 0.2313 - acc: 0.9258 - val_loss: 0.1533 - val_acc: 0.9502\n",
      "Epoch 134/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2292 - acc: 0.9271\n",
      "Epoch 00134: val_loss did not improve from 0.15034\n",
      "36805/36805 [==============================] - 18s 483us/sample - loss: 0.2291 - acc: 0.9271 - val_loss: 0.1521 - val_acc: 0.9522\n",
      "Epoch 135/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2290 - acc: 0.9274\n",
      "Epoch 00135: val_loss did not improve from 0.15034\n",
      "36805/36805 [==============================] - 18s 476us/sample - loss: 0.2291 - acc: 0.9274 - val_loss: 0.1543 - val_acc: 0.9520\n",
      "Epoch 136/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2278 - acc: 0.9270\n",
      "Epoch 00136: val_loss did not improve from 0.15034\n",
      "36805/36805 [==============================] - 18s 477us/sample - loss: 0.2279 - acc: 0.9269 - val_loss: 0.1592 - val_acc: 0.9511\n",
      "Epoch 137/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2285 - acc: 0.9273\n",
      "Epoch 00137: val_loss improved from 0.15034 to 0.14983, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_6_conv_checkpoint/137-0.1498.hdf5\n",
      "36805/36805 [==============================] - 18s 481us/sample - loss: 0.2285 - acc: 0.9273 - val_loss: 0.1498 - val_acc: 0.9536\n",
      "Epoch 138/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2225 - acc: 0.9287\n",
      "Epoch 00138: val_loss did not improve from 0.14983\n",
      "36805/36805 [==============================] - 18s 479us/sample - loss: 0.2225 - acc: 0.9287 - val_loss: 0.1518 - val_acc: 0.9515\n",
      "Epoch 139/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2278 - acc: 0.9283\n",
      "Epoch 00139: val_loss did not improve from 0.14983\n",
      "36805/36805 [==============================] - 18s 478us/sample - loss: 0.2279 - acc: 0.9283 - val_loss: 0.1523 - val_acc: 0.9520\n",
      "Epoch 140/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2204 - acc: 0.9293\n",
      "Epoch 00140: val_loss did not improve from 0.14983\n",
      "36805/36805 [==============================] - 17s 472us/sample - loss: 0.2204 - acc: 0.9293 - val_loss: 0.1521 - val_acc: 0.9518\n",
      "Epoch 141/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2216 - acc: 0.9291\n",
      "Epoch 00141: val_loss improved from 0.14983 to 0.14478, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_6_conv_checkpoint/141-0.1448.hdf5\n",
      "36805/36805 [==============================] - 18s 479us/sample - loss: 0.2216 - acc: 0.9291 - val_loss: 0.1448 - val_acc: 0.9567\n",
      "Epoch 142/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2203 - acc: 0.9306\n",
      "Epoch 00142: val_loss did not improve from 0.14478\n",
      "36805/36805 [==============================] - 17s 475us/sample - loss: 0.2203 - acc: 0.9306 - val_loss: 0.1513 - val_acc: 0.9529\n",
      "Epoch 143/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2195 - acc: 0.9309\n",
      "Epoch 00143: val_loss did not improve from 0.14478\n",
      "36805/36805 [==============================] - 17s 471us/sample - loss: 0.2195 - acc: 0.9309 - val_loss: 0.1531 - val_acc: 0.9522\n",
      "Epoch 144/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2178 - acc: 0.9308\n",
      "Epoch 00144: val_loss did not improve from 0.14478\n",
      "36805/36805 [==============================] - 18s 478us/sample - loss: 0.2178 - acc: 0.9308 - val_loss: 0.1557 - val_acc: 0.9509\n",
      "Epoch 145/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2189 - acc: 0.9289\n",
      "Epoch 00145: val_loss did not improve from 0.14478\n",
      "36805/36805 [==============================] - 17s 475us/sample - loss: 0.2190 - acc: 0.9288 - val_loss: 0.1565 - val_acc: 0.9511\n",
      "Epoch 146/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2183 - acc: 0.9302\n",
      "Epoch 00146: val_loss did not improve from 0.14478\n",
      "36805/36805 [==============================] - 17s 472us/sample - loss: 0.2182 - acc: 0.9302 - val_loss: 0.1512 - val_acc: 0.9518\n",
      "Epoch 147/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2143 - acc: 0.9311\n",
      "Epoch 00147: val_loss did not improve from 0.14478\n",
      "36805/36805 [==============================] - 18s 479us/sample - loss: 0.2143 - acc: 0.9311 - val_loss: 0.1489 - val_acc: 0.9534\n",
      "Epoch 148/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2142 - acc: 0.9328\n",
      "Epoch 00148: val_loss did not improve from 0.14478\n",
      "36805/36805 [==============================] - 18s 482us/sample - loss: 0.2143 - acc: 0.9328 - val_loss: 0.1510 - val_acc: 0.9520\n",
      "Epoch 149/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2164 - acc: 0.9294\n",
      "Epoch 00149: val_loss did not improve from 0.14478\n",
      "36805/36805 [==============================] - 18s 476us/sample - loss: 0.2164 - acc: 0.9294 - val_loss: 0.1541 - val_acc: 0.9506\n",
      "Epoch 150/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2137 - acc: 0.9312\n",
      "Epoch 00150: val_loss did not improve from 0.14478\n",
      "36805/36805 [==============================] - 18s 480us/sample - loss: 0.2136 - acc: 0.9312 - val_loss: 0.1476 - val_acc: 0.9543\n",
      "Epoch 151/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2121 - acc: 0.9312\n",
      "Epoch 00151: val_loss did not improve from 0.14478\n",
      "36805/36805 [==============================] - 18s 479us/sample - loss: 0.2121 - acc: 0.9312 - val_loss: 0.1470 - val_acc: 0.9529\n",
      "Epoch 152/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2113 - acc: 0.9317\n",
      "Epoch 00152: val_loss did not improve from 0.14478\n",
      "36805/36805 [==============================] - 18s 477us/sample - loss: 0.2113 - acc: 0.9317 - val_loss: 0.1468 - val_acc: 0.9518\n",
      "Epoch 153/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2138 - acc: 0.9320\n",
      "Epoch 00153: val_loss did not improve from 0.14478\n",
      "36805/36805 [==============================] - 18s 480us/sample - loss: 0.2138 - acc: 0.9320 - val_loss: 0.1551 - val_acc: 0.9513\n",
      "Epoch 154/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2114 - acc: 0.9317\n",
      "Epoch 00154: val_loss did not improve from 0.14478\n",
      "36805/36805 [==============================] - 17s 474us/sample - loss: 0.2113 - acc: 0.9317 - val_loss: 0.1533 - val_acc: 0.9525\n",
      "Epoch 155/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2114 - acc: 0.9323\n",
      "Epoch 00155: val_loss did not improve from 0.14478\n",
      "36805/36805 [==============================] - 17s 475us/sample - loss: 0.2114 - acc: 0.9323 - val_loss: 0.1511 - val_acc: 0.9520\n",
      "Epoch 156/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2102 - acc: 0.9322\n",
      "Epoch 00156: val_loss did not improve from 0.14478\n",
      "36805/36805 [==============================] - 18s 483us/sample - loss: 0.2101 - acc: 0.9322 - val_loss: 0.1530 - val_acc: 0.9539\n",
      "Epoch 157/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2100 - acc: 0.9328\n",
      "Epoch 00157: val_loss did not improve from 0.14478\n",
      "36805/36805 [==============================] - 18s 482us/sample - loss: 0.2100 - acc: 0.9328 - val_loss: 0.1524 - val_acc: 0.9543\n",
      "Epoch 158/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2061 - acc: 0.9335\n",
      "Epoch 00158: val_loss did not improve from 0.14478\n",
      "36805/36805 [==============================] - 18s 479us/sample - loss: 0.2062 - acc: 0.9335 - val_loss: 0.1496 - val_acc: 0.9525\n",
      "Epoch 159/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2066 - acc: 0.9347\n",
      "Epoch 00159: val_loss did not improve from 0.14478\n",
      "36805/36805 [==============================] - 18s 477us/sample - loss: 0.2067 - acc: 0.9347 - val_loss: 0.1774 - val_acc: 0.9446\n",
      "Epoch 160/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2114 - acc: 0.9316\n",
      "Epoch 00160: val_loss did not improve from 0.14478\n",
      "36805/36805 [==============================] - 17s 471us/sample - loss: 0.2114 - acc: 0.9316 - val_loss: 0.1573 - val_acc: 0.9515\n",
      "Epoch 161/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2061 - acc: 0.9323\n",
      "Epoch 00161: val_loss did not improve from 0.14478\n",
      "36805/36805 [==============================] - 17s 474us/sample - loss: 0.2061 - acc: 0.9323 - val_loss: 0.1527 - val_acc: 0.9541\n",
      "Epoch 162/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2049 - acc: 0.9337\n",
      "Epoch 00162: val_loss did not improve from 0.14478\n",
      "36805/36805 [==============================] - 17s 470us/sample - loss: 0.2049 - acc: 0.9338 - val_loss: 0.1554 - val_acc: 0.9515\n",
      "Epoch 163/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2030 - acc: 0.9336\n",
      "Epoch 00163: val_loss did not improve from 0.14478\n",
      "36805/36805 [==============================] - 17s 472us/sample - loss: 0.2029 - acc: 0.9336 - val_loss: 0.1552 - val_acc: 0.9506\n",
      "Epoch 164/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2045 - acc: 0.9347\n",
      "Epoch 00164: val_loss did not improve from 0.14478\n",
      "36805/36805 [==============================] - 17s 470us/sample - loss: 0.2045 - acc: 0.9347 - val_loss: 0.1506 - val_acc: 0.9536\n",
      "Epoch 165/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2045 - acc: 0.9349\n",
      "Epoch 00165: val_loss did not improve from 0.14478\n",
      "36805/36805 [==============================] - 18s 478us/sample - loss: 0.2045 - acc: 0.9348 - val_loss: 0.1482 - val_acc: 0.9541\n",
      "Epoch 166/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2016 - acc: 0.9357\n",
      "Epoch 00166: val_loss did not improve from 0.14478\n",
      "36805/36805 [==============================] - 17s 472us/sample - loss: 0.2016 - acc: 0.9357 - val_loss: 0.1526 - val_acc: 0.9527\n",
      "Epoch 167/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2053 - acc: 0.9335\n",
      "Epoch 00167: val_loss did not improve from 0.14478\n",
      "36805/36805 [==============================] - 18s 477us/sample - loss: 0.2053 - acc: 0.9335 - val_loss: 0.1560 - val_acc: 0.9532\n",
      "Epoch 168/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1985 - acc: 0.9354\n",
      "Epoch 00168: val_loss did not improve from 0.14478\n",
      "36805/36805 [==============================] - 18s 479us/sample - loss: 0.1984 - acc: 0.9354 - val_loss: 0.1590 - val_acc: 0.9536\n",
      "Epoch 169/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2031 - acc: 0.9338\n",
      "Epoch 00169: val_loss did not improve from 0.14478\n",
      "36805/36805 [==============================] - 18s 485us/sample - loss: 0.2031 - acc: 0.9338 - val_loss: 0.1567 - val_acc: 0.9488\n",
      "Epoch 170/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1974 - acc: 0.9368\n",
      "Epoch 00170: val_loss did not improve from 0.14478\n",
      "36805/36805 [==============================] - 18s 482us/sample - loss: 0.1974 - acc: 0.9368 - val_loss: 0.1521 - val_acc: 0.9541\n",
      "Epoch 171/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2002 - acc: 0.9353\n",
      "Epoch 00171: val_loss did not improve from 0.14478\n",
      "36805/36805 [==============================] - 18s 482us/sample - loss: 0.2002 - acc: 0.9353 - val_loss: 0.1496 - val_acc: 0.9529\n",
      "Epoch 172/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1941 - acc: 0.9369\n",
      "Epoch 00172: val_loss did not improve from 0.14478\n",
      "36805/36805 [==============================] - 18s 478us/sample - loss: 0.1941 - acc: 0.9369 - val_loss: 0.1517 - val_acc: 0.9509\n",
      "Epoch 173/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1966 - acc: 0.9369\n",
      "Epoch 00173: val_loss did not improve from 0.14478\n",
      "36805/36805 [==============================] - 18s 479us/sample - loss: 0.1966 - acc: 0.9369 - val_loss: 0.1497 - val_acc: 0.9539\n",
      "Epoch 174/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1967 - acc: 0.9358\n",
      "Epoch 00174: val_loss did not improve from 0.14478\n",
      "36805/36805 [==============================] - 18s 481us/sample - loss: 0.1966 - acc: 0.9359 - val_loss: 0.1508 - val_acc: 0.9529\n",
      "Epoch 175/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2001 - acc: 0.9343\n",
      "Epoch 00175: val_loss did not improve from 0.14478\n",
      "36805/36805 [==============================] - 18s 482us/sample - loss: 0.2001 - acc: 0.9342 - val_loss: 0.1505 - val_acc: 0.9550\n",
      "Epoch 176/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1972 - acc: 0.9368\n",
      "Epoch 00176: val_loss did not improve from 0.14478\n",
      "36805/36805 [==============================] - 17s 475us/sample - loss: 0.1973 - acc: 0.9367 - val_loss: 0.1561 - val_acc: 0.9504\n",
      "Epoch 177/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1952 - acc: 0.9373\n",
      "Epoch 00177: val_loss did not improve from 0.14478\n",
      "36805/36805 [==============================] - 18s 480us/sample - loss: 0.1952 - acc: 0.9373 - val_loss: 0.1475 - val_acc: 0.9557\n",
      "Epoch 178/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1929 - acc: 0.9378\n",
      "Epoch 00178: val_loss did not improve from 0.14478\n",
      "36805/36805 [==============================] - 18s 481us/sample - loss: 0.1930 - acc: 0.9378 - val_loss: 0.1484 - val_acc: 0.9553\n",
      "Epoch 179/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1901 - acc: 0.9371\n",
      "Epoch 00179: val_loss did not improve from 0.14478\n",
      "36805/36805 [==============================] - 18s 481us/sample - loss: 0.1902 - acc: 0.9371 - val_loss: 0.1747 - val_acc: 0.9441\n",
      "Epoch 180/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1896 - acc: 0.9395\n",
      "Epoch 00180: val_loss did not improve from 0.14478\n",
      "36805/36805 [==============================] - 18s 482us/sample - loss: 0.1897 - acc: 0.9394 - val_loss: 0.1512 - val_acc: 0.9541\n",
      "Epoch 181/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1909 - acc: 0.9375\n",
      "Epoch 00181: val_loss did not improve from 0.14478\n",
      "36805/36805 [==============================] - 18s 479us/sample - loss: 0.1909 - acc: 0.9375 - val_loss: 0.1470 - val_acc: 0.9525\n",
      "Epoch 182/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1887 - acc: 0.9380\n",
      "Epoch 00182: val_loss did not improve from 0.14478\n",
      "36805/36805 [==============================] - 17s 474us/sample - loss: 0.1887 - acc: 0.9380 - val_loss: 0.1503 - val_acc: 0.9550\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 183/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1906 - acc: 0.9381\n",
      "Epoch 00183: val_loss did not improve from 0.14478\n",
      "36805/36805 [==============================] - 18s 483us/sample - loss: 0.1906 - acc: 0.9381 - val_loss: 0.1558 - val_acc: 0.9539\n",
      "Epoch 184/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1887 - acc: 0.9395\n",
      "Epoch 00184: val_loss did not improve from 0.14478\n",
      "36805/36805 [==============================] - 17s 474us/sample - loss: 0.1887 - acc: 0.9395 - val_loss: 0.1553 - val_acc: 0.9525\n",
      "Epoch 185/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1889 - acc: 0.9407\n",
      "Epoch 00185: val_loss did not improve from 0.14478\n",
      "36805/36805 [==============================] - 17s 472us/sample - loss: 0.1889 - acc: 0.9407 - val_loss: 0.1510 - val_acc: 0.9543\n",
      "Epoch 186/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1881 - acc: 0.9389\n",
      "Epoch 00186: val_loss did not improve from 0.14478\n",
      "36805/36805 [==============================] - 18s 479us/sample - loss: 0.1881 - acc: 0.9389 - val_loss: 0.1514 - val_acc: 0.9546\n",
      "Epoch 187/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1871 - acc: 0.9392\n",
      "Epoch 00187: val_loss did not improve from 0.14478\n",
      "36805/36805 [==============================] - 18s 479us/sample - loss: 0.1871 - acc: 0.9392 - val_loss: 0.1563 - val_acc: 0.9546\n",
      "Epoch 188/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1884 - acc: 0.9383\n",
      "Epoch 00188: val_loss did not improve from 0.14478\n",
      "36805/36805 [==============================] - 18s 481us/sample - loss: 0.1883 - acc: 0.9383 - val_loss: 0.1518 - val_acc: 0.9534\n",
      "Epoch 189/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1888 - acc: 0.9389\n",
      "Epoch 00189: val_loss did not improve from 0.14478\n",
      "36805/36805 [==============================] - 17s 474us/sample - loss: 0.1888 - acc: 0.9389 - val_loss: 0.1571 - val_acc: 0.9529\n",
      "Epoch 190/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1890 - acc: 0.9380\n",
      "Epoch 00190: val_loss did not improve from 0.14478\n",
      "36805/36805 [==============================] - 18s 479us/sample - loss: 0.1891 - acc: 0.9381 - val_loss: 0.1521 - val_acc: 0.9536\n",
      "Epoch 191/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1840 - acc: 0.9403\n",
      "Epoch 00191: val_loss did not improve from 0.14478\n",
      "36805/36805 [==============================] - 18s 481us/sample - loss: 0.1840 - acc: 0.9403 - val_loss: 0.1536 - val_acc: 0.9522\n",
      "\n",
      "1D_CNN_custom_multi_2_GMP_ch_32_DO_6_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 1s 305us/sample - loss: 0.1952 - acc: 0.9383\n",
      "Loss: 0.19517268415304484 Accuracy: 0.9383178\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.4429 - acc: 0.1967\n",
      "Epoch 00001: val_loss improved from inf to 1.83021, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_7_conv_checkpoint/001-1.8302.hdf5\n",
      "36805/36805 [==============================] - 22s 606us/sample - loss: 2.4429 - acc: 0.1967 - val_loss: 1.8302 - val_acc: 0.5073\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.7724 - acc: 0.4263\n",
      "Epoch 00002: val_loss improved from 1.83021 to 1.27481, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_7_conv_checkpoint/002-1.2748.hdf5\n",
      "36805/36805 [==============================] - 18s 502us/sample - loss: 1.7725 - acc: 0.4263 - val_loss: 1.2748 - val_acc: 0.6473\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4331 - acc: 0.5340\n",
      "Epoch 00003: val_loss improved from 1.27481 to 0.99863, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_7_conv_checkpoint/003-0.9986.hdf5\n",
      "36805/36805 [==============================] - 19s 503us/sample - loss: 1.4331 - acc: 0.5341 - val_loss: 0.9986 - val_acc: 0.7151\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2284 - acc: 0.6004\n",
      "Epoch 00004: val_loss improved from 0.99863 to 0.85415, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_7_conv_checkpoint/004-0.8541.hdf5\n",
      "36805/36805 [==============================] - 18s 501us/sample - loss: 1.2284 - acc: 0.6004 - val_loss: 0.8541 - val_acc: 0.7519\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0829 - acc: 0.6489\n",
      "Epoch 00005: val_loss improved from 0.85415 to 0.74046, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_7_conv_checkpoint/005-0.7405.hdf5\n",
      "36805/36805 [==============================] - 18s 500us/sample - loss: 1.0829 - acc: 0.6488 - val_loss: 0.7405 - val_acc: 0.7869\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9725 - acc: 0.6908\n",
      "Epoch 00006: val_loss improved from 0.74046 to 0.64945, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_7_conv_checkpoint/006-0.6494.hdf5\n",
      "36805/36805 [==============================] - 18s 502us/sample - loss: 0.9724 - acc: 0.6908 - val_loss: 0.6494 - val_acc: 0.8230\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8798 - acc: 0.7247\n",
      "Epoch 00007: val_loss improved from 0.64945 to 0.56198, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_7_conv_checkpoint/007-0.5620.hdf5\n",
      "36805/36805 [==============================] - 18s 501us/sample - loss: 0.8798 - acc: 0.7247 - val_loss: 0.5620 - val_acc: 0.8411\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8072 - acc: 0.7432\n",
      "Epoch 00008: val_loss improved from 0.56198 to 0.51619, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_7_conv_checkpoint/008-0.5162.hdf5\n",
      "36805/36805 [==============================] - 18s 499us/sample - loss: 0.8071 - acc: 0.7432 - val_loss: 0.5162 - val_acc: 0.8526\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7462 - acc: 0.7671\n",
      "Epoch 00009: val_loss improved from 0.51619 to 0.46047, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_7_conv_checkpoint/009-0.4605.hdf5\n",
      "36805/36805 [==============================] - 18s 501us/sample - loss: 0.7462 - acc: 0.7671 - val_loss: 0.4605 - val_acc: 0.8656\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6967 - acc: 0.7810\n",
      "Epoch 00010: val_loss improved from 0.46047 to 0.45623, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_7_conv_checkpoint/010-0.4562.hdf5\n",
      "36805/36805 [==============================] - 19s 505us/sample - loss: 0.6967 - acc: 0.7810 - val_loss: 0.4562 - val_acc: 0.8703\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6576 - acc: 0.7942\n",
      "Epoch 00011: val_loss improved from 0.45623 to 0.42470, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_7_conv_checkpoint/011-0.4247.hdf5\n",
      "36805/36805 [==============================] - 19s 504us/sample - loss: 0.6576 - acc: 0.7942 - val_loss: 0.4247 - val_acc: 0.8821\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6179 - acc: 0.8070\n",
      "Epoch 00012: val_loss improved from 0.42470 to 0.38715, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_7_conv_checkpoint/012-0.3872.hdf5\n",
      "36805/36805 [==============================] - 18s 502us/sample - loss: 0.6179 - acc: 0.8070 - val_loss: 0.3872 - val_acc: 0.8880\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5929 - acc: 0.8130\n",
      "Epoch 00013: val_loss improved from 0.38715 to 0.35783, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_7_conv_checkpoint/013-0.3578.hdf5\n",
      "36805/36805 [==============================] - 19s 505us/sample - loss: 0.5928 - acc: 0.8130 - val_loss: 0.3578 - val_acc: 0.8963\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5634 - acc: 0.8229\n",
      "Epoch 00014: val_loss improved from 0.35783 to 0.34020, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_7_conv_checkpoint/014-0.3402.hdf5\n",
      "36805/36805 [==============================] - 19s 503us/sample - loss: 0.5634 - acc: 0.8229 - val_loss: 0.3402 - val_acc: 0.9038\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5382 - acc: 0.8322\n",
      "Epoch 00015: val_loss improved from 0.34020 to 0.32545, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_7_conv_checkpoint/015-0.3254.hdf5\n",
      "36805/36805 [==============================] - 19s 505us/sample - loss: 0.5382 - acc: 0.8322 - val_loss: 0.3254 - val_acc: 0.9113\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5182 - acc: 0.8361\n",
      "Epoch 00016: val_loss improved from 0.32545 to 0.31206, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_7_conv_checkpoint/016-0.3121.hdf5\n",
      "36805/36805 [==============================] - 19s 505us/sample - loss: 0.5182 - acc: 0.8361 - val_loss: 0.3121 - val_acc: 0.9150\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4984 - acc: 0.8427\n",
      "Epoch 00017: val_loss did not improve from 0.31206\n",
      "36805/36805 [==============================] - 19s 503us/sample - loss: 0.4983 - acc: 0.8427 - val_loss: 0.3128 - val_acc: 0.9094\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4840 - acc: 0.8492\n",
      "Epoch 00018: val_loss improved from 0.31206 to 0.29221, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_7_conv_checkpoint/018-0.2922.hdf5\n",
      "36805/36805 [==============================] - 19s 504us/sample - loss: 0.4839 - acc: 0.8492 - val_loss: 0.2922 - val_acc: 0.9129\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4722 - acc: 0.8526\n",
      "Epoch 00019: val_loss improved from 0.29221 to 0.28192, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_7_conv_checkpoint/019-0.2819.hdf5\n",
      "36805/36805 [==============================] - 19s 505us/sample - loss: 0.4722 - acc: 0.8526 - val_loss: 0.2819 - val_acc: 0.9185\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4455 - acc: 0.8607\n",
      "Epoch 00020: val_loss did not improve from 0.28192\n",
      "36805/36805 [==============================] - 19s 503us/sample - loss: 0.4455 - acc: 0.8607 - val_loss: 0.2824 - val_acc: 0.9159\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4444 - acc: 0.8627\n",
      "Epoch 00021: val_loss improved from 0.28192 to 0.26573, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_7_conv_checkpoint/021-0.2657.hdf5\n",
      "36805/36805 [==============================] - 18s 500us/sample - loss: 0.4444 - acc: 0.8627 - val_loss: 0.2657 - val_acc: 0.9234\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4249 - acc: 0.8679\n",
      "Epoch 00022: val_loss improved from 0.26573 to 0.25120, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_7_conv_checkpoint/022-0.2512.hdf5\n",
      "36805/36805 [==============================] - 18s 498us/sample - loss: 0.4249 - acc: 0.8679 - val_loss: 0.2512 - val_acc: 0.9255\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4132 - acc: 0.8702\n",
      "Epoch 00023: val_loss improved from 0.25120 to 0.23601, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_7_conv_checkpoint/023-0.2360.hdf5\n",
      "36805/36805 [==============================] - 19s 505us/sample - loss: 0.4132 - acc: 0.8702 - val_loss: 0.2360 - val_acc: 0.9334\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3992 - acc: 0.8745\n",
      "Epoch 00024: val_loss did not improve from 0.23601\n",
      "36805/36805 [==============================] - 18s 502us/sample - loss: 0.3991 - acc: 0.8746 - val_loss: 0.2372 - val_acc: 0.9308\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3923 - acc: 0.8778\n",
      "Epoch 00025: val_loss improved from 0.23601 to 0.22170, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_7_conv_checkpoint/025-0.2217.hdf5\n",
      "36805/36805 [==============================] - 19s 505us/sample - loss: 0.3923 - acc: 0.8778 - val_loss: 0.2217 - val_acc: 0.9366\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3790 - acc: 0.8844\n",
      "Epoch 00026: val_loss did not improve from 0.22170\n",
      "36805/36805 [==============================] - 18s 502us/sample - loss: 0.3790 - acc: 0.8843 - val_loss: 0.2293 - val_acc: 0.9331\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3682 - acc: 0.8866\n",
      "Epoch 00027: val_loss improved from 0.22170 to 0.21262, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_7_conv_checkpoint/027-0.2126.hdf5\n",
      "36805/36805 [==============================] - 19s 505us/sample - loss: 0.3682 - acc: 0.8866 - val_loss: 0.2126 - val_acc: 0.9408\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3552 - acc: 0.8893\n",
      "Epoch 00028: val_loss improved from 0.21262 to 0.20027, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_7_conv_checkpoint/028-0.2003.hdf5\n",
      "36805/36805 [==============================] - 19s 506us/sample - loss: 0.3552 - acc: 0.8893 - val_loss: 0.2003 - val_acc: 0.9413\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3538 - acc: 0.8898\n",
      "Epoch 00029: val_loss did not improve from 0.20027\n",
      "36805/36805 [==============================] - 19s 504us/sample - loss: 0.3537 - acc: 0.8899 - val_loss: 0.2104 - val_acc: 0.9352\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3450 - acc: 0.8930\n",
      "Epoch 00030: val_loss improved from 0.20027 to 0.19850, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_7_conv_checkpoint/030-0.1985.hdf5\n",
      "36805/36805 [==============================] - 18s 502us/sample - loss: 0.3450 - acc: 0.8930 - val_loss: 0.1985 - val_acc: 0.9413\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3395 - acc: 0.8945\n",
      "Epoch 00031: val_loss did not improve from 0.19850\n",
      "36805/36805 [==============================] - 18s 501us/sample - loss: 0.3395 - acc: 0.8946 - val_loss: 0.2016 - val_acc: 0.9383\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3334 - acc: 0.8962\n",
      "Epoch 00032: val_loss improved from 0.19850 to 0.18522, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_7_conv_checkpoint/032-0.1852.hdf5\n",
      "36805/36805 [==============================] - 19s 507us/sample - loss: 0.3334 - acc: 0.8962 - val_loss: 0.1852 - val_acc: 0.9450\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3221 - acc: 0.9019\n",
      "Epoch 00033: val_loss did not improve from 0.18522\n",
      "36805/36805 [==============================] - 19s 503us/sample - loss: 0.3222 - acc: 0.9019 - val_loss: 0.1866 - val_acc: 0.9446\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3193 - acc: 0.8999\n",
      "Epoch 00034: val_loss did not improve from 0.18522\n",
      "36805/36805 [==============================] - 18s 500us/sample - loss: 0.3193 - acc: 0.8999 - val_loss: 0.1920 - val_acc: 0.9420\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3127 - acc: 0.9023\n",
      "Epoch 00035: val_loss did not improve from 0.18522\n",
      "36805/36805 [==============================] - 18s 502us/sample - loss: 0.3129 - acc: 0.9023 - val_loss: 0.1881 - val_acc: 0.9439\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3097 - acc: 0.9029\n",
      "Epoch 00036: val_loss did not improve from 0.18522\n",
      "36805/36805 [==============================] - 18s 501us/sample - loss: 0.3096 - acc: 0.9029 - val_loss: 0.1868 - val_acc: 0.9434\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3051 - acc: 0.9063\n",
      "Epoch 00037: val_loss improved from 0.18522 to 0.18381, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_7_conv_checkpoint/037-0.1838.hdf5\n",
      "36805/36805 [==============================] - 18s 503us/sample - loss: 0.3051 - acc: 0.9063 - val_loss: 0.1838 - val_acc: 0.9450\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2976 - acc: 0.9078\n",
      "Epoch 00038: val_loss improved from 0.18381 to 0.17092, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_7_conv_checkpoint/038-0.1709.hdf5\n",
      "36805/36805 [==============================] - 19s 504us/sample - loss: 0.2977 - acc: 0.9077 - val_loss: 0.1709 - val_acc: 0.9467\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2956 - acc: 0.9084\n",
      "Epoch 00039: val_loss did not improve from 0.17092\n",
      "36805/36805 [==============================] - 18s 502us/sample - loss: 0.2956 - acc: 0.9084 - val_loss: 0.1746 - val_acc: 0.9474\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2872 - acc: 0.9130\n",
      "Epoch 00040: val_loss did not improve from 0.17092\n",
      "36805/36805 [==============================] - 18s 502us/sample - loss: 0.2872 - acc: 0.9130 - val_loss: 0.1741 - val_acc: 0.9441\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2836 - acc: 0.9113\n",
      "Epoch 00041: val_loss improved from 0.17092 to 0.16455, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_7_conv_checkpoint/041-0.1646.hdf5\n",
      "36805/36805 [==============================] - 19s 504us/sample - loss: 0.2840 - acc: 0.9113 - val_loss: 0.1646 - val_acc: 0.9502\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2797 - acc: 0.9117\n",
      "Epoch 00042: val_loss did not improve from 0.16455\n",
      "36805/36805 [==============================] - 18s 499us/sample - loss: 0.2797 - acc: 0.9117 - val_loss: 0.1821 - val_acc: 0.9439\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2733 - acc: 0.9150\n",
      "Epoch 00043: val_loss improved from 0.16455 to 0.16141, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_7_conv_checkpoint/043-0.1614.hdf5\n",
      "36805/36805 [==============================] - 19s 503us/sample - loss: 0.2733 - acc: 0.9150 - val_loss: 0.1614 - val_acc: 0.9527\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2765 - acc: 0.9143\n",
      "Epoch 00044: val_loss did not improve from 0.16141\n",
      "36805/36805 [==============================] - 18s 501us/sample - loss: 0.2765 - acc: 0.9144 - val_loss: 0.1678 - val_acc: 0.9490\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2660 - acc: 0.9154\n",
      "Epoch 00045: val_loss did not improve from 0.16141\n",
      "36805/36805 [==============================] - 18s 500us/sample - loss: 0.2659 - acc: 0.9154 - val_loss: 0.1774 - val_acc: 0.9471\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2640 - acc: 0.9172\n",
      "Epoch 00046: val_loss did not improve from 0.16141\n",
      "36805/36805 [==============================] - 18s 501us/sample - loss: 0.2640 - acc: 0.9172 - val_loss: 0.1651 - val_acc: 0.9513\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2563 - acc: 0.9195\n",
      "Epoch 00047: val_loss did not improve from 0.16141\n",
      "36805/36805 [==============================] - 18s 497us/sample - loss: 0.2564 - acc: 0.9194 - val_loss: 0.1623 - val_acc: 0.9502\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2596 - acc: 0.9184\n",
      "Epoch 00048: val_loss did not improve from 0.16141\n",
      "36805/36805 [==============================] - 18s 497us/sample - loss: 0.2596 - acc: 0.9184 - val_loss: 0.1653 - val_acc: 0.9483\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2550 - acc: 0.9208\n",
      "Epoch 00049: val_loss improved from 0.16141 to 0.16067, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_7_conv_checkpoint/049-0.1607.hdf5\n",
      "36805/36805 [==============================] - 18s 500us/sample - loss: 0.2550 - acc: 0.9208 - val_loss: 0.1607 - val_acc: 0.9513\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2514 - acc: 0.9211\n",
      "Epoch 00050: val_loss did not improve from 0.16067\n",
      "36805/36805 [==============================] - 18s 500us/sample - loss: 0.2513 - acc: 0.9211 - val_loss: 0.1659 - val_acc: 0.9488\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2420 - acc: 0.9240\n",
      "Epoch 00051: val_loss improved from 0.16067 to 0.15564, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_7_conv_checkpoint/051-0.1556.hdf5\n",
      "36805/36805 [==============================] - 19s 504us/sample - loss: 0.2420 - acc: 0.9240 - val_loss: 0.1556 - val_acc: 0.9520\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2429 - acc: 0.9239\n",
      "Epoch 00052: val_loss improved from 0.15564 to 0.15159, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_7_conv_checkpoint/052-0.1516.hdf5\n",
      "36805/36805 [==============================] - 18s 499us/sample - loss: 0.2429 - acc: 0.9238 - val_loss: 0.1516 - val_acc: 0.9520\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2388 - acc: 0.9248\n",
      "Epoch 00053: val_loss improved from 0.15159 to 0.15113, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_7_conv_checkpoint/053-0.1511.hdf5\n",
      "36805/36805 [==============================] - 18s 495us/sample - loss: 0.2389 - acc: 0.9247 - val_loss: 0.1511 - val_acc: 0.9529\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2357 - acc: 0.9262\n",
      "Epoch 00054: val_loss improved from 0.15113 to 0.14759, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_7_conv_checkpoint/054-0.1476.hdf5\n",
      "36805/36805 [==============================] - 18s 496us/sample - loss: 0.2357 - acc: 0.9263 - val_loss: 0.1476 - val_acc: 0.9557\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2303 - acc: 0.9268\n",
      "Epoch 00055: val_loss did not improve from 0.14759\n",
      "36805/36805 [==============================] - 18s 492us/sample - loss: 0.2303 - acc: 0.9268 - val_loss: 0.1533 - val_acc: 0.9529\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2322 - acc: 0.9277\n",
      "Epoch 00056: val_loss did not improve from 0.14759\n",
      "36805/36805 [==============================] - 18s 492us/sample - loss: 0.2324 - acc: 0.9276 - val_loss: 0.1537 - val_acc: 0.9492\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2329 - acc: 0.9271\n",
      "Epoch 00057: val_loss improved from 0.14759 to 0.14658, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_7_conv_checkpoint/057-0.1466.hdf5\n",
      "36805/36805 [==============================] - 18s 498us/sample - loss: 0.2329 - acc: 0.9271 - val_loss: 0.1466 - val_acc: 0.9532\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2251 - acc: 0.9284\n",
      "Epoch 00058: val_loss improved from 0.14658 to 0.14473, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_7_conv_checkpoint/058-0.1447.hdf5\n",
      "36805/36805 [==============================] - 18s 480us/sample - loss: 0.2251 - acc: 0.9284 - val_loss: 0.1447 - val_acc: 0.9553\n",
      "Epoch 59/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2192 - acc: 0.9291\n",
      "Epoch 00059: val_loss did not improve from 0.14473\n",
      "36805/36805 [==============================] - 18s 479us/sample - loss: 0.2195 - acc: 0.9291 - val_loss: 0.1451 - val_acc: 0.9543\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2207 - acc: 0.9301\n",
      "Epoch 00060: val_loss did not improve from 0.14473\n",
      "36805/36805 [==============================] - 18s 476us/sample - loss: 0.2207 - acc: 0.9301 - val_loss: 0.1452 - val_acc: 0.9525\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2181 - acc: 0.9301\n",
      "Epoch 00061: val_loss improved from 0.14473 to 0.14030, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_7_conv_checkpoint/061-0.1403.hdf5\n",
      "36805/36805 [==============================] - 18s 496us/sample - loss: 0.2182 - acc: 0.9300 - val_loss: 0.1403 - val_acc: 0.9555\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2190 - acc: 0.9305\n",
      "Epoch 00062: val_loss did not improve from 0.14030\n",
      "36805/36805 [==============================] - 18s 496us/sample - loss: 0.2190 - acc: 0.9305 - val_loss: 0.1583 - val_acc: 0.9488\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2114 - acc: 0.9330\n",
      "Epoch 00063: val_loss did not improve from 0.14030\n",
      "36805/36805 [==============================] - 18s 493us/sample - loss: 0.2114 - acc: 0.9330 - val_loss: 0.1504 - val_acc: 0.9536\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2124 - acc: 0.9330\n",
      "Epoch 00064: val_loss did not improve from 0.14030\n",
      "36805/36805 [==============================] - 18s 492us/sample - loss: 0.2123 - acc: 0.9330 - val_loss: 0.1462 - val_acc: 0.9541\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2074 - acc: 0.9332\n",
      "Epoch 00065: val_loss did not improve from 0.14030\n",
      "36805/36805 [==============================] - 18s 492us/sample - loss: 0.2073 - acc: 0.9332 - val_loss: 0.1500 - val_acc: 0.9525\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2085 - acc: 0.9348\n",
      "Epoch 00066: val_loss did not improve from 0.14030\n",
      "36805/36805 [==============================] - 18s 489us/sample - loss: 0.2087 - acc: 0.9348 - val_loss: 0.1477 - val_acc: 0.9550\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2046 - acc: 0.9342\n",
      "Epoch 00067: val_loss improved from 0.14030 to 0.13825, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_7_conv_checkpoint/067-0.1383.hdf5\n",
      "36805/36805 [==============================] - 18s 497us/sample - loss: 0.2047 - acc: 0.9342 - val_loss: 0.1383 - val_acc: 0.9571\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2076 - acc: 0.9349\n",
      "Epoch 00068: val_loss did not improve from 0.13825\n",
      "36805/36805 [==============================] - 18s 490us/sample - loss: 0.2076 - acc: 0.9349 - val_loss: 0.1587 - val_acc: 0.9509\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2007 - acc: 0.9367\n",
      "Epoch 00069: val_loss did not improve from 0.13825\n",
      "36805/36805 [==============================] - 18s 489us/sample - loss: 0.2006 - acc: 0.9367 - val_loss: 0.1393 - val_acc: 0.9567\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1957 - acc: 0.9390\n",
      "Epoch 00070: val_loss did not improve from 0.13825\n",
      "36805/36805 [==============================] - 18s 491us/sample - loss: 0.1957 - acc: 0.9389 - val_loss: 0.1633 - val_acc: 0.9481\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1973 - acc: 0.9370\n",
      "Epoch 00071: val_loss did not improve from 0.13825\n",
      "36805/36805 [==============================] - 18s 494us/sample - loss: 0.1974 - acc: 0.9370 - val_loss: 0.1424 - val_acc: 0.9553\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1935 - acc: 0.9379\n",
      "Epoch 00072: val_loss did not improve from 0.13825\n",
      "36805/36805 [==============================] - 18s 500us/sample - loss: 0.1934 - acc: 0.9379 - val_loss: 0.1383 - val_acc: 0.9564\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1933 - acc: 0.9399\n",
      "Epoch 00073: val_loss did not improve from 0.13825\n",
      "36805/36805 [==============================] - 18s 490us/sample - loss: 0.1933 - acc: 0.9399 - val_loss: 0.1396 - val_acc: 0.9571\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1898 - acc: 0.9382\n",
      "Epoch 00074: val_loss did not improve from 0.13825\n",
      "36805/36805 [==============================] - 18s 493us/sample - loss: 0.1897 - acc: 0.9382 - val_loss: 0.1399 - val_acc: 0.9550\n",
      "Epoch 75/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1878 - acc: 0.9414\n",
      "Epoch 00075: val_loss improved from 0.13825 to 0.13394, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_7_conv_checkpoint/075-0.1339.hdf5\n",
      "36805/36805 [==============================] - 18s 493us/sample - loss: 0.1878 - acc: 0.9414 - val_loss: 0.1339 - val_acc: 0.9592\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1827 - acc: 0.9410\n",
      "Epoch 00076: val_loss did not improve from 0.13394\n",
      "36805/36805 [==============================] - 18s 495us/sample - loss: 0.1827 - acc: 0.9410 - val_loss: 0.1399 - val_acc: 0.9555\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1838 - acc: 0.9412\n",
      "Epoch 00077: val_loss did not improve from 0.13394\n",
      "36805/36805 [==============================] - 18s 493us/sample - loss: 0.1838 - acc: 0.9412 - val_loss: 0.1359 - val_acc: 0.9576\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1846 - acc: 0.9400\n",
      "Epoch 00078: val_loss did not improve from 0.13394\n",
      "36805/36805 [==============================] - 18s 492us/sample - loss: 0.1847 - acc: 0.9400 - val_loss: 0.1514 - val_acc: 0.9497\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1860 - acc: 0.9412\n",
      "Epoch 00079: val_loss did not improve from 0.13394\n",
      "36805/36805 [==============================] - 18s 495us/sample - loss: 0.1861 - acc: 0.9411 - val_loss: 0.1350 - val_acc: 0.9599\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1813 - acc: 0.9415\n",
      "Epoch 00080: val_loss did not improve from 0.13394\n",
      "36805/36805 [==============================] - 18s 493us/sample - loss: 0.1813 - acc: 0.9415 - val_loss: 0.1340 - val_acc: 0.9597\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1706 - acc: 0.9456\n",
      "Epoch 00081: val_loss did not improve from 0.13394\n",
      "36805/36805 [==============================] - 18s 494us/sample - loss: 0.1706 - acc: 0.9456 - val_loss: 0.1357 - val_acc: 0.9590\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1780 - acc: 0.9427\n",
      "Epoch 00082: val_loss improved from 0.13394 to 0.13266, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_7_conv_checkpoint/082-0.1327.hdf5\n",
      "36805/36805 [==============================] - 18s 494us/sample - loss: 0.1780 - acc: 0.9427 - val_loss: 0.1327 - val_acc: 0.9581\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1763 - acc: 0.9431\n",
      "Epoch 00083: val_loss did not improve from 0.13266\n",
      "36805/36805 [==============================] - 18s 495us/sample - loss: 0.1763 - acc: 0.9431 - val_loss: 0.1343 - val_acc: 0.9599\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1784 - acc: 0.9427\n",
      "Epoch 00084: val_loss improved from 0.13266 to 0.12527, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_7_conv_checkpoint/084-0.1253.hdf5\n",
      "36805/36805 [==============================] - 18s 496us/sample - loss: 0.1784 - acc: 0.9427 - val_loss: 0.1253 - val_acc: 0.9611\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1727 - acc: 0.9444\n",
      "Epoch 00085: val_loss did not improve from 0.12527\n",
      "36805/36805 [==============================] - 18s 492us/sample - loss: 0.1726 - acc: 0.9444 - val_loss: 0.1285 - val_acc: 0.9611\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1703 - acc: 0.9457\n",
      "Epoch 00086: val_loss did not improve from 0.12527\n",
      "36805/36805 [==============================] - 18s 493us/sample - loss: 0.1703 - acc: 0.9457 - val_loss: 0.1332 - val_acc: 0.9590\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1705 - acc: 0.9456\n",
      "Epoch 00087: val_loss did not improve from 0.12527\n",
      "36805/36805 [==============================] - 18s 492us/sample - loss: 0.1705 - acc: 0.9456 - val_loss: 0.1314 - val_acc: 0.9576\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1686 - acc: 0.9467\n",
      "Epoch 00088: val_loss did not improve from 0.12527\n",
      "36805/36805 [==============================] - 18s 494us/sample - loss: 0.1686 - acc: 0.9467 - val_loss: 0.1300 - val_acc: 0.9595\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1679 - acc: 0.9457\n",
      "Epoch 00089: val_loss did not improve from 0.12527\n",
      "36805/36805 [==============================] - 18s 492us/sample - loss: 0.1679 - acc: 0.9457 - val_loss: 0.1283 - val_acc: 0.9618\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1686 - acc: 0.9458\n",
      "Epoch 00090: val_loss did not improve from 0.12527\n",
      "36805/36805 [==============================] - 18s 493us/sample - loss: 0.1686 - acc: 0.9458 - val_loss: 0.1380 - val_acc: 0.9567\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1613 - acc: 0.9484\n",
      "Epoch 00091: val_loss did not improve from 0.12527\n",
      "36805/36805 [==============================] - 18s 491us/sample - loss: 0.1613 - acc: 0.9484 - val_loss: 0.1428 - val_acc: 0.9550\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1650 - acc: 0.9457\n",
      "Epoch 00092: val_loss did not improve from 0.12527\n",
      "36805/36805 [==============================] - 18s 489us/sample - loss: 0.1650 - acc: 0.9457 - val_loss: 0.1320 - val_acc: 0.9590\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1593 - acc: 0.9492\n",
      "Epoch 00093: val_loss did not improve from 0.12527\n",
      "36805/36805 [==============================] - 18s 488us/sample - loss: 0.1592 - acc: 0.9492 - val_loss: 0.1296 - val_acc: 0.9590\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1581 - acc: 0.9490\n",
      "Epoch 00094: val_loss did not improve from 0.12527\n",
      "36805/36805 [==============================] - 18s 490us/sample - loss: 0.1581 - acc: 0.9491 - val_loss: 0.1317 - val_acc: 0.9578\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1564 - acc: 0.9491\n",
      "Epoch 00095: val_loss did not improve from 0.12527\n",
      "36805/36805 [==============================] - 18s 491us/sample - loss: 0.1564 - acc: 0.9491 - val_loss: 0.1293 - val_acc: 0.9602\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1564 - acc: 0.9499\n",
      "Epoch 00096: val_loss did not improve from 0.12527\n",
      "36805/36805 [==============================] - 18s 489us/sample - loss: 0.1564 - acc: 0.9500 - val_loss: 0.1342 - val_acc: 0.9578\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1537 - acc: 0.9492\n",
      "Epoch 00097: val_loss did not improve from 0.12527\n",
      "36805/36805 [==============================] - 18s 489us/sample - loss: 0.1537 - acc: 0.9492 - val_loss: 0.1289 - val_acc: 0.9597\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1574 - acc: 0.9482\n",
      "Epoch 00098: val_loss did not improve from 0.12527\n",
      "36805/36805 [==============================] - 18s 491us/sample - loss: 0.1574 - acc: 0.9481 - val_loss: 0.1276 - val_acc: 0.9618\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1521 - acc: 0.9504\n",
      "Epoch 00099: val_loss improved from 0.12527 to 0.12347, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_7_conv_checkpoint/099-0.1235.hdf5\n",
      "36805/36805 [==============================] - 18s 493us/sample - loss: 0.1521 - acc: 0.9504 - val_loss: 0.1235 - val_acc: 0.9623\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1500 - acc: 0.9499\n",
      "Epoch 00100: val_loss did not improve from 0.12347\n",
      "36805/36805 [==============================] - 18s 493us/sample - loss: 0.1500 - acc: 0.9499 - val_loss: 0.1251 - val_acc: 0.9618\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1471 - acc: 0.9514\n",
      "Epoch 00101: val_loss did not improve from 0.12347\n",
      "36805/36805 [==============================] - 18s 490us/sample - loss: 0.1473 - acc: 0.9514 - val_loss: 0.1397 - val_acc: 0.9560\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1496 - acc: 0.9516\n",
      "Epoch 00102: val_loss did not improve from 0.12347\n",
      "36805/36805 [==============================] - 18s 492us/sample - loss: 0.1496 - acc: 0.9516 - val_loss: 0.1263 - val_acc: 0.9606\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1479 - acc: 0.9534\n",
      "Epoch 00103: val_loss did not improve from 0.12347\n",
      "36805/36805 [==============================] - 18s 490us/sample - loss: 0.1479 - acc: 0.9534 - val_loss: 0.1339 - val_acc: 0.9567\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1448 - acc: 0.9528\n",
      "Epoch 00104: val_loss did not improve from 0.12347\n",
      "36805/36805 [==============================] - 18s 496us/sample - loss: 0.1448 - acc: 0.9528 - val_loss: 0.1315 - val_acc: 0.9595\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1450 - acc: 0.9525\n",
      "Epoch 00105: val_loss did not improve from 0.12347\n",
      "36805/36805 [==============================] - 18s 494us/sample - loss: 0.1451 - acc: 0.9525 - val_loss: 0.1350 - val_acc: 0.9581\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1489 - acc: 0.9517\n",
      "Epoch 00106: val_loss did not improve from 0.12347\n",
      "36805/36805 [==============================] - 18s 493us/sample - loss: 0.1489 - acc: 0.9517 - val_loss: 0.1294 - val_acc: 0.9611\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1428 - acc: 0.9546\n",
      "Epoch 00107: val_loss did not improve from 0.12347\n",
      "36805/36805 [==============================] - 18s 492us/sample - loss: 0.1428 - acc: 0.9547 - val_loss: 0.1303 - val_acc: 0.9574\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1420 - acc: 0.9543\n",
      "Epoch 00108: val_loss did not improve from 0.12347\n",
      "36805/36805 [==============================] - 18s 491us/sample - loss: 0.1420 - acc: 0.9543 - val_loss: 0.1320 - val_acc: 0.9618\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1363 - acc: 0.9553\n",
      "Epoch 00109: val_loss did not improve from 0.12347\n",
      "36805/36805 [==============================] - 18s 489us/sample - loss: 0.1363 - acc: 0.9553 - val_loss: 0.1283 - val_acc: 0.9569\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1397 - acc: 0.9538\n",
      "Epoch 00110: val_loss did not improve from 0.12347\n",
      "36805/36805 [==============================] - 18s 490us/sample - loss: 0.1397 - acc: 0.9538 - val_loss: 0.1338 - val_acc: 0.9574\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1378 - acc: 0.9554\n",
      "Epoch 00111: val_loss did not improve from 0.12347\n",
      "36805/36805 [==============================] - 18s 491us/sample - loss: 0.1378 - acc: 0.9554 - val_loss: 0.1355 - val_acc: 0.9574\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1423 - acc: 0.9527\n",
      "Epoch 00112: val_loss did not improve from 0.12347\n",
      "36805/36805 [==============================] - 18s 487us/sample - loss: 0.1423 - acc: 0.9527 - val_loss: 0.1296 - val_acc: 0.9618\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1385 - acc: 0.9545\n",
      "Epoch 00113: val_loss did not improve from 0.12347\n",
      "36805/36805 [==============================] - 18s 490us/sample - loss: 0.1385 - acc: 0.9545 - val_loss: 0.1321 - val_acc: 0.9618\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1395 - acc: 0.9550\n",
      "Epoch 00114: val_loss did not improve from 0.12347\n",
      "36805/36805 [==============================] - 18s 491us/sample - loss: 0.1396 - acc: 0.9550 - val_loss: 0.1298 - val_acc: 0.9585\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1341 - acc: 0.9555\n",
      "Epoch 00115: val_loss did not improve from 0.12347\n",
      "36805/36805 [==============================] - 18s 492us/sample - loss: 0.1341 - acc: 0.9555 - val_loss: 0.1282 - val_acc: 0.9613\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1309 - acc: 0.9567\n",
      "Epoch 00116: val_loss did not improve from 0.12347\n",
      "36805/36805 [==============================] - 18s 490us/sample - loss: 0.1309 - acc: 0.9567 - val_loss: 0.1363 - val_acc: 0.9609\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1354 - acc: 0.9559\n",
      "Epoch 00117: val_loss did not improve from 0.12347\n",
      "36805/36805 [==============================] - 18s 489us/sample - loss: 0.1354 - acc: 0.9559 - val_loss: 0.1406 - val_acc: 0.9536\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1330 - acc: 0.9567\n",
      "Epoch 00118: val_loss did not improve from 0.12347\n",
      "36805/36805 [==============================] - 18s 490us/sample - loss: 0.1331 - acc: 0.9566 - val_loss: 0.1349 - val_acc: 0.9585\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1342 - acc: 0.9561\n",
      "Epoch 00119: val_loss did not improve from 0.12347\n",
      "36805/36805 [==============================] - 18s 493us/sample - loss: 0.1342 - acc: 0.9561 - val_loss: 0.1270 - val_acc: 0.9625\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1318 - acc: 0.9572\n",
      "Epoch 00120: val_loss did not improve from 0.12347\n",
      "36805/36805 [==============================] - 18s 490us/sample - loss: 0.1318 - acc: 0.9572 - val_loss: 0.1292 - val_acc: 0.9625\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1290 - acc: 0.9571\n",
      "Epoch 00121: val_loss did not improve from 0.12347\n",
      "36805/36805 [==============================] - 18s 492us/sample - loss: 0.1290 - acc: 0.9571 - val_loss: 0.1285 - val_acc: 0.9592\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1290 - acc: 0.9578\n",
      "Epoch 00122: val_loss improved from 0.12347 to 0.12102, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_7_conv_checkpoint/122-0.1210.hdf5\n",
      "36805/36805 [==============================] - 18s 491us/sample - loss: 0.1290 - acc: 0.9578 - val_loss: 0.1210 - val_acc: 0.9627\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1281 - acc: 0.9577\n",
      "Epoch 00123: val_loss did not improve from 0.12102\n",
      "36805/36805 [==============================] - 18s 493us/sample - loss: 0.1281 - acc: 0.9578 - val_loss: 0.1249 - val_acc: 0.9637\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1327 - acc: 0.9567\n",
      "Epoch 00124: val_loss did not improve from 0.12102\n",
      "36805/36805 [==============================] - 18s 493us/sample - loss: 0.1327 - acc: 0.9567 - val_loss: 0.1314 - val_acc: 0.9620\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1278 - acc: 0.9568\n",
      "Epoch 00125: val_loss did not improve from 0.12102\n",
      "36805/36805 [==============================] - 18s 488us/sample - loss: 0.1278 - acc: 0.9568 - val_loss: 0.1276 - val_acc: 0.9632\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1286 - acc: 0.9575\n",
      "Epoch 00126: val_loss did not improve from 0.12102\n",
      "36805/36805 [==============================] - 18s 489us/sample - loss: 0.1285 - acc: 0.9575 - val_loss: 0.1224 - val_acc: 0.9648\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1232 - acc: 0.9598\n",
      "Epoch 00127: val_loss did not improve from 0.12102\n",
      "36805/36805 [==============================] - 18s 491us/sample - loss: 0.1233 - acc: 0.9598 - val_loss: 0.1220 - val_acc: 0.9613\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1232 - acc: 0.9599\n",
      "Epoch 00128: val_loss did not improve from 0.12102\n",
      "36805/36805 [==============================] - 18s 494us/sample - loss: 0.1231 - acc: 0.9599 - val_loss: 0.1276 - val_acc: 0.9627\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1214 - acc: 0.9591\n",
      "Epoch 00129: val_loss did not improve from 0.12102\n",
      "36805/36805 [==============================] - 18s 490us/sample - loss: 0.1214 - acc: 0.9591 - val_loss: 0.1344 - val_acc: 0.9597\n",
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1185 - acc: 0.9609\n",
      "Epoch 00130: val_loss did not improve from 0.12102\n",
      "36805/36805 [==============================] - 18s 486us/sample - loss: 0.1186 - acc: 0.9608 - val_loss: 0.1300 - val_acc: 0.9602\n",
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1239 - acc: 0.9593\n",
      "Epoch 00131: val_loss did not improve from 0.12102\n",
      "36805/36805 [==============================] - 18s 487us/sample - loss: 0.1240 - acc: 0.9593 - val_loss: 0.1254 - val_acc: 0.9644\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1229 - acc: 0.9603\n",
      "Epoch 00132: val_loss did not improve from 0.12102\n",
      "36805/36805 [==============================] - 18s 490us/sample - loss: 0.1229 - acc: 0.9603 - val_loss: 0.1304 - val_acc: 0.9618\n",
      "Epoch 133/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1171 - acc: 0.9613\n",
      "Epoch 00133: val_loss did not improve from 0.12102\n",
      "36805/36805 [==============================] - 18s 492us/sample - loss: 0.1171 - acc: 0.9613 - val_loss: 0.1314 - val_acc: 0.9623\n",
      "Epoch 134/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1200 - acc: 0.9601\n",
      "Epoch 00134: val_loss did not improve from 0.12102\n",
      "36805/36805 [==============================] - 18s 493us/sample - loss: 0.1200 - acc: 0.9600 - val_loss: 0.1394 - val_acc: 0.9578\n",
      "Epoch 135/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1197 - acc: 0.9605\n",
      "Epoch 00135: val_loss did not improve from 0.12102\n",
      "36805/36805 [==============================] - 18s 498us/sample - loss: 0.1197 - acc: 0.9605 - val_loss: 0.1213 - val_acc: 0.9651\n",
      "Epoch 136/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1193 - acc: 0.9616\n",
      "Epoch 00136: val_loss did not improve from 0.12102\n",
      "36805/36805 [==============================] - 18s 494us/sample - loss: 0.1193 - acc: 0.9616 - val_loss: 0.1309 - val_acc: 0.9613\n",
      "Epoch 137/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1160 - acc: 0.9615\n",
      "Epoch 00137: val_loss did not improve from 0.12102\n",
      "36805/36805 [==============================] - 18s 495us/sample - loss: 0.1160 - acc: 0.9615 - val_loss: 0.1350 - val_acc: 0.9604\n",
      "Epoch 138/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1136 - acc: 0.9629\n",
      "Epoch 00138: val_loss did not improve from 0.12102\n",
      "36805/36805 [==============================] - 18s 493us/sample - loss: 0.1136 - acc: 0.9629 - val_loss: 0.1389 - val_acc: 0.9597\n",
      "Epoch 139/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1196 - acc: 0.9602\n",
      "Epoch 00139: val_loss did not improve from 0.12102\n",
      "36805/36805 [==============================] - 18s 491us/sample - loss: 0.1197 - acc: 0.9602 - val_loss: 0.1365 - val_acc: 0.9616\n",
      "Epoch 140/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1161 - acc: 0.9628\n",
      "Epoch 00140: val_loss did not improve from 0.12102\n",
      "36805/36805 [==============================] - 18s 495us/sample - loss: 0.1161 - acc: 0.9628 - val_loss: 0.1337 - val_acc: 0.9602\n",
      "Epoch 141/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1105 - acc: 0.9638\n",
      "Epoch 00141: val_loss did not improve from 0.12102\n",
      "36805/36805 [==============================] - 18s 493us/sample - loss: 0.1105 - acc: 0.9638 - val_loss: 0.1309 - val_acc: 0.9627\n",
      "Epoch 142/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1131 - acc: 0.9623\n",
      "Epoch 00142: val_loss did not improve from 0.12102\n",
      "36805/36805 [==============================] - 18s 494us/sample - loss: 0.1131 - acc: 0.9623 - val_loss: 0.1337 - val_acc: 0.9595\n",
      "Epoch 143/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1143 - acc: 0.9619\n",
      "Epoch 00143: val_loss did not improve from 0.12102\n",
      "36805/36805 [==============================] - 18s 497us/sample - loss: 0.1143 - acc: 0.9619 - val_loss: 0.1246 - val_acc: 0.9651\n",
      "Epoch 144/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1119 - acc: 0.9625\n",
      "Epoch 00144: val_loss did not improve from 0.12102\n",
      "36805/36805 [==============================] - 18s 492us/sample - loss: 0.1119 - acc: 0.9625 - val_loss: 0.1293 - val_acc: 0.9630\n",
      "Epoch 145/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1125 - acc: 0.9619\n",
      "Epoch 00145: val_loss did not improve from 0.12102\n",
      "36805/36805 [==============================] - 18s 495us/sample - loss: 0.1124 - acc: 0.9619 - val_loss: 0.1325 - val_acc: 0.9620\n",
      "Epoch 146/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1107 - acc: 0.9627\n",
      "Epoch 00146: val_loss did not improve from 0.12102\n",
      "36805/36805 [==============================] - 18s 492us/sample - loss: 0.1108 - acc: 0.9627 - val_loss: 0.1363 - val_acc: 0.9611\n",
      "Epoch 147/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1158 - acc: 0.9611\n",
      "Epoch 00147: val_loss did not improve from 0.12102\n",
      "36805/36805 [==============================] - 18s 492us/sample - loss: 0.1158 - acc: 0.9611 - val_loss: 0.1300 - val_acc: 0.9613\n",
      "Epoch 148/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1097 - acc: 0.9624\n",
      "Epoch 00148: val_loss did not improve from 0.12102\n",
      "36805/36805 [==============================] - 18s 493us/sample - loss: 0.1097 - acc: 0.9625 - val_loss: 0.1363 - val_acc: 0.9576\n",
      "Epoch 149/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1135 - acc: 0.9622\n",
      "Epoch 00149: val_loss did not improve from 0.12102\n",
      "36805/36805 [==============================] - 18s 490us/sample - loss: 0.1137 - acc: 0.9622 - val_loss: 0.1320 - val_acc: 0.9609\n",
      "Epoch 150/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1083 - acc: 0.9643\n",
      "Epoch 00150: val_loss did not improve from 0.12102\n",
      "36805/36805 [==============================] - 18s 488us/sample - loss: 0.1083 - acc: 0.9643 - val_loss: 0.1325 - val_acc: 0.9618\n",
      "Epoch 151/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1050 - acc: 0.9652\n",
      "Epoch 00151: val_loss did not improve from 0.12102\n",
      "36805/36805 [==============================] - 18s 494us/sample - loss: 0.1050 - acc: 0.9652 - val_loss: 0.1333 - val_acc: 0.9625\n",
      "Epoch 152/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1064 - acc: 0.9646\n",
      "Epoch 00152: val_loss did not improve from 0.12102\n",
      "36805/36805 [==============================] - 18s 495us/sample - loss: 0.1064 - acc: 0.9646 - val_loss: 0.1271 - val_acc: 0.9611\n",
      "Epoch 153/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1023 - acc: 0.9664\n",
      "Epoch 00153: val_loss did not improve from 0.12102\n",
      "36805/36805 [==============================] - 18s 493us/sample - loss: 0.1023 - acc: 0.9664 - val_loss: 0.1337 - val_acc: 0.9613\n",
      "Epoch 154/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1047 - acc: 0.9648\n",
      "Epoch 00154: val_loss did not improve from 0.12102\n",
      "36805/36805 [==============================] - 18s 492us/sample - loss: 0.1046 - acc: 0.9648 - val_loss: 0.1329 - val_acc: 0.9618\n",
      "Epoch 155/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1054 - acc: 0.9653\n",
      "Epoch 00155: val_loss did not improve from 0.12102\n",
      "36805/36805 [==============================] - 18s 494us/sample - loss: 0.1054 - acc: 0.9653 - val_loss: 0.1294 - val_acc: 0.9639\n",
      "Epoch 156/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1001 - acc: 0.9672\n",
      "Epoch 00156: val_loss did not improve from 0.12102\n",
      "36805/36805 [==============================] - 18s 491us/sample - loss: 0.1001 - acc: 0.9672 - val_loss: 0.1402 - val_acc: 0.9632\n",
      "Epoch 157/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1080 - acc: 0.9632\n",
      "Epoch 00157: val_loss did not improve from 0.12102\n",
      "36805/36805 [==============================] - 18s 490us/sample - loss: 0.1080 - acc: 0.9632 - val_loss: 0.1414 - val_acc: 0.9583\n",
      "Epoch 158/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1036 - acc: 0.9654\n",
      "Epoch 00158: val_loss did not improve from 0.12102\n",
      "36805/36805 [==============================] - 18s 490us/sample - loss: 0.1036 - acc: 0.9654 - val_loss: 0.1316 - val_acc: 0.9630\n",
      "Epoch 159/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1020 - acc: 0.9651\n",
      "Epoch 00159: val_loss did not improve from 0.12102\n",
      "36805/36805 [==============================] - 18s 491us/sample - loss: 0.1019 - acc: 0.9651 - val_loss: 0.1333 - val_acc: 0.9613\n",
      "Epoch 160/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1020 - acc: 0.9663\n",
      "Epoch 00160: val_loss did not improve from 0.12102\n",
      "36805/36805 [==============================] - 18s 494us/sample - loss: 0.1020 - acc: 0.9663 - val_loss: 0.1312 - val_acc: 0.9625\n",
      "Epoch 161/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0997 - acc: 0.9659\n",
      "Epoch 00161: val_loss did not improve from 0.12102\n",
      "36805/36805 [==============================] - 18s 494us/sample - loss: 0.0996 - acc: 0.9659 - val_loss: 0.1288 - val_acc: 0.9648\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 162/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1012 - acc: 0.9657\n",
      "Epoch 00162: val_loss did not improve from 0.12102\n",
      "36805/36805 [==============================] - 18s 492us/sample - loss: 0.1012 - acc: 0.9657 - val_loss: 0.1341 - val_acc: 0.9616\n",
      "Epoch 163/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0983 - acc: 0.9679\n",
      "Epoch 00163: val_loss did not improve from 0.12102\n",
      "36805/36805 [==============================] - 18s 494us/sample - loss: 0.0984 - acc: 0.9679 - val_loss: 0.1427 - val_acc: 0.9562\n",
      "Epoch 164/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1042 - acc: 0.9662\n",
      "Epoch 00164: val_loss did not improve from 0.12102\n",
      "36805/36805 [==============================] - 18s 494us/sample - loss: 0.1042 - acc: 0.9662 - val_loss: 0.1383 - val_acc: 0.9623\n",
      "Epoch 165/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0971 - acc: 0.9668\n",
      "Epoch 00165: val_loss did not improve from 0.12102\n",
      "36805/36805 [==============================] - 18s 490us/sample - loss: 0.0972 - acc: 0.9668 - val_loss: 0.1313 - val_acc: 0.9613\n",
      "Epoch 166/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0943 - acc: 0.9682\n",
      "Epoch 00166: val_loss did not improve from 0.12102\n",
      "36805/36805 [==============================] - 18s 490us/sample - loss: 0.0944 - acc: 0.9682 - val_loss: 0.1462 - val_acc: 0.9606\n",
      "Epoch 167/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1035 - acc: 0.9662\n",
      "Epoch 00167: val_loss did not improve from 0.12102\n",
      "36805/36805 [==============================] - 18s 493us/sample - loss: 0.1035 - acc: 0.9662 - val_loss: 0.1307 - val_acc: 0.9618\n",
      "Epoch 168/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0969 - acc: 0.9676\n",
      "Epoch 00168: val_loss did not improve from 0.12102\n",
      "36805/36805 [==============================] - 18s 491us/sample - loss: 0.0969 - acc: 0.9676 - val_loss: 0.1377 - val_acc: 0.9611\n",
      "Epoch 169/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0990 - acc: 0.9668\n",
      "Epoch 00169: val_loss did not improve from 0.12102\n",
      "36805/36805 [==============================] - 18s 489us/sample - loss: 0.0991 - acc: 0.9668 - val_loss: 0.1342 - val_acc: 0.9620\n",
      "Epoch 170/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0979 - acc: 0.9665\n",
      "Epoch 00170: val_loss did not improve from 0.12102\n",
      "36805/36805 [==============================] - 18s 492us/sample - loss: 0.0979 - acc: 0.9665 - val_loss: 0.1335 - val_acc: 0.9630\n",
      "Epoch 171/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0940 - acc: 0.9679\n",
      "Epoch 00171: val_loss did not improve from 0.12102\n",
      "36805/36805 [==============================] - 18s 488us/sample - loss: 0.0940 - acc: 0.9679 - val_loss: 0.1419 - val_acc: 0.9630\n",
      "Epoch 172/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0944 - acc: 0.9681\n",
      "Epoch 00172: val_loss did not improve from 0.12102\n",
      "36805/36805 [==============================] - 18s 490us/sample - loss: 0.0944 - acc: 0.9681 - val_loss: 0.1286 - val_acc: 0.9634\n",
      "\n",
      "1D_CNN_custom_multi_2_GMP_ch_32_DO_7_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 317us/sample - loss: 0.1875 - acc: 0.9456\n",
      "Loss: 0.18745066056710785 Accuracy: 0.9455867\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.4047 - acc: 0.2185\n",
      "Epoch 00001: val_loss improved from inf to 1.63711, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_8_conv_checkpoint/001-1.6371.hdf5\n",
      "36805/36805 [==============================] - 23s 623us/sample - loss: 2.4046 - acc: 0.2185 - val_loss: 1.6371 - val_acc: 0.5132\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6074 - acc: 0.4836\n",
      "Epoch 00002: val_loss improved from 1.63711 to 1.16170, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_8_conv_checkpoint/002-1.1617.hdf5\n",
      "36805/36805 [==============================] - 18s 493us/sample - loss: 1.6073 - acc: 0.4836 - val_loss: 1.1617 - val_acc: 0.6420\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2610 - acc: 0.5968\n",
      "Epoch 00003: val_loss improved from 1.16170 to 0.89219, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_8_conv_checkpoint/003-0.8922.hdf5\n",
      "36805/36805 [==============================] - 18s 494us/sample - loss: 1.2610 - acc: 0.5968 - val_loss: 0.8922 - val_acc: 0.7319\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0731 - acc: 0.6562\n",
      "Epoch 00004: val_loss improved from 0.89219 to 0.78013, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_8_conv_checkpoint/004-0.7801.hdf5\n",
      "36805/36805 [==============================] - 18s 495us/sample - loss: 1.0732 - acc: 0.6562 - val_loss: 0.7801 - val_acc: 0.7657\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9352 - acc: 0.7028\n",
      "Epoch 00005: val_loss improved from 0.78013 to 0.66044, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_8_conv_checkpoint/005-0.6604.hdf5\n",
      "36805/36805 [==============================] - 18s 496us/sample - loss: 0.9351 - acc: 0.7028 - val_loss: 0.6604 - val_acc: 0.8090\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8334 - acc: 0.7343\n",
      "Epoch 00006: val_loss improved from 0.66044 to 0.56210, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_8_conv_checkpoint/006-0.5621.hdf5\n",
      "36805/36805 [==============================] - 18s 497us/sample - loss: 0.8333 - acc: 0.7343 - val_loss: 0.5621 - val_acc: 0.8358\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7617 - acc: 0.7567\n",
      "Epoch 00007: val_loss improved from 0.56210 to 0.55332, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_8_conv_checkpoint/007-0.5533.hdf5\n",
      "36805/36805 [==============================] - 18s 501us/sample - loss: 0.7616 - acc: 0.7567 - val_loss: 0.5533 - val_acc: 0.8423\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6928 - acc: 0.7804\n",
      "Epoch 00008: val_loss improved from 0.55332 to 0.46253, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_8_conv_checkpoint/008-0.4625.hdf5\n",
      "36805/36805 [==============================] - 18s 499us/sample - loss: 0.6928 - acc: 0.7803 - val_loss: 0.4625 - val_acc: 0.8654\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6378 - acc: 0.7996\n",
      "Epoch 00009: val_loss improved from 0.46253 to 0.43341, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_8_conv_checkpoint/009-0.4334.hdf5\n",
      "36805/36805 [==============================] - 18s 495us/sample - loss: 0.6378 - acc: 0.7996 - val_loss: 0.4334 - val_acc: 0.8744\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5966 - acc: 0.8112\n",
      "Epoch 00010: val_loss did not improve from 0.43341\n",
      "36805/36805 [==============================] - 18s 495us/sample - loss: 0.5966 - acc: 0.8112 - val_loss: 0.4361 - val_acc: 0.8730\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5547 - acc: 0.8245\n",
      "Epoch 00011: val_loss improved from 0.43341 to 0.37532, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_8_conv_checkpoint/011-0.3753.hdf5\n",
      "36805/36805 [==============================] - 18s 494us/sample - loss: 0.5547 - acc: 0.8245 - val_loss: 0.3753 - val_acc: 0.8898\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5229 - acc: 0.8339\n",
      "Epoch 00012: val_loss did not improve from 0.37532\n",
      "36805/36805 [==============================] - 18s 495us/sample - loss: 0.5230 - acc: 0.8339 - val_loss: 0.3764 - val_acc: 0.8880\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5017 - acc: 0.8410\n",
      "Epoch 00013: val_loss improved from 0.37532 to 0.35940, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_8_conv_checkpoint/013-0.3594.hdf5\n",
      "36805/36805 [==============================] - 18s 499us/sample - loss: 0.5017 - acc: 0.8410 - val_loss: 0.3594 - val_acc: 0.8938\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4743 - acc: 0.8500\n",
      "Epoch 00014: val_loss improved from 0.35940 to 0.31865, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_8_conv_checkpoint/014-0.3187.hdf5\n",
      "36805/36805 [==============================] - 18s 496us/sample - loss: 0.4742 - acc: 0.8500 - val_loss: 0.3187 - val_acc: 0.9045\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4571 - acc: 0.8568\n",
      "Epoch 00015: val_loss did not improve from 0.31865\n",
      "36805/36805 [==============================] - 18s 492us/sample - loss: 0.4571 - acc: 0.8568 - val_loss: 0.3293 - val_acc: 0.9024\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4375 - acc: 0.8615\n",
      "Epoch 00016: val_loss improved from 0.31865 to 0.30079, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_8_conv_checkpoint/016-0.3008.hdf5\n",
      "36805/36805 [==============================] - 18s 492us/sample - loss: 0.4376 - acc: 0.8615 - val_loss: 0.3008 - val_acc: 0.9087\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4158 - acc: 0.8696\n",
      "Epoch 00017: val_loss improved from 0.30079 to 0.28636, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_8_conv_checkpoint/017-0.2864.hdf5\n",
      "36805/36805 [==============================] - 18s 499us/sample - loss: 0.4157 - acc: 0.8696 - val_loss: 0.2864 - val_acc: 0.9147\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3994 - acc: 0.8726\n",
      "Epoch 00018: val_loss improved from 0.28636 to 0.27780, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_8_conv_checkpoint/018-0.2778.hdf5\n",
      "36805/36805 [==============================] - 18s 497us/sample - loss: 0.3994 - acc: 0.8726 - val_loss: 0.2778 - val_acc: 0.9166\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3860 - acc: 0.8780\n",
      "Epoch 00019: val_loss did not improve from 0.27780\n",
      "36805/36805 [==============================] - 18s 493us/sample - loss: 0.3860 - acc: 0.8780 - val_loss: 0.2828 - val_acc: 0.9085\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3729 - acc: 0.8817\n",
      "Epoch 00020: val_loss improved from 0.27780 to 0.27172, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_8_conv_checkpoint/020-0.2717.hdf5\n",
      "36805/36805 [==============================] - 18s 497us/sample - loss: 0.3729 - acc: 0.8817 - val_loss: 0.2717 - val_acc: 0.9161\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3616 - acc: 0.8851\n",
      "Epoch 00021: val_loss improved from 0.27172 to 0.25588, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_8_conv_checkpoint/021-0.2559.hdf5\n",
      "36805/36805 [==============================] - 18s 495us/sample - loss: 0.3615 - acc: 0.8852 - val_loss: 0.2559 - val_acc: 0.9215\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3455 - acc: 0.8896\n",
      "Epoch 00022: val_loss improved from 0.25588 to 0.24976, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_8_conv_checkpoint/022-0.2498.hdf5\n",
      "36805/36805 [==============================] - 18s 498us/sample - loss: 0.3456 - acc: 0.8896 - val_loss: 0.2498 - val_acc: 0.9250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3386 - acc: 0.8929\n",
      "Epoch 00023: val_loss did not improve from 0.24976\n",
      "36805/36805 [==============================] - 18s 495us/sample - loss: 0.3386 - acc: 0.8929 - val_loss: 0.2625 - val_acc: 0.9164\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3261 - acc: 0.8968\n",
      "Epoch 00024: val_loss improved from 0.24976 to 0.24598, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_8_conv_checkpoint/024-0.2460.hdf5\n",
      "36805/36805 [==============================] - 18s 497us/sample - loss: 0.3261 - acc: 0.8968 - val_loss: 0.2460 - val_acc: 0.9206\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3233 - acc: 0.8977\n",
      "Epoch 00025: val_loss improved from 0.24598 to 0.24062, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_8_conv_checkpoint/025-0.2406.hdf5\n",
      "36805/36805 [==============================] - 18s 496us/sample - loss: 0.3233 - acc: 0.8977 - val_loss: 0.2406 - val_acc: 0.9222\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3116 - acc: 0.8999\n",
      "Epoch 00026: val_loss improved from 0.24062 to 0.23005, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_8_conv_checkpoint/026-0.2300.hdf5\n",
      "36805/36805 [==============================] - 18s 498us/sample - loss: 0.3116 - acc: 0.8999 - val_loss: 0.2300 - val_acc: 0.9306\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3026 - acc: 0.9042\n",
      "Epoch 00027: val_loss did not improve from 0.23005\n",
      "36805/36805 [==============================] - 18s 494us/sample - loss: 0.3026 - acc: 0.9042 - val_loss: 0.2405 - val_acc: 0.9222\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2878 - acc: 0.9104\n",
      "Epoch 00028: val_loss improved from 0.23005 to 0.21050, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_8_conv_checkpoint/028-0.2105.hdf5\n",
      "36805/36805 [==============================] - 18s 499us/sample - loss: 0.2878 - acc: 0.9104 - val_loss: 0.2105 - val_acc: 0.9345\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2853 - acc: 0.9113\n",
      "Epoch 00029: val_loss did not improve from 0.21050\n",
      "36805/36805 [==============================] - 18s 491us/sample - loss: 0.2854 - acc: 0.9113 - val_loss: 0.2202 - val_acc: 0.9350\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2757 - acc: 0.9117\n",
      "Epoch 00030: val_loss improved from 0.21050 to 0.20514, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_8_conv_checkpoint/030-0.2051.hdf5\n",
      "36805/36805 [==============================] - 18s 497us/sample - loss: 0.2758 - acc: 0.9117 - val_loss: 0.2051 - val_acc: 0.9401\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2663 - acc: 0.9156\n",
      "Epoch 00031: val_loss did not improve from 0.20514\n",
      "36805/36805 [==============================] - 18s 491us/sample - loss: 0.2663 - acc: 0.9156 - val_loss: 0.2237 - val_acc: 0.9283\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2636 - acc: 0.9160\n",
      "Epoch 00032: val_loss did not improve from 0.20514\n",
      "36805/36805 [==============================] - 18s 495us/sample - loss: 0.2636 - acc: 0.9160 - val_loss: 0.2249 - val_acc: 0.9329\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2582 - acc: 0.9179\n",
      "Epoch 00033: val_loss improved from 0.20514 to 0.19298, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_8_conv_checkpoint/033-0.1930.hdf5\n",
      "36805/36805 [==============================] - 18s 500us/sample - loss: 0.2582 - acc: 0.9179 - val_loss: 0.1930 - val_acc: 0.9397\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2509 - acc: 0.9210\n",
      "Epoch 00034: val_loss improved from 0.19298 to 0.19114, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_8_conv_checkpoint/034-0.1911.hdf5\n",
      "36805/36805 [==============================] - 18s 500us/sample - loss: 0.2509 - acc: 0.9210 - val_loss: 0.1911 - val_acc: 0.9411\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2454 - acc: 0.9216\n",
      "Epoch 00035: val_loss did not improve from 0.19114\n",
      "36805/36805 [==============================] - 18s 494us/sample - loss: 0.2453 - acc: 0.9216 - val_loss: 0.1922 - val_acc: 0.9401\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2407 - acc: 0.9226\n",
      "Epoch 00036: val_loss improved from 0.19114 to 0.18235, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_8_conv_checkpoint/036-0.1824.hdf5\n",
      "36805/36805 [==============================] - 18s 496us/sample - loss: 0.2407 - acc: 0.9226 - val_loss: 0.1824 - val_acc: 0.9425\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2361 - acc: 0.9252\n",
      "Epoch 00037: val_loss improved from 0.18235 to 0.18128, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_8_conv_checkpoint/037-0.1813.hdf5\n",
      "36805/36805 [==============================] - 18s 493us/sample - loss: 0.2361 - acc: 0.9252 - val_loss: 0.1813 - val_acc: 0.9422\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2264 - acc: 0.9274\n",
      "Epoch 00038: val_loss improved from 0.18128 to 0.17933, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_8_conv_checkpoint/038-0.1793.hdf5\n",
      "36805/36805 [==============================] - 18s 496us/sample - loss: 0.2264 - acc: 0.9274 - val_loss: 0.1793 - val_acc: 0.9441\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2261 - acc: 0.9270\n",
      "Epoch 00039: val_loss did not improve from 0.17933\n",
      "36805/36805 [==============================] - 18s 495us/sample - loss: 0.2261 - acc: 0.9270 - val_loss: 0.1905 - val_acc: 0.9427\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2172 - acc: 0.9301\n",
      "Epoch 00040: val_loss improved from 0.17933 to 0.17641, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_8_conv_checkpoint/040-0.1764.hdf5\n",
      "36805/36805 [==============================] - 18s 498us/sample - loss: 0.2172 - acc: 0.9301 - val_loss: 0.1764 - val_acc: 0.9439\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2123 - acc: 0.9315\n",
      "Epoch 00041: val_loss improved from 0.17641 to 0.17328, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_8_conv_checkpoint/041-0.1733.hdf5\n",
      "36805/36805 [==============================] - 18s 499us/sample - loss: 0.2122 - acc: 0.9315 - val_loss: 0.1733 - val_acc: 0.9460\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2075 - acc: 0.9328\n",
      "Epoch 00042: val_loss improved from 0.17328 to 0.17326, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_8_conv_checkpoint/042-0.1733.hdf5\n",
      "36805/36805 [==============================] - 18s 496us/sample - loss: 0.2075 - acc: 0.9328 - val_loss: 0.1733 - val_acc: 0.9488\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2053 - acc: 0.9330\n",
      "Epoch 00043: val_loss improved from 0.17326 to 0.16744, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_8_conv_checkpoint/043-0.1674.hdf5\n",
      "36805/36805 [==============================] - 18s 495us/sample - loss: 0.2053 - acc: 0.9330 - val_loss: 0.1674 - val_acc: 0.9483\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1990 - acc: 0.9354\n",
      "Epoch 00044: val_loss did not improve from 0.16744\n",
      "36805/36805 [==============================] - 18s 496us/sample - loss: 0.1990 - acc: 0.9354 - val_loss: 0.1707 - val_acc: 0.9453\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2008 - acc: 0.9355\n",
      "Epoch 00045: val_loss did not improve from 0.16744\n",
      "36805/36805 [==============================] - 18s 496us/sample - loss: 0.2008 - acc: 0.9355 - val_loss: 0.1681 - val_acc: 0.9492\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1900 - acc: 0.9370\n",
      "Epoch 00046: val_loss did not improve from 0.16744\n",
      "36805/36805 [==============================] - 18s 493us/sample - loss: 0.1899 - acc: 0.9370 - val_loss: 0.1732 - val_acc: 0.9448\n",
      "Epoch 47/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1925 - acc: 0.9368\n",
      "Epoch 00047: val_loss did not improve from 0.16744\n",
      "36805/36805 [==============================] - 18s 496us/sample - loss: 0.1925 - acc: 0.9369 - val_loss: 0.1716 - val_acc: 0.9464\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1845 - acc: 0.9396\n",
      "Epoch 00048: val_loss did not improve from 0.16744\n",
      "36805/36805 [==============================] - 18s 495us/sample - loss: 0.1845 - acc: 0.9396 - val_loss: 0.1695 - val_acc: 0.9464\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1827 - acc: 0.9405\n",
      "Epoch 00049: val_loss improved from 0.16744 to 0.16458, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_8_conv_checkpoint/049-0.1646.hdf5\n",
      "36805/36805 [==============================] - 18s 494us/sample - loss: 0.1827 - acc: 0.9406 - val_loss: 0.1646 - val_acc: 0.9471\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1759 - acc: 0.9433\n",
      "Epoch 00050: val_loss did not improve from 0.16458\n",
      "36805/36805 [==============================] - 18s 499us/sample - loss: 0.1759 - acc: 0.9433 - val_loss: 0.1656 - val_acc: 0.9490\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1770 - acc: 0.9429\n",
      "Epoch 00051: val_loss did not improve from 0.16458\n",
      "36805/36805 [==============================] - 18s 493us/sample - loss: 0.1771 - acc: 0.9429 - val_loss: 0.1720 - val_acc: 0.9483\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1692 - acc: 0.9454\n",
      "Epoch 00052: val_loss did not improve from 0.16458\n",
      "36805/36805 [==============================] - 18s 496us/sample - loss: 0.1693 - acc: 0.9454 - val_loss: 0.1765 - val_acc: 0.9455\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1749 - acc: 0.9421\n",
      "Epoch 00053: val_loss improved from 0.16458 to 0.16419, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_8_conv_checkpoint/053-0.1642.hdf5\n",
      "36805/36805 [==============================] - 18s 497us/sample - loss: 0.1750 - acc: 0.9421 - val_loss: 0.1642 - val_acc: 0.9506\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1676 - acc: 0.9446\n",
      "Epoch 00054: val_loss improved from 0.16419 to 0.16186, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_8_conv_checkpoint/054-0.1619.hdf5\n",
      "36805/36805 [==============================] - 18s 497us/sample - loss: 0.1676 - acc: 0.9446 - val_loss: 0.1619 - val_acc: 0.9485\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1673 - acc: 0.9453\n",
      "Epoch 00055: val_loss did not improve from 0.16186\n",
      "36805/36805 [==============================] - 18s 496us/sample - loss: 0.1672 - acc: 0.9453 - val_loss: 0.1632 - val_acc: 0.9492\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1600 - acc: 0.9465\n",
      "Epoch 00056: val_loss did not improve from 0.16186\n",
      "36805/36805 [==============================] - 18s 492us/sample - loss: 0.1600 - acc: 0.9465 - val_loss: 0.1630 - val_acc: 0.9518\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1566 - acc: 0.9488\n",
      "Epoch 00057: val_loss did not improve from 0.16186\n",
      "36805/36805 [==============================] - 18s 494us/sample - loss: 0.1566 - acc: 0.9488 - val_loss: 0.1675 - val_acc: 0.9492\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1534 - acc: 0.9501\n",
      "Epoch 00058: val_loss improved from 0.16186 to 0.15581, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_8_conv_checkpoint/058-0.1558.hdf5\n",
      "36805/36805 [==============================] - 18s 500us/sample - loss: 0.1534 - acc: 0.9501 - val_loss: 0.1558 - val_acc: 0.9548\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1551 - acc: 0.9489\n",
      "Epoch 00059: val_loss did not improve from 0.15581\n",
      "36805/36805 [==============================] - 18s 499us/sample - loss: 0.1551 - acc: 0.9489 - val_loss: 0.1858 - val_acc: 0.9436\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1486 - acc: 0.9523\n",
      "Epoch 00060: val_loss did not improve from 0.15581\n",
      "36805/36805 [==============================] - 18s 499us/sample - loss: 0.1486 - acc: 0.9523 - val_loss: 0.1601 - val_acc: 0.9495\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1498 - acc: 0.9509\n",
      "Epoch 00061: val_loss did not improve from 0.15581\n",
      "36805/36805 [==============================] - 18s 499us/sample - loss: 0.1497 - acc: 0.9509 - val_loss: 0.1646 - val_acc: 0.9511\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1466 - acc: 0.9525\n",
      "Epoch 00062: val_loss did not improve from 0.15581\n",
      "36805/36805 [==============================] - 18s 498us/sample - loss: 0.1466 - acc: 0.9525 - val_loss: 0.1573 - val_acc: 0.9553\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1467 - acc: 0.9522\n",
      "Epoch 00063: val_loss improved from 0.15581 to 0.14974, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_8_conv_checkpoint/063-0.1497.hdf5\n",
      "36805/36805 [==============================] - 18s 500us/sample - loss: 0.1467 - acc: 0.9522 - val_loss: 0.1497 - val_acc: 0.9529\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1401 - acc: 0.9533\n",
      "Epoch 00064: val_loss did not improve from 0.14974\n",
      "36805/36805 [==============================] - 18s 501us/sample - loss: 0.1401 - acc: 0.9533 - val_loss: 0.1567 - val_acc: 0.9509\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1377 - acc: 0.9539\n",
      "Epoch 00065: val_loss did not improve from 0.14974\n",
      "36805/36805 [==============================] - 18s 498us/sample - loss: 0.1377 - acc: 0.9539 - val_loss: 0.1535 - val_acc: 0.9541\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1348 - acc: 0.9560\n",
      "Epoch 00066: val_loss did not improve from 0.14974\n",
      "36805/36805 [==============================] - 18s 496us/sample - loss: 0.1348 - acc: 0.9560 - val_loss: 0.1564 - val_acc: 0.9536\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1342 - acc: 0.9557\n",
      "Epoch 00067: val_loss did not improve from 0.14974\n",
      "36805/36805 [==============================] - 18s 498us/sample - loss: 0.1342 - acc: 0.9557 - val_loss: 0.1504 - val_acc: 0.9539\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1330 - acc: 0.9561\n",
      "Epoch 00068: val_loss did not improve from 0.14974\n",
      "36805/36805 [==============================] - 18s 499us/sample - loss: 0.1330 - acc: 0.9561 - val_loss: 0.1583 - val_acc: 0.9543\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1329 - acc: 0.9560\n",
      "Epoch 00069: val_loss did not improve from 0.14974\n",
      "36805/36805 [==============================] - 18s 497us/sample - loss: 0.1329 - acc: 0.9560 - val_loss: 0.1726 - val_acc: 0.9464\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1299 - acc: 0.9566\n",
      "Epoch 00070: val_loss did not improve from 0.14974\n",
      "36805/36805 [==============================] - 18s 499us/sample - loss: 0.1299 - acc: 0.9566 - val_loss: 0.1620 - val_acc: 0.9511\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1271 - acc: 0.9582\n",
      "Epoch 00071: val_loss did not improve from 0.14974\n",
      "36805/36805 [==============================] - 18s 496us/sample - loss: 0.1271 - acc: 0.9582 - val_loss: 0.1582 - val_acc: 0.9518\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1233 - acc: 0.9593\n",
      "Epoch 00072: val_loss did not improve from 0.14974\n",
      "36805/36805 [==============================] - 18s 497us/sample - loss: 0.1233 - acc: 0.9593 - val_loss: 0.1582 - val_acc: 0.9543\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1223 - acc: 0.9602\n",
      "Epoch 00073: val_loss did not improve from 0.14974\n",
      "36805/36805 [==============================] - 18s 497us/sample - loss: 0.1223 - acc: 0.9602 - val_loss: 0.1521 - val_acc: 0.9529\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1199 - acc: 0.9609\n",
      "Epoch 00074: val_loss did not improve from 0.14974\n",
      "36805/36805 [==============================] - 18s 500us/sample - loss: 0.1199 - acc: 0.9609 - val_loss: 0.1558 - val_acc: 0.9522\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1204 - acc: 0.9601\n",
      "Epoch 00075: val_loss did not improve from 0.14974\n",
      "36805/36805 [==============================] - 18s 498us/sample - loss: 0.1204 - acc: 0.9601 - val_loss: 0.1507 - val_acc: 0.9525\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1168 - acc: 0.9613\n",
      "Epoch 00076: val_loss did not improve from 0.14974\n",
      "36805/36805 [==============================] - 18s 493us/sample - loss: 0.1168 - acc: 0.9613 - val_loss: 0.1586 - val_acc: 0.9513\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1119 - acc: 0.9624\n",
      "Epoch 00077: val_loss did not improve from 0.14974\n",
      "36805/36805 [==============================] - 18s 494us/sample - loss: 0.1119 - acc: 0.9624 - val_loss: 0.1546 - val_acc: 0.9548\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1134 - acc: 0.9621\n",
      "Epoch 00078: val_loss did not improve from 0.14974\n",
      "36805/36805 [==============================] - 18s 497us/sample - loss: 0.1134 - acc: 0.9622 - val_loss: 0.1540 - val_acc: 0.9520\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1141 - acc: 0.9621\n",
      "Epoch 00079: val_loss did not improve from 0.14974\n",
      "36805/36805 [==============================] - 18s 496us/sample - loss: 0.1141 - acc: 0.9621 - val_loss: 0.1665 - val_acc: 0.9529\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1073 - acc: 0.9640\n",
      "Epoch 00080: val_loss did not improve from 0.14974\n",
      "36805/36805 [==============================] - 18s 495us/sample - loss: 0.1073 - acc: 0.9640 - val_loss: 0.1538 - val_acc: 0.9553\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1096 - acc: 0.9632\n",
      "Epoch 00081: val_loss did not improve from 0.14974\n",
      "36805/36805 [==============================] - 18s 495us/sample - loss: 0.1096 - acc: 0.9632 - val_loss: 0.1576 - val_acc: 0.9527\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1077 - acc: 0.9643\n",
      "Epoch 00082: val_loss did not improve from 0.14974\n",
      "36805/36805 [==============================] - 18s 496us/sample - loss: 0.1077 - acc: 0.9643 - val_loss: 0.1580 - val_acc: 0.9539\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1088 - acc: 0.9639\n",
      "Epoch 00083: val_loss did not improve from 0.14974\n",
      "36805/36805 [==============================] - 18s 497us/sample - loss: 0.1088 - acc: 0.9639 - val_loss: 0.1641 - val_acc: 0.9509\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1033 - acc: 0.9651\n",
      "Epoch 00084: val_loss did not improve from 0.14974\n",
      "36805/36805 [==============================] - 18s 493us/sample - loss: 0.1033 - acc: 0.9651 - val_loss: 0.1574 - val_acc: 0.9543\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1051 - acc: 0.9651\n",
      "Epoch 00085: val_loss did not improve from 0.14974\n",
      "36805/36805 [==============================] - 18s 494us/sample - loss: 0.1051 - acc: 0.9650 - val_loss: 0.1694 - val_acc: 0.9522\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1063 - acc: 0.9648\n",
      "Epoch 00086: val_loss did not improve from 0.14974\n",
      "36805/36805 [==============================] - 18s 499us/sample - loss: 0.1063 - acc: 0.9648 - val_loss: 0.1679 - val_acc: 0.9529\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1005 - acc: 0.9669\n",
      "Epoch 00087: val_loss improved from 0.14974 to 0.14904, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_ch_32_DO_8_conv_checkpoint/087-0.1490.hdf5\n",
      "36805/36805 [==============================] - 19s 503us/sample - loss: 0.1005 - acc: 0.9669 - val_loss: 0.1490 - val_acc: 0.9541\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1001 - acc: 0.9654\n",
      "Epoch 00088: val_loss did not improve from 0.14904\n",
      "36805/36805 [==============================] - 18s 499us/sample - loss: 0.1001 - acc: 0.9654 - val_loss: 0.1509 - val_acc: 0.9574\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0983 - acc: 0.9672\n",
      "Epoch 00089: val_loss did not improve from 0.14904\n",
      "36805/36805 [==============================] - 18s 496us/sample - loss: 0.0984 - acc: 0.9672 - val_loss: 0.1661 - val_acc: 0.9557\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0973 - acc: 0.9670\n",
      "Epoch 00090: val_loss did not improve from 0.14904\n",
      "36805/36805 [==============================] - 18s 502us/sample - loss: 0.0973 - acc: 0.9670 - val_loss: 0.1608 - val_acc: 0.9541\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0971 - acc: 0.9676\n",
      "Epoch 00091: val_loss did not improve from 0.14904\n",
      "36805/36805 [==============================] - 18s 499us/sample - loss: 0.0971 - acc: 0.9676 - val_loss: 0.1537 - val_acc: 0.9562\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0942 - acc: 0.9681\n",
      "Epoch 00092: val_loss did not improve from 0.14904\n",
      "36805/36805 [==============================] - 18s 500us/sample - loss: 0.0942 - acc: 0.9681 - val_loss: 0.1568 - val_acc: 0.9555\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0937 - acc: 0.9689\n",
      "Epoch 00093: val_loss did not improve from 0.14904\n",
      "36805/36805 [==============================] - 18s 501us/sample - loss: 0.0936 - acc: 0.9689 - val_loss: 0.1630 - val_acc: 0.9548\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0895 - acc: 0.9694\n",
      "Epoch 00094: val_loss did not improve from 0.14904\n",
      "36805/36805 [==============================] - 18s 500us/sample - loss: 0.0896 - acc: 0.9694 - val_loss: 0.1730 - val_acc: 0.9548\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0936 - acc: 0.9693\n",
      "Epoch 00095: val_loss did not improve from 0.14904\n",
      "36805/36805 [==============================] - 18s 495us/sample - loss: 0.0936 - acc: 0.9694 - val_loss: 0.1663 - val_acc: 0.9555\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0854 - acc: 0.9720\n",
      "Epoch 00096: val_loss did not improve from 0.14904\n",
      "36805/36805 [==============================] - 18s 502us/sample - loss: 0.0854 - acc: 0.9720 - val_loss: 0.1646 - val_acc: 0.9585\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0902 - acc: 0.9696\n",
      "Epoch 00097: val_loss did not improve from 0.14904\n",
      "36805/36805 [==============================] - 18s 500us/sample - loss: 0.0902 - acc: 0.9697 - val_loss: 0.1568 - val_acc: 0.9555\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0874 - acc: 0.9708\n",
      "Epoch 00098: val_loss did not improve from 0.14904\n",
      "36805/36805 [==============================] - 18s 495us/sample - loss: 0.0874 - acc: 0.9708 - val_loss: 0.1632 - val_acc: 0.9571\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0875 - acc: 0.9703\n",
      "Epoch 00099: val_loss did not improve from 0.14904\n",
      "36805/36805 [==============================] - 18s 498us/sample - loss: 0.0875 - acc: 0.9703 - val_loss: 0.1583 - val_acc: 0.9583\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0829 - acc: 0.9702\n",
      "Epoch 00100: val_loss did not improve from 0.14904\n",
      "36805/36805 [==============================] - 18s 501us/sample - loss: 0.0829 - acc: 0.9702 - val_loss: 0.1747 - val_acc: 0.9548\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0830 - acc: 0.9722\n",
      "Epoch 00101: val_loss did not improve from 0.14904\n",
      "36805/36805 [==============================] - 18s 496us/sample - loss: 0.0831 - acc: 0.9722 - val_loss: 0.1643 - val_acc: 0.9567\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0890 - acc: 0.9696\n",
      "Epoch 00102: val_loss did not improve from 0.14904\n",
      "36805/36805 [==============================] - 18s 499us/sample - loss: 0.0889 - acc: 0.9696 - val_loss: 0.1684 - val_acc: 0.9571\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0784 - acc: 0.9732\n",
      "Epoch 00103: val_loss did not improve from 0.14904\n",
      "36805/36805 [==============================] - 18s 498us/sample - loss: 0.0784 - acc: 0.9732 - val_loss: 0.1604 - val_acc: 0.9574\n",
      "Epoch 104/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0794 - acc: 0.9728\n",
      "Epoch 00104: val_loss did not improve from 0.14904\n",
      "36805/36805 [==============================] - 18s 496us/sample - loss: 0.0794 - acc: 0.9728 - val_loss: 0.1752 - val_acc: 0.9564\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0823 - acc: 0.9720\n",
      "Epoch 00105: val_loss did not improve from 0.14904\n",
      "36805/36805 [==============================] - 19s 504us/sample - loss: 0.0823 - acc: 0.9720 - val_loss: 0.1831 - val_acc: 0.9532\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0833 - acc: 0.9717\n",
      "Epoch 00106: val_loss did not improve from 0.14904\n",
      "36805/36805 [==============================] - 18s 499us/sample - loss: 0.0833 - acc: 0.9717 - val_loss: 0.1661 - val_acc: 0.9567\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0750 - acc: 0.9739\n",
      "Epoch 00107: val_loss did not improve from 0.14904\n",
      "36805/36805 [==============================] - 18s 498us/sample - loss: 0.0750 - acc: 0.9739 - val_loss: 0.1655 - val_acc: 0.9555\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0780 - acc: 0.9739\n",
      "Epoch 00108: val_loss did not improve from 0.14904\n",
      "36805/36805 [==============================] - 18s 498us/sample - loss: 0.0780 - acc: 0.9739 - val_loss: 0.1697 - val_acc: 0.9543\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0749 - acc: 0.9747\n",
      "Epoch 00109: val_loss did not improve from 0.14904\n",
      "36805/36805 [==============================] - 18s 498us/sample - loss: 0.0749 - acc: 0.9747 - val_loss: 0.1839 - val_acc: 0.9527\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0779 - acc: 0.9728\n",
      "Epoch 00110: val_loss did not improve from 0.14904\n",
      "36805/36805 [==============================] - 18s 497us/sample - loss: 0.0779 - acc: 0.9728 - val_loss: 0.1619 - val_acc: 0.9576\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0718 - acc: 0.9747\n",
      "Epoch 00111: val_loss did not improve from 0.14904\n",
      "36805/36805 [==============================] - 18s 497us/sample - loss: 0.0718 - acc: 0.9747 - val_loss: 0.1842 - val_acc: 0.9539\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0732 - acc: 0.9753\n",
      "Epoch 00112: val_loss did not improve from 0.14904\n",
      "36805/36805 [==============================] - 18s 499us/sample - loss: 0.0732 - acc: 0.9753 - val_loss: 0.1799 - val_acc: 0.9534\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0722 - acc: 0.9751\n",
      "Epoch 00113: val_loss did not improve from 0.14904\n",
      "36805/36805 [==============================] - 18s 503us/sample - loss: 0.0723 - acc: 0.9750 - val_loss: 0.1959 - val_acc: 0.9511\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0704 - acc: 0.9753\n",
      "Epoch 00114: val_loss did not improve from 0.14904\n",
      "36805/36805 [==============================] - 18s 500us/sample - loss: 0.0703 - acc: 0.9753 - val_loss: 0.1960 - val_acc: 0.9546\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0741 - acc: 0.9751\n",
      "Epoch 00115: val_loss did not improve from 0.14904\n",
      "36805/36805 [==============================] - 18s 498us/sample - loss: 0.0741 - acc: 0.9751 - val_loss: 0.1684 - val_acc: 0.9553\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0685 - acc: 0.9767\n",
      "Epoch 00116: val_loss did not improve from 0.14904\n",
      "36805/36805 [==============================] - 18s 500us/sample - loss: 0.0684 - acc: 0.9767 - val_loss: 0.1912 - val_acc: 0.9527\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0733 - acc: 0.9749\n",
      "Epoch 00117: val_loss did not improve from 0.14904\n",
      "36805/36805 [==============================] - 18s 500us/sample - loss: 0.0733 - acc: 0.9749 - val_loss: 0.1631 - val_acc: 0.9588\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0678 - acc: 0.9764\n",
      "Epoch 00118: val_loss did not improve from 0.14904\n",
      "36805/36805 [==============================] - 18s 500us/sample - loss: 0.0678 - acc: 0.9764 - val_loss: 0.1827 - val_acc: 0.9557\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0668 - acc: 0.9778\n",
      "Epoch 00119: val_loss did not improve from 0.14904\n",
      "36805/36805 [==============================] - 18s 501us/sample - loss: 0.0668 - acc: 0.9778 - val_loss: 0.1684 - val_acc: 0.9569\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0692 - acc: 0.9762\n",
      "Epoch 00120: val_loss did not improve from 0.14904\n",
      "36805/36805 [==============================] - 18s 496us/sample - loss: 0.0692 - acc: 0.9762 - val_loss: 0.1759 - val_acc: 0.9562\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0665 - acc: 0.9768\n",
      "Epoch 00121: val_loss did not improve from 0.14904\n",
      "36805/36805 [==============================] - 18s 500us/sample - loss: 0.0665 - acc: 0.9769 - val_loss: 0.1798 - val_acc: 0.9567\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0629 - acc: 0.9779\n",
      "Epoch 00122: val_loss did not improve from 0.14904\n",
      "36805/36805 [==============================] - 18s 499us/sample - loss: 0.0629 - acc: 0.9779 - val_loss: 0.1767 - val_acc: 0.9539\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0671 - acc: 0.9771\n",
      "Epoch 00123: val_loss did not improve from 0.14904\n",
      "36805/36805 [==============================] - 18s 499us/sample - loss: 0.0671 - acc: 0.9771 - val_loss: 0.1771 - val_acc: 0.9557\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0660 - acc: 0.9784\n",
      "Epoch 00124: val_loss did not improve from 0.14904\n",
      "36805/36805 [==============================] - 18s 497us/sample - loss: 0.0659 - acc: 0.9784 - val_loss: 0.1895 - val_acc: 0.9541\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0634 - acc: 0.9783\n",
      "Epoch 00125: val_loss did not improve from 0.14904\n",
      "36805/36805 [==============================] - 18s 497us/sample - loss: 0.0634 - acc: 0.9783 - val_loss: 0.1692 - val_acc: 0.9555\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0653 - acc: 0.9769\n",
      "Epoch 00126: val_loss did not improve from 0.14904\n",
      "36805/36805 [==============================] - 18s 496us/sample - loss: 0.0653 - acc: 0.9769 - val_loss: 0.1905 - val_acc: 0.9539\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0615 - acc: 0.9785\n",
      "Epoch 00127: val_loss did not improve from 0.14904\n",
      "36805/36805 [==============================] - 18s 499us/sample - loss: 0.0615 - acc: 0.9785 - val_loss: 0.1914 - val_acc: 0.9546\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0617 - acc: 0.9787\n",
      "Epoch 00128: val_loss did not improve from 0.14904\n",
      "36805/36805 [==============================] - 18s 501us/sample - loss: 0.0617 - acc: 0.9787 - val_loss: 0.1962 - val_acc: 0.9548\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0618 - acc: 0.9793\n",
      "Epoch 00129: val_loss did not improve from 0.14904\n",
      "36805/36805 [==============================] - 18s 502us/sample - loss: 0.0619 - acc: 0.9792 - val_loss: 0.1855 - val_acc: 0.9541\n",
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0632 - acc: 0.9780\n",
      "Epoch 00130: val_loss did not improve from 0.14904\n",
      "36805/36805 [==============================] - 18s 503us/sample - loss: 0.0632 - acc: 0.9780 - val_loss: 0.1901 - val_acc: 0.9548\n",
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0646 - acc: 0.9768\n",
      "Epoch 00131: val_loss did not improve from 0.14904\n",
      "36805/36805 [==============================] - 18s 503us/sample - loss: 0.0646 - acc: 0.9768 - val_loss: 0.1899 - val_acc: 0.9557\n",
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0596 - acc: 0.9794\n",
      "Epoch 00132: val_loss did not improve from 0.14904\n",
      "36805/36805 [==============================] - 18s 501us/sample - loss: 0.0596 - acc: 0.9794 - val_loss: 0.2070 - val_acc: 0.9495\n",
      "Epoch 133/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0634 - acc: 0.9791\n",
      "Epoch 00133: val_loss did not improve from 0.14904\n",
      "36805/36805 [==============================] - 18s 502us/sample - loss: 0.0634 - acc: 0.9791 - val_loss: 0.1939 - val_acc: 0.9534\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 134/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0565 - acc: 0.9808\n",
      "Epoch 00134: val_loss did not improve from 0.14904\n",
      "36805/36805 [==============================] - 19s 504us/sample - loss: 0.0565 - acc: 0.9808 - val_loss: 0.1926 - val_acc: 0.9592\n",
      "Epoch 135/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0559 - acc: 0.9809\n",
      "Epoch 00135: val_loss did not improve from 0.14904\n",
      "36805/36805 [==============================] - 18s 501us/sample - loss: 0.0559 - acc: 0.9809 - val_loss: 0.1946 - val_acc: 0.9564\n",
      "Epoch 136/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0590 - acc: 0.9795\n",
      "Epoch 00136: val_loss did not improve from 0.14904\n",
      "36805/36805 [==============================] - 18s 500us/sample - loss: 0.0590 - acc: 0.9795 - val_loss: 0.1924 - val_acc: 0.9585\n",
      "Epoch 137/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0549 - acc: 0.9814\n",
      "Epoch 00137: val_loss did not improve from 0.14904\n",
      "36805/36805 [==============================] - 18s 499us/sample - loss: 0.0549 - acc: 0.9814 - val_loss: 0.2054 - val_acc: 0.9543\n",
      "\n",
      "1D_CNN_custom_multi_2_GMP_ch_32_DO_8_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 322us/sample - loss: 0.2199 - acc: 0.9383\n",
      "Loss: 0.21994790246058352 Accuracy: 0.9383178\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base = '1D_CNN_custom_multi_2_GMP_ch_32_DO'\n",
    "\n",
    "for i in range(3, 9):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    model = build_cnn(conv_num=i)\n",
    "#         model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4),\n",
    "          metrics=['accuracy'])\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model_filename = model_path+'{epoch:03d}-{val_loss:.4f}.hdf5'\n",
    "    checkpointer = ModelCheckpoint(filepath = model_filename, monitor = \"val_loss\", \n",
    "                                   verbose=1, save_best_only=True)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=50)\n",
    "    hist = model.fit(x_train_abs, y_train_onehot, batch_size=64, epochs=500, \n",
    "                     validation_data=[x_val_abs, y_val_onehot], shuffle=True, \n",
    "                     callbacks = [checkpointer, early_stopping])\n",
    "\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "    ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "    ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "    ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    png_path = 'visualization/learning_curve/'\n",
    "    filename = model_name+'.png'\n",
    "    os.makedirs(png_path, exist_ok=True)\n",
    "    fig.savefig(png_path+filename, transparent=True)\n",
    "\n",
    "    model.save(model_path+'000_last.hdf5')\n",
    "    del(model)\n",
    "\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "    model = load_model(model_filename)\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "    print()\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_33 (Conv1D)              (None, 16000, 32)    192         input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 16000, 32)    0           conv1d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling1D) (None, 5333, 32)     0           activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_34 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_33[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 5333, 32)     0           conv1d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling1D) (None, 1777, 32)     0           activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_35 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 1777, 32)     0           conv1d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling1D) (None, 592, 32)      0           activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_12 (Global (None, 32)           0           max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_13 (Global (None, 32)           0           max_pooling1d_35[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 64)           0           global_max_pooling1d_12[0][0]    \n",
      "                                                                 global_max_pooling1d_13[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 64)           0           concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 16)           1040        dropout_6[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 11,536\n",
      "Trainable params: 11,536\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 1s 290us/sample - loss: 0.8415 - acc: 0.7468\n",
      "Loss: 0.8415316059829537 Accuracy: 0.7468328\n",
      "\n",
      "1D_CNN_custom_multi_2_GMP_ch_32_DO_4_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_8 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_36 (Conv1D)              (None, 16000, 32)    192         input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 16000, 32)    0           conv1d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling1D) (None, 5333, 32)     0           activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_37 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_36[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 5333, 32)     0           conv1d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling1D) (None, 1777, 32)     0           activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_38 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_37[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 1777, 32)     0           conv1d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling1D) (None, 592, 32)      0           activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_39 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 592, 32)      0           conv1d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling1D) (None, 197, 32)      0           activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_14 (Global (None, 32)           0           max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_15 (Global (None, 32)           0           max_pooling1d_39[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 64)           0           global_max_pooling1d_14[0][0]    \n",
      "                                                                 global_max_pooling1d_15[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 64)           0           concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 16)           1040        dropout_7[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 16,688\n",
      "Trainable params: 16,688\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 1s 295us/sample - loss: 0.5608 - acc: 0.8289\n",
      "Loss: 0.5607636648422709 Accuracy: 0.8288681\n",
      "\n",
      "1D_CNN_custom_multi_2_GMP_ch_32_DO_5_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_40 (Conv1D)              (None, 16000, 32)    192         input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 16000, 32)    0           conv1d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling1D) (None, 5333, 32)     0           activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_41 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_40[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 5333, 32)     0           conv1d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling1D) (None, 1777, 32)     0           activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_42 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_41[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 1777, 32)     0           conv1d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling1D) (None, 592, 32)      0           activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_43 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_42[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 592, 32)      0           conv1d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling1D) (None, 197, 32)      0           activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_44 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 197, 64)      0           conv1d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling1D) (None, 65, 64)       0           activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_16 (Global (None, 32)           0           max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_17 (Global (None, 64)           0           max_pooling1d_44[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 96)           0           global_max_pooling1d_16[0][0]    \n",
      "                                                                 global_max_pooling1d_17[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 96)           0           concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 16)           1552        dropout_8[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 27,504\n",
      "Trainable params: 27,504\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 322us/sample - loss: 0.3142 - acc: 0.9055\n",
      "Loss: 0.31418762652673454 Accuracy: 0.90550363\n",
      "\n",
      "1D_CNN_custom_multi_2_GMP_ch_32_DO_6_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_10 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_45 (Conv1D)              (None, 16000, 32)    192         input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 16000, 32)    0           conv1d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling1D) (None, 5333, 32)     0           activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_46 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_45[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 5333, 32)     0           conv1d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling1D) (None, 1777, 32)     0           activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_47 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_46[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 1777, 32)     0           conv1d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling1D) (None, 592, 32)      0           activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_48 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_47[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 592, 32)      0           conv1d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling1D) (None, 197, 32)      0           activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_49 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_48[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 197, 64)      0           conv1d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling1D) (None, 65, 64)       0           activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_50 (Conv1D)              (None, 65, 64)       20544       max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 65, 64)       0           conv1d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling1D) (None, 21, 64)       0           activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_18 (Global (None, 64)           0           max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_19 (Global (None, 64)           0           max_pooling1d_50[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 128)          0           global_max_pooling1d_18[0][0]    \n",
      "                                                                 global_max_pooling1d_19[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 128)          0           concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 16)           2064        dropout_9[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 48,560\n",
      "Trainable params: 48,560\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 2s 321us/sample - loss: 0.1952 - acc: 0.9383\n",
      "Loss: 0.19517268415304484 Accuracy: 0.9383178\n",
      "\n",
      "1D_CNN_custom_multi_2_GMP_ch_32_DO_7_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_51 (Conv1D)              (None, 16000, 32)    192         input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 16000, 32)    0           conv1d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling1D) (None, 5333, 32)     0           activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_52 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_51[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 5333, 32)     0           conv1d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling1D) (None, 1777, 32)     0           activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_53 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_52[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 1777, 32)     0           conv1d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling1D) (None, 592, 32)      0           activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_54 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_53[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 592, 32)      0           conv1d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling1D) (None, 197, 32)      0           activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_55 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_54[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 197, 64)      0           conv1d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling1D) (None, 65, 64)       0           activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_56 (Conv1D)              (None, 65, 64)       20544       max_pooling1d_55[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 65, 64)       0           conv1d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling1D) (None, 21, 64)       0           activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_57 (Conv1D)              (None, 21, 64)       20544       max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 21, 64)       0           conv1d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling1D) (None, 7, 64)        0           activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_20 (Global (None, 64)           0           max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_21 (Global (None, 64)           0           max_pooling1d_57[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 128)          0           global_max_pooling1d_20[0][0]    \n",
      "                                                                 global_max_pooling1d_21[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 128)          0           concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 16)           2064        dropout_10[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 69,104\n",
      "Trainable params: 69,104\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 339us/sample - loss: 0.1875 - acc: 0.9456\n",
      "Loss: 0.18745066056710785 Accuracy: 0.9455867\n",
      "\n",
      "1D_CNN_custom_multi_2_GMP_ch_32_DO_8_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_12 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_58 (Conv1D)              (None, 16000, 32)    192         input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 16000, 32)    0           conv1d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling1D) (None, 5333, 32)     0           activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_59 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_58[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 5333, 32)     0           conv1d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling1D) (None, 1777, 32)     0           activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_60 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_59[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 1777, 32)     0           conv1d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling1D) (None, 592, 32)      0           activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_61 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_60[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 592, 32)      0           conv1d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling1D) (None, 197, 32)      0           activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_62 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_61[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 197, 64)      0           conv1d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling1D) (None, 65, 64)       0           activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_63 (Conv1D)              (None, 65, 64)       20544       max_pooling1d_62[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 65, 64)       0           conv1d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling1D) (None, 21, 64)       0           activation_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_64 (Conv1D)              (None, 21, 64)       20544       max_pooling1d_63[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 21, 64)       0           conv1d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling1D) (None, 7, 64)        0           activation_64[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_65 (Conv1D)              (None, 7, 64)        20544       max_pooling1d_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 7, 64)        0           conv1d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling1D) (None, 2, 64)        0           activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_22 (Global (None, 64)           0           max_pooling1d_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_23 (Global (None, 64)           0           max_pooling1d_65[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 128)          0           global_max_pooling1d_22[0][0]    \n",
      "                                                                 global_max_pooling1d_23[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 128)          0           concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 16)           2064        dropout_11[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 89,648\n",
      "Trainable params: 89,648\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 2s 370us/sample - loss: 0.2199 - acc: 0.9383\n",
      "Loss: 0.21994790246058352 Accuracy: 0.9383178\n"
     ]
    }
   ],
   "source": [
    "log_dir = 'log'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "base = '1D_CNN_custom_multi_2_GMP_ch_32_DO'\n",
    "\n",
    "with open(path.join(log_dir, base), 'w') as log_file:\n",
    "    for i in range(3, 9):\n",
    "        model_name = base+'_{}_conv'.format(i)\n",
    "        print()\n",
    "        print(model_name, 'Model')\n",
    "        model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "        model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "\n",
    "        model = load_model(model_filename)\n",
    "        model.summary()\n",
    "\n",
    "        [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "        print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "        del(model)\n",
    "\n",
    "        log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_multi_2_GMP_ch_32_DO_3_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_33 (Conv1D)              (None, 16000, 32)    192         input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 16000, 32)    0           conv1d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling1D) (None, 5333, 32)     0           activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_34 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_33[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 5333, 32)     0           conv1d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling1D) (None, 1777, 32)     0           activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_35 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 1777, 32)     0           conv1d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling1D) (None, 592, 32)      0           activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_12 (Global (None, 32)           0           max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_13 (Global (None, 32)           0           max_pooling1d_35[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 64)           0           global_max_pooling1d_12[0][0]    \n",
      "                                                                 global_max_pooling1d_13[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 64)           0           concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 16)           1040        dropout_6[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 11,536\n",
      "Trainable params: 11,536\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 2s 338us/sample - loss: 0.8463 - acc: 0.7441\n",
      "Loss: 0.8462791951769733 Accuracy: 0.74413294\n",
      "\n",
      "1D_CNN_custom_multi_2_GMP_ch_32_DO_4_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_8 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_36 (Conv1D)              (None, 16000, 32)    192         input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 16000, 32)    0           conv1d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling1D) (None, 5333, 32)     0           activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_37 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_36[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 5333, 32)     0           conv1d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling1D) (None, 1777, 32)     0           activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_38 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_37[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 1777, 32)     0           conv1d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling1D) (None, 592, 32)      0           activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_39 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 592, 32)      0           conv1d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling1D) (None, 197, 32)      0           activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_14 (Global (None, 32)           0           max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_15 (Global (None, 32)           0           max_pooling1d_39[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 64)           0           global_max_pooling1d_14[0][0]    \n",
      "                                                                 global_max_pooling1d_15[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 64)           0           concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 16)           1040        dropout_7[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 16,688\n",
      "Trainable params: 16,688\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 2s 363us/sample - loss: 0.5649 - acc: 0.8285\n",
      "Loss: 0.5648519297255162 Accuracy: 0.82845277\n",
      "\n",
      "1D_CNN_custom_multi_2_GMP_ch_32_DO_5_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_40 (Conv1D)              (None, 16000, 32)    192         input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 16000, 32)    0           conv1d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling1D) (None, 5333, 32)     0           activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_41 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_40[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 5333, 32)     0           conv1d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling1D) (None, 1777, 32)     0           activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_42 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_41[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 1777, 32)     0           conv1d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling1D) (None, 592, 32)      0           activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_43 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_42[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 592, 32)      0           conv1d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling1D) (None, 197, 32)      0           activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_44 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 197, 64)      0           conv1d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling1D) (None, 65, 64)       0           activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_16 (Global (None, 32)           0           max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_17 (Global (None, 64)           0           max_pooling1d_44[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 96)           0           global_max_pooling1d_16[0][0]    \n",
      "                                                                 global_max_pooling1d_17[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 96)           0           concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 16)           1552        dropout_8[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 27,504\n",
      "Trainable params: 27,504\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 368us/sample - loss: 0.3168 - acc: 0.9063\n",
      "Loss: 0.3167691239439191 Accuracy: 0.9063344\n",
      "\n",
      "1D_CNN_custom_multi_2_GMP_ch_32_DO_6_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_10 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_45 (Conv1D)              (None, 16000, 32)    192         input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 16000, 32)    0           conv1d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling1D) (None, 5333, 32)     0           activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_46 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_45[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 5333, 32)     0           conv1d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling1D) (None, 1777, 32)     0           activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_47 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_46[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 1777, 32)     0           conv1d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling1D) (None, 592, 32)      0           activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_48 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_47[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 592, 32)      0           conv1d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling1D) (None, 197, 32)      0           activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_49 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_48[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 197, 64)      0           conv1d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling1D) (None, 65, 64)       0           activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_50 (Conv1D)              (None, 65, 64)       20544       max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 65, 64)       0           conv1d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling1D) (None, 21, 64)       0           activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_18 (Global (None, 64)           0           max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_19 (Global (None, 64)           0           max_pooling1d_50[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 128)          0           global_max_pooling1d_18[0][0]    \n",
      "                                                                 global_max_pooling1d_19[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 128)          0           concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 16)           2064        dropout_9[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 48,560\n",
      "Trainable params: 48,560\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 2s 406us/sample - loss: 0.1984 - acc: 0.9406\n",
      "Loss: 0.19838035502166393 Accuracy: 0.9406023\n",
      "\n",
      "1D_CNN_custom_multi_2_GMP_ch_32_DO_7_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_51 (Conv1D)              (None, 16000, 32)    192         input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 16000, 32)    0           conv1d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling1D) (None, 5333, 32)     0           activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_52 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_51[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 5333, 32)     0           conv1d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling1D) (None, 1777, 32)     0           activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_53 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_52[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 1777, 32)     0           conv1d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling1D) (None, 592, 32)      0           activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_54 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_53[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 592, 32)      0           conv1d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling1D) (None, 197, 32)      0           activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_55 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_54[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 197, 64)      0           conv1d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling1D) (None, 65, 64)       0           activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_56 (Conv1D)              (None, 65, 64)       20544       max_pooling1d_55[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 65, 64)       0           conv1d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling1D) (None, 21, 64)       0           activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_57 (Conv1D)              (None, 21, 64)       20544       max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 21, 64)       0           conv1d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling1D) (None, 7, 64)        0           activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_20 (Global (None, 64)           0           max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_21 (Global (None, 64)           0           max_pooling1d_57[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 128)          0           global_max_pooling1d_20[0][0]    \n",
      "                                                                 global_max_pooling1d_21[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 128)          0           concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 16)           2064        dropout_10[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 69,104\n",
      "Trainable params: 69,104\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 374us/sample - loss: 0.2123 - acc: 0.9454\n",
      "Loss: 0.21231936488536535 Accuracy: 0.945379\n",
      "\n",
      "1D_CNN_custom_multi_2_GMP_ch_32_DO_8_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_12 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_58 (Conv1D)              (None, 16000, 32)    192         input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 16000, 32)    0           conv1d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling1D) (None, 5333, 32)     0           activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_59 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_58[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 5333, 32)     0           conv1d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling1D) (None, 1777, 32)     0           activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_60 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_59[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 1777, 32)     0           conv1d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling1D) (None, 592, 32)      0           activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_61 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_60[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 592, 32)      0           conv1d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling1D) (None, 197, 32)      0           activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_62 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_61[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 197, 64)      0           conv1d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling1D) (None, 65, 64)       0           activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_63 (Conv1D)              (None, 65, 64)       20544       max_pooling1d_62[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 65, 64)       0           conv1d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling1D) (None, 21, 64)       0           activation_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_64 (Conv1D)              (None, 21, 64)       20544       max_pooling1d_63[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 21, 64)       0           conv1d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling1D) (None, 7, 64)        0           activation_64[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_65 (Conv1D)              (None, 7, 64)        20544       max_pooling1d_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 7, 64)        0           conv1d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling1D) (None, 2, 64)        0           activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_22 (Global (None, 64)           0           max_pooling1d_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_23 (Global (None, 64)           0           max_pooling1d_65[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 128)          0           global_max_pooling1d_22[0][0]    \n",
      "                                                                 global_max_pooling1d_23[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 128)          0           concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 16)           2064        dropout_11[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 89,648\n",
      "Trainable params: 89,648\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 2s 428us/sample - loss: 0.2750 - acc: 0.9416\n",
      "Loss: 0.27496681712153886 Accuracy: 0.94164073\n"
     ]
    }
   ],
   "source": [
    "for i in range(3, 9):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + '000_last.hdf5'\n",
    "\n",
    "    model = load_model(model_filename)\n",
    "    model.summary()\n",
    "\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
