{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path as path\n",
    "import itertools\n",
    "from sklearn.preprocessing import maxabs_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Input, Dense, Activation, BatchNormalization, \\\n",
    "                                    Flatten, Conv1D, MaxPooling1D, Dropout, \\\n",
    "                                    Concatenate, GlobalMaxPool1D, GlobalAvgPool1D\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler, \\\n",
    "                                        EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = path.join('..', 'data')\n",
    "data_dir = path.join(base_dir, 'data_speech_commands_v0.02')\n",
    " \n",
    "train_txt = path.join(data_dir, 'wav_train_16words.txt')\n",
    "val_txt = path.join(data_dir, 'wav_validation_16words.txt')\n",
    "test_txt = path.join(data_dir, 'wav_test_16words.txt')\n",
    "\n",
    "train_data = np.load(path.join(data_dir, 'wav_train_data.npz'))\n",
    "val_data = np.load(path.join(data_dir, 'wav_validation_data.npz'))\n",
    "test_data = np.load(path.join(data_dir, 'wav_test_data.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36805, 16000, 1),\n",
       " (36805,),\n",
       " (4293, 16000, 1),\n",
       " (4293,),\n",
       " (4815, 16000, 1),\n",
       " (4815,),\n",
       " (16, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = train_data['x_train']\n",
    "y_train = train_data['y_train']\n",
    "x_val = val_data['x_val']\n",
    "y_val = val_data['y_val']\n",
    "x_test = test_data['x_test']\n",
    "y_test = test_data['y_test']\n",
    "y_table = test_data['table']\n",
    "\n",
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape, y_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_test[0].shape\n",
    "output_size = y_table.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_abs = np.asarray([maxabs_scale(wav) for wav in x_train])\n",
    "y_train_onehot = np.asarray([to_categorical(label, output_size) for label in y_train])\n",
    "del x_train, y_train\n",
    "\n",
    "x_val_abs = np.asarray([maxabs_scale(wav) for wav in x_val])\n",
    "y_val_onehot = np.asarray([to_categorical(label, output_size) for label in y_val])\n",
    "del x_val, y_val\n",
    "\n",
    "x_test_abs = np.asarray([maxabs_scale(wav) for wav in x_test])\n",
    "y_test_onehot = np.asarray([to_categorical(label, output_size) for label in y_test])\n",
    "del x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn(conv_num=1):\n",
    "    filter_size = 32\n",
    "\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    x = input_layer\n",
    "\n",
    "    layer_outputs = []\n",
    "    for i in range(conv_num):\n",
    "        x = Conv1D (kernel_size=5, filters=filter_size*(2**(i//4)), \n",
    "                          strides=1, padding='same')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('relu')(x)\n",
    "        x = MaxPooling1D(pool_size=3, strides=3)(x)\n",
    "        layer_outputs.append(x)    \n",
    "    \n",
    "    x = Concatenate()([GlobalAvgPool1D()(output) for output in layer_outputs[-2:]])\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(output_size, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 16000, 32)    192         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1 (BatchNo (None, 16000, 32)    128         conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 16000, 32)    0           batch_normalization_v1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D)    (None, 5333, 32)     0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 5333, 32)     5152        max_pooling1d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_1 (Batch (None, 5333, 32)     128         conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 5333, 32)     0           batch_normalization_v1_1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 1777, 32)     0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 1777, 32)     5152        max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_2 (Batch (None, 1777, 32)     128         conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 1777, 32)     0           batch_normalization_v1_2[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 592, 32)      0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d (Globa (None, 32)           0           max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glo (None, 32)           0           max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 64)           0           global_average_pooling1d[0][0]   \n",
      "                                                                 global_average_pooling1d_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 64)           0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 16)           1040        dropout[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 11,920\n",
      "Trainable params: 11,728\n",
      "Non-trainable params: 192\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 16000, 32)    192         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_3 (Batch (None, 16000, 32)    128         conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 16000, 32)    0           batch_normalization_v1_3[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 5333, 32)     0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 5333, 32)     5152        max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_4 (Batch (None, 5333, 32)     128         conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 5333, 32)     0           batch_normalization_v1_4[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 1777, 32)     0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 1777, 32)     5152        max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_5 (Batch (None, 1777, 32)     128         conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 1777, 32)     0           batch_normalization_v1_5[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 592, 32)      0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 592, 32)      5152        max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_6 (Batch (None, 592, 32)      128         conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 592, 32)      0           batch_normalization_v1_6[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1D)  (None, 197, 32)      0           activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_2 (Glo (None, 32)           0           max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_3 (Glo (None, 32)           0           max_pooling1d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 64)           0           global_average_pooling1d_2[0][0] \n",
      "                                                                 global_average_pooling1d_3[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 64)           0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 16)           1040        dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 17,200\n",
      "Trainable params: 16,944\n",
      "Non-trainable params: 256\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 16000, 32)    192         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_7 (Batch (None, 16000, 32)    128         conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 16000, 32)    0           batch_normalization_v1_7[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1D)  (None, 5333, 32)     0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 5333, 32)     5152        max_pooling1d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_8 (Batch (None, 5333, 32)     128         conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 5333, 32)     0           batch_normalization_v1_8[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1D)  (None, 1777, 32)     0           activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 1777, 32)     5152        max_pooling1d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_9 (Batch (None, 1777, 32)     128         conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 1777, 32)     0           batch_normalization_v1_9[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1D)  (None, 592, 32)      0           activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_10 (Batc (None, 592, 32)      128         conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 592, 32)      0           batch_normalization_v1_10[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling1D) (None, 197, 32)      0           activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_11 (Batc (None, 197, 64)      256         conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 197, 64)      0           batch_normalization_v1_11[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling1D) (None, 65, 64)       0           activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_4 (Glo (None, 32)           0           max_pooling1d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_5 (Glo (None, 64)           0           max_pooling1d_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 96)           0           global_average_pooling1d_4[0][0] \n",
      "                                                                 global_average_pooling1d_5[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 96)           0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 16)           1552        dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 28,272\n",
      "Trainable params: 27,888\n",
      "Non-trainable params: 384\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 16000, 32)    192         input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_12 (Batc (None, 16000, 32)    128         conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_12[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling1D) (None, 5333, 32)     0           activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_13 (Batc (None, 5333, 32)     128         conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_13[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling1D) (None, 1777, 32)     0           activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_13[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_14 (Batc (None, 1777, 32)     128         conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_14[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling1D) (None, 592, 32)      0           activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_14[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_15 (Batc (None, 592, 32)      128         conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 592, 32)      0           batch_normalization_v1_15[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling1D) (None, 197, 32)      0           activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_15[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_16 (Batc (None, 197, 64)      256         conv1d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 197, 64)      0           batch_normalization_v1_16[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling1D) (None, 65, 64)       0           activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_17 (Conv1D)              (None, 65, 64)       20544       max_pooling1d_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_17 (Batc (None, 65, 64)       256         conv1d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 65, 64)       0           batch_normalization_v1_17[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling1D) (None, 21, 64)       0           activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_6 (Glo (None, 64)           0           max_pooling1d_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_7 (Glo (None, 64)           0           max_pooling1d_17[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 128)          0           global_average_pooling1d_6[0][0] \n",
      "                                                                 global_average_pooling1d_7[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 128)          0           concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 16)           2064        dropout_3[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 49,584\n",
      "Trainable params: 49,072\n",
      "Non-trainable params: 512\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_18 (Conv1D)              (None, 16000, 32)    192         input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_18 (Batc (None, 16000, 32)    128         conv1d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_18[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling1D) (None, 5333, 32)     0           activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_19 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_18[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_19 (Batc (None, 5333, 32)     128         conv1d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_19[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling1D) (None, 1777, 32)     0           activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_20 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_19[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_20 (Batc (None, 1777, 32)     128         conv1d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_20[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling1D) (None, 592, 32)      0           activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_21 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_20[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_21 (Batc (None, 592, 32)      128         conv1d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 592, 32)      0           batch_normalization_v1_21[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling1D) (None, 197, 32)      0           activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_22 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_21[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_22 (Batc (None, 197, 64)      256         conv1d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 197, 64)      0           batch_normalization_v1_22[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling1D) (None, 65, 64)       0           activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_23 (Conv1D)              (None, 65, 64)       20544       max_pooling1d_22[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_23 (Batc (None, 65, 64)       256         conv1d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 65, 64)       0           batch_normalization_v1_23[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling1D) (None, 21, 64)       0           activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_24 (Conv1D)              (None, 21, 64)       20544       max_pooling1d_23[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_24 (Batc (None, 21, 64)       256         conv1d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 21, 64)       0           batch_normalization_v1_24[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling1D) (None, 7, 64)        0           activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_8 (Glo (None, 64)           0           max_pooling1d_23[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_9 (Glo (None, 64)           0           max_pooling1d_24[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 128)          0           global_average_pooling1d_8[0][0] \n",
      "                                                                 global_average_pooling1d_9[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 128)          0           concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 16)           2064        dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 70,384\n",
      "Trainable params: 69,744\n",
      "Non-trainable params: 640\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_25 (Conv1D)              (None, 16000, 32)    192         input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_25 (Batc (None, 16000, 32)    128         conv1d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_25[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling1D) (None, 5333, 32)     0           activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_26 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_25[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_26 (Batc (None, 5333, 32)     128         conv1d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_26[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_26 (MaxPooling1D) (None, 1777, 32)     0           activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_27 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_26[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_27 (Batc (None, 1777, 32)     128         conv1d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_27[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_27 (MaxPooling1D) (None, 592, 32)      0           activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_28 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_27[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_28 (Batc (None, 592, 32)      128         conv1d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 592, 32)      0           batch_normalization_v1_28[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_28 (MaxPooling1D) (None, 197, 32)      0           activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_29 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_28[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_29 (Batc (None, 197, 64)      256         conv1d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 197, 64)      0           batch_normalization_v1_29[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_29 (MaxPooling1D) (None, 65, 64)       0           activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_30 (Conv1D)              (None, 65, 64)       20544       max_pooling1d_29[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_30 (Batc (None, 65, 64)       256         conv1d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 65, 64)       0           batch_normalization_v1_30[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_30 (MaxPooling1D) (None, 21, 64)       0           activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_31 (Conv1D)              (None, 21, 64)       20544       max_pooling1d_30[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_31 (Batc (None, 21, 64)       256         conv1d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 21, 64)       0           batch_normalization_v1_31[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_31 (MaxPooling1D) (None, 7, 64)        0           activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_32 (Conv1D)              (None, 7, 64)        20544       max_pooling1d_31[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_32 (Batc (None, 7, 64)        256         conv1d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 7, 64)        0           batch_normalization_v1_32[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_32 (MaxPooling1D) (None, 2, 64)        0           activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_10 (Gl (None, 64)           0           max_pooling1d_31[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_11 (Gl (None, 64)           0           max_pooling1d_32[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 128)          0           global_average_pooling1d_10[0][0]\n",
      "                                                                 global_average_pooling1d_11[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 128)          0           concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 16)           2064        dropout_5[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 91,184\n",
      "Trainable params: 90,416\n",
      "Non-trainable params: 768\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in range(3, 9):\n",
    "    model = build_cnn(conv_num=i)\n",
    "    model.summary()\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36805 samples, validate on 4293 samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.6881 - acc: 0.1421\n",
      "Epoch 00001: val_loss improved from inf to 2.53409, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_3_conv_checkpoint/001-2.5341.hdf5\n",
      "36805/36805 [==============================] - 31s 829us/sample - loss: 2.6882 - acc: 0.1421 - val_loss: 2.5341 - val_acc: 0.2958\n",
      "Epoch 2/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 2.4682 - acc: 0.2104\n",
      "Epoch 00002: val_loss improved from 2.53409 to 2.30702, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_3_conv_checkpoint/002-2.3070.hdf5\n",
      "36805/36805 [==============================] - 28s 752us/sample - loss: 2.4682 - acc: 0.2103 - val_loss: 2.3070 - val_acc: 0.3634\n",
      "Epoch 3/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 2.3535 - acc: 0.2426\n",
      "Epoch 00003: val_loss improved from 2.30702 to 2.21344, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_3_conv_checkpoint/003-2.2134.hdf5\n",
      "36805/36805 [==============================] - 28s 750us/sample - loss: 2.3537 - acc: 0.2424 - val_loss: 2.2134 - val_acc: 0.4037\n",
      "Epoch 4/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 2.2819 - acc: 0.2676\n",
      "Epoch 00004: val_loss improved from 2.21344 to 2.12815, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_3_conv_checkpoint/004-2.1281.hdf5\n",
      "36805/36805 [==============================] - 28s 749us/sample - loss: 2.2819 - acc: 0.2676 - val_loss: 2.1281 - val_acc: 0.4053\n",
      "Epoch 5/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 2.2159 - acc: 0.2879\n",
      "Epoch 00005: val_loss improved from 2.12815 to 2.06164, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_3_conv_checkpoint/005-2.0616.hdf5\n",
      "36805/36805 [==============================] - 28s 748us/sample - loss: 2.2156 - acc: 0.2880 - val_loss: 2.0616 - val_acc: 0.4398\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.1495 - acc: 0.3054\n",
      "Epoch 00006: val_loss improved from 2.06164 to 1.99009, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_3_conv_checkpoint/006-1.9901.hdf5\n",
      "36805/36805 [==============================] - 28s 750us/sample - loss: 2.1496 - acc: 0.3053 - val_loss: 1.9901 - val_acc: 0.4538\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.0964 - acc: 0.3220\n",
      "Epoch 00007: val_loss improved from 1.99009 to 1.93722, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_3_conv_checkpoint/007-1.9372.hdf5\n",
      "36805/36805 [==============================] - 27s 747us/sample - loss: 2.0963 - acc: 0.3220 - val_loss: 1.9372 - val_acc: 0.4764\n",
      "Epoch 8/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 2.0559 - acc: 0.3319\n",
      "Epoch 00008: val_loss improved from 1.93722 to 1.87757, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_3_conv_checkpoint/008-1.8776.hdf5\n",
      "36805/36805 [==============================] - 28s 749us/sample - loss: 2.0559 - acc: 0.3319 - val_loss: 1.8776 - val_acc: 0.4873\n",
      "Epoch 9/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 2.0080 - acc: 0.3489\n",
      "Epoch 00009: val_loss improved from 1.87757 to 1.83466, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_3_conv_checkpoint/009-1.8347.hdf5\n",
      "36805/36805 [==============================] - 28s 749us/sample - loss: 2.0077 - acc: 0.3490 - val_loss: 1.8347 - val_acc: 0.4945\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9740 - acc: 0.3605\n",
      "Epoch 00010: val_loss improved from 1.83466 to 1.79280, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_3_conv_checkpoint/010-1.7928.hdf5\n",
      "36805/36805 [==============================] - 27s 746us/sample - loss: 1.9741 - acc: 0.3605 - val_loss: 1.7928 - val_acc: 0.5024\n",
      "Epoch 11/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.9338 - acc: 0.3687\n",
      "Epoch 00011: val_loss improved from 1.79280 to 1.76077, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_3_conv_checkpoint/011-1.7608.hdf5\n",
      "36805/36805 [==============================] - 28s 751us/sample - loss: 1.9341 - acc: 0.3686 - val_loss: 1.7608 - val_acc: 0.5101\n",
      "Epoch 12/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.9072 - acc: 0.3816\n",
      "Epoch 00012: val_loss improved from 1.76077 to 1.72642, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_3_conv_checkpoint/012-1.7264.hdf5\n",
      "36805/36805 [==============================] - 27s 746us/sample - loss: 1.9072 - acc: 0.3816 - val_loss: 1.7264 - val_acc: 0.5090\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.8811 - acc: 0.3892\n",
      "Epoch 00013: val_loss improved from 1.72642 to 1.70290, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_3_conv_checkpoint/013-1.7029.hdf5\n",
      "36805/36805 [==============================] - 27s 745us/sample - loss: 1.8811 - acc: 0.3892 - val_loss: 1.7029 - val_acc: 0.5397\n",
      "Epoch 14/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.8576 - acc: 0.3983\n",
      "Epoch 00014: val_loss improved from 1.70290 to 1.66333, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_3_conv_checkpoint/014-1.6633.hdf5\n",
      "36805/36805 [==============================] - 28s 748us/sample - loss: 1.8573 - acc: 0.3984 - val_loss: 1.6633 - val_acc: 0.5425\n",
      "Epoch 15/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.8354 - acc: 0.4061\n",
      "Epoch 00015: val_loss improved from 1.66333 to 1.63637, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_3_conv_checkpoint/015-1.6364.hdf5\n",
      "36805/36805 [==============================] - 28s 749us/sample - loss: 1.8353 - acc: 0.4062 - val_loss: 1.6364 - val_acc: 0.5404\n",
      "Epoch 16/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.8148 - acc: 0.4110\n",
      "Epoch 00016: val_loss improved from 1.63637 to 1.60053, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_3_conv_checkpoint/016-1.6005.hdf5\n",
      "36805/36805 [==============================] - 28s 748us/sample - loss: 1.8147 - acc: 0.4111 - val_loss: 1.6005 - val_acc: 0.5609\n",
      "Epoch 17/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.7901 - acc: 0.4242\n",
      "Epoch 00017: val_loss improved from 1.60053 to 1.59902, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_3_conv_checkpoint/017-1.5990.hdf5\n",
      "36805/36805 [==============================] - 28s 749us/sample - loss: 1.7899 - acc: 0.4244 - val_loss: 1.5990 - val_acc: 0.5707\n",
      "Epoch 18/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.7796 - acc: 0.4276\n",
      "Epoch 00018: val_loss improved from 1.59902 to 1.56899, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_3_conv_checkpoint/018-1.5690.hdf5\n",
      "36805/36805 [==============================] - 28s 749us/sample - loss: 1.7794 - acc: 0.4275 - val_loss: 1.5690 - val_acc: 0.5660\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.7583 - acc: 0.4310\n",
      "Epoch 00019: val_loss improved from 1.56899 to 1.54504, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_3_conv_checkpoint/019-1.5450.hdf5\n",
      "36805/36805 [==============================] - 27s 747us/sample - loss: 1.7583 - acc: 0.4310 - val_loss: 1.5450 - val_acc: 0.5779\n",
      "Epoch 20/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.7430 - acc: 0.4356\n",
      "Epoch 00020: val_loss did not improve from 1.54504\n",
      "36805/36805 [==============================] - 27s 746us/sample - loss: 1.7436 - acc: 0.4356 - val_loss: 1.5473 - val_acc: 0.5467\n",
      "Epoch 21/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.7336 - acc: 0.4419\n",
      "Epoch 00021: val_loss improved from 1.54504 to 1.51435, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_3_conv_checkpoint/021-1.5144.hdf5\n",
      "36805/36805 [==============================] - 28s 750us/sample - loss: 1.7336 - acc: 0.4420 - val_loss: 1.5144 - val_acc: 0.5786\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.7208 - acc: 0.4429\n",
      "Epoch 00022: val_loss improved from 1.51435 to 1.51294, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_3_conv_checkpoint/022-1.5129.hdf5\n",
      "36805/36805 [==============================] - 28s 752us/sample - loss: 1.7212 - acc: 0.4428 - val_loss: 1.5129 - val_acc: 0.5723\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.7020 - acc: 0.4525\n",
      "Epoch 00023: val_loss improved from 1.51294 to 1.47630, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_3_conv_checkpoint/023-1.4763.hdf5\n",
      "36805/36805 [==============================] - 28s 751us/sample - loss: 1.7020 - acc: 0.4525 - val_loss: 1.4763 - val_acc: 0.5949\n",
      "Epoch 24/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.6934 - acc: 0.4548\n",
      "Epoch 00024: val_loss improved from 1.47630 to 1.46509, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_3_conv_checkpoint/024-1.4651.hdf5\n",
      "36805/36805 [==============================] - 28s 750us/sample - loss: 1.6934 - acc: 0.4547 - val_loss: 1.4651 - val_acc: 0.5947\n",
      "Epoch 25/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.6745 - acc: 0.4613\n",
      "Epoch 00025: val_loss did not improve from 1.46509\n",
      "36805/36805 [==============================] - 28s 748us/sample - loss: 1.6745 - acc: 0.4611 - val_loss: 1.4996 - val_acc: 0.5611\n",
      "Epoch 26/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.6703 - acc: 0.4628\n",
      "Epoch 00026: val_loss did not improve from 1.46509\n",
      "36805/36805 [==============================] - 27s 747us/sample - loss: 1.6708 - acc: 0.4626 - val_loss: 1.4760 - val_acc: 0.5584\n",
      "Epoch 27/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.6616 - acc: 0.4648\n",
      "Epoch 00027: val_loss improved from 1.46509 to 1.44594, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_3_conv_checkpoint/027-1.4459.hdf5\n",
      "36805/36805 [==============================] - 28s 748us/sample - loss: 1.6618 - acc: 0.4647 - val_loss: 1.4459 - val_acc: 0.5877\n",
      "Epoch 28/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.6572 - acc: 0.4679\n",
      "Epoch 00028: val_loss improved from 1.44594 to 1.42859, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_3_conv_checkpoint/028-1.4286.hdf5\n",
      "36805/36805 [==============================] - 28s 751us/sample - loss: 1.6575 - acc: 0.4676 - val_loss: 1.4286 - val_acc: 0.5954\n",
      "Epoch 29/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.6358 - acc: 0.4759\n",
      "Epoch 00029: val_loss improved from 1.42859 to 1.42453, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_3_conv_checkpoint/029-1.4245.hdf5\n",
      "36805/36805 [==============================] - 28s 749us/sample - loss: 1.6359 - acc: 0.4758 - val_loss: 1.4245 - val_acc: 0.5931\n",
      "Epoch 30/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.6304 - acc: 0.4757\n",
      "Epoch 00030: val_loss improved from 1.42453 to 1.40100, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_3_conv_checkpoint/030-1.4010.hdf5\n",
      "36805/36805 [==============================] - 28s 749us/sample - loss: 1.6302 - acc: 0.4758 - val_loss: 1.4010 - val_acc: 0.6122\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6261 - acc: 0.4785\n",
      "Epoch 00031: val_loss did not improve from 1.40100\n",
      "36805/36805 [==============================] - 28s 748us/sample - loss: 1.6261 - acc: 0.4784 - val_loss: 1.4128 - val_acc: 0.5993\n",
      "Epoch 32/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.6148 - acc: 0.4814\n",
      "Epoch 00032: val_loss did not improve from 1.40100\n",
      "36805/36805 [==============================] - 27s 747us/sample - loss: 1.6152 - acc: 0.4813 - val_loss: 1.4416 - val_acc: 0.5579\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6013 - acc: 0.4858\n",
      "Epoch 00033: val_loss did not improve from 1.40100\n",
      "36805/36805 [==============================] - 27s 746us/sample - loss: 1.6013 - acc: 0.4858 - val_loss: 1.4223 - val_acc: 0.5921\n",
      "Epoch 34/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.5978 - acc: 0.4869\n",
      "Epoch 00034: val_loss improved from 1.40100 to 1.37432, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_3_conv_checkpoint/034-1.3743.hdf5\n",
      "36805/36805 [==============================] - 28s 747us/sample - loss: 1.5979 - acc: 0.4869 - val_loss: 1.3743 - val_acc: 0.6133\n",
      "Epoch 35/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.5907 - acc: 0.4895\n",
      "Epoch 00035: val_loss did not improve from 1.37432\n",
      "36805/36805 [==============================] - 28s 748us/sample - loss: 1.5908 - acc: 0.4894 - val_loss: 1.3785 - val_acc: 0.6012\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5817 - acc: 0.4935\n",
      "Epoch 00036: val_loss improved from 1.37432 to 1.36359, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_3_conv_checkpoint/036-1.3636.hdf5\n",
      "36805/36805 [==============================] - 28s 748us/sample - loss: 1.5817 - acc: 0.4934 - val_loss: 1.3636 - val_acc: 0.6150\n",
      "Epoch 37/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.5765 - acc: 0.4935\n",
      "Epoch 00037: val_loss improved from 1.36359 to 1.35460, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_3_conv_checkpoint/037-1.3546.hdf5\n",
      "36805/36805 [==============================] - 28s 747us/sample - loss: 1.5768 - acc: 0.4934 - val_loss: 1.3546 - val_acc: 0.6175\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5711 - acc: 0.4957\n",
      "Epoch 00038: val_loss did not improve from 1.35460\n",
      "36805/36805 [==============================] - 28s 748us/sample - loss: 1.5711 - acc: 0.4957 - val_loss: 1.3677 - val_acc: 0.5893\n",
      "Epoch 39/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.5631 - acc: 0.4973\n",
      "Epoch 00039: val_loss improved from 1.35460 to 1.34500, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_3_conv_checkpoint/039-1.3450.hdf5\n",
      "36805/36805 [==============================] - 28s 753us/sample - loss: 1.5633 - acc: 0.4974 - val_loss: 1.3450 - val_acc: 0.6161\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5580 - acc: 0.4986\n",
      "Epoch 00040: val_loss improved from 1.34500 to 1.30936, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_3_conv_checkpoint/040-1.3094.hdf5\n",
      "36805/36805 [==============================] - 28s 751us/sample - loss: 1.5580 - acc: 0.4986 - val_loss: 1.3094 - val_acc: 0.6261\n",
      "Epoch 41/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.5548 - acc: 0.5008\n",
      "Epoch 00041: val_loss did not improve from 1.30936\n",
      "36805/36805 [==============================] - 28s 749us/sample - loss: 1.5552 - acc: 0.5007 - val_loss: 1.3138 - val_acc: 0.6313\n",
      "Epoch 42/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.5486 - acc: 0.5065\n",
      "Epoch 00042: val_loss did not improve from 1.30936\n",
      "36805/36805 [==============================] - 28s 748us/sample - loss: 1.5488 - acc: 0.5065 - val_loss: 1.3763 - val_acc: 0.5795\n",
      "Epoch 43/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.5417 - acc: 0.5055\n",
      "Epoch 00043: val_loss improved from 1.30936 to 1.29336, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_3_conv_checkpoint/043-1.2934.hdf5\n",
      "36805/36805 [==============================] - 28s 749us/sample - loss: 1.5418 - acc: 0.5054 - val_loss: 1.2934 - val_acc: 0.6264\n",
      "Epoch 44/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.5362 - acc: 0.5108\n",
      "Epoch 00044: val_loss did not improve from 1.29336\n",
      "36805/36805 [==============================] - 27s 744us/sample - loss: 1.5362 - acc: 0.5107 - val_loss: 1.3375 - val_acc: 0.6073\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5339 - acc: 0.5101\n",
      "Epoch 00045: val_loss did not improve from 1.29336\n",
      "36805/36805 [==============================] - 28s 747us/sample - loss: 1.5339 - acc: 0.5101 - val_loss: 1.2943 - val_acc: 0.6275\n",
      "Epoch 46/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5245 - acc: 0.5150\n",
      "Epoch 00046: val_loss did not improve from 1.29336\n",
      "36805/36805 [==============================] - 27s 746us/sample - loss: 1.5245 - acc: 0.5149 - val_loss: 1.2968 - val_acc: 0.6257\n",
      "Epoch 47/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.5237 - acc: 0.5131\n",
      "Epoch 00047: val_loss did not improve from 1.29336\n",
      "36805/36805 [==============================] - 28s 748us/sample - loss: 1.5239 - acc: 0.5130 - val_loss: 1.2937 - val_acc: 0.6166\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5123 - acc: 0.5165\n",
      "Epoch 00048: val_loss improved from 1.29336 to 1.25037, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_3_conv_checkpoint/048-1.2504.hdf5\n",
      "36805/36805 [==============================] - 27s 747us/sample - loss: 1.5123 - acc: 0.5165 - val_loss: 1.2504 - val_acc: 0.6448\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5120 - acc: 0.5199\n",
      "Epoch 00049: val_loss did not improve from 1.25037\n",
      "36805/36805 [==============================] - 27s 747us/sample - loss: 1.5120 - acc: 0.5199 - val_loss: 1.2721 - val_acc: 0.6348\n",
      "Epoch 50/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.5102 - acc: 0.5182\n",
      "Epoch 00050: val_loss did not improve from 1.25037\n",
      "36805/36805 [==============================] - 27s 745us/sample - loss: 1.5103 - acc: 0.5182 - val_loss: 1.2711 - val_acc: 0.6350\n",
      "Epoch 51/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.4987 - acc: 0.5224\n",
      "Epoch 00051: val_loss did not improve from 1.25037\n",
      "36805/36805 [==============================] - 28s 748us/sample - loss: 1.4985 - acc: 0.5224 - val_loss: 1.2721 - val_acc: 0.6245\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4933 - acc: 0.5221\n",
      "Epoch 00052: val_loss did not improve from 1.25037\n",
      "36805/36805 [==============================] - 27s 746us/sample - loss: 1.4935 - acc: 0.5220 - val_loss: 1.3264 - val_acc: 0.5907\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4944 - acc: 0.5251\n",
      "Epoch 00053: val_loss did not improve from 1.25037\n",
      "36805/36805 [==============================] - 27s 747us/sample - loss: 1.4944 - acc: 0.5251 - val_loss: 1.2741 - val_acc: 0.6296\n",
      "Epoch 54/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.4852 - acc: 0.5251\n",
      "Epoch 00054: val_loss did not improve from 1.25037\n",
      "36805/36805 [==============================] - 28s 749us/sample - loss: 1.4851 - acc: 0.5252 - val_loss: 1.3226 - val_acc: 0.5651\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4849 - acc: 0.5265\n",
      "Epoch 00055: val_loss improved from 1.25037 to 1.24621, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_3_conv_checkpoint/055-1.2462.hdf5\n",
      "36805/36805 [==============================] - 28s 749us/sample - loss: 1.4851 - acc: 0.5265 - val_loss: 1.2462 - val_acc: 0.6420\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4771 - acc: 0.5298\n",
      "Epoch 00056: val_loss did not improve from 1.24621\n",
      "36805/36805 [==============================] - 27s 746us/sample - loss: 1.4771 - acc: 0.5298 - val_loss: 1.2743 - val_acc: 0.6133\n",
      "Epoch 57/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.4775 - acc: 0.5307\n",
      "Epoch 00057: val_loss did not improve from 1.24621\n",
      "36805/36805 [==============================] - 28s 749us/sample - loss: 1.4774 - acc: 0.5306 - val_loss: 1.2662 - val_acc: 0.6182\n",
      "Epoch 58/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.4751 - acc: 0.5306\n",
      "Epoch 00058: val_loss did not improve from 1.24621\n",
      "36805/36805 [==============================] - 27s 745us/sample - loss: 1.4755 - acc: 0.5305 - val_loss: 1.3087 - val_acc: 0.5912\n",
      "Epoch 59/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.4738 - acc: 0.5296\n",
      "Epoch 00059: val_loss improved from 1.24621 to 1.23165, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_3_conv_checkpoint/059-1.2317.hdf5\n",
      "36805/36805 [==============================] - 27s 747us/sample - loss: 1.4735 - acc: 0.5297 - val_loss: 1.2317 - val_acc: 0.6471\n",
      "Epoch 60/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.4625 - acc: 0.5352\n",
      "Epoch 00060: val_loss did not improve from 1.23165\n",
      "36805/36805 [==============================] - 28s 748us/sample - loss: 1.4625 - acc: 0.5353 - val_loss: 1.2726 - val_acc: 0.6033\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4660 - acc: 0.5306\n",
      "Epoch 00061: val_loss improved from 1.23165 to 1.21657, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_3_conv_checkpoint/061-1.2166.hdf5\n",
      "36805/36805 [==============================] - 28s 748us/sample - loss: 1.4661 - acc: 0.5305 - val_loss: 1.2166 - val_acc: 0.6424\n",
      "Epoch 62/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.4503 - acc: 0.5374\n",
      "Epoch 00062: val_loss did not improve from 1.21657\n",
      "36805/36805 [==============================] - 28s 748us/sample - loss: 1.4499 - acc: 0.5376 - val_loss: 1.2231 - val_acc: 0.6478\n",
      "Epoch 63/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.4529 - acc: 0.5368\n",
      "Epoch 00063: val_loss did not improve from 1.21657\n",
      "36805/36805 [==============================] - 28s 749us/sample - loss: 1.4529 - acc: 0.5367 - val_loss: 1.2282 - val_acc: 0.6410\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4481 - acc: 0.5396\n",
      "Epoch 00064: val_loss improved from 1.21657 to 1.20074, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_3_conv_checkpoint/064-1.2007.hdf5\n",
      "36805/36805 [==============================] - 27s 747us/sample - loss: 1.4482 - acc: 0.5396 - val_loss: 1.2007 - val_acc: 0.6590\n",
      "Epoch 65/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.4499 - acc: 0.5395\n",
      "Epoch 00065: val_loss did not improve from 1.20074\n",
      "36805/36805 [==============================] - 27s 746us/sample - loss: 1.4497 - acc: 0.5393 - val_loss: 1.6295 - val_acc: 0.4710\n",
      "Epoch 66/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.4434 - acc: 0.5435\n",
      "Epoch 00066: val_loss did not improve from 1.20074\n",
      "36805/36805 [==============================] - 28s 749us/sample - loss: 1.4432 - acc: 0.5436 - val_loss: 1.2277 - val_acc: 0.6464\n",
      "Epoch 67/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.4352 - acc: 0.5445\n",
      "Epoch 00067: val_loss improved from 1.20074 to 1.17514, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_3_conv_checkpoint/067-1.1751.hdf5\n",
      "36805/36805 [==============================] - 27s 747us/sample - loss: 1.4347 - acc: 0.5446 - val_loss: 1.1751 - val_acc: 0.6611\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4387 - acc: 0.5429\n",
      "Epoch 00068: val_loss did not improve from 1.17514\n",
      "36805/36805 [==============================] - 28s 748us/sample - loss: 1.4388 - acc: 0.5428 - val_loss: 1.2124 - val_acc: 0.6455\n",
      "Epoch 69/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.4375 - acc: 0.5440\n",
      "Epoch 00069: val_loss did not improve from 1.17514\n",
      "36805/36805 [==============================] - 28s 748us/sample - loss: 1.4375 - acc: 0.5440 - val_loss: 1.2209 - val_acc: 0.6320\n",
      "Epoch 70/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.4350 - acc: 0.5430\n",
      "Epoch 00070: val_loss did not improve from 1.17514\n",
      "36805/36805 [==============================] - 28s 748us/sample - loss: 1.4353 - acc: 0.5428 - val_loss: 1.2165 - val_acc: 0.6536\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4266 - acc: 0.5490\n",
      "Epoch 00071: val_loss did not improve from 1.17514\n",
      "36805/36805 [==============================] - 27s 744us/sample - loss: 1.4267 - acc: 0.5490 - val_loss: 1.1772 - val_acc: 0.6646\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4208 - acc: 0.5503\n",
      "Epoch 00072: val_loss did not improve from 1.17514\n",
      "36805/36805 [==============================] - 28s 750us/sample - loss: 1.4209 - acc: 0.5503 - val_loss: 1.3848 - val_acc: 0.5448\n",
      "Epoch 73/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4214 - acc: 0.5509\n",
      "Epoch 00073: val_loss did not improve from 1.17514\n",
      "36805/36805 [==============================] - 28s 750us/sample - loss: 1.4215 - acc: 0.5508 - val_loss: 1.2086 - val_acc: 0.6191\n",
      "Epoch 74/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.4209 - acc: 0.5488\n",
      "Epoch 00074: val_loss improved from 1.17514 to 1.14829, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_3_conv_checkpoint/074-1.1483.hdf5\n",
      "36805/36805 [==============================] - 28s 749us/sample - loss: 1.4205 - acc: 0.5489 - val_loss: 1.1483 - val_acc: 0.6723\n",
      "Epoch 75/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.4150 - acc: 0.5494\n",
      "Epoch 00075: val_loss did not improve from 1.14829\n",
      "36805/36805 [==============================] - 28s 750us/sample - loss: 1.4148 - acc: 0.5493 - val_loss: 1.1926 - val_acc: 0.6480\n",
      "Epoch 76/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.4153 - acc: 0.5480\n",
      "Epoch 00076: val_loss did not improve from 1.14829\n",
      "36805/36805 [==============================] - 28s 749us/sample - loss: 1.4150 - acc: 0.5480 - val_loss: 1.1709 - val_acc: 0.6650\n",
      "Epoch 77/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.4126 - acc: 0.5505\n",
      "Epoch 00077: val_loss did not improve from 1.14829\n",
      "36805/36805 [==============================] - 27s 746us/sample - loss: 1.4126 - acc: 0.5503 - val_loss: 1.1508 - val_acc: 0.6737\n",
      "Epoch 78/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.4037 - acc: 0.5541\n",
      "Epoch 00078: val_loss did not improve from 1.14829\n",
      "36805/36805 [==============================] - 28s 748us/sample - loss: 1.4041 - acc: 0.5543 - val_loss: 1.1547 - val_acc: 0.6695\n",
      "Epoch 79/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.4054 - acc: 0.5533\n",
      "Epoch 00079: val_loss did not improve from 1.14829\n",
      "36805/36805 [==============================] - 27s 746us/sample - loss: 1.4054 - acc: 0.5535 - val_loss: 1.2107 - val_acc: 0.6415\n",
      "Epoch 80/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.4033 - acc: 0.5540\n",
      "Epoch 00080: val_loss did not improve from 1.14829\n",
      "36805/36805 [==============================] - 28s 749us/sample - loss: 1.4032 - acc: 0.5539 - val_loss: 1.2468 - val_acc: 0.6073\n",
      "Epoch 81/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.4083 - acc: 0.5543\n",
      "Epoch 00081: val_loss did not improve from 1.14829\n",
      "36805/36805 [==============================] - 27s 746us/sample - loss: 1.4084 - acc: 0.5545 - val_loss: 1.2093 - val_acc: 0.6231\n",
      "Epoch 82/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3974 - acc: 0.5595\n",
      "Epoch 00082: val_loss did not improve from 1.14829\n",
      "36805/36805 [==============================] - 28s 749us/sample - loss: 1.3976 - acc: 0.5594 - val_loss: 1.2732 - val_acc: 0.5905\n",
      "Epoch 83/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3975 - acc: 0.5559\n",
      "Epoch 00083: val_loss did not improve from 1.14829\n",
      "36805/36805 [==============================] - 28s 748us/sample - loss: 1.3974 - acc: 0.5558 - val_loss: 1.1549 - val_acc: 0.6527\n",
      "Epoch 84/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3920 - acc: 0.5586\n",
      "Epoch 00084: val_loss did not improve from 1.14829\n",
      "36805/36805 [==============================] - 28s 751us/sample - loss: 1.3918 - acc: 0.5587 - val_loss: 1.1638 - val_acc: 0.6585\n",
      "Epoch 85/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3911 - acc: 0.5604\n",
      "Epoch 00085: val_loss did not improve from 1.14829\n",
      "36805/36805 [==============================] - 28s 749us/sample - loss: 1.3909 - acc: 0.5603 - val_loss: 1.1743 - val_acc: 0.6378\n",
      "Epoch 86/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3948 - acc: 0.5565\n",
      "Epoch 00086: val_loss improved from 1.14829 to 1.13191, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_3_conv_checkpoint/086-1.1319.hdf5\n",
      "36805/36805 [==============================] - 28s 751us/sample - loss: 1.3950 - acc: 0.5566 - val_loss: 1.1319 - val_acc: 0.6713\n",
      "Epoch 87/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3926 - acc: 0.5578\n",
      "Epoch 00087: val_loss did not improve from 1.13191\n",
      "36805/36805 [==============================] - 28s 748us/sample - loss: 1.3928 - acc: 0.5576 - val_loss: 1.2477 - val_acc: 0.5858\n",
      "Epoch 88/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3899 - acc: 0.5560\n",
      "Epoch 00088: val_loss did not improve from 1.13191\n",
      "36805/36805 [==============================] - 28s 748us/sample - loss: 1.3900 - acc: 0.5560 - val_loss: 1.1677 - val_acc: 0.6632\n",
      "Epoch 89/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3829 - acc: 0.5615\n",
      "Epoch 00089: val_loss did not improve from 1.13191\n",
      "36805/36805 [==============================] - 28s 749us/sample - loss: 1.3828 - acc: 0.5615 - val_loss: 2.4984 - val_acc: 0.3834\n",
      "Epoch 90/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3764 - acc: 0.5648\n",
      "Epoch 00090: val_loss improved from 1.13191 to 1.12754, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_3_conv_checkpoint/090-1.1275.hdf5\n",
      "36805/36805 [==============================] - 28s 747us/sample - loss: 1.3767 - acc: 0.5647 - val_loss: 1.1275 - val_acc: 0.6820\n",
      "Epoch 91/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3749 - acc: 0.5640\n",
      "Epoch 00091: val_loss did not improve from 1.12754\n",
      "36805/36805 [==============================] - 28s 749us/sample - loss: 1.3749 - acc: 0.5641 - val_loss: 1.3231 - val_acc: 0.5772\n",
      "Epoch 92/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3751 - acc: 0.5653\n",
      "Epoch 00092: val_loss did not improve from 1.12754\n",
      "36805/36805 [==============================] - 28s 748us/sample - loss: 1.3755 - acc: 0.5653 - val_loss: 1.1967 - val_acc: 0.6369\n",
      "Epoch 93/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3767 - acc: 0.5662\n",
      "Epoch 00093: val_loss did not improve from 1.12754\n",
      "36805/36805 [==============================] - 28s 750us/sample - loss: 1.3770 - acc: 0.5661 - val_loss: 1.1307 - val_acc: 0.6739\n",
      "Epoch 94/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3667 - acc: 0.5673\n",
      "Epoch 00094: val_loss did not improve from 1.12754\n",
      "36805/36805 [==============================] - 27s 744us/sample - loss: 1.3670 - acc: 0.5673 - val_loss: 1.2552 - val_acc: 0.5905\n",
      "Epoch 95/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3633 - acc: 0.5673\n",
      "Epoch 00095: val_loss did not improve from 1.12754\n",
      "36805/36805 [==============================] - 28s 748us/sample - loss: 1.3632 - acc: 0.5674 - val_loss: 1.1434 - val_acc: 0.6492\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3645 - acc: 0.5685\n",
      "Epoch 00096: val_loss did not improve from 1.12754\n",
      "36805/36805 [==============================] - 28s 748us/sample - loss: 1.3645 - acc: 0.5686 - val_loss: 1.1809 - val_acc: 0.6224\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3606 - acc: 0.5683\n",
      "Epoch 00097: val_loss improved from 1.12754 to 1.10997, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_3_conv_checkpoint/097-1.1100.hdf5\n",
      "36805/36805 [==============================] - 27s 747us/sample - loss: 1.3608 - acc: 0.5683 - val_loss: 1.1100 - val_acc: 0.6732\n",
      "Epoch 98/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3626 - acc: 0.5673\n",
      "Epoch 00098: val_loss improved from 1.10997 to 1.10154, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_3_conv_checkpoint/098-1.1015.hdf5\n",
      "36805/36805 [==============================] - 28s 751us/sample - loss: 1.3629 - acc: 0.5673 - val_loss: 1.1015 - val_acc: 0.6806\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3609 - acc: 0.5692\n",
      "Epoch 00099: val_loss did not improve from 1.10154\n",
      "36805/36805 [==============================] - 27s 745us/sample - loss: 1.3610 - acc: 0.5691 - val_loss: 1.1017 - val_acc: 0.6883\n",
      "Epoch 100/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3630 - acc: 0.5703\n",
      "Epoch 00100: val_loss did not improve from 1.10154\n",
      "36805/36805 [==============================] - 28s 747us/sample - loss: 1.3631 - acc: 0.5702 - val_loss: 1.1391 - val_acc: 0.6653\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3548 - acc: 0.5716\n",
      "Epoch 00101: val_loss did not improve from 1.10154\n",
      "36805/36805 [==============================] - 27s 746us/sample - loss: 1.3550 - acc: 0.5716 - val_loss: 1.5104 - val_acc: 0.5313\n",
      "Epoch 102/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3535 - acc: 0.5729\n",
      "Epoch 00102: val_loss did not improve from 1.10154\n",
      "36805/36805 [==============================] - 27s 746us/sample - loss: 1.3540 - acc: 0.5727 - val_loss: 1.2002 - val_acc: 0.6173\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3547 - acc: 0.5737\n",
      "Epoch 00103: val_loss did not improve from 1.10154\n",
      "36805/36805 [==============================] - 27s 744us/sample - loss: 1.3547 - acc: 0.5737 - val_loss: 2.2215 - val_acc: 0.4142\n",
      "Epoch 104/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3546 - acc: 0.5713\n",
      "Epoch 00104: val_loss did not improve from 1.10154\n",
      "36805/36805 [==============================] - 28s 747us/sample - loss: 1.3545 - acc: 0.5713 - val_loss: 1.1194 - val_acc: 0.6697\n",
      "Epoch 105/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3500 - acc: 0.5754\n",
      "Epoch 00105: val_loss did not improve from 1.10154\n",
      "36805/36805 [==============================] - 27s 746us/sample - loss: 1.3503 - acc: 0.5754 - val_loss: 1.1563 - val_acc: 0.6576\n",
      "Epoch 106/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3431 - acc: 0.5758\n",
      "Epoch 00106: val_loss did not improve from 1.10154\n",
      "36805/36805 [==============================] - 27s 746us/sample - loss: 1.3429 - acc: 0.5757 - val_loss: 1.8158 - val_acc: 0.4826\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3548 - acc: 0.5701\n",
      "Epoch 00107: val_loss did not improve from 1.10154\n",
      "36805/36805 [==============================] - 27s 745us/sample - loss: 1.3548 - acc: 0.5701 - val_loss: 1.1213 - val_acc: 0.6688\n",
      "Epoch 108/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3427 - acc: 0.5733\n",
      "Epoch 00108: val_loss did not improve from 1.10154\n",
      "36805/36805 [==============================] - 27s 745us/sample - loss: 1.3433 - acc: 0.5733 - val_loss: 1.4095 - val_acc: 0.5702\n",
      "Epoch 109/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3369 - acc: 0.5766\n",
      "Epoch 00109: val_loss did not improve from 1.10154\n",
      "36805/36805 [==============================] - 28s 748us/sample - loss: 1.3365 - acc: 0.5766 - val_loss: 1.1084 - val_acc: 0.6844\n",
      "Epoch 110/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3437 - acc: 0.5737\n",
      "Epoch 00110: val_loss improved from 1.10154 to 1.06426, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_3_conv_checkpoint/110-1.0643.hdf5\n",
      "36805/36805 [==============================] - 27s 743us/sample - loss: 1.3439 - acc: 0.5736 - val_loss: 1.0643 - val_acc: 0.6923\n",
      "Epoch 111/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3381 - acc: 0.5774\n",
      "Epoch 00111: val_loss did not improve from 1.06426\n",
      "36805/36805 [==============================] - 27s 745us/sample - loss: 1.3378 - acc: 0.5774 - val_loss: 1.1501 - val_acc: 0.6580\n",
      "Epoch 112/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3387 - acc: 0.5768\n",
      "Epoch 00112: val_loss did not improve from 1.06426\n",
      "36805/36805 [==============================] - 27s 747us/sample - loss: 1.3389 - acc: 0.5767 - val_loss: 1.0681 - val_acc: 0.6837\n",
      "Epoch 113/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3263 - acc: 0.5796\n",
      "Epoch 00113: val_loss did not improve from 1.06426\n",
      "36805/36805 [==============================] - 27s 745us/sample - loss: 1.3261 - acc: 0.5796 - val_loss: 1.0769 - val_acc: 0.6888\n",
      "Epoch 114/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3313 - acc: 0.5783\n",
      "Epoch 00114: val_loss improved from 1.06426 to 1.05210, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_3_conv_checkpoint/114-1.0521.hdf5\n",
      "36805/36805 [==============================] - 27s 746us/sample - loss: 1.3319 - acc: 0.5781 - val_loss: 1.0521 - val_acc: 0.7021\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3306 - acc: 0.5783\n",
      "Epoch 00115: val_loss did not improve from 1.05210\n",
      "36805/36805 [==============================] - 27s 747us/sample - loss: 1.3307 - acc: 0.5783 - val_loss: 1.0851 - val_acc: 0.6918\n",
      "Epoch 116/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3275 - acc: 0.5834\n",
      "Epoch 00116: val_loss did not improve from 1.05210\n",
      "36805/36805 [==============================] - 28s 750us/sample - loss: 1.3275 - acc: 0.5835 - val_loss: 1.0591 - val_acc: 0.6969\n",
      "Epoch 117/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3252 - acc: 0.5845\n",
      "Epoch 00117: val_loss did not improve from 1.05210\n",
      "36805/36805 [==============================] - 28s 748us/sample - loss: 1.3254 - acc: 0.5846 - val_loss: 1.0814 - val_acc: 0.6925\n",
      "Epoch 118/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3241 - acc: 0.5804\n",
      "Epoch 00118: val_loss improved from 1.05210 to 1.04924, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_3_conv_checkpoint/118-1.0492.hdf5\n",
      "36805/36805 [==============================] - 28s 749us/sample - loss: 1.3235 - acc: 0.5807 - val_loss: 1.0492 - val_acc: 0.6965\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3234 - acc: 0.5826\n",
      "Epoch 00119: val_loss did not improve from 1.04924\n",
      "36805/36805 [==============================] - 28s 750us/sample - loss: 1.3234 - acc: 0.5826 - val_loss: 1.0702 - val_acc: 0.6893\n",
      "Epoch 120/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3237 - acc: 0.5848\n",
      "Epoch 00120: val_loss did not improve from 1.04924\n",
      "36805/36805 [==============================] - 28s 747us/sample - loss: 1.3236 - acc: 0.5848 - val_loss: 1.0984 - val_acc: 0.6844\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3180 - acc: 0.5831\n",
      "Epoch 00121: val_loss improved from 1.04924 to 1.04343, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_3_conv_checkpoint/121-1.0434.hdf5\n",
      "36805/36805 [==============================] - 28s 748us/sample - loss: 1.3181 - acc: 0.5831 - val_loss: 1.0434 - val_acc: 0.7107\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3230 - acc: 0.5832\n",
      "Epoch 00122: val_loss did not improve from 1.04343\n",
      "36805/36805 [==============================] - 28s 750us/sample - loss: 1.3231 - acc: 0.5832 - val_loss: 1.0757 - val_acc: 0.6879\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3172 - acc: 0.5882\n",
      "Epoch 00123: val_loss did not improve from 1.04343\n",
      "36805/36805 [==============================] - 27s 746us/sample - loss: 1.3172 - acc: 0.5881 - val_loss: 1.0545 - val_acc: 0.6995\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3151 - acc: 0.5874\n",
      "Epoch 00124: val_loss did not improve from 1.04343\n",
      "36805/36805 [==============================] - 28s 750us/sample - loss: 1.3151 - acc: 0.5874 - val_loss: 1.0534 - val_acc: 0.7014\n",
      "Epoch 125/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3127 - acc: 0.5854\n",
      "Epoch 00125: val_loss did not improve from 1.04343\n",
      "36805/36805 [==============================] - 27s 746us/sample - loss: 1.3130 - acc: 0.5855 - val_loss: 1.0934 - val_acc: 0.6695\n",
      "Epoch 126/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3165 - acc: 0.5870\n",
      "Epoch 00126: val_loss did not improve from 1.04343\n",
      "36805/36805 [==============================] - 27s 747us/sample - loss: 1.3164 - acc: 0.5870 - val_loss: 1.5631 - val_acc: 0.4724\n",
      "Epoch 127/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3127 - acc: 0.5855\n",
      "Epoch 00127: val_loss did not improve from 1.04343\n",
      "36805/36805 [==============================] - 27s 746us/sample - loss: 1.3130 - acc: 0.5854 - val_loss: 1.0697 - val_acc: 0.6862\n",
      "Epoch 128/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3051 - acc: 0.5899\n",
      "Epoch 00128: val_loss did not improve from 1.04343\n",
      "36805/36805 [==============================] - 27s 746us/sample - loss: 1.3051 - acc: 0.5900 - val_loss: 1.2150 - val_acc: 0.6087\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 129/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3072 - acc: 0.5862\n",
      "Epoch 00129: val_loss did not improve from 1.04343\n",
      "36805/36805 [==============================] - 27s 745us/sample - loss: 1.3075 - acc: 0.5861 - val_loss: 1.0479 - val_acc: 0.6935\n",
      "Epoch 130/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3049 - acc: 0.5918\n",
      "Epoch 00130: val_loss did not improve from 1.04343\n",
      "36805/36805 [==============================] - 27s 747us/sample - loss: 1.3049 - acc: 0.5918 - val_loss: 1.0705 - val_acc: 0.6841\n",
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2993 - acc: 0.5909\n",
      "Epoch 00131: val_loss did not improve from 1.04343\n",
      "36805/36805 [==============================] - 27s 746us/sample - loss: 1.2993 - acc: 0.5909 - val_loss: 1.4624 - val_acc: 0.5309\n",
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3035 - acc: 0.5871\n",
      "Epoch 00132: val_loss did not improve from 1.04343\n",
      "36805/36805 [==============================] - 27s 746us/sample - loss: 1.3038 - acc: 0.5870 - val_loss: 1.2073 - val_acc: 0.6068\n",
      "Epoch 133/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3062 - acc: 0.5885\n",
      "Epoch 00133: val_loss did not improve from 1.04343\n",
      "36805/36805 [==============================] - 28s 750us/sample - loss: 1.3065 - acc: 0.5883 - val_loss: 1.0465 - val_acc: 0.7009\n",
      "Epoch 134/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3002 - acc: 0.5917\n",
      "Epoch 00134: val_loss improved from 1.04343 to 1.04139, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_3_conv_checkpoint/134-1.0414.hdf5\n",
      "36805/36805 [==============================] - 28s 748us/sample - loss: 1.2999 - acc: 0.5917 - val_loss: 1.0414 - val_acc: 0.7070\n",
      "Epoch 135/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3010 - acc: 0.5932\n",
      "Epoch 00135: val_loss did not improve from 1.04139\n",
      "36805/36805 [==============================] - 28s 749us/sample - loss: 1.3010 - acc: 0.5932 - val_loss: 1.2056 - val_acc: 0.6012\n",
      "Epoch 136/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2963 - acc: 0.5910\n",
      "Epoch 00136: val_loss did not improve from 1.04139\n",
      "36805/36805 [==============================] - 27s 744us/sample - loss: 1.2963 - acc: 0.5910 - val_loss: 1.0916 - val_acc: 0.6692\n",
      "Epoch 137/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2915 - acc: 0.5936\n",
      "Epoch 00137: val_loss did not improve from 1.04139\n",
      "36805/36805 [==============================] - 28s 749us/sample - loss: 1.2918 - acc: 0.5936 - val_loss: 1.7345 - val_acc: 0.4782\n",
      "Epoch 138/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2938 - acc: 0.5939\n",
      "Epoch 00138: val_loss did not improve from 1.04139\n",
      "36805/36805 [==============================] - 27s 747us/sample - loss: 1.2938 - acc: 0.5939 - val_loss: 1.4476 - val_acc: 0.5565\n",
      "Epoch 139/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2992 - acc: 0.5932\n",
      "Epoch 00139: val_loss did not improve from 1.04139\n",
      "36805/36805 [==============================] - 28s 749us/sample - loss: 1.2990 - acc: 0.5932 - val_loss: 1.2484 - val_acc: 0.6024\n",
      "Epoch 140/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2903 - acc: 0.5943\n",
      "Epoch 00140: val_loss did not improve from 1.04139\n",
      "36805/36805 [==============================] - 27s 747us/sample - loss: 1.2903 - acc: 0.5943 - val_loss: 1.0488 - val_acc: 0.6904\n",
      "Epoch 141/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2876 - acc: 0.5963\n",
      "Epoch 00141: val_loss did not improve from 1.04139\n",
      "36805/36805 [==============================] - 28s 748us/sample - loss: 1.2877 - acc: 0.5963 - val_loss: 1.0722 - val_acc: 0.6976\n",
      "Epoch 142/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2894 - acc: 0.5937\n",
      "Epoch 00142: val_loss improved from 1.04139 to 1.02300, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_3_conv_checkpoint/142-1.0230.hdf5\n",
      "36805/36805 [==============================] - 28s 750us/sample - loss: 1.2895 - acc: 0.5935 - val_loss: 1.0230 - val_acc: 0.7067\n",
      "Epoch 143/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2862 - acc: 0.5927\n",
      "Epoch 00143: val_loss did not improve from 1.02300\n",
      "36805/36805 [==============================] - 27s 744us/sample - loss: 1.2861 - acc: 0.5925 - val_loss: 1.4481 - val_acc: 0.5222\n",
      "Epoch 144/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2899 - acc: 0.5954\n",
      "Epoch 00144: val_loss did not improve from 1.02300\n",
      "36805/36805 [==============================] - 28s 749us/sample - loss: 1.2896 - acc: 0.5956 - val_loss: 1.0243 - val_acc: 0.7142\n",
      "Epoch 145/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2881 - acc: 0.5923\n",
      "Epoch 00145: val_loss did not improve from 1.02300\n",
      "36805/36805 [==============================] - 28s 748us/sample - loss: 1.2882 - acc: 0.5923 - val_loss: 1.0458 - val_acc: 0.6879\n",
      "Epoch 146/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2829 - acc: 0.5967\n",
      "Epoch 00146: val_loss did not improve from 1.02300\n",
      "36805/36805 [==============================] - 28s 748us/sample - loss: 1.2830 - acc: 0.5967 - val_loss: 1.2747 - val_acc: 0.5570\n",
      "Epoch 147/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2796 - acc: 0.5979\n",
      "Epoch 00147: val_loss did not improve from 1.02300\n",
      "36805/36805 [==============================] - 28s 749us/sample - loss: 1.2796 - acc: 0.5978 - val_loss: 1.0461 - val_acc: 0.6921\n",
      "Epoch 148/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2814 - acc: 0.5963\n",
      "Epoch 00148: val_loss did not improve from 1.02300\n",
      "36805/36805 [==============================] - 28s 750us/sample - loss: 1.2814 - acc: 0.5964 - val_loss: 1.1078 - val_acc: 0.6532\n",
      "Epoch 149/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2841 - acc: 0.5974\n",
      "Epoch 00149: val_loss did not improve from 1.02300\n",
      "36805/36805 [==============================] - 28s 748us/sample - loss: 1.2846 - acc: 0.5971 - val_loss: 1.7212 - val_acc: 0.4955\n",
      "Epoch 150/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2875 - acc: 0.5949\n",
      "Epoch 00150: val_loss did not improve from 1.02300\n",
      "36805/36805 [==============================] - 28s 749us/sample - loss: 1.2872 - acc: 0.5949 - val_loss: 1.1956 - val_acc: 0.6224\n",
      "Epoch 151/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2791 - acc: 0.5999\n",
      "Epoch 00151: val_loss improved from 1.02300 to 1.01137, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_3_conv_checkpoint/151-1.0114.hdf5\n",
      "36805/36805 [==============================] - 28s 749us/sample - loss: 1.2791 - acc: 0.5999 - val_loss: 1.0114 - val_acc: 0.7160\n",
      "Epoch 152/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2780 - acc: 0.5974\n",
      "Epoch 00152: val_loss did not improve from 1.01137\n",
      "36805/36805 [==============================] - 27s 747us/sample - loss: 1.2779 - acc: 0.5974 - val_loss: 1.0364 - val_acc: 0.6983\n",
      "Epoch 153/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2689 - acc: 0.6020\n",
      "Epoch 00153: val_loss did not improve from 1.01137\n",
      "36805/36805 [==============================] - 27s 747us/sample - loss: 1.2687 - acc: 0.6021 - val_loss: 1.8621 - val_acc: 0.5015\n",
      "Epoch 154/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2770 - acc: 0.5988\n",
      "Epoch 00154: val_loss did not improve from 1.01137\n",
      "36805/36805 [==============================] - 28s 747us/sample - loss: 1.2772 - acc: 0.5988 - val_loss: 1.0287 - val_acc: 0.7130\n",
      "Epoch 155/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2780 - acc: 0.5976\n",
      "Epoch 00155: val_loss did not improve from 1.01137\n",
      "36805/36805 [==============================] - 28s 748us/sample - loss: 1.2779 - acc: 0.5975 - val_loss: 1.1267 - val_acc: 0.6317\n",
      "Epoch 156/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2698 - acc: 0.6022\n",
      "Epoch 00156: val_loss did not improve from 1.01137\n",
      "36805/36805 [==============================] - 27s 746us/sample - loss: 1.2698 - acc: 0.6022 - val_loss: 6.2089 - val_acc: 0.2471\n",
      "Epoch 157/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2674 - acc: 0.5997\n",
      "Epoch 00157: val_loss did not improve from 1.01137\n",
      "36805/36805 [==============================] - 28s 748us/sample - loss: 1.2675 - acc: 0.5998 - val_loss: 1.4484 - val_acc: 0.4934\n",
      "Epoch 158/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2648 - acc: 0.5985\n",
      "Epoch 00158: val_loss did not improve from 1.01137\n",
      "36805/36805 [==============================] - 27s 746us/sample - loss: 1.2642 - acc: 0.5987 - val_loss: 1.0210 - val_acc: 0.7009\n",
      "Epoch 159/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2739 - acc: 0.5989\n",
      "Epoch 00159: val_loss did not improve from 1.01137\n",
      "36805/36805 [==============================] - 28s 750us/sample - loss: 1.2743 - acc: 0.5990 - val_loss: 1.5886 - val_acc: 0.5050\n",
      "Epoch 160/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2641 - acc: 0.6060\n",
      "Epoch 00160: val_loss did not improve from 1.01137\n",
      "36805/36805 [==============================] - 27s 747us/sample - loss: 1.2640 - acc: 0.6059 - val_loss: 1.0877 - val_acc: 0.6706\n",
      "Epoch 161/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2614 - acc: 0.6046\n",
      "Epoch 00161: val_loss did not improve from 1.01137\n",
      "36805/36805 [==============================] - 27s 746us/sample - loss: 1.2614 - acc: 0.6047 - val_loss: 1.0680 - val_acc: 0.6741\n",
      "Epoch 162/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2741 - acc: 0.6012\n",
      "Epoch 00162: val_loss improved from 1.01137 to 1.00660, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_3_conv_checkpoint/162-1.0066.hdf5\n",
      "36805/36805 [==============================] - 28s 749us/sample - loss: 1.2745 - acc: 0.6011 - val_loss: 1.0066 - val_acc: 0.7172\n",
      "Epoch 163/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2625 - acc: 0.6021\n",
      "Epoch 00163: val_loss improved from 1.00660 to 0.99281, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_3_conv_checkpoint/163-0.9928.hdf5\n",
      "36805/36805 [==============================] - 28s 747us/sample - loss: 1.2632 - acc: 0.6020 - val_loss: 0.9928 - val_acc: 0.7205\n",
      "Epoch 164/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2719 - acc: 0.5995\n",
      "Epoch 00164: val_loss did not improve from 0.99281\n",
      "36805/36805 [==============================] - 27s 745us/sample - loss: 1.2719 - acc: 0.5995 - val_loss: 1.1592 - val_acc: 0.6310\n",
      "Epoch 165/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2660 - acc: 0.6044\n",
      "Epoch 00165: val_loss improved from 0.99281 to 0.98732, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_3_conv_checkpoint/165-0.9873.hdf5\n",
      "36805/36805 [==============================] - 28s 752us/sample - loss: 1.2661 - acc: 0.6045 - val_loss: 0.9873 - val_acc: 0.7209\n",
      "Epoch 166/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2660 - acc: 0.6028\n",
      "Epoch 00166: val_loss did not improve from 0.98732\n",
      "36805/36805 [==============================] - 28s 748us/sample - loss: 1.2658 - acc: 0.6029 - val_loss: 1.1163 - val_acc: 0.6580\n",
      "Epoch 167/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2605 - acc: 0.6047\n",
      "Epoch 00167: val_loss did not improve from 0.98732\n",
      "36805/36805 [==============================] - 27s 746us/sample - loss: 1.2605 - acc: 0.6046 - val_loss: 1.4501 - val_acc: 0.5614\n",
      "Epoch 168/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2641 - acc: 0.6040\n",
      "Epoch 00168: val_loss did not improve from 0.98732\n",
      "36805/36805 [==============================] - 28s 749us/sample - loss: 1.2643 - acc: 0.6038 - val_loss: 1.0018 - val_acc: 0.7133\n",
      "Epoch 169/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2547 - acc: 0.6045\n",
      "Epoch 00169: val_loss did not improve from 0.98732\n",
      "36805/36805 [==============================] - 28s 748us/sample - loss: 1.2543 - acc: 0.6048 - val_loss: 1.0401 - val_acc: 0.7088\n",
      "Epoch 170/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2525 - acc: 0.6081\n",
      "Epoch 00170: val_loss did not improve from 0.98732\n",
      "36805/36805 [==============================] - 28s 749us/sample - loss: 1.2525 - acc: 0.6081 - val_loss: 1.0339 - val_acc: 0.7039\n",
      "Epoch 171/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2603 - acc: 0.6040\n",
      "Epoch 00171: val_loss did not improve from 0.98732\n",
      "36805/36805 [==============================] - 27s 745us/sample - loss: 1.2599 - acc: 0.6040 - val_loss: 1.0219 - val_acc: 0.6911\n",
      "Epoch 172/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2589 - acc: 0.6050\n",
      "Epoch 00172: val_loss did not improve from 0.98732\n",
      "36805/36805 [==============================] - 28s 750us/sample - loss: 1.2587 - acc: 0.6050 - val_loss: 1.1504 - val_acc: 0.6394\n",
      "Epoch 173/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2620 - acc: 0.6033\n",
      "Epoch 00173: val_loss did not improve from 0.98732\n",
      "36805/36805 [==============================] - 27s 746us/sample - loss: 1.2619 - acc: 0.6032 - val_loss: 1.2471 - val_acc: 0.5667\n",
      "Epoch 174/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2520 - acc: 0.6070\n",
      "Epoch 00174: val_loss did not improve from 0.98732\n",
      "36805/36805 [==============================] - 27s 745us/sample - loss: 1.2523 - acc: 0.6069 - val_loss: 0.9935 - val_acc: 0.7249\n",
      "Epoch 175/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2570 - acc: 0.6032\n",
      "Epoch 00175: val_loss did not improve from 0.98732\n",
      "36805/36805 [==============================] - 27s 747us/sample - loss: 1.2569 - acc: 0.6033 - val_loss: 1.0638 - val_acc: 0.6669\n",
      "Epoch 176/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2511 - acc: 0.6045\n",
      "Epoch 00176: val_loss did not improve from 0.98732\n",
      "36805/36805 [==============================] - 27s 747us/sample - loss: 1.2513 - acc: 0.6046 - val_loss: 1.1364 - val_acc: 0.6431\n",
      "Epoch 177/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2518 - acc: 0.6096\n",
      "Epoch 00177: val_loss improved from 0.98732 to 0.98671, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_3_conv_checkpoint/177-0.9867.hdf5\n",
      "36805/36805 [==============================] - 27s 746us/sample - loss: 1.2521 - acc: 0.6095 - val_loss: 0.9867 - val_acc: 0.7216\n",
      "Epoch 178/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2545 - acc: 0.6103\n",
      "Epoch 00178: val_loss did not improve from 0.98671\n",
      "36805/36805 [==============================] - 28s 748us/sample - loss: 1.2545 - acc: 0.6103 - val_loss: 1.1023 - val_acc: 0.6792\n",
      "Epoch 179/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2464 - acc: 0.6140\n",
      "Epoch 00179: val_loss did not improve from 0.98671\n",
      "36805/36805 [==============================] - 27s 744us/sample - loss: 1.2465 - acc: 0.6140 - val_loss: 2.4541 - val_acc: 0.4377\n",
      "Epoch 180/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2435 - acc: 0.6103\n",
      "Epoch 00180: val_loss did not improve from 0.98671\n",
      "36805/36805 [==============================] - 28s 749us/sample - loss: 1.2444 - acc: 0.6102 - val_loss: 1.0377 - val_acc: 0.6825\n",
      "Epoch 181/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2475 - acc: 0.6062\n",
      "Epoch 00181: val_loss did not improve from 0.98671\n",
      "36805/36805 [==============================] - 27s 745us/sample - loss: 1.2477 - acc: 0.6062 - val_loss: 1.0420 - val_acc: 0.6834\n",
      "Epoch 182/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2465 - acc: 0.6107\n",
      "Epoch 00182: val_loss did not improve from 0.98671\n",
      "36805/36805 [==============================] - 27s 747us/sample - loss: 1.2463 - acc: 0.6107 - val_loss: 2.1565 - val_acc: 0.4570\n",
      "Epoch 183/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2417 - acc: 0.6088\n",
      "Epoch 00183: val_loss did not improve from 0.98671\n",
      "36805/36805 [==============================] - 27s 744us/sample - loss: 1.2412 - acc: 0.6090 - val_loss: 1.0292 - val_acc: 0.6972\n",
      "Epoch 184/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2419 - acc: 0.6099\n",
      "Epoch 00184: val_loss did not improve from 0.98671\n",
      "36805/36805 [==============================] - 28s 748us/sample - loss: 1.2425 - acc: 0.6099 - val_loss: 0.9992 - val_acc: 0.7219\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 185/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2485 - acc: 0.6091\n",
      "Epoch 00185: val_loss did not improve from 0.98671\n",
      "36805/36805 [==============================] - 27s 746us/sample - loss: 1.2485 - acc: 0.6091 - val_loss: 1.1010 - val_acc: 0.6327\n",
      "Epoch 186/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2406 - acc: 0.6078\n",
      "Epoch 00186: val_loss did not improve from 0.98671\n",
      "36805/36805 [==============================] - 27s 745us/sample - loss: 1.2408 - acc: 0.6079 - val_loss: 0.9926 - val_acc: 0.7144\n",
      "Epoch 187/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2476 - acc: 0.6128\n",
      "Epoch 00187: val_loss improved from 0.98671 to 0.97086, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_3_conv_checkpoint/187-0.9709.hdf5\n",
      "36805/36805 [==============================] - 28s 750us/sample - loss: 1.2475 - acc: 0.6127 - val_loss: 0.9709 - val_acc: 0.7265\n",
      "Epoch 188/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2389 - acc: 0.6153\n",
      "Epoch 00188: val_loss improved from 0.97086 to 0.95438, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_3_conv_checkpoint/188-0.9544.hdf5\n",
      "36805/36805 [==============================] - 28s 748us/sample - loss: 1.2385 - acc: 0.6153 - val_loss: 0.9544 - val_acc: 0.7335\n",
      "Epoch 189/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2411 - acc: 0.6125\n",
      "Epoch 00189: val_loss did not improve from 0.95438\n",
      "36805/36805 [==============================] - 27s 746us/sample - loss: 1.2407 - acc: 0.6125 - val_loss: 0.9961 - val_acc: 0.7044\n",
      "Epoch 190/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2351 - acc: 0.6138\n",
      "Epoch 00190: val_loss did not improve from 0.95438\n",
      "36805/36805 [==============================] - 27s 745us/sample - loss: 1.2348 - acc: 0.6139 - val_loss: 1.2777 - val_acc: 0.6045\n",
      "Epoch 191/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2387 - acc: 0.6109\n",
      "Epoch 00191: val_loss did not improve from 0.95438\n",
      "36805/36805 [==============================] - 27s 745us/sample - loss: 1.2388 - acc: 0.6108 - val_loss: 1.8737 - val_acc: 0.4722\n",
      "Epoch 192/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2419 - acc: 0.6150\n",
      "Epoch 00192: val_loss did not improve from 0.95438\n",
      "36805/36805 [==============================] - 27s 742us/sample - loss: 1.2420 - acc: 0.6150 - val_loss: 1.2179 - val_acc: 0.6198\n",
      "Epoch 193/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2322 - acc: 0.6145\n",
      "Epoch 00193: val_loss did not improve from 0.95438\n",
      "36805/36805 [==============================] - 27s 747us/sample - loss: 1.2324 - acc: 0.6143 - val_loss: 0.9888 - val_acc: 0.7179\n",
      "Epoch 194/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2309 - acc: 0.6176\n",
      "Epoch 00194: val_loss did not improve from 0.95438\n",
      "36805/36805 [==============================] - 27s 746us/sample - loss: 1.2312 - acc: 0.6175 - val_loss: 1.0826 - val_acc: 0.6790\n",
      "Epoch 195/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2300 - acc: 0.6165\n",
      "Epoch 00195: val_loss did not improve from 0.95438\n",
      "36805/36805 [==============================] - 27s 745us/sample - loss: 1.2302 - acc: 0.6164 - val_loss: 1.1136 - val_acc: 0.6643\n",
      "Epoch 196/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2337 - acc: 0.6139\n",
      "Epoch 00196: val_loss did not improve from 0.95438\n",
      "36805/36805 [==============================] - 28s 749us/sample - loss: 1.2334 - acc: 0.6140 - val_loss: 1.0157 - val_acc: 0.6827\n",
      "Epoch 197/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2318 - acc: 0.6132\n",
      "Epoch 00197: val_loss did not improve from 0.95438\n",
      "36805/36805 [==============================] - 27s 745us/sample - loss: 1.2325 - acc: 0.6131 - val_loss: 1.0197 - val_acc: 0.6974\n",
      "Epoch 198/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2343 - acc: 0.6119\n",
      "Epoch 00198: val_loss did not improve from 0.95438\n",
      "36805/36805 [==============================] - 27s 744us/sample - loss: 1.2338 - acc: 0.6121 - val_loss: 0.9651 - val_acc: 0.7200\n",
      "Epoch 199/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2270 - acc: 0.6190\n",
      "Epoch 00199: val_loss did not improve from 0.95438\n",
      "36805/36805 [==============================] - 28s 747us/sample - loss: 1.2272 - acc: 0.6189 - val_loss: 1.0696 - val_acc: 0.6567\n",
      "Epoch 200/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2317 - acc: 0.6150\n",
      "Epoch 00200: val_loss did not improve from 0.95438\n",
      "36805/36805 [==============================] - 27s 747us/sample - loss: 1.2317 - acc: 0.6149 - val_loss: 1.1821 - val_acc: 0.5938\n",
      "Epoch 201/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2315 - acc: 0.6155\n",
      "Epoch 00201: val_loss improved from 0.95438 to 0.94257, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_3_conv_checkpoint/201-0.9426.hdf5\n",
      "36805/36805 [==============================] - 28s 751us/sample - loss: 1.2315 - acc: 0.6155 - val_loss: 0.9426 - val_acc: 0.7370\n",
      "Epoch 202/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2245 - acc: 0.6209\n",
      "Epoch 00202: val_loss did not improve from 0.94257\n",
      "36805/36805 [==============================] - 27s 747us/sample - loss: 1.2246 - acc: 0.6208 - val_loss: 0.9955 - val_acc: 0.7135\n",
      "Epoch 203/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2258 - acc: 0.6184\n",
      "Epoch 00203: val_loss did not improve from 0.94257\n",
      "36805/36805 [==============================] - 27s 745us/sample - loss: 1.2259 - acc: 0.6186 - val_loss: 0.9700 - val_acc: 0.7249\n",
      "Epoch 204/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2235 - acc: 0.6136\n",
      "Epoch 00204: val_loss did not improve from 0.94257\n",
      "36805/36805 [==============================] - 28s 750us/sample - loss: 1.2237 - acc: 0.6134 - val_loss: 0.9608 - val_acc: 0.7261\n",
      "Epoch 205/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2280 - acc: 0.6174\n",
      "Epoch 00205: val_loss did not improve from 0.94257\n",
      "36805/36805 [==============================] - 27s 746us/sample - loss: 1.2281 - acc: 0.6173 - val_loss: 1.1449 - val_acc: 0.6268\n",
      "Epoch 206/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2247 - acc: 0.6140\n",
      "Epoch 00206: val_loss did not improve from 0.94257\n",
      "36805/36805 [==============================] - 28s 748us/sample - loss: 1.2253 - acc: 0.6138 - val_loss: 0.9796 - val_acc: 0.7163\n",
      "Epoch 207/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2244 - acc: 0.6164\n",
      "Epoch 00207: val_loss improved from 0.94257 to 0.93351, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_3_conv_checkpoint/207-0.9335.hdf5\n",
      "36805/36805 [==============================] - 27s 746us/sample - loss: 1.2244 - acc: 0.6164 - val_loss: 0.9335 - val_acc: 0.7428\n",
      "Epoch 208/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2191 - acc: 0.6196\n",
      "Epoch 00208: val_loss did not improve from 0.93351\n",
      "36805/36805 [==============================] - 28s 749us/sample - loss: 1.2196 - acc: 0.6195 - val_loss: 0.9996 - val_acc: 0.7207\n",
      "Epoch 209/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2247 - acc: 0.6176\n",
      "Epoch 00209: val_loss did not improve from 0.93351\n",
      "36805/36805 [==============================] - 27s 743us/sample - loss: 1.2251 - acc: 0.6175 - val_loss: 1.0995 - val_acc: 0.6504\n",
      "Epoch 210/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2167 - acc: 0.6203\n",
      "Epoch 00210: val_loss did not improve from 0.93351\n",
      "36805/36805 [==============================] - 27s 747us/sample - loss: 1.2166 - acc: 0.6204 - val_loss: 1.4862 - val_acc: 0.4976\n",
      "Epoch 211/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2212 - acc: 0.6202\n",
      "Epoch 00211: val_loss did not improve from 0.93351\n",
      "36805/36805 [==============================] - 27s 746us/sample - loss: 1.2217 - acc: 0.6201 - val_loss: 0.9560 - val_acc: 0.7328\n",
      "Epoch 212/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2158 - acc: 0.6182\n",
      "Epoch 00212: val_loss did not improve from 0.93351\n",
      "36805/36805 [==============================] - 27s 746us/sample - loss: 1.2162 - acc: 0.6181 - val_loss: 1.0269 - val_acc: 0.6916\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 213/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2253 - acc: 0.6180\n",
      "Epoch 00213: val_loss did not improve from 0.93351\n",
      "36805/36805 [==============================] - 27s 747us/sample - loss: 1.2256 - acc: 0.6179 - val_loss: 0.9431 - val_acc: 0.7331\n",
      "Epoch 214/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2188 - acc: 0.6179\n",
      "Epoch 00214: val_loss did not improve from 0.93351\n",
      "36805/36805 [==============================] - 27s 745us/sample - loss: 1.2187 - acc: 0.6180 - val_loss: 1.4114 - val_acc: 0.5700\n",
      "Epoch 215/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2124 - acc: 0.6209\n",
      "Epoch 00215: val_loss did not improve from 0.93351\n",
      "36805/36805 [==============================] - 27s 745us/sample - loss: 1.2126 - acc: 0.6209 - val_loss: 0.9378 - val_acc: 0.7400\n",
      "Epoch 216/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2129 - acc: 0.6231\n",
      "Epoch 00216: val_loss did not improve from 0.93351\n",
      "36805/36805 [==============================] - 28s 748us/sample - loss: 1.2127 - acc: 0.6230 - val_loss: 0.9678 - val_acc: 0.7263\n",
      "Epoch 217/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2163 - acc: 0.6191\n",
      "Epoch 00217: val_loss did not improve from 0.93351\n",
      "36805/36805 [==============================] - 27s 745us/sample - loss: 1.2162 - acc: 0.6191 - val_loss: 1.0182 - val_acc: 0.7028\n",
      "Epoch 218/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2130 - acc: 0.6194\n",
      "Epoch 00218: val_loss did not improve from 0.93351\n",
      "36805/36805 [==============================] - 28s 749us/sample - loss: 1.2130 - acc: 0.6194 - val_loss: 0.9799 - val_acc: 0.7184\n",
      "Epoch 219/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2204 - acc: 0.6184\n",
      "Epoch 00219: val_loss did not improve from 0.93351\n",
      "36805/36805 [==============================] - 27s 746us/sample - loss: 1.2206 - acc: 0.6184 - val_loss: 0.9921 - val_acc: 0.7093\n",
      "Epoch 220/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2081 - acc: 0.6220\n",
      "Epoch 00220: val_loss did not improve from 0.93351\n",
      "36805/36805 [==============================] - 27s 747us/sample - loss: 1.2082 - acc: 0.6220 - val_loss: 0.9903 - val_acc: 0.7233\n",
      "Epoch 221/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2143 - acc: 0.6213\n",
      "Epoch 00221: val_loss did not improve from 0.93351\n",
      "36805/36805 [==============================] - 27s 745us/sample - loss: 1.2140 - acc: 0.6214 - val_loss: 1.2184 - val_acc: 0.6205\n",
      "Epoch 222/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2087 - acc: 0.6236\n",
      "Epoch 00222: val_loss did not improve from 0.93351\n",
      "36805/36805 [==============================] - 28s 750us/sample - loss: 1.2087 - acc: 0.6234 - val_loss: 2.2604 - val_acc: 0.4144\n",
      "Epoch 223/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2136 - acc: 0.6212\n",
      "Epoch 00223: val_loss did not improve from 0.93351\n",
      "36805/36805 [==============================] - 27s 746us/sample - loss: 1.2138 - acc: 0.6211 - val_loss: 1.3099 - val_acc: 0.5486\n",
      "Epoch 224/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2164 - acc: 0.6211\n",
      "Epoch 00224: val_loss did not improve from 0.93351\n",
      "36805/36805 [==============================] - 28s 749us/sample - loss: 1.2167 - acc: 0.6211 - val_loss: 1.2980 - val_acc: 0.5628\n",
      "Epoch 225/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2036 - acc: 0.6232\n",
      "Epoch 00225: val_loss did not improve from 0.93351\n",
      "36805/36805 [==============================] - 28s 748us/sample - loss: 1.2033 - acc: 0.6233 - val_loss: 0.9995 - val_acc: 0.7119\n",
      "Epoch 226/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2056 - acc: 0.6230\n",
      "Epoch 00226: val_loss did not improve from 0.93351\n",
      "36805/36805 [==============================] - 28s 749us/sample - loss: 1.2060 - acc: 0.6229 - val_loss: 0.9811 - val_acc: 0.7258\n",
      "Epoch 227/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2107 - acc: 0.6222\n",
      "Epoch 00227: val_loss did not improve from 0.93351\n",
      "36805/36805 [==============================] - 28s 747us/sample - loss: 1.2111 - acc: 0.6221 - val_loss: 1.1611 - val_acc: 0.6250\n",
      "Epoch 228/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2077 - acc: 0.6230\n",
      "Epoch 00228: val_loss did not improve from 0.93351\n",
      "36805/36805 [==============================] - 27s 746us/sample - loss: 1.2078 - acc: 0.6230 - val_loss: 1.1109 - val_acc: 0.6452\n",
      "Epoch 229/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2112 - acc: 0.6201\n",
      "Epoch 00229: val_loss did not improve from 0.93351\n",
      "36805/36805 [==============================] - 28s 749us/sample - loss: 1.2112 - acc: 0.6201 - val_loss: 2.8626 - val_acc: 0.3154\n",
      "Epoch 230/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2100 - acc: 0.6205\n",
      "Epoch 00230: val_loss did not improve from 0.93351\n",
      "36805/36805 [==============================] - 27s 745us/sample - loss: 1.2099 - acc: 0.6205 - val_loss: 1.0839 - val_acc: 0.6604\n",
      "Epoch 231/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2025 - acc: 0.6212\n",
      "Epoch 00231: val_loss did not improve from 0.93351\n",
      "36805/36805 [==============================] - 27s 747us/sample - loss: 1.2024 - acc: 0.6212 - val_loss: 1.3979 - val_acc: 0.5637\n",
      "Epoch 232/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2049 - acc: 0.6234\n",
      "Epoch 00232: val_loss did not improve from 0.93351\n",
      "36805/36805 [==============================] - 28s 749us/sample - loss: 1.2057 - acc: 0.6233 - val_loss: 0.9711 - val_acc: 0.7198\n",
      "Epoch 233/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2044 - acc: 0.6234\n",
      "Epoch 00233: val_loss did not improve from 0.93351\n",
      "36805/36805 [==============================] - 27s 746us/sample - loss: 1.2045 - acc: 0.6235 - val_loss: 1.0645 - val_acc: 0.6504\n",
      "Epoch 234/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2051 - acc: 0.6231\n",
      "Epoch 00234: val_loss did not improve from 0.93351\n",
      "36805/36805 [==============================] - 27s 746us/sample - loss: 1.2050 - acc: 0.6231 - val_loss: 1.2246 - val_acc: 0.6021\n",
      "Epoch 235/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1990 - acc: 0.6266\n",
      "Epoch 00235: val_loss did not improve from 0.93351\n",
      "36805/36805 [==============================] - 27s 746us/sample - loss: 1.1993 - acc: 0.6265 - val_loss: 1.0208 - val_acc: 0.7037\n",
      "Epoch 236/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2105 - acc: 0.6249\n",
      "Epoch 00236: val_loss did not improve from 0.93351\n",
      "36805/36805 [==============================] - 27s 745us/sample - loss: 1.2105 - acc: 0.6250 - val_loss: 1.0286 - val_acc: 0.6990\n",
      "Epoch 237/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2053 - acc: 0.6241\n",
      "Epoch 00237: val_loss did not improve from 0.93351\n",
      "36805/36805 [==============================] - 27s 744us/sample - loss: 1.2053 - acc: 0.6241 - val_loss: 1.9670 - val_acc: 0.4510\n",
      "Epoch 238/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2085 - acc: 0.6242\n",
      "Epoch 00238: val_loss did not improve from 0.93351\n",
      "36805/36805 [==============================] - 27s 744us/sample - loss: 1.2083 - acc: 0.6242 - val_loss: 0.9420 - val_acc: 0.7368\n",
      "Epoch 239/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2016 - acc: 0.6282\n",
      "Epoch 00239: val_loss did not improve from 0.93351\n",
      "36805/36805 [==============================] - 27s 746us/sample - loss: 1.2015 - acc: 0.6282 - val_loss: 0.9388 - val_acc: 0.7421\n",
      "Epoch 240/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1948 - acc: 0.6286\n",
      "Epoch 00240: val_loss did not improve from 0.93351\n",
      "36805/36805 [==============================] - 27s 744us/sample - loss: 1.1947 - acc: 0.6286 - val_loss: 0.9509 - val_acc: 0.7233\n",
      "Epoch 241/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1981 - acc: 0.6279\n",
      "Epoch 00241: val_loss did not improve from 0.93351\n",
      "36805/36805 [==============================] - 28s 749us/sample - loss: 1.1985 - acc: 0.6278 - val_loss: 0.9524 - val_acc: 0.7303\n",
      "Epoch 242/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1976 - acc: 0.6243\n",
      "Epoch 00242: val_loss did not improve from 0.93351\n",
      "36805/36805 [==============================] - 28s 749us/sample - loss: 1.1976 - acc: 0.6243 - val_loss: 1.4361 - val_acc: 0.5618\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 243/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1980 - acc: 0.6265\n",
      "Epoch 00243: val_loss did not improve from 0.93351\n",
      "36805/36805 [==============================] - 28s 748us/sample - loss: 1.1981 - acc: 0.6265 - val_loss: 1.0009 - val_acc: 0.7170\n",
      "Epoch 244/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1987 - acc: 0.6252\n",
      "Epoch 00244: val_loss did not improve from 0.93351\n",
      "36805/36805 [==============================] - 28s 748us/sample - loss: 1.1988 - acc: 0.6251 - val_loss: 0.9686 - val_acc: 0.7098\n",
      "Epoch 245/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2013 - acc: 0.6250\n",
      "Epoch 00245: val_loss did not improve from 0.93351\n",
      "36805/36805 [==============================] - 28s 749us/sample - loss: 1.2012 - acc: 0.6250 - val_loss: 0.9851 - val_acc: 0.7102\n",
      "Epoch 246/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1959 - acc: 0.6273\n",
      "Epoch 00246: val_loss did not improve from 0.93351\n",
      "36805/36805 [==============================] - 27s 747us/sample - loss: 1.1955 - acc: 0.6276 - val_loss: 1.0779 - val_acc: 0.6436\n",
      "Epoch 247/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1882 - acc: 0.6296\n",
      "Epoch 00247: val_loss did not improve from 0.93351\n",
      "36805/36805 [==============================] - 27s 745us/sample - loss: 1.1881 - acc: 0.6296 - val_loss: 1.4422 - val_acc: 0.5139\n",
      "Epoch 248/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1948 - acc: 0.6257\n",
      "Epoch 00248: val_loss did not improve from 0.93351\n",
      "36805/36805 [==============================] - 27s 746us/sample - loss: 1.1947 - acc: 0.6257 - val_loss: 1.0181 - val_acc: 0.6809\n",
      "Epoch 249/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1978 - acc: 0.6290\n",
      "Epoch 00249: val_loss did not improve from 0.93351\n",
      "36805/36805 [==============================] - 28s 748us/sample - loss: 1.1974 - acc: 0.6292 - val_loss: 1.2161 - val_acc: 0.6005\n",
      "Epoch 250/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1953 - acc: 0.6292\n",
      "Epoch 00250: val_loss did not improve from 0.93351\n",
      "36805/36805 [==============================] - 27s 745us/sample - loss: 1.1952 - acc: 0.6292 - val_loss: 1.6000 - val_acc: 0.5269\n",
      "Epoch 251/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1947 - acc: 0.6286\n",
      "Epoch 00251: val_loss did not improve from 0.93351\n",
      "36805/36805 [==============================] - 28s 747us/sample - loss: 1.1948 - acc: 0.6287 - val_loss: 0.9579 - val_acc: 0.7212\n",
      "Epoch 252/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1931 - acc: 0.6263\n",
      "Epoch 00252: val_loss did not improve from 0.93351\n",
      "36805/36805 [==============================] - 27s 742us/sample - loss: 1.1932 - acc: 0.6262 - val_loss: 1.5716 - val_acc: 0.5320\n",
      "Epoch 253/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1951 - acc: 0.6278\n",
      "Epoch 00253: val_loss did not improve from 0.93351\n",
      "36805/36805 [==============================] - 28s 747us/sample - loss: 1.1960 - acc: 0.6277 - val_loss: 1.0429 - val_acc: 0.6881\n",
      "Epoch 254/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1907 - acc: 0.6295\n",
      "Epoch 00254: val_loss did not improve from 0.93351\n",
      "36805/36805 [==============================] - 27s 743us/sample - loss: 1.1910 - acc: 0.6295 - val_loss: 0.9686 - val_acc: 0.7091\n",
      "Epoch 255/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1946 - acc: 0.6284\n",
      "Epoch 00255: val_loss did not improve from 0.93351\n",
      "36805/36805 [==============================] - 27s 747us/sample - loss: 1.1945 - acc: 0.6284 - val_loss: 1.0749 - val_acc: 0.6627\n",
      "Epoch 256/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1891 - acc: 0.6317\n",
      "Epoch 00256: val_loss did not improve from 0.93351\n",
      "36805/36805 [==============================] - 27s 745us/sample - loss: 1.1891 - acc: 0.6318 - val_loss: 1.1779 - val_acc: 0.6238\n",
      "Epoch 257/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1895 - acc: 0.6295\n",
      "Epoch 00257: val_loss did not improve from 0.93351\n",
      "36805/36805 [==============================] - 27s 746us/sample - loss: 1.1898 - acc: 0.6295 - val_loss: 1.0799 - val_acc: 0.6723\n",
      "\n",
      "1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_3_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEKCAYAAAARnO4WAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzsnXd4FNX6xz8nm03vhRY6KJ2EjkZBRIoiKCpiV7yC3mu5iqL8vOr16vWCHRsiKIpSFBFUBEVUEESK9N5JSAKppLdt5/fHyWSTkEACuyQh5/M8+8zszJmZM7Oz3/Oe97zzjpBSotFoNJqLH4/aroBGo9FoLgxa8DUajaaBoAVfo9FoGgha8DUajaaBoAVfo9FoGgha8DUajaaBoAVfo9FoGgha8DUajaaBoAVfo9FoGgietV2BskRERMjWrVvXdjU0Go2m3rBly5Z0KWVkdcrWKcFv3bo1mzdvru1qaDQaTb1BCBFf3bLapaPRaDQNBC34Go1G00DQgq/RaDQNhDrlw68Mq9VKYmIiRUVFtV2VeomPjw/NmzfHbDbXdlU0Gk0tU+cFPzExkcDAQFq3bo0QorarU6+QUpKRkUFiYiJt2rSp7epoNJpaps67dIqKiggPD9difw4IIQgPD9e9I41GA9QDwQe02J8H+tppNBqDeiH4Gk2DZvduWLeutmuhuQjQgn8WsrKymD59+jlte91115GVlVXt8i+++CJvvPHGOR1LcxHz0kvw8MO1XQvNRYAW/LNwJsG32Wxn3Hb58uWEhIS4o1qahoTFAlZrbddCcxGgBf8sTJ48mSNHjhATE8OkSZNYvXo1V155JaNGjaJz584A3HjjjfTq1YsuXbowc+bM0m1bt25Neno6cXFxdOrUifHjx9OlSxeGDh1KYWHhGY+7fft2+vfvT/fu3Rk9ejSZmZkAvPvuu3Tu3Jnu3btz2223AfD7778TExNDTEwMPXr0IDc3101XQ1MrOBzqo9GcJ24NyxRChAAfA10BCdwvpVx/rvs7dOhx8vK2u6p6AAQExHDJJdOqXD916lR2797N9u3quKtXr2br1q3s3r27NNRx9uzZhIWFUVhYSJ8+fbj55psJDw+vUPdDLFiwgFmzZnHrrbfyzTffcNddd1V53HvuuYf33nuPgQMH8sILL/Cf//yHadOmMXXqVI4dO4a3t3epu+iNN97ggw8+IDY2lry8PHx8fM73smjqElrwNS7C3Rb+O8BPUsqOQDSwz83HuyD07du3XFz7u+++S3R0NP379ychIYFDhw6dtk2bNm2IiYkBoFevXsTFxVW5/+zsbLKyshg4cCAA9957L2vWrAGge/fu3HnnncydOxdPT9Vex8bGMnHiRN59912ysrJKl2suEqTUgq9xCW5TBiFEMDAAuA9ASmkBLOezzzNZ4hcSf3//0vnVq1fzyy+/sH79evz8/LjqqqsqjXv39vYunTeZTGd16VTFsmXLWLNmDUuXLuWVV15h165dTJ48mREjRrB8+XJiY2NZsWIFHTt2PKf9a+og2sLXuAh3WvhtgDTgUyHENiHEx0II/7NtVNcIDAw8o088Ozub0NBQ/Pz82L9/Pxs2bDjvYwYHBxMaGsratWsB+OKLLxg4cCAOh4OEhAQGDRrEq6++SnZ2Nnl5eRw5coRu3brxzDPP0KdPH/bv33/eddDUIbTga1yEO/v+nkBP4FEp5UYhxDvAZOD5soWEEBOACQAtW7Z0Y3XOjfDwcGJjY+natSvXXnstI0aMKLd++PDhzJgxg06dOtGhQwf69+/vkuPOmTOHhx56iIKCAtq2bcunn36K3W7nrrvuIjs7Gykljz32GCEhITz//POsWrUKDw8PunTpwrXXXuuSOmjqCFrwNS5CSCnds2MhmgAbpJStS75fCUyWUo6oapvevXvLii9A2bdvH506dXJLHRsK+hrWcwYPhoMHISGhtmuiqYMIIbZIKXtXp6zbXDpSymQgQQjRoWTRYGCvu46n0Vy0aAtf4yLcHc7xKDBPCOEFHAXGufl4Gs3Fh47S0bgItwq+lHI7UK2uhkajqQJt4WtchH7SVqOp62jB17gILfgaTV1HC77GRWjB12jqOlrwNS5CC74bCAgIqNFyjeaMaMHXuAgt+BpNXUdK9dFozhMt+Gdh8uTJfPDBB6XfjZeU5OXlMXjwYHr27Em3bt347rvvqr1PKSWTJk2ia9eudOvWja+++gqAkydPMmDAAGJiYujatStr167Fbrdz3333lZZ9++23XX6OmjqOtvA1LqJ+pVV8/HHY7tr0yMTEwLSqk7KNHTuWxx9/nIdL3ji0cOFCVqxYgY+PD0uWLCEoKIj09HT69+/PqFGjqvUO2cWLF7N9+3Z27NhBeno6ffr0YcCAAcyfP59hw4bxr3/9C7vdTkFBAdu3bycpKYndu3cD1OgNWpqLBC34GhdRvwS/FujRowepqamcOHGCtLQ0QkNDadGiBVarlWeffZY1a9bg4eFBUlISKSkpNGnS5Kz7/OOPP7j99tsxmUw0btyYgQMH8tdff9GnTx/uv/9+rFYrN954IzExMbRt25ajR4/y6KOPMmLECIYOHXoBzlpTp9CCr3ER9Uvwz2CJu5MxY8awaNEikpOTGTt2LADz5s0jLS2NLVu2YDabad26daVpkWvCgAEDWLNmDcuWLeO+++5j4sSJ3HPPPezYsYMVK1YwY8YMFi5cyOzZs11xWpr6ghZ8jYvQPvxqMHbsWL788ksWLVrEmDFjAJUWuVGjRpjNZlatWkV8fHy193fllVfy1VdfYbfbSUtLY82aNfTt25f4+HgaN27M+PHjeeCBB9i6dSvp6ek4HA5uvvlm/vvf/7J161Z3naamrqIFX+Mi6peFX0t06dKF3NxcoqKiaNq0KQB33nknI0eOpFu3bvTu3btGLxwZPXo069evJzo6GiEEr732Gk2aNGHOnDm8/vrrmM1mAgIC+Pzzz0lKSmLcuHE4Sv7wU6ZMccs5auowOpeOxkW4LT3yuaDTI7sHfQ3rOR07woEDSvSrERSgaVjUifTIGo3GRRjWfR0yzjT1Ey34Gk1dxxB87dbRnCda8DWauo4WfI2L0IKv0dR1tOBrXIQWfI2mrqMFX+MitOBrNHUdY7BWC77mPNGCfxaysrKYPn36OW173XXX6dw3mvNHW/gaF6EF/yycSfBtNtsZt12+fDkhISHuqJamIaEFX+MitOCfhcmTJ3PkyBFiYmKYNGkSq1ev5sorr2TUqFF07twZgBtvvJFevXrRpUsXZs6cWbpt69atSU9PJy4ujk6dOjF+/Hi6dOnC0KFDKSwsPO1YS5cupV+/fvTo0YNrrrmGlJQUAPLy8hg3bhzdunWje/fufPPNNwD89NNP9OzZk+joaAYPHnwBroamVtCCr3ER9Sq1Qi1kR2bq1Kns3r2b7SUHXr16NVu3bmX37t20adMGgNmzZxMWFkZhYSF9+vTh5ptvJjw8vNx+Dh06xIIFC5g1axa33nor33zzDXfddVe5MldccQUbNmxACMHHH3/Ma6+9xptvvsnLL79McHAwu3btAiAzM5O0tDTGjx/PmjVraNOmDadOnXLhVdHUKbTga1xEvRL8ukLfvn1LxR7g3XffZcmSJQAkJCRw6NCh0wS/TZs2xMTEANCrVy/i4uJO229iYiJjx47l5MmTWCyW0mP88ssvfPnll6XlQkNDWbp0KQMGDCgtExYW5tJz1NQhtOBrXES9Evxayo58Gv7+/qXzq1ev5pdffmH9+vX4+flx1VVXVZom2dvbu3TeZDJV6tJ59NFHmThxIqNGjWL16tW8+OKLbqm/pp6ho3Q0LsKtPnwhRJwQYpcQYrsQYvPZt6h7BAYGkpubW+X67OxsQkND8fPzY//+/WzYsOGcj5WdnU1UVBQAc+bMKV0+ZMiQcq9ZzMzMpH///qxZs4Zjx44BaJfOxYy28DUu4kIM2g6SUsZUN5tbXSM8PJzY2Fi6du3KpEmTTls/fPhwbDYbnTp1YvLkyfTv3/+cj/Xiiy8yZswYevXqRUREROny5557jszMTLp27Up0dDSrVq0iMjKSmTNnctNNNxEdHV36YhbNRYgWfI2LcGt6ZCFEHNBbSplenfI6PbJ70NewnhMQAPn5EB8PLVvWdm00dYy6lB5ZAj8LIbYIISa4+VgazcWJtvA1LsLdg7ZXSCmThBCNgJVCiP1SyjVlC5Q0BBMAWmrrRaM5HS34GhfhVgtfSplUMk0FlgB9KykzU0rZW0rZOzIy0p3V0WjqJzpKR+Mi3Cb4Qgh/IUSgMQ8MBXa763gazUWLtvA1LsKdLp3GwBKh3sHpCcyXUv7kxuNpNBcnWvA1LsJtgi+lPApEu2v/Gk2DQQu+xkXo5GluICAgoLaroLlYKBs2rQVfc55owddo6jJlRV4LvuY80YJ/FiZPnlwurcGLL77IG2+8QV5eHoMHD6Znz55069aN77777qz7qiqNcmVpjqtKiaxpYJS18N34kKSmYVCvkqc9/tPjbE92bX7kmCYxTBtedVa2sWPH8vjjj/Pwww8DsHDhQlasWIGPjw9LliwhKCiI9PR0+vfvz6hRoygZpK6UytIoOxyOStMcV5YSWdMA0Ra+xoXUK8GvDXr06EFqaionTpwgLS2N0NBQWrRogdVq5dlnn2XNmjV4eHiQlJRESkoKTZo0qXJflaVRTktLqzTNcWUpkTUNEC34GhdSrwT/TJa4OxkzZgyLFi0iOTm5NEnZvHnzSEtLY8uWLZjNZlq3bl1pWmSD6qZR1mjKoQVf40K0D78ajB07li+//JJFixYxZswYQKUybtSoEWazmVWrVhEfH3/GfVSVRrmqNMeVpUTWNEC04GtciBb8atClSxdyc3OJioqiadOmANx5551s3ryZbt268fnnn9OxY8cz7qOqNMpVpTmuLCWypgGiBV/jQtyaHrmm6PTI7kFfw3pMdjaEhKj5P/6A2NjarY+mzlGX0iNrNJrzQVv4GheiBV+jqctowde4kHoh+HXJ7VTf0NeunqMFX+NC6rzg+/j4kJGRoYXrHJBSkpGRgY+PT21XRXOuaMHXuJA6H4ffvHlzEhMTSUtLq+2q1Et8fHxo3rx5bVdDc65owde4kDov+GazufQpVI2mwaGzZWpcSJ136Wg0DRpt4WtciBZ8jaYuowVf40K04Gs0dRkt+BoXogVfo6nLaMHXuBAt+BpNXUYLvsaFaMHXaOoyOkpH40K04Gs0dRlt4WtciBZ8jaYuowVf40K04Gs0dRkt+BoX4nbBF0KYhBDbhBA/uPtYGs1FhxZ8jQu5EBb+P4F9F+A4Gs3FhxZ8jQtxq+ALIZoDI4CP3XkcjeaiRQu+xoW428KfBjwNVHmnCiEmCCE2CyE264yYGk0FdFimxoW4TfCFENcDqVLKLWcqJ6WcKaXsLaXsHRkZ6a7qaDT1E23ha1yIOy38WGCUECIO+BK4Wggx143H02guPrTga1yI2wRfSvl/UsrmUsrWwG3Ab1LKu9x1PI3mokQLvsaF6Dh8jaYuowVf40IuyBuvpJSrgdUX4lgazUVFWZHX73XWnCfawtdo6jI6SkfjQrTgazR1Ge3S0bgQLfgaTV1GC77GhWjB12jqMlrwNS5EC75GU5fRgq9xIVrwNZq6jBZ8jQvRgq/R1GV0lI7GhWjB12jqMtrC17gQLfgaTV1GC77GhWjB12jqMlrwNS5EC75GU5fRgq9xIVrwNZq6jBZ8jQvRgq/R1GV0lI7GhWjB12jqMtrC17iQagm+EOKfQoggofhECLFVCDHU3ZXTaBo8WvA1LqS6Fv79UsocYCgQCtwNTHVbrTQajUILvsaFVFfwRcn0OuALKeWeMss0Go270IKvcSHVFfwtQoifUYK/QggRCOi7T6NxN1rwNS6kuq84/BsQAxyVUhYIIcKAce6rlkajAXSUjsalVNfCvww4IKXMEkLcBTwHZLuvWhqNBtAWvsalVFfwPwQKhBDRwJPAEeBzt9VKo9EotOBrXEh1Bd8mpZTADcD7UsoPgED3VUuj0QBa8DUupbo+/FwhxP+hwjGvFEJ4AGb3VUuj0QBa8DUupboW/ligGBWPnww0B14/0wZCCB8hxCYhxA4hxB4hxH/Os64aTcNDC77GhVRL8EtEfh4QLIS4HiiSUp7Nh18MXC2ljEZF+AwXQvQ/r9pqNA0NI0rH01MLvua8qW5qhVuBTcAY4FZgoxDiljNtIxV5JV/NJR95hk00Gk1FDJE3mbTga86b6vrw/wX0kVKmAgghIoFfgEVn2kgIYQK2AO2BD6SUG8+jrhpNw8MQeW3ha1xAdX34HobYl5BRnW2llHYpZQzK599XCNG1YhkhxAQhxGYhxOa0tLRqVkejaSBowde4kOoK/k9CiBVCiPuEEPcBy4Dl1T2IlDILWAUMr2TdTCllbyll78jIyOruUqNpGGjB17iQarl0pJSThBA3A7Eli2ZKKZecaZsSt4+15OlcX2AI8Op51VajaWiUFXyph8A050d1ffhIKb8BvqnBvpsCc0r8+B7AQinlDzWsn0bTsNFROu7ljz+gWTNo27a2a3JBOKPgCyFyqTyyRqACcYKq2lZKuRPocX7V02gaONql417uvhuGDIGZM2u7JheEMwq+lFKnT9BoahMdluleCgrUp4Gg32mr0dRltOC7F4tFfRoIWvA1mrqMdum4F6tVfRoIWvA1mrqMFnz3ogW/fuFw2Ni2bSCJie/VdlU0GtejBd99SKncOVrw6w8eHp4UFcWRk7Ohtqui0bgeIyxT+/Bdj92uptqHX7/w9+9MQcHe2q6GRuN6HA4QAjw8tOC7GsOy1xZ+/cLPrwsFBfuR0l7bVdFoXIvDocReC77rMSx7Lfj1C3//zjgcRRQWHqvtqmg0rkULvvvQFn49xGYj8vYPafYtFBTsqe3aaDSuRQu++zCEXvvw6xGenpiOnCRoL+Tnaz9+nSQ7G6ZM0YJ1LmjBdx/apVM/EZ06E5BgJj9fW/h1kp9+gmefhT3696kxUl54wc/JuTDHqW20S6ee0qkTvvEOsjNXI3UK2bpHYaGaFhfXbj3qIxfawl+7FiIi4MQJ9x+rttGCX0/p3BlToR0Sk8jN3VzbtdFUxBB6Lfg150KHZR4/rgQwOdn9x6ptDJeO9uHXMzp1AsAv3oP09MW1XBnNaWjBP3cutIXfkH4rbeHXU0oEPyKtPampX+t4/LpGQxIRV1Nbgt8QrF4t+PWUyEgIDycspQVFRUdISZlf2zXSlEUL/rmjLXz3oaN06jHR0fhsSiTAP5q4uH/jcDQAC6W+0JCsRldzoaN0GtJvpePw6zH33IM4cIBLTt5BUdExTp6cVds10hg0JKvR1WgL/9zJyoKhQyEhofL1huBL6UykdpFz8Qj+rbdCaChB87cSHDyAuLiXsdvza7tWGnCNiPz5J/z4o2vqU5+40FE6F5Pg79kDK1fC5ioi98pa9g3ErXPxCL6vL9x3H2LxYtoFTMJqTSEh4e3arpUGXCMiU6bAM8+4pj71CT1oe+4Yz39U9c7asiKvBb8e8uCDYLUStGgXERE3kZDwKhZLSm3XSuMKwS8ocP6BGxK17dLZsQN27XL/cd1BTQT/YmjgqsHFJfgdOsCgQfDRR7Rt9QoORxGHDz9Z27XSuMJq1IJfOxb+Y4/Bk/X0P2TcL1XdN9qlcxHwyCMQH4/ft3/RqtVzpKbOIzX169quVcPGFRZ+YSEUFbmmPvWJ2orSMaa5uepTHzHuF+3SKcVtgi+EaCGEWCWE2CuE2COE+Ke7jlWOG2+EHj3ghRdo2eQpAgP7sX//feTkbLwgh9dUgqsEX1v47j9exd+qqKj+NrTah38a7rTwbcCTUsrOQH/gYSFEZzceT+HhAf/7H8TF4fHxp3Tr9h1eXk3ZtWskxcUNICFUXcSVFn5DS453oaN0KuaXqc89q7MJflmXjvbhnx9SypNSyq0l87nAPiDKXccrx7BhMHAg/Pe/eFkD6NZtKXZ7AXv3jsVub4BWYm3jKsF3OBqMJVaKYeELUXsWfn3tWWkL/zQuiA9fCNEa6AGc5lcRQkwQQmwWQmxOS0tz1QFVGF9KCkyZgr9/Jzp0+Jjs7HXs3DkMm62B5PuuK7hi0Nb489ZXa/NcKevSuRC9m4q/1cVs4VdX8O122HtxvFzJ7YIvhAgAvgEel1KeprRSyplSyt5Syt6RkZGuO/Bll8G998LUqbBxI40b30bnzgvIzv6T3btH43BcBA+W1BfO18KX8uwRFxcr2od/7rgqSufbb6Fbt4siZbRbBV8IYUaJ/Twp5YXPW/zOO9CsGdx/P1gsNGo0lo4dZ5OV9Rtbt8aSn7//glepQXK+gm+xOMWuvorPuVKbuXSkVN/rayPrKgs/JUVd+4wM19WtlnBnlI4APgH2SSnfctdxzkhwMHzwgeqOva2eum3S5B66dFlMUVEcW7b0JClpun5Llrs5X8EvKzgNTfBr08I3rnXZBvdMxMfXLV+4qx68Mravrw1fGdxp4ccCdwNXCyG2l3yuc+PxKmfkSBWq+fzzsGYNAJGRo+nTZxfBwQM4dOhhdu26jsLCuAtetQaDKwX/IvjTnZXUVBV0cPJk3RD8ssurIi9PvZfiiy/cV7eaUpMonTM1VFrwz46U8g8ppZBSdpdSxpR8lrvreGdk9mxo104J/4EDAHh7N6V79x9p3/49srLW8tdfnTl+/FWdVtkdnO+gbWUW/p9/XryhdDt2KONk+/baS55msdSsZ5WerspXlZmyNnCVS0cLfj0jNBSWLQNPTxgxQllOgBCC5s0foW/fvYSFDePo0cls3tyD5OQvsFoza7nSFxGutvBPnIDYWFi06PzrVhfJy3NO64qFfzaxy8pS05w6FAGnBf80GobgA7RtC99/r0baY2Ph0KHSVT4+LenadQlduy7F4Shk//572LChDcePv66F3xW42odvhO+mpp5fveoq+fnOaW3m0ikr+Gez8LOz1bQ+CX51H7zSgl9P6d8fVq9WuUFiY2HDhnKrIyKup1+/w/TsuYGgoP4cPfo0f/7ZhJ07r+PEiY+wWrNqp971mbIPS52r4Jf9wxYWOkWlLolLVpbK1noudTp2rLyYlBX82sylUxOXTl228KsSam3hNwB694Z16yAgQIn+v/9d7oEWITwICupHdPRP9Oq1jaiohykoOMDBgw+xcWMbDh9+glOnftGRPdWlrOXkKgvfEJW6lNRr3TqYORN+/71m29ntEB0N77/vXGa4dGrTwq/o0qmu4BuWfl2gOi4dX1/nfFVcRILvWdsVqBUuvRS2bVOpX196Sf2ZXnhBDY6VITAwhsDAGNq1e5O8vK3Exb1MUtKHJCZOw98/Gn//rgQF9cPPrxNBQX3x9AyqpROqw5QVeVcM2padr0vWpCF0x47VbDsjG+Xx485lteXScTjAZlPzFV069dmHX1ysGlaTqfx6iwX8/FS56gh+VQ1HPaJhCj6oGP1PP1XzL74I+/apaB4/v9OKCiEIDOxFt27fYrcXkZz8GampC8jO/p3U1HkAmM2RREU9hr9/J3x9L8XfvyuiQgNSZ0hIUD2c0FD3H6us4LvKwjf+nHXJwjeELi6uZtsZDcWpU85lhuAbg7YXKkqnYm+sJi6duuzDN+YDAsqvt1rB3189UNVAfPgNV/BB/Yk+/VTFDz/7rBoEfPRRuOqqKsXQZPIhKuohoqIeAqCoKIH8/D0cP/4KcXHPl5bz9m5Fo0a3EhTUDx+fNvj5dcZk8rkQZ3V2hg9X7qyZM91/LEPkvb1dF6VjCF9dEpdztfANy7is4NeWS6dib+xcXDp16TcpLHRet4KCqgXfmK+K8xX8gwchIgLCws5texfSsAUf1A0xeTK0aKFy76xaBS1bqvwZPXqcdXMfnxb4+LQgPHw4Nls2hYVHycvbTlraIhIT30bKki4yJvz8OhAQEI2/f3cCAqIJCIjGy6vphe0JSKlEqWnTC3M8Q0SCgs79D1PR0jTEx50WfmGh079bHc7Xws8sEw1W0aVjMjmFKz9fNdbTp8Pll9fsWGejYm+sJg+81VUffliYekagMneM4dIB9wr+VVfB7bcrnTlyRAWP1BJa8A3uvBOuuQZ27oRx46BfP7jjDmjcWFn/wcFn3YWnZzCBgT0IDOxB06bjShqAYxQWHiI/fyd5eTvIzl5HauqC0m1MpgC8vJrg59cZf/+u+Pt3w8enJT4+rfD2dkM26fx8deOmXKB3/ZYV/KxzjHKqKDyG0LvLmly9WvWCatIwnquFfyaXTmVROgkJ6sGsDRvcJ/hmc80HbY3zyM119kpqEylVnVu1qlrwrVb3C35RkXruJzlZpXd59111jWrJ3asFvyyNG8OQIeoJxyeeUHH7OTnw448wejTcfTe0b6/K2mzqQa4zoBoANfALY0qXW62Z5OfvIi9vB4WFR7BYTpCfv4eMjGWAvbScl1czPD1DCAkZhI9PSzw8vPHxaUNo6BBMphpYn2UxYtgvtOAHBqo/lZQ1u9l/+80Zb+/jo/5AZcXFHezdq+p9LoKfna2s9eqOj1Qm+Gd68MroCbgjkVfZxvlcXTpSqoYqMND19asJRn3Dw9W0MrG2Wp2/k7t8+Mb/LDtb3cf5+Wp/hivpAqMFvzIiIpw5QX7+Ge67D15+GT76SLl6vvpK+b//8x/VMFQc/T8LZnMoISEDCAkZUG65w1FMQcEBiotPUFCwl7y8nVitaSQnf4LDUfYPZ8LLqwlCeJS4haLw8mqCv39nPDy8EcILT88g/P274+lZ4Y9niGd6erUarfOmrIiA+mN5e1dv2/R01evy9VX1DAi4MHH46elqWhNRLVuXuLiaC35mplPcq4rSkfLCCH5goLJKzyUOH9S1cKXgHz4MP/wAjz9e/W2MuhuCX5WF7+Wl7i13WfhGSuWcHGWwgGrcteDXUYYOVY/y798PgwapPPsAXbrApEkq7vqzz5w3lkFyMjRqVKOurYeHNwEB3QkI6E54+PDS5VLacTiKcDiKyM3dSlbW71gsSUhpIyfnL3JyNmG1pgPlB/WEMOPpGYynZwiRkbcCkqC96USonVKYsA2f1j2x2bLx8PDFw8PH9eMJFQW/uLj6gp+YqESuoEDrxeioAAAgAElEQVQJiK/vhYnDr0rwt21TSfgWLXL+eQ2ys1WDlJenegYVx39Wr1YuGC+v8ssNoXQ41HmFhJQXfG9vp+CDsydQtkdQGcePK/dPbOxZT7eUsr9VfHzNfPjZ2coVZLWq+SgXuiPnzIH//le5WqvhWgWc9TUGSqvy4ZvNznpXhsNR83cx2GzqGoSHl6ZxITvbed9nZqoxw1pAC3516dgRdu2CxYuV+Nx2G3z4oYrlb9UKBg+GK66Ahx5Sfrrnn1cvTfj4Y+jT57wOLYQJk8kfk8mfsLAhhIUNOa2MzZZLUVEcUlpxOIqx2U6RlbUGmy2bgoI9HD/+P8CDJnsdSvCBPb/2Ja+9AIyHyEz4+rYlIKAnfn6XIoQJITzx9m6F2RyOh4cPXl6N8fPriBBKgOz2fITwxsOjilupMsGvLsafBZTYGy4dQ/CN1+95eqo/rauoSvBXrlQ5mfbuhZ49y6/LyYGYGPjjj3JpOwBl8Q8apHqNd91Vfl3ZQc7MTCX4ZaN0zGblAjPOz3ARnM3Cf+klWLq0Zq67sha+lKoehiupOhZ+8+aqsXN1z8u4DzIznYJfUKDqWJWlXBML/0yCf6ZnEX7+GSZOhM2byxsAH30Ezz2netNlLXxD8M/WWLsRLfg1ISICJkxwfv/HP1Qa29dfh7/+Uj7/yZPVH2TkSDUWcP316inMRo2colcZNfVtV8DTM5CAgG7lloWHjyiddzgsCGHGvu4F4L8AtPZ9hJyWwZjNEUhZXNI47Cf/xJ/kn/iKglaVH0sIM1LaMZn8sNvz8PQMISAgBg8Pf3x92+Dh4YvVmgYIguJO0Qwo8srCB7Dln0KEBSKlDQ8PMx4eZ7D2Kwq+r295lw7ADTeo6+rKRGpVCb4hnocOnS742dkqvLd5czXwX5bExPLTitsZnDoFbdqUj8MPClKia7wNzmhMzib4cXFqvKayB46qomLjnJWlBPVsrzmUUpXt2tU9gm+IZmYmtG6t5sePV9+XV5GAt7qCbzYr0V+zRvXYX3+9fJmKaT3KsnEj7NkDSUkqG6/Bnj3qeqSlOetu9IBAC369pksX5dIBdfP98ot6ifrQoephrj594JJL1I99ww0qJGvjRhUF9NhjavnGjcpXvWwZDBhwxsOdKx4eypXgmZFfuizC3oeItvecXvizp5AffQTpaTg8oajoGDZbNg5HEUVFcRQU7EcITxyOfMzmCAoLj1JYeJDi4kSys//A4SjCbFb9CEdyGs2A1MLltAQ2/9mRombqMEJ4ERTUD2/vFoADkykYITxLGgMvQnZtwHjppc3bAZ4WZO5JTFkZCLMZYbUi1/wOTZvhUkdUVYJv/HkPHz59m5wcZX1GR6somrIY4yaVJXurKPhQuQ+/USO17ODB8mWrwnCHZWQ4tz0bFQU/J0c1skbES1UUFKiGpWVL53auxLjuZc/54MEzR31VR/DLunS2bVOfF18s32sou13FfRj3SWpqecE3DJWKFr4h+Jm1l5BRC74rue469THo3FlFmaxerW6CBQuUJRoWBl9/rQaAly6FRx5R1tyUKWpwqmVLtawqiorUfocNU77aJk2qHzOemqp6KunpVXf3t2xB5OXBnr2YevbE379TtS/BaRydC9xNZPv7gdm0L7iPgohWEBSAxXKSnJz15ORsQAgPbLYspJQIYcLhKMb/uFNkCmUCdjuQDoHZYAkDnxQQxRYcCXH8tb4TRdbj+Pi0RghPAgN74eXVDJsts2QcIxiTKRiTyRchvBHCEw8PH7y9m2KxpOLl1QRv72aYTMF4ZGQgAJmRXr4hMf68FV02UirhDgpSgv/TT+XHKs4m+P7+StwNQavswSvDwi95n8MZLXwpnb2JtLSaC74x4JqdrVwVZd8pXBmG8BqC7+pY/MoEPz39/AW/rEvHIC2tasGveA2M38CIfDM4cUJNU1Od4m+3O8tpC/8ipl8/9QEVh3vihAr1W7BARf80baoEvG9fJRQ//aTKJiWpMNHWrZV7qGy3/PHHlZ9w3DiYN0+FjH75pVp34IBa/9ZbysVQkdRUlSo6P79ywZdSjVWAclNVdF3UlBIR8W0UDUDE+M+UK+yDF8+6qZx2M6BehewbHo3090akn8JUfBhri5aQovLPeNghKK8Foa2HUlx8HIfDSlraIuz2fDw9Q7Dbc8o8AHd2rkwFE5B1ZDFHNvcoGcz2olP8DnyAgh0/cHD7ILy8muLpGYrJ6kk7q5U8UwKWFv6E2e2k/v4S3v1GYjL54ZmwTbmzThymMHcLnp7heHtH4eFhRmZlQetWiD17saedwGSxqEE/oxEwnhY1RNsQk4ICdd9UHDwGJbhGL6GiGJ0JIzTRsPDLCv6ZLPyKgu9KC9/hcN6nZS3j9HTVMFYVaVZR8OfPV4PmZcfTDJdOWcFPTXW6jcAp+MHBVQt+xYa8rOCXffG5kXBRC34DQQhn9MKdd6o/8ZIlSlRHjVIDwzfeqKz2qVOd2z38sBrMO3JE3aTffKNuZCMX0MKFaoB4zx71QM6xY0owBg1SL3EfPdq5r7Q0FSFQ8WY0SE523sh//aVS/p6NJUtUA1VZr6SimwBUr+b999X1sFhOj1wxLteJE6XC5xkQqaJgDqg/k0/b/rDZmXCso+8LcMkVpd/Vm8skHh7eSClxOApK3VIORzFS2rDb87FYTmA2N8JiScZqTcGem4ap6D8A+BVGYjY3Buw4HMWY0lRUkDk+C4fDSnb2n9jteZjS82kHnMibT6Yf9ANO/fY/kr3+B8AleyEKKDq+kS1behOyFUJ2QuID4fQ4mUFhFEQA8dsnkrHqTfoAljCBVz5YTh2lIDebjNxplHEaAJB2YDa5gYkUFcUREBBDYeEhzObG+B4pwnh6IOvwErz7tcbLK5KCgkPY7blIaUVKO4GBPbHZcjGbIzCZfHEUZGGC8hZ+dVw6hkXfvLmaulLwT51yJnQzhLKoyNkLOnWq8h6MIc6hofCvf8E77yh3zbJlzjKGS6fs/VdRvA3BDw8/PSqsMgvf4XD+r4z/WMW0IlrwGyhDhqiPQUKCEjgplVXj5aWyeH7wgRLH9u3V9IYb1LJJk1TDccstKiogLExZOzfeqBqFb75RFtrOnWpdWprab69eqvdQmYVvDDgGByvBr4rkZDWAPWyYehK5uFj1WMrmK3n1VVUvKB+XnZCgGqcDB9Q2Bw6ohqkiJ08qi2z1ameUjvGHNMTFID5eRUmVYIxZgEp+Z0Q5nZWEBOA/YDLhnWsmOrqkx2WxQI43BAZizsylZ/vlpY2YPHAA6EjLbq/Q8uY7kQ91ol3eWCK7jcHhKCTI8TawDt+8ULp2/Qz/t/+Lz9K/sE68Ce+iBcgWnXD47iBc9MJubgospii4CK8E8CxQz2ckZX9May8wlXk+KG7Lw+S3NWE2h5OaugBPz1BstmzCtjlKBT91z7uc2PhuuVMM2QZRS2Ddv1FdmRKa7IaOwPHM6bQELOmHsDTyxCRNWFN+59iOIRQVxePr2w6rNROTKQA/v0sI2HSIZkCCXEiUryc5x3+gOOWSkp6RCav1FFJaAYmnZzheXo2xWFIQwgNf30swmfxLxm7sWK0ZmM1hJY1QIPbEo6UiJU+dQgDW5COU2uTp6WcWfF9fFdK5e/fprjjDwi8boWOIt82metVlBb9iY2D48MsKvvF8C6j/V3Kyys67e7ezjPbhawCn71AI5ZcHmDZNDQgNHHi6e2X+fDWdMUNZG//8p+r+p6er5wOuuEJFH3TtWv5JwrAwZeX//jusX6/eCbBxoxIwowdyxx3q4bL8fCW0ZV1Kp06p3sP+/cpaN1ixAm6+Wc3bbErwjZu/YoTS8uWwaZOy1L75RiWtK4uUSvBvvhnWrnVG6RjnUZnguwLjT9ymjRL/hx9Wva8uXdTyyy9X53nokGo49+1D/PADAD6NuoJ/K+jaFfPeeMLDS8Zzst8DwJSRS0TY9RD/Aki4RD4MefMJiLoSwk8SbO9IcLOngcUEtRsBu7/Do9BOWMQwBgz8CtmkJRxPUG7Akyfp2vRTTJePwGyOwGpNw2yOREob8sBHgLqerfwewv+S7ths2fj4KEs/4N+TMK/dxiXezyDatsdqTcHhsBIcvBNYgik0CkjDlCehdSAOCnEUZGOzZePv343CwsOYzWFYremkpm7HfECJa7zHAhr7SwqTt3Bg353n/VMIYSbkLyvRJd+T977BsT/nYt57EsMxc3jDveRbwwGJlDYKCg7g6RlE8yOBNAMOJT5LkcVCI9+NRBw7RWLcFCzWZKyWNDpZLaTnLCfsZFxpu5d54CvyE3JoMnAKxbdfQ35rQSOgwC8T38J8TiRNL4lI8yEgPRUBFCfuJP/UL9jtOXgfOIVxp9v3bMVksWBt1xhzGcG3pyUibbmlEW7n/NT8OaAFv67j6ame5j0T995b/ntkpBLAgACVFuK776BDB9Uo/O9/0L073HqrGjS+/HJV/o47lICvXq0aG+M5g0mTVLl27VTj06uXGiM4fFiJ9iefqPjzt992ltuwQXWly1oyZS38mBiYO9eZaGz+fOUSGjvW+cBSZqYS9+bNVYPVokX5ATRD8KOiVO/C1YLfoYM6x+nTlaU2ebJaPmiQEvydO9W1mDjROe5ixIhHR6vnNYxQW8MytNnUb2AMvO7apRrU4GBlQaakOF0VZa3WkoeuRKPGSvDbtYOTJ/EtDAIvNZjr5aXKC2GGk+nquP7+eOd4EhX1d+e+4uJgzTZ16QquhmZDnev8pwFLiOr4DHA7pkI7AeG9wLcAPDzo1WtV5dds7kPI8EXEDk9DtO1Pk0JfgvrMQErlOvP0DCvpcQkslmTsh3bh//5Sil57igL7MaS0lI6xqMYrE6s1Has1neCtR4BFOLxMBNraERranzA/E6DcmSIjC5vNo+S5EEFIyFVYLEk4jm5DekCG50ZMRRHYm4djKkwjYcezyLBAzCICIaHIloAp3zm+k3d0BSdWraD5Mcj5eR7pw6ERkON1GD87HN73MNIThA0Glnh48o/9ys6dvwIQthG6A9IDHGt/wQQkRPxK25L92/ygMGk9W/5QzUL4nyB9vMi/rBGXXXYBXgAvpawzn169ekmNm8nJkdJuV/Nvvillu3ZS7tmjvlssUt52m5SPPKK+jx4tJUgZEiJlZKSUQkjZt69a9txz5fd7991qufHx8JDSx0fKDh3U9+3bnfuaN89Zrndv53yPHlI6HGp/u3erZV9+KWV2tpRFRVJOnOgsu26dmsbGStmrl5TDhkmZl6fqnph47tdn/ny13yeecB4rKkrKpUvV/Pr16hzGj1fXMSTEWW7bNrWP995T3416REQ4yy1b5iz/yCNq+s47Ut5xh5TNm0u5erXz+hrl7rhD7ee669T3e+9V05kznfVOSZHykkuk/OsvKf/2NymbNpWyfXspx44tf37/+Y9zv++/r5adPKmu3dSpp9dx9Ggphw6Vsn//qq/ZkCFS9umj5u+9Vx37TDz2mPNaliU9Xcovvii/7LXXVNnoaCkHDVLLvvzSWb+y16AsN9+srofB4sVSgrRu+F3KnTul9PZW20+ZUrovh5eXtN95u7R+NE1KkPZG4dLy4RtqXUmdC1P2yLS072Xark9Kt7N2u0RmZq6ROTnbZO676r4pbhtWuj7z+1dK5y2dmktriwgZHz9VJh59R9pCfKW1SaCMO/LSma/ZGQA2y2pqbMN7xWFDJzDQ+Zj+xInKNdG5s/puNqvoofeUC4Jp01Rq12++UeUmTVLjCo884vTNG0yapJ4ynjPHmYdo+HBlCcfHO3O5Dxigeg+9e6t9zZqlXBSjR6s46A8+UFEOb76pyrdvr9xB3t5Od06PHsq69vVVrpdWrdQxZs9Wg8H/+9+5X5+yFr5BUhJs2aLmmzVT6TX+/FNZ6mVDAw23VXSJE2LHDqdV37WrWma8AlEI5zuVg4PV+SQmOrNtNm7s3K/xexmhmUbMd9nQzFWr1G+0dKnaT1SUKl8xSmfJEuXq8/dX5XNy1ID/k0+eHpYJyp3n4+P0idvtnMaxY+p3ABUZdvKkGsg9dUr15Mpu43CokGRw9nQMZsxQCQr373cuS05WGS1btnQOdhq/UcX5suzbVz5KrZV6itAzKR1+/dV5rkaPChBduuCRloHnJhWl5pGagTlJ/b4iQj1X4iPDiYgYSYQoibwLCMDzVAEhIVcSGBhDQLZK5eDV+xq1PiSEkCucPSxzh954Zlto2fIZova0xZRViGdyLq2OXlb5ebgYtwm+EGK2ECJVCLH77KU1tcaZnu5t2VIJydVXK1F69VXlT3/vvdPz4XTrplxA99yj0gesX6/+wF5eaj8xMSrlxOefKwFbuFBFTMTEKEFduFAJ5aOPKrH69FM1ttCrl/MYN96oPitXquM/9ZRyZ3XqpMTLEPpPP1XCevKkWv7660qYFy5Uy4qKVGhox47K3bJ/v1MAjIeqjKyohpB9952aNm6s8tPs2aPcZWUxBL97dzXdtk25UKQ8XfAvu0ytB+VCM8Zn1q5V07IuHWOdsaxZM3X+u3Y5x0iMxmPDBvWE96WXKsEvK4gnT6p1I0ao8zt0SDXq6enqmmZlqUa/7DMdhuAXFamUEoGB5d/ba7erxta4Th07qulPP6lw5LvvVgaDwbp1ztj0ioK/ebOarl/vXBYfr65PWJjTRWick49P5YJvs6lzq0TwiY9XxzHSIsfEOMs0b67uh3XrnMnvNm5U04pZN43G1riHjJDLEydUWSNENTq6/PhVmzaqkbXZlHEVGqoi8ObMOf083EF1uwI1/QADgJ7A7upuo106DZy8PCl//FHKGTOk/OEHp3vnbGRlSdmxo+o2T57s7O77+0sZVtK19vJS0+bNnWVNJtXtB+XaevBB5bYaM0bKffvU8rffdnb/IyPV8X77TX1v1UrK0FDlynnssfL1bd1a7b+s+8SYb9RIykmTnG4sm02dAyg3DEi5apWzfEqK2ufrr6vv33wj5a23qvlrr1XH7dfPeU4g5Zw5TtdOYaEq8+mnTtfTLbcoV1VwsJR+fmp506ZSXnWVcnkYx/7HP5SbplUrKZ96yulSMjh+XC2bMUN9P3BAfQ8KkjIgQF2z665zln/kEeXqa9FCyptuKv87tmihtp0wQX3PzVW/4fjxysUWEODcR2iolC1blq+Lwf79aj+ffeZc5nCo7R97TLkZb7hByuJite5vf5PyiivU1LhPHn9cTQMD1dRw9RnuzxIXUakrMytLLY+NlbJnT6cr6rHH1PKAAOf9BFLGxzvP7cEH1X1q1KeGUBdcOlLKNUDtBZxq6h/+/soN9OCDygqtbm6h4GBlbU+ZolJWf/mlciXExiord+5cldzuvfeUjNntqvzLLytLcPRoNcD9xRfq+HPmKMtt8WL4+9/VQG2TJsplBCpU1NdXWYuXX66sxHfeKV/fq68un0HVsHxB9Vp691Y9nfffVxFQwcHK6jZ6GGXz8BuWvTENDVXnOHWqOo+5c1VvoXFjp/tk2DB17idPKgszJESFzzZtqqzOSy5RPavsbDUwDars6NHl3+scHKws6awsZZGCGpw3ekRHj6qpYeG3bat6CTk5qrf3t7+pQe7p01XZRYvU0+g9epS38NPSSkJicVr4S5aoQe2771bnnJenItHWr1dPi4eHKwtfSvXkuTGov2+fmpa18IVQVv7u3eq4hksRVILDtWvV9TLchuPGqeuZm6uuh3FNDAvf6FkYxzB6iuvWqeAD47cyehBBQap3ZETfvf66Orc77lA92cOHq3wexaVUt2U4lw/QGm3ha2qTir2EwkJlURvrtm49e0+isPB06ysuTspvv1UWbmVYreo4ixdL2batstJvuUXKu+5Sg6R2u5RJSeW3MazFZ55R68eNk3LjRuf6XbukbNzYORhss6kegtEDefFFNe3ZU61/5RVnL2L8eGVB/9//qXWflAw6jhmj9hMc7LQ8HQ4pZ81SZTIypPz+e2fPwRgwNgZXZ85U3w8edNazc2e1bOtWZW0b20ZEyNKB+EmTlDX98stSLl8u5U8/qXUDB6peVk6OlFdfLWWbNupalO0hgRpEHjJE9WzefVctGzxYyg8/VMtBDfaX5brrVDABqJ5kRd58U6279FL1/Z57VE9lwgQpV6xQ69auVeuMwV7D0l+0SPVCPD2lTE5Wg+dms+rxSCllp07q+ufkOHudzZo578XzgBpY+LUu+MAEYDOwuWXLlud98hpNvSUtTUUn1YSDB5V7p3Nn1ag0a6bESEopjxyR8umnlWukIocPS9mtmxJkKVU0z4ABVR/nl1+kfOghJaKXXqqk45ZblMumfXsV4WXw0EPl93XsmJRz56ptfH1VfT7+uLyAt2mjpgsXqul996npq6+qfcyeXb58TIyUt9+u5r29nY2M8end+/Rz+PZb5aLx9JQyNfX09W+oiBz5r385lxnGwJo1at3PP6vrfM89yjWVna3cgkajVjYqqrDQOd+vn5Rdu6r5l19WZSdOrPp614B6JfhlP9rC12jOk8JCZ9htTSguLi9QZyI3V8pnn1XWb1iY04o1cDgqr8PkyVK+8IKaX7tWyc+wYVI+/7xqNIYMURbv4MFqXYsWUhYUqPJ//inL+cDbtZPy/vudFnlKipR//KHGH+z2qq9BeroKEa5q3dNPS5mff/q6zZtl6RiQEGreCPtMS1MNwBtvVN64SqnGB4zGIDtb9ZSOHq28bA2pieALVd49CCFaAz9IKbtWp3zv3r3lZmOkXqPR1G3y8lT0TkTE2ctWxGaD115TPv6yIaig/OO3364e8BvhfKcDubnKD75ggXryOStLjV+8+mr1Xyl5rlgs8H//p0JQu3dX4zhdupRPtFZLCCG2SCl7V6usuwRfCLEAuAqVFyoF+LeU8pMzbaMFX6PRaGpGTQTfbakVpJS3u2vfGo1Go6k5+klbjUajaSBowddoNJoGghZ8jUZzTljsFmyO6r9JrCJJOUk4pOO05VJKlh5YSlJOUrnlVru1dCqlJLc4l42JGyvdhzvYlbKLr3Z/RXyWizKz1gJa8DUXLQ7pwGK3VLm+yFbEj4d+xGK3IKWk0FpYbfEothWzO3U32UVnfn9rTnEOhzLUizd+Pfore9P2Vll2e/J2Zm2ZRb5FvZ5w+aHlZBWd4b2tJdtMWDqBCUsnUGgt5F+//ou18SofT1ZRFjO3zOT7A9+zK2UXdoczidnx7OMMmzuMY5kqWduCXQt4a/1bVBXE8cPBH/j7D3+nyFbE5zs+J3Z2LAH/C6DF2y1Kj2ewJn4NX+7+styy7KJs/rf2f3y+43MA9qTuoeW0llz56ZVcO+9aOr7fkbsW30WhtZCRC0Yy6stRdHi/A1/vUYnWNp/YTOCUQN7Z8A5t323Lc789x0u/v0T/T/rT7t12zNk+h+d/e57X1r3GP3/8J0/9/NRp55CQncCEpRO4bt51FNuKT1ufmp/KkyueJM+iEqoV24rJKMhASsmB9AN0n9Gd2765jUveu4Sot6K45vNrKLQW8tLvLzFryyy+2v3VaY2UwVe7v2LOdpUvxyEdbEjccF6N5bmi8+FrzkiRrQgfT/Xe1HxLPhmFGTQNaIrZZD7jdg7pQCAQZdINLD2wlCHthuDj6YNDOiiwFhDg5XxDVp4ljz8T/mRT0ibyLHk80f8JGgc0rmz35UjITmDKH1PILMpkyuAp/HbsNzYkbmD5oeWkF6TTKbIT6QXpRPhFkFucS6B3IJ0jO7M7dTc7U3bSPqw92UXZpBWk0b1xd76/7XtaBLdg2oZp3Bt9LzM2z+CvE39xZ7c7GdNlDHmWPLp/2J1jWccYeelIvr/9+9Pq9PWer3l5zcvsTt2NRLLtwW1cN/86BIJPRn1C/+b9eWXtK0wZPKX0HB/84UE2JW1iyh9T+PWeXxkxfwQP9HiAWaNmAbDt5Dau+eIatkzYQlRgFLtSdzFs7jAKrAUUWAsQCGZuncn83fPZ9/A+PvzrQ5797dnSOjUJaMLUwVO5N+Zevt3/LT8f+ZmHlj3EV7d8xYM/PEiuJReTMPFYv8f4YucXpOSlMKbLGFYeWcmEHyYA4OPpw7SN0+gS2YUn+j/Bkv1LGDp3KMf+eYwmAU2QUjLuu3EczTzKidwT3N/jfj7Y9AFvrH+DrKIszB5mLmt+GSuOrMAhHRxIP0CobyiRfpHM2zWPEJ8Qlh1axsuDXua7A9/x92V/Z3j74WxI3ECxvZjHVzwOwE9HfsLL5MUlYZfga/blvu/uw0N4lGuwJ/SawKXhl5Z+f+THR1h2cBl2aefHwz9yY8cby/1mC3Yt4K0NbxHsE8wLA19g5IKRrDy6kr5RfZkcq96HMHf0XDYlbWLN8TX8euxXPt/xOf9e/e/SffRu1puND6hexy9Hf2FYu2EIIXh5zcvkW/MZ3n441y+4ns0nNvP33n9n+ojpZBVlkVucS4vgFme9188Xt8bh1xQdllk5STlJhPiE4O9V+Sv6LHYLJmHC5OF8K9WRU0eICorC08OT7cnbySzMxM/sx8I9C/lHn38Q4RfB+5veZ3/Gfm7rchvD2g/jix1fsHDvQi5rfhkvDXqJNfFrGPLFEEZ1GEV2UTar4lZhc9gI8Qlh1shZ3NL5ltLjpean8t7G90jJT2Fwm8E8/cvTjO0ylgGtBrB432Kejn2aTh90YurgqTzS9xFGzB/BkcwjHHjkAJuSNtElsguxs2M5dEpZwyZhItgnmK0TttIqRGU6nLN9Dk+seIKbOt3E9BHTGfz5YEZ3HM26hHUsO7gMTw9PLHYLVoeVMN8wLm9xOe1D27MvfR+NAxqTUZBBoHcg2UXZ7Enbg5SSR/o+wuJ9i2kT2oaO4R15e8PbhPuFM3f0XC6ffTm3dL6Fb/d/i0DgITzInpzNvF3z+Nv3fyO2RSwbkzYyZfAUZmyewe5/7MbH04ciWxHN32pOhF8E17a/lmkbp/Fwn4f54K8P8Df7E2abehsAABzySURBVOEXwa1dbuX1P1+ne+PurB23lrisOKJnRHNFyyv44/gfvDzoZZ5f9TzeJm+OP3GcRv6NePPPN3lq5VN8fuPnvLruVfak7SHUJ5QND2zgpq9uYk/aHrxMXljsFl65+hXWxK/hePZxPrvxMw6kH2D65ulsObGFTeNVo/L1nq+RSHo17cWWk1vo3aw3u1J2sf5v6+k5U2XofKjXQ5zMO8metD2YhIkDGQfwN/uTODGREJ8Q9qXto/P0zrw19C2euOwJ1savZcBnA2gT0oZjWcdKRXjkpSN5uM/DjPl6DIPaDMIhHRzMOMj+h1Uq5LSCNJq92Qy7tNOjSQ+2TNjC5hOb6ftxX1666iVS8lOYs2MON3a8kYyCDH4+8jOeHp480vcRpgyews9HfqZH0x6YhImMwgy6Tu/K8wOe5z+D1PuJ7Q474a+Fc1Onm1h+aDmxLWOZNmwazQKbsXjfYlLzU1kdv5pFexcR6BXIqntX0XtWbxr7NyYlP4UXB77Ii7+/SMbTGYT5hvHbsd8Y/PlgOoR3ICEnge0Pbmf5oeU8vuJxFty8AA/hwdhFY5l30zxGdxxN4JRA7NLO5NjJTF03lRs63MB3B77jo+s/YsWRFaw7vo7Djx0uZwBVlzoRh38u1HfBtzvs5FpyCfEJKV2WZ8nDYrfgb/ZnY9JGCqwFXNX6KpLzklm0dxGbkjZhdViZPWo2H27+kM6RnTmRe4INiRto5N8Iu8POtI3T8DJ5EeEXQbPAZtza+VbujbmXF1a9wMTLJvLQDw8Rnx3Pm0PfJNArkFDfUPrO6suVra7EIR2siV9Trp6TLp9EXFYci/YuItQ3lFOFp0qFIsArAId0EPfPOPp+3JcCawGZhZlE+EVwb/S9tAltw+xts9mYtJGXrnqJ5kHN6RDRgZd+f4mVR1fi6+lLvlW5JPzMfjTyb0RcVhzzbprHnYvvJLpxNC2DW7L0oHo14qDWg1gVtwp/sz+FtkLm3zSf4e2HcyTzCL1m9uKtoW8xssNIQn1CeWrlU3y2/TMAZo+azf3f30+zwGZkFWVxX/R9jO81nkd/fJQHejzAPdH3lOtdVJcP//qQfyz/B89d+Rz/XftfAASC14e8zlMrn+KPcX8waeUkMosymXfTPHrNdKZv3vTAJvpE9eGLHV9wz7f3sPLulVzd5mpCXw3FQ3iQVZTFk5c9yZvr3yTSLxKzycyJ3BPMGjmLXSm7mLFlBuvuX0efWX3oGNGR/elKDF+66iWeH/g89357L5/v+Jwxncfw9d6veaDHA/zflf9H29C2vL/pfR798VEe6qXuhW3J28i35HNX97uYPmI6DgdkFmXQ7cNuRPpHklGQQWzLWMJ8wpixZQZD2g5ldLs7+cfKe5nc92WmbnqeAK8AoiN7kpKfyqWhnbg8agDP/fEED/d+jOd6vcOJE+q5qwl/9cHmsLNs1FYmrn6AlSe/YkHvRNYlr+Rg8VqujbqTXk364usL7255jZnHnsEkPBkW+TduMM0AVO66aSkj+SvnB57pPJOBAeNJT4c3k0eQYNlBY88OFNrzeKHpRvbalvHGiesBuMt7IR1sY2jUSL0czuFQn7cyBnPKfpwXQg4ipeC4ZRtv5PXkDu+5JNg384ftHSSSm02fssX+KYlsxEwAobIdJ9mCD8EUilNcbXmL37wm0sx+Oekeu3goKxvpEBSTw8ywEBCSZsVXc13ar9gcdr5r0hOJnSYF17A/5B2Ci7vR9+QnrGzdFwAvWzhetnCu2ruHTe1GkRqsUmzHpL7Btg+erPH9ClrwXY7dYUcimbVlFrtSd9G9cXce7PUgQgh+OvwTq46totBWyK/HlI92QKsBzB09l58O/8QzvzxDdnE2AV4B5BTnANAlsgvHso5RYC2gbWhb4rPiiQqK4nj28dJjNgtsxqnCUxTZiri7+900C2xGWn4ae9P3siFxA0HeQeQU55RaUoZlCeDr6Yunhye5llwEgmnDp9GtUTdO5J5g6rqpNPJvxJ7UPQxtN5TZN8xm2cFlfH/ge66/9Hp8zb5cO+9aLmt+GesT1/PHuD/o1rgb3iZvvD1VDnyL3cK478Yxf5d6p65JmLBLO28Pe5txMeP4fMfntAppxQ1f3lB6Pg/0eICPt31c+v2NIW/wxc4v2JGyg+jG0aQXpPNYv8d4Ovbp0jKdPuhEpF8ke9L2MPLSkRzJPEK+JZ/tydvpG9WXjUkbS8v+cvcvDG47uPQ9K7m56uFIT0/1jur4eJVMMSxMJafMzlaJD7281PekJCVecbYNPB9/GY292pBhTcQubcT4Xc9dgZ/wZEojenreyVbbPMYEv84VHk/yUmZHMjgIwHX2j7gkfxzz/fphFfmMSdmH1eLBymaDSPJajdkRwF15O/g0SL3ApHPyfzkSMY3mhSM44beCsLwr6LR7Iav7hWDzzMFc3BgfeyTmvHa0/PNbDl3dk/ygbQiHJ9LDRvCXmwkp7IW/P/gE5ZLY/RG81v+bPP+dZA0fDUDwygV47r+NU6dUwkbZdQE5Q+4A/r+9M4+Pqjr7+PeZyWQlyySBEAJkD0sShIRAyiqC7AqoFKq4W6tvqKK1RUEKVq3Wttq64ItV3oKi9lXBBaGCtoC4FHiRPewSypqAWUhiQjI57x9nskESIc0wJHO+n8/9zM255955npx7f3Pu2R7wWvUCETnTKQ/Zync5UVQFH4CfZsKRftB5A2z/CcR/Aj5n4MtfwOezYMQjsG42FNdZzbP/8zDmfpi/HW4bCvvGwbLFDT9M1nLISobQA/DO27BzSu2x6LVw5WPw1odw1lnTzZgP47Kgwg923aCv63caZjpn9z6XA4Vdz/+e9AVwzT3wwh44nQSZf4LRD8Czh8FWClMnQdhe+OpB6LFU2wOw/GXEpxh19S+Rw4Px+uRlKn6aAlVWLKeSCVyyFRG90GnhTak4wnYQsOnXBG9+DBEoSX2OgswHsRZ3pco3F+VVRsC/J1DS5YMa0+yH7qDrN6+BrZRvMyZjqwij75FF/H3lxVdQwAh+sygqL2LBpgWsOrgKi1g4VHAIR5WDW664hafXP42IUFpRSohvCAVlBUxNmYrNYuP1ba/jbfXGz8uPLsFdGJ84nvmb5uNt9eZU6SmGxQxjQJcBnCg+wbXdruVM+Rmmr5xOWmQar17zKrH2WB5b8xjz1s7jtt63MTp+NHY/O1fHXU1FVQXHzxyvadKo5pkvnuG3n/+WcUnjeHP7m0QERLD5Z5vJzstm68mtPL7ucRZNXEReSR52PzvX9biu5ty7P7qbxVsXU+4o58UxL5LVL6vetQuLy4l6PpySimJGdr6OBcPfw9tbx9twOLRolpRAcUkV7x54jVBrV5Yc+gPFlQU8Gfcl3xfbKCrSq/H+Ie8qcsq3UKrysasECjmEooqQs70YlbORw/7L2BB5F0P3/gvfkm44KoXKSv09DgfsibufkzHPA+Cdn0KlTx7eOeM42+FrqsJ056elIhBVacP2/AkC/W31gkA1C1sJzAoEUXA0A9bMhdxULSpZPaF9NpwN0ELzfRh0+wDa74ZBv4UdN+KFL5V9/4T/ircIzJmKzQZ5fX5BefqzeB0biPcb6/n+zu6osD3Ert5Abo8nKI34FGUrpeNXi4guuIWd/YZQHPY59vzhVFZYwaeAwfu+4O9p7aiy6M5Ga5Ufd+YVUlZio6REl0lZmV792D/wLEsioiiznmLaySMEEkVoqP6RKzvr4L0OqeR7ZXNz6Wa88vpgsehVe/3D8pldpCM2BUsUGZUP8qlV1zqnBf4PGV63UV6uf0jtdh2HxdcXcvKPcs/ezvwoYBpflbzBrxIXMzbqZoKCtE2lpfrHtbRU3xffWlbx1DczeKnv56TEhWGz6R/o4mK9VecLC4O9hdu5ZoUOKDOz75PckzyL8nIYu6I7xRWF7LvnGL6+Qm6urtlbLHrbdnoTY5Zm8Nqodxnc+Upu//v1HDlzmG13HqzJk/qXBPpGZvDhvvdrKkvb7tlGSocUnl7/NMPjhtMrohf+T/qjUEzoNoH3p75fc6vc9eFdvPbNa6y+eTUj4nSEq+omLoAHMh9gafZScgpzCPQOJMgniKNnjrLw2oXc3uf2//BGreWymGl7ueOocjBvzTyW7V5GaUUpBWUF5Jfl0yuiFzaLjejgaPac3sPcNXMZEj2EnuE9uTr+aiZ1n8Scf87hqfVPUaWqeGTQI8wdOrem9gswIm4E498azz3p9/Di2Bfrta0D3NDzBryt3jVNDo8OeZSMqAyGxw6vdx1vqzfRIdGcPavjWyulH6AB6le83echiksd7G5XRLr/daz4WyeKijpRUjKcB3mAHe8JR47oZcA/Qy8BIgJ7g9Io7647AWffnsGvT9QG67HZ4NQpH6quHw3J77Lq0TnE3tXYf9AC/NS5PwqkimnqnEFf3h+A/ymYEUe+7MdakEjoxt/jU5TCxlIvrNbJJHpdT56XBS8vXdOu/rRaodP3IzmJFvyKkGyUOOgVlcgZHwfZ7KJdeRLdTz1MUFAVaVla+MLD9cMcGKjFqKJCXys6Wi+3f/q0FobgYL2kfWWlzhMZqY9XVAQwdmUiOcV7GdcvmZdmj6OsTL8JPLl1MK9tzebejHuZmR1GQACEhEzAYpnA0L+uYF/YMk6WnOTn/X7O83On1vwb3tyezk1L4WcTevPiApj92fW8sf0N9q1L449fDWTmp7rTd+t7o+kQAPet7M0LGz7nltEp5Jfls+bQbv44az8rXiono1MGG49tZEBMXxY81ljHuTex62aw+uBqXp8bdc4xK1MOPs/8jfNZODkVr3pFZudPv29PXmkeadFJPDigB5/qlzjunZzEgEb7FKN4YX4yX+ctASBr3FC6BjeWF2AkPxvR+GiluiSoZOz/sJNflk+/uO41S9f8pnwOJRUlBAXpZ6g6pn01Ie17YhELe85sYM7/3sexM8eYO3RuveBTXYI7s/PUdsoqy/hR5x/hbfUmuUMyIsIjgx+pydc1uCs5hTnEhMTU+47q/qPMzpk1ad3Du9M1uCuHCw8zuOtgEkITyFqRRe+OvbH72Tm65yiDug66IN9dgUcIvqPKwc68nWw4uoGNRzey5eQWTpee5kD+AUbEjSAiIAIR4b5+95ERlVFz3snik3y872Om9ZqGt7U2OMETVz3Bo0MepbyynGDf8+/s4XHD+e5X3+Fn88Ph0KE48/NrP/PzferVaE6ftnLo0FheUbppoaRE13jy8nTNuqioIa8szu0jNgN/qXdMPwQhzq4Eh0PHuwAIsurOOIuyMWXoFdgstXE7zp7VNT1rxOMcVRMY8ofeVFToWBfl5Vo4/f21MFbHhLDZdNyIgAALRUW1cR7y80GpQNq1C6TbwhAKygsYlhbN6ucmUJ/GRwaXnL2S9Fe60SuiF+/s0sPzfnlnArklQWStWMT49DTeur7lakrV9Nt5BTm79jKsZ0pNZDyAyY7r+CxnFXOGP0hkYP1z0jv1Yf2/1xHkE8Tjwx6vd6x/VH8EqRGGx4Y9xqzBs7BarDUPf0anDDoE6KAZV0TomLgpHVLYd3ofJ4pPsO3kNgBu7nUzG49trCcyDTF7yGxmD5nd4LERcSNqaqTn0i28G3mH80gKS6Jn+5616WHdGsxfzaj4UezM20lMSAxdgxtoYmkmFrEwsOtAlu9dTvfw2iAyN/W6qcnz/G3+JIQmsHDLQk6VnmLJdUu4MfXGenm6BHep6d+akTmDHyf/uMFrJYUlkVOYQ2xIbL30cUnjGJc0rl6aiDA6fjSvbH6F/p37Y/e188wXzzAkegiJoYkUlReREJpwwf63NG1a8PO/z2fumrm8uf1NTn+v3/VDfENIj0wnIiCCWYNncUefOxo9P6JdxHnHldJxlffv96WgwJfCQi1uO3boWnT1q2t+vh/5+fo1+odazfz9dcAgq1ULbmysrn1GRGihDQ/X+1arFtjq2qivr6552mz6GsHB+lM5gzr5+GgxV6pW1L+vSCXwKSt9Ol3Bgnk+jVjU3bk1n5Dafmti7bF8c+IbooOjGz+hAQK8A9g9fTfZedk1gp8YmlhznT4d+/xHNjZG7469eWfXOyR3SK6XPiphFN/e/22D51Tbcm/fe8+rBMSHxrPzv3bSLVyLppfFCy9v/eilR6YT7h9eb8TT8Dg9+mNYzLCaTv8vDn+BIEzrNY1PDnzCT1Jcs1RVt7BurD+8nqSwJLoEd8Hf5o+flx9h/mFNnjcyfiTPfv0sQ6OHtrhNE7tNZOPRjcTb4y/qvNQOqbyX/R7eVm8mdDu3ogGdAzuj0A9nVOC5b0K1JIYmsvrgamLtsY3mqcuswbPI7JxJp8BOAGRnZePj5YNFLNza+9aL8qGlaZOCf6b8DHP+OYfFWxdTVF7ElJQpjEkYQ/+o/iSEJjQ5eqOyUgv6sWN6y8nREdGKimDDBh2lzeE4/7zgYB3Fzt9fC3KPHrqD0G7XW919u13XhNu108Ltishmtjpv+3Xd9bP5MTl5Mn0jL6jJr0WIs8c1S/CrSQxLxMfqQ7mjnITQBPxsfjw36jmm9ZrWwpZqrkm6hg/3fEj/qP4XfM74pPHc2edOHhpw/oQfgB7tezSY7uPlw8H7DuJvqw0rGBMSw+7peoROx3Y6JN7anLXEhMRg97Oz/MblF2zXxVI9bj0pLAmLWEjpkIKPtbGKQS1DoocwuOtgl5TJHX3u4I4+d1z0qKtqwR/YZWCDQ5o7B9W2A0UFNS741f+Tc5t0GiM6JLpeG72fza+J3JeWNiX4uSW5rNy3kpc3vcymY5uYnDyZmQNn0rtj70bPqaiAXbt0IPsvvtBhTAvPmTxpt2tBT02FG27Qgp6UpDuVgoP1Vt123Bp46/q3Lun3Vb8KX+gDcy5eFi+SOyRz/Mzxmgd3RuaMljLvPFIjUvn6rq8v6pww/zBevfbVH87YAIE+gY0eqxb8bSe3MTJ+ZLOufzEMjR5Kh4AOpEfq4aaLJy7GIj98Y/vZ/Fh3+7ofzNccmjO8FqBXhO7sbez/VneiU3VtvCFuTL2RssoyUjpcUFiPy5o2I/i5JbkMXDiQ/d/tx9fLl3d//O55M+lAt1OvWwcff6xjIW/ZUhuPOTgYrr1Wx77u1ElvXbroNupm3nMGqHkVPne00cWQlZFFbkluS5nUaqgWfIWqN2vUVfTv3J+TD52s+bu6Gao1cmXMlUzqPum8tvtqqmv47f3b1+ujO5f2Ae2ZOWimS2y81LQJwVdKMeXdKRwtOsqqaavI7JxZr9akFKxapYPTf/KJ7hD18YH+/WH6dEhP10Hs4+NbTy29NTEmYQyTuk8iLTKt2ddoqq+lLRPZrna8+6UQ/LaE3c/O0ilLGz1eLfhNNee0NdqE4C/fu5w1h9Ywf+x8ro6/uib94EH485/h/ffh8GHd8Tl1KowfD8OH6/Zzg+uJtcc2+eAZGifIJ6hmUp0R/JYl3D8cb6t3kx22bY1WL/iOKgcPf/YwSWFJ3JWmB43n5MDjj8Nf/6pr7OPH67+nTnVNB6nB4CpEhI7tOnKo4JAR/BbGIhauir2KwV0Hu9uUS0arF/zSilIGdRnE6ITRUGXjN0/CE0/oNvesLJg5U7fFGwytlch2kRw/c5wuQa5fTdHTWHnTSnebcElp9YIf6BPIgmsWUFAAo0fDP/6hA97/7ne6w9VgaO0khiVSWVV53oxtg+FiafWCD7pTduJE+PJL3Yxzq3vnNhgMLcoLY15oMpCLwXChtAnB/+gjWLsW5s83Ym9oewT5BP1wJoPhAmj1gxAdDpg1S68Vc1ejC30ZDAaDodXX8EtLITMTxoypv5yAwWAwGOrjUsEXkdHAnwEr8KpS6umW/o7AQD2hymAwGAxN47ImHRGxAi8BY4CewE9EpGfTZxkMBoPBVbiyDb8fsF8pdVApdRZ4Gzh/jVKDwWAwXBJcKfhRwL/r/H3EmWYwGAwGN+D2UToicreIbBKRTXl5ee42x2AwGNosrhT8o0Ddua6dnWn1UEq9opTqq5Tq2759exeaYzAYDJ6NKwV/I5AoIrEi4g1MBT504fcZDAaDoQlcNixTKVUpItOBT9DDMhcqpXa66vsMBoPB0DQuHYevlFoBrHDldxgMBoPhwhCllLttqEFE8oCcZp4eDpxqQXMuZzzJV/Asfz3JV/Asf13la7RS6oI6QC8rwf9PEJFNSqm+7rbjUuBJvoJn+etJvoJn+Xs5+Or2YZkGg8FguDQYwTcYDAYPoS0J/ivuNuAS4km+gmf560m+gmf563Zf20wbvsFgMBiapi3V8A0Gg8HQBK1e8EVktIjsEZH9IvKwu+1xBSJySES2i8gWEdnkTAsVkdUiss/5aXe3nc1BRBaKSK6I7KiT1qBvonneWdbbRCTNfZY3j0b8nSciR53lu0VExtY59ojT3z0iMso9VjcPEekiIv8UkV0islNE7nemt7nybcLXy6tslVKtdkPP4D0AxAHewFagp7vtcoGfh4Dwc9KeAR527j8M/M7ddjbTtyFAGrDjh3wDxgIrAQEygX+52/4W8nce8FADeXs672kfINZ5r1vd7cNF+BoJpDn3A4G9Tp/aXPk24etlVbatvYbvyWvuTwAWOfcXARPdaEuzUUqtA747J7kx3yYAi5XmayBERCIvjaUtQyP+NsYE4G2lVLlS6ltgP/qebxUopY4rpTY7988A2egl0ttc+Tbha2O4pWxbu+B7ypr7ClglIv8nInc70yKUUsed+yeACPeY5hIa860tl/d0ZzPGwjrNc23GXxGJAfoA/6KNl+85vsJlVLatXfA9hUFKqTR0uMgsERlS96DS74htcrhVW/atDi8D8UBv4DjwR/ea07KISDvgPWCGUqqo7rG2Vr4N+HpZlW1rF/wLWnO/taOUOur8zAWWoV/9Tla/7jo/c91nYYvTmG9tsryVUieVUg6lVBXwF2pf7Vu9vyJiQwvgEqXUUmdymyzfhny93Mq2tQt+m19zX0QCRCSweh8YCexA+3mrM9utwAfusdAlNObbh8AtztEcmUBhnaaBVss57dST0OUL2t+pIuIjIrFAIrDhUtvXXEREgNeAbKXUs3UOtbnybczXy65s3d273QK942PRPeIHgNnutscF/sWhe/O3AjurfQTCgM+AfcCnQKi7bW2mf2+hX3Ur0O2YdzbmG3r0xkvOst4O9HW3/S3k7+tOf7ahhSCyTv7ZTn/3AGPcbf9F+joI3VyzDdji3Ma2xfJtwtfLqmzNTFuDwWDwEFp7k47BYDAYLhAj+AaDweAhGME3GAwGD8EIvsFgMHgIRvANBoPBQzCCbzC0ACJypYgsd7cdBkNTGME3GAwGD8EIvsGjEJFpIrLBuTb5AhGxikixiDznXMf8MxFp78zbW0S+di58tazOuu0JIvKpiGwVkc0iEu+8fDsReVdEdovIEufsS4PhssEIvsFjEJEewBRgoFKqN+AAbgICgE1KqWRgLTDXecpiYKZSqhd6tmR1+hLgJaXUFcAA9MxZ0CskzkCvdR4HDHS5UwbDReDlbgMMhkvIcCAd2OisfPuhF+6qAv7mzPMGsFREgoEQpdRaZ/oi4B3nukZRSqllAEqpMgDn9TYopY44/94CxADrXe+WwXBhGME3eBICLFJKPVIvUWTOOfmau95IeZ19B+b5MlxmmCYdgyfxGXCDiHSAmtiq0ejn4AZnnhuB9UqpQiBfRAY7028G1iodzeiIiEx0XsNHRPwvqRcGQzMxNRCDx6CU2iUij6Kjh1nQK1ZmASVAP+exXHQ7P+ile//bKegHgdud6TcDC0TkN85rTL6EbhgMzcaslmnweESkWCnVzt12GAyuxjTpGAwGg4dgavgGg8HgIZgavsFgMHgIRvANBoPBQzCCbzAYDB6CEXyDwWDwEIzgGwwGg4dgBN9gMBg8hP8HjeTbeDTKCx0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 363us/sample - loss: 0.9860 - acc: 0.7092\n",
      "Loss: 0.98602715012688 Accuracy: 0.7092419\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 2.7278 - acc: 0.1344\n",
      "Epoch 00001: val_loss improved from inf to 2.50811, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_4_conv_checkpoint/001-2.5081.hdf5\n",
      "36805/36805 [==============================] - 33s 886us/sample - loss: 2.7272 - acc: 0.1345 - val_loss: 2.5081 - val_acc: 0.2732\n",
      "Epoch 2/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 2.4087 - acc: 0.2309\n",
      "Epoch 00002: val_loss improved from 2.50811 to 2.18789, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_4_conv_checkpoint/002-2.1879.hdf5\n",
      "36805/36805 [==============================] - 29s 789us/sample - loss: 2.4086 - acc: 0.2309 - val_loss: 2.1879 - val_acc: 0.3855\n",
      "Epoch 3/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 2.2478 - acc: 0.2791\n",
      "Epoch 00003: val_loss improved from 2.18789 to 2.03313, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_4_conv_checkpoint/003-2.0331.hdf5\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 2.2476 - acc: 0.2791 - val_loss: 2.0331 - val_acc: 0.4503\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.1268 - acc: 0.3158\n",
      "Epoch 00004: val_loss improved from 2.03313 to 1.91509, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_4_conv_checkpoint/004-1.9151.hdf5\n",
      "36805/36805 [==============================] - 29s 789us/sample - loss: 2.1268 - acc: 0.3158 - val_loss: 1.9151 - val_acc: 0.4992\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.0367 - acc: 0.3494\n",
      "Epoch 00005: val_loss improved from 1.91509 to 1.81346, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_4_conv_checkpoint/005-1.8135.hdf5\n",
      "36805/36805 [==============================] - 29s 786us/sample - loss: 2.0367 - acc: 0.3494 - val_loss: 1.8135 - val_acc: 0.5134\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9525 - acc: 0.3759\n",
      "Epoch 00006: val_loss improved from 1.81346 to 1.73397, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_4_conv_checkpoint/006-1.7340.hdf5\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 1.9526 - acc: 0.3759 - val_loss: 1.7340 - val_acc: 0.5427\n",
      "Epoch 7/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.8892 - acc: 0.3989\n",
      "Epoch 00007: val_loss improved from 1.73397 to 1.67083, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_4_conv_checkpoint/007-1.6708.hdf5\n",
      "36805/36805 [==============================] - 29s 790us/sample - loss: 1.8892 - acc: 0.3989 - val_loss: 1.6708 - val_acc: 0.5523\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.8222 - acc: 0.4234\n",
      "Epoch 00008: val_loss improved from 1.67083 to 1.60411, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_4_conv_checkpoint/008-1.6041.hdf5\n",
      "36805/36805 [==============================] - 29s 789us/sample - loss: 1.8222 - acc: 0.4234 - val_loss: 1.6041 - val_acc: 0.5761\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.7735 - acc: 0.4382\n",
      "Epoch 00009: val_loss improved from 1.60411 to 1.54785, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_4_conv_checkpoint/009-1.5478.hdf5\n",
      "36805/36805 [==============================] - 29s 789us/sample - loss: 1.7735 - acc: 0.4382 - val_loss: 1.5478 - val_acc: 0.5842\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.7232 - acc: 0.4554\n",
      "Epoch 00010: val_loss improved from 1.54785 to 1.50167, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_4_conv_checkpoint/010-1.5017.hdf5\n",
      "36805/36805 [==============================] - 29s 787us/sample - loss: 1.7232 - acc: 0.4554 - val_loss: 1.5017 - val_acc: 0.6101\n",
      "Epoch 11/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.6789 - acc: 0.4689\n",
      "Epoch 00011: val_loss improved from 1.50167 to 1.45309, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_4_conv_checkpoint/011-1.4531.hdf5\n",
      "36805/36805 [==============================] - 29s 789us/sample - loss: 1.6789 - acc: 0.4688 - val_loss: 1.4531 - val_acc: 0.5998\n",
      "Epoch 12/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.6387 - acc: 0.4821\n",
      "Epoch 00012: val_loss improved from 1.45309 to 1.39973, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_4_conv_checkpoint/012-1.3997.hdf5\n",
      "36805/36805 [==============================] - 29s 790us/sample - loss: 1.6386 - acc: 0.4822 - val_loss: 1.3997 - val_acc: 0.6273\n",
      "Epoch 13/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.5991 - acc: 0.4956\n",
      "Epoch 00013: val_loss improved from 1.39973 to 1.36072, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_4_conv_checkpoint/013-1.3607.hdf5\n",
      "36805/36805 [==============================] - 29s 789us/sample - loss: 1.5993 - acc: 0.4956 - val_loss: 1.3607 - val_acc: 0.6415\n",
      "Epoch 14/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.5752 - acc: 0.5062\n",
      "Epoch 00014: val_loss did not improve from 1.36072\n",
      "36805/36805 [==============================] - 29s 786us/sample - loss: 1.5753 - acc: 0.5061 - val_loss: 1.3716 - val_acc: 0.6054\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5455 - acc: 0.5156\n",
      "Epoch 00015: val_loss improved from 1.36072 to 1.30412, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_4_conv_checkpoint/015-1.3041.hdf5\n",
      "36805/36805 [==============================] - 29s 787us/sample - loss: 1.5454 - acc: 0.5156 - val_loss: 1.3041 - val_acc: 0.6685\n",
      "Epoch 16/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.5222 - acc: 0.5240\n",
      "Epoch 00016: val_loss improved from 1.30412 to 1.30309, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_4_conv_checkpoint/016-1.3031.hdf5\n",
      "36805/36805 [==============================] - 29s 791us/sample - loss: 1.5223 - acc: 0.5240 - val_loss: 1.3031 - val_acc: 0.6436\n",
      "Epoch 17/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.4884 - acc: 0.5358\n",
      "Epoch 00017: val_loss improved from 1.30309 to 1.24431, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_4_conv_checkpoint/017-1.2443.hdf5\n",
      "36805/36805 [==============================] - 29s 790us/sample - loss: 1.4884 - acc: 0.5357 - val_loss: 1.2443 - val_acc: 0.6709\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4719 - acc: 0.5368\n",
      "Epoch 00018: val_loss improved from 1.24431 to 1.22611, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_4_conv_checkpoint/018-1.2261.hdf5\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 1.4719 - acc: 0.5367 - val_loss: 1.2261 - val_acc: 0.6755\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4434 - acc: 0.5469\n",
      "Epoch 00019: val_loss improved from 1.22611 to 1.19010, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_4_conv_checkpoint/019-1.1901.hdf5\n",
      "36805/36805 [==============================] - 29s 789us/sample - loss: 1.4434 - acc: 0.5469 - val_loss: 1.1901 - val_acc: 0.6832\n",
      "Epoch 20/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.4297 - acc: 0.5523\n",
      "Epoch 00020: val_loss improved from 1.19010 to 1.17519, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_4_conv_checkpoint/020-1.1752.hdf5\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 1.4293 - acc: 0.5524 - val_loss: 1.1752 - val_acc: 0.6944\n",
      "Epoch 21/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.4054 - acc: 0.5634\n",
      "Epoch 00021: val_loss improved from 1.17519 to 1.15788, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_4_conv_checkpoint/021-1.1579.hdf5\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 1.4055 - acc: 0.5633 - val_loss: 1.1579 - val_acc: 0.6951\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.4002 - acc: 0.5618\n",
      "Epoch 00022: val_loss did not improve from 1.15788\n",
      "36805/36805 [==============================] - 29s 785us/sample - loss: 1.4003 - acc: 0.5617 - val_loss: 1.1581 - val_acc: 0.6953\n",
      "Epoch 23/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3733 - acc: 0.5740\n",
      "Epoch 00023: val_loss improved from 1.15788 to 1.13736, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_4_conv_checkpoint/023-1.1374.hdf5\n",
      "36805/36805 [==============================] - 29s 791us/sample - loss: 1.3734 - acc: 0.5739 - val_loss: 1.1374 - val_acc: 0.6909\n",
      "Epoch 24/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3653 - acc: 0.5742\n",
      "Epoch 00024: val_loss improved from 1.13736 to 1.12026, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_4_conv_checkpoint/024-1.1203.hdf5\n",
      "36805/36805 [==============================] - 29s 792us/sample - loss: 1.3653 - acc: 0.5742 - val_loss: 1.1203 - val_acc: 0.7079\n",
      "Epoch 25/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3524 - acc: 0.5791\n",
      "Epoch 00025: val_loss did not improve from 1.12026\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 1.3522 - acc: 0.5790 - val_loss: 1.1413 - val_acc: 0.6650\n",
      "Epoch 26/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3390 - acc: 0.5823\n",
      "Epoch 00026: val_loss did not improve from 1.12026\n",
      "36805/36805 [==============================] - 29s 787us/sample - loss: 1.3388 - acc: 0.5824 - val_loss: 1.1213 - val_acc: 0.7067\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3242 - acc: 0.5892\n",
      "Epoch 00027: val_loss improved from 1.12026 to 1.09549, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_4_conv_checkpoint/027-1.0955.hdf5\n",
      "36805/36805 [==============================] - 29s 791us/sample - loss: 1.3242 - acc: 0.5892 - val_loss: 1.0955 - val_acc: 0.6962\n",
      "Epoch 28/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3100 - acc: 0.5966\n",
      "Epoch 00028: val_loss improved from 1.09549 to 1.07931, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_4_conv_checkpoint/028-1.0793.hdf5\n",
      "36805/36805 [==============================] - 29s 791us/sample - loss: 1.3098 - acc: 0.5965 - val_loss: 1.0793 - val_acc: 0.7067\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3020 - acc: 0.5982\n",
      "Epoch 00029: val_loss improved from 1.07931 to 1.06190, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_4_conv_checkpoint/029-1.0619.hdf5\n",
      "36805/36805 [==============================] - 29s 791us/sample - loss: 1.3020 - acc: 0.5982 - val_loss: 1.0619 - val_acc: 0.7084\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2851 - acc: 0.6023\n",
      "Epoch 00030: val_loss did not improve from 1.06190\n",
      "36805/36805 [==============================] - 29s 787us/sample - loss: 1.2851 - acc: 0.6023 - val_loss: 1.2871 - val_acc: 0.5756\n",
      "Epoch 31/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2733 - acc: 0.6044\n",
      "Epoch 00031: val_loss did not improve from 1.06190\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 1.2737 - acc: 0.6043 - val_loss: 1.0697 - val_acc: 0.6967\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2699 - acc: 0.6044\n",
      "Epoch 00032: val_loss improved from 1.06190 to 1.05427, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_4_conv_checkpoint/032-1.0543.hdf5\n",
      "36805/36805 [==============================] - 29s 790us/sample - loss: 1.2700 - acc: 0.6043 - val_loss: 1.0543 - val_acc: 0.6930\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2617 - acc: 0.6089\n",
      "Epoch 00033: val_loss improved from 1.05427 to 1.00429, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_4_conv_checkpoint/033-1.0043.hdf5\n",
      "36805/36805 [==============================] - 29s 785us/sample - loss: 1.2618 - acc: 0.6089 - val_loss: 1.0043 - val_acc: 0.7226\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2538 - acc: 0.6098\n",
      "Epoch 00034: val_loss did not improve from 1.00429\n",
      "36805/36805 [==============================] - 29s 787us/sample - loss: 1.2537 - acc: 0.6098 - val_loss: 1.1372 - val_acc: 0.6362\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2427 - acc: 0.6133\n",
      "Epoch 00035: val_loss did not improve from 1.00429\n",
      "36805/36805 [==============================] - 29s 785us/sample - loss: 1.2430 - acc: 0.6132 - val_loss: 1.0982 - val_acc: 0.6615\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2388 - acc: 0.6173\n",
      "Epoch 00036: val_loss did not improve from 1.00429\n",
      "36805/36805 [==============================] - 29s 782us/sample - loss: 1.2389 - acc: 0.6173 - val_loss: 1.0594 - val_acc: 0.6916\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2289 - acc: 0.6209\n",
      "Epoch 00037: val_loss improved from 1.00429 to 0.95789, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_4_conv_checkpoint/037-0.9579.hdf5\n",
      "36805/36805 [==============================] - 29s 790us/sample - loss: 1.2289 - acc: 0.6209 - val_loss: 0.9579 - val_acc: 0.7445\n",
      "Epoch 38/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2172 - acc: 0.6229\n",
      "Epoch 00038: val_loss did not improve from 0.95789\n",
      "36805/36805 [==============================] - 29s 782us/sample - loss: 1.2171 - acc: 0.6228 - val_loss: 0.9891 - val_acc: 0.7212\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2122 - acc: 0.6257\n",
      "Epoch 00039: val_loss did not improve from 0.95789\n",
      "36805/36805 [==============================] - 29s 784us/sample - loss: 1.2121 - acc: 0.6258 - val_loss: 1.0165 - val_acc: 0.7011\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2053 - acc: 0.6280\n",
      "Epoch 00040: val_loss did not improve from 0.95789\n",
      "36805/36805 [==============================] - 29s 785us/sample - loss: 1.2052 - acc: 0.6280 - val_loss: 0.9665 - val_acc: 0.7310\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1973 - acc: 0.6323\n",
      "Epoch 00041: val_loss improved from 0.95789 to 0.95668, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_4_conv_checkpoint/041-0.9567.hdf5\n",
      "36805/36805 [==============================] - 29s 789us/sample - loss: 1.1973 - acc: 0.6323 - val_loss: 0.9567 - val_acc: 0.7317\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1948 - acc: 0.6312\n",
      "Epoch 00042: val_loss did not improve from 0.95668\n",
      "36805/36805 [==============================] - 29s 791us/sample - loss: 1.1953 - acc: 0.6311 - val_loss: 0.9624 - val_acc: 0.7356\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1914 - acc: 0.6348\n",
      "Epoch 00043: val_loss improved from 0.95668 to 0.94348, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_4_conv_checkpoint/043-0.9435.hdf5\n",
      "36805/36805 [==============================] - 29s 785us/sample - loss: 1.1914 - acc: 0.6348 - val_loss: 0.9435 - val_acc: 0.7363\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1820 - acc: 0.6351\n",
      "Epoch 00044: val_loss did not improve from 0.94348\n",
      "36805/36805 [==============================] - 29s 786us/sample - loss: 1.1820 - acc: 0.6350 - val_loss: 0.9929 - val_acc: 0.7058\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1671 - acc: 0.6404\n",
      "Epoch 00045: val_loss did not improve from 0.94348\n",
      "36805/36805 [==============================] - 29s 784us/sample - loss: 1.1670 - acc: 0.6404 - val_loss: 1.1220 - val_acc: 0.6294\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1639 - acc: 0.6401\n",
      "Epoch 00046: val_loss did not improve from 0.94348\n",
      "36805/36805 [==============================] - 29s 778us/sample - loss: 1.1640 - acc: 0.6401 - val_loss: 1.0853 - val_acc: 0.6573\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1613 - acc: 0.6417\n",
      "Epoch 00047: val_loss improved from 0.94348 to 0.93265, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_4_conv_checkpoint/047-0.9326.hdf5\n",
      "36805/36805 [==============================] - 29s 778us/sample - loss: 1.1615 - acc: 0.6416 - val_loss: 0.9326 - val_acc: 0.7482\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1555 - acc: 0.6428\n",
      "Epoch 00048: val_loss did not improve from 0.93265\n",
      "36805/36805 [==============================] - 29s 786us/sample - loss: 1.1557 - acc: 0.6428 - val_loss: 0.9409 - val_acc: 0.7195\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1466 - acc: 0.6443\n",
      "Epoch 00049: val_loss did not improve from 0.93265\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 1.1465 - acc: 0.6444 - val_loss: 0.9439 - val_acc: 0.7237\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1462 - acc: 0.6491\n",
      "Epoch 00050: val_loss improved from 0.93265 to 0.87580, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_4_conv_checkpoint/050-0.8758.hdf5\n",
      "36805/36805 [==============================] - 29s 786us/sample - loss: 1.1461 - acc: 0.6492 - val_loss: 0.8758 - val_acc: 0.7619\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1451 - acc: 0.6476\n",
      "Epoch 00051: val_loss did not improve from 0.87580\n",
      "36805/36805 [==============================] - 29s 787us/sample - loss: 1.1451 - acc: 0.6476 - val_loss: 1.0749 - val_acc: 0.6564\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1442 - acc: 0.6484\n",
      "Epoch 00052: val_loss did not improve from 0.87580\n",
      "36805/36805 [==============================] - 29s 784us/sample - loss: 1.1442 - acc: 0.6484 - val_loss: 0.8900 - val_acc: 0.7554\n",
      "Epoch 53/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1344 - acc: 0.6509\n",
      "Epoch 00053: val_loss did not improve from 0.87580\n",
      "36805/36805 [==============================] - 29s 785us/sample - loss: 1.1345 - acc: 0.6507 - val_loss: 1.0102 - val_acc: 0.6776\n",
      "Epoch 54/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1282 - acc: 0.6516\n",
      "Epoch 00054: val_loss improved from 0.87580 to 0.86341, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_4_conv_checkpoint/054-0.8634.hdf5\n",
      "36805/36805 [==============================] - 29s 792us/sample - loss: 1.1282 - acc: 0.6515 - val_loss: 0.8634 - val_acc: 0.7680\n",
      "Epoch 55/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1237 - acc: 0.6548\n",
      "Epoch 00055: val_loss did not improve from 0.86341\n",
      "36805/36805 [==============================] - 29s 786us/sample - loss: 1.1238 - acc: 0.6547 - val_loss: 0.9001 - val_acc: 0.7449\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1151 - acc: 0.6560\n",
      "Epoch 00056: val_loss did not improve from 0.86341\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 1.1152 - acc: 0.6560 - val_loss: 0.8928 - val_acc: 0.7480\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1168 - acc: 0.6541\n",
      "Epoch 00057: val_loss did not improve from 0.86341\n",
      "36805/36805 [==============================] - 29s 779us/sample - loss: 1.1169 - acc: 0.6540 - val_loss: 1.1634 - val_acc: 0.6257\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1111 - acc: 0.6574\n",
      "Epoch 00058: val_loss improved from 0.86341 to 0.85992, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_4_conv_checkpoint/058-0.8599.hdf5\n",
      "36805/36805 [==============================] - 29s 778us/sample - loss: 1.1111 - acc: 0.6574 - val_loss: 0.8599 - val_acc: 0.7661\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1018 - acc: 0.6635\n",
      "Epoch 00059: val_loss did not improve from 0.85992\n",
      "36805/36805 [==============================] - 29s 778us/sample - loss: 1.1018 - acc: 0.6635 - val_loss: 0.8789 - val_acc: 0.7547\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1056 - acc: 0.6597\n",
      "Epoch 00060: val_loss did not improve from 0.85992\n",
      "36805/36805 [==============================] - 29s 778us/sample - loss: 1.1056 - acc: 0.6597 - val_loss: 1.3498 - val_acc: 0.5679\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1039 - acc: 0.6619\n",
      "Epoch 00061: val_loss improved from 0.85992 to 0.85045, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_4_conv_checkpoint/061-0.8505.hdf5\n",
      "36805/36805 [==============================] - 29s 782us/sample - loss: 1.1040 - acc: 0.6618 - val_loss: 0.8505 - val_acc: 0.7622\n",
      "Epoch 62/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0945 - acc: 0.6619\n",
      "Epoch 00062: val_loss improved from 0.85045 to 0.84837, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_4_conv_checkpoint/062-0.8484.hdf5\n",
      "36805/36805 [==============================] - 29s 790us/sample - loss: 1.0945 - acc: 0.6619 - val_loss: 0.8484 - val_acc: 0.7657\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0910 - acc: 0.6647\n",
      "Epoch 00063: val_loss did not improve from 0.84837\n",
      "36805/36805 [==============================] - 29s 784us/sample - loss: 1.0910 - acc: 0.6647 - val_loss: 0.8751 - val_acc: 0.7517\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0919 - acc: 0.6631\n",
      "Epoch 00064: val_loss improved from 0.84837 to 0.83261, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_4_conv_checkpoint/064-0.8326.hdf5\n",
      "36805/36805 [==============================] - 29s 791us/sample - loss: 1.0922 - acc: 0.6630 - val_loss: 0.8326 - val_acc: 0.7680\n",
      "Epoch 65/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0893 - acc: 0.6667\n",
      "Epoch 00065: val_loss improved from 0.83261 to 0.83009, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_4_conv_checkpoint/065-0.8301.hdf5\n",
      "36805/36805 [==============================] - 29s 789us/sample - loss: 1.0887 - acc: 0.6670 - val_loss: 0.8301 - val_acc: 0.7727\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0830 - acc: 0.6682\n",
      "Epoch 00066: val_loss did not improve from 0.83009\n",
      "36805/36805 [==============================] - 29s 790us/sample - loss: 1.0829 - acc: 0.6682 - val_loss: 0.8892 - val_acc: 0.7286\n",
      "Epoch 67/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0754 - acc: 0.6716\n",
      "Epoch 00067: val_loss did not improve from 0.83009\n",
      "36805/36805 [==============================] - 29s 787us/sample - loss: 1.0753 - acc: 0.6716 - val_loss: 0.9557 - val_acc: 0.7060\n",
      "Epoch 68/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0710 - acc: 0.6711\n",
      "Epoch 00068: val_loss did not improve from 0.83009\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 1.0710 - acc: 0.6712 - val_loss: 1.5074 - val_acc: 0.5353\n",
      "Epoch 69/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0707 - acc: 0.6706\n",
      "Epoch 00069: val_loss did not improve from 0.83009\n",
      "36805/36805 [==============================] - 29s 784us/sample - loss: 1.0705 - acc: 0.6707 - val_loss: 0.8369 - val_acc: 0.7666\n",
      "Epoch 70/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0709 - acc: 0.6707\n",
      "Epoch 00070: val_loss did not improve from 0.83009\n",
      "36805/36805 [==============================] - 29s 787us/sample - loss: 1.0706 - acc: 0.6706 - val_loss: 0.9422 - val_acc: 0.7216\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0656 - acc: 0.6742\n",
      "Epoch 00071: val_loss improved from 0.83009 to 0.82579, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_4_conv_checkpoint/071-0.8258.hdf5\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 1.0656 - acc: 0.6742 - val_loss: 0.8258 - val_acc: 0.7741\n",
      "Epoch 72/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0640 - acc: 0.6740\n",
      "Epoch 00072: val_loss did not improve from 0.82579\n",
      "36805/36805 [==============================] - 29s 787us/sample - loss: 1.0642 - acc: 0.6740 - val_loss: 0.8377 - val_acc: 0.7529\n",
      "Epoch 73/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0611 - acc: 0.6749\n",
      "Epoch 00073: val_loss did not improve from 0.82579\n",
      "36805/36805 [==============================] - 29s 786us/sample - loss: 1.0609 - acc: 0.6751 - val_loss: 0.8456 - val_acc: 0.7568\n",
      "Epoch 74/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0630 - acc: 0.6754\n",
      "Epoch 00074: val_loss improved from 0.82579 to 0.81532, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_4_conv_checkpoint/074-0.8153.hdf5\n",
      "36805/36805 [==============================] - 29s 792us/sample - loss: 1.0630 - acc: 0.6753 - val_loss: 0.8153 - val_acc: 0.7817\n",
      "Epoch 75/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0597 - acc: 0.6754\n",
      "Epoch 00075: val_loss did not improve from 0.81532\n",
      "36805/36805 [==============================] - 29s 786us/sample - loss: 1.0595 - acc: 0.6754 - val_loss: 1.0631 - val_acc: 0.6548\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0505 - acc: 0.6793\n",
      "Epoch 00076: val_loss did not improve from 0.81532\n",
      "36805/36805 [==============================] - 29s 790us/sample - loss: 1.0505 - acc: 0.6793 - val_loss: 0.8433 - val_acc: 0.7612\n",
      "Epoch 77/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0461 - acc: 0.6793\n",
      "Epoch 00077: val_loss did not improve from 0.81532\n",
      "36805/36805 [==============================] - 29s 791us/sample - loss: 1.0466 - acc: 0.6793 - val_loss: 0.8319 - val_acc: 0.7720\n",
      "Epoch 78/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0495 - acc: 0.6775\n",
      "Epoch 00078: val_loss did not improve from 0.81532\n",
      "36805/36805 [==============================] - 29s 789us/sample - loss: 1.0494 - acc: 0.6775 - val_loss: 0.8510 - val_acc: 0.7596\n",
      "Epoch 79/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0519 - acc: 0.6787\n",
      "Epoch 00079: val_loss improved from 0.81532 to 0.79419, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_4_conv_checkpoint/079-0.7942.hdf5\n",
      "36805/36805 [==============================] - 29s 791us/sample - loss: 1.0517 - acc: 0.6786 - val_loss: 0.7942 - val_acc: 0.7841\n",
      "Epoch 80/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0433 - acc: 0.6793\n",
      "Epoch 00080: val_loss did not improve from 0.79419\n",
      "36805/36805 [==============================] - 29s 792us/sample - loss: 1.0430 - acc: 0.6793 - val_loss: 0.8295 - val_acc: 0.7692\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0382 - acc: 0.6835\n",
      "Epoch 00081: val_loss did not improve from 0.79419\n",
      "36805/36805 [==============================] - 29s 785us/sample - loss: 1.0382 - acc: 0.6835 - val_loss: 0.8264 - val_acc: 0.7640\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0388 - acc: 0.6812\n",
      "Epoch 00082: val_loss did not improve from 0.79419\n",
      "36805/36805 [==============================] - 29s 790us/sample - loss: 1.0388 - acc: 0.6812 - val_loss: 0.9375 - val_acc: 0.7158\n",
      "Epoch 83/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0406 - acc: 0.6811\n",
      "Epoch 00083: val_loss did not improve from 0.79419\n",
      "36805/36805 [==============================] - 29s 784us/sample - loss: 1.0403 - acc: 0.6812 - val_loss: 0.8250 - val_acc: 0.7706\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0390 - acc: 0.6821\n",
      "Epoch 00084: val_loss did not improve from 0.79419\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 1.0391 - acc: 0.6821 - val_loss: 0.9993 - val_acc: 0.6860\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0354 - acc: 0.6835\n",
      "Epoch 00085: val_loss did not improve from 0.79419\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 1.0354 - acc: 0.6835 - val_loss: 0.9941 - val_acc: 0.6771\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0298 - acc: 0.6860\n",
      "Epoch 00086: val_loss did not improve from 0.79419\n",
      "36805/36805 [==============================] - 29s 791us/sample - loss: 1.0297 - acc: 0.6860 - val_loss: 0.8960 - val_acc: 0.7424\n",
      "Epoch 87/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0200 - acc: 0.6881\n",
      "Epoch 00087: val_loss did not improve from 0.79419\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 1.0201 - acc: 0.6881 - val_loss: 1.2481 - val_acc: 0.5975\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0182 - acc: 0.6857\n",
      "Epoch 00088: val_loss did not improve from 0.79419\n",
      "36805/36805 [==============================] - 29s 785us/sample - loss: 1.0182 - acc: 0.6857 - val_loss: 1.0782 - val_acc: 0.6534\n",
      "Epoch 89/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0238 - acc: 0.6879\n",
      "Epoch 00089: val_loss did not improve from 0.79419\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 1.0232 - acc: 0.6881 - val_loss: 0.9444 - val_acc: 0.6988\n",
      "Epoch 90/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0221 - acc: 0.6868\n",
      "Epoch 00090: val_loss did not improve from 0.79419\n",
      "36805/36805 [==============================] - 29s 787us/sample - loss: 1.0220 - acc: 0.6869 - val_loss: 1.6772 - val_acc: 0.5036\n",
      "Epoch 91/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0178 - acc: 0.6907\n",
      "Epoch 00091: val_loss improved from 0.79419 to 0.75940, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_4_conv_checkpoint/091-0.7594.hdf5\n",
      "36805/36805 [==============================] - 29s 789us/sample - loss: 1.0177 - acc: 0.6907 - val_loss: 0.7594 - val_acc: 0.7878\n",
      "Epoch 92/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0110 - acc: 0.6903\n",
      "Epoch 00092: val_loss did not improve from 0.75940\n",
      "36805/36805 [==============================] - 29s 786us/sample - loss: 1.0106 - acc: 0.6905 - val_loss: 0.8611 - val_acc: 0.7489\n",
      "Epoch 93/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0127 - acc: 0.6891\n",
      "Epoch 00093: val_loss did not improve from 0.75940\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 1.0129 - acc: 0.6889 - val_loss: 0.8118 - val_acc: 0.7713\n",
      "Epoch 94/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0084 - acc: 0.6919\n",
      "Epoch 00094: val_loss did not improve from 0.75940\n",
      "36805/36805 [==============================] - 29s 786us/sample - loss: 1.0083 - acc: 0.6919 - val_loss: 0.7794 - val_acc: 0.7862\n",
      "Epoch 95/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0073 - acc: 0.6921\n",
      "Epoch 00095: val_loss did not improve from 0.75940\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 1.0077 - acc: 0.6920 - val_loss: 0.9151 - val_acc: 0.7030\n",
      "Epoch 96/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0068 - acc: 0.6956\n",
      "Epoch 00096: val_loss did not improve from 0.75940\n",
      "36805/36805 [==============================] - 29s 786us/sample - loss: 1.0066 - acc: 0.6956 - val_loss: 1.4252 - val_acc: 0.5653\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0047 - acc: 0.6918\n",
      "Epoch 00097: val_loss did not improve from 0.75940\n",
      "36805/36805 [==============================] - 29s 789us/sample - loss: 1.0048 - acc: 0.6917 - val_loss: 1.1256 - val_acc: 0.6420\n",
      "Epoch 98/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0072 - acc: 0.6913\n",
      "Epoch 00098: val_loss did not improve from 0.75940\n",
      "36805/36805 [==============================] - 29s 790us/sample - loss: 1.0069 - acc: 0.6913 - val_loss: 0.9952 - val_acc: 0.6783\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0031 - acc: 0.6950\n",
      "Epoch 00099: val_loss improved from 0.75940 to 0.73165, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_4_conv_checkpoint/099-0.7317.hdf5\n",
      "36805/36805 [==============================] - 29s 789us/sample - loss: 1.0032 - acc: 0.6950 - val_loss: 0.7317 - val_acc: 0.8004\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0042 - acc: 0.6951\n",
      "Epoch 00100: val_loss did not improve from 0.73165\n",
      "36805/36805 [==============================] - 29s 790us/sample - loss: 1.0041 - acc: 0.6951 - val_loss: 0.7759 - val_acc: 0.7864\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0022 - acc: 0.6924\n",
      "Epoch 00101: val_loss did not improve from 0.73165\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 1.0022 - acc: 0.6924 - val_loss: 0.7484 - val_acc: 0.7929\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9979 - acc: 0.6963\n",
      "Epoch 00102: val_loss did not improve from 0.73165\n",
      "36805/36805 [==============================] - 29s 786us/sample - loss: 0.9980 - acc: 0.6962 - val_loss: 0.7597 - val_acc: 0.7908\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9966 - acc: 0.6944\n",
      "Epoch 00103: val_loss did not improve from 0.73165\n",
      "36805/36805 [==============================] - 29s 791us/sample - loss: 0.9966 - acc: 0.6944 - val_loss: 1.1786 - val_acc: 0.6359\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9875 - acc: 0.6950\n",
      "Epoch 00104: val_loss did not improve from 0.73165\n",
      "36805/36805 [==============================] - 29s 786us/sample - loss: 0.9875 - acc: 0.6950 - val_loss: 0.8716 - val_acc: 0.7321\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9874 - acc: 0.6971\n",
      "Epoch 00105: val_loss did not improve from 0.73165\n",
      "36805/36805 [==============================] - 29s 785us/sample - loss: 0.9873 - acc: 0.6971 - val_loss: 1.5944 - val_acc: 0.5402\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9897 - acc: 0.6968\n",
      "Epoch 00106: val_loss improved from 0.73165 to 0.72687, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_4_conv_checkpoint/106-0.7269.hdf5\n",
      "36805/36805 [==============================] - 29s 786us/sample - loss: 0.9896 - acc: 0.6969 - val_loss: 0.7269 - val_acc: 0.8074\n",
      "Epoch 107/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9903 - acc: 0.6973\n",
      "Epoch 00107: val_loss did not improve from 0.72687\n",
      "36805/36805 [==============================] - 29s 786us/sample - loss: 0.9899 - acc: 0.6973 - val_loss: 0.7352 - val_acc: 0.8032\n",
      "Epoch 108/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9781 - acc: 0.7022\n",
      "Epoch 00108: val_loss did not improve from 0.72687\n",
      "36805/36805 [==============================] - 29s 790us/sample - loss: 0.9783 - acc: 0.7022 - val_loss: 0.8568 - val_acc: 0.7340\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9861 - acc: 0.6987\n",
      "Epoch 00109: val_loss did not improve from 0.72687\n",
      "36805/36805 [==============================] - 29s 784us/sample - loss: 0.9861 - acc: 0.6987 - val_loss: 0.7553 - val_acc: 0.7761\n",
      "Epoch 110/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9815 - acc: 0.7008\n",
      "Epoch 00110: val_loss did not improve from 0.72687\n",
      "36805/36805 [==============================] - 29s 789us/sample - loss: 0.9814 - acc: 0.7007 - val_loss: 0.7563 - val_acc: 0.7922\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9819 - acc: 0.7013\n",
      "Epoch 00111: val_loss did not improve from 0.72687\n",
      "36805/36805 [==============================] - 29s 787us/sample - loss: 0.9819 - acc: 0.7013 - val_loss: 0.7904 - val_acc: 0.7612\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9800 - acc: 0.7013\n",
      "Epoch 00112: val_loss did not improve from 0.72687\n",
      "36805/36805 [==============================] - 29s 782us/sample - loss: 0.9800 - acc: 0.7013 - val_loss: 0.7553 - val_acc: 0.7934\n",
      "Epoch 113/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9791 - acc: 0.7007\n",
      "Epoch 00113: val_loss did not improve from 0.72687\n",
      "36805/36805 [==============================] - 29s 786us/sample - loss: 0.9795 - acc: 0.7007 - val_loss: 0.7987 - val_acc: 0.7666\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9846 - acc: 0.7020\n",
      "Epoch 00114: val_loss did not improve from 0.72687\n",
      "36805/36805 [==============================] - 29s 787us/sample - loss: 0.9846 - acc: 0.7020 - val_loss: 1.9088 - val_acc: 0.5148\n",
      "Epoch 115/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9797 - acc: 0.7020\n",
      "Epoch 00115: val_loss did not improve from 0.72687\n",
      "36805/36805 [==============================] - 29s 786us/sample - loss: 0.9792 - acc: 0.7022 - val_loss: 0.7510 - val_acc: 0.7957\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9728 - acc: 0.7040\n",
      "Epoch 00116: val_loss improved from 0.72687 to 0.70986, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_4_conv_checkpoint/116-0.7099.hdf5\n",
      "36805/36805 [==============================] - 29s 790us/sample - loss: 0.9727 - acc: 0.7041 - val_loss: 0.7099 - val_acc: 0.8097\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9713 - acc: 0.7052\n",
      "Epoch 00117: val_loss did not improve from 0.70986\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 0.9714 - acc: 0.7051 - val_loss: 0.7336 - val_acc: 0.7948\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9706 - acc: 0.7025\n",
      "Epoch 00118: val_loss did not improve from 0.70986\n",
      "36805/36805 [==============================] - 29s 785us/sample - loss: 0.9706 - acc: 0.7025 - val_loss: 1.0323 - val_acc: 0.6753\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9720 - acc: 0.7020\n",
      "Epoch 00119: val_loss did not improve from 0.70986\n",
      "36805/36805 [==============================] - 29s 784us/sample - loss: 0.9722 - acc: 0.7019 - val_loss: 0.7504 - val_acc: 0.7838\n",
      "Epoch 120/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9719 - acc: 0.7033\n",
      "Epoch 00120: val_loss did not improve from 0.70986\n",
      "36805/36805 [==============================] - 29s 784us/sample - loss: 0.9724 - acc: 0.7032 - val_loss: 0.7564 - val_acc: 0.7864\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9670 - acc: 0.7060\n",
      "Epoch 00121: val_loss did not improve from 0.70986\n",
      "36805/36805 [==============================] - 29s 783us/sample - loss: 0.9670 - acc: 0.7060 - val_loss: 1.0684 - val_acc: 0.6676\n",
      "Epoch 122/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9555 - acc: 0.7097\n",
      "Epoch 00122: val_loss did not improve from 0.70986\n",
      "36805/36805 [==============================] - 29s 786us/sample - loss: 0.9555 - acc: 0.7096 - val_loss: 0.7226 - val_acc: 0.8013\n",
      "Epoch 123/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9607 - acc: 0.7082\n",
      "Epoch 00123: val_loss did not improve from 0.70986\n",
      "36805/36805 [==============================] - 29s 789us/sample - loss: 0.9605 - acc: 0.7082 - val_loss: 0.7479 - val_acc: 0.7980\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9642 - acc: 0.7037\n",
      "Epoch 00124: val_loss did not improve from 0.70986\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 0.9643 - acc: 0.7037 - val_loss: 0.7835 - val_acc: 0.7633\n",
      "Epoch 125/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9597 - acc: 0.7073\n",
      "Epoch 00125: val_loss did not improve from 0.70986\n",
      "36805/36805 [==============================] - 29s 783us/sample - loss: 0.9596 - acc: 0.7073 - val_loss: 0.8312 - val_acc: 0.7484\n",
      "Epoch 126/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9553 - acc: 0.7066\n",
      "Epoch 00126: val_loss improved from 0.70986 to 0.70883, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_4_conv_checkpoint/126-0.7088.hdf5\n",
      "36805/36805 [==============================] - 29s 790us/sample - loss: 0.9552 - acc: 0.7067 - val_loss: 0.7088 - val_acc: 0.8036\n",
      "Epoch 127/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9617 - acc: 0.7073\n",
      "Epoch 00127: val_loss did not improve from 0.70883\n",
      "36805/36805 [==============================] - 29s 786us/sample - loss: 0.9620 - acc: 0.7073 - val_loss: 0.8241 - val_acc: 0.7405\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9610 - acc: 0.7085\n",
      "Epoch 00128: val_loss did not improve from 0.70883\n",
      "36805/36805 [==============================] - 29s 787us/sample - loss: 0.9610 - acc: 0.7085 - val_loss: 0.7696 - val_acc: 0.7838\n",
      "Epoch 129/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9553 - acc: 0.7086\n",
      "Epoch 00129: val_loss did not improve from 0.70883\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 0.9553 - acc: 0.7085 - val_loss: 0.7470 - val_acc: 0.7925\n",
      "Epoch 130/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9514 - acc: 0.7082\n",
      "Epoch 00130: val_loss did not improve from 0.70883\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 0.9514 - acc: 0.7082 - val_loss: 1.0280 - val_acc: 0.6664\n",
      "Epoch 131/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9564 - acc: 0.7088\n",
      "Epoch 00131: val_loss did not improve from 0.70883\n",
      "36805/36805 [==============================] - 29s 791us/sample - loss: 0.9563 - acc: 0.7087 - val_loss: 0.7852 - val_acc: 0.7685\n",
      "Epoch 132/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9492 - acc: 0.7107\n",
      "Epoch 00132: val_loss did not improve from 0.70883\n",
      "36805/36805 [==============================] - 29s 789us/sample - loss: 0.9491 - acc: 0.7107 - val_loss: 0.8207 - val_acc: 0.7517\n",
      "Epoch 133/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9471 - acc: 0.7110\n",
      "Epoch 00133: val_loss did not improve from 0.70883\n",
      "36805/36805 [==============================] - 29s 786us/sample - loss: 0.9471 - acc: 0.7110 - val_loss: 0.7626 - val_acc: 0.7859\n",
      "Epoch 134/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9549 - acc: 0.7078\n",
      "Epoch 00134: val_loss did not improve from 0.70883\n",
      "36805/36805 [==============================] - 29s 790us/sample - loss: 0.9548 - acc: 0.7078 - val_loss: 0.7134 - val_acc: 0.8029\n",
      "Epoch 135/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9421 - acc: 0.7104\n",
      "Epoch 00135: val_loss did not improve from 0.70883\n",
      "36805/36805 [==============================] - 29s 787us/sample - loss: 0.9421 - acc: 0.7105 - val_loss: 0.7697 - val_acc: 0.7629\n",
      "Epoch 136/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9457 - acc: 0.7115\n",
      "Epoch 00136: val_loss did not improve from 0.70883\n",
      "36805/36805 [==============================] - 29s 783us/sample - loss: 0.9456 - acc: 0.7115 - val_loss: 0.7305 - val_acc: 0.7883\n",
      "Epoch 137/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9421 - acc: 0.7136\n",
      "Epoch 00137: val_loss did not improve from 0.70883\n",
      "36805/36805 [==============================] - 29s 786us/sample - loss: 0.9425 - acc: 0.7135 - val_loss: 0.7177 - val_acc: 0.8067\n",
      "Epoch 138/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9383 - acc: 0.7121\n",
      "Epoch 00138: val_loss did not improve from 0.70883\n",
      "36805/36805 [==============================] - 29s 789us/sample - loss: 0.9381 - acc: 0.7122 - val_loss: 0.7215 - val_acc: 0.8013\n",
      "Epoch 139/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9409 - acc: 0.7103\n",
      "Epoch 00139: val_loss improved from 0.70883 to 0.69505, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_4_conv_checkpoint/139-0.6951.hdf5\n",
      "36805/36805 [==============================] - 29s 789us/sample - loss: 0.9410 - acc: 0.7103 - val_loss: 0.6951 - val_acc: 0.8155\n",
      "Epoch 140/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9358 - acc: 0.7131\n",
      "Epoch 00140: val_loss did not improve from 0.69505\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 0.9359 - acc: 0.7131 - val_loss: 0.8552 - val_acc: 0.7275\n",
      "Epoch 141/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9417 - acc: 0.7101\n",
      "Epoch 00141: val_loss improved from 0.69505 to 0.68844, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_4_conv_checkpoint/141-0.6884.hdf5\n",
      "36805/36805 [==============================] - 29s 787us/sample - loss: 0.9419 - acc: 0.7100 - val_loss: 0.6884 - val_acc: 0.8109\n",
      "Epoch 142/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9357 - acc: 0.7171\n",
      "Epoch 00142: val_loss did not improve from 0.68844\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 0.9358 - acc: 0.7172 - val_loss: 0.7412 - val_acc: 0.7834\n",
      "Epoch 143/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9463 - acc: 0.7081\n",
      "Epoch 00143: val_loss did not improve from 0.68844\n",
      "36805/36805 [==============================] - 29s 785us/sample - loss: 0.9458 - acc: 0.7082 - val_loss: 1.0711 - val_acc: 0.6483\n",
      "Epoch 144/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9365 - acc: 0.7137\n",
      "Epoch 00144: val_loss improved from 0.68844 to 0.67780, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_4_conv_checkpoint/144-0.6778.hdf5\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 0.9366 - acc: 0.7136 - val_loss: 0.6778 - val_acc: 0.8174\n",
      "Epoch 145/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9366 - acc: 0.7138\n",
      "Epoch 00145: val_loss did not improve from 0.67780\n",
      "36805/36805 [==============================] - 29s 785us/sample - loss: 0.9371 - acc: 0.7136 - val_loss: 1.1080 - val_acc: 0.6350\n",
      "Epoch 146/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9346 - acc: 0.7146\n",
      "Epoch 00146: val_loss did not improve from 0.67780\n",
      "36805/36805 [==============================] - 29s 783us/sample - loss: 0.9351 - acc: 0.7143 - val_loss: 0.6975 - val_acc: 0.8088\n",
      "Epoch 147/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9308 - acc: 0.7134\n",
      "Epoch 00147: val_loss did not improve from 0.67780\n",
      "36805/36805 [==============================] - 29s 786us/sample - loss: 0.9309 - acc: 0.7133 - val_loss: 0.8731 - val_acc: 0.7268\n",
      "Epoch 148/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9321 - acc: 0.7135\n",
      "Epoch 00148: val_loss did not improve from 0.67780\n",
      "36805/36805 [==============================] - 29s 782us/sample - loss: 0.9320 - acc: 0.7137 - val_loss: 0.7061 - val_acc: 0.8071\n",
      "Epoch 149/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9323 - acc: 0.7163\n",
      "Epoch 00149: val_loss did not improve from 0.67780\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 0.9327 - acc: 0.7162 - val_loss: 0.6931 - val_acc: 0.8130\n",
      "Epoch 150/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9317 - acc: 0.7155\n",
      "Epoch 00150: val_loss did not improve from 0.67780\n",
      "36805/36805 [==============================] - 29s 784us/sample - loss: 0.9318 - acc: 0.7154 - val_loss: 0.6875 - val_acc: 0.8127\n",
      "Epoch 151/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9304 - acc: 0.7148\n",
      "Epoch 00151: val_loss did not improve from 0.67780\n",
      "36805/36805 [==============================] - 29s 785us/sample - loss: 0.9303 - acc: 0.7148 - val_loss: 0.6971 - val_acc: 0.8050\n",
      "Epoch 152/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9265 - acc: 0.7183\n",
      "Epoch 00152: val_loss did not improve from 0.67780\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 0.9265 - acc: 0.7183 - val_loss: 1.3752 - val_acc: 0.5772\n",
      "Epoch 153/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9278 - acc: 0.7182\n",
      "Epoch 00153: val_loss improved from 0.67780 to 0.67043, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_4_conv_checkpoint/153-0.6704.hdf5\n",
      "36805/36805 [==============================] - 29s 787us/sample - loss: 0.9284 - acc: 0.7182 - val_loss: 0.6704 - val_acc: 0.8190\n",
      "Epoch 154/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9313 - acc: 0.7165\n",
      "Epoch 00154: val_loss did not improve from 0.67043\n",
      "36805/36805 [==============================] - 29s 785us/sample - loss: 0.9314 - acc: 0.7165 - val_loss: 0.7284 - val_acc: 0.7983\n",
      "Epoch 155/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9254 - acc: 0.7210\n",
      "Epoch 00155: val_loss did not improve from 0.67043\n",
      "36805/36805 [==============================] - 29s 785us/sample - loss: 0.9253 - acc: 0.7210 - val_loss: 0.7153 - val_acc: 0.7985\n",
      "Epoch 156/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9232 - acc: 0.7171\n",
      "Epoch 00156: val_loss did not improve from 0.67043\n",
      "36805/36805 [==============================] - 29s 784us/sample - loss: 0.9232 - acc: 0.7171 - val_loss: 0.6718 - val_acc: 0.8174\n",
      "Epoch 157/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9200 - acc: 0.7203\n",
      "Epoch 00157: val_loss did not improve from 0.67043\n",
      "36805/36805 [==============================] - 29s 787us/sample - loss: 0.9201 - acc: 0.7203 - val_loss: 0.6842 - val_acc: 0.8095\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 158/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9252 - acc: 0.7155\n",
      "Epoch 00158: val_loss did not improve from 0.67043\n",
      "36805/36805 [==============================] - 29s 790us/sample - loss: 0.9253 - acc: 0.7154 - val_loss: 0.7264 - val_acc: 0.7918\n",
      "Epoch 159/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9227 - acc: 0.7202\n",
      "Epoch 00159: val_loss did not improve from 0.67043\n",
      "36805/36805 [==============================] - 29s 783us/sample - loss: 0.9227 - acc: 0.7201 - val_loss: 0.7324 - val_acc: 0.7817\n",
      "Epoch 160/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9223 - acc: 0.7175\n",
      "Epoch 00160: val_loss did not improve from 0.67043\n",
      "36805/36805 [==============================] - 29s 787us/sample - loss: 0.9222 - acc: 0.7175 - val_loss: 0.8161 - val_acc: 0.7480\n",
      "Epoch 161/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9210 - acc: 0.7199\n",
      "Epoch 00161: val_loss did not improve from 0.67043\n",
      "36805/36805 [==============================] - 29s 789us/sample - loss: 0.9213 - acc: 0.7198 - val_loss: 0.6953 - val_acc: 0.8183\n",
      "Epoch 162/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9260 - acc: 0.7171\n",
      "Epoch 00162: val_loss did not improve from 0.67043\n",
      "36805/36805 [==============================] - 29s 782us/sample - loss: 0.9261 - acc: 0.7171 - val_loss: 0.7473 - val_acc: 0.7852\n",
      "Epoch 163/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9191 - acc: 0.7213\n",
      "Epoch 00163: val_loss did not improve from 0.67043\n",
      "36805/36805 [==============================] - 29s 779us/sample - loss: 0.9191 - acc: 0.7213 - val_loss: 0.8922 - val_acc: 0.7135\n",
      "Epoch 164/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9146 - acc: 0.7207\n",
      "Epoch 00164: val_loss did not improve from 0.67043\n",
      "36805/36805 [==============================] - 29s 777us/sample - loss: 0.9146 - acc: 0.7207 - val_loss: 0.7345 - val_acc: 0.7948\n",
      "Epoch 165/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9186 - acc: 0.7213\n",
      "Epoch 00165: val_loss did not improve from 0.67043\n",
      "36805/36805 [==============================] - 28s 774us/sample - loss: 0.9186 - acc: 0.7213 - val_loss: 0.6952 - val_acc: 0.8076\n",
      "Epoch 166/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9158 - acc: 0.7211\n",
      "Epoch 00166: val_loss improved from 0.67043 to 0.66976, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_4_conv_checkpoint/166-0.6698.hdf5\n",
      "36805/36805 [==============================] - 29s 779us/sample - loss: 0.9159 - acc: 0.7211 - val_loss: 0.6698 - val_acc: 0.8167\n",
      "Epoch 167/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9082 - acc: 0.7216\n",
      "Epoch 00167: val_loss did not improve from 0.66976\n",
      "36805/36805 [==============================] - 29s 775us/sample - loss: 0.9082 - acc: 0.7216 - val_loss: 0.7234 - val_acc: 0.8046\n",
      "Epoch 168/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9119 - acc: 0.7210\n",
      "Epoch 00168: val_loss did not improve from 0.66976\n",
      "36805/36805 [==============================] - 28s 774us/sample - loss: 0.9119 - acc: 0.7210 - val_loss: 0.7113 - val_acc: 0.8095\n",
      "Epoch 169/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9113 - acc: 0.7225\n",
      "Epoch 00169: val_loss did not improve from 0.66976\n",
      "36805/36805 [==============================] - 29s 778us/sample - loss: 0.9112 - acc: 0.7225 - val_loss: 0.6931 - val_acc: 0.8109\n",
      "Epoch 170/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9115 - acc: 0.7228\n",
      "Epoch 00170: val_loss did not improve from 0.66976\n",
      "36805/36805 [==============================] - 29s 775us/sample - loss: 0.9115 - acc: 0.7228 - val_loss: 0.6720 - val_acc: 0.8113\n",
      "Epoch 171/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9088 - acc: 0.7200\n",
      "Epoch 00171: val_loss did not improve from 0.66976\n",
      "36805/36805 [==============================] - 29s 775us/sample - loss: 0.9089 - acc: 0.7200 - val_loss: 0.7155 - val_acc: 0.8078\n",
      "Epoch 172/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9093 - acc: 0.7244\n",
      "Epoch 00172: val_loss did not improve from 0.66976\n",
      "36805/36805 [==============================] - 28s 774us/sample - loss: 0.9092 - acc: 0.7244 - val_loss: 0.8565 - val_acc: 0.7305\n",
      "Epoch 173/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9083 - acc: 0.7234\n",
      "Epoch 00173: val_loss did not improve from 0.66976\n",
      "36805/36805 [==============================] - 29s 787us/sample - loss: 0.9083 - acc: 0.7234 - val_loss: 1.5219 - val_acc: 0.5388\n",
      "Epoch 174/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9070 - acc: 0.7230\n",
      "Epoch 00174: val_loss did not improve from 0.66976\n",
      "36805/36805 [==============================] - 29s 784us/sample - loss: 0.9070 - acc: 0.7230 - val_loss: 0.6987 - val_acc: 0.8036\n",
      "Epoch 175/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9027 - acc: 0.7250\n",
      "Epoch 00175: val_loss did not improve from 0.66976\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 0.9026 - acc: 0.7250 - val_loss: 0.9453 - val_acc: 0.6886\n",
      "Epoch 176/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9035 - acc: 0.7235\n",
      "Epoch 00176: val_loss did not improve from 0.66976\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 0.9034 - acc: 0.7235 - val_loss: 1.3016 - val_acc: 0.5700\n",
      "Epoch 177/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9084 - acc: 0.7252\n",
      "Epoch 00177: val_loss did not improve from 0.66976\n",
      "36805/36805 [==============================] - 29s 789us/sample - loss: 0.9088 - acc: 0.7250 - val_loss: 1.6791 - val_acc: 0.4962\n",
      "Epoch 178/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8987 - acc: 0.7273\n",
      "Epoch 00178: val_loss did not improve from 0.66976\n",
      "36805/36805 [==============================] - 29s 789us/sample - loss: 0.8986 - acc: 0.7274 - val_loss: 0.7376 - val_acc: 0.7720\n",
      "Epoch 179/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8993 - acc: 0.7244\n",
      "Epoch 00179: val_loss did not improve from 0.66976\n",
      "36805/36805 [==============================] - 29s 790us/sample - loss: 0.8992 - acc: 0.7243 - val_loss: 0.6751 - val_acc: 0.8181\n",
      "Epoch 180/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8974 - acc: 0.7281\n",
      "Epoch 00180: val_loss did not improve from 0.66976\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 0.8974 - acc: 0.7281 - val_loss: 0.6991 - val_acc: 0.8046\n",
      "Epoch 181/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9018 - acc: 0.7245\n",
      "Epoch 00181: val_loss improved from 0.66976 to 0.65860, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_4_conv_checkpoint/181-0.6586.hdf5\n",
      "36805/36805 [==============================] - 29s 791us/sample - loss: 0.9019 - acc: 0.7245 - val_loss: 0.6586 - val_acc: 0.8183\n",
      "Epoch 182/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9022 - acc: 0.7252\n",
      "Epoch 00182: val_loss did not improve from 0.65860\n",
      "36805/36805 [==============================] - 29s 780us/sample - loss: 0.9024 - acc: 0.7252 - val_loss: 0.7655 - val_acc: 0.7727\n",
      "Epoch 183/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8991 - acc: 0.7257\n",
      "Epoch 00183: val_loss did not improve from 0.65860\n",
      "36805/36805 [==============================] - 29s 789us/sample - loss: 0.8990 - acc: 0.7257 - val_loss: 0.7741 - val_acc: 0.7617\n",
      "Epoch 184/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9016 - acc: 0.7244\n",
      "Epoch 00184: val_loss did not improve from 0.65860\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 0.9020 - acc: 0.7244 - val_loss: 0.7003 - val_acc: 0.8008\n",
      "Epoch 185/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9029 - acc: 0.7252\n",
      "Epoch 00185: val_loss did not improve from 0.65860\n",
      "36805/36805 [==============================] - 29s 784us/sample - loss: 0.9030 - acc: 0.7251 - val_loss: 0.6748 - val_acc: 0.8111\n",
      "Epoch 186/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8959 - acc: 0.7270\n",
      "Epoch 00186: val_loss did not improve from 0.65860\n",
      "36805/36805 [==============================] - 29s 785us/sample - loss: 0.8953 - acc: 0.7272 - val_loss: 1.9867 - val_acc: 0.5143\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 187/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8976 - acc: 0.7266\n",
      "Epoch 00187: val_loss did not improve from 0.65860\n",
      "36805/36805 [==============================] - 29s 784us/sample - loss: 0.8975 - acc: 0.7266 - val_loss: 0.6773 - val_acc: 0.8020\n",
      "Epoch 188/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8946 - acc: 0.7271\n",
      "Epoch 00188: val_loss improved from 0.65860 to 0.64300, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_4_conv_checkpoint/188-0.6430.hdf5\n",
      "36805/36805 [==============================] - 29s 790us/sample - loss: 0.8946 - acc: 0.7271 - val_loss: 0.6430 - val_acc: 0.8258\n",
      "Epoch 189/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8938 - acc: 0.7282\n",
      "Epoch 00189: val_loss did not improve from 0.64300\n",
      "36805/36805 [==============================] - 29s 786us/sample - loss: 0.8939 - acc: 0.7282 - val_loss: 1.1161 - val_acc: 0.6455\n",
      "Epoch 190/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8878 - acc: 0.7291\n",
      "Epoch 00190: val_loss did not improve from 0.64300\n",
      "36805/36805 [==============================] - 29s 790us/sample - loss: 0.8878 - acc: 0.7291 - val_loss: 0.9051 - val_acc: 0.7109\n",
      "Epoch 191/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8941 - acc: 0.7285\n",
      "Epoch 00191: val_loss did not improve from 0.64300\n",
      "36805/36805 [==============================] - 29s 784us/sample - loss: 0.8942 - acc: 0.7285 - val_loss: 0.6729 - val_acc: 0.8155\n",
      "Epoch 192/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8991 - acc: 0.7267\n",
      "Epoch 00192: val_loss did not improve from 0.64300\n",
      "36805/36805 [==============================] - 29s 787us/sample - loss: 0.8992 - acc: 0.7266 - val_loss: 0.7464 - val_acc: 0.7857\n",
      "Epoch 193/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8970 - acc: 0.7260\n",
      "Epoch 00193: val_loss did not improve from 0.64300\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 0.8970 - acc: 0.7260 - val_loss: 0.8053 - val_acc: 0.7589\n",
      "Epoch 194/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8879 - acc: 0.7298\n",
      "Epoch 00194: val_loss did not improve from 0.64300\n",
      "36805/36805 [==============================] - 29s 786us/sample - loss: 0.8878 - acc: 0.7298 - val_loss: 1.5318 - val_acc: 0.5607\n",
      "Epoch 195/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8929 - acc: 0.7269\n",
      "Epoch 00195: val_loss did not improve from 0.64300\n",
      "36805/36805 [==============================] - 29s 785us/sample - loss: 0.8932 - acc: 0.7269 - val_loss: 0.6493 - val_acc: 0.8232\n",
      "Epoch 196/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8885 - acc: 0.7301\n",
      "Epoch 00196: val_loss did not improve from 0.64300\n",
      "36805/36805 [==============================] - 29s 784us/sample - loss: 0.8885 - acc: 0.7300 - val_loss: 0.6909 - val_acc: 0.8097\n",
      "Epoch 197/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8912 - acc: 0.7270\n",
      "Epoch 00197: val_loss did not improve from 0.64300\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 0.8914 - acc: 0.7270 - val_loss: 1.0893 - val_acc: 0.6511\n",
      "Epoch 198/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8870 - acc: 0.7298\n",
      "Epoch 00198: val_loss did not improve from 0.64300\n",
      "36805/36805 [==============================] - 29s 787us/sample - loss: 0.8869 - acc: 0.7297 - val_loss: 3.2541 - val_acc: 0.4251\n",
      "Epoch 199/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8894 - acc: 0.7302\n",
      "Epoch 00199: val_loss did not improve from 0.64300\n",
      "36805/36805 [==============================] - 29s 777us/sample - loss: 0.8895 - acc: 0.7301 - val_loss: 0.8404 - val_acc: 0.7379\n",
      "Epoch 200/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8887 - acc: 0.7283\n",
      "Epoch 00200: val_loss did not improve from 0.64300\n",
      "36805/36805 [==============================] - 29s 785us/sample - loss: 0.8890 - acc: 0.7283 - val_loss: 0.6713 - val_acc: 0.8137\n",
      "Epoch 201/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8851 - acc: 0.7293\n",
      "Epoch 00201: val_loss did not improve from 0.64300\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 0.8851 - acc: 0.7292 - val_loss: 0.7320 - val_acc: 0.7887\n",
      "Epoch 202/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8747 - acc: 0.7343\n",
      "Epoch 00202: val_loss did not improve from 0.64300\n",
      "36805/36805 [==============================] - 29s 775us/sample - loss: 0.8751 - acc: 0.7343 - val_loss: 0.6445 - val_acc: 0.8253\n",
      "Epoch 203/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8821 - acc: 0.7326\n",
      "Epoch 00203: val_loss did not improve from 0.64300\n",
      "36805/36805 [==============================] - 29s 789us/sample - loss: 0.8821 - acc: 0.7326 - val_loss: 0.6974 - val_acc: 0.8025\n",
      "Epoch 204/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8850 - acc: 0.7298\n",
      "Epoch 00204: val_loss did not improve from 0.64300\n",
      "36805/36805 [==============================] - 29s 779us/sample - loss: 0.8850 - acc: 0.7298 - val_loss: 0.8079 - val_acc: 0.7489\n",
      "Epoch 205/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8877 - acc: 0.7291\n",
      "Epoch 00205: val_loss did not improve from 0.64300\n",
      "36805/36805 [==============================] - 29s 790us/sample - loss: 0.8877 - acc: 0.7291 - val_loss: 0.6602 - val_acc: 0.8204\n",
      "Epoch 206/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8864 - acc: 0.7303\n",
      "Epoch 00206: val_loss improved from 0.64300 to 0.64112, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_4_conv_checkpoint/206-0.6411.hdf5\n",
      "36805/36805 [==============================] - 29s 782us/sample - loss: 0.8864 - acc: 0.7303 - val_loss: 0.6411 - val_acc: 0.8293\n",
      "Epoch 207/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8854 - acc: 0.7286\n",
      "Epoch 00207: val_loss did not improve from 0.64112\n",
      "36805/36805 [==============================] - 29s 786us/sample - loss: 0.8856 - acc: 0.7286 - val_loss: 1.0113 - val_acc: 0.6606\n",
      "Epoch 208/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8775 - acc: 0.7320\n",
      "Epoch 00208: val_loss did not improve from 0.64112\n",
      "36805/36805 [==============================] - 29s 786us/sample - loss: 0.8773 - acc: 0.7320 - val_loss: 1.7737 - val_acc: 0.5008\n",
      "Epoch 209/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8775 - acc: 0.7336\n",
      "Epoch 00209: val_loss did not improve from 0.64112\n",
      "36805/36805 [==============================] - 29s 789us/sample - loss: 0.8775 - acc: 0.7336 - val_loss: 0.6824 - val_acc: 0.8069\n",
      "Epoch 210/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8844 - acc: 0.7291\n",
      "Epoch 00210: val_loss did not improve from 0.64112\n",
      "36805/36805 [==============================] - 29s 790us/sample - loss: 0.8845 - acc: 0.7291 - val_loss: 0.7380 - val_acc: 0.7771\n",
      "Epoch 211/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8752 - acc: 0.7337\n",
      "Epoch 00211: val_loss did not improve from 0.64112\n",
      "36805/36805 [==============================] - 29s 787us/sample - loss: 0.8752 - acc: 0.7337 - val_loss: 0.6770 - val_acc: 0.8146\n",
      "Epoch 212/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8824 - acc: 0.7314\n",
      "Epoch 00212: val_loss did not improve from 0.64112\n",
      "36805/36805 [==============================] - 29s 784us/sample - loss: 0.8823 - acc: 0.7313 - val_loss: 0.6557 - val_acc: 0.8150\n",
      "Epoch 213/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8784 - acc: 0.7357\n",
      "Epoch 00213: val_loss did not improve from 0.64112\n",
      "36805/36805 [==============================] - 29s 789us/sample - loss: 0.8784 - acc: 0.7357 - val_loss: 0.8089 - val_acc: 0.7538\n",
      "Epoch 214/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8740 - acc: 0.7315\n",
      "Epoch 00214: val_loss improved from 0.64112 to 0.63775, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_4_conv_checkpoint/214-0.6377.hdf5\n",
      "36805/36805 [==============================] - 29s 787us/sample - loss: 0.8734 - acc: 0.7316 - val_loss: 0.6377 - val_acc: 0.8316\n",
      "Epoch 215/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8765 - acc: 0.7354\n",
      "Epoch 00215: val_loss did not improve from 0.63775\n",
      "36805/36805 [==============================] - 29s 784us/sample - loss: 0.8764 - acc: 0.7354 - val_loss: 0.7413 - val_acc: 0.7796\n",
      "Epoch 216/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8755 - acc: 0.7327\n",
      "Epoch 00216: val_loss did not improve from 0.63775\n",
      "36805/36805 [==============================] - 29s 787us/sample - loss: 0.8754 - acc: 0.7327 - val_loss: 1.0449 - val_acc: 0.6699\n",
      "Epoch 217/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8801 - acc: 0.7312\n",
      "Epoch 00217: val_loss did not improve from 0.63775\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 0.8801 - acc: 0.7312 - val_loss: 1.3878 - val_acc: 0.5723\n",
      "Epoch 218/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8758 - acc: 0.7338\n",
      "Epoch 00218: val_loss did not improve from 0.63775\n",
      "36805/36805 [==============================] - 29s 786us/sample - loss: 0.8758 - acc: 0.7337 - val_loss: 0.8185 - val_acc: 0.7419\n",
      "Epoch 219/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8762 - acc: 0.7340\n",
      "Epoch 00219: val_loss did not improve from 0.63775\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 0.8763 - acc: 0.7339 - val_loss: 1.1186 - val_acc: 0.6450\n",
      "Epoch 220/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8709 - acc: 0.7328\n",
      "Epoch 00220: val_loss did not improve from 0.63775\n",
      "36805/36805 [==============================] - 29s 786us/sample - loss: 0.8709 - acc: 0.7328 - val_loss: 0.6994 - val_acc: 0.7897\n",
      "Epoch 221/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8687 - acc: 0.7364\n",
      "Epoch 00221: val_loss did not improve from 0.63775\n",
      "36805/36805 [==============================] - 29s 783us/sample - loss: 0.8687 - acc: 0.7364 - val_loss: 0.6899 - val_acc: 0.8092\n",
      "Epoch 222/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8720 - acc: 0.7364\n",
      "Epoch 00222: val_loss did not improve from 0.63775\n",
      "36805/36805 [==============================] - 29s 785us/sample - loss: 0.8720 - acc: 0.7364 - val_loss: 0.6788 - val_acc: 0.8139\n",
      "Epoch 223/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8716 - acc: 0.7385\n",
      "Epoch 00223: val_loss did not improve from 0.63775\n",
      "36805/36805 [==============================] - 29s 780us/sample - loss: 0.8716 - acc: 0.7385 - val_loss: 0.8202 - val_acc: 0.7407\n",
      "Epoch 224/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8650 - acc: 0.7365\n",
      "Epoch 00224: val_loss improved from 0.63775 to 0.63604, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_4_conv_checkpoint/224-0.6360.hdf5\n",
      "36805/36805 [==============================] - 29s 778us/sample - loss: 0.8650 - acc: 0.7365 - val_loss: 0.6360 - val_acc: 0.8290\n",
      "Epoch 225/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8673 - acc: 0.7358\n",
      "Epoch 00225: val_loss did not improve from 0.63604\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 0.8680 - acc: 0.7358 - val_loss: 0.6962 - val_acc: 0.8050\n",
      "Epoch 226/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8627 - acc: 0.7353\n",
      "Epoch 00226: val_loss did not improve from 0.63604\n",
      "36805/36805 [==============================] - 29s 790us/sample - loss: 0.8627 - acc: 0.7353 - val_loss: 0.6446 - val_acc: 0.8204\n",
      "Epoch 227/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8751 - acc: 0.7354\n",
      "Epoch 00227: val_loss did not improve from 0.63604\n",
      "36805/36805 [==============================] - 29s 779us/sample - loss: 0.8746 - acc: 0.7356 - val_loss: 0.6747 - val_acc: 0.8143\n",
      "Epoch 228/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8627 - acc: 0.7357\n",
      "Epoch 00228: val_loss did not improve from 0.63604\n",
      "36805/36805 [==============================] - 29s 785us/sample - loss: 0.8628 - acc: 0.7357 - val_loss: 0.6896 - val_acc: 0.8022\n",
      "Epoch 229/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8616 - acc: 0.7395\n",
      "Epoch 00229: val_loss did not improve from 0.63604\n",
      "36805/36805 [==============================] - 29s 783us/sample - loss: 0.8621 - acc: 0.7396 - val_loss: 0.9744 - val_acc: 0.6848\n",
      "Epoch 230/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8656 - acc: 0.7358\n",
      "Epoch 00230: val_loss did not improve from 0.63604\n",
      "36805/36805 [==============================] - 29s 787us/sample - loss: 0.8663 - acc: 0.7357 - val_loss: 0.6373 - val_acc: 0.8225\n",
      "Epoch 231/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8621 - acc: 0.7362\n",
      "Epoch 00231: val_loss improved from 0.63604 to 0.62815, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_4_conv_checkpoint/231-0.6281.hdf5\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 0.8621 - acc: 0.7362 - val_loss: 0.6281 - val_acc: 0.8316\n",
      "Epoch 232/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8667 - acc: 0.7370\n",
      "Epoch 00232: val_loss did not improve from 0.62815\n",
      "36805/36805 [==============================] - 29s 789us/sample - loss: 0.8666 - acc: 0.7370 - val_loss: 0.6752 - val_acc: 0.8088\n",
      "Epoch 233/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8629 - acc: 0.7373\n",
      "Epoch 00233: val_loss did not improve from 0.62815\n",
      "36805/36805 [==============================] - 29s 785us/sample - loss: 0.8630 - acc: 0.7372 - val_loss: 0.6513 - val_acc: 0.8260\n",
      "Epoch 234/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8675 - acc: 0.7331\n",
      "Epoch 00234: val_loss did not improve from 0.62815\n",
      "36805/36805 [==============================] - 29s 789us/sample - loss: 0.8676 - acc: 0.7330 - val_loss: 0.6608 - val_acc: 0.8150\n",
      "Epoch 235/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8645 - acc: 0.7357\n",
      "Epoch 00235: val_loss did not improve from 0.62815\n",
      "36805/36805 [==============================] - 29s 784us/sample - loss: 0.8644 - acc: 0.7357 - val_loss: 0.6386 - val_acc: 0.8276\n",
      "Epoch 236/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8610 - acc: 0.7361\n",
      "Epoch 00236: val_loss did not improve from 0.62815\n",
      "36805/36805 [==============================] - 29s 790us/sample - loss: 0.8614 - acc: 0.7361 - val_loss: 0.7186 - val_acc: 0.7913\n",
      "Epoch 237/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8600 - acc: 0.7360\n",
      "Epoch 00237: val_loss did not improve from 0.62815\n",
      "36805/36805 [==============================] - 29s 787us/sample - loss: 0.8602 - acc: 0.7360 - val_loss: 0.8607 - val_acc: 0.7228\n",
      "Epoch 238/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8640 - acc: 0.7363\n",
      "Epoch 00238: val_loss did not improve from 0.62815\n",
      "36805/36805 [==============================] - 29s 790us/sample - loss: 0.8639 - acc: 0.7363 - val_loss: 0.7278 - val_acc: 0.7866\n",
      "Epoch 239/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8587 - acc: 0.7368\n",
      "Epoch 00239: val_loss did not improve from 0.62815\n",
      "36805/36805 [==============================] - 29s 782us/sample - loss: 0.8592 - acc: 0.7367 - val_loss: 0.6584 - val_acc: 0.8167\n",
      "Epoch 240/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8559 - acc: 0.7391\n",
      "Epoch 00240: val_loss did not improve from 0.62815\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 0.8559 - acc: 0.7390 - val_loss: 0.6963 - val_acc: 0.7990\n",
      "Epoch 241/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8603 - acc: 0.7378\n",
      "Epoch 00241: val_loss did not improve from 0.62815\n",
      "36805/36805 [==============================] - 29s 786us/sample - loss: 0.8603 - acc: 0.7378 - val_loss: 0.6371 - val_acc: 0.8274\n",
      "Epoch 242/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8558 - acc: 0.7357\n",
      "Epoch 00242: val_loss did not improve from 0.62815\n",
      "36805/36805 [==============================] - 29s 784us/sample - loss: 0.8568 - acc: 0.7355 - val_loss: 0.7094 - val_acc: 0.7971\n",
      "Epoch 243/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8569 - acc: 0.7378\n",
      "Epoch 00243: val_loss did not improve from 0.62815\n",
      "36805/36805 [==============================] - 29s 789us/sample - loss: 0.8569 - acc: 0.7378 - val_loss: 0.6826 - val_acc: 0.7987\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 244/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8569 - acc: 0.7391\n",
      "Epoch 00244: val_loss did not improve from 0.62815\n",
      "36805/36805 [==============================] - 29s 781us/sample - loss: 0.8569 - acc: 0.7391 - val_loss: 0.6393 - val_acc: 0.8253\n",
      "Epoch 245/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8588 - acc: 0.7380\n",
      "Epoch 00245: val_loss did not improve from 0.62815\n",
      "36805/36805 [==============================] - 29s 785us/sample - loss: 0.8588 - acc: 0.7380 - val_loss: 0.6536 - val_acc: 0.8081\n",
      "Epoch 246/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8586 - acc: 0.7383\n",
      "Epoch 00246: val_loss did not improve from 0.62815\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 0.8589 - acc: 0.7383 - val_loss: 0.7508 - val_acc: 0.7782\n",
      "Epoch 247/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8634 - acc: 0.7386\n",
      "Epoch 00247: val_loss did not improve from 0.62815\n",
      "36805/36805 [==============================] - 29s 785us/sample - loss: 0.8631 - acc: 0.7387 - val_loss: 0.8235 - val_acc: 0.7312\n",
      "Epoch 248/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8514 - acc: 0.7396\n",
      "Epoch 00248: val_loss did not improve from 0.62815\n",
      "36805/36805 [==============================] - 29s 786us/sample - loss: 0.8513 - acc: 0.7397 - val_loss: 0.6828 - val_acc: 0.7997\n",
      "Epoch 249/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8566 - acc: 0.7390\n",
      "Epoch 00249: val_loss did not improve from 0.62815\n",
      "36805/36805 [==============================] - 29s 786us/sample - loss: 0.8565 - acc: 0.7391 - val_loss: 0.6302 - val_acc: 0.8314\n",
      "Epoch 250/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8545 - acc: 0.7385\n",
      "Epoch 00250: val_loss did not improve from 0.62815\n",
      "36805/36805 [==============================] - 29s 786us/sample - loss: 0.8553 - acc: 0.7383 - val_loss: 0.6666 - val_acc: 0.8137\n",
      "Epoch 251/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8545 - acc: 0.7404\n",
      "Epoch 00251: val_loss did not improve from 0.62815\n",
      "36805/36805 [==============================] - 29s 786us/sample - loss: 0.8544 - acc: 0.7404 - val_loss: 0.6782 - val_acc: 0.8090\n",
      "Epoch 252/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8563 - acc: 0.7395\n",
      "Epoch 00252: val_loss did not improve from 0.62815\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 0.8563 - acc: 0.7394 - val_loss: 0.6458 - val_acc: 0.8234\n",
      "Epoch 253/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8547 - acc: 0.7380\n",
      "Epoch 00253: val_loss did not improve from 0.62815\n",
      "36805/36805 [==============================] - 29s 782us/sample - loss: 0.8547 - acc: 0.7380 - val_loss: 0.6434 - val_acc: 0.8239\n",
      "Epoch 254/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8498 - acc: 0.7405\n",
      "Epoch 00254: val_loss did not improve from 0.62815\n",
      "36805/36805 [==============================] - 29s 787us/sample - loss: 0.8498 - acc: 0.7404 - val_loss: 0.7857 - val_acc: 0.7580\n",
      "Epoch 255/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8577 - acc: 0.7391\n",
      "Epoch 00255: val_loss did not improve from 0.62815\n",
      "36805/36805 [==============================] - 29s 782us/sample - loss: 0.8581 - acc: 0.7390 - val_loss: 0.6892 - val_acc: 0.8034\n",
      "Epoch 256/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8474 - acc: 0.7422\n",
      "Epoch 00256: val_loss did not improve from 0.62815\n",
      "36805/36805 [==============================] - 29s 783us/sample - loss: 0.8473 - acc: 0.7422 - val_loss: 2.9341 - val_acc: 0.4167\n",
      "Epoch 257/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8531 - acc: 0.7426\n",
      "Epoch 00257: val_loss did not improve from 0.62815\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 0.8531 - acc: 0.7425 - val_loss: 0.6375 - val_acc: 0.8274\n",
      "Epoch 258/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8501 - acc: 0.7395\n",
      "Epoch 00258: val_loss did not improve from 0.62815\n",
      "36805/36805 [==============================] - 29s 786us/sample - loss: 0.8500 - acc: 0.7394 - val_loss: 0.6564 - val_acc: 0.8178\n",
      "Epoch 259/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8461 - acc: 0.7422\n",
      "Epoch 00259: val_loss did not improve from 0.62815\n",
      "36805/36805 [==============================] - 29s 786us/sample - loss: 0.8461 - acc: 0.7422 - val_loss: 0.6559 - val_acc: 0.8204\n",
      "Epoch 260/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8439 - acc: 0.7425\n",
      "Epoch 00260: val_loss did not improve from 0.62815\n",
      "36805/36805 [==============================] - 29s 786us/sample - loss: 0.8439 - acc: 0.7425 - val_loss: 0.7358 - val_acc: 0.7813\n",
      "Epoch 261/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8507 - acc: 0.7404\n",
      "Epoch 00261: val_loss did not improve from 0.62815\n",
      "36805/36805 [==============================] - 28s 774us/sample - loss: 0.8508 - acc: 0.7404 - val_loss: 0.7013 - val_acc: 0.8041\n",
      "Epoch 262/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8511 - acc: 0.7398\n",
      "Epoch 00262: val_loss did not improve from 0.62815\n",
      "36805/36805 [==============================] - 29s 787us/sample - loss: 0.8511 - acc: 0.7398 - val_loss: 0.7682 - val_acc: 0.7489\n",
      "Epoch 263/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8489 - acc: 0.7390\n",
      "Epoch 00263: val_loss improved from 0.62815 to 0.61493, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_4_conv_checkpoint/263-0.6149.hdf5\n",
      "36805/36805 [==============================] - 29s 785us/sample - loss: 0.8492 - acc: 0.7388 - val_loss: 0.6149 - val_acc: 0.8265\n",
      "Epoch 264/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8492 - acc: 0.7397\n",
      "Epoch 00264: val_loss improved from 0.61493 to 0.61404, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_4_conv_checkpoint/264-0.6140.hdf5\n",
      "36805/36805 [==============================] - 29s 787us/sample - loss: 0.8491 - acc: 0.7397 - val_loss: 0.6140 - val_acc: 0.8297\n",
      "Epoch 265/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8401 - acc: 0.7438\n",
      "Epoch 00265: val_loss did not improve from 0.61404\n",
      "36805/36805 [==============================] - 29s 789us/sample - loss: 0.8402 - acc: 0.7437 - val_loss: 1.7107 - val_acc: 0.5455\n",
      "Epoch 266/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8409 - acc: 0.7440\n",
      "Epoch 00266: val_loss did not improve from 0.61404\n",
      "36805/36805 [==============================] - 29s 785us/sample - loss: 0.8410 - acc: 0.7440 - val_loss: 0.6485 - val_acc: 0.8199\n",
      "Epoch 267/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8467 - acc: 0.7415\n",
      "Epoch 00267: val_loss did not improve from 0.61404\n",
      "36805/36805 [==============================] - 29s 783us/sample - loss: 0.8467 - acc: 0.7415 - val_loss: 0.6365 - val_acc: 0.8269\n",
      "Epoch 268/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8472 - acc: 0.7434\n",
      "Epoch 00268: val_loss did not improve from 0.61404\n",
      "36805/36805 [==============================] - 29s 786us/sample - loss: 0.8472 - acc: 0.7434 - val_loss: 0.7439 - val_acc: 0.7759\n",
      "Epoch 269/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8412 - acc: 0.7440\n",
      "Epoch 00269: val_loss did not improve from 0.61404\n",
      "36805/36805 [==============================] - 29s 783us/sample - loss: 0.8411 - acc: 0.7440 - val_loss: 0.6318 - val_acc: 0.8309\n",
      "Epoch 270/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8456 - acc: 0.7429\n",
      "Epoch 00270: val_loss did not improve from 0.61404\n",
      "36805/36805 [==============================] - 29s 786us/sample - loss: 0.8456 - acc: 0.7429 - val_loss: 0.6256 - val_acc: 0.8300\n",
      "Epoch 271/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8451 - acc: 0.7418\n",
      "Epoch 00271: val_loss did not improve from 0.61404\n",
      "36805/36805 [==============================] - 29s 784us/sample - loss: 0.8451 - acc: 0.7418 - val_loss: 0.7000 - val_acc: 0.8071\n",
      "Epoch 272/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8478 - acc: 0.7431\n",
      "Epoch 00272: val_loss did not improve from 0.61404\n",
      "36805/36805 [==============================] - 29s 787us/sample - loss: 0.8475 - acc: 0.7432 - val_loss: 0.7153 - val_acc: 0.7878\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 273/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8414 - acc: 0.7432\n",
      "Epoch 00273: val_loss improved from 0.61404 to 0.61402, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_4_conv_checkpoint/273-0.6140.hdf5\n",
      "36805/36805 [==============================] - 29s 787us/sample - loss: 0.8415 - acc: 0.7431 - val_loss: 0.6140 - val_acc: 0.8372\n",
      "Epoch 274/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8432 - acc: 0.7435\n",
      "Epoch 00274: val_loss did not improve from 0.61402\n",
      "36805/36805 [==============================] - 29s 785us/sample - loss: 0.8432 - acc: 0.7435 - val_loss: 1.7909 - val_acc: 0.4985\n",
      "Epoch 275/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8409 - acc: 0.7450\n",
      "Epoch 00275: val_loss did not improve from 0.61402\n",
      "36805/36805 [==============================] - 29s 785us/sample - loss: 0.8411 - acc: 0.7450 - val_loss: 0.6155 - val_acc: 0.8279\n",
      "Epoch 276/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8415 - acc: 0.7421\n",
      "Epoch 00276: val_loss did not improve from 0.61402\n",
      "36805/36805 [==============================] - 29s 784us/sample - loss: 0.8415 - acc: 0.7422 - val_loss: 0.6283 - val_acc: 0.8255\n",
      "Epoch 277/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8348 - acc: 0.7465\n",
      "Epoch 00277: val_loss improved from 0.61402 to 0.60815, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_4_conv_checkpoint/277-0.6082.hdf5\n",
      "36805/36805 [==============================] - 29s 784us/sample - loss: 0.8351 - acc: 0.7464 - val_loss: 0.6082 - val_acc: 0.8339\n",
      "Epoch 278/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8327 - acc: 0.7467\n",
      "Epoch 00278: val_loss did not improve from 0.60815\n",
      "36805/36805 [==============================] - 29s 787us/sample - loss: 0.8324 - acc: 0.7468 - val_loss: 2.7393 - val_acc: 0.4167\n",
      "Epoch 279/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8462 - acc: 0.7423\n",
      "Epoch 00279: val_loss improved from 0.60815 to 0.60338, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_4_conv_checkpoint/279-0.6034.hdf5\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 0.8461 - acc: 0.7425 - val_loss: 0.6034 - val_acc: 0.8369\n",
      "Epoch 280/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8389 - acc: 0.7443\n",
      "Epoch 00280: val_loss did not improve from 0.60338\n",
      "36805/36805 [==============================] - 29s 789us/sample - loss: 0.8388 - acc: 0.7444 - val_loss: 0.9214 - val_acc: 0.7179\n",
      "Epoch 281/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8384 - acc: 0.7462\n",
      "Epoch 00281: val_loss did not improve from 0.60338\n",
      "36805/36805 [==============================] - 29s 786us/sample - loss: 0.8385 - acc: 0.7460 - val_loss: 0.6262 - val_acc: 0.8297\n",
      "Epoch 282/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8351 - acc: 0.7481\n",
      "Epoch 00282: val_loss did not improve from 0.60338\n",
      "36805/36805 [==============================] - 29s 783us/sample - loss: 0.8358 - acc: 0.7478 - val_loss: 0.6466 - val_acc: 0.8225\n",
      "Epoch 283/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8348 - acc: 0.7451\n",
      "Epoch 00283: val_loss did not improve from 0.60338\n",
      "36805/36805 [==============================] - 29s 785us/sample - loss: 0.8350 - acc: 0.7450 - val_loss: 0.6411 - val_acc: 0.8223\n",
      "Epoch 284/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8277 - acc: 0.7469\n",
      "Epoch 00284: val_loss did not improve from 0.60338\n",
      "36805/36805 [==============================] - 29s 784us/sample - loss: 0.8278 - acc: 0.7468 - val_loss: 0.6285 - val_acc: 0.8300\n",
      "Epoch 285/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8321 - acc: 0.7466\n",
      "Epoch 00285: val_loss did not improve from 0.60338\n",
      "36805/36805 [==============================] - 29s 782us/sample - loss: 0.8321 - acc: 0.7466 - val_loss: 0.6263 - val_acc: 0.8269\n",
      "Epoch 286/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8352 - acc: 0.7468\n",
      "Epoch 00286: val_loss did not improve from 0.60338\n",
      "36805/36805 [==============================] - 29s 783us/sample - loss: 0.8351 - acc: 0.7468 - val_loss: 0.6431 - val_acc: 0.8160\n",
      "Epoch 287/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8333 - acc: 0.7473\n",
      "Epoch 00287: val_loss did not improve from 0.60338\n",
      "36805/36805 [==============================] - 29s 786us/sample - loss: 0.8336 - acc: 0.7472 - val_loss: 0.6216 - val_acc: 0.8318\n",
      "Epoch 288/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8356 - acc: 0.7473\n",
      "Epoch 00288: val_loss did not improve from 0.60338\n",
      "36805/36805 [==============================] - 29s 781us/sample - loss: 0.8356 - acc: 0.7473 - val_loss: 0.6374 - val_acc: 0.8251\n",
      "Epoch 289/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8331 - acc: 0.7443\n",
      "Epoch 00289: val_loss did not improve from 0.60338\n",
      "36805/36805 [==============================] - 29s 785us/sample - loss: 0.8331 - acc: 0.7443 - val_loss: 0.6501 - val_acc: 0.8127\n",
      "Epoch 290/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8284 - acc: 0.7440\n",
      "Epoch 00290: val_loss improved from 0.60338 to 0.60234, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_4_conv_checkpoint/290-0.6023.hdf5\n",
      "36805/36805 [==============================] - 29s 783us/sample - loss: 0.8284 - acc: 0.7440 - val_loss: 0.6023 - val_acc: 0.8388\n",
      "Epoch 291/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8321 - acc: 0.7471\n",
      "Epoch 00291: val_loss did not improve from 0.60234\n",
      "36805/36805 [==============================] - 29s 780us/sample - loss: 0.8320 - acc: 0.7472 - val_loss: 1.0866 - val_acc: 0.6688\n",
      "Epoch 292/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8362 - acc: 0.7447\n",
      "Epoch 00292: val_loss did not improve from 0.60234\n",
      "36805/36805 [==============================] - 29s 779us/sample - loss: 0.8363 - acc: 0.7446 - val_loss: 0.6080 - val_acc: 0.8362\n",
      "Epoch 293/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8375 - acc: 0.7455\n",
      "Epoch 00293: val_loss did not improve from 0.60234\n",
      "36805/36805 [==============================] - 29s 782us/sample - loss: 0.8374 - acc: 0.7456 - val_loss: 0.8606 - val_acc: 0.7310\n",
      "Epoch 294/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8280 - acc: 0.7473\n",
      "Epoch 00294: val_loss did not improve from 0.60234\n",
      "36805/36805 [==============================] - 29s 784us/sample - loss: 0.8280 - acc: 0.7473 - val_loss: 0.6384 - val_acc: 0.8174\n",
      "Epoch 295/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8314 - acc: 0.7462\n",
      "Epoch 00295: val_loss did not improve from 0.60234\n",
      "36805/36805 [==============================] - 29s 780us/sample - loss: 0.8314 - acc: 0.7462 - val_loss: 2.1300 - val_acc: 0.4787\n",
      "Epoch 296/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8287 - acc: 0.7465\n",
      "Epoch 00296: val_loss improved from 0.60234 to 0.58994, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_4_conv_checkpoint/296-0.5899.hdf5\n",
      "36805/36805 [==============================] - 29s 782us/sample - loss: 0.8287 - acc: 0.7464 - val_loss: 0.5899 - val_acc: 0.8402\n",
      "Epoch 297/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8302 - acc: 0.7441\n",
      "Epoch 00297: val_loss did not improve from 0.58994\n",
      "36805/36805 [==============================] - 29s 781us/sample - loss: 0.8301 - acc: 0.7442 - val_loss: 0.6257 - val_acc: 0.8253\n",
      "Epoch 298/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8280 - acc: 0.7452\n",
      "Epoch 00298: val_loss did not improve from 0.58994\n",
      "36805/36805 [==============================] - 29s 780us/sample - loss: 0.8280 - acc: 0.7451 - val_loss: 0.6019 - val_acc: 0.8360\n",
      "Epoch 299/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8218 - acc: 0.7476\n",
      "Epoch 00299: val_loss did not improve from 0.58994\n",
      "36805/36805 [==============================] - 29s 781us/sample - loss: 0.8216 - acc: 0.7476 - val_loss: 0.6174 - val_acc: 0.8253\n",
      "Epoch 300/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8312 - acc: 0.7473\n",
      "Epoch 00300: val_loss did not improve from 0.58994\n",
      "36805/36805 [==============================] - 29s 782us/sample - loss: 0.8314 - acc: 0.7472 - val_loss: 0.6199 - val_acc: 0.8281\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 301/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8314 - acc: 0.7459\n",
      "Epoch 00301: val_loss did not improve from 0.58994\n",
      "36805/36805 [==============================] - 29s 778us/sample - loss: 0.8317 - acc: 0.7459 - val_loss: 0.7464 - val_acc: 0.7654\n",
      "Epoch 302/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8311 - acc: 0.7461\n",
      "Epoch 00302: val_loss did not improve from 0.58994\n",
      "36805/36805 [==============================] - 29s 782us/sample - loss: 0.8311 - acc: 0.7461 - val_loss: 0.6781 - val_acc: 0.8132\n",
      "Epoch 303/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8284 - acc: 0.7490\n",
      "Epoch 00303: val_loss did not improve from 0.58994\n",
      "36805/36805 [==============================] - 29s 781us/sample - loss: 0.8283 - acc: 0.7490 - val_loss: 0.6065 - val_acc: 0.8318\n",
      "Epoch 304/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8267 - acc: 0.7494\n",
      "Epoch 00304: val_loss did not improve from 0.58994\n",
      "36805/36805 [==============================] - 29s 781us/sample - loss: 0.8266 - acc: 0.7494 - val_loss: 0.6712 - val_acc: 0.8123\n",
      "Epoch 305/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8262 - acc: 0.7486\n",
      "Epoch 00305: val_loss did not improve from 0.58994\n",
      "36805/36805 [==============================] - 29s 779us/sample - loss: 0.8262 - acc: 0.7486 - val_loss: 0.6097 - val_acc: 0.8358\n",
      "Epoch 306/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8234 - acc: 0.7497\n",
      "Epoch 00306: val_loss did not improve from 0.58994\n",
      "36805/36805 [==============================] - 29s 780us/sample - loss: 0.8231 - acc: 0.7498 - val_loss: 1.0365 - val_acc: 0.6564\n",
      "Epoch 307/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8347 - acc: 0.7440\n",
      "Epoch 00307: val_loss did not improve from 0.58994\n",
      "36805/36805 [==============================] - 29s 780us/sample - loss: 0.8350 - acc: 0.7439 - val_loss: 0.6675 - val_acc: 0.8067\n",
      "Epoch 308/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8199 - acc: 0.7491\n",
      "Epoch 00308: val_loss did not improve from 0.58994\n",
      "36805/36805 [==============================] - 29s 784us/sample - loss: 0.8198 - acc: 0.7491 - val_loss: 0.6614 - val_acc: 0.8078\n",
      "Epoch 309/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8224 - acc: 0.7471\n",
      "Epoch 00309: val_loss did not improve from 0.58994\n",
      "36805/36805 [==============================] - 29s 783us/sample - loss: 0.8224 - acc: 0.7471 - val_loss: 0.5952 - val_acc: 0.8379\n",
      "Epoch 310/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8224 - acc: 0.7490\n",
      "Epoch 00310: val_loss did not improve from 0.58994\n",
      "36805/36805 [==============================] - 29s 781us/sample - loss: 0.8223 - acc: 0.7490 - val_loss: 0.6078 - val_acc: 0.8323\n",
      "Epoch 311/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8206 - acc: 0.7489\n",
      "Epoch 00311: val_loss did not improve from 0.58994\n",
      "36805/36805 [==============================] - 29s 781us/sample - loss: 0.8205 - acc: 0.7489 - val_loss: 0.6044 - val_acc: 0.8353\n",
      "Epoch 312/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8240 - acc: 0.7498\n",
      "Epoch 00312: val_loss did not improve from 0.58994\n",
      "36805/36805 [==============================] - 29s 778us/sample - loss: 0.8237 - acc: 0.7501 - val_loss: 0.7839 - val_acc: 0.7459\n",
      "Epoch 313/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8229 - acc: 0.7492\n",
      "Epoch 00313: val_loss did not improve from 0.58994\n",
      "36805/36805 [==============================] - 29s 779us/sample - loss: 0.8231 - acc: 0.7492 - val_loss: 0.7620 - val_acc: 0.7768\n",
      "Epoch 314/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8215 - acc: 0.7502\n",
      "Epoch 00314: val_loss did not improve from 0.58994\n",
      "36805/36805 [==============================] - 29s 779us/sample - loss: 0.8215 - acc: 0.7502 - val_loss: 0.6530 - val_acc: 0.8127\n",
      "Epoch 315/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8178 - acc: 0.7503\n",
      "Epoch 00315: val_loss did not improve from 0.58994\n",
      "36805/36805 [==============================] - 29s 780us/sample - loss: 0.8175 - acc: 0.7505 - val_loss: 1.6017 - val_acc: 0.5225\n",
      "Epoch 316/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8235 - acc: 0.7474\n",
      "Epoch 00316: val_loss did not improve from 0.58994\n",
      "36805/36805 [==============================] - 29s 784us/sample - loss: 0.8235 - acc: 0.7473 - val_loss: 0.6038 - val_acc: 0.8365\n",
      "Epoch 317/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8189 - acc: 0.7494\n",
      "Epoch 00317: val_loss did not improve from 0.58994\n",
      "36805/36805 [==============================] - 29s 781us/sample - loss: 0.8193 - acc: 0.7494 - val_loss: 0.5920 - val_acc: 0.8367\n",
      "Epoch 318/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8155 - acc: 0.7497\n",
      "Epoch 00318: val_loss did not improve from 0.58994\n",
      "36805/36805 [==============================] - 29s 782us/sample - loss: 0.8155 - acc: 0.7497 - val_loss: 0.6033 - val_acc: 0.8360\n",
      "Epoch 319/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8146 - acc: 0.7517\n",
      "Epoch 00319: val_loss did not improve from 0.58994\n",
      "36805/36805 [==============================] - 29s 781us/sample - loss: 0.8146 - acc: 0.7517 - val_loss: 0.6530 - val_acc: 0.8071\n",
      "Epoch 320/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8249 - acc: 0.7498\n",
      "Epoch 00320: val_loss did not improve from 0.58994\n",
      "36805/36805 [==============================] - 29s 781us/sample - loss: 0.8249 - acc: 0.7498 - val_loss: 0.7831 - val_acc: 0.7473\n",
      "Epoch 321/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8203 - acc: 0.7496\n",
      "Epoch 00321: val_loss did not improve from 0.58994\n",
      "36805/36805 [==============================] - 29s 778us/sample - loss: 0.8204 - acc: 0.7496 - val_loss: 0.6446 - val_acc: 0.8209\n",
      "Epoch 322/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8228 - acc: 0.7496\n",
      "Epoch 00322: val_loss did not improve from 0.58994\n",
      "36805/36805 [==============================] - 29s 783us/sample - loss: 0.8231 - acc: 0.7497 - val_loss: 0.5955 - val_acc: 0.8411\n",
      "Epoch 323/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8122 - acc: 0.7505\n",
      "Epoch 00323: val_loss did not improve from 0.58994\n",
      "36805/36805 [==============================] - 29s 782us/sample - loss: 0.8123 - acc: 0.7506 - val_loss: 0.5975 - val_acc: 0.8383\n",
      "Epoch 324/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8170 - acc: 0.7507\n",
      "Epoch 00324: val_loss did not improve from 0.58994\n",
      "36805/36805 [==============================] - 29s 779us/sample - loss: 0.8178 - acc: 0.7505 - val_loss: 0.6054 - val_acc: 0.8346\n",
      "Epoch 325/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8170 - acc: 0.7518\n",
      "Epoch 00325: val_loss did not improve from 0.58994\n",
      "36805/36805 [==============================] - 29s 781us/sample - loss: 0.8171 - acc: 0.7518 - val_loss: 0.6536 - val_acc: 0.8127\n",
      "Epoch 326/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8169 - acc: 0.7500\n",
      "Epoch 00326: val_loss did not improve from 0.58994\n",
      "36805/36805 [==============================] - 29s 781us/sample - loss: 0.8170 - acc: 0.7500 - val_loss: 0.5991 - val_acc: 0.8346\n",
      "Epoch 327/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8154 - acc: 0.7518\n",
      "Epoch 00327: val_loss did not improve from 0.58994\n",
      "36805/36805 [==============================] - 29s 781us/sample - loss: 0.8155 - acc: 0.7517 - val_loss: 0.6223 - val_acc: 0.8323\n",
      "Epoch 328/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8171 - acc: 0.7530\n",
      "Epoch 00328: val_loss did not improve from 0.58994\n",
      "36805/36805 [==============================] - 29s 780us/sample - loss: 0.8170 - acc: 0.7529 - val_loss: 0.6722 - val_acc: 0.8055\n",
      "Epoch 329/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8136 - acc: 0.7514\n",
      "Epoch 00329: val_loss did not improve from 0.58994\n",
      "36805/36805 [==============================] - 29s 778us/sample - loss: 0.8138 - acc: 0.7514 - val_loss: 0.6629 - val_acc: 0.8116\n",
      "Epoch 330/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8123 - acc: 0.7527\n",
      "Epoch 00330: val_loss did not improve from 0.58994\n",
      "36805/36805 [==============================] - 29s 777us/sample - loss: 0.8121 - acc: 0.7527 - val_loss: 0.6575 - val_acc: 0.8090\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 331/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8178 - acc: 0.7497\n",
      "Epoch 00331: val_loss did not improve from 0.58994\n",
      "36805/36805 [==============================] - 29s 779us/sample - loss: 0.8179 - acc: 0.7497 - val_loss: 0.7786 - val_acc: 0.7608\n",
      "Epoch 332/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8145 - acc: 0.7520\n",
      "Epoch 00332: val_loss did not improve from 0.58994\n",
      "36805/36805 [==============================] - 29s 778us/sample - loss: 0.8145 - acc: 0.7520 - val_loss: 0.6308 - val_acc: 0.8132\n",
      "Epoch 333/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8096 - acc: 0.7518\n",
      "Epoch 00333: val_loss did not improve from 0.58994\n",
      "36805/36805 [==============================] - 29s 779us/sample - loss: 0.8096 - acc: 0.7518 - val_loss: 1.0757 - val_acc: 0.6778\n",
      "Epoch 334/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8085 - acc: 0.7540\n",
      "Epoch 00334: val_loss did not improve from 0.58994\n",
      "36805/36805 [==============================] - 29s 777us/sample - loss: 0.8087 - acc: 0.7539 - val_loss: 0.6176 - val_acc: 0.8281\n",
      "Epoch 335/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8187 - acc: 0.7504\n",
      "Epoch 00335: val_loss did not improve from 0.58994\n",
      "36805/36805 [==============================] - 29s 781us/sample - loss: 0.8186 - acc: 0.7504 - val_loss: 0.7037 - val_acc: 0.7922\n",
      "Epoch 336/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8028 - acc: 0.7545\n",
      "Epoch 00336: val_loss did not improve from 0.58994\n",
      "36805/36805 [==============================] - 28s 771us/sample - loss: 0.8029 - acc: 0.7544 - val_loss: 1.2254 - val_acc: 0.6466\n",
      "Epoch 337/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8138 - acc: 0.7492\n",
      "Epoch 00337: val_loss did not improve from 0.58994\n",
      "36805/36805 [==============================] - 28s 761us/sample - loss: 0.8138 - acc: 0.7492 - val_loss: 0.6186 - val_acc: 0.8274\n",
      "Epoch 338/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8136 - acc: 0.7524\n",
      "Epoch 00338: val_loss did not improve from 0.58994\n",
      "36805/36805 [==============================] - 29s 778us/sample - loss: 0.8136 - acc: 0.7524 - val_loss: 0.5989 - val_acc: 0.8376\n",
      "Epoch 339/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8151 - acc: 0.7520\n",
      "Epoch 00339: val_loss did not improve from 0.58994\n",
      "36805/36805 [==============================] - 29s 779us/sample - loss: 0.8151 - acc: 0.7519 - val_loss: 0.7339 - val_acc: 0.7678\n",
      "Epoch 340/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8073 - acc: 0.7522\n",
      "Epoch 00340: val_loss did not improve from 0.58994\n",
      "36805/36805 [==============================] - 29s 783us/sample - loss: 0.8073 - acc: 0.7522 - val_loss: 0.5968 - val_acc: 0.8351\n",
      "Epoch 341/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8057 - acc: 0.7539\n",
      "Epoch 00341: val_loss did not improve from 0.58994\n",
      "36805/36805 [==============================] - 29s 781us/sample - loss: 0.8063 - acc: 0.7538 - val_loss: 1.1766 - val_acc: 0.6434\n",
      "Epoch 342/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8135 - acc: 0.7523\n",
      "Epoch 00342: val_loss did not improve from 0.58994\n",
      "36805/36805 [==============================] - 29s 779us/sample - loss: 0.8134 - acc: 0.7523 - val_loss: 0.6669 - val_acc: 0.8155\n",
      "Epoch 343/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8057 - acc: 0.7528\n",
      "Epoch 00343: val_loss did not improve from 0.58994\n",
      "36805/36805 [==============================] - 29s 780us/sample - loss: 0.8056 - acc: 0.7528 - val_loss: 2.3431 - val_acc: 0.4677\n",
      "Epoch 344/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8089 - acc: 0.7558\n",
      "Epoch 00344: val_loss did not improve from 0.58994\n",
      "36805/36805 [==============================] - 29s 777us/sample - loss: 0.8089 - acc: 0.7558 - val_loss: 0.5937 - val_acc: 0.8367\n",
      "Epoch 345/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8092 - acc: 0.7528\n",
      "Epoch 00345: val_loss did not improve from 0.58994\n",
      "36805/36805 [==============================] - 29s 780us/sample - loss: 0.8091 - acc: 0.7528 - val_loss: 0.8587 - val_acc: 0.7298\n",
      "Epoch 346/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8096 - acc: 0.7538\n",
      "Epoch 00346: val_loss did not improve from 0.58994\n",
      "36805/36805 [==============================] - 28s 771us/sample - loss: 0.8095 - acc: 0.7538 - val_loss: 0.6040 - val_acc: 0.8360\n",
      "\n",
      "1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_4_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 368us/sample - loss: 0.6497 - acc: 0.8064\n",
      "Loss: 0.649664846313334 Accuracy: 0.8064382\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.5553 - acc: 0.1998\n",
      "Epoch 00001: val_loss improved from inf to 2.26710, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_5_conv_checkpoint/001-2.2671.hdf5\n",
      "36805/36805 [==============================] - 36s 968us/sample - loss: 2.5553 - acc: 0.1998 - val_loss: 2.2671 - val_acc: 0.3722\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.0967 - acc: 0.3319\n",
      "Epoch 00002: val_loss improved from 2.26710 to 1.78809, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_5_conv_checkpoint/002-1.7881.hdf5\n",
      "36805/36805 [==============================] - 30s 821us/sample - loss: 2.0967 - acc: 0.3319 - val_loss: 1.7881 - val_acc: 0.5160\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.8882 - acc: 0.3978\n",
      "Epoch 00003: val_loss improved from 1.78809 to 1.62031, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_5_conv_checkpoint/003-1.6203.hdf5\n",
      "36805/36805 [==============================] - 30s 821us/sample - loss: 1.8882 - acc: 0.3979 - val_loss: 1.6203 - val_acc: 0.5723\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.7412 - acc: 0.4479\n",
      "Epoch 00004: val_loss improved from 1.62031 to 1.47530, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_5_conv_checkpoint/004-1.4753.hdf5\n",
      "36805/36805 [==============================] - 30s 823us/sample - loss: 1.7411 - acc: 0.4479 - val_loss: 1.4753 - val_acc: 0.6219\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6257 - acc: 0.4921\n",
      "Epoch 00005: val_loss improved from 1.47530 to 1.39896, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_5_conv_checkpoint/005-1.3990.hdf5\n",
      "36805/36805 [==============================] - 30s 824us/sample - loss: 1.6258 - acc: 0.4921 - val_loss: 1.3990 - val_acc: 0.6303\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5393 - acc: 0.5201\n",
      "Epoch 00006: val_loss improved from 1.39896 to 1.32189, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_5_conv_checkpoint/006-1.3219.hdf5\n",
      "36805/36805 [==============================] - 30s 823us/sample - loss: 1.5394 - acc: 0.5201 - val_loss: 1.3219 - val_acc: 0.6194\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4650 - acc: 0.5485\n",
      "Epoch 00007: val_loss improved from 1.32189 to 1.23529, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_5_conv_checkpoint/007-1.2353.hdf5\n",
      "36805/36805 [==============================] - 30s 820us/sample - loss: 1.4649 - acc: 0.5485 - val_loss: 1.2353 - val_acc: 0.6699\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4024 - acc: 0.5712\n",
      "Epoch 00008: val_loss improved from 1.23529 to 1.22789, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_5_conv_checkpoint/008-1.2279.hdf5\n",
      "36805/36805 [==============================] - 30s 819us/sample - loss: 1.4025 - acc: 0.5712 - val_loss: 1.2279 - val_acc: 0.6457\n",
      "Epoch 9/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3518 - acc: 0.5883\n",
      "Epoch 00009: val_loss improved from 1.22789 to 1.13215, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_5_conv_checkpoint/009-1.1321.hdf5\n",
      "36805/36805 [==============================] - 30s 823us/sample - loss: 1.3516 - acc: 0.5882 - val_loss: 1.1321 - val_acc: 0.6918\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3063 - acc: 0.6029\n",
      "Epoch 00010: val_loss improved from 1.13215 to 1.09434, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_5_conv_checkpoint/010-1.0943.hdf5\n",
      "36805/36805 [==============================] - 30s 820us/sample - loss: 1.3065 - acc: 0.6028 - val_loss: 1.0943 - val_acc: 0.6844\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2714 - acc: 0.6101\n",
      "Epoch 00011: val_loss improved from 1.09434 to 1.06739, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_5_conv_checkpoint/011-1.0674.hdf5\n",
      "36805/36805 [==============================] - 30s 819us/sample - loss: 1.2714 - acc: 0.6101 - val_loss: 1.0674 - val_acc: 0.6939\n",
      "Epoch 12/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2307 - acc: 0.6274\n",
      "Epoch 00012: val_loss improved from 1.06739 to 0.99893, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_5_conv_checkpoint/012-0.9989.hdf5\n",
      "36805/36805 [==============================] - 30s 824us/sample - loss: 1.2309 - acc: 0.6274 - val_loss: 0.9989 - val_acc: 0.7345\n",
      "Epoch 13/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1946 - acc: 0.6385\n",
      "Epoch 00013: val_loss improved from 0.99893 to 0.99231, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_5_conv_checkpoint/013-0.9923.hdf5\n",
      "36805/36805 [==============================] - 30s 819us/sample - loss: 1.1950 - acc: 0.6385 - val_loss: 0.9923 - val_acc: 0.7244\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1687 - acc: 0.6468\n",
      "Epoch 00014: val_loss did not improve from 0.99231\n",
      "36805/36805 [==============================] - 30s 810us/sample - loss: 1.1688 - acc: 0.6467 - val_loss: 1.0251 - val_acc: 0.7007\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1485 - acc: 0.6515\n",
      "Epoch 00015: val_loss improved from 0.99231 to 0.94071, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_5_conv_checkpoint/015-0.9407.hdf5\n",
      "36805/36805 [==============================] - 30s 822us/sample - loss: 1.1485 - acc: 0.6514 - val_loss: 0.9407 - val_acc: 0.7538\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1213 - acc: 0.6622\n",
      "Epoch 00016: val_loss did not improve from 0.94071\n",
      "36805/36805 [==============================] - 30s 820us/sample - loss: 1.1213 - acc: 0.6622 - val_loss: 0.9836 - val_acc: 0.7109\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1010 - acc: 0.6691\n",
      "Epoch 00017: val_loss improved from 0.94071 to 0.88460, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_5_conv_checkpoint/017-0.8846.hdf5\n",
      "36805/36805 [==============================] - 30s 825us/sample - loss: 1.1011 - acc: 0.6691 - val_loss: 0.8846 - val_acc: 0.7633\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0762 - acc: 0.6780\n",
      "Epoch 00018: val_loss improved from 0.88460 to 0.87515, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_5_conv_checkpoint/018-0.8752.hdf5\n",
      "36805/36805 [==============================] - 30s 822us/sample - loss: 1.0762 - acc: 0.6781 - val_loss: 0.8752 - val_acc: 0.7724\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0601 - acc: 0.6802\n",
      "Epoch 00019: val_loss improved from 0.87515 to 0.84403, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_5_conv_checkpoint/019-0.8440.hdf5\n",
      "36805/36805 [==============================] - 30s 823us/sample - loss: 1.0606 - acc: 0.6801 - val_loss: 0.8440 - val_acc: 0.7696\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0464 - acc: 0.6876\n",
      "Epoch 00020: val_loss improved from 0.84403 to 0.81875, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_5_conv_checkpoint/020-0.8188.hdf5\n",
      "36805/36805 [==============================] - 30s 823us/sample - loss: 1.0465 - acc: 0.6875 - val_loss: 0.8188 - val_acc: 0.7813\n",
      "Epoch 21/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0293 - acc: 0.6933\n",
      "Epoch 00021: val_loss did not improve from 0.81875\n",
      "36805/36805 [==============================] - 30s 817us/sample - loss: 1.0291 - acc: 0.6932 - val_loss: 0.8508 - val_acc: 0.7654\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0104 - acc: 0.6977\n",
      "Epoch 00022: val_loss improved from 0.81875 to 0.80813, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_5_conv_checkpoint/022-0.8081.hdf5\n",
      "36805/36805 [==============================] - 30s 821us/sample - loss: 1.0105 - acc: 0.6977 - val_loss: 0.8081 - val_acc: 0.7831\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9961 - acc: 0.7021\n",
      "Epoch 00023: val_loss improved from 0.80813 to 0.79144, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_5_conv_checkpoint/023-0.7914.hdf5\n",
      "36805/36805 [==============================] - 30s 822us/sample - loss: 0.9963 - acc: 0.7021 - val_loss: 0.7914 - val_acc: 0.7892\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9828 - acc: 0.7073\n",
      "Epoch 00024: val_loss did not improve from 0.79144\n",
      "36805/36805 [==============================] - 30s 821us/sample - loss: 0.9829 - acc: 0.7073 - val_loss: 1.2078 - val_acc: 0.5926\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9664 - acc: 0.7096\n",
      "Epoch 00025: val_loss did not improve from 0.79144\n",
      "36805/36805 [==============================] - 30s 821us/sample - loss: 0.9664 - acc: 0.7097 - val_loss: 0.8301 - val_acc: 0.7591\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9656 - acc: 0.7125\n",
      "Epoch 00026: val_loss improved from 0.79144 to 0.77239, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_5_conv_checkpoint/026-0.7724.hdf5\n",
      "36805/36805 [==============================] - 30s 822us/sample - loss: 0.9657 - acc: 0.7125 - val_loss: 0.7724 - val_acc: 0.8027\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9449 - acc: 0.7181\n",
      "Epoch 00027: val_loss improved from 0.77239 to 0.75565, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_5_conv_checkpoint/027-0.7557.hdf5\n",
      "36805/36805 [==============================] - 30s 820us/sample - loss: 0.9450 - acc: 0.7181 - val_loss: 0.7557 - val_acc: 0.7922\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9344 - acc: 0.7189\n",
      "Epoch 00028: val_loss improved from 0.75565 to 0.73773, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_5_conv_checkpoint/028-0.7377.hdf5\n",
      "36805/36805 [==============================] - 30s 819us/sample - loss: 0.9348 - acc: 0.7189 - val_loss: 0.7377 - val_acc: 0.8069\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9204 - acc: 0.7242\n",
      "Epoch 00029: val_loss did not improve from 0.73773\n",
      "36805/36805 [==============================] - 30s 818us/sample - loss: 0.9206 - acc: 0.7241 - val_loss: 0.7428 - val_acc: 0.8095\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9137 - acc: 0.7294\n",
      "Epoch 00030: val_loss improved from 0.73773 to 0.72230, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_5_conv_checkpoint/030-0.7223.hdf5\n",
      "36805/36805 [==============================] - 30s 821us/sample - loss: 0.9138 - acc: 0.7294 - val_loss: 0.7223 - val_acc: 0.8176\n",
      "Epoch 31/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9042 - acc: 0.7324\n",
      "Epoch 00031: val_loss improved from 0.72230 to 0.72096, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_5_conv_checkpoint/031-0.7210.hdf5\n",
      "36805/36805 [==============================] - 30s 821us/sample - loss: 0.9045 - acc: 0.7323 - val_loss: 0.7210 - val_acc: 0.8185\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9000 - acc: 0.7296\n",
      "Epoch 00032: val_loss improved from 0.72096 to 0.69565, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_5_conv_checkpoint/032-0.6957.hdf5\n",
      "36805/36805 [==============================] - 30s 822us/sample - loss: 0.9000 - acc: 0.7296 - val_loss: 0.6957 - val_acc: 0.8251\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8864 - acc: 0.7404\n",
      "Epoch 00033: val_loss did not improve from 0.69565\n",
      "36805/36805 [==============================] - 30s 814us/sample - loss: 0.8865 - acc: 0.7404 - val_loss: 0.7101 - val_acc: 0.7964\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8811 - acc: 0.7396\n",
      "Epoch 00034: val_loss improved from 0.69565 to 0.69297, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_5_conv_checkpoint/034-0.6930.hdf5\n",
      "36805/36805 [==============================] - 30s 821us/sample - loss: 0.8811 - acc: 0.7395 - val_loss: 0.6930 - val_acc: 0.8174\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8737 - acc: 0.7400\n",
      "Epoch 00035: val_loss did not improve from 0.69297\n",
      "36805/36805 [==============================] - 30s 825us/sample - loss: 0.8737 - acc: 0.7400 - val_loss: 0.7077 - val_acc: 0.8090\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8664 - acc: 0.7455\n",
      "Epoch 00036: val_loss did not improve from 0.69297\n",
      "36805/36805 [==============================] - 30s 816us/sample - loss: 0.8665 - acc: 0.7455 - val_loss: 0.7104 - val_acc: 0.7985\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8594 - acc: 0.7464\n",
      "Epoch 00037: val_loss did not improve from 0.69297\n",
      "36805/36805 [==============================] - 30s 817us/sample - loss: 0.8594 - acc: 0.7464 - val_loss: 0.8095 - val_acc: 0.7543\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8583 - acc: 0.7475\n",
      "Epoch 00038: val_loss improved from 0.69297 to 0.67952, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_5_conv_checkpoint/038-0.6795.hdf5\n",
      "36805/36805 [==============================] - 30s 821us/sample - loss: 0.8585 - acc: 0.7475 - val_loss: 0.6795 - val_acc: 0.8267\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8416 - acc: 0.7523\n",
      "Epoch 00039: val_loss improved from 0.67952 to 0.65777, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_5_conv_checkpoint/039-0.6578.hdf5\n",
      "36805/36805 [==============================] - 30s 823us/sample - loss: 0.8419 - acc: 0.7523 - val_loss: 0.6578 - val_acc: 0.8272\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8377 - acc: 0.7549\n",
      "Epoch 00040: val_loss improved from 0.65777 to 0.64556, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_5_conv_checkpoint/040-0.6456.hdf5\n",
      "36805/36805 [==============================] - 30s 822us/sample - loss: 0.8378 - acc: 0.7548 - val_loss: 0.6456 - val_acc: 0.8325\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8363 - acc: 0.7519\n",
      "Epoch 00041: val_loss did not improve from 0.64556\n",
      "36805/36805 [==============================] - 30s 819us/sample - loss: 0.8364 - acc: 0.7519 - val_loss: 0.6882 - val_acc: 0.8153\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8225 - acc: 0.7581\n",
      "Epoch 00042: val_loss did not improve from 0.64556\n",
      "36805/36805 [==============================] - 30s 821us/sample - loss: 0.8229 - acc: 0.7580 - val_loss: 0.6694 - val_acc: 0.8153\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8262 - acc: 0.7582\n",
      "Epoch 00043: val_loss did not improve from 0.64556\n",
      "36805/36805 [==============================] - 30s 820us/sample - loss: 0.8264 - acc: 0.7581 - val_loss: 0.6671 - val_acc: 0.8146\n",
      "Epoch 44/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8139 - acc: 0.7589\n",
      "Epoch 00044: val_loss did not improve from 0.64556\n",
      "36805/36805 [==============================] - 30s 820us/sample - loss: 0.8141 - acc: 0.7589 - val_loss: 0.7258 - val_acc: 0.7927\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8027 - acc: 0.7638\n",
      "Epoch 00045: val_loss improved from 0.64556 to 0.61506, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_5_conv_checkpoint/045-0.6151.hdf5\n",
      "36805/36805 [==============================] - 30s 820us/sample - loss: 0.8028 - acc: 0.7638 - val_loss: 0.6151 - val_acc: 0.8479\n",
      "Epoch 46/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8008 - acc: 0.7660\n",
      "Epoch 00046: val_loss did not improve from 0.61506\n",
      "36805/36805 [==============================] - 30s 818us/sample - loss: 0.8011 - acc: 0.7658 - val_loss: 0.6216 - val_acc: 0.8407\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7941 - acc: 0.7691\n",
      "Epoch 00047: val_loss did not improve from 0.61506\n",
      "36805/36805 [==============================] - 30s 821us/sample - loss: 0.7941 - acc: 0.7691 - val_loss: 0.7236 - val_acc: 0.7864\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7897 - acc: 0.7682\n",
      "Epoch 00048: val_loss did not improve from 0.61506\n",
      "36805/36805 [==============================] - 30s 817us/sample - loss: 0.7898 - acc: 0.7682 - val_loss: 0.6623 - val_acc: 0.8258\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7876 - acc: 0.7682\n",
      "Epoch 00049: val_loss improved from 0.61506 to 0.59049, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_5_conv_checkpoint/049-0.5905.hdf5\n",
      "36805/36805 [==============================] - 30s 822us/sample - loss: 0.7876 - acc: 0.7683 - val_loss: 0.5905 - val_acc: 0.8498\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7793 - acc: 0.7693\n",
      "Epoch 00050: val_loss did not improve from 0.59049\n",
      "36805/36805 [==============================] - 30s 819us/sample - loss: 0.7793 - acc: 0.7693 - val_loss: 0.6171 - val_acc: 0.8330\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7731 - acc: 0.7732\n",
      "Epoch 00051: val_loss did not improve from 0.59049\n",
      "36805/36805 [==============================] - 30s 820us/sample - loss: 0.7734 - acc: 0.7731 - val_loss: 0.9929 - val_acc: 0.6918\n",
      "Epoch 52/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7769 - acc: 0.7701\n",
      "Epoch 00052: val_loss did not improve from 0.59049\n",
      "36805/36805 [==============================] - 30s 821us/sample - loss: 0.7772 - acc: 0.7701 - val_loss: 0.6017 - val_acc: 0.8411\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7682 - acc: 0.7757\n",
      "Epoch 00053: val_loss did not improve from 0.59049\n",
      "36805/36805 [==============================] - 30s 823us/sample - loss: 0.7681 - acc: 0.7757 - val_loss: 0.6247 - val_acc: 0.8425\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7592 - acc: 0.7770\n",
      "Epoch 00054: val_loss did not improve from 0.59049\n",
      "36805/36805 [==============================] - 30s 818us/sample - loss: 0.7592 - acc: 0.7770 - val_loss: 0.6293 - val_acc: 0.8393\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7543 - acc: 0.7799\n",
      "Epoch 00055: val_loss improved from 0.59049 to 0.58727, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_5_conv_checkpoint/055-0.5873.hdf5\n",
      "36805/36805 [==============================] - 30s 823us/sample - loss: 0.7543 - acc: 0.7799 - val_loss: 0.5873 - val_acc: 0.8500\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7501 - acc: 0.7792\n",
      "Epoch 00056: val_loss did not improve from 0.58727\n",
      "36805/36805 [==============================] - 30s 821us/sample - loss: 0.7504 - acc: 0.7791 - val_loss: 0.8979 - val_acc: 0.7128\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7518 - acc: 0.7792\n",
      "Epoch 00057: val_loss improved from 0.58727 to 0.57714, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_5_conv_checkpoint/057-0.5771.hdf5\n",
      "36805/36805 [==============================] - 30s 824us/sample - loss: 0.7520 - acc: 0.7792 - val_loss: 0.5771 - val_acc: 0.8486\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7492 - acc: 0.7769\n",
      "Epoch 00058: val_loss improved from 0.57714 to 0.56303, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_5_conv_checkpoint/058-0.5630.hdf5\n",
      "36805/36805 [==============================] - 30s 819us/sample - loss: 0.7493 - acc: 0.7769 - val_loss: 0.5630 - val_acc: 0.8542\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7478 - acc: 0.7828\n",
      "Epoch 00059: val_loss did not improve from 0.56303\n",
      "36805/36805 [==============================] - 30s 819us/sample - loss: 0.7477 - acc: 0.7828 - val_loss: 0.5761 - val_acc: 0.8553\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7365 - acc: 0.7864\n",
      "Epoch 00060: val_loss did not improve from 0.56303\n",
      "36805/36805 [==============================] - 30s 820us/sample - loss: 0.7365 - acc: 0.7864 - val_loss: 0.6129 - val_acc: 0.8307\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7320 - acc: 0.7869\n",
      "Epoch 00061: val_loss did not improve from 0.56303\n",
      "36805/36805 [==============================] - 30s 822us/sample - loss: 0.7320 - acc: 0.7869 - val_loss: 0.6223 - val_acc: 0.8218\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7327 - acc: 0.7862\n",
      "Epoch 00062: val_loss improved from 0.56303 to 0.55642, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_5_conv_checkpoint/062-0.5564.hdf5\n",
      "36805/36805 [==============================] - 30s 821us/sample - loss: 0.7326 - acc: 0.7862 - val_loss: 0.5564 - val_acc: 0.8609\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7242 - acc: 0.7880\n",
      "Epoch 00063: val_loss did not improve from 0.55642\n",
      "36805/36805 [==============================] - 30s 819us/sample - loss: 0.7243 - acc: 0.7880 - val_loss: 0.5692 - val_acc: 0.8551\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7227 - acc: 0.7900\n",
      "Epoch 00064: val_loss did not improve from 0.55642\n",
      "36805/36805 [==============================] - 30s 820us/sample - loss: 0.7226 - acc: 0.7900 - val_loss: 0.6230 - val_acc: 0.8246\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7230 - acc: 0.7870\n",
      "Epoch 00065: val_loss did not improve from 0.55642\n",
      "36805/36805 [==============================] - 30s 820us/sample - loss: 0.7229 - acc: 0.7870 - val_loss: 0.6355 - val_acc: 0.8169\n",
      "Epoch 66/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7168 - acc: 0.7896\n",
      "Epoch 00066: val_loss did not improve from 0.55642\n",
      "36805/36805 [==============================] - 30s 821us/sample - loss: 0.7169 - acc: 0.7895 - val_loss: 0.6166 - val_acc: 0.8286\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7137 - acc: 0.7917\n",
      "Epoch 00067: val_loss improved from 0.55642 to 0.54981, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_5_conv_checkpoint/067-0.5498.hdf5\n",
      "36805/36805 [==============================] - 30s 824us/sample - loss: 0.7137 - acc: 0.7917 - val_loss: 0.5498 - val_acc: 0.8609\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7042 - acc: 0.7952\n",
      "Epoch 00068: val_loss did not improve from 0.54981\n",
      "36805/36805 [==============================] - 30s 819us/sample - loss: 0.7046 - acc: 0.7951 - val_loss: 0.5499 - val_acc: 0.8572\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7086 - acc: 0.7924\n",
      "Epoch 00069: val_loss did not improve from 0.54981\n",
      "36805/36805 [==============================] - 30s 823us/sample - loss: 0.7086 - acc: 0.7924 - val_loss: 0.5928 - val_acc: 0.8369\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6947 - acc: 0.7962\n",
      "Epoch 00070: val_loss improved from 0.54981 to 0.54809, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_5_conv_checkpoint/070-0.5481.hdf5\n",
      "36805/36805 [==============================] - 30s 823us/sample - loss: 0.6947 - acc: 0.7962 - val_loss: 0.5481 - val_acc: 0.8560\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6949 - acc: 0.7985\n",
      "Epoch 00071: val_loss did not improve from 0.54809\n",
      "36805/36805 [==============================] - 30s 821us/sample - loss: 0.6948 - acc: 0.7986 - val_loss: 0.5700 - val_acc: 0.8481\n",
      "Epoch 72/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6939 - acc: 0.7960\n",
      "Epoch 00072: val_loss improved from 0.54809 to 0.53337, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_5_conv_checkpoint/072-0.5334.hdf5\n",
      "36805/36805 [==============================] - 30s 818us/sample - loss: 0.6942 - acc: 0.7960 - val_loss: 0.5334 - val_acc: 0.8602\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6895 - acc: 0.7992\n",
      "Epoch 00073: val_loss did not improve from 0.53337\n",
      "36805/36805 [==============================] - 29s 799us/sample - loss: 0.6896 - acc: 0.7992 - val_loss: 0.6631 - val_acc: 0.7978\n",
      "Epoch 74/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6857 - acc: 0.7998\n",
      "Epoch 00074: val_loss improved from 0.53337 to 0.52518, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_5_conv_checkpoint/074-0.5252.hdf5\n",
      "36805/36805 [==============================] - 30s 815us/sample - loss: 0.6856 - acc: 0.7998 - val_loss: 0.5252 - val_acc: 0.8661\n",
      "Epoch 75/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6857 - acc: 0.7982\n",
      "Epoch 00075: val_loss did not improve from 0.52518\n",
      "36805/36805 [==============================] - 30s 806us/sample - loss: 0.6855 - acc: 0.7983 - val_loss: 0.5690 - val_acc: 0.8493\n",
      "Epoch 76/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6853 - acc: 0.7992\n",
      "Epoch 00076: val_loss improved from 0.52518 to 0.51055, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_5_conv_checkpoint/076-0.5105.hdf5\n",
      "36805/36805 [==============================] - 30s 819us/sample - loss: 0.6850 - acc: 0.7993 - val_loss: 0.5105 - val_acc: 0.8717\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6834 - acc: 0.8010\n",
      "Epoch 00077: val_loss did not improve from 0.51055\n",
      "36805/36805 [==============================] - 30s 816us/sample - loss: 0.6835 - acc: 0.8010 - val_loss: 0.5902 - val_acc: 0.8346\n",
      "Epoch 78/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6741 - acc: 0.8050\n",
      "Epoch 00078: val_loss did not improve from 0.51055\n",
      "36805/36805 [==============================] - 30s 818us/sample - loss: 0.6745 - acc: 0.8050 - val_loss: 0.5445 - val_acc: 0.8542\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6697 - acc: 0.8033\n",
      "Epoch 00079: val_loss did not improve from 0.51055\n",
      "36805/36805 [==============================] - 30s 819us/sample - loss: 0.6697 - acc: 0.8033 - val_loss: 0.5260 - val_acc: 0.8619\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6686 - acc: 0.8059\n",
      "Epoch 00080: val_loss did not improve from 0.51055\n",
      "36805/36805 [==============================] - 30s 816us/sample - loss: 0.6686 - acc: 0.8059 - val_loss: 0.7658 - val_acc: 0.7498\n",
      "Epoch 81/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6670 - acc: 0.8036\n",
      "Epoch 00081: val_loss did not improve from 0.51055\n",
      "36805/36805 [==============================] - 30s 818us/sample - loss: 0.6670 - acc: 0.8036 - val_loss: 0.5257 - val_acc: 0.8577\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6595 - acc: 0.8065\n",
      "Epoch 00082: val_loss did not improve from 0.51055\n",
      "36805/36805 [==============================] - 30s 817us/sample - loss: 0.6596 - acc: 0.8065 - val_loss: 0.5453 - val_acc: 0.8512\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6620 - acc: 0.8059\n",
      "Epoch 00083: val_loss did not improve from 0.51055\n",
      "36805/36805 [==============================] - 30s 820us/sample - loss: 0.6620 - acc: 0.8059 - val_loss: 0.5932 - val_acc: 0.8260\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6548 - acc: 0.8094\n",
      "Epoch 00084: val_loss improved from 0.51055 to 0.49890, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_5_conv_checkpoint/084-0.4989.hdf5\n",
      "36805/36805 [==============================] - 30s 820us/sample - loss: 0.6547 - acc: 0.8094 - val_loss: 0.4989 - val_acc: 0.8761\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6556 - acc: 0.8100\n",
      "Epoch 00085: val_loss did not improve from 0.49890\n",
      "36805/36805 [==============================] - 30s 818us/sample - loss: 0.6556 - acc: 0.8101 - val_loss: 0.5033 - val_acc: 0.8749\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6503 - acc: 0.8117\n",
      "Epoch 00086: val_loss did not improve from 0.49890\n",
      "36805/36805 [==============================] - 30s 821us/sample - loss: 0.6503 - acc: 0.8118 - val_loss: 0.5007 - val_acc: 0.8689\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6473 - acc: 0.8123\n",
      "Epoch 00087: val_loss improved from 0.49890 to 0.49723, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_5_conv_checkpoint/087-0.4972.hdf5\n",
      "36805/36805 [==============================] - 30s 821us/sample - loss: 0.6473 - acc: 0.8123 - val_loss: 0.4972 - val_acc: 0.8744\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6436 - acc: 0.8089\n",
      "Epoch 00088: val_loss did not improve from 0.49723\n",
      "36805/36805 [==============================] - 30s 820us/sample - loss: 0.6436 - acc: 0.8089 - val_loss: 0.8983 - val_acc: 0.7205\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6438 - acc: 0.8125\n",
      "Epoch 00089: val_loss did not improve from 0.49723\n",
      "36805/36805 [==============================] - 30s 814us/sample - loss: 0.6439 - acc: 0.8125 - val_loss: 0.5443 - val_acc: 0.8514\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6474 - acc: 0.8101\n",
      "Epoch 00090: val_loss did not improve from 0.49723\n",
      "36805/36805 [==============================] - 30s 818us/sample - loss: 0.6475 - acc: 0.8100 - val_loss: 0.8256 - val_acc: 0.7414\n",
      "Epoch 91/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6366 - acc: 0.8123\n",
      "Epoch 00091: val_loss did not improve from 0.49723\n",
      "36805/36805 [==============================] - 30s 815us/sample - loss: 0.6368 - acc: 0.8124 - val_loss: 0.5100 - val_acc: 0.8644\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6348 - acc: 0.8155\n",
      "Epoch 00092: val_loss did not improve from 0.49723\n",
      "36805/36805 [==============================] - 30s 816us/sample - loss: 0.6348 - acc: 0.8155 - val_loss: 0.5240 - val_acc: 0.8621\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6345 - acc: 0.8153\n",
      "Epoch 00093: val_loss did not improve from 0.49723\n",
      "36805/36805 [==============================] - 30s 820us/sample - loss: 0.6349 - acc: 0.8152 - val_loss: 0.7085 - val_acc: 0.7759\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6299 - acc: 0.8149\n",
      "Epoch 00094: val_loss did not improve from 0.49723\n",
      "36805/36805 [==============================] - 30s 818us/sample - loss: 0.6300 - acc: 0.8149 - val_loss: 0.5189 - val_acc: 0.8549\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6283 - acc: 0.8166\n",
      "Epoch 00095: val_loss improved from 0.49723 to 0.49630, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_5_conv_checkpoint/095-0.4963.hdf5\n",
      "36805/36805 [==============================] - 30s 819us/sample - loss: 0.6284 - acc: 0.8166 - val_loss: 0.4963 - val_acc: 0.8703\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6189 - acc: 0.8185\n",
      "Epoch 00096: val_loss improved from 0.49630 to 0.46808, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_5_conv_checkpoint/096-0.4681.hdf5\n",
      "36805/36805 [==============================] - 30s 823us/sample - loss: 0.6189 - acc: 0.8185 - val_loss: 0.4681 - val_acc: 0.8824\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6253 - acc: 0.8180\n",
      "Epoch 00097: val_loss did not improve from 0.46808\n",
      "36805/36805 [==============================] - 30s 818us/sample - loss: 0.6254 - acc: 0.8180 - val_loss: 0.5857 - val_acc: 0.8246\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6220 - acc: 0.8200\n",
      "Epoch 00098: val_loss did not improve from 0.46808\n",
      "36805/36805 [==============================] - 30s 819us/sample - loss: 0.6220 - acc: 0.8199 - val_loss: 0.5804 - val_acc: 0.8314\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6165 - acc: 0.8210\n",
      "Epoch 00099: val_loss did not improve from 0.46808\n",
      "36805/36805 [==============================] - 30s 819us/sample - loss: 0.6164 - acc: 0.8210 - val_loss: 0.6878 - val_acc: 0.7892\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6182 - acc: 0.8204\n",
      "Epoch 00100: val_loss did not improve from 0.46808\n",
      "36805/36805 [==============================] - 30s 824us/sample - loss: 0.6186 - acc: 0.8203 - val_loss: 0.4760 - val_acc: 0.8763\n",
      "Epoch 101/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6155 - acc: 0.8218\n",
      "Epoch 00101: val_loss did not improve from 0.46808\n",
      "36805/36805 [==============================] - 30s 818us/sample - loss: 0.6155 - acc: 0.8218 - val_loss: 0.6577 - val_acc: 0.8025\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6115 - acc: 0.8227\n",
      "Epoch 00102: val_loss did not improve from 0.46808\n",
      "36805/36805 [==============================] - 30s 818us/sample - loss: 0.6114 - acc: 0.8227 - val_loss: 0.5800 - val_acc: 0.8197\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6130 - acc: 0.8234\n",
      "Epoch 00103: val_loss did not improve from 0.46808\n",
      "36805/36805 [==============================] - 30s 816us/sample - loss: 0.6133 - acc: 0.8233 - val_loss: 0.4795 - val_acc: 0.8733\n",
      "Epoch 104/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6126 - acc: 0.8216\n",
      "Epoch 00104: val_loss did not improve from 0.46808\n",
      "36805/36805 [==============================] - 30s 816us/sample - loss: 0.6126 - acc: 0.8216 - val_loss: 0.4779 - val_acc: 0.8772\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6050 - acc: 0.8230\n",
      "Epoch 00105: val_loss did not improve from 0.46808\n",
      "36805/36805 [==============================] - 30s 823us/sample - loss: 0.6053 - acc: 0.8229 - val_loss: 0.7719 - val_acc: 0.7668\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5992 - acc: 0.8249\n",
      "Epoch 00106: val_loss improved from 0.46808 to 0.44776, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_5_conv_checkpoint/106-0.4478.hdf5\n",
      "36805/36805 [==============================] - 30s 821us/sample - loss: 0.5993 - acc: 0.8248 - val_loss: 0.4478 - val_acc: 0.8810\n",
      "Epoch 107/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6036 - acc: 0.8235\n",
      "Epoch 00107: val_loss did not improve from 0.44776\n",
      "36805/36805 [==============================] - 30s 817us/sample - loss: 0.6034 - acc: 0.8237 - val_loss: 0.5196 - val_acc: 0.8556\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5985 - acc: 0.8246\n",
      "Epoch 00108: val_loss did not improve from 0.44776\n",
      "36805/36805 [==============================] - 30s 819us/sample - loss: 0.5984 - acc: 0.8246 - val_loss: 0.4560 - val_acc: 0.8828\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5970 - acc: 0.8252\n",
      "Epoch 00109: val_loss improved from 0.44776 to 0.44665, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_5_conv_checkpoint/109-0.4466.hdf5\n",
      "36805/36805 [==============================] - 30s 816us/sample - loss: 0.5970 - acc: 0.8252 - val_loss: 0.4466 - val_acc: 0.8887\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5965 - acc: 0.8250\n",
      "Epoch 00110: val_loss did not improve from 0.44665\n",
      "36805/36805 [==============================] - 30s 820us/sample - loss: 0.5964 - acc: 0.8250 - val_loss: 0.4791 - val_acc: 0.8721\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5963 - acc: 0.8254\n",
      "Epoch 00111: val_loss did not improve from 0.44665\n",
      "36805/36805 [==============================] - 30s 820us/sample - loss: 0.5962 - acc: 0.8254 - val_loss: 0.4673 - val_acc: 0.8786\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5928 - acc: 0.8271\n",
      "Epoch 00112: val_loss did not improve from 0.44665\n",
      "36805/36805 [==============================] - 30s 821us/sample - loss: 0.5927 - acc: 0.8271 - val_loss: 0.4533 - val_acc: 0.8849\n",
      "Epoch 113/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5906 - acc: 0.8294\n",
      "Epoch 00113: val_loss did not improve from 0.44665\n",
      "36805/36805 [==============================] - 30s 814us/sample - loss: 0.5910 - acc: 0.8293 - val_loss: 0.4501 - val_acc: 0.8814\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5876 - acc: 0.8300\n",
      "Epoch 00114: val_loss did not improve from 0.44665\n",
      "36805/36805 [==============================] - 30s 820us/sample - loss: 0.5879 - acc: 0.8299 - val_loss: 0.4811 - val_acc: 0.8721\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5840 - acc: 0.8314\n",
      "Epoch 00115: val_loss did not improve from 0.44665\n",
      "36805/36805 [==============================] - 30s 817us/sample - loss: 0.5841 - acc: 0.8313 - val_loss: 0.4582 - val_acc: 0.8847\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5767 - acc: 0.8318\n",
      "Epoch 00116: val_loss did not improve from 0.44665\n",
      "36805/36805 [==============================] - 30s 815us/sample - loss: 0.5770 - acc: 0.8317 - val_loss: 0.9820 - val_acc: 0.7032\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5843 - acc: 0.8315\n",
      "Epoch 00117: val_loss did not improve from 0.44665\n",
      "36805/36805 [==============================] - 30s 816us/sample - loss: 0.5843 - acc: 0.8315 - val_loss: 0.4926 - val_acc: 0.8656\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5812 - acc: 0.8311\n",
      "Epoch 00118: val_loss did not improve from 0.44665\n",
      "36805/36805 [==============================] - 30s 819us/sample - loss: 0.5812 - acc: 0.8311 - val_loss: 0.4516 - val_acc: 0.8835\n",
      "Epoch 119/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5807 - acc: 0.8313\n",
      "Epoch 00119: val_loss improved from 0.44665 to 0.44329, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_5_conv_checkpoint/119-0.4433.hdf5\n",
      "36805/36805 [==============================] - 30s 820us/sample - loss: 0.5810 - acc: 0.8313 - val_loss: 0.4433 - val_acc: 0.8849\n",
      "Epoch 120/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5730 - acc: 0.8309\n",
      "Epoch 00120: val_loss did not improve from 0.44329\n",
      "36805/36805 [==============================] - 30s 816us/sample - loss: 0.5734 - acc: 0.8309 - val_loss: 0.6113 - val_acc: 0.8143\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5731 - acc: 0.8333\n",
      "Epoch 00121: val_loss did not improve from 0.44329\n",
      "36805/36805 [==============================] - 30s 819us/sample - loss: 0.5731 - acc: 0.8333 - val_loss: 0.4599 - val_acc: 0.8751\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5676 - acc: 0.8327\n",
      "Epoch 00122: val_loss improved from 0.44329 to 0.42650, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_5_conv_checkpoint/122-0.4265.hdf5\n",
      "36805/36805 [==============================] - 30s 819us/sample - loss: 0.5675 - acc: 0.8328 - val_loss: 0.4265 - val_acc: 0.8908\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5686 - acc: 0.8336\n",
      "Epoch 00123: val_loss did not improve from 0.42650\n",
      "36805/36805 [==============================] - 30s 816us/sample - loss: 0.5690 - acc: 0.8335 - val_loss: 0.7317 - val_acc: 0.7738\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5666 - acc: 0.8355\n",
      "Epoch 00124: val_loss did not improve from 0.42650\n",
      "36805/36805 [==============================] - 30s 817us/sample - loss: 0.5666 - acc: 0.8355 - val_loss: 0.4734 - val_acc: 0.8698\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5652 - acc: 0.8354\n",
      "Epoch 00125: val_loss did not improve from 0.42650\n",
      "36805/36805 [==============================] - 30s 816us/sample - loss: 0.5652 - acc: 0.8354 - val_loss: 0.4319 - val_acc: 0.8875\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5610 - acc: 0.8377\n",
      "Epoch 00126: val_loss improved from 0.42650 to 0.42504, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_5_conv_checkpoint/126-0.4250.hdf5\n",
      "36805/36805 [==============================] - 30s 823us/sample - loss: 0.5612 - acc: 0.8377 - val_loss: 0.4250 - val_acc: 0.8938\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5634 - acc: 0.8380\n",
      "Epoch 00127: val_loss did not improve from 0.42504\n",
      "36805/36805 [==============================] - 30s 820us/sample - loss: 0.5635 - acc: 0.8380 - val_loss: 0.4933 - val_acc: 0.8612\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5553 - acc: 0.8374\n",
      "Epoch 00128: val_loss improved from 0.42504 to 0.41114, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_5_conv_checkpoint/128-0.4111.hdf5\n",
      "36805/36805 [==============================] - 30s 823us/sample - loss: 0.5554 - acc: 0.8374 - val_loss: 0.4111 - val_acc: 0.8945\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5568 - acc: 0.8385\n",
      "Epoch 00129: val_loss did not improve from 0.41114\n",
      "36805/36805 [==============================] - 30s 816us/sample - loss: 0.5573 - acc: 0.8385 - val_loss: 0.5020 - val_acc: 0.8586\n",
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5565 - acc: 0.8393\n",
      "Epoch 00130: val_loss did not improve from 0.41114\n",
      "36805/36805 [==============================] - 30s 819us/sample - loss: 0.5565 - acc: 0.8393 - val_loss: 0.5518 - val_acc: 0.8362\n",
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5483 - acc: 0.8405\n",
      "Epoch 00131: val_loss did not improve from 0.41114\n",
      "36805/36805 [==============================] - 30s 820us/sample - loss: 0.5482 - acc: 0.8405 - val_loss: 0.4911 - val_acc: 0.8535\n",
      "Epoch 132/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5483 - acc: 0.8385\n",
      "Epoch 00132: val_loss did not improve from 0.41114\n",
      "36805/36805 [==============================] - 30s 816us/sample - loss: 0.5482 - acc: 0.8385 - val_loss: 0.4266 - val_acc: 0.8891\n",
      "Epoch 133/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5452 - acc: 0.8406\n",
      "Epoch 00133: val_loss did not improve from 0.41114\n",
      "36805/36805 [==============================] - 30s 819us/sample - loss: 0.5454 - acc: 0.8405 - val_loss: 0.4121 - val_acc: 0.8926\n",
      "Epoch 134/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5534 - acc: 0.8402\n",
      "Epoch 00134: val_loss improved from 0.41114 to 0.40363, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_5_conv_checkpoint/134-0.4036.hdf5\n",
      "36805/36805 [==============================] - 30s 819us/sample - loss: 0.5534 - acc: 0.8401 - val_loss: 0.4036 - val_acc: 0.9005\n",
      "Epoch 135/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5511 - acc: 0.8401\n",
      "Epoch 00135: val_loss did not improve from 0.40363\n",
      "36805/36805 [==============================] - 30s 804us/sample - loss: 0.5511 - acc: 0.8400 - val_loss: 0.4299 - val_acc: 0.8852\n",
      "Epoch 136/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5459 - acc: 0.8431\n",
      "Epoch 00136: val_loss did not improve from 0.40363\n",
      "36805/36805 [==============================] - 30s 821us/sample - loss: 0.5461 - acc: 0.8431 - val_loss: 0.4377 - val_acc: 0.8838\n",
      "Epoch 137/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5497 - acc: 0.8392\n",
      "Epoch 00137: val_loss did not improve from 0.40363\n",
      "36805/36805 [==============================] - 30s 818us/sample - loss: 0.5496 - acc: 0.8393 - val_loss: 0.4460 - val_acc: 0.8751\n",
      "Epoch 138/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5396 - acc: 0.8422\n",
      "Epoch 00138: val_loss did not improve from 0.40363\n",
      "36805/36805 [==============================] - 30s 820us/sample - loss: 0.5396 - acc: 0.8422 - val_loss: 0.4278 - val_acc: 0.8882\n",
      "Epoch 139/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5378 - acc: 0.8439\n",
      "Epoch 00139: val_loss did not improve from 0.40363\n",
      "36805/36805 [==============================] - 30s 817us/sample - loss: 0.5378 - acc: 0.8439 - val_loss: 0.4195 - val_acc: 0.8901\n",
      "Epoch 140/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5376 - acc: 0.8423\n",
      "Epoch 00140: val_loss did not improve from 0.40363\n",
      "36805/36805 [==============================] - 30s 824us/sample - loss: 0.5376 - acc: 0.8424 - val_loss: 0.4070 - val_acc: 0.8945\n",
      "Epoch 141/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5344 - acc: 0.8437\n",
      "Epoch 00141: val_loss did not improve from 0.40363\n",
      "36805/36805 [==============================] - 30s 818us/sample - loss: 0.5343 - acc: 0.8437 - val_loss: 0.5500 - val_acc: 0.8390\n",
      "Epoch 142/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5360 - acc: 0.8442\n",
      "Epoch 00142: val_loss did not improve from 0.40363\n",
      "36805/36805 [==============================] - 30s 818us/sample - loss: 0.5361 - acc: 0.8442 - val_loss: 0.4726 - val_acc: 0.8721\n",
      "Epoch 143/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5322 - acc: 0.8431\n",
      "Epoch 00143: val_loss did not improve from 0.40363\n",
      "36805/36805 [==============================] - 30s 819us/sample - loss: 0.5322 - acc: 0.8431 - val_loss: 0.4139 - val_acc: 0.8898\n",
      "Epoch 144/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5303 - acc: 0.8449\n",
      "Epoch 00144: val_loss did not improve from 0.40363\n",
      "36805/36805 [==============================] - 30s 823us/sample - loss: 0.5304 - acc: 0.8448 - val_loss: 0.4193 - val_acc: 0.8863\n",
      "Epoch 145/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5301 - acc: 0.8465\n",
      "Epoch 00145: val_loss did not improve from 0.40363\n",
      "36805/36805 [==============================] - 30s 813us/sample - loss: 0.5300 - acc: 0.8467 - val_loss: 0.4249 - val_acc: 0.8875\n",
      "Epoch 146/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5311 - acc: 0.8449\n",
      "Epoch 00146: val_loss did not improve from 0.40363\n",
      "36805/36805 [==============================] - 30s 804us/sample - loss: 0.5312 - acc: 0.8449 - val_loss: 0.4226 - val_acc: 0.8840\n",
      "Epoch 147/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5316 - acc: 0.8427\n",
      "Epoch 00147: val_loss improved from 0.40363 to 0.39160, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_5_conv_checkpoint/147-0.3916.hdf5\n",
      "36805/36805 [==============================] - 30s 807us/sample - loss: 0.5317 - acc: 0.8427 - val_loss: 0.3916 - val_acc: 0.8966\n",
      "Epoch 148/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5235 - acc: 0.8486\n",
      "Epoch 00148: val_loss did not improve from 0.39160\n",
      "36805/36805 [==============================] - 30s 821us/sample - loss: 0.5235 - acc: 0.8486 - val_loss: 0.5448 - val_acc: 0.8334\n",
      "Epoch 149/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5294 - acc: 0.8462\n",
      "Epoch 00149: val_loss did not improve from 0.39160\n",
      "36805/36805 [==============================] - 30s 822us/sample - loss: 0.5295 - acc: 0.8462 - val_loss: 0.4542 - val_acc: 0.8730\n",
      "Epoch 150/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5232 - acc: 0.8464\n",
      "Epoch 00150: val_loss did not improve from 0.39160\n",
      "36805/36805 [==============================] - 30s 817us/sample - loss: 0.5230 - acc: 0.8464 - val_loss: 0.4238 - val_acc: 0.8854\n",
      "Epoch 151/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5270 - acc: 0.8468\n",
      "Epoch 00151: val_loss did not improve from 0.39160\n",
      "36805/36805 [==============================] - 30s 820us/sample - loss: 0.5270 - acc: 0.8468 - val_loss: 0.4185 - val_acc: 0.8919\n",
      "Epoch 152/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5186 - acc: 0.8474\n",
      "Epoch 00152: val_loss did not improve from 0.39160\n",
      "36805/36805 [==============================] - 30s 820us/sample - loss: 0.5189 - acc: 0.8474 - val_loss: 0.4330 - val_acc: 0.8849\n",
      "Epoch 153/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5218 - acc: 0.8479\n",
      "Epoch 00153: val_loss did not improve from 0.39160\n",
      "36805/36805 [==============================] - 30s 820us/sample - loss: 0.5219 - acc: 0.8479 - val_loss: 0.4068 - val_acc: 0.8947\n",
      "Epoch 154/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5177 - acc: 0.8493\n",
      "Epoch 00154: val_loss did not improve from 0.39160\n",
      "36805/36805 [==============================] - 30s 820us/sample - loss: 0.5177 - acc: 0.8494 - val_loss: 0.4021 - val_acc: 0.8908\n",
      "Epoch 155/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5209 - acc: 0.8500\n",
      "Epoch 00155: val_loss did not improve from 0.39160\n",
      "36805/36805 [==============================] - 30s 819us/sample - loss: 0.5215 - acc: 0.8499 - val_loss: 0.5258 - val_acc: 0.8435\n",
      "Epoch 156/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5256 - acc: 0.8458\n",
      "Epoch 00156: val_loss did not improve from 0.39160\n",
      "36805/36805 [==============================] - 30s 819us/sample - loss: 0.5255 - acc: 0.8458 - val_loss: 0.3953 - val_acc: 0.8959\n",
      "Epoch 157/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5134 - acc: 0.8500\n",
      "Epoch 00157: val_loss did not improve from 0.39160\n",
      "36805/36805 [==============================] - 30s 820us/sample - loss: 0.5135 - acc: 0.8500 - val_loss: 0.4881 - val_acc: 0.8605\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 158/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5131 - acc: 0.8487\n",
      "Epoch 00158: val_loss did not improve from 0.39160\n",
      "36805/36805 [==============================] - 30s 816us/sample - loss: 0.5132 - acc: 0.8487 - val_loss: 0.3979 - val_acc: 0.8912\n",
      "Epoch 159/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5137 - acc: 0.8509\n",
      "Epoch 00159: val_loss did not improve from 0.39160\n",
      "36805/36805 [==============================] - 29s 799us/sample - loss: 0.5139 - acc: 0.8509 - val_loss: 0.4084 - val_acc: 0.8870\n",
      "Epoch 160/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5142 - acc: 0.8497\n",
      "Epoch 00160: val_loss improved from 0.39160 to 0.37735, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_5_conv_checkpoint/160-0.3773.hdf5\n",
      "36805/36805 [==============================] - 30s 821us/sample - loss: 0.5144 - acc: 0.8497 - val_loss: 0.3773 - val_acc: 0.8998\n",
      "Epoch 161/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5117 - acc: 0.8509\n",
      "Epoch 00161: val_loss did not improve from 0.37735\n",
      "36805/36805 [==============================] - 30s 821us/sample - loss: 0.5122 - acc: 0.8508 - val_loss: 0.3842 - val_acc: 0.9024\n",
      "Epoch 162/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5077 - acc: 0.8524\n",
      "Epoch 00162: val_loss did not improve from 0.37735\n",
      "36805/36805 [==============================] - 30s 804us/sample - loss: 0.5077 - acc: 0.8524 - val_loss: 0.4081 - val_acc: 0.8894\n",
      "Epoch 163/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5073 - acc: 0.8518\n",
      "Epoch 00163: val_loss did not improve from 0.37735\n",
      "36805/36805 [==============================] - 29s 799us/sample - loss: 0.5072 - acc: 0.8518 - val_loss: 0.3877 - val_acc: 0.8996\n",
      "Epoch 164/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5052 - acc: 0.8517\n",
      "Epoch 00164: val_loss improved from 0.37735 to 0.37731, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_5_conv_checkpoint/164-0.3773.hdf5\n",
      "36805/36805 [==============================] - 29s 801us/sample - loss: 0.5052 - acc: 0.8517 - val_loss: 0.3773 - val_acc: 0.9012\n",
      "Epoch 165/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5096 - acc: 0.8500\n",
      "Epoch 00165: val_loss did not improve from 0.37731\n",
      "36805/36805 [==============================] - 29s 800us/sample - loss: 0.5100 - acc: 0.8499 - val_loss: 0.3894 - val_acc: 0.8982\n",
      "Epoch 166/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5033 - acc: 0.8535\n",
      "Epoch 00166: val_loss did not improve from 0.37731\n",
      "36805/36805 [==============================] - 29s 796us/sample - loss: 0.5034 - acc: 0.8535 - val_loss: 0.3952 - val_acc: 0.8980\n",
      "Epoch 167/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5014 - acc: 0.8530\n",
      "Epoch 00167: val_loss did not improve from 0.37731\n",
      "36805/36805 [==============================] - 29s 799us/sample - loss: 0.5014 - acc: 0.8530 - val_loss: 0.4362 - val_acc: 0.8791\n",
      "Epoch 168/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4982 - acc: 0.8544\n",
      "Epoch 00168: val_loss did not improve from 0.37731\n",
      "36805/36805 [==============================] - 29s 800us/sample - loss: 0.4981 - acc: 0.8544 - val_loss: 0.3929 - val_acc: 0.8961\n",
      "Epoch 169/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5022 - acc: 0.8544\n",
      "Epoch 00169: val_loss did not improve from 0.37731\n",
      "36805/36805 [==============================] - 29s 793us/sample - loss: 0.5022 - acc: 0.8544 - val_loss: 0.3850 - val_acc: 0.9017\n",
      "Epoch 170/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5030 - acc: 0.8518\n",
      "Epoch 00170: val_loss did not improve from 0.37731\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 0.5030 - acc: 0.8519 - val_loss: 0.3856 - val_acc: 0.9003\n",
      "Epoch 171/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4925 - acc: 0.8559\n",
      "Epoch 00171: val_loss did not improve from 0.37731\n",
      "36805/36805 [==============================] - 29s 797us/sample - loss: 0.4925 - acc: 0.8559 - val_loss: 0.3828 - val_acc: 0.8956\n",
      "Epoch 172/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4934 - acc: 0.8551\n",
      "Epoch 00172: val_loss did not improve from 0.37731\n",
      "36805/36805 [==============================] - 29s 794us/sample - loss: 0.4933 - acc: 0.8550 - val_loss: 0.4266 - val_acc: 0.8868\n",
      "Epoch 173/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4917 - acc: 0.8568\n",
      "Epoch 00173: val_loss did not improve from 0.37731\n",
      "36805/36805 [==============================] - 29s 798us/sample - loss: 0.4918 - acc: 0.8568 - val_loss: 0.4216 - val_acc: 0.8866\n",
      "Epoch 174/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4896 - acc: 0.8541\n",
      "Epoch 00174: val_loss improved from 0.37731 to 0.37639, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_5_conv_checkpoint/174-0.3764.hdf5\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 0.4896 - acc: 0.8541 - val_loss: 0.3764 - val_acc: 0.9003\n",
      "Epoch 175/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4937 - acc: 0.8554\n",
      "Epoch 00175: val_loss did not improve from 0.37639\n",
      "36805/36805 [==============================] - 29s 786us/sample - loss: 0.4936 - acc: 0.8554 - val_loss: 0.3897 - val_acc: 0.8945\n",
      "Epoch 176/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4822 - acc: 0.8587\n",
      "Epoch 00176: val_loss did not improve from 0.37639\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 0.4822 - acc: 0.8587 - val_loss: 0.3850 - val_acc: 0.8977\n",
      "Epoch 177/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4899 - acc: 0.8559\n",
      "Epoch 00177: val_loss did not improve from 0.37639\n",
      "36805/36805 [==============================] - 29s 796us/sample - loss: 0.4903 - acc: 0.8558 - val_loss: 0.5728 - val_acc: 0.8334\n",
      "Epoch 178/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4882 - acc: 0.8547\n",
      "Epoch 00178: val_loss did not improve from 0.37639\n",
      "36805/36805 [==============================] - 29s 794us/sample - loss: 0.4889 - acc: 0.8546 - val_loss: 0.9467 - val_acc: 0.7184\n",
      "Epoch 179/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4881 - acc: 0.8570\n",
      "Epoch 00179: val_loss did not improve from 0.37639\n",
      "36805/36805 [==============================] - 29s 792us/sample - loss: 0.4877 - acc: 0.8571 - val_loss: 0.7383 - val_acc: 0.7824\n",
      "Epoch 180/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4841 - acc: 0.8588\n",
      "Epoch 00180: val_loss did not improve from 0.37639\n",
      "36805/36805 [==============================] - 30s 804us/sample - loss: 0.4840 - acc: 0.8588 - val_loss: 0.3895 - val_acc: 0.8940\n",
      "Epoch 181/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4808 - acc: 0.8602\n",
      "Epoch 00181: val_loss did not improve from 0.37639\n",
      "36805/36805 [==============================] - 30s 827us/sample - loss: 0.4813 - acc: 0.8601 - val_loss: 0.3954 - val_acc: 0.8963\n",
      "Epoch 182/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4792 - acc: 0.8586\n",
      "Epoch 00182: val_loss improved from 0.37639 to 0.36486, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_5_conv_checkpoint/182-0.3649.hdf5\n",
      "36805/36805 [==============================] - 31s 831us/sample - loss: 0.4793 - acc: 0.8586 - val_loss: 0.3649 - val_acc: 0.9040\n",
      "Epoch 183/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4832 - acc: 0.8588\n",
      "Epoch 00183: val_loss did not improve from 0.36486\n",
      "36805/36805 [==============================] - 31s 829us/sample - loss: 0.4832 - acc: 0.8588 - val_loss: 0.4750 - val_acc: 0.8675\n",
      "Epoch 184/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4785 - acc: 0.8588\n",
      "Epoch 00184: val_loss did not improve from 0.36486\n",
      "36805/36805 [==============================] - 31s 830us/sample - loss: 0.4784 - acc: 0.8589 - val_loss: 0.5453 - val_acc: 0.8328\n",
      "Epoch 185/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4823 - acc: 0.8587\n",
      "Epoch 00185: val_loss did not improve from 0.36486\n",
      "36805/36805 [==============================] - 30s 826us/sample - loss: 0.4824 - acc: 0.8587 - val_loss: 0.6696 - val_acc: 0.7922\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 186/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4808 - acc: 0.8604\n",
      "Epoch 00186: val_loss did not improve from 0.36486\n",
      "36805/36805 [==============================] - 30s 826us/sample - loss: 0.4808 - acc: 0.8604 - val_loss: 0.3981 - val_acc: 0.8912\n",
      "Epoch 187/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4826 - acc: 0.8593\n",
      "Epoch 00187: val_loss did not improve from 0.36486\n",
      "36805/36805 [==============================] - 31s 829us/sample - loss: 0.4826 - acc: 0.8593 - val_loss: 0.4613 - val_acc: 0.8675\n",
      "Epoch 188/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4749 - acc: 0.8600\n",
      "Epoch 00188: val_loss did not improve from 0.36486\n",
      "36805/36805 [==============================] - 31s 830us/sample - loss: 0.4748 - acc: 0.8602 - val_loss: 0.3948 - val_acc: 0.8921\n",
      "Epoch 189/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4829 - acc: 0.8599\n",
      "Epoch 00189: val_loss improved from 0.36486 to 0.35835, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_5_conv_checkpoint/189-0.3583.hdf5\n",
      "36805/36805 [==============================] - 30s 817us/sample - loss: 0.4828 - acc: 0.8600 - val_loss: 0.3583 - val_acc: 0.9045\n",
      "Epoch 190/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4692 - acc: 0.8624\n",
      "Epoch 00190: val_loss did not improve from 0.35835\n",
      "36805/36805 [==============================] - 30s 827us/sample - loss: 0.4692 - acc: 0.8624 - val_loss: 0.3747 - val_acc: 0.9008\n",
      "Epoch 191/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4766 - acc: 0.8598\n",
      "Epoch 00191: val_loss did not improve from 0.35835\n",
      "36805/36805 [==============================] - 30s 827us/sample - loss: 0.4767 - acc: 0.8597 - val_loss: 0.3683 - val_acc: 0.9040\n",
      "Epoch 192/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4732 - acc: 0.8607\n",
      "Epoch 00192: val_loss did not improve from 0.35835\n",
      "36805/36805 [==============================] - 30s 825us/sample - loss: 0.4732 - acc: 0.8607 - val_loss: 0.3790 - val_acc: 0.8921\n",
      "Epoch 193/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4697 - acc: 0.8609\n",
      "Epoch 00193: val_loss did not improve from 0.35835\n",
      "36805/36805 [==============================] - 30s 825us/sample - loss: 0.4698 - acc: 0.8608 - val_loss: 0.3769 - val_acc: 0.8991\n",
      "Epoch 194/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4753 - acc: 0.8603\n",
      "Epoch 00194: val_loss did not improve from 0.35835\n",
      "36805/36805 [==============================] - 30s 826us/sample - loss: 0.4753 - acc: 0.8603 - val_loss: 0.3722 - val_acc: 0.9040\n",
      "Epoch 195/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4701 - acc: 0.8620\n",
      "Epoch 00195: val_loss did not improve from 0.35835\n",
      "36805/36805 [==============================] - 31s 833us/sample - loss: 0.4703 - acc: 0.8620 - val_loss: 0.3666 - val_acc: 0.9015\n",
      "Epoch 196/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4655 - acc: 0.8634\n",
      "Epoch 00196: val_loss did not improve from 0.35835\n",
      "36805/36805 [==============================] - 30s 828us/sample - loss: 0.4657 - acc: 0.8633 - val_loss: 0.5919 - val_acc: 0.8232\n",
      "Epoch 197/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4709 - acc: 0.8639\n",
      "Epoch 00197: val_loss did not improve from 0.35835\n",
      "36805/36805 [==============================] - 30s 825us/sample - loss: 0.4714 - acc: 0.8636 - val_loss: 0.3778 - val_acc: 0.9043\n",
      "Epoch 198/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4729 - acc: 0.8620\n",
      "Epoch 00198: val_loss did not improve from 0.35835\n",
      "36805/36805 [==============================] - 31s 829us/sample - loss: 0.4729 - acc: 0.8620 - val_loss: 0.3923 - val_acc: 0.8924\n",
      "Epoch 199/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4653 - acc: 0.8626\n",
      "Epoch 00199: val_loss did not improve from 0.35835\n",
      "36805/36805 [==============================] - 30s 810us/sample - loss: 0.4652 - acc: 0.8626 - val_loss: 0.4096 - val_acc: 0.8849\n",
      "Epoch 200/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4619 - acc: 0.8635\n",
      "Epoch 00200: val_loss did not improve from 0.35835\n",
      "36805/36805 [==============================] - 30s 803us/sample - loss: 0.4622 - acc: 0.8635 - val_loss: 0.3601 - val_acc: 0.9038\n",
      "Epoch 201/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4620 - acc: 0.8651\n",
      "Epoch 00201: val_loss improved from 0.35835 to 0.35800, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_5_conv_checkpoint/201-0.3580.hdf5\n",
      "36805/36805 [==============================] - 29s 801us/sample - loss: 0.4622 - acc: 0.8650 - val_loss: 0.3580 - val_acc: 0.9078\n",
      "Epoch 202/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4674 - acc: 0.8609\n",
      "Epoch 00202: val_loss did not improve from 0.35800\n",
      "36805/36805 [==============================] - 30s 828us/sample - loss: 0.4674 - acc: 0.8609 - val_loss: 0.3739 - val_acc: 0.8977\n",
      "Epoch 203/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4617 - acc: 0.8640\n",
      "Epoch 00203: val_loss did not improve from 0.35800\n",
      "36805/36805 [==============================] - 30s 821us/sample - loss: 0.4620 - acc: 0.8640 - val_loss: 0.3708 - val_acc: 0.9061\n",
      "Epoch 204/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4655 - acc: 0.8611\n",
      "Epoch 00204: val_loss did not improve from 0.35800\n",
      "36805/36805 [==============================] - 30s 824us/sample - loss: 0.4656 - acc: 0.8611 - val_loss: 0.3728 - val_acc: 0.9001\n",
      "Epoch 205/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4593 - acc: 0.8641\n",
      "Epoch 00205: val_loss did not improve from 0.35800\n",
      "36805/36805 [==============================] - 30s 827us/sample - loss: 0.4594 - acc: 0.8640 - val_loss: 0.3628 - val_acc: 0.9026\n",
      "Epoch 206/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4636 - acc: 0.8653\n",
      "Epoch 00206: val_loss improved from 0.35800 to 0.34946, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_5_conv_checkpoint/206-0.3495.hdf5\n",
      "36805/36805 [==============================] - 30s 827us/sample - loss: 0.4641 - acc: 0.8652 - val_loss: 0.3495 - val_acc: 0.9057\n",
      "Epoch 207/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4577 - acc: 0.8652\n",
      "Epoch 00207: val_loss did not improve from 0.34946\n",
      "36805/36805 [==============================] - 30s 825us/sample - loss: 0.4579 - acc: 0.8651 - val_loss: 0.4303 - val_acc: 0.8789\n",
      "Epoch 208/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4516 - acc: 0.8666\n",
      "Epoch 00208: val_loss did not improve from 0.34946\n",
      "36805/36805 [==============================] - 30s 827us/sample - loss: 0.4515 - acc: 0.8666 - val_loss: 0.3723 - val_acc: 0.9001\n",
      "Epoch 209/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4577 - acc: 0.8668\n",
      "Epoch 00209: val_loss did not improve from 0.34946\n",
      "36805/36805 [==============================] - 30s 827us/sample - loss: 0.4578 - acc: 0.8668 - val_loss: 0.4063 - val_acc: 0.8896\n",
      "Epoch 210/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4529 - acc: 0.8641\n",
      "Epoch 00210: val_loss did not improve from 0.34946\n",
      "36805/36805 [==============================] - 30s 827us/sample - loss: 0.4528 - acc: 0.8641 - val_loss: 0.3986 - val_acc: 0.8945\n",
      "Epoch 211/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4550 - acc: 0.8659\n",
      "Epoch 00211: val_loss did not improve from 0.34946\n",
      "36805/36805 [==============================] - 30s 828us/sample - loss: 0.4561 - acc: 0.8658 - val_loss: 0.3944 - val_acc: 0.8942\n",
      "Epoch 212/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4576 - acc: 0.8653\n",
      "Epoch 00212: val_loss did not improve from 0.34946\n",
      "36805/36805 [==============================] - 30s 824us/sample - loss: 0.4579 - acc: 0.8653 - val_loss: 0.3606 - val_acc: 0.9050\n",
      "Epoch 213/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4509 - acc: 0.8669\n",
      "Epoch 00213: val_loss did not improve from 0.34946\n",
      "36805/36805 [==============================] - 30s 825us/sample - loss: 0.4509 - acc: 0.8670 - val_loss: 0.3654 - val_acc: 0.8987\n",
      "Epoch 214/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4512 - acc: 0.8678\n",
      "Epoch 00214: val_loss did not improve from 0.34946\n",
      "36805/36805 [==============================] - 30s 825us/sample - loss: 0.4511 - acc: 0.8678 - val_loss: 0.4163 - val_acc: 0.8824\n",
      "Epoch 215/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4522 - acc: 0.8670\n",
      "Epoch 00215: val_loss did not improve from 0.34946\n",
      "36805/36805 [==============================] - 30s 813us/sample - loss: 0.4522 - acc: 0.8670 - val_loss: 0.3610 - val_acc: 0.9005\n",
      "Epoch 216/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4527 - acc: 0.8669\n",
      "Epoch 00216: val_loss did not improve from 0.34946\n",
      "36805/36805 [==============================] - 30s 812us/sample - loss: 0.4527 - acc: 0.8669 - val_loss: 0.3587 - val_acc: 0.9089\n",
      "Epoch 217/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4462 - acc: 0.8681\n",
      "Epoch 00217: val_loss did not improve from 0.34946\n",
      "36805/36805 [==============================] - 30s 814us/sample - loss: 0.4462 - acc: 0.8681 - val_loss: 0.3612 - val_acc: 0.9024\n",
      "Epoch 218/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4463 - acc: 0.8694\n",
      "Epoch 00218: val_loss improved from 0.34946 to 0.34895, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_5_conv_checkpoint/218-0.3490.hdf5\n",
      "36805/36805 [==============================] - 30s 820us/sample - loss: 0.4464 - acc: 0.8694 - val_loss: 0.3490 - val_acc: 0.9092\n",
      "Epoch 219/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4453 - acc: 0.8724\n",
      "Epoch 00219: val_loss did not improve from 0.34895\n",
      "36805/36805 [==============================] - 30s 828us/sample - loss: 0.4453 - acc: 0.8724 - val_loss: 0.3625 - val_acc: 0.9008\n",
      "Epoch 220/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4404 - acc: 0.8703\n",
      "Epoch 00220: val_loss did not improve from 0.34895\n",
      "36805/36805 [==============================] - 31s 829us/sample - loss: 0.4404 - acc: 0.8703 - val_loss: 0.4129 - val_acc: 0.8898\n",
      "Epoch 221/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4488 - acc: 0.8669\n",
      "Epoch 00221: val_loss did not improve from 0.34895\n",
      "36805/36805 [==============================] - 31s 829us/sample - loss: 0.4487 - acc: 0.8669 - val_loss: 0.3982 - val_acc: 0.8891\n",
      "Epoch 222/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4408 - acc: 0.8698\n",
      "Epoch 00222: val_loss did not improve from 0.34895\n",
      "36805/36805 [==============================] - 31s 829us/sample - loss: 0.4408 - acc: 0.8699 - val_loss: 0.4114 - val_acc: 0.8866\n",
      "Epoch 223/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4448 - acc: 0.8677\n",
      "Epoch 00223: val_loss did not improve from 0.34895\n",
      "36805/36805 [==============================] - 30s 827us/sample - loss: 0.4447 - acc: 0.8677 - val_loss: 0.3603 - val_acc: 0.9064\n",
      "Epoch 224/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4404 - acc: 0.8697\n",
      "Epoch 00224: val_loss did not improve from 0.34895\n",
      "36805/36805 [==============================] - 30s 823us/sample - loss: 0.4404 - acc: 0.8697 - val_loss: 0.5133 - val_acc: 0.8505\n",
      "Epoch 225/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4399 - acc: 0.8696\n",
      "Epoch 00225: val_loss did not improve from 0.34895\n",
      "36805/36805 [==============================] - 30s 825us/sample - loss: 0.4400 - acc: 0.8696 - val_loss: 0.3515 - val_acc: 0.9108\n",
      "Epoch 226/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4430 - acc: 0.8685\n",
      "Epoch 00226: val_loss did not improve from 0.34895\n",
      "36805/36805 [==============================] - 30s 825us/sample - loss: 0.4430 - acc: 0.8685 - val_loss: 0.3715 - val_acc: 0.9008\n",
      "Epoch 227/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4341 - acc: 0.8717\n",
      "Epoch 00227: val_loss did not improve from 0.34895\n",
      "36805/36805 [==============================] - 30s 825us/sample - loss: 0.4342 - acc: 0.8717 - val_loss: 0.6116 - val_acc: 0.8137\n",
      "Epoch 228/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4400 - acc: 0.8707\n",
      "Epoch 00228: val_loss did not improve from 0.34895\n",
      "36805/36805 [==============================] - 30s 812us/sample - loss: 0.4400 - acc: 0.8707 - val_loss: 0.3749 - val_acc: 0.8959\n",
      "Epoch 229/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4360 - acc: 0.8715\n",
      "Epoch 00229: val_loss did not improve from 0.34895\n",
      "36805/36805 [==============================] - 30s 813us/sample - loss: 0.4361 - acc: 0.8715 - val_loss: 0.3948 - val_acc: 0.8942\n",
      "Epoch 230/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4391 - acc: 0.8720\n",
      "Epoch 00230: val_loss did not improve from 0.34895\n",
      "36805/36805 [==============================] - 30s 819us/sample - loss: 0.4391 - acc: 0.8720 - val_loss: 0.3641 - val_acc: 0.9022\n",
      "Epoch 231/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4371 - acc: 0.8717\n",
      "Epoch 00231: val_loss did not improve from 0.34895\n",
      "36805/36805 [==============================] - 30s 816us/sample - loss: 0.4372 - acc: 0.8716 - val_loss: 0.3554 - val_acc: 0.9043\n",
      "Epoch 232/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4362 - acc: 0.8702\n",
      "Epoch 00232: val_loss did not improve from 0.34895\n",
      "36805/36805 [==============================] - 30s 814us/sample - loss: 0.4362 - acc: 0.8702 - val_loss: 0.3623 - val_acc: 0.9045\n",
      "Epoch 233/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4344 - acc: 0.8711\n",
      "Epoch 00233: val_loss did not improve from 0.34895\n",
      "36805/36805 [==============================] - 30s 815us/sample - loss: 0.4344 - acc: 0.8711 - val_loss: 0.3563 - val_acc: 0.9057\n",
      "Epoch 234/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4309 - acc: 0.8726\n",
      "Epoch 00234: val_loss did not improve from 0.34895\n",
      "36805/36805 [==============================] - 30s 813us/sample - loss: 0.4310 - acc: 0.8726 - val_loss: 0.3572 - val_acc: 0.9033\n",
      "Epoch 235/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4323 - acc: 0.8725\n",
      "Epoch 00235: val_loss did not improve from 0.34895\n",
      "36805/36805 [==============================] - 30s 813us/sample - loss: 0.4323 - acc: 0.8725 - val_loss: 0.3607 - val_acc: 0.9026\n",
      "Epoch 236/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4352 - acc: 0.8726\n",
      "Epoch 00236: val_loss improved from 0.34895 to 0.34406, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_5_conv_checkpoint/236-0.3441.hdf5\n",
      "36805/36805 [==============================] - 30s 813us/sample - loss: 0.4352 - acc: 0.8726 - val_loss: 0.3441 - val_acc: 0.9101\n",
      "Epoch 237/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4274 - acc: 0.8753\n",
      "Epoch 00237: val_loss did not improve from 0.34406\n",
      "36805/36805 [==============================] - 30s 827us/sample - loss: 0.4274 - acc: 0.8753 - val_loss: 0.3654 - val_acc: 0.9012\n",
      "Epoch 238/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4294 - acc: 0.8731\n",
      "Epoch 00238: val_loss did not improve from 0.34406\n",
      "36805/36805 [==============================] - 30s 826us/sample - loss: 0.4296 - acc: 0.8730 - val_loss: 0.4092 - val_acc: 0.8933\n",
      "Epoch 239/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4326 - acc: 0.8719\n",
      "Epoch 00239: val_loss did not improve from 0.34406\n",
      "36805/36805 [==============================] - 30s 826us/sample - loss: 0.4326 - acc: 0.8719 - val_loss: 0.3582 - val_acc: 0.9022\n",
      "Epoch 240/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4330 - acc: 0.8707\n",
      "Epoch 00240: val_loss did not improve from 0.34406\n",
      "36805/36805 [==============================] - 30s 825us/sample - loss: 0.4331 - acc: 0.8707 - val_loss: 0.3955 - val_acc: 0.8845\n",
      "Epoch 241/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4306 - acc: 0.8752\n",
      "Epoch 00241: val_loss did not improve from 0.34406\n",
      "36805/36805 [==============================] - 30s 826us/sample - loss: 0.4306 - acc: 0.8752 - val_loss: 0.5151 - val_acc: 0.8500\n",
      "Epoch 242/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4306 - acc: 0.8715\n",
      "Epoch 00242: val_loss did not improve from 0.34406\n",
      "36805/36805 [==============================] - 30s 825us/sample - loss: 0.4307 - acc: 0.8715 - val_loss: 0.3585 - val_acc: 0.9089\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 243/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4272 - acc: 0.8751\n",
      "Epoch 00243: val_loss did not improve from 0.34406\n",
      "36805/36805 [==============================] - 30s 829us/sample - loss: 0.4274 - acc: 0.8750 - val_loss: 0.5371 - val_acc: 0.8444\n",
      "Epoch 244/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4216 - acc: 0.8739\n",
      "Epoch 00244: val_loss did not improve from 0.34406\n",
      "36805/36805 [==============================] - 30s 825us/sample - loss: 0.4217 - acc: 0.8738 - val_loss: 0.3696 - val_acc: 0.9012\n",
      "Epoch 245/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4211 - acc: 0.8760\n",
      "Epoch 00245: val_loss did not improve from 0.34406\n",
      "36805/36805 [==============================] - 30s 826us/sample - loss: 0.4214 - acc: 0.8760 - val_loss: 0.3688 - val_acc: 0.9045\n",
      "Epoch 246/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4265 - acc: 0.8729\n",
      "Epoch 00246: val_loss did not improve from 0.34406\n",
      "36805/36805 [==============================] - 30s 828us/sample - loss: 0.4266 - acc: 0.8729 - val_loss: 0.3744 - val_acc: 0.8921\n",
      "Epoch 247/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4253 - acc: 0.8746\n",
      "Epoch 00247: val_loss did not improve from 0.34406\n",
      "36805/36805 [==============================] - 30s 817us/sample - loss: 0.4254 - acc: 0.8746 - val_loss: 0.3996 - val_acc: 0.8891\n",
      "Epoch 248/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4197 - acc: 0.8751\n",
      "Epoch 00248: val_loss did not improve from 0.34406\n",
      "36805/36805 [==============================] - 30s 825us/sample - loss: 0.4201 - acc: 0.8750 - val_loss: 0.3781 - val_acc: 0.9008\n",
      "Epoch 249/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4252 - acc: 0.8732\n",
      "Epoch 00249: val_loss did not improve from 0.34406\n",
      "36805/36805 [==============================] - 31s 834us/sample - loss: 0.4251 - acc: 0.8732 - val_loss: 0.3453 - val_acc: 0.9122\n",
      "Epoch 250/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4202 - acc: 0.8736\n",
      "Epoch 00250: val_loss improved from 0.34406 to 0.34169, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_5_conv_checkpoint/250-0.3417.hdf5\n",
      "36805/36805 [==============================] - 30s 810us/sample - loss: 0.4201 - acc: 0.8737 - val_loss: 0.3417 - val_acc: 0.9085\n",
      "Epoch 251/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4166 - acc: 0.8774\n",
      "Epoch 00251: val_loss did not improve from 0.34169\n",
      "36805/36805 [==============================] - 31s 830us/sample - loss: 0.4167 - acc: 0.8774 - val_loss: 0.3690 - val_acc: 0.9012\n",
      "Epoch 252/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4189 - acc: 0.8774\n",
      "Epoch 00252: val_loss improved from 0.34169 to 0.34026, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_5_conv_checkpoint/252-0.3403.hdf5\n",
      "36805/36805 [==============================] - 31s 831us/sample - loss: 0.4191 - acc: 0.8773 - val_loss: 0.3403 - val_acc: 0.9099\n",
      "Epoch 253/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4151 - acc: 0.8794\n",
      "Epoch 00253: val_loss did not improve from 0.34026\n",
      "36805/36805 [==============================] - 30s 825us/sample - loss: 0.4153 - acc: 0.8793 - val_loss: 0.3434 - val_acc: 0.9099\n",
      "Epoch 254/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4202 - acc: 0.8778\n",
      "Epoch 00254: val_loss did not improve from 0.34026\n",
      "36805/36805 [==============================] - 30s 828us/sample - loss: 0.4202 - acc: 0.8778 - val_loss: 0.3656 - val_acc: 0.9033\n",
      "Epoch 255/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4148 - acc: 0.8763\n",
      "Epoch 00255: val_loss did not improve from 0.34026\n",
      "36805/36805 [==============================] - 30s 827us/sample - loss: 0.4148 - acc: 0.8763 - val_loss: 0.3995 - val_acc: 0.8896\n",
      "Epoch 256/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4153 - acc: 0.8762\n",
      "Epoch 00256: val_loss did not improve from 0.34026\n",
      "36805/36805 [==============================] - 30s 824us/sample - loss: 0.4152 - acc: 0.8761 - val_loss: 0.3788 - val_acc: 0.8959\n",
      "Epoch 257/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4159 - acc: 0.8764\n",
      "Epoch 00257: val_loss did not improve from 0.34026\n",
      "36805/36805 [==============================] - 30s 828us/sample - loss: 0.4158 - acc: 0.8764 - val_loss: 0.3490 - val_acc: 0.9110\n",
      "Epoch 258/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4183 - acc: 0.8756\n",
      "Epoch 00258: val_loss did not improve from 0.34026\n",
      "36805/36805 [==============================] - 31s 833us/sample - loss: 0.4183 - acc: 0.8756 - val_loss: 0.3439 - val_acc: 0.9085\n",
      "Epoch 259/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4168 - acc: 0.8751\n",
      "Epoch 00259: val_loss improved from 0.34026 to 0.33829, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_5_conv_checkpoint/259-0.3383.hdf5\n",
      "36805/36805 [==============================] - 31s 834us/sample - loss: 0.4168 - acc: 0.8751 - val_loss: 0.3383 - val_acc: 0.9110\n",
      "Epoch 260/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4127 - acc: 0.8765\n",
      "Epoch 00260: val_loss did not improve from 0.33829\n",
      "36805/36805 [==============================] - 30s 826us/sample - loss: 0.4127 - acc: 0.8765 - val_loss: 0.5262 - val_acc: 0.8346\n",
      "Epoch 261/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4093 - acc: 0.8784\n",
      "Epoch 00261: val_loss did not improve from 0.33829\n",
      "36805/36805 [==============================] - 31s 829us/sample - loss: 0.4093 - acc: 0.8784 - val_loss: 0.3721 - val_acc: 0.9043\n",
      "Epoch 262/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4179 - acc: 0.8750\n",
      "Epoch 00262: val_loss improved from 0.33829 to 0.33500, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_5_conv_checkpoint/262-0.3350.hdf5\n",
      "36805/36805 [==============================] - 31s 840us/sample - loss: 0.4179 - acc: 0.8750 - val_loss: 0.3350 - val_acc: 0.9110\n",
      "Epoch 263/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4083 - acc: 0.8805\n",
      "Epoch 00263: val_loss did not improve from 0.33500\n",
      "36805/36805 [==============================] - 30s 814us/sample - loss: 0.4083 - acc: 0.8805 - val_loss: 0.3629 - val_acc: 0.8991\n",
      "Epoch 264/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4103 - acc: 0.8796\n",
      "Epoch 00264: val_loss did not improve from 0.33500\n",
      "36805/36805 [==============================] - 31s 830us/sample - loss: 0.4104 - acc: 0.8795 - val_loss: 0.3505 - val_acc: 0.9029\n",
      "Epoch 265/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4129 - acc: 0.8782\n",
      "Epoch 00265: val_loss did not improve from 0.33500\n",
      "36805/36805 [==============================] - 30s 827us/sample - loss: 0.4131 - acc: 0.8781 - val_loss: 0.3598 - val_acc: 0.9043\n",
      "Epoch 266/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4128 - acc: 0.8767\n",
      "Epoch 00266: val_loss did not improve from 0.33500\n",
      "36805/36805 [==============================] - 30s 826us/sample - loss: 0.4131 - acc: 0.8767 - val_loss: 0.3492 - val_acc: 0.9043\n",
      "Epoch 267/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4125 - acc: 0.8790\n",
      "Epoch 00267: val_loss did not improve from 0.33500\n",
      "36805/36805 [==============================] - 30s 811us/sample - loss: 0.4125 - acc: 0.8789 - val_loss: 0.3511 - val_acc: 0.9094\n",
      "Epoch 268/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4072 - acc: 0.8786\n",
      "Epoch 00268: val_loss did not improve from 0.33500\n",
      "36805/36805 [==============================] - 30s 812us/sample - loss: 0.4072 - acc: 0.8786 - val_loss: 0.3478 - val_acc: 0.9092\n",
      "Epoch 269/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4075 - acc: 0.8788\n",
      "Epoch 00269: val_loss did not improve from 0.33500\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 0.4075 - acc: 0.8788 - val_loss: 0.3971 - val_acc: 0.8831\n",
      "Epoch 270/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4054 - acc: 0.8799\n",
      "Epoch 00270: val_loss did not improve from 0.33500\n",
      "36805/36805 [==============================] - 29s 786us/sample - loss: 0.4056 - acc: 0.8799 - val_loss: 0.3642 - val_acc: 0.9024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 271/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4058 - acc: 0.8787\n",
      "Epoch 00271: val_loss did not improve from 0.33500\n",
      "36805/36805 [==============================] - 29s 790us/sample - loss: 0.4059 - acc: 0.8786 - val_loss: 0.4153 - val_acc: 0.8854\n",
      "Epoch 272/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4068 - acc: 0.8785\n",
      "Epoch 00272: val_loss did not improve from 0.33500\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 0.4070 - acc: 0.8784 - val_loss: 0.3392 - val_acc: 0.9115\n",
      "Epoch 273/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4040 - acc: 0.8820\n",
      "Epoch 00273: val_loss did not improve from 0.33500\n",
      "36805/36805 [==============================] - 29s 793us/sample - loss: 0.4041 - acc: 0.8819 - val_loss: 0.3927 - val_acc: 0.8891\n",
      "Epoch 274/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4067 - acc: 0.8786\n",
      "Epoch 00274: val_loss did not improve from 0.33500\n",
      "36805/36805 [==============================] - 29s 781us/sample - loss: 0.4068 - acc: 0.8786 - val_loss: 0.3927 - val_acc: 0.8952\n",
      "Epoch 275/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4094 - acc: 0.8777\n",
      "Epoch 00275: val_loss did not improve from 0.33500\n",
      "36805/36805 [==============================] - 29s 781us/sample - loss: 0.4094 - acc: 0.8777 - val_loss: 0.3546 - val_acc: 0.9059\n",
      "Epoch 276/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4082 - acc: 0.8800\n",
      "Epoch 00276: val_loss did not improve from 0.33500\n",
      "36805/36805 [==============================] - 29s 782us/sample - loss: 0.4084 - acc: 0.8799 - val_loss: 0.3360 - val_acc: 0.9131\n",
      "Epoch 277/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3997 - acc: 0.8814\n",
      "Epoch 00277: val_loss did not improve from 0.33500\n",
      "36805/36805 [==============================] - 29s 784us/sample - loss: 0.3997 - acc: 0.8813 - val_loss: 0.3426 - val_acc: 0.9061\n",
      "Epoch 278/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4026 - acc: 0.8804\n",
      "Epoch 00278: val_loss did not improve from 0.33500\n",
      "36805/36805 [==============================] - 29s 783us/sample - loss: 0.4025 - acc: 0.8804 - val_loss: 0.4371 - val_acc: 0.8796\n",
      "Epoch 279/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4015 - acc: 0.8794\n",
      "Epoch 00279: val_loss did not improve from 0.33500\n",
      "36805/36805 [==============================] - 29s 782us/sample - loss: 0.4015 - acc: 0.8794 - val_loss: 0.3429 - val_acc: 0.9117\n",
      "Epoch 280/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3985 - acc: 0.8812\n",
      "Epoch 00280: val_loss did not improve from 0.33500\n",
      "36805/36805 [==============================] - 29s 778us/sample - loss: 0.3987 - acc: 0.8811 - val_loss: 0.3497 - val_acc: 0.9066\n",
      "Epoch 281/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3956 - acc: 0.8818\n",
      "Epoch 00281: val_loss did not improve from 0.33500\n",
      "36805/36805 [==============================] - 29s 781us/sample - loss: 0.3956 - acc: 0.8818 - val_loss: 0.3516 - val_acc: 0.9089\n",
      "Epoch 282/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3998 - acc: 0.8797\n",
      "Epoch 00282: val_loss did not improve from 0.33500\n",
      "36805/36805 [==============================] - 29s 781us/sample - loss: 0.3998 - acc: 0.8797 - val_loss: 0.3445 - val_acc: 0.9103\n",
      "Epoch 283/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3968 - acc: 0.8830\n",
      "Epoch 00283: val_loss did not improve from 0.33500\n",
      "36805/36805 [==============================] - 29s 793us/sample - loss: 0.3967 - acc: 0.8830 - val_loss: 0.3589 - val_acc: 0.9047\n",
      "Epoch 284/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3978 - acc: 0.8822\n",
      "Epoch 00284: val_loss did not improve from 0.33500\n",
      "36805/36805 [==============================] - 29s 789us/sample - loss: 0.3980 - acc: 0.8821 - val_loss: 0.3958 - val_acc: 0.8891\n",
      "Epoch 285/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4017 - acc: 0.8810\n",
      "Epoch 00285: val_loss did not improve from 0.33500\n",
      "36805/36805 [==============================] - 29s 800us/sample - loss: 0.4021 - acc: 0.8809 - val_loss: 0.3825 - val_acc: 0.8994\n",
      "Epoch 286/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3941 - acc: 0.8843\n",
      "Epoch 00286: val_loss did not improve from 0.33500\n",
      "36805/36805 [==============================] - 29s 792us/sample - loss: 0.3941 - acc: 0.8844 - val_loss: 0.3845 - val_acc: 0.8919\n",
      "Epoch 287/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3976 - acc: 0.8822\n",
      "Epoch 00287: val_loss did not improve from 0.33500\n",
      "36805/36805 [==============================] - 29s 789us/sample - loss: 0.3977 - acc: 0.8822 - val_loss: 0.3419 - val_acc: 0.9101\n",
      "Epoch 288/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3969 - acc: 0.8808\n",
      "Epoch 00288: val_loss did not improve from 0.33500\n",
      "36805/36805 [==============================] - 29s 793us/sample - loss: 0.3968 - acc: 0.8809 - val_loss: 0.3719 - val_acc: 0.9026\n",
      "Epoch 289/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3956 - acc: 0.8830\n",
      "Epoch 00289: val_loss did not improve from 0.33500\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 0.3957 - acc: 0.8830 - val_loss: 0.3413 - val_acc: 0.9126\n",
      "Epoch 290/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3988 - acc: 0.8827\n",
      "Epoch 00290: val_loss did not improve from 0.33500\n",
      "36805/36805 [==============================] - 29s 792us/sample - loss: 0.3989 - acc: 0.8827 - val_loss: 0.3585 - val_acc: 0.9047\n",
      "Epoch 291/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3985 - acc: 0.8807\n",
      "Epoch 00291: val_loss improved from 0.33500 to 0.32952, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_5_conv_checkpoint/291-0.3295.hdf5\n",
      "36805/36805 [==============================] - 29s 792us/sample - loss: 0.3985 - acc: 0.8806 - val_loss: 0.3295 - val_acc: 0.9101\n",
      "Epoch 292/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3907 - acc: 0.8837\n",
      "Epoch 00292: val_loss did not improve from 0.32952\n",
      "36805/36805 [==============================] - 29s 792us/sample - loss: 0.3907 - acc: 0.8837 - val_loss: 0.3612 - val_acc: 0.9061\n",
      "Epoch 293/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3949 - acc: 0.8805\n",
      "Epoch 00293: val_loss did not improve from 0.32952\n",
      "36805/36805 [==============================] - 29s 789us/sample - loss: 0.3950 - acc: 0.8805 - val_loss: 0.4033 - val_acc: 0.8926\n",
      "Epoch 294/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3905 - acc: 0.8812\n",
      "Epoch 00294: val_loss did not improve from 0.32952\n",
      "36805/36805 [==============================] - 29s 797us/sample - loss: 0.3904 - acc: 0.8812 - val_loss: 0.3523 - val_acc: 0.9061\n",
      "Epoch 295/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3924 - acc: 0.8823\n",
      "Epoch 00295: val_loss did not improve from 0.32952\n",
      "36805/36805 [==============================] - 29s 790us/sample - loss: 0.3926 - acc: 0.8822 - val_loss: 0.5331 - val_acc: 0.8479\n",
      "Epoch 296/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3966 - acc: 0.8804\n",
      "Epoch 00296: val_loss did not improve from 0.32952\n",
      "36805/36805 [==============================] - 29s 787us/sample - loss: 0.3966 - acc: 0.8804 - val_loss: 0.4020 - val_acc: 0.8910\n",
      "Epoch 297/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3951 - acc: 0.8829\n",
      "Epoch 00297: val_loss did not improve from 0.32952\n",
      "36805/36805 [==============================] - 29s 787us/sample - loss: 0.3956 - acc: 0.8826 - val_loss: 0.3476 - val_acc: 0.9045\n",
      "Epoch 298/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3920 - acc: 0.8831\n",
      "Epoch 00298: val_loss did not improve from 0.32952\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 0.3922 - acc: 0.8830 - val_loss: 0.3611 - val_acc: 0.9005\n",
      "Epoch 299/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3925 - acc: 0.8840\n",
      "Epoch 00299: val_loss did not improve from 0.32952\n",
      "36805/36805 [==============================] - 29s 790us/sample - loss: 0.3925 - acc: 0.8840 - val_loss: 0.3790 - val_acc: 0.8984\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 300/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3862 - acc: 0.8851\n",
      "Epoch 00300: val_loss did not improve from 0.32952\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 0.3861 - acc: 0.8851 - val_loss: 0.3645 - val_acc: 0.9031\n",
      "Epoch 301/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3890 - acc: 0.8827\n",
      "Epoch 00301: val_loss did not improve from 0.32952\n",
      "36805/36805 [==============================] - 29s 790us/sample - loss: 0.3889 - acc: 0.8827 - val_loss: 0.4113 - val_acc: 0.8861\n",
      "Epoch 302/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3837 - acc: 0.8836\n",
      "Epoch 00302: val_loss did not improve from 0.32952\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 0.3837 - acc: 0.8836 - val_loss: 0.3431 - val_acc: 0.9054\n",
      "Epoch 303/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3891 - acc: 0.8841\n",
      "Epoch 00303: val_loss did not improve from 0.32952\n",
      "36805/36805 [==============================] - 29s 782us/sample - loss: 0.3893 - acc: 0.8841 - val_loss: 0.3421 - val_acc: 0.9094\n",
      "Epoch 304/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3934 - acc: 0.8831\n",
      "Epoch 00304: val_loss did not improve from 0.32952\n",
      "36805/36805 [==============================] - 29s 794us/sample - loss: 0.3935 - acc: 0.8830 - val_loss: 0.3335 - val_acc: 0.9061\n",
      "Epoch 305/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3886 - acc: 0.8842\n",
      "Epoch 00305: val_loss did not improve from 0.32952\n",
      "36805/36805 [==============================] - 29s 791us/sample - loss: 0.3886 - acc: 0.8841 - val_loss: 0.3591 - val_acc: 0.9033\n",
      "Epoch 306/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3827 - acc: 0.8871\n",
      "Epoch 00306: val_loss did not improve from 0.32952\n",
      "36805/36805 [==============================] - 29s 792us/sample - loss: 0.3825 - acc: 0.8871 - val_loss: 0.4601 - val_acc: 0.8670\n",
      "Epoch 307/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3886 - acc: 0.8846\n",
      "Epoch 00307: val_loss did not improve from 0.32952\n",
      "36805/36805 [==============================] - 29s 789us/sample - loss: 0.3885 - acc: 0.8846 - val_loss: 0.3345 - val_acc: 0.9110\n",
      "Epoch 308/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3876 - acc: 0.8831\n",
      "Epoch 00308: val_loss did not improve from 0.32952\n",
      "36805/36805 [==============================] - 29s 786us/sample - loss: 0.3877 - acc: 0.8831 - val_loss: 0.3677 - val_acc: 0.9061\n",
      "Epoch 309/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3836 - acc: 0.8863\n",
      "Epoch 00309: val_loss did not improve from 0.32952\n",
      "36805/36805 [==============================] - 29s 787us/sample - loss: 0.3837 - acc: 0.8862 - val_loss: 0.4071 - val_acc: 0.8884\n",
      "Epoch 310/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3852 - acc: 0.8846\n",
      "Epoch 00310: val_loss improved from 0.32952 to 0.32739, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_5_conv_checkpoint/310-0.3274.hdf5\n",
      "36805/36805 [==============================] - 29s 791us/sample - loss: 0.3853 - acc: 0.8846 - val_loss: 0.3274 - val_acc: 0.9106\n",
      "Epoch 311/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3851 - acc: 0.8860\n",
      "Epoch 00311: val_loss did not improve from 0.32739\n",
      "36805/36805 [==============================] - 29s 791us/sample - loss: 0.3850 - acc: 0.8860 - val_loss: 0.3312 - val_acc: 0.9119\n",
      "Epoch 312/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3770 - acc: 0.8871\n",
      "Epoch 00312: val_loss did not improve from 0.32739\n",
      "36805/36805 [==============================] - 29s 789us/sample - loss: 0.3772 - acc: 0.8870 - val_loss: 0.3390 - val_acc: 0.9075\n",
      "Epoch 313/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3849 - acc: 0.8856\n",
      "Epoch 00313: val_loss did not improve from 0.32739\n",
      "36805/36805 [==============================] - 29s 783us/sample - loss: 0.3847 - acc: 0.8856 - val_loss: 1.0326 - val_acc: 0.7240\n",
      "Epoch 314/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3823 - acc: 0.8845\n",
      "Epoch 00314: val_loss did not improve from 0.32739\n",
      "36805/36805 [==============================] - 29s 781us/sample - loss: 0.3823 - acc: 0.8845 - val_loss: 0.3693 - val_acc: 0.9061\n",
      "Epoch 315/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3815 - acc: 0.8853\n",
      "Epoch 00315: val_loss did not improve from 0.32739\n",
      "36805/36805 [==============================] - 29s 787us/sample - loss: 0.3817 - acc: 0.8852 - val_loss: 0.4117 - val_acc: 0.8887\n",
      "Epoch 316/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3839 - acc: 0.8865\n",
      "Epoch 00316: val_loss did not improve from 0.32739\n",
      "36805/36805 [==============================] - 29s 783us/sample - loss: 0.3838 - acc: 0.8866 - val_loss: 0.3299 - val_acc: 0.9124\n",
      "Epoch 317/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3797 - acc: 0.8869\n",
      "Epoch 00317: val_loss did not improve from 0.32739\n",
      "36805/36805 [==============================] - 29s 784us/sample - loss: 0.3798 - acc: 0.8869 - val_loss: 0.4026 - val_acc: 0.8898\n",
      "Epoch 318/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3785 - acc: 0.8879\n",
      "Epoch 00318: val_loss did not improve from 0.32739\n",
      "36805/36805 [==============================] - 29s 782us/sample - loss: 0.3785 - acc: 0.8879 - val_loss: 0.3733 - val_acc: 0.9033\n",
      "Epoch 319/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3789 - acc: 0.8858\n",
      "Epoch 00319: val_loss did not improve from 0.32739\n",
      "36805/36805 [==============================] - 29s 785us/sample - loss: 0.3789 - acc: 0.8858 - val_loss: 0.4289 - val_acc: 0.8887\n",
      "Epoch 320/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3784 - acc: 0.8875\n",
      "Epoch 00320: val_loss did not improve from 0.32739\n",
      "36805/36805 [==============================] - 29s 780us/sample - loss: 0.3783 - acc: 0.8875 - val_loss: 0.3313 - val_acc: 0.9096\n",
      "Epoch 321/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3700 - acc: 0.8894\n",
      "Epoch 00321: val_loss did not improve from 0.32739\n",
      "36805/36805 [==============================] - 29s 780us/sample - loss: 0.3704 - acc: 0.8893 - val_loss: 0.5336 - val_acc: 0.8474\n",
      "Epoch 322/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3823 - acc: 0.8861\n",
      "Epoch 00322: val_loss did not improve from 0.32739\n",
      "36805/36805 [==============================] - 29s 777us/sample - loss: 0.3823 - acc: 0.8860 - val_loss: 0.4755 - val_acc: 0.8656\n",
      "Epoch 323/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3775 - acc: 0.8870\n",
      "Epoch 00323: val_loss did not improve from 0.32739\n",
      "36805/36805 [==============================] - 29s 793us/sample - loss: 0.3774 - acc: 0.8869 - val_loss: 0.3999 - val_acc: 0.8933\n",
      "Epoch 324/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3786 - acc: 0.8874\n",
      "Epoch 00324: val_loss did not improve from 0.32739\n",
      "36805/36805 [==============================] - 29s 791us/sample - loss: 0.3783 - acc: 0.8876 - val_loss: 0.4896 - val_acc: 0.8600\n",
      "Epoch 325/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3733 - acc: 0.8881\n",
      "Epoch 00325: val_loss did not improve from 0.32739\n",
      "36805/36805 [==============================] - 29s 787us/sample - loss: 0.3733 - acc: 0.8881 - val_loss: 0.5244 - val_acc: 0.8500\n",
      "Epoch 326/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3720 - acc: 0.8881\n",
      "Epoch 00326: val_loss did not improve from 0.32739\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 0.3720 - acc: 0.8880 - val_loss: 0.6016 - val_acc: 0.8330\n",
      "Epoch 327/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3782 - acc: 0.8872\n",
      "Epoch 00327: val_loss did not improve from 0.32739\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 0.3785 - acc: 0.8871 - val_loss: 0.3416 - val_acc: 0.9113\n",
      "Epoch 328/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3780 - acc: 0.8877\n",
      "Epoch 00328: val_loss did not improve from 0.32739\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 0.3782 - acc: 0.8876 - val_loss: 0.3542 - val_acc: 0.9050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 329/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3722 - acc: 0.8876\n",
      "Epoch 00329: val_loss did not improve from 0.32739\n",
      "36805/36805 [==============================] - 29s 790us/sample - loss: 0.3723 - acc: 0.8876 - val_loss: 0.3446 - val_acc: 0.9057\n",
      "Epoch 330/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3755 - acc: 0.8873\n",
      "Epoch 00330: val_loss did not improve from 0.32739\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 0.3755 - acc: 0.8873 - val_loss: 0.4264 - val_acc: 0.8821\n",
      "Epoch 331/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3716 - acc: 0.8901\n",
      "Epoch 00331: val_loss did not improve from 0.32739\n",
      "36805/36805 [==============================] - 29s 789us/sample - loss: 0.3719 - acc: 0.8900 - val_loss: 0.3404 - val_acc: 0.9073\n",
      "Epoch 332/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3718 - acc: 0.8892\n",
      "Epoch 00332: val_loss improved from 0.32739 to 0.32010, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_5_conv_checkpoint/332-0.3201.hdf5\n",
      "36805/36805 [==============================] - 29s 783us/sample - loss: 0.3723 - acc: 0.8890 - val_loss: 0.3201 - val_acc: 0.9168\n",
      "Epoch 333/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3758 - acc: 0.8877\n",
      "Epoch 00333: val_loss did not improve from 0.32010\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 0.3758 - acc: 0.8877 - val_loss: 0.4777 - val_acc: 0.8570\n",
      "Epoch 334/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3721 - acc: 0.8874\n",
      "Epoch 00334: val_loss did not improve from 0.32010\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 0.3719 - acc: 0.8875 - val_loss: 0.3352 - val_acc: 0.9152\n",
      "Epoch 335/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3734 - acc: 0.8879\n",
      "Epoch 00335: val_loss did not improve from 0.32010\n",
      "36805/36805 [==============================] - 29s 791us/sample - loss: 0.3732 - acc: 0.8880 - val_loss: 0.3904 - val_acc: 0.8842\n",
      "Epoch 336/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3693 - acc: 0.8890\n",
      "Epoch 00336: val_loss did not improve from 0.32010\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 0.3690 - acc: 0.8891 - val_loss: 0.4245 - val_acc: 0.8812\n",
      "Epoch 337/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3677 - acc: 0.8894\n",
      "Epoch 00337: val_loss did not improve from 0.32010\n",
      "36805/36805 [==============================] - 29s 791us/sample - loss: 0.3678 - acc: 0.8894 - val_loss: 0.3752 - val_acc: 0.8938\n",
      "Epoch 338/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3671 - acc: 0.8904\n",
      "Epoch 00338: val_loss did not improve from 0.32010\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 0.3676 - acc: 0.8903 - val_loss: 0.4827 - val_acc: 0.8612\n",
      "Epoch 339/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3706 - acc: 0.8889\n",
      "Epoch 00339: val_loss did not improve from 0.32010\n",
      "36805/36805 [==============================] - 29s 779us/sample - loss: 0.3706 - acc: 0.8889 - val_loss: 0.3374 - val_acc: 0.9082\n",
      "Epoch 340/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3662 - acc: 0.8911\n",
      "Epoch 00340: val_loss did not improve from 0.32010\n",
      "36805/36805 [==============================] - 29s 787us/sample - loss: 0.3662 - acc: 0.8911 - val_loss: 0.3305 - val_acc: 0.9129\n",
      "Epoch 341/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3696 - acc: 0.8900\n",
      "Epoch 00341: val_loss did not improve from 0.32010\n",
      "36805/36805 [==============================] - 29s 783us/sample - loss: 0.3699 - acc: 0.8899 - val_loss: 0.3310 - val_acc: 0.9157\n",
      "Epoch 342/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3737 - acc: 0.8889\n",
      "Epoch 00342: val_loss did not improve from 0.32010\n",
      "36805/36805 [==============================] - 29s 791us/sample - loss: 0.3737 - acc: 0.8888 - val_loss: 0.3417 - val_acc: 0.9110\n",
      "Epoch 343/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3664 - acc: 0.8908\n",
      "Epoch 00343: val_loss did not improve from 0.32010\n",
      "36805/36805 [==============================] - 29s 782us/sample - loss: 0.3664 - acc: 0.8907 - val_loss: 0.5360 - val_acc: 0.8537\n",
      "Epoch 344/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3684 - acc: 0.8895\n",
      "Epoch 00344: val_loss did not improve from 0.32010\n",
      "36805/36805 [==============================] - 29s 792us/sample - loss: 0.3684 - acc: 0.8895 - val_loss: 0.3673 - val_acc: 0.8980\n",
      "Epoch 345/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3685 - acc: 0.8900\n",
      "Epoch 00345: val_loss did not improve from 0.32010\n",
      "36805/36805 [==============================] - 29s 791us/sample - loss: 0.3683 - acc: 0.8900 - val_loss: 0.3306 - val_acc: 0.9078\n",
      "Epoch 346/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3642 - acc: 0.8906\n",
      "Epoch 00346: val_loss did not improve from 0.32010\n",
      "36805/36805 [==============================] - 29s 792us/sample - loss: 0.3641 - acc: 0.8906 - val_loss: 0.4403 - val_acc: 0.8810\n",
      "Epoch 347/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3641 - acc: 0.8896\n",
      "Epoch 00347: val_loss did not improve from 0.32010\n",
      "36805/36805 [==============================] - 29s 791us/sample - loss: 0.3642 - acc: 0.8896 - val_loss: 0.3822 - val_acc: 0.8931\n",
      "Epoch 348/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3682 - acc: 0.8904\n",
      "Epoch 00348: val_loss did not improve from 0.32010\n",
      "36805/36805 [==============================] - 29s 786us/sample - loss: 0.3683 - acc: 0.8903 - val_loss: 0.3211 - val_acc: 0.9138\n",
      "Epoch 349/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3568 - acc: 0.8919\n",
      "Epoch 00349: val_loss did not improve from 0.32010\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 0.3568 - acc: 0.8919 - val_loss: 0.3209 - val_acc: 0.9145\n",
      "Epoch 350/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3627 - acc: 0.8902\n",
      "Epoch 00350: val_loss did not improve from 0.32010\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 0.3628 - acc: 0.8902 - val_loss: 0.3739 - val_acc: 0.8970\n",
      "Epoch 351/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3630 - acc: 0.8917\n",
      "Epoch 00351: val_loss improved from 0.32010 to 0.31736, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_5_conv_checkpoint/351-0.3174.hdf5\n",
      "36805/36805 [==============================] - 29s 793us/sample - loss: 0.3631 - acc: 0.8917 - val_loss: 0.3174 - val_acc: 0.9173\n",
      "Epoch 352/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3641 - acc: 0.8920\n",
      "Epoch 00352: val_loss did not improve from 0.31736\n",
      "36805/36805 [==============================] - 29s 787us/sample - loss: 0.3641 - acc: 0.8919 - val_loss: 0.3498 - val_acc: 0.9066\n",
      "Epoch 353/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3576 - acc: 0.8919\n",
      "Epoch 00353: val_loss did not improve from 0.31736\n",
      "36805/36805 [==============================] - 29s 787us/sample - loss: 0.3576 - acc: 0.8919 - val_loss: 0.3318 - val_acc: 0.9092\n",
      "Epoch 354/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3584 - acc: 0.8909\n",
      "Epoch 00354: val_loss did not improve from 0.31736\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 0.3584 - acc: 0.8908 - val_loss: 0.3328 - val_acc: 0.9145\n",
      "Epoch 355/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3570 - acc: 0.8916\n",
      "Epoch 00355: val_loss did not improve from 0.31736\n",
      "36805/36805 [==============================] - 29s 790us/sample - loss: 0.3574 - acc: 0.8915 - val_loss: 0.5583 - val_acc: 0.8314\n",
      "Epoch 356/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3645 - acc: 0.8909- ETA: 1s - loss:\n",
      "Epoch 00356: val_loss did not improve from 0.31736\n",
      "36805/36805 [==============================] - 29s 792us/sample - loss: 0.3645 - acc: 0.8909 - val_loss: 0.3616 - val_acc: 0.9059\n",
      "Epoch 357/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3553 - acc: 0.8931\n",
      "Epoch 00357: val_loss did not improve from 0.31736\n",
      "36805/36805 [==============================] - 29s 786us/sample - loss: 0.3554 - acc: 0.8931 - val_loss: 0.3355 - val_acc: 0.9089\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 358/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3617 - acc: 0.8917\n",
      "Epoch 00358: val_loss did not improve from 0.31736\n",
      "36805/36805 [==============================] - 29s 786us/sample - loss: 0.3616 - acc: 0.8916 - val_loss: 0.3240 - val_acc: 0.9157\n",
      "Epoch 359/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3591 - acc: 0.8917\n",
      "Epoch 00359: val_loss did not improve from 0.31736\n",
      "36805/36805 [==============================] - 29s 786us/sample - loss: 0.3591 - acc: 0.8916 - val_loss: 0.6205 - val_acc: 0.8190\n",
      "Epoch 360/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3578 - acc: 0.8925\n",
      "Epoch 00360: val_loss did not improve from 0.31736\n",
      "36805/36805 [==============================] - 29s 785us/sample - loss: 0.3580 - acc: 0.8924 - val_loss: 0.3273 - val_acc: 0.9129\n",
      "Epoch 361/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3632 - acc: 0.8926\n",
      "Epoch 00361: val_loss did not improve from 0.31736\n",
      "36805/36805 [==============================] - 29s 787us/sample - loss: 0.3634 - acc: 0.8925 - val_loss: 0.3835 - val_acc: 0.8984\n",
      "Epoch 362/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3602 - acc: 0.8923\n",
      "Epoch 00362: val_loss did not improve from 0.31736\n",
      "36805/36805 [==============================] - 29s 789us/sample - loss: 0.3603 - acc: 0.8923 - val_loss: 0.3261 - val_acc: 0.9161\n",
      "Epoch 363/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3539 - acc: 0.8930\n",
      "Epoch 00363: val_loss did not improve from 0.31736\n",
      "36805/36805 [==============================] - 29s 789us/sample - loss: 0.3540 - acc: 0.8929 - val_loss: 0.3361 - val_acc: 0.9096\n",
      "Epoch 364/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3589 - acc: 0.8923\n",
      "Epoch 00364: val_loss did not improve from 0.31736\n",
      "36805/36805 [==============================] - 29s 790us/sample - loss: 0.3589 - acc: 0.8924 - val_loss: 0.3844 - val_acc: 0.8963\n",
      "Epoch 365/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3549 - acc: 0.8924\n",
      "Epoch 00365: val_loss did not improve from 0.31736\n",
      "36805/36805 [==============================] - 29s 787us/sample - loss: 0.3548 - acc: 0.8924 - val_loss: 0.4917 - val_acc: 0.8633\n",
      "Epoch 366/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3579 - acc: 0.8929\n",
      "Epoch 00366: val_loss did not improve from 0.31736\n",
      "36805/36805 [==============================] - 29s 789us/sample - loss: 0.3580 - acc: 0.8930 - val_loss: 0.3187 - val_acc: 0.9140\n",
      "Epoch 367/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3582 - acc: 0.8933\n",
      "Epoch 00367: val_loss did not improve from 0.31736\n",
      "36805/36805 [==============================] - 29s 790us/sample - loss: 0.3583 - acc: 0.8933 - val_loss: 0.3211 - val_acc: 0.9115\n",
      "Epoch 368/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3616 - acc: 0.8922\n",
      "Epoch 00368: val_loss did not improve from 0.31736\n",
      "36805/36805 [==============================] - 29s 789us/sample - loss: 0.3617 - acc: 0.8921 - val_loss: 0.3389 - val_acc: 0.9108\n",
      "Epoch 369/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3532 - acc: 0.8934\n",
      "Epoch 00369: val_loss did not improve from 0.31736\n",
      "36805/36805 [==============================] - 29s 789us/sample - loss: 0.3531 - acc: 0.8935 - val_loss: 0.3284 - val_acc: 0.9136\n",
      "Epoch 370/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3548 - acc: 0.8930\n",
      "Epoch 00370: val_loss improved from 0.31736 to 0.31587, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_5_conv_checkpoint/370-0.3159.hdf5\n",
      "36805/36805 [==============================] - 29s 789us/sample - loss: 0.3547 - acc: 0.8931 - val_loss: 0.3159 - val_acc: 0.9136\n",
      "Epoch 371/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3573 - acc: 0.8904\n",
      "Epoch 00371: val_loss did not improve from 0.31587\n",
      "36805/36805 [==============================] - 29s 791us/sample - loss: 0.3573 - acc: 0.8904 - val_loss: 0.3878 - val_acc: 0.8947\n",
      "Epoch 372/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3495 - acc: 0.8945\n",
      "Epoch 00372: val_loss did not improve from 0.31587\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 0.3499 - acc: 0.8944 - val_loss: 0.3788 - val_acc: 0.8945\n",
      "Epoch 373/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3554 - acc: 0.8924\n",
      "Epoch 00373: val_loss did not improve from 0.31587\n",
      "36805/36805 [==============================] - 29s 787us/sample - loss: 0.3554 - acc: 0.8923 - val_loss: 0.3607 - val_acc: 0.9029\n",
      "Epoch 374/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3546 - acc: 0.8937\n",
      "Epoch 00374: val_loss did not improve from 0.31587\n",
      "36805/36805 [==============================] - 29s 778us/sample - loss: 0.3546 - acc: 0.8937 - val_loss: 0.5481 - val_acc: 0.8442\n",
      "Epoch 375/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3500 - acc: 0.8954\n",
      "Epoch 00375: val_loss did not improve from 0.31587\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 0.3501 - acc: 0.8954 - val_loss: 0.3248 - val_acc: 0.9129\n",
      "Epoch 376/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3537 - acc: 0.8913\n",
      "Epoch 00376: val_loss did not improve from 0.31587\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 0.3537 - acc: 0.8913 - val_loss: 0.3170 - val_acc: 0.9147\n",
      "Epoch 377/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3544 - acc: 0.8943\n",
      "Epoch 00377: val_loss did not improve from 0.31587\n",
      "36805/36805 [==============================] - 29s 784us/sample - loss: 0.3544 - acc: 0.8944 - val_loss: 0.3399 - val_acc: 0.9071\n",
      "Epoch 378/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3518 - acc: 0.8940\n",
      "Epoch 00378: val_loss did not improve from 0.31587\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 0.3518 - acc: 0.8940 - val_loss: 0.3467 - val_acc: 0.9117\n",
      "Epoch 379/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3493 - acc: 0.8953\n",
      "Epoch 00379: val_loss did not improve from 0.31587\n",
      "36805/36805 [==============================] - 29s 789us/sample - loss: 0.3493 - acc: 0.8953 - val_loss: 0.3690 - val_acc: 0.8980\n",
      "Epoch 380/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3500 - acc: 0.8935\n",
      "Epoch 00380: val_loss did not improve from 0.31587\n",
      "36805/36805 [==============================] - 29s 787us/sample - loss: 0.3500 - acc: 0.8935 - val_loss: 0.3695 - val_acc: 0.8987\n",
      "Epoch 381/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3503 - acc: 0.8950\n",
      "Epoch 00381: val_loss did not improve from 0.31587\n",
      "36805/36805 [==============================] - 29s 791us/sample - loss: 0.3503 - acc: 0.8950 - val_loss: 0.3642 - val_acc: 0.9064\n",
      "Epoch 382/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3472 - acc: 0.8945\n",
      "Epoch 00382: val_loss did not improve from 0.31587\n",
      "36805/36805 [==============================] - 29s 796us/sample - loss: 0.3473 - acc: 0.8945 - val_loss: 0.3742 - val_acc: 0.9043\n",
      "Epoch 383/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3486 - acc: 0.8948\n",
      "Epoch 00383: val_loss did not improve from 0.31587\n",
      "36805/36805 [==============================] - 29s 791us/sample - loss: 0.3486 - acc: 0.8948 - val_loss: 0.3206 - val_acc: 0.9164\n",
      "Epoch 384/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3479 - acc: 0.8942\n",
      "Epoch 00384: val_loss did not improve from 0.31587\n",
      "36805/36805 [==============================] - 29s 782us/sample - loss: 0.3479 - acc: 0.8943 - val_loss: 0.3598 - val_acc: 0.9024\n",
      "Epoch 385/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3520 - acc: 0.8949\n",
      "Epoch 00385: val_loss did not improve from 0.31587\n",
      "36805/36805 [==============================] - 29s 781us/sample - loss: 0.3518 - acc: 0.8950 - val_loss: 0.3212 - val_acc: 0.9129\n",
      "Epoch 386/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3472 - acc: 0.8963\n",
      "Epoch 00386: val_loss did not improve from 0.31587\n",
      "36805/36805 [==============================] - 29s 778us/sample - loss: 0.3472 - acc: 0.8963 - val_loss: 0.3193 - val_acc: 0.9147\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 387/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3428 - acc: 0.8961\n",
      "Epoch 00387: val_loss did not improve from 0.31587\n",
      "36805/36805 [==============================] - 29s 779us/sample - loss: 0.3428 - acc: 0.8961 - val_loss: 0.3598 - val_acc: 0.9038\n",
      "Epoch 388/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3486 - acc: 0.8949\n",
      "Epoch 00388: val_loss did not improve from 0.31587\n",
      "36805/36805 [==============================] - 29s 780us/sample - loss: 0.3485 - acc: 0.8949 - val_loss: 0.3495 - val_acc: 0.9110\n",
      "Epoch 389/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3495 - acc: 0.8960\n",
      "Epoch 00389: val_loss did not improve from 0.31587\n",
      "36805/36805 [==============================] - 29s 783us/sample - loss: 0.3494 - acc: 0.8960 - val_loss: 0.3171 - val_acc: 0.9196\n",
      "Epoch 390/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3490 - acc: 0.8942\n",
      "Epoch 00390: val_loss did not improve from 0.31587\n",
      "36805/36805 [==============================] - 29s 780us/sample - loss: 0.3495 - acc: 0.8941 - val_loss: 0.3261 - val_acc: 0.9147\n",
      "Epoch 391/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3543 - acc: 0.8933\n",
      "Epoch 00391: val_loss did not improve from 0.31587\n",
      "36805/36805 [==============================] - 29s 783us/sample - loss: 0.3548 - acc: 0.8932 - val_loss: 0.3546 - val_acc: 0.9026\n",
      "Epoch 392/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3435 - acc: 0.8957\n",
      "Epoch 00392: val_loss did not improve from 0.31587\n",
      "36805/36805 [==============================] - 29s 781us/sample - loss: 0.3435 - acc: 0.8957 - val_loss: 0.4358 - val_acc: 0.8828\n",
      "Epoch 393/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3469 - acc: 0.8962\n",
      "Epoch 00393: val_loss did not improve from 0.31587\n",
      "36805/36805 [==============================] - 29s 781us/sample - loss: 0.3471 - acc: 0.8962 - val_loss: 0.3281 - val_acc: 0.9140\n",
      "Epoch 394/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3452 - acc: 0.8949\n",
      "Epoch 00394: val_loss did not improve from 0.31587\n",
      "36805/36805 [==============================] - 29s 784us/sample - loss: 0.3452 - acc: 0.8949 - val_loss: 0.3556 - val_acc: 0.9059\n",
      "Epoch 395/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3478 - acc: 0.8937\n",
      "Epoch 00395: val_loss did not improve from 0.31587\n",
      "36805/36805 [==============================] - 29s 782us/sample - loss: 0.3478 - acc: 0.8937 - val_loss: 0.3340 - val_acc: 0.9092\n",
      "Epoch 396/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3503 - acc: 0.8933\n",
      "Epoch 00396: val_loss did not improve from 0.31587\n",
      "36805/36805 [==============================] - 29s 779us/sample - loss: 0.3505 - acc: 0.8933 - val_loss: 0.3379 - val_acc: 0.9059\n",
      "Epoch 397/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3451 - acc: 0.8938\n",
      "Epoch 00397: val_loss did not improve from 0.31587\n",
      "36805/36805 [==============================] - 29s 781us/sample - loss: 0.3452 - acc: 0.8937 - val_loss: 0.8071 - val_acc: 0.7605\n",
      "Epoch 398/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3454 - acc: 0.8959\n",
      "Epoch 00398: val_loss did not improve from 0.31587\n",
      "36805/36805 [==============================] - 29s 778us/sample - loss: 0.3454 - acc: 0.8959 - val_loss: 0.4270 - val_acc: 0.8730\n",
      "Epoch 399/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3468 - acc: 0.8943\n",
      "Epoch 00399: val_loss did not improve from 0.31587\n",
      "36805/36805 [==============================] - 29s 780us/sample - loss: 0.3466 - acc: 0.8944 - val_loss: 0.3425 - val_acc: 0.9143\n",
      "Epoch 400/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3478 - acc: 0.8948\n",
      "Epoch 00400: val_loss did not improve from 0.31587\n",
      "36805/36805 [==============================] - 29s 779us/sample - loss: 0.3478 - acc: 0.8948 - val_loss: 0.3162 - val_acc: 0.9154\n",
      "Epoch 401/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3400 - acc: 0.8984\n",
      "Epoch 00401: val_loss did not improve from 0.31587\n",
      "36805/36805 [==============================] - 29s 776us/sample - loss: 0.3399 - acc: 0.8984 - val_loss: 0.5584 - val_acc: 0.8358\n",
      "Epoch 402/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3350 - acc: 0.8982\n",
      "Epoch 00402: val_loss did not improve from 0.31587\n",
      "36805/36805 [==============================] - 29s 780us/sample - loss: 0.3350 - acc: 0.8982 - val_loss: 0.3273 - val_acc: 0.9085\n",
      "Epoch 403/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3451 - acc: 0.8957\n",
      "Epoch 00403: val_loss did not improve from 0.31587\n",
      "36805/36805 [==============================] - 29s 784us/sample - loss: 0.3451 - acc: 0.8957 - val_loss: 0.3353 - val_acc: 0.9136\n",
      "Epoch 404/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3439 - acc: 0.8965\n",
      "Epoch 00404: val_loss did not improve from 0.31587\n",
      "36805/36805 [==============================] - 29s 780us/sample - loss: 0.3442 - acc: 0.8965 - val_loss: 0.3433 - val_acc: 0.9119\n",
      "Epoch 405/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3448 - acc: 0.8946\n",
      "Epoch 00405: val_loss did not improve from 0.31587\n",
      "36805/36805 [==============================] - 29s 782us/sample - loss: 0.3448 - acc: 0.8946 - val_loss: 0.5676 - val_acc: 0.8341\n",
      "Epoch 406/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3381 - acc: 0.8983\n",
      "Epoch 00406: val_loss did not improve from 0.31587\n",
      "36805/36805 [==============================] - 29s 789us/sample - loss: 0.3385 - acc: 0.8983 - val_loss: 0.3459 - val_acc: 0.9119\n",
      "Epoch 407/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3422 - acc: 0.8983\n",
      "Epoch 00407: val_loss did not improve from 0.31587\n",
      "36805/36805 [==============================] - 29s 792us/sample - loss: 0.3424 - acc: 0.8983 - val_loss: 0.3402 - val_acc: 0.9099\n",
      "Epoch 408/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3399 - acc: 0.8968\n",
      "Epoch 00408: val_loss did not improve from 0.31587\n",
      "36805/36805 [==============================] - 29s 787us/sample - loss: 0.3399 - acc: 0.8968 - val_loss: 0.3274 - val_acc: 0.9147\n",
      "Epoch 409/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3392 - acc: 0.8985\n",
      "Epoch 00409: val_loss did not improve from 0.31587\n",
      "36805/36805 [==============================] - 29s 790us/sample - loss: 0.3388 - acc: 0.8986 - val_loss: 0.3286 - val_acc: 0.9113\n",
      "Epoch 410/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3412 - acc: 0.8959\n",
      "Epoch 00410: val_loss improved from 0.31587 to 0.31340, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_5_conv_checkpoint/410-0.3134.hdf5\n",
      "36805/36805 [==============================] - 29s 793us/sample - loss: 0.3411 - acc: 0.8959 - val_loss: 0.3134 - val_acc: 0.9159\n",
      "Epoch 411/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3381 - acc: 0.8985\n",
      "Epoch 00411: val_loss did not improve from 0.31340\n",
      "36805/36805 [==============================] - 29s 792us/sample - loss: 0.3383 - acc: 0.8986 - val_loss: 0.4875 - val_acc: 0.8551\n",
      "Epoch 412/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3402 - acc: 0.8977\n",
      "Epoch 00412: val_loss did not improve from 0.31340\n",
      "36805/36805 [==============================] - 29s 791us/sample - loss: 0.3401 - acc: 0.8977 - val_loss: 0.3427 - val_acc: 0.9080\n",
      "Epoch 413/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3388 - acc: 0.8978\n",
      "Epoch 00413: val_loss did not improve from 0.31340\n",
      "36805/36805 [==============================] - 29s 790us/sample - loss: 0.3392 - acc: 0.8978 - val_loss: 0.3236 - val_acc: 0.9180\n",
      "Epoch 414/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3411 - acc: 0.8954\n",
      "Epoch 00414: val_loss did not improve from 0.31340\n",
      "36805/36805 [==============================] - 29s 793us/sample - loss: 0.3410 - acc: 0.8954 - val_loss: 0.3327 - val_acc: 0.9108\n",
      "Epoch 415/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3396 - acc: 0.8969\n",
      "Epoch 00415: val_loss did not improve from 0.31340\n",
      "36805/36805 [==============================] - 29s 790us/sample - loss: 0.3395 - acc: 0.8969 - val_loss: 0.3357 - val_acc: 0.9147\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 416/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3374 - acc: 0.8966\n",
      "Epoch 00416: val_loss did not improve from 0.31340\n",
      "36805/36805 [==============================] - 29s 791us/sample - loss: 0.3374 - acc: 0.8965 - val_loss: 0.4866 - val_acc: 0.8588\n",
      "Epoch 417/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3296 - acc: 0.9004\n",
      "Epoch 00417: val_loss did not improve from 0.31340\n",
      "36805/36805 [==============================] - 29s 789us/sample - loss: 0.3297 - acc: 0.9004 - val_loss: 0.3232 - val_acc: 0.9154\n",
      "Epoch 418/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3333 - acc: 0.8989\n",
      "Epoch 00418: val_loss did not improve from 0.31340\n",
      "36805/36805 [==============================] - 29s 790us/sample - loss: 0.3333 - acc: 0.8989 - val_loss: 0.3401 - val_acc: 0.9075\n",
      "Epoch 419/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3381 - acc: 0.8974\n",
      "Epoch 00419: val_loss improved from 0.31340 to 0.30913, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_5_conv_checkpoint/419-0.3091.hdf5\n",
      "36805/36805 [==============================] - 29s 794us/sample - loss: 0.3381 - acc: 0.8974 - val_loss: 0.3091 - val_acc: 0.9210\n",
      "Epoch 420/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3354 - acc: 0.8985\n",
      "Epoch 00420: val_loss did not improve from 0.30913\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 0.3355 - acc: 0.8985 - val_loss: 0.3587 - val_acc: 0.9108\n",
      "Epoch 421/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3357 - acc: 0.8982\n",
      "Epoch 00421: val_loss did not improve from 0.30913\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 0.3364 - acc: 0.8981 - val_loss: 0.3262 - val_acc: 0.9094\n",
      "Epoch 422/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3441 - acc: 0.8960\n",
      "Epoch 00422: val_loss did not improve from 0.30913\n",
      "36805/36805 [==============================] - 29s 793us/sample - loss: 0.3442 - acc: 0.8960 - val_loss: 0.3381 - val_acc: 0.9117\n",
      "Epoch 423/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3335 - acc: 0.8999\n",
      "Epoch 00423: val_loss did not improve from 0.30913\n",
      "36805/36805 [==============================] - 29s 791us/sample - loss: 0.3335 - acc: 0.8999 - val_loss: 0.4134 - val_acc: 0.8917\n",
      "Epoch 424/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3358 - acc: 0.8991\n",
      "Epoch 00424: val_loss did not improve from 0.30913\n",
      "36805/36805 [==============================] - 29s 790us/sample - loss: 0.3359 - acc: 0.8991 - val_loss: 0.3316 - val_acc: 0.9150\n",
      "Epoch 425/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3310 - acc: 0.8999\n",
      "Epoch 00425: val_loss did not improve from 0.30913\n",
      "36805/36805 [==============================] - 29s 789us/sample - loss: 0.3311 - acc: 0.8999 - val_loss: 0.4320 - val_acc: 0.8784\n",
      "Epoch 426/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3331 - acc: 0.8984\n",
      "Epoch 00426: val_loss did not improve from 0.30913\n",
      "36805/36805 [==============================] - 29s 790us/sample - loss: 0.3330 - acc: 0.8984 - val_loss: 0.3226 - val_acc: 0.9180\n",
      "Epoch 427/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3311 - acc: 0.8996\n",
      "Epoch 00427: val_loss did not improve from 0.30913\n",
      "36805/36805 [==============================] - 29s 800us/sample - loss: 0.3311 - acc: 0.8996 - val_loss: 0.3270 - val_acc: 0.9115\n",
      "Epoch 428/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3331 - acc: 0.9001\n",
      "Epoch 00428: val_loss did not improve from 0.30913\n",
      "36805/36805 [==============================] - 29s 790us/sample - loss: 0.3331 - acc: 0.9000 - val_loss: 0.3315 - val_acc: 0.9110\n",
      "Epoch 429/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3314 - acc: 0.9004\n",
      "Epoch 00429: val_loss did not improve from 0.30913\n",
      "36805/36805 [==============================] - 29s 789us/sample - loss: 0.3318 - acc: 0.9004 - val_loss: 0.3298 - val_acc: 0.9175\n",
      "Epoch 430/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3394 - acc: 0.8976\n",
      "Epoch 00430: val_loss did not improve from 0.30913\n",
      "36805/36805 [==============================] - 29s 784us/sample - loss: 0.3394 - acc: 0.8976 - val_loss: 0.3600 - val_acc: 0.9047\n",
      "Epoch 431/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3329 - acc: 0.8994\n",
      "Epoch 00431: val_loss did not improve from 0.30913\n",
      "36805/36805 [==============================] - 29s 781us/sample - loss: 0.3330 - acc: 0.8994 - val_loss: 0.3612 - val_acc: 0.8980\n",
      "Epoch 432/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3343 - acc: 0.8966\n",
      "Epoch 00432: val_loss did not improve from 0.30913\n",
      "36805/36805 [==============================] - 29s 781us/sample - loss: 0.3343 - acc: 0.8966 - val_loss: 0.3211 - val_acc: 0.9145\n",
      "Epoch 433/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3259 - acc: 0.8998\n",
      "Epoch 00433: val_loss did not improve from 0.30913\n",
      "36805/36805 [==============================] - 29s 781us/sample - loss: 0.3260 - acc: 0.8997 - val_loss: 0.3189 - val_acc: 0.9113\n",
      "Epoch 434/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3358 - acc: 0.8977\n",
      "Epoch 00434: val_loss did not improve from 0.30913\n",
      "36805/36805 [==============================] - 29s 782us/sample - loss: 0.3359 - acc: 0.8976 - val_loss: 0.3516 - val_acc: 0.9131\n",
      "Epoch 435/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3320 - acc: 0.8980\n",
      "Epoch 00435: val_loss did not improve from 0.30913\n",
      "36805/36805 [==============================] - 29s 782us/sample - loss: 0.3319 - acc: 0.8981 - val_loss: 0.3136 - val_acc: 0.9140\n",
      "Epoch 436/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3299 - acc: 0.9001\n",
      "Epoch 00436: val_loss did not improve from 0.30913\n",
      "36805/36805 [==============================] - 29s 781us/sample - loss: 0.3299 - acc: 0.9000 - val_loss: 0.3544 - val_acc: 0.9075\n",
      "Epoch 437/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3290 - acc: 0.9009\n",
      "Epoch 00437: val_loss did not improve from 0.30913\n",
      "36805/36805 [==============================] - 29s 782us/sample - loss: 0.3291 - acc: 0.9009 - val_loss: 0.3893 - val_acc: 0.8959\n",
      "Epoch 438/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3260 - acc: 0.9007\n",
      "Epoch 00438: val_loss did not improve from 0.30913\n",
      "36805/36805 [==============================] - 29s 780us/sample - loss: 0.3260 - acc: 0.9007 - val_loss: 0.3406 - val_acc: 0.9089\n",
      "Epoch 439/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3270 - acc: 0.9004\n",
      "Epoch 00439: val_loss did not improve from 0.30913\n",
      "36805/36805 [==============================] - 29s 778us/sample - loss: 0.3270 - acc: 0.9004 - val_loss: 1.0431 - val_acc: 0.7270\n",
      "Epoch 440/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3258 - acc: 0.9024\n",
      "Epoch 00440: val_loss did not improve from 0.30913\n",
      "36805/36805 [==============================] - 29s 784us/sample - loss: 0.3258 - acc: 0.9024 - val_loss: 0.3164 - val_acc: 0.9164\n",
      "Epoch 441/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3276 - acc: 0.9010\n",
      "Epoch 00441: val_loss did not improve from 0.30913\n",
      "36805/36805 [==============================] - 29s 790us/sample - loss: 0.3277 - acc: 0.9009 - val_loss: 0.3248 - val_acc: 0.9164\n",
      "Epoch 442/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3312 - acc: 0.8984\n",
      "Epoch 00442: val_loss did not improve from 0.30913\n",
      "36805/36805 [==============================] - 29s 790us/sample - loss: 0.3313 - acc: 0.8984 - val_loss: 0.4589 - val_acc: 0.8712\n",
      "Epoch 443/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3242 - acc: 0.9006\n",
      "Epoch 00443: val_loss did not improve from 0.30913\n",
      "36805/36805 [==============================] - 29s 790us/sample - loss: 0.3243 - acc: 0.9005 - val_loss: 0.3195 - val_acc: 0.9143\n",
      "Epoch 444/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3278 - acc: 0.8996\n",
      "Epoch 00444: val_loss did not improve from 0.30913\n",
      "36805/36805 [==============================] - 29s 790us/sample - loss: 0.3280 - acc: 0.8996 - val_loss: 0.3297 - val_acc: 0.9168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 445/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3296 - acc: 0.9015\n",
      "Epoch 00445: val_loss did not improve from 0.30913\n",
      "36805/36805 [==============================] - 29s 793us/sample - loss: 0.3295 - acc: 0.9015 - val_loss: 0.3216 - val_acc: 0.9159\n",
      "Epoch 446/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3236 - acc: 0.9010\n",
      "Epoch 00446: val_loss did not improve from 0.30913\n",
      "36805/36805 [==============================] - 29s 789us/sample - loss: 0.3243 - acc: 0.9009 - val_loss: 0.4596 - val_acc: 0.8842\n",
      "Epoch 447/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3243 - acc: 0.9018\n",
      "Epoch 00447: val_loss did not improve from 0.30913\n",
      "36805/36805 [==============================] - 29s 791us/sample - loss: 0.3246 - acc: 0.9017 - val_loss: 0.3243 - val_acc: 0.9113\n",
      "Epoch 448/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3283 - acc: 0.9014\n",
      "Epoch 00448: val_loss did not improve from 0.30913\n",
      "36805/36805 [==============================] - 29s 790us/sample - loss: 0.3283 - acc: 0.9015 - val_loss: 0.3232 - val_acc: 0.9145\n",
      "Epoch 449/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3319 - acc: 0.8985\n",
      "Epoch 00449: val_loss did not improve from 0.30913\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 0.3322 - acc: 0.8985 - val_loss: 0.3375 - val_acc: 0.9103\n",
      "Epoch 450/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3257 - acc: 0.9009\n",
      "Epoch 00450: val_loss did not improve from 0.30913\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 0.3260 - acc: 0.9009 - val_loss: 0.3302 - val_acc: 0.9129\n",
      "Epoch 451/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3267 - acc: 0.8997\n",
      "Epoch 00451: val_loss did not improve from 0.30913\n",
      "36805/36805 [==============================] - 29s 790us/sample - loss: 0.3265 - acc: 0.8997 - val_loss: 0.3405 - val_acc: 0.9131\n",
      "Epoch 452/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3268 - acc: 0.9003\n",
      "Epoch 00452: val_loss did not improve from 0.30913\n",
      "36805/36805 [==============================] - 29s 792us/sample - loss: 0.3273 - acc: 0.9003 - val_loss: 0.4521 - val_acc: 0.8693\n",
      "Epoch 453/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3303 - acc: 0.9005\n",
      "Epoch 00453: val_loss did not improve from 0.30913\n",
      "36805/36805 [==============================] - 29s 782us/sample - loss: 0.3303 - acc: 0.9005 - val_loss: 0.3269 - val_acc: 0.9140\n",
      "Epoch 454/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3307 - acc: 0.8990\n",
      "Epoch 00454: val_loss did not improve from 0.30913\n",
      "36805/36805 [==============================] - 29s 781us/sample - loss: 0.3308 - acc: 0.8990 - val_loss: 0.5424 - val_acc: 0.8528\n",
      "Epoch 455/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3308 - acc: 0.8993\n",
      "Epoch 00455: val_loss did not improve from 0.30913\n",
      "36805/36805 [==============================] - 29s 779us/sample - loss: 0.3308 - acc: 0.8993 - val_loss: 0.3399 - val_acc: 0.9124\n",
      "Epoch 456/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3258 - acc: 0.9013\n",
      "Epoch 00456: val_loss did not improve from 0.30913\n",
      "36805/36805 [==============================] - 29s 781us/sample - loss: 0.3260 - acc: 0.9012 - val_loss: 0.4527 - val_acc: 0.8728\n",
      "Epoch 457/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3248 - acc: 0.9029\n",
      "Epoch 00457: val_loss did not improve from 0.30913\n",
      "36805/36805 [==============================] - 29s 782us/sample - loss: 0.3247 - acc: 0.9029 - val_loss: 0.3168 - val_acc: 0.9203\n",
      "Epoch 458/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3254 - acc: 0.9000\n",
      "Epoch 00458: val_loss did not improve from 0.30913\n",
      "36805/36805 [==============================] - 29s 779us/sample - loss: 0.3254 - acc: 0.9000 - val_loss: 0.3286 - val_acc: 0.9166\n",
      "Epoch 459/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3236 - acc: 0.9022\n",
      "Epoch 00459: val_loss did not improve from 0.30913\n",
      "36805/36805 [==============================] - 29s 782us/sample - loss: 0.3236 - acc: 0.9022 - val_loss: 0.3475 - val_acc: 0.9119\n",
      "Epoch 460/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3178 - acc: 0.9033\n",
      "Epoch 00460: val_loss did not improve from 0.30913\n",
      "36805/36805 [==============================] - 29s 780us/sample - loss: 0.3179 - acc: 0.9032 - val_loss: 0.3465 - val_acc: 0.9061\n",
      "Epoch 461/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3268 - acc: 0.9016\n",
      "Epoch 00461: val_loss did not improve from 0.30913\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 0.3271 - acc: 0.9016 - val_loss: 0.3491 - val_acc: 0.9080\n",
      "Epoch 462/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3254 - acc: 0.9016\n",
      "Epoch 00462: val_loss did not improve from 0.30913\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 0.3253 - acc: 0.9016 - val_loss: 0.3823 - val_acc: 0.9033\n",
      "Epoch 463/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3189 - acc: 0.9028\n",
      "Epoch 00463: val_loss did not improve from 0.30913\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 0.3190 - acc: 0.9028 - val_loss: 0.3419 - val_acc: 0.9124\n",
      "Epoch 464/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3230 - acc: 0.9029\n",
      "Epoch 00464: val_loss did not improve from 0.30913\n",
      "36805/36805 [==============================] - 29s 789us/sample - loss: 0.3231 - acc: 0.9028 - val_loss: 0.3490 - val_acc: 0.9038\n",
      "Epoch 465/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3203 - acc: 0.9023\n",
      "Epoch 00465: val_loss did not improve from 0.30913\n",
      "36805/36805 [==============================] - 29s 787us/sample - loss: 0.3204 - acc: 0.9023 - val_loss: 0.4302 - val_acc: 0.8847\n",
      "Epoch 466/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3233 - acc: 0.9006\n",
      "Epoch 00466: val_loss did not improve from 0.30913\n",
      "36805/36805 [==============================] - 29s 789us/sample - loss: 0.3233 - acc: 0.9006 - val_loss: 0.3455 - val_acc: 0.9159\n",
      "Epoch 467/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3207 - acc: 0.9026\n",
      "Epoch 00467: val_loss did not improve from 0.30913\n",
      "36805/36805 [==============================] - 29s 787us/sample - loss: 0.3207 - acc: 0.9026 - val_loss: 0.3249 - val_acc: 0.9147\n",
      "Epoch 468/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3177 - acc: 0.9030\n",
      "Epoch 00468: val_loss did not improve from 0.30913\n",
      "36805/36805 [==============================] - 29s 794us/sample - loss: 0.3179 - acc: 0.9029 - val_loss: 0.3294 - val_acc: 0.9164\n",
      "Epoch 469/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3244 - acc: 0.9007\n",
      "Epoch 00469: val_loss did not improve from 0.30913\n",
      "36805/36805 [==============================] - 29s 794us/sample - loss: 0.3245 - acc: 0.9006 - val_loss: 0.3287 - val_acc: 0.9140\n",
      "\n",
      "1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_5_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzsnXd8VFX6h58zk0kmvZOEEAglQEjoBNFQVIoKCgjSRFkb6upPRV1WrOuqu7J2sbGoKFZkQVSaKEpVeo/SS4AQ0nufmfv7405NJgXIpJ7n80nmzr1nzj33ztzzPec973mPUBQFiUQikUgANI1dAIlEIpE0HaQoSCQSicSKFAWJRCKRWJGiIJFIJBIrUhQkEolEYkWKgkQikUisSFGQSCQSiRUpChKJRCKxIkVBIpFIJFbcGrsAF0tISIgSHR3d2MWQSCSSZsXu3bszFUUJrS1dsxOF6Ohodu3a1djFkEgkkmaFECK5Lumk+UgikUgkVqQoSCQSicSKFAWJRCKRWGl2YwrOqKio4Ny5c5SWljZ2UZoter2edu3aodPpGrsoEomkEWkRonDu3Dl8fX2Jjo5GCNHYxWl2KIpCVlYW586do2PHjo1dHIlE0oi0CPNRaWkpwcHBUhAuESEEwcHBsqclkUhahigAUhAuE3n/JBIJtCBRqA2jsYSyshRMporGLopEIpE0WVqNKJhMJZSXp6IohnrPOzc3l/fff/+SPjt69Ghyc3PrnP7555/ntddeu6RzSSQSSW24TBSEEFFCiPVCiD+FEH8IIR5xkuZqIUSeEGKf+e85V5UHLOYRpd5zrkkUDIaaRWj16tUEBATUe5kkEonkUnBlT8EAPK4oSg9gEPCgEKKHk3SbFUXpY/57wXXFcZ0ozJkzhxMnTtCnTx9mz57Nhg0bGDJkCGPHjqVHD/WSx48fT//+/YmLi2PBggXWz0ZHR5OZmcnp06eJjY1l5syZxMXFMWrUKEpKSmo87759+xg0aBC9evXi5ptvJicnB4B58+bRo0cPevXqxdSpUwHYuHEjffr0oU+fPvTt25eCgoJ6vw8SiaT54zKXVEVRUoFU83aBEOIQEAn86apzAhw7NovCwn1OymPAZCpBo/FCCO1F5enj04eYmLeqPT537lySkpLYt08974YNG9izZw9JSUlWF8+FCxcSFBRESUkJCQkJTJw4keDg4EplP8bXX3/Nhx9+yOTJk1m2bBm33XZbteedMWMG77zzDsOGDeO5557jn//8J2+99RZz587l1KlTeHh4WE1Tr732Gu+99x6JiYkUFhai1+sv6h5IJJLWQYOMKQghooG+wHYnh68UQuwXQqwRQsS5sBSuy9oJAwcOdPD5nzdvHr1792bQoEGcPXuWY8eOVflMx44d6dOnDwD9+/fn9OnT1eafl5dHbm4uw4YNA+Avf/kLmzZtAqBXr15Mnz6dL774Ajc3VfcTExN57LHHmDdvHrm5udb9EolEYo/LawYhhA+wDJilKEp+pcN7gA6KohQKIUYD3wExTvK4F7gXoH379jWer7oWvcFQQEnJETw9u+Lm5nfR13GxeHt7W7c3bNjAunXr2Lp1K15eXlx99dVO5wR4eHhYt7Vaba3mo+pYtWoVmzZtYsWKFfzrX//i4MGDzJkzhzFjxrB69WoSExNZu3Yt3bt3v6T8JRJJy8WlPQUhhA5VEL5UFOXbyscVRclXFKXQvL0a0AkhQpykW6AoygBFUQaEhtYaDry60lhyu8TPV4+vr2+NNvq8vDwCAwPx8vLi8OHDbNu27bLP6e/vT2BgIJs3bwbg888/Z9iwYZhMJs6ePcs111zDf/7zH/Ly8igsLOTEiRP07NmTJ554goSEBA4fPnzZZZBIJC0Pl/UUhDob6mPgkKIob1STJhxIUxRFEUIMRBWpLBeVxxXZAhAcHExiYiLx8fHccMMNjBkzxuH49ddfz/z584mNjaVbt24MGjSoXs67aNEi7r//foqLi+nUqROffPIJRqOR2267jby8PBRF4eGHHyYgIIBnn32W9evXo9FoiIuL44YbbqiXMkgkkpaFUJT6bzkDCCEGA5uBg4DJvPspoD2AoijzhRD/B/wV1VOpBHhMUZTfa8p3wIABSuVFdg4dOkRsbGyN5TEaiyguPoRe3wWdTrqAOqMu91EikTRPhBC7FUUZUFs6V3ofbaGW0V1FUd4F3nVVGRxxnflIIpFIWgqtZkazFAWJRCKpHSkKEolEIrHSakTBMtDsqjEUiUQiaQm0GlGQPQWJRCKpHSkKEolEIrEiRaGR8PHxuaj9EolE0hC0GlGQC4tJJBJJ7bQaUbD0FFwx0Dxnzhzee+8963vLQjiFhYUMHz6cfv360bNnT77//vs656koCrNnzyY+Pp6ePXvyzTffAJCamsrQoUPp06cP8fHxbN68GaPRyB133GFN++abb9b7NUokktZBywuVOWsW7KsaOhvA01iARniAxv3i8uzTB96qPnT2lClTmDVrFg8++CAAS5YsYe3atej1epYvX46fnx+ZmZkMGjSIsWPH1inkxrfffsu+ffvYv38/mZmZJCQkMHToUL766iuuu+46nn76aYxGI8XFxezbt4+UlBSSkpIALmolN4lEIrGn5YlCI9C3b1/S09M5f/48GRkZBAYGEhUVRUVFBU899RSbNm1Co9GQkpJCWloa4eHhtea5ZcsWpk2bhlarJSwsjGHDhrFz504SEhK46667qKioYPz48fTp04dOnTpx8uRJHnroIcaMGcOoUaMa4KolEklLpOWJQjUtegGUFOzC3T0CD4/Iej/tpEmTWLp0KRcuXGDKlCkAfPnll2RkZLB79250Oh3R0dFOQ2ZfDEOHDmXTpk2sWrWKO+64g8cee4wZM2awf/9+1q5dy/z581myZAkLFy6sj8uSSCStjFY0pgCqNLjG+2jKlCksXryYpUuXMmnSJEANmd2mTRt0Oh3r168nOTm5zvkNGTKEb775BqPRSEZGBps2bWLgwIEkJycTFhbGzJkzueeee9izZw+ZmZmYTCYmTpzISy+9xJ49e1xyjRKJpOXT8noKNSJcNqM5Li6OgoICIiMjiYiIAGD69OncdNNN9OzZkwEDBlzUojY333wzW7dupXfv3ggheOWVVwgPD2fRokW8+uqr6HQ6fHx8+Oyzz0hJSeHOO+/EZFKD0b788ssuuUaJRNLycVnobFdxqaGzKS+nJDMJERCM3quDC0vYfJGhsyWSlktdQ2e3HvNRYSGe502ICmNjl0QikUiaLK1HFCxuoM2sZySRSCQNiRQFiUQikViRoiCRSCQSK1IUJBKJRGJFioJEIpFIrLQeUbDgAlHIzc3l/fffv6TPjh49WsYqkkgkTYbWIwou7CnUJAoGg6HGz65evZqAgIB6L5NEIpFcClIU6oE5c+Zw4sQJ+vTpw+zZs9mwYQNDhgxh7Nix9OjRA4Dx48fTv39/4uLiWLBggfWz0dHRZGZmcvr0aWJjY5k5cyZxcXGMGjWKkpKSKudasWIFV1xxBX379mXEiBGkpaUBUFhYyJ133knPnj3p1asXy5YtA+DHH3+kX79+9O7dm+HDh9f7tUskkpZFiwtzUW3kbJMeirph8tDUd+Rs5s6dS1JSEvvMJ96wYQN79uwhKSmJjh07ArBw4UKCgoIoKSkhISGBiRMnEhwc7JDPsWPH+Prrr/nwww+ZPHkyy5Yt47bbbnNIM3jwYLZt24YQgo8++ohXXnmF119/nRdffBF/f38OHjwIQE5ODhkZGcycOZNNmzbRsWNHsrOzL+7CJRJJq6PFiUJTYeDAgVZBAJg3bx7Lly8H4OzZsxw7dqyKKHTs2JE+ffoA0L9/f06fPl0l33PnzjFlyhRSU1MpLy+3nmPdunUsXrzYmi4wMJAVK1YwdOhQa5qgoKB6vUaJRNLyaHGiUG2LvqQc/jhCWaQej4h4l5fD29vbur1hwwbWrVvH1q1b8fLy4uqrr3YaQtvDw8O6rdVqnZqPHnroIR577DHGjh3Lhg0beP75511SfolE0jqRYwr1gK+vLwUFBdUez8vLIzAwEC8vLw4fPsy2bdsu+Vx5eXlERqrrQSxatMi6f+TIkQ5Lgubk5DBo0CA2bdrEqVOnAKT5SCKR1IoUhXogODiYxMRE4uPjmT17dpXj119/PQaDgdjYWObMmcOgQYMu+VzPP/88kyZNon///oSEhFj3P/PMM+Tk5BAfH0/v3r1Zv349oaGhLFiwgAkTJtC7d2/r4j8SiURSHa0qdDYHDlAW4Y5HZC8XlrD5IkNnSyQtFxk6uzLWnoKpccshkUgkTZhWKArNq2ckkUgkDUnrEQULUhQkEomkWlqPKMiegkQikdSKy0RBCBElhFgvhPhTCPGHEOIRJ2mEEGKeEOK4EOKAEKKfq8pjLwrNbXBdIpFIGgpXTl4zAI8rirJHCOEL7BZC/Kwoyp92aW4AYsx/VwAfmF/rH6soWP4Jl5xGIpFImjMu6ykoipKqKMoe83YBcAiIrJRsHPCZorINCBBCRLikQEJg6R8oTcADycfHp7GLIJFIJFVokDEFIUQ00BfYXulQJHDW7v05qgpHPRYEc0+h8UVBIpFImiIuFwUhhA+wDJilKEr+JeZxrxBilxBiV0ZGxuWUBpT67ynMmTPHIcTE888/z2uvvUZhYSHDhw+nX79+9OzZk++//77WvKoLse0sBHZ14bIlEonkUnFpQDwhhA5VEL5UFOVbJ0lSgCi79+3M+xxQFGUBsADUGc01nXPWj7PYd8FZ7GygsBBFq4DeGyHqrod9wvvw1vXVx86eMmUKs2bN4sEHHwRgyZIlrF27Fr1ez/Lly/Hz8yMzM5NBgwYxduxYhKh+PMNZiG2TyeQ0BLazcNkSiURyObhMFIRa830MHFIU5Y1qkv0A/J8QYjHqAHOeoiipriqTjfr1Purbty/p6emcP3+ejIwMAgMDiYqKoqKigqeeeopNmzah0WhISUkhLS2N8PDwavNyFmI7IyPDaQhsZ+GyJRKJ5HJwZU8hEbgdOCiEsDTdnwLaAyiKMh9YDYwGjgPFwJ2Xe9KaWvTKvr1U+BjRRHfFzc3vck/lwKRJk1i6dCkXLlywBp778ssvycjIYPfu3eh0OqKjo52GzLZQ1xDbEolE4ipcJgqKomyhFr9PRZ0w8KCrylAFIVw20DxlyhRmzpxJZmYmGzduBNQw123atEGn07F+/XqSk5NrzKO6ENuDBg3igQce4NSpU1bzUVBQkDVc9lvmRSRycnJkb0EikVwWrWdGM1hFwRUuqXFxcRQUFBAZGUlEhOpVO336dHbt2kXPnj357LPP6N69e415VBdiu7oQ2M7CZUskEsnl0HpCZwPKwQMYPMqhYzQ6XUit6VsbMnS2RNJykaGzneK6noJEIpG0BFqXKFhdQZtX70gikUgaihYjCnUyg1nHFIyuL1Azo7mZESUSiWtoEaKg1+vJysqqtWITQiCk+agKiqKQlZWFXq9v7KJIJJJGxqUzmhuKdu3ace7cOWoNgXHhAibKMVaUotMVNEzhmgl6vZ527do1djEkEkkj0yJEQafTWWf71sjMmeSV7SFl0QRiY79wfcEkEomkmdEizEd1xs0NjUmD0XhJcfkkEomkxdPqREGYtBiN0nQkkUgkzmh9omDUYDDInoJEIpE4o3WJgk6HxiRkT0EikUiqoXWJgpsbmgohewoSiURSDa1LFLy9EaUm2VOQSCSSamhdouDri7aoApOpGJPJ0NilkUgkkiZH6xIFHx9EUTkARmNhIxdGIpFImh6tSxR8fdGUVoAROVdBIpFInNDqRAFAWwIGg1zkXiKRSCrTukTBxwcAtxKoqJCiIJFIJJVpXaIgewoSiURSI61TFIqlKEgkEokzWpcomM1HUhQkEonEOa1LFKw9BQ0VFdmNXBiJRCJperRKUfCo8JI9BYlEInFC6xIFs/lIV+opRUEikUic0LpEwdxT0JXqpUuqRCKROKF1iYKXFwiBrsyDiorMxi6NRCKRNDlalyhoNODtja7Ug/LyC41dGolEImlytC5RAPD1xa1ER0VFGopiauzSSCQSSZOidYpCqQZFMUgTkkQikVSi9YmCjw/aYnVTmpAkEonEkdYnCr6+aItVs1F5eWojF0YikUiaFq1SFDRFFQCUlUlRkEgkEntcJgpCiIVCiHQhRFI1x68WQuQJIfaZ/55zVVkc8PFBFJcBsqcgkUgklXFzYd6fAu8Cn9WQZrOiKDe6sAxV8fVFFBSi1frJMQWJRCKphMt6CoqibAKaXtQ5X18oKMDdPUL2FCQSiaQSjT2mcKUQYr8QYo0QIq5BzujjA0VFuLuFSVGQSCSSSjSmKOwBOiiK0ht4B/iuuoRCiHuFELuEELsyMjIu76zm+EeeplBpPpJIJJJKNJooKIqSryhKoXl7NaATQoRUk3aBoigDFEUZEBoaenkntoTPLg+U3kcSiURSiUYTBSFEuBBCmLcHmsuS5fITBwcDoC/wxmQqwmDId/kpJRKJpLlQJ1EQQjwihPATKh8LIfYIIUbV8pmvga1ANyHEOSHE3UKI+4UQ95uT3AIkCSH2A/OAqYqiKJdzMXUiIgIAfY4egLKysy4/pUQikTQX6uqSepeiKG8LIa4DAoHbgc+Bn6r7gKIo02rKUFGUd1FdVhsWsyh45LhBBJSWnsbbu2HGuCUSiaSpU1fzkTC/jgY+VxTlD7t9zYvwcADcs9RQF6WlpxuxMBKJRNK0qKso7BZC/IQqCmuFEL5A84w77eUFfn5o0wsQwkOKgkQikdhRV/PR3UAf4KSiKMVCiCDgTtcVy8VERCAuXECv7yBFQSKRSOyoa0/hSuCIoii5QojbgGeAPNcVy8VERMCFC3h6dqa4+Fhjl0YikUiaDHUVhQ+AYiFEb+Bx4AQ1xzRq2kREQGoqXl49KC4+jKIYG7tEEolE0iSoqygYzO6i44B3FUV5D/B1XbFcjFkUvL26oyhl0oQkkUgkZuoqCgVCiCdRXVFXCSE0gM51xXIx4eFQXIyXsQMARUWHGrlAEolE0jSoqyhMAcpQ5ytcANoBr7qsVK7GPFfBK88fgOJiKQoSiUQCdRQFsxB8CfgLIW4EShVFad5jCoAuqwR393CKi/9s5AJJJBJJ06CuYS4mAzuAScBkYLsQ4hZXFsylmEXBMtgszUcSiUSiUtd5Ck8DCYqipAMIIUKBdcBSVxXMpdiLQr9Y0tI+R1EUzPH5JBKJpNVS1zEFjUUQzGRdxGebHgEB4OEBqan4+PTCaMyXHkgSiURC3XsKPwoh1gJfm99PAVa7pkgNgBCqB1JqKr6+UwEoKNiFp2fHRi6YRCKRNC51HWieDSwAepn/FiiK8oQrC+ZyzLOavb3jEcKdgoKdjV0iiUQiaXTq2lNAUZRlwDIXlqVhiYiAo0fRaNzx8elDQcGuxi6RRCKRNDo19hSEEAVCiHwnfwVCiOa9ZJl5VjOAr+8ACgp2oyjNM/CrRCKR1Bc1ioKiKL6Kovg5+fNVFMWvoQrpEsLDITsbMjPx9U3AaMynpEQGx5NIJK2b5utBdLlcf73qgfTII/j6DgAgP39HIxdKIpFIGpfWKwoJCXDNNXD0KN7esWi1PuTnb3eetrQUfqp25VGJRCJpMbReUQAIDYWMDITQ4uubQEFBNaLwyCNw3XWwf3/Dlk8ikUgamNYtCiEhkJkJgJ/flRQW7sNgcLJ20OHD6mtOTgMWTiKRSBqe1i0KoaFQVAQlJQQFXY+iGMjO/rn69IrScGWTSCSSRkCKAkBmJn5+V+LmFkhm5ndV08mYSBKJpJUgRQEgIwONxo02baaQmfltVROSFAWJRNJKaN2iEBKivmZkABAefgcmUwmZmSucp5fmI4lE0sJp3aLQpo36euECAL6+Ceh0bcjOXuOYztJTkKIgkUhaOK1bFDp2BL3e6moqhIagoOvJzl6DyVRmS2cRBaOxEQopkUgkDUfrFgU3N+jbF3bZguG1aTMNgyHHuQmpoqIBCyeRSCQNT+sWBVBnNu/ZAwYDAEFBI3F3b0t6+ldV00pRkEgkLRwpCkOHqnMVtm4FQAgtISHjyM5ei9FYgnmn+lpe3kiFlEgkkoZBisKIEaoZabVtIbnQ0ImYTMWkp3+j7rCIguwpSCSSFo4UBX9/6N1bNSGZCQi4Fh+fvpw58y9MJoMtrRQFiUTSwpGiANC+PZw9a30rhKBDh+coKTlOevrXtnRSFCQSSV05dQr++9/GLsVF4zJREEIsFEKkCyGSqjkuhBDzhBDHhRAHhBD9XFWWWomKUkXBaLTORQgJGYu3dy+Sk19CsUxolmMKktbA0aPw2GNyXs7lMnQo3H8/lJQ0dkkuClf2FD4Frq/h+A1AjPnvXuADF5alZqKioLBQHVv4y18Adc5Chw7PUlJylIoKNZKq7ClIWgXjx8Obb6riILl0zBGYm5u4ukwUFEXZBGTXkGQc8Jmisg0IEEJEuKo8NRIVZdv+/HPrZkjIeDw82lFalqzukKIgaQ3ISZr1g0UMpCjUmUjgrN37c+Z9DU/79k53azRutGs3C0OFeR2FyzUfHT4MZ85cXh4SiaR50cxEtlkMNAsh7hVC7BJC7MowB6+rVwYMgJ49be8XLbJutm37AELrAYByuaIQGwsdOlxeHg1BSgr8+mtjl0Iiad400/A4jSkKKYCd3YZ25n1VUBRlgaIoAxRFGRBqCXddn+h0avyj//s/9f0dd1gPabWeeOk6AlCUuxulmXUFL4m+fWH48MYuhUTSMpCiUGd+AGaYvZAGAXmKoqQ2WmmEgM6dbe/tPAbcNaoQZaetIjX1o4YuWcPjit6YRNLasDQgTabGLcdF4kqX1K+BrUA3IcQ5IcTdQoj7hRD3m5OsBk4Cx4EPgQdcVZY64+dn287Ksm6KCnUCm9bkwfnzzc/vuMVhMsHHH8uBf1fTGnrFDUEz6ym4uSpjRVGm1XJcAR501fkvCS8v23ZmJrRrp26bg+X5ew3kWOFmLlz4gvDw2xqhgA2MyQSaJjjs9MUXcM89kJoKzzzT2KVpuTSzFm6TpZmJQhN84huRSZPgrrvUbYuPMVhbpN66rvj5JXL8+KyqS3a2RAyG2tM0Bnnme5/aeNbGFs0lDJBe7FiboiguH587nXuakopLnzhWZrCtqVJUXsSu87scjhvMIXCMplruk/k+1na9qQWpnM07W2OahsBlPYVmiVYLjz8OCxc6FQVhMBITM4/duxM4cGAMvXuvRav1bqTCNgAGA7i7N3YpqqLTqa+XIFpGk5GjWUfpGNgRD60HCgoaYWsbKYqCEILc0lwOZRyiqKKIoR2G4q51p6SihKNZR0ktTOX6LrZ5mQaTAYPJQFZxFnsv7OXGrjcCkFWcRUZxBt1DulvP/e/N/6ZXWC9rhTIhdgKrj63mSNYRogOiiQ6I5nTuaSbETiC/LJ8VR1bQO7w3caFxvLvjXW7pcQtJ6UmsPLqS23vfzobTG2jn1w5vnTc3dr2RcmM53x/5nr2pe4nyjyI2JJbE9om4adyqXOfG5I0EewbTOagzi/Yt4qZuN/Hxno8J7ZLDzGNwOOcwP2z6gaeGPMXeC3sJ9Qolyl/1DSk1lLJw70Imxk4kuySboZ8OpYN/B8Z3H88zQ5+hsLyQ5zc8z4/Hf+SRKx5hWs9p5JXm8cupX/hg1wecyz+Hr7svg9sPpndYb6IDonn616cZ1G4Q5wvOk1aUxhc3f8F7O9+jqLyI3LJcAD4Z9wl6Nz33rriXY9nH6B3Wm+eGPYdOo2PikokYFSNGk5FQ71BWHl1Je//2LBy7kA92fcDomNGM7DSSQ5mHeGjNQ0yNm0paURoVxgpiQ2PZnrKdo1nqhL1RnUYx97e5xIbEEu4TzvrT6wF4eODDBOgD2HxmMzvP7yTUK5T8snzu638fR7OP4qZxI680j/v630dYWxOvJsCPX8QyKOpKfjvzGwPaDiAuNI6UghT0bnoifSPJKsmiU2AnPtn3CRcKLzCz30ymxU9j1/ldxIbGMm/7PPan7cdN48YTiU/wQIJrLe2iuXnTDBgwQNm1a1ftCS+V9HQIC4N33rF5I3Xrps7uvPVW+PJL0tOX8OefU/Hzu4K4uOV4eITXLW/7ZT0vXFDnLAwcWH9l/+MP9TUu7vLysZQzL89xnAW1MqkwqSLprnVHURTm75pP56DOjOo8ClBbWG4aN7QaLaBWmgfTDtI3oq81j5zSHLx0XmQVZxHpF8m7O95l5dGV9GzTk9TCVAa0HUBiVCLzdswjSB/Ek0OeZN3JdSTnJnP1wQIS/+8/FNx9O09PDCC3NJfPbv4MUFtbVy+6mlJDKR0DOqIRGkoMJSiKwqB2g9iftp8NpzcAEOIVQn5ZPuO7j2da/DT8PfyZ8d0MJveYzC+nfmF/mroi39T4qSwcuxCvf9vMi6tuXcXZvLO8tf0tjmQeQSM0GBW1RXhF5BUMajeI7w5/R3JeMolRiejd9IzsNJI5v8xxuJ+Tekzif3/+r8pX4KH1oMJUgUlRTThtfdtyvuA8Pu4+FJYXOv3abup6EydzTvJHxh9Vjk2MnUiJoYS80jw8dZ4cSDtAelG603wqo3fTU2oopXNgZ44/fByArw9+za3f3uo0fcWzFby48UVe2PQC7lp3yo3lBHsGk1+WT4hXCKmFdevhdQvuxpGsIwDoNDoqTBVcFTmElMKzJOedtqbz0OopM5ZW+byX1pdiY4HDPj93P64KuYkfz39ZpzJcCv4ikjzFqSNlrehLoyn1SAbhWC8Lo56g7OuY1PNmPrjvL5eUtxBit6IoA2pNJ0WhEgaD2hL9xz/g+efVfZ07w8mTqnlpyRIA0tK+4vDhOwgJGU+PHosRog6WOHtRiIhQhaE+77+TtaQVRaHMWIbeTe/0I5nFmbz+++vM7D+TM3ln6Bvel+OdAuhzAQ4e2oB7UCi/nfmNkzknGRY9jAnfTKDEUELX4K4c/OtBpiydwneHvwNg691b+WTvJyzYs4AXr3mRjKIMhBC8vf1tAA7+9SBP//o0Pxz5AYFAQS1nkGcQ2SW2ye+WigRAK7QIIVAUxVrpApS/AL3n+HPIXTUl5c3J447v7mDrua1cKLxQp9vVJagLuaW5ZBYklJvEAAAgAElEQVRnOj0e3yaeUkMpx7OPV5tH1+CuTImbQpmhjJSCFHan7uZw5uFq04+JGUNcaBwmxUR2STYL9y0E1Ep72aFl1nTXdryWfuH9SClI4eukrx3y6ODfgT7hffn+yHdEB0SzcMyX/GvzS/xyxra2+AM95xDmHcHCpHdILnRe/q6+fWnv3ZV1F76x7nPDHQPVz8fxox09yu5AlAWw1e9v1aYD8CnpQbtzj3A45j6H/fqUEZRGrsN/0wJKOi+mPLLqnBjt/nsw9jZ7+n2zFA5PgHv7Q8ReW6LvPoH2W6Dfx5AyAA5NBJ9U2Hs3xKyCnQ/A0Jcg8TU1/ZEbodtKKIgAX7MwVehBZxaUH9+AI+PgEbMX4jfLYMwD4JMGB2+Fbz+HkEPwYLy1CLpFW/Et70r29TeCSQufbAKPAnjSv8o1ee17nOI+r6v38U0TSsI8CgbPckgTtmM+7TQJ5PlvodTjDOfaq+mvPbqXnEN9uP12ePTRGm97tUhRuByioiA4GLZtU9dwtkRRHT8eli+3Jjt9+gVOn/4HwcHj6NFjMVqt84oXUCtqy6CtojitwGujzFBGXlkebbzbWPdlFGXw+tbXiQmKIXLKPVx/3DHPcYvH8duZ38iYncGi/YsY0HYA5wvOs+LICr44+AW5pWq3PEAfYN0GeHAHvDcQ3DRuVlNHe//2nMmzzcj++fafGfn5SHqH9SYpPcmh0tYKrcN7sLV2nRHmHcaL17zIwfSDvHndm4z4fAQbTm/gw5s+pGebnoz5agxCCPpF9OOnEz9x6i3o8ojAXaenxFDCssnLmLhkIgD39L2HD278gPd2vMeozqPo8X4P63kifCI4/vBxdBodOq2OW5fdaq10X7j6BcZ3H0+v+b3QCA2GZw2kpSv87efH2JKptvors3poDiE+AZSXQ04OHDtVxu7yL/im6K8YFJt31AjtC5wVW5jhu4jcc+EcOABnM3LITXyAvuIOtJ7F/OA5AYB2X2XiYQymSxc4m3ueP2+wTfQXR8cSn/oqx0q3UHrd3XjsfpyyFa9B4Am47jHI7ahWhlld1Q94p8H9fcD3ApQEwCtZ0HYn3H2VWoF55sCtN6lpj4+C7Q+r7y0t1T8nIjwKUDr/5HDdnvseoyRuPm4H7sbQ/x2n32lgymTCz8/kUMJIh/1TjpSTU1xIu+BAdgc8xX6/l/Gt6EyB7gQAg4tfp59pJvN81F7qX5UkQkxxnC04zT7d++zzfhWAmcUn6d8pGpO2BG93L7RaNehAYCB4e6vb+88e4+m0rowOv4cow7X8N9Oxd9MveDB7srYA8OHI/zE0bBzdvlBNpp8NOsDje68no+w8n4z9nNGRt2E0whdJi/j773cAUPRkMZ46T/LzIS/fSGCAlvJyKHFLIeqtdg7n2nvfXvr+ty/Xdb6OH2/7kV9P/crwzxznA228YyNDOwy1vh/z1RhWH1uN4VkDWo3Woeq4WKQoXA7/+x9MnqwKwPjxtlb96NGwapU1maIopKTM4/jxWQQEXE337ovQ652HzKC8HDw8LB+sIgqZxZnkl+XTKbAT5cZy9qTuYWDkQDRCw9Gso3x76Fue/OVJAE49coq0wjQSIhOYv2s+D662OXGlvgbbdi7ncOZhroi8gms/uxaAjNkZhL5a88S/afHTqrRKa2Jq/FQWJy0mc3YmmcWZzPhuBiM6juBQ5iGWH15OiFcIybOSWX9qPTd+rdrZnxz8JH+76m888+szfLDLFgPR9JwJYfdrT85N5t0d7/LStS/h4eaBSTGhKAo/n/iVG74axcov4cbpMKPTE3x28j/c0ukelp5UW5Yv9vie9iVjiYyE0lK4cZea7wS3j2iXO5nCbF/atFGHS/44e5Yd2jcIPfAyURF6ysvhSE4S5UVeaPI6WaOS6HRgGDsdpafdMq177oYfqpm3Mn00xNha7jxve840GnUCfWQknDunTiAv9N9O2YxBAMR8ZSI+TnDiBLRtC2VBewl370RRnic+nu6cPw8x3cs4GPhvuuc+QlRIEEKo7Rh3d/WnZfHWLS6GjUXzWa35K8HEsLDvUcLC4Mw5I34+WtKL0plxMAwfN39+Hp6Lnx8M/EpLkc7ELX7j+WTmcry84PlfX+LF3561XsO4buNISk8i6YEk1p9az6NrH+VI1hHmDp/LskPL2Hl+J7f3up1/Xv1POs3rBMDSSUvpE96HzkG2+UBvb3ubWWtnEd8mnqR0NaDyZ+M/4/betyP+qX5vpU+X4uGmPjvZJdkEvxIMQNkzZbhrax/z2n1+N91DurP+9Hpu+loVwE6BnTiZc5IpcVP45g+1p7Tlzi0ktk+0nrf4qWK6vNOF8wXn2Xr3Vga1G2TN85eTv7AjZQdPDnnS6TlNigntC1qHfWXPlJFTkoOfhx+eOk8yizOrPJMZszMI8Qqxvi83llNYXkiQZ1Ct11kbdRUFOdDsjLFj1R7C2rXqJDZLeItKfvFCCNq1ewSdLoRDh/7C9u0x9OmzHn//qwDIK83DYDIQ7BXML8d+ItEN9AZ1wHFtDPiWwXOfXk3noC5kl2Sz/PBy1kxfw9Gsozzy4yME6gPJKc2pUryOb6szrN217ggEejc9CW0T2HxmM+8MhP8suaVKK72y54Q9625fR+egzkQHRPP3xL/T97+q7V+v9aDUWOaQ9qnBT/HvLf8GYHHSYmKCYgj2CibYK5jt92wHYPZPswG4JvoaNEYvrgq3tRQH+I7lzJEgRFZ3675O7lfw7rsCo1HVXoALFzqQmfkqI15Ub39amoayMjAGdICpcHvEo8CbfPZmVxgazdK9P4I/cL4/z75wA9h7Uz6vvnz7aVs8zvqi0djmJkZGRtGhw5vofeDgQQgIgPh28dahlG7d1Jbn7t3we7iWI3bZThgVzn0PQlmZ6qMQGgpBQWqw3Zk/RPKzXTjIlBR1f0WF+tMKDnb8Ds7mtaX9W+r20SOVm4J9nXxrHsA/neyvSq/j0az+EtqH+zB2rLrviissFVYbUtq8zMhOI+nfVt3jbRQU6SDSV4uPj7ovJtQxPMv3R75ncPvB6N303BBzA+/vep8jWUeYEDuB0TGj6TW/F6NjRlsHpgEm9phYpWzhPup4nFbYKtBQb7WifGrwU/x6+lerIAAE6gOt23URBID+bfsD4OdhGx/rFdaLkzkncdPYqsC2vm0dPuep8+Sa6Gv48uCXdArs5HBseKfhDO9U/ax/jdCgNYFRA3Ni7mLGyL/hrnUnzCfMmibEK4Tvp37PuMXjgKqCYLnG+hCEi0GKgjM8POCqq2D+fPXPQjWTpcLCpuPp2ZV9+4Zx7NhDdO36Pn5+V3Db8ttYeXQliycuZuqyqUwZD14V8MmLbjDd/OHkjWxI3mjN65N9n1hd4ZwJgj0Wu3vvsN78PfHvbD6zmX8PBewEYVC7QWw7t42tZ7c6zeOqqKu4Ovpq66BwB3/bwx/nH8PubMflMHrn/oO5AX/jnzkxlIgsfNJGcf/9auTx9HTVtX1fiBvEwo+LO+I3HYxGd3hO/fzEwb2gAohvA7cASVM4+f1CHjbfWp1O7Tzp9dCpE/j6qmaZhAT1uFG0ZzngGf47OcATDwfxa0YCO4vVwdo3h88nfpIOf3+1TF5eMOhH9bMbV4czuLMqMoWFamXetm3du+O/nPwLIz7/nJigGI5lH+OahHBGVeMn8OzYGfz8qdqL8NB60Lat83QWLJWjK7BUaOO6jXN6fM5gx8Fvb4MGMOKlsVW60QHRALx0zUv8dvY31hxf4zBO9em4T1lzfA0xwTEAZP89m0BPtQJ/IvEJroq6yum5LZWkvWdUqJcqCv8a/q8q6cWl2k5wFAXLOezPG+GrBmledesqsorVyasLblrAI1c84mCyrSsWC1yYeyCxobFO04ztNpa5w+dSVFFURRAaCykK1TFqVNWgcHYB8RRFYX/afnqH9UYIgd67N5vLJ/L5ri8Zf3wQd/S5g5VHVwIwddlUAL6Jpwo3d76R5SdWWt//cvIXFBQGtx/MljNbePzKx3l9qzrY9Pqo13n8p8er5JFWlIavu6/TyxgcNZht57bxwqYXnB6/K+JtVq3UUloKycmwa683dFOPHdsVD50cRWHKLe6AO4y4Gwa/wt75D5NsHi4RQm0pxwbdwhbmMsRvBj0fU1vIlsf70w+98PMDH/8JLEqew/Buf6P/s16EhICnp9pSt9xmDw+coCf8BV/04WqvZPQ1QQSfS2DnOlUUxo8KITqg0kfMotA1IgKNRhUcfQ3DP9UxvNNwlH8ojPlqDMeyjxHmHVZt2iEdhlDxbAX/++N/Vq+rmtBpVTfbAH3lwl8+XYO7cujBQ3QN7lqn9G7mXpa3sH0Bg9sPZsc9OxjQdgBJ6UmsOb7GwT8/2CuY23rZJnRaBAFg7oi51Z7LW6e6dNu3hi09her4+fafEVy8ONg/I4PaDeLDPR9yZbsrWXF0BbmluVaRGx0z2prOS+dFQmTCRZ/LHm+N0x+ylScGP3FZ+dc3UhSqY9w4mOPYgrLMXdhyZgsPr3mYvRf2MqzDMP521d/QCA3P/PYFAP89qSGv4tNqs/785s+5ffntACSE9OJw3gkOZR7inr738NFetXX5+JWPs+SWJYT7hFtFoV9EP3bN3MXqY6t5bsNz1vzu7/cguenOReH3VV2gcu/TqAOt2jS/Z3oQ2HVIoqLcraIQ5daFys6NW7ZA167g4/cSKUUzaf9kFydTGfoDjmNVE1P3YDAZSLCOmbozkpedltm5GNgIFT4kBamuhoH6QIeH1llryyKw9dUSs4zD2Zs1nOGmcWNazxon9juw7759l9QirQuWuRIXgze2L1YIYb3PPcN6snji4suuLEE17Tw95GnuH3A/Me/EUGootbbiq2NEpxGXdC77nsK4buMY+NeBxIXGcVO3m0jJvzQX0pqwyJaXaIJzfWpAikJ1dOumisIrr1in+69wO8mGHx/lnZ3v4a9XXc42Jm9kY/JGJh/XQxd1kOyuH+5i/knHEAHhnqFcKFEDzbXzs3kl+Gu82H7Pdr47/B1T46eid9NjVIyM6zYOIYSDxUrkR5Gd1pntPyWD2dZ75bpSXnzJHaPfcXi46mUc+C2Cfl5p7Blua9V28RrI8bLfAFj8aSCdItRWekSEausWZlP1I2MF9+6DvuF92XtBdQVMTLTkoqOLR5c63866tJbrSiC2Zn6QZxDRAdEIBO5ad2vL055Vt67idO5pB/vx5XBLj1tYc3wNPUJ71J74Iugd3rte87tcvGuozKbET6mXc2iEhpeufQmA3+/6nRVHV+Cp86yXvCvj62FrOAXoAwj2Ugd22vm1c3gm6xv7HldzQIa5qA4h4OWX4V//4kAYHAmGsZMNvLH9LYZ2GErSX5PoGNDRmnxJl1I0JpjWcxrFTxU7tEoAEgNtFWiYl6016KfxxB1frg66nQP7dIyoeIdhhe8zbZpg6FDVnEK6Wvlc3T+SUaNg1VJb19xQ5sGDDwjeesV5T2HddxHs3uTY+lw4zdZCn3STPwkJEB9fdfDTR3hges7EN7d8Q1MiULE9ZEGeQfh6+BIbGkuIV4hTm7Ofhx+9wnrV2/nv7HMn+XPy6RJUd1Fsjtj3FBqCvhF9eW7Yc7UnvEQ8tLbfjWUMzZVYxhS80Ln8XPWJ7ClUg6IovLXtLQ52TuKTv8I1ZW0B1cd+8S2LCfEKYcW0FWxM3kh5YR6PbnoKjYK1Nbr61tU8tvZRgt0V1pzeRccS20Dv4b22H+eaNW14YAIUOE68JDhYnUw8cSLo3NbhE7yX2Hf0dOgAAd2CuMq8auiOHeprUbkvs5xYYyL91MGzq6Ku4vezv1PxbIVDi1lTw6Q7H+GBEMLBPtwUCDLpQYDeKKytyuk9p3Mq51SDnF8I4dDqbGlY7PUNLQqu5nIGqS+H2sYUmhpSFJyw5cwWRn0+ihKDLZjWeg9VEPJC38DPbJuOaxNHXJs41m1WV2oz2DU+Etsnsm7qDr5cWoCS9SuHD52CWHUq4oRxoWB2b/5qWRQ3XQ033aQKgbe36ms+ZIg6QKsSYf5TOZtX1UXNS+fl8N5P50N+RaF1MPSXGb9QUlFyUSYUX9Qfs2Xws6m0jANN7qCFsDLbtTw15KlGLFHLxLuZtXCbGtYxhWZ2H6UoVGJHyg5u+/Y2B0GwEF3oht/qxfDXWQ5+jD0V28DY0qWqT/uBA7B9O2Rl+eLvP44K3z/A7JX293tn8Yo5/eJnzzH5roubpeis5V65FfRT4nz84/pbvVr0bnoHF8KrzsDJWjoAPuaWopvGjZXTVtbruMDlEGjUgRbaFrneBNAasYQf8VJk9XA5WMxHza3HJccU7Fh5dCVXfHQFyXnJvDbyNZR/KKQ8lmJtbbcPiFbtNU89BaWlmExq5b9+pepj7nFgIpMmwdy5aqikxETYtEn1sz++yBZ/5r7ptsrMu3AGhw//BUWpe+x6bze1VxBcXH2attqAGj1OflsIqa/XfB4fux/zmK5jqkzuaSwCDarQ+ZXVklByaQiL+ah5tXDrwsovYf2nDXvO5iauzau0LqK4ophHf3zUwb5u8elu69vWaprxbd8FOE7R3Hl89Wci/z5wI6dPA/Qjyu0wfY1J3LtS9dW/4grH9Wn8TLYHrFO75wHV9TTKdwRpaZ9hMOQRHn47oaFVZ31WRhgMrPwS4tOB/5h3VloQJVC5BEf8SvgodpXC2rXQuzeEu26SVV3xM6g31q9ELgLjStxNjWODdyVjjjXcuSx3ryYvrqaIFAVgU/ImFuxZ4LDPMjMTbPZ6D50/n/z9EC+9ouPkD2r8lkcfheF53zJy4VTcqYDRJrWltXYt3HADrF8PH3+M1xQ7Fz47P9PY8Ic4G9Wbs2dfJSvre/z8EgkIGEqHDs+i1VbjmldaWvXHbZdn+ivgM+zyTStWUVAUuP566NIFjjXgU1UNJeaZ3L6lzStuV3PBx6BWZxqTvL+XQ/8Lgi1RCp7NrKcgzUfA3tS9VfbZxzrRm801a1d4c9cr3aFtW37lGk7Qide9n2PMgZdVQQA1AhnAs8+qlekbb8DnnyOOHrVlbleBuxsVOnd+haFDy2jX7nGMxkLOnHmZ338P4+jRBygvdxLzvsTJalIVFUz4Eyb8CaHFqParpUsv+l7YYxUFS3mPVx9CuiHpV6IOfE84JCstV7Dst3Y8uxG6iponkUlq5of/ubHxE9ArzWvsq3lJmAsoKi9i74W9DqGjnxz8JO5ad4xGWLQIDu7xggjw0/uw7Ee49lpPdG/eAE88AS+96JhhXp7qQpRrDkN93hwq+sQJsDxjFRXs/wD2RADxaqtXo3GnSxc17ntOzgb+/HMy589/QEnJSdq2vZ+goFFotWYPo9KqC4pQUcGyJXbv777but/OjalOdMmC48HgZjRXunbhPZoCCYV+FP4LvA3Na+3b5kLHInde2AVMlea5yyGwBIbm0+zWaG7VonCh8AIRr6uunlPjpzKy00i6BXcjsX0ieXkwYwb88AP4P+BJOXDnrT5cZwmMOGmSKgqgzn5+/nmYNg327IGZM22taou55fhxB1HolQa90lCjslUiMPBq+vffTXLyS6SmLiAnZy3u7m0JCRmHThdKRP4QqowYVBOsj5ISNarcDz/Azp3w4ovO09nx20KzZ9Jr5uUunZSxUamowLsCQFEfOG3zaok1G5pZZdZkMTUvcW21olBuLOerg7bY+HOHz6VDgBohdPVqtc4vLYV58+CnYDdWHgMfdx9bBh07ql/2mTPg72+bRfbSS6ovqgXLIvMnTnBklxollevszD/VtML1+ii6dfsvHTo8TXHxEZKTXyIj439UVGSSn9yGKgERqhOFoiJVFMaZI2TWQRTaFKl/1jWQm1hPweFay8vVGB15eap4tXFN7KBWiRSF+qGZ3cdWO6bwj/X/sEYczf57tlUQ1q5VZxF37gy//w4PPQQmc3B+B1EAdUC5Qwc1FsWgQWroTXtBsOfECbpmQbt81LjNFmqpcPX69gSdDqbvD9eSmJhB//57oNRWKf7xx1RyczehlDsxKYFtjMNC5XGBmn6whibaU7C/Z5btDh3UtbUll49lzkszq8yaHM30PrZaUdhxfod12zIZbMsW1QLUtSv88ovqVgpYF0+vIgr2+PnZWuPt28Nnnzket1/hrqjItl2XVvgVV6jmqfJyfH37Et/lc+uhjIxv2LdvGPt2DnH+WftzAcTEqB5RFqrrYUDz6ClYti09Mkn90cwqs1ppaDOO5ZlvZuajVisK9usOg1pPDh+uWoKWLlVX0bJQJ1EAeMG8ZsG4cXD77WrYUWfUJApHj0KS4xoG1srZHCBJW2772q66Ko1Onf6D1uQY5sKCUlkUwGbqcnZ+eywVblPrKVQ2H0lcQ0sThca6nmZ2H1vdmEK5sZzbvr2NTcmbGNttLIvGLyI3F6ZPV01Gv/2mLr9oj0UUvN2rhmV2oGtXOH3aZtf294fU1Krppk+3bZeVqT+agwfVAnQzL2ZQXq4uQ2ZPXp4aIMnO+8jdvQ3t2/+dqOyRQL8qpzq8ZypuwTcRY7/T4hllOU91NCfzkaT+aejKzGRSBb+2BTUuFcvvGRzXSXc1zUwUWl1PYXHSYv73p7pKV682vQjQB/Df/6p196JFVQUBbIuq1GlN2A4d1IFPsJk04p0suWahvBwmT4a+fW09DYBVq6qmzc9XX524pIpq3DNNBemknHnXYV/ByR9tb2qq8A0GNY6HM2FrTBq7p1BR4WiCa6k0dGX2wAOXtiReXXFmdmwIpCg0bZLSk3DXuvPNLd/w2JWPsXOn6jA0YoRtHeDKWHoKNYWZdsqcOXD//fDMM9Wnyc6Gb79Vt1essLVedu+umrYGUajuR96l7Vx6tJ3vsK/s3D7r9snDfyMnZwOlpcmYTBWO9k+DQQ1tYVntvalQ+eFWGngS29NPw7XXOprhWiI1VWbffKPOvalP/vtf9dW+RV+f2OfbkKLQzMYUWp356Fj2MboEdWFy3GQURQ1T4e+v9hKqw9JD0IqL9Id/2LwUmqXycHeHqCi1e/znn+q+rbZ1FjhyRDUfFRWpYwu33AL/+IfteF4efPWV6hZlYdIkVXgqm5rMeBh8aZPvuEJYcHl/QBWdjJTFnNEuBkCnCyPE/0bLapwo5WWXsBJuA1Bers5NMBrVbftxE5PJMegUqPf/hx9U9a8P/jAvUpqRUT/5NTUsIludKBgMMHWqOmZmmZxZn5SVXfSEyzrRWKLQzHoKrU4UjmYdJSZItbBv2KCOIbz7LrStIQDoR2M/4o2tbzC4/eBLO2l8PFx9tbqS26BBsHkzDB2qHtu50zFtTIzae1hinp6cYrd2bH4+3HabY/qlS2HNGvj+e+fnLi5Wz2mHSD5j3e4Xv5H8DsWUlZ0hO/snMlK/torCucP/IurirrRhqKhQZ43n56uiUHmMpLIJwuJG9sILVQXjUrBUmg3dQ2koLC3b6iqztDT1NSvLNecvK1O/3/rGXhQa0uzYzEShVZmPjCYjJ7JPWCOg/uc/amPHEhGiOtr5teON69649CX8vLxUG7Slch4yRPUkijUvsNC1qzoWAaootG9v++y2bbbtyoJgITi4+i63M+8juxauTvEmOPh62ra9l/j4pVzR/5D1mGdpsJOPLic7e511nKVRKC+3VRoVFY6i4My0VpdjEhuWSqy6yszSOwiquthTveCqCru6MYXNm+Htt11zTpCi0JQ5m3+WMmMZMUEx5OWpcxFmzHDt2Fa1+PjYZhe//DKcO6du33SToyjUhdDQ6rvDltacZQ5FZSo9gDphW2YyxK3q3Ic//pjAgQMj2fZrOCe+GEZJyUmys9dx4cLnlJaew2g0V7z2PZz6pqTEvHg1VXsKNQ2cOwskeClYxn1aak+hNlGwOB7UhygUFKiTDn/5xbbPVd5u9g2nM2fUZ3DvXrXXPmtW/Z+vmc5TaFXmo2NZahyirsFd+fln9TcyZkwjFmjiRLVC8/dXB+4OH4Zrrrm0AbzqRMFSOVdnH6vcKrOvCOwrWzO9eq0lPX0JYQ//QODPm/jNuzMVdh5bfn5X0v3P8Xjd/gTKL+sQ15qDRX30kfqwP/hgHS+oBoqKVPddS/ntZ23XVKEUF6u9qvqiLgOipaXqeE9zis/UkD2FpCRIT1cXrrJQmyj07KmaAffvv7hz2X9f33+v/o7eeefi8rgUauspbN6sXvOIEa4vSx1waU9BCHG9EOKIEOK4EGKOk+N3CCEyhBD7zH/3uLI8x7JVUYgJjmHlStX99MorXXnGOuDvr75OnKh6tYA68e2nn9QHxn4dhurIyLCJgmWswoKlB1JXUbB/cJzMEg4KHEn37h8RuEtN10n7V6KiZhMXtxx//2Hk528lc7UaKPDM12M5cuRejhy5Tw0S+H//R07OesrK1ErFlJ5KRUVO7ddnj6Kolbulp1BRARcu2I7XZCKqr55CXc5lwdOzerNfZbKy6r+MoFY4EyaojY664EwUsrJg8GA4dcomCpbfbm0IAffeW/0xcOx11SYKSUmqq/TFUttAc333/Ooa5mLoUBg5sn7PfRm4TBSEEFrgPeAGoAcwTQjRw0nSbxRF6WP++8hV5QF1kNlb502YVwRr1sB117nGyeGy8fBQfyRxcapb1OzZ1acNCXEUhY8q3UJLTyEy0vnnZ892/NHaPzhOegpWETFXiBFl19C58yuEho6nb98NDBx4hKD2kwHwNIWTmbmczMzl1o/v338tW7dG8sdX3dGEteXk85Hk5m4iP3875eV18OYpLVUfXsuEkvJym/CB681H9r2S2vKzzExfvLhueYeEwFVXXVq5auL332H5ctVLrS44E4Vvv1W9Ml54wXa/L8Z19MMPne+3VJz2JhZXmY+czW+5GDG6VJrZmIIrq8SBwHFFUU4CCCEWA+OAP114zho5nXuajoEd2b1bkJ4ON97YWCW5CDw84JVX1NHw7uY1l3NyVA+l3CIq1ZsAABsrSURBVFy1R/Hkk2rIbnB0TY2NhUPmgePqegr796tzIgYOVN/XJgqlpaprraWVnJzscNjLqyuEJABLaOM1mjaJlu65+vBHhM0kr2AL7odVD5aAneXs2zfMnEZLcPAYFMWARuNJTMzbGAwFCCHw8jL7RFkGzu3HFBpKFL79Vu3RWQS2pp7CsmWqS/HFsm9f7WkuFkvLp66VuDNRsMzST021NTTq0lOqrfXtrHJ21UCz/fU7uxclJfU7wCjHFKoQCZy1e38OuMJJuolCiKHAUeBRRVHOOklTL+SV5RGoD2TVKtUkef31rjqTC+jWTW157t2rVoiWStFiV33jDfVVp1PdWb281NXXLHMlquspgKNrYW2ikJWl5m/5wauLVDtiiQJrjtXkcBlhcyE2CA5/BvyF4OAxxMXNQAh3srPXkp29htLSkwBkZi6zfq5t2/sJDLwO46k/CAcMPkL98VYWhZoqqsoRYy+WFSvU17pUihaRbgpYvqu6ioIzl1TL9pEjtoZAXVrWtVXwFqF21mLPzlb/unSp/Tx1oS6i4CykwaVS23yPJkpjG09WAF8rilImhLgPWARcWzmREOJe4F6A9hfrmWNHQVkB4T7hrF8PAwbU75hjgxAXp/7ZM3GiOlBnMqmvERHqhDZQKy+LKISHV5/vsWPqwG3Xro6T6ZxVonfeCZs22d47EwXLzGt7W7+F9HR1gNL8oLgt/oFQnzbw4YeEhNwEqGFFSkpOkJ6+2PyRxZw/P5/z5+fjlQzhQHL+e3QGMs5/g/+pPWgDPdHmlJB25hNKTm8gLOxW9PpohP0s9MvtKVRu9daU38XOynXlZCpL76qu53DWU7D8Fizft4eHoygqitoY8PVVf4tvvw333FP7fahJFOLj1Z5Jfdn6nc1TsM/bUpa77lIbVBs2qOFmHn1UbZC51yHMjT21zfdoorhyoDkFHOY+tTPvs6IoSpaiKJbmxkdAf2cZKYqyQFGUAYqiDAgNvfR1Y/PL8vHz8OOPP6BPn0vOpmnh56euAPfkk/Dmm46Ts+x7B5aeheXVnr/9TTU17dih+ujWhP1ku4CAKuYjwCYKlgFJ+4cx3bzmtH0votI4iGou6kJ09DNERz9DQsIBrroqjd69fyXMVxW8oE63ApCTtgZNSgZFYeoDnXZmIadPP8v27Z3ZujWSbdtsa22fOfIihYX7KS/PoLj4CBUV2Zc336KmnsLFisLl9mJqwiIKl2M+qly+bt0cr//ll9XfYmamWpE+9pj6u6ztuiwVsbMxBWcxty7n+7IXRfs1TSqX5ZNPYONGdfv++9VG08XO3FaUZttTcKUo7ARihBAdhRDuwFTgB/sEQgj72NJjgUO4kPyyfNyMvmRnQw9nQ94tkQkT1Fc3N3X84cAB1Ud7uW3wlwpzzKNrrrHtq87d0L51nJCgthwrP6gWUThjnjltLwAWUXBmmqoGITS4u7chMPAaOoT8HwCBHW8GoFPYP3ArAs/YUQD0/Kc7QV7D0Wi8cHMLoqIi25pPUcZ2du9OYNu29uzY0Z3ffgtm40YNhw/fRUrKfNLT/0dKyvuUlaWiKEZMpkqVaOXrrE9RcDbJsL6oTRRefVU1S1qoiyi0b+9oPvryS/X1wgWb11pubu2iYLmHNQ34OmvNXwr212/5jdaWt+X4xfbk7O/dkSPwxRfq9scfV29abMjQGzXgMvORoigGIcT/AWsBLbBQUZQ/hBAvALsURfkBeFgIMRYwANnAHa4qD6iiUFbgB1S1wrRYliyxPXj2bm8+ldaGaNPGVmEDPP64zUW2OgYOhJ9/Vu2+wcFqCzEmxvbA5eWpg+L2onDWPGR0EaLggKWSMfd43LLUFp8uqjvwE6K0nF55T6CMGG5nOlIHubtE/htdZCYVFekI4UFJyVH+v70zj7OiuPb478ydfZhhGWeQDIRhiwZcMBBAEoNiiBpCcEFUeC6J5hlXXCKKCb6o4aGRPPFFQ1QMoOACKKKgog4IKnEZQBFk3xlmYRbmDrPe5eSP0327+947q8wM0Of7+fTnVlfX7VtdM12/qlNVp6qqtqCgYA4KCuaEfmLnzrvA7Edc3CnIyroNMTHJqKzchJ6V2+HYtaKhCqq5L3h79RR8PmDyZGk0mHluiih07ep02mjOIvL5rPvExjYudtHMR+HjEDU1ludhez58vnp9fkXF/vxRxrsaFIXmira97JYskWPiRDGpmff1+535j4+XdyYtrXm/dYxp1TEFZn4HwDthcQ/ZwlMATGnNPJj4g35U+6tRWSoF7pqegscT3Y9MWprMyR07Vpw/zZwpLfuCAllIVFMj3l2TkqwX8fvft1r/APAjY/+GffuAV16RvUuHDHEu1Nq92zmjY80aGbtozB33gQNil54+3fnimC+nOSBojlvYNzQ6etQ5lmAQ509C375/c0Yyo3TXYlQnFSEurivi4tJRUvI2PJ5UlJS8hb17/xxK2tkLhygU7vsX9n7+PlJTh6Bz55EACD5fETIzJwCF36BZ81iaW+lUVclCwOnTGx4vst87miiUlkZea4ooJCU5e0qmKFRUWPFxcc7vDRsmleJNtuVIDZmP7Pk3RcFeTtXVjYsCs/SExo2LLgr23w3v+dmfP9zcdOSImMrqGwSPZjIyG0smpaWRaQoLnaLg90u+whtxrUh7DzS3GRW18k9wpDANHTvWvymaa/B4gPeMfRVuuSXyelKSvIBHjljTWYcOdYpCr17y+cYbwLRpQPfulkfYPn1kZfYjj1grtE89FXjzTTnCqaqSGVMmt9wiPY8xY2RWwBVXiICZL4e5cCqaKByoZwJbtNb488+jy803i1fafuIosXPnC4zHexjBYC2Ki5ciJiYBKYmPwvQuCwCx/gQkJvZGWdkHKCpaEIrfvfsBnJkHhyhs2HA+mH2Ij89Ep04j0amTLDL0eNKQmJiNurJdMLeWCdR64UlopLW4aBEwd66MIb3wQsNpzYo02kyg4uLIuHBRmDIFeOwxZ5rERKm8N24EJk2yKvdPP7VWJ4eLwuefy/9FU0TB3nM4elTWcACR60Qaa1Xn58vYxksvOfcrieaGPrynUFFh5SNcFIYOlf+Z+sY4oolCuOuXsigLN8PzMG6crL5uQ5cqrhEFb638ExTnpaF//7bbdOmEJilJjo8+kor2F7+QLq5pPx4wQHoP06bJ+Zo1smFQbq6Yk3btEpfVJrfeCjz0UMTPAABWrpTl5UeOSMVheuLcIavQsWKFHOZvpaRIpRNNFOzCZW8d7twpjgntYyfLlsnn2rXS6iMCRowQwZs7FzExCch8fodUVG8797hITx6J9LNfBzOjqupbMAdQVfkt6L0cdPSvhX1JTlXFZsATA693LYqLnaLo8aQibV0FzjbOv/yoL7r0Gofq6l2Ij89ERsZ4dHxoETjoQ+DJ/0V5+RqkVWyVXktTzFSmKJit49pamZQwaZJzOvKhQ1L5mhXQ4sViUgwXBMCafXTnndagLBDpriJciPfscZ6blaD971Rb6+wR1BduyviC6fzR63WWlTlwbDcjhd/P67XEyi4K06eLIADRXbWb8eGEN1aiiUK4FwHT+3FdXfNnP7UQ14lC/t40/NQt4wnHihEjrPD8+TLt0OeTf9InnhBXHJmZUpEuXAj07g2cdRZw6aVS0Y6XFc4YM0ZcePzlL5Gt2zFjpKW+Y4dUNuaYw8aNztac2RNJThZRMPc2qE8U7C/6nDly+P2Wics0S9xwA/Dii1IRrlkjx9y5kra+TZL27wcCAZDHg5QU+afqkLMHuCFyYf653T4BXlmAwL23o6J2I6qqNqOu6hBSZ61CxVVnIa0zQSbgAew9jIKCeUhJGQCv9zPUfDgfp8yS+6we9yrYA3TbDZwGoKxsFcr3PgwO+kCbdsCb7UWXLpcgNrYzqqq24JQNKUibMcO4rxd11QcR/+xi0JQpQEwMgr17WrNNsrIie41du0Z/9sREa/vM+qioiDSLhYuC+be1/53q6pxiZa+QmysKppkyPt4pPGalbTfhVFc7W+T2noJdPOzC5/VGn9EXracQLgrRzEfhJiaToiLpibcBrhMFb1EaTr+inTNzojNrlhUeP97aPAgQYdizR1qcpqnnvPPE6VdSEpCdLWMFPXqIONhfVLNXMGqUtOoBGaCzjz+YLafkZGcr1D79dvduKxyt4ti3T4QLcHYZV64ELrnEOmeOrMTs5OZKz8Xe+9m2LWrSmEl3A+++i5i+p6PLhAno0uXn8nsz/4qMbaeIWBqiMPi0tYgZMBAeTxLqCnYg/mc/CN1nxCig7obLUJa6DcC38PlKsHfvn5H+CXDmVKBwJLD9nvcQMIaRuv/OygMxY/2yHvj+F0nIApC/71l4C/eE9s8AgMCbr6Ih133B8ZcjZvRYa1JCQxWz1xvZUygocJoKze/bK/4tW5yiYArBBx+IqJs0ZUW12ZMMFwWTcFGw39PrjTQfhZuRSkuji4L9HTGxL7IM/20Ts6cQCDjzUljYZqLgGtfZpiigLhXZ2e2alZOPSy8FBtmWmGRnOwfGliwRIfiBUbmlpMiOcl6vtJ5MU5HJxx+L+9qlS6VyCAScM6cyMqSXYPe42qmTvMCTJ8usmAcflDz99reR+V2+XHawe+89529PmCB2b5PiYquSN92ch7NggfO8Pg+3H34on+bmSYD1Wzk5jsomrsYDj0d6MPG7I/1BxX/wBbrGSHlkJI7CkCFb0b/ibgBA15XAsNevxI+z3sfw4Yfh8Sc4vjtg6WBkvSIVsa9kD1LrejuuV3Vo2EHhx7cuw/oznsXhihUAgJqy7fWmDZaXobZsR+SFrVutKdGmKNjNJvPmWSv0Aatsrr5aJjSY2AVl927gHcecFiFaT8G+atluwqmudlb69la7GR++XiFaxQ5E7102padglsPttzvfoWgLQVsJ1/UUUJvW7O0KlO9Ierq1stpOUpLV+vnjH6X1PmSI+HqaMUNMUoWF0lrPzJQZUl26yMIoQGZNPfOMhE27rrnL2vTpzvhBg4CbbxZvndHyAshe1C+/bJ3PnGmZDcaOBaZOta4RiQgdOAA895yI1t69zvn+dnw++c7KlWIqI7JMGHV1zpXks2ZJvr/3Pct3lZ28vJCzPTpcLH6hNmwSE1p+PuLmLELcnEVS6dQ6ByjTFuSGwlmBX8ET3w+AVQEnJ/4A4nEmOsx1qK09iNLK/cgAEFMWOWuqKgtIzgOqC3NRuD0XvcOu14wZhsRDPux/7TJ0LS9FQsQd4BDbyqJc+MvTkdahAyi8Zf/MM1KBmmzebPn86t/fqkyrqy1TV/fulhiE389unjJ39gNEFEzxsWM2WlaskPG1M86ofy8Re08hGIw+pmAK0T+d+6pHNJxaE2Y+oY5BgwZxSyirLuP7Z+YyYqs5P79Ft1COR15+mfmBB6zzgwfNtaTMq1Yx79zJfO+9zCUlcj0jQ67ddBPz8OESHjGCuaKC+dAh5vh45m7drHsAzFlZzMGgM27TJrm/PS78SE+Xz+eeY166lHn+/Mg0l18un2a+WnLcfbd8/v73zBMnNp6+Y0f5HD6cedy46Hm+4QYr7oUXQuFg0M/BoJ9rnn+83vvv3TKVy6/oz3WnpnDZHSPqTXdoTCyXntP48229F7xqJdgf74zf8eQPubpfRw4SOeKPXjWUGWDv1AnsvaiXlffEeGaA/RedH/23HnyQ+ZtvrPPZs5ljYiR8551W2H68/DLzggUSzs5mzslhvvXW6Pfv398Kl5fLPcPT/OEPzP/+d2T8tGnf+VWBrA9rtI5tVoV8PBwtFQVm5smT5Z0PBFp8C+VE4JNPmKuqol/bv595yRL5Jzh0SF6Bf/zDul5WJtd27WK+/36pCBYulGslJcyPP858ySVW+tdeY546lfnRR60X+OOPmdevZy4uZt661Uq7b1/ky/7pp8znnCPhsMotdAwbxvz11xLu1o05OTl6uo0bRezC4y++mPmxx5hTU+U3AgHm665ruDKeOdMSj3XrrHiTOXPq/y4z86RJzGlpzHfcwZyUZF2zh40j0Cn681T3ld+vnPgz3vtXKaOdtyfyltmnMQNc/uNUZoC33Qle/1Tk930p4OqMyPi80dHzXd0jkQ/869eh86I7ftRwGQHMTz/N/mvHN54OkPI3wwsXMl94YdO+BzD/7nff+bVQUYjC1Vcz9+7d4q8rJyM1NQ1fP3q06ffavl16BQ0xezbzlVcyn3uu9GACAea8PObRo5mHDGEePFhey7o6aU3m5TEfPiz56NyZ+c03pWcEMN93H/Of/iQtyzfekPvv3898/fXMl11mVSjBoFzLzxfRY2Zeu9bqEZmV0wUXWN9Ztow5M1PCBQXOCt/8nQkTmP/2N+af/zxSFGbOtM4vusgK21vL5nHBBcxxcdErwnABefUV6f0Z58GRF3BN+W6u3vNlkyrXYEwMl85w9qZqeqbyzimdOUhNrKDtQnKqCLk/0RnvS4vlqnOzm30/BjjQoxvnL7+HKx+9metmzWD/5aOZAa4d9WP2zbM1YJqJikIURo1iHjq0xV9XlNantFTEpSH8fuYDBxq/16xZUjnXR2WlmLXq6pjXrGH+7DPmPn2Yn3pKrq9bx3zXXSIqCxaI+ETj3XelKhk9mrmwUOJ8PulB/eY30iPLyWF++mnmMWOcleA990jPLrznkZho9eQASzTWrhVhM+OXL5ffCzfvDRpkhRcvFsFduFBEtqjIqoD79mQ+cICDwaCIrCke46+Uz2l/4aAnzDz19H1cdu+F7E+J40CChyuH9+SSf9zoFJpu8bxqVcOVf9F54C33i0lqw4fnhOLXzwSvWmUdX80w7nkKuHDysMb/7vXQVFEgSXviMHjwYM7NzW08YRSGDpVJKitWHONMKYqbYZbB8bFjG963A5B1J2+/LQvUOnSQ2WIme/YAPXvKIHsgIGtJHn9cBmUHDpSV1atXy2r2hQtlBtO8edairuXLZTB5zRqZSfbEEzIz7pprIvNxzTWyvWj4nuEzZshuhHl5MvDcp49MJhg4UAalS0osZ5GbN0tezMkSs2eD16wGvTQf+PvfEbjlt6B77gNRLAIDeoE4DoEe6Qh08CAx40wE+/VETExCyCWL7+HJ4Gov9t+UhISE7ggEqhAIHEVsbCf4CnYB6Z3Qq9cj8HiS0RKIaB0zD240nZtE4fTTZU2VfVagoiiKg3CXK4DMYvrsMxGZxqitFaE6ztwmNFUUXDMlFZApwE3da1xRFJcSLgiA+O1qiiAA4gLkBMY1i9cAFQVFUZTGcI0o+HxiIlRRUBRFqR/XiIK5ejyamxJFURRFcJ0oaE9BURSlflQUFEVRlBAqCoqiKEoIFQVFURQlhGtEITNTtvmtbyMpRVEUxUWL14YPl0NRFEWpH9f0FBRFUZTGUVFQFEVRQqgoKIqiKCFUFBRFUZQQKgqKoihKCBUFRVEUJYSKgqIoihJCRUFRFEUJccJtx0lEhwHsa+HXTwFQfAyzcyKiZaBlYKLl4K4y6MnMGY0lOuFE4btARLlN2aP0ZEbLQMvARMtByyAaaj5SFEVRQqgoKIqiKCHcJgrPtXcGjgO0DLQMTLQctAwicNWYgqIoitIwbuspKIqiKA3gGlEgoouJaBsR7SSiB9o7P60FEf2LiIqIaJMtrgsRfUBEO4zPzkY8EdH/G2WykYh+1H45P3YQUQ8iWkVE3xLRZiKaZMS7phyIKJGIviCir40yeNiI70VEnxvP+hoRxRvxCcb5TuN6dnvm/1hCRB4i2kBEy4xz15VBc3CFKBCRB8AzAC4B0B/ANUTUv31z1WrMBXBxWNwDAHKYuR+AHOMckPLoZxz/DWBWG+WxtfEDuJeZ+wMYBuA24+/tpnKoBTCSmc8GMBDAxUQ0DMDjAJ5k5r4AygDcaKS/EUCZEf+kke5kYRKALbZzN5ZB02Hmk/4AcC6AFbbzKQCmtHe+WvF5swFssp1vA9DNCHcDsM0IPwvgmmjpTqYDwFIAo9xaDgCSAawHMBSyUCvWiA+9FwBWADjXCMca6ai9834Mnr07pAEwEsAyAOS2Mmju4YqeAoAsAAds5weNOLfQlZnzjXABAHOn6pO+XAwTwDkAPofLysEwm3wFoAjABwB2ATjCzH4jif05Q2VgXC8HkN62OW4VZgKYDCBonKfDfWXQLNwiCooBSzPIFVPOiKgDgNcB3MXMXvs1N5QDMweYeSCktTwEwOntnKU2hYh+BaCImde1d15OJNwiCnkAetjOuxtxbqGQiLoBgPFZZMSftOVCRHEQQVjAzG8Y0a4rBwBg5iMAVkFMJZ2IKNa4ZH/OUBkY1zsCKGnjrB5rfgLg10S0F8CrEBPSU3BXGTQbt4jClwD6GbMO4gFcDeCtds5TW/IWgOuN8PUQG7sZf50x+2YYgHKbeeWEhYgIwAsAtjDz/9kuuaYciCiDiDoZ4STImMoWiDiMM5KFl4FZNuMArDR6UycszDyFmbszczbknV/JzBPhojJoEe09qNFWB4BfAtgOsav+sb3z04rP+QqAfAA+iL30RohdNAfADgAfAuhipCXIrKxdAL4BMLi983+MyuCnENPQRgBfGccv3VQOAM4CsMEog00AHjLiewP4AsBOAIsAJBjxicb5TuN67/Z+hmNcHucDWObmMmjqoSuaFUVRlBBuMR8piqIoTUBFQVEURQmhoqAoiqKEUFFQFEVRQqgoKIqiKCFUFBSlDSGi801vnYpyPKKioCiKooRQUVCUKBDRfxn7EXxFRM8azuWOEtGTxv4EOUSUYaQdSESfGXsxLLHt09CXiD409jRYT0R9jNt3IKLFRLSViBYYK7AV5bhARUFRwiCiHwK4CsBPWBzKBQBMBJACIJeZBwBYDeB/jK+8COB+Zj4LsiLajF8A4BmWPQ2GQ1aaA+K19S7I3h69IT56FOW4ILbxJIriOi4EMAjAl0YjPgniPC8I4DUjzXwAbxBRRwCdmHm1ET8PwCIiSgWQxcxLAICZawDAuN8XzHzQOP8Ksv/FJ63/WIrSOCoKihIJAZjHzFMckURTw9K11EdMrS0cgL6HynGEmo8UJZIcAOOIKBMI7e3cE/K+mN41JwD4hJnLAZQR0XlG/LUAVjNzBYCDRHSpcY8EIkpu06dQlBagLRRFCYOZvyWiPwF4n4hiIB5nbwNQCWCIca0IMu4AiLvlfxqV/m4AvzHirwXwLBE9YtzjyjZ8DEVpEeolVVGaCBEdZeYO7Z0PRWlN1HykKIqihNCegqIoihJCewqKoihKCBUFRVEUJYSKgqIoihJCRUFRFEUJoaKgKIqihFBRUBRFUUL8Bwie1Y8dZsaQAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 411us/sample - loss: 0.3813 - acc: 0.8916\n",
      "Loss: 0.38126937850984827 Accuracy: 0.8915888\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 2.3542 - acc: 0.2527\n",
      "Epoch 00001: val_loss improved from inf to 2.08141, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_6_conv_checkpoint/001-2.0814.hdf5\n",
      "36805/36805 [==============================] - 37s 1ms/sample - loss: 2.3535 - acc: 0.2531 - val_loss: 2.0814 - val_acc: 0.4051\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.8488 - acc: 0.4152\n",
      "Epoch 00002: val_loss improved from 2.08141 to 1.50902, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_6_conv_checkpoint/002-1.5090.hdf5\n",
      "36805/36805 [==============================] - 30s 810us/sample - loss: 1.8489 - acc: 0.4152 - val_loss: 1.5090 - val_acc: 0.5982\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6167 - acc: 0.4980\n",
      "Epoch 00003: val_loss improved from 1.50902 to 1.33901, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_6_conv_checkpoint/003-1.3390.hdf5\n",
      "36805/36805 [==============================] - 30s 809us/sample - loss: 1.6169 - acc: 0.4980 - val_loss: 1.3390 - val_acc: 0.6562\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4492 - acc: 0.5600\n",
      "Epoch 00004: val_loss improved from 1.33901 to 1.17470, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_6_conv_checkpoint/004-1.1747.hdf5\n",
      "36805/36805 [==============================] - 30s 811us/sample - loss: 1.4494 - acc: 0.5599 - val_loss: 1.1747 - val_acc: 0.7063\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3293 - acc: 0.6053\n",
      "Epoch 00005: val_loss improved from 1.17470 to 1.06213, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_6_conv_checkpoint/005-1.0621.hdf5\n",
      "36805/36805 [==============================] - 30s 806us/sample - loss: 1.3294 - acc: 0.6052 - val_loss: 1.0621 - val_acc: 0.7556\n",
      "Epoch 6/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2206 - acc: 0.6432- ETA: 1s - loss:\n",
      "Epoch 00006: val_loss improved from 1.06213 to 1.03478, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_6_conv_checkpoint/006-1.0348.hdf5\n",
      "36805/36805 [==============================] - 30s 810us/sample - loss: 1.2201 - acc: 0.6434 - val_loss: 1.0348 - val_acc: 0.7275\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1409 - acc: 0.6662\n",
      "Epoch 00007: val_loss improved from 1.03478 to 0.97335, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_6_conv_checkpoint/007-0.9734.hdf5\n",
      "36805/36805 [==============================] - 30s 811us/sample - loss: 1.1409 - acc: 0.6662 - val_loss: 0.9734 - val_acc: 0.7552\n",
      "Epoch 8/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0731 - acc: 0.6916\n",
      "Epoch 00008: val_loss improved from 0.97335 to 0.82584, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_6_conv_checkpoint/008-0.8258.hdf5\n",
      "36805/36805 [==============================] - 30s 809us/sample - loss: 1.0734 - acc: 0.6915 - val_loss: 0.8258 - val_acc: 0.7869\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0129 - acc: 0.7115\n",
      "Epoch 00009: val_loss improved from 0.82584 to 0.80640, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_6_conv_checkpoint/009-0.8064.hdf5\n",
      "36805/36805 [==============================] - 30s 809us/sample - loss: 1.0130 - acc: 0.7115 - val_loss: 0.8064 - val_acc: 0.7999\n",
      "Epoch 10/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9547 - acc: 0.7303\n",
      "Epoch 00010: val_loss improved from 0.80640 to 0.77360, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_6_conv_checkpoint/010-0.7736.hdf5\n",
      "36805/36805 [==============================] - 30s 810us/sample - loss: 0.9555 - acc: 0.7301 - val_loss: 0.7736 - val_acc: 0.7808\n",
      "Epoch 11/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9067 - acc: 0.7432\n",
      "Epoch 00011: val_loss improved from 0.77360 to 0.67621, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_6_conv_checkpoint/011-0.6762.hdf5\n",
      "36805/36805 [==============================] - 30s 813us/sample - loss: 0.9067 - acc: 0.7433 - val_loss: 0.6762 - val_acc: 0.8484\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8671 - acc: 0.7549\n",
      "Epoch 00012: val_loss improved from 0.67621 to 0.65749, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_6_conv_checkpoint/012-0.6575.hdf5\n",
      "36805/36805 [==============================] - 30s 808us/sample - loss: 0.8673 - acc: 0.7549 - val_loss: 0.6575 - val_acc: 0.8281\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8313 - acc: 0.7646\n",
      "Epoch 00013: val_loss did not improve from 0.65749\n",
      "36805/36805 [==============================] - 30s 804us/sample - loss: 0.8314 - acc: 0.7645 - val_loss: 0.6776 - val_acc: 0.8167\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8040 - acc: 0.7749\n",
      "Epoch 00014: val_loss improved from 0.65749 to 0.65743, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_6_conv_checkpoint/014-0.6574.hdf5\n",
      "36805/36805 [==============================] - 30s 805us/sample - loss: 0.8041 - acc: 0.7749 - val_loss: 0.6574 - val_acc: 0.8397\n",
      "Epoch 15/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7704 - acc: 0.7856\n",
      "Epoch 00015: val_loss improved from 0.65743 to 0.57748, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_6_conv_checkpoint/015-0.5775.hdf5\n",
      "36805/36805 [==============================] - 30s 811us/sample - loss: 0.7702 - acc: 0.7857 - val_loss: 0.5775 - val_acc: 0.8521\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7445 - acc: 0.7902\n",
      "Epoch 00016: val_loss improved from 0.57748 to 0.54989, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_6_conv_checkpoint/016-0.5499.hdf5\n",
      "36805/36805 [==============================] - 30s 812us/sample - loss: 0.7446 - acc: 0.7902 - val_loss: 0.5499 - val_acc: 0.8756\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7160 - acc: 0.7988\n",
      "Epoch 00017: val_loss did not improve from 0.54989\n",
      "36805/36805 [==============================] - 30s 807us/sample - loss: 0.7161 - acc: 0.7988 - val_loss: 0.6999 - val_acc: 0.7901\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7020 - acc: 0.8042\n",
      "Epoch 00018: val_loss improved from 0.54989 to 0.52656, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_6_conv_checkpoint/018-0.5266.hdf5\n",
      "36805/36805 [==============================] - 30s 810us/sample - loss: 0.7020 - acc: 0.8043 - val_loss: 0.5266 - val_acc: 0.8675\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6802 - acc: 0.8098- ETA: 0s - loss: 0.6808 - acc: \n",
      "Epoch 00019: val_loss improved from 0.52656 to 0.48781, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_6_conv_checkpoint/019-0.4878.hdf5\n",
      "36805/36805 [==============================] - 30s 809us/sample - loss: 0.6802 - acc: 0.8098 - val_loss: 0.4878 - val_acc: 0.8845\n",
      "Epoch 20/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6576 - acc: 0.8143\n",
      "Epoch 00020: val_loss improved from 0.48781 to 0.45881, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_6_conv_checkpoint/020-0.4588.hdf5\n",
      "36805/36805 [==============================] - 30s 810us/sample - loss: 0.6575 - acc: 0.8144 - val_loss: 0.4588 - val_acc: 0.8912\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6440 - acc: 0.8195\n",
      "Epoch 00021: val_loss did not improve from 0.45881\n",
      "36805/36805 [==============================] - 30s 808us/sample - loss: 0.6440 - acc: 0.8195 - val_loss: 0.4968 - val_acc: 0.8733\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6265 - acc: 0.8239\n",
      "Epoch 00022: val_loss did not improve from 0.45881\n",
      "36805/36805 [==============================] - 30s 806us/sample - loss: 0.6266 - acc: 0.8239 - val_loss: 0.6027 - val_acc: 0.8255\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6087 - acc: 0.8293\n",
      "Epoch 00023: val_loss improved from 0.45881 to 0.44431, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_6_conv_checkpoint/023-0.4443.hdf5\n",
      "36805/36805 [==============================] - 30s 816us/sample - loss: 0.6087 - acc: 0.8293 - val_loss: 0.4443 - val_acc: 0.8852\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5967 - acc: 0.8322\n",
      "Epoch 00024: val_loss did not improve from 0.44431\n",
      "36805/36805 [==============================] - 30s 806us/sample - loss: 0.5967 - acc: 0.8322 - val_loss: 0.4618 - val_acc: 0.8859\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5839 - acc: 0.8374\n",
      "Epoch 00025: val_loss improved from 0.44431 to 0.43647, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_6_conv_checkpoint/025-0.4365.hdf5\n",
      "36805/36805 [==============================] - 30s 807us/sample - loss: 0.5839 - acc: 0.8374 - val_loss: 0.4365 - val_acc: 0.8924\n",
      "Epoch 26/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5722 - acc: 0.8383\n",
      "Epoch 00026: val_loss improved from 0.43647 to 0.41939, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_6_conv_checkpoint/026-0.4194.hdf5\n",
      "36805/36805 [==============================] - 30s 810us/sample - loss: 0.5723 - acc: 0.8383 - val_loss: 0.4194 - val_acc: 0.8938\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5598 - acc: 0.8421\n",
      "Epoch 00027: val_loss improved from 0.41939 to 0.41565, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_6_conv_checkpoint/027-0.4157.hdf5\n",
      "36805/36805 [==============================] - 30s 812us/sample - loss: 0.5598 - acc: 0.8421 - val_loss: 0.4157 - val_acc: 0.8961\n",
      "Epoch 28/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5492 - acc: 0.8451\n",
      "Epoch 00028: val_loss did not improve from 0.41565\n",
      "36805/36805 [==============================] - 30s 808us/sample - loss: 0.5492 - acc: 0.8451 - val_loss: 0.4236 - val_acc: 0.8942\n",
      "Epoch 29/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5392 - acc: 0.8476\n",
      "Epoch 00029: val_loss improved from 0.41565 to 0.40003, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_6_conv_checkpoint/029-0.4000.hdf5\n",
      "36805/36805 [==============================] - 30s 814us/sample - loss: 0.5393 - acc: 0.8475 - val_loss: 0.4000 - val_acc: 0.9012\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5288 - acc: 0.8511\n",
      "Epoch 00030: val_loss improved from 0.40003 to 0.37348, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_6_conv_checkpoint/030-0.3735.hdf5\n",
      "36805/36805 [==============================] - 30s 807us/sample - loss: 0.5288 - acc: 0.8511 - val_loss: 0.3735 - val_acc: 0.9047\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5251 - acc: 0.8488\n",
      "Epoch 00031: val_loss improved from 0.37348 to 0.36422, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_6_conv_checkpoint/031-0.3642.hdf5\n",
      "36805/36805 [==============================] - 30s 811us/sample - loss: 0.5252 - acc: 0.8488 - val_loss: 0.3642 - val_acc: 0.9143\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5149 - acc: 0.8552\n",
      "Epoch 00032: val_loss did not improve from 0.36422\n",
      "36805/36805 [==============================] - 30s 806us/sample - loss: 0.5150 - acc: 0.8551 - val_loss: 0.4467 - val_acc: 0.8737\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5132 - acc: 0.8545\n",
      "Epoch 00033: val_loss improved from 0.36422 to 0.36287, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_6_conv_checkpoint/033-0.3629.hdf5\n",
      "36805/36805 [==============================] - 30s 809us/sample - loss: 0.5134 - acc: 0.8544 - val_loss: 0.3629 - val_acc: 0.9115\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5027 - acc: 0.8565\n",
      "Epoch 00034: val_loss did not improve from 0.36287\n",
      "36805/36805 [==============================] - 30s 807us/sample - loss: 0.5029 - acc: 0.8565 - val_loss: 0.4436 - val_acc: 0.8758\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4919 - acc: 0.8599\n",
      "Epoch 00035: val_loss did not improve from 0.36287\n",
      "36805/36805 [==============================] - 30s 805us/sample - loss: 0.4920 - acc: 0.8599 - val_loss: 0.3672 - val_acc: 0.9122\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4799 - acc: 0.8629\n",
      "Epoch 00036: val_loss improved from 0.36287 to 0.35074, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_6_conv_checkpoint/036-0.3507.hdf5\n",
      "36805/36805 [==============================] - 30s 811us/sample - loss: 0.4799 - acc: 0.8629 - val_loss: 0.3507 - val_acc: 0.9129\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4768 - acc: 0.8627\n",
      "Epoch 00037: val_loss improved from 0.35074 to 0.34641, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_6_conv_checkpoint/037-0.3464.hdf5\n",
      "36805/36805 [==============================] - 30s 809us/sample - loss: 0.4768 - acc: 0.8627 - val_loss: 0.3464 - val_acc: 0.9089\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4705 - acc: 0.8655\n",
      "Epoch 00038: val_loss did not improve from 0.34641\n",
      "36805/36805 [==============================] - 30s 804us/sample - loss: 0.4705 - acc: 0.8655 - val_loss: 0.3742 - val_acc: 0.8998\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4618 - acc: 0.8674\n",
      "Epoch 00039: val_loss did not improve from 0.34641\n",
      "36805/36805 [==============================] - 30s 806us/sample - loss: 0.4619 - acc: 0.8674 - val_loss: 0.3515 - val_acc: 0.9068\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4619 - acc: 0.8687\n",
      "Epoch 00040: val_loss improved from 0.34641 to 0.33907, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_6_conv_checkpoint/040-0.3391.hdf5\n",
      "36805/36805 [==============================] - 30s 817us/sample - loss: 0.4619 - acc: 0.8687 - val_loss: 0.3391 - val_acc: 0.9131\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4504 - acc: 0.8719\n",
      "Epoch 00041: val_loss improved from 0.33907 to 0.32883, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_6_conv_checkpoint/041-0.3288.hdf5\n",
      "36805/36805 [==============================] - 30s 807us/sample - loss: 0.4504 - acc: 0.8719 - val_loss: 0.3288 - val_acc: 0.9117\n",
      "Epoch 42/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4466 - acc: 0.8727\n",
      "Epoch 00042: val_loss did not improve from 0.32883\n",
      "36805/36805 [==============================] - 30s 806us/sample - loss: 0.4465 - acc: 0.8727 - val_loss: 0.3315 - val_acc: 0.9131\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4428 - acc: 0.8726\n",
      "Epoch 00043: val_loss improved from 0.32883 to 0.30728, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_6_conv_checkpoint/043-0.3073.hdf5\n",
      "36805/36805 [==============================] - 30s 805us/sample - loss: 0.4428 - acc: 0.8726 - val_loss: 0.3073 - val_acc: 0.9227\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4334 - acc: 0.8768\n",
      "Epoch 00044: val_loss did not improve from 0.30728\n",
      "36805/36805 [==============================] - 30s 804us/sample - loss: 0.4334 - acc: 0.8768 - val_loss: 0.4239 - val_acc: 0.8796\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4356 - acc: 0.8760\n",
      "Epoch 00045: val_loss did not improve from 0.30728\n",
      "36805/36805 [==============================] - 30s 805us/sample - loss: 0.4356 - acc: 0.8760 - val_loss: 0.3210 - val_acc: 0.9182\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4298 - acc: 0.8754\n",
      "Epoch 00046: val_loss improved from 0.30728 to 0.30386, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_6_conv_checkpoint/046-0.3039.hdf5\n",
      "36805/36805 [==============================] - 30s 812us/sample - loss: 0.4299 - acc: 0.8754 - val_loss: 0.3039 - val_acc: 0.9231\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4233 - acc: 0.8779\n",
      "Epoch 00047: val_loss improved from 0.30386 to 0.29491, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_6_conv_checkpoint/047-0.2949.hdf5\n",
      "36805/36805 [==============================] - 30s 806us/sample - loss: 0.4233 - acc: 0.8779 - val_loss: 0.2949 - val_acc: 0.9243\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4167 - acc: 0.8800\n",
      "Epoch 00048: val_loss did not improve from 0.29491\n",
      "36805/36805 [==============================] - 30s 803us/sample - loss: 0.4167 - acc: 0.8800 - val_loss: 0.3532 - val_acc: 0.9005\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4106 - acc: 0.8811\n",
      "Epoch 00049: val_loss did not improve from 0.29491\n",
      "36805/36805 [==============================] - 30s 806us/sample - loss: 0.4106 - acc: 0.8810 - val_loss: 0.3360 - val_acc: 0.9110\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4079 - acc: 0.8825\n",
      "Epoch 00050: val_loss improved from 0.29491 to 0.28479, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_6_conv_checkpoint/050-0.2848.hdf5\n",
      "36805/36805 [==============================] - 30s 810us/sample - loss: 0.4080 - acc: 0.8824 - val_loss: 0.2848 - val_acc: 0.9280\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4046 - acc: 0.8832\n",
      "Epoch 00051: val_loss improved from 0.28479 to 0.28409, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_6_conv_checkpoint/051-0.2841.hdf5\n",
      "36805/36805 [==============================] - 30s 814us/sample - loss: 0.4046 - acc: 0.8831 - val_loss: 0.2841 - val_acc: 0.9264\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4006 - acc: 0.8848\n",
      "Epoch 00052: val_loss did not improve from 0.28409\n",
      "36805/36805 [==============================] - 30s 809us/sample - loss: 0.4006 - acc: 0.8848 - val_loss: 0.3589 - val_acc: 0.9005\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3953 - acc: 0.8842\n",
      "Epoch 00053: val_loss did not improve from 0.28409\n",
      "36805/36805 [==============================] - 30s 809us/sample - loss: 0.3954 - acc: 0.8842 - val_loss: 0.3416 - val_acc: 0.9094\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3914 - acc: 0.8865\n",
      "Epoch 00054: val_loss did not improve from 0.28409\n",
      "36805/36805 [==============================] - 30s 809us/sample - loss: 0.3914 - acc: 0.8865 - val_loss: 0.2842 - val_acc: 0.9273\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3914 - acc: 0.8865\n",
      "Epoch 00055: val_loss did not improve from 0.28409\n",
      "36805/36805 [==============================] - 30s 809us/sample - loss: 0.3914 - acc: 0.8865 - val_loss: 0.5565 - val_acc: 0.8353\n",
      "Epoch 56/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3868 - acc: 0.8874\n",
      "Epoch 00056: val_loss did not improve from 0.28409\n",
      "36805/36805 [==============================] - 30s 807us/sample - loss: 0.3866 - acc: 0.8875 - val_loss: 0.2910 - val_acc: 0.9229\n",
      "Epoch 57/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3786 - acc: 0.8904\n",
      "Epoch 00057: val_loss did not improve from 0.28409\n",
      "36805/36805 [==============================] - 30s 809us/sample - loss: 0.3788 - acc: 0.8903 - val_loss: 0.2991 - val_acc: 0.9196\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3827 - acc: 0.8904\n",
      "Epoch 00058: val_loss did not improve from 0.28409\n",
      "36805/36805 [==============================] - 30s 808us/sample - loss: 0.3828 - acc: 0.8903 - val_loss: 0.3074 - val_acc: 0.9185\n",
      "Epoch 59/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3707 - acc: 0.8926\n",
      "Epoch 00059: val_loss did not improve from 0.28409\n",
      "36805/36805 [==============================] - 30s 805us/sample - loss: 0.3706 - acc: 0.8925 - val_loss: 0.2907 - val_acc: 0.9243\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3735 - acc: 0.8926\n",
      "Epoch 00060: val_loss improved from 0.28409 to 0.27673, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_6_conv_checkpoint/060-0.2767.hdf5\n",
      "36805/36805 [==============================] - 30s 812us/sample - loss: 0.3737 - acc: 0.8926 - val_loss: 0.2767 - val_acc: 0.9213\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3724 - acc: 0.8919\n",
      "Epoch 00061: val_loss did not improve from 0.27673\n",
      "36805/36805 [==============================] - 30s 806us/sample - loss: 0.3724 - acc: 0.8919 - val_loss: 0.4638 - val_acc: 0.8609\n",
      "Epoch 62/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3608 - acc: 0.8958\n",
      "Epoch 00062: val_loss did not improve from 0.27673\n",
      "36805/36805 [==============================] - 30s 808us/sample - loss: 0.3607 - acc: 0.8958 - val_loss: 0.3047 - val_acc: 0.9185\n",
      "Epoch 63/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3610 - acc: 0.8946\n",
      "Epoch 00063: val_loss did not improve from 0.27673\n",
      "36805/36805 [==============================] - 30s 807us/sample - loss: 0.3609 - acc: 0.8946 - val_loss: 0.3749 - val_acc: 0.8942\n",
      "Epoch 64/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3597 - acc: 0.8950\n",
      "Epoch 00064: val_loss did not improve from 0.27673\n",
      "36805/36805 [==============================] - 30s 805us/sample - loss: 0.3596 - acc: 0.8950 - val_loss: 0.2901 - val_acc: 0.9182\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3580 - acc: 0.8957\n",
      "Epoch 00065: val_loss did not improve from 0.27673\n",
      "36805/36805 [==============================] - 30s 808us/sample - loss: 0.3580 - acc: 0.8957 - val_loss: 0.3862 - val_acc: 0.8882\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3516 - acc: 0.8957\n",
      "Epoch 00066: val_loss improved from 0.27673 to 0.26697, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_6_conv_checkpoint/066-0.2670.hdf5\n",
      "36805/36805 [==============================] - 30s 814us/sample - loss: 0.3516 - acc: 0.8957 - val_loss: 0.2670 - val_acc: 0.9290\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3505 - acc: 0.8972\n",
      "Epoch 00067: val_loss did not improve from 0.26697\n",
      "36805/36805 [==============================] - 30s 805us/sample - loss: 0.3506 - acc: 0.8972 - val_loss: 0.2803 - val_acc: 0.9236\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3487 - acc: 0.8984\n",
      "Epoch 00068: val_loss improved from 0.26697 to 0.25841, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_6_conv_checkpoint/068-0.2584.hdf5\n",
      "36805/36805 [==============================] - 30s 811us/sample - loss: 0.3487 - acc: 0.8984 - val_loss: 0.2584 - val_acc: 0.9299\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3422 - acc: 0.8995\n",
      "Epoch 00069: val_loss improved from 0.25841 to 0.24602, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_6_conv_checkpoint/069-0.2460.hdf5\n",
      "36805/36805 [==============================] - 30s 809us/sample - loss: 0.3422 - acc: 0.8995 - val_loss: 0.2460 - val_acc: 0.9376\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3405 - acc: 0.8990\n",
      "Epoch 00070: val_loss did not improve from 0.24602\n",
      "36805/36805 [==============================] - 30s 810us/sample - loss: 0.3406 - acc: 0.8990 - val_loss: 0.2670 - val_acc: 0.9308\n",
      "Epoch 71/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3434 - acc: 0.9002\n",
      "Epoch 00071: val_loss did not improve from 0.24602\n",
      "36805/36805 [==============================] - 30s 812us/sample - loss: 0.3433 - acc: 0.9003 - val_loss: 0.2727 - val_acc: 0.9264\n",
      "Epoch 72/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3334 - acc: 0.9015\n",
      "Epoch 00072: val_loss did not improve from 0.24602\n",
      "36805/36805 [==============================] - 30s 813us/sample - loss: 0.3336 - acc: 0.9015 - val_loss: 0.2789 - val_acc: 0.9227\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3343 - acc: 0.9021\n",
      "Epoch 00073: val_loss did not improve from 0.24602\n",
      "36805/36805 [==============================] - 30s 809us/sample - loss: 0.3343 - acc: 0.9021 - val_loss: 0.2893 - val_acc: 0.9245\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3324 - acc: 0.9041\n",
      "Epoch 00074: val_loss did not improve from 0.24602\n",
      "36805/36805 [==============================] - 30s 804us/sample - loss: 0.3323 - acc: 0.9042 - val_loss: 0.2813 - val_acc: 0.9236\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3285 - acc: 0.9036\n",
      "Epoch 00075: val_loss did not improve from 0.24602\n",
      "36805/36805 [==============================] - 30s 807us/sample - loss: 0.3285 - acc: 0.9037 - val_loss: 0.2725 - val_acc: 0.9271\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3235 - acc: 0.9049\n",
      "Epoch 00076: val_loss did not improve from 0.24602\n",
      "36805/36805 [==============================] - 30s 806us/sample - loss: 0.3235 - acc: 0.9050 - val_loss: 0.3666 - val_acc: 0.8952\n",
      "Epoch 77/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3223 - acc: 0.9048\n",
      "Epoch 00077: val_loss did not improve from 0.24602\n",
      "36805/36805 [==============================] - 30s 804us/sample - loss: 0.3226 - acc: 0.9048 - val_loss: 0.2549 - val_acc: 0.9304\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3235 - acc: 0.9050\n",
      "Epoch 00078: val_loss did not improve from 0.24602\n",
      "36805/36805 [==============================] - 30s 807us/sample - loss: 0.3234 - acc: 0.9050 - val_loss: 0.2568 - val_acc: 0.9322\n",
      "Epoch 79/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3198 - acc: 0.9058\n",
      "Epoch 00079: val_loss did not improve from 0.24602\n",
      "36805/36805 [==============================] - 30s 810us/sample - loss: 0.3202 - acc: 0.9056 - val_loss: 0.3429 - val_acc: 0.8991\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3157 - acc: 0.9057\n",
      "Epoch 00080: val_loss did not improve from 0.24602\n",
      "36805/36805 [==============================] - 30s 806us/sample - loss: 0.3162 - acc: 0.9056 - val_loss: 0.2880 - val_acc: 0.9243\n",
      "Epoch 81/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3186 - acc: 0.9055\n",
      "Epoch 00081: val_loss did not improve from 0.24602\n",
      "36805/36805 [==============================] - 30s 810us/sample - loss: 0.3189 - acc: 0.9054 - val_loss: 0.3022 - val_acc: 0.9189\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3114 - acc: 0.9074\n",
      "Epoch 00082: val_loss did not improve from 0.24602\n",
      "36805/36805 [==============================] - 30s 806us/sample - loss: 0.3115 - acc: 0.9073 - val_loss: 0.2553 - val_acc: 0.9329\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3114 - acc: 0.9082\n",
      "Epoch 00083: val_loss did not improve from 0.24602\n",
      "36805/36805 [==============================] - 30s 807us/sample - loss: 0.3114 - acc: 0.9082 - val_loss: 0.2585 - val_acc: 0.9306\n",
      "Epoch 84/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3070 - acc: 0.9096\n",
      "Epoch 00084: val_loss did not improve from 0.24602\n",
      "36805/36805 [==============================] - 30s 805us/sample - loss: 0.3069 - acc: 0.9096 - val_loss: 0.3018 - val_acc: 0.9152\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3045 - acc: 0.9111\n",
      "Epoch 00085: val_loss improved from 0.24602 to 0.23738, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_6_conv_checkpoint/085-0.2374.hdf5\n",
      "36805/36805 [==============================] - 30s 820us/sample - loss: 0.3045 - acc: 0.9111 - val_loss: 0.2374 - val_acc: 0.9380\n",
      "Epoch 86/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3048 - acc: 0.9097\n",
      "Epoch 00086: val_loss did not improve from 0.23738\n",
      "36805/36805 [==============================] - 30s 808us/sample - loss: 0.3048 - acc: 0.9098 - val_loss: 0.2422 - val_acc: 0.9338\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3019 - acc: 0.9117\n",
      "Epoch 00087: val_loss did not improve from 0.23738\n",
      "36805/36805 [==============================] - 30s 808us/sample - loss: 0.3018 - acc: 0.9117 - val_loss: 0.2511 - val_acc: 0.9308\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2994 - acc: 0.9132\n",
      "Epoch 00088: val_loss did not improve from 0.23738\n",
      "36805/36805 [==============================] - 30s 809us/sample - loss: 0.2994 - acc: 0.9131 - val_loss: 0.2593 - val_acc: 0.9306\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2995 - acc: 0.9108\n",
      "Epoch 00089: val_loss did not improve from 0.23738\n",
      "36805/36805 [==============================] - 30s 810us/sample - loss: 0.2996 - acc: 0.9108 - val_loss: 0.3924 - val_acc: 0.8889\n",
      "Epoch 90/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2977 - acc: 0.9113\n",
      "Epoch 00090: val_loss did not improve from 0.23738\n",
      "36805/36805 [==============================] - 30s 808us/sample - loss: 0.2978 - acc: 0.9112 - val_loss: 0.2744 - val_acc: 0.9217\n",
      "Epoch 91/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2932 - acc: 0.9151\n",
      "Epoch 00091: val_loss improved from 0.23738 to 0.22958, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_6_conv_checkpoint/091-0.2296.hdf5\n",
      "36805/36805 [==============================] - 30s 808us/sample - loss: 0.2934 - acc: 0.9150 - val_loss: 0.2296 - val_acc: 0.9357\n",
      "Epoch 92/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2886 - acc: 0.9148\n",
      "Epoch 00092: val_loss did not improve from 0.22958\n",
      "36805/36805 [==============================] - 30s 807us/sample - loss: 0.2889 - acc: 0.9147 - val_loss: 0.2399 - val_acc: 0.9364\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2916 - acc: 0.9132\n",
      "Epoch 00093: val_loss did not improve from 0.22958\n",
      "36805/36805 [==============================] - 30s 809us/sample - loss: 0.2917 - acc: 0.9131 - val_loss: 0.2643 - val_acc: 0.9315\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2908 - acc: 0.9135\n",
      "Epoch 00094: val_loss did not improve from 0.22958\n",
      "36805/36805 [==============================] - 30s 809us/sample - loss: 0.2908 - acc: 0.9134 - val_loss: 0.2820 - val_acc: 0.9224\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2910 - acc: 0.9142- ET\n",
      "Epoch 00095: val_loss did not improve from 0.22958\n",
      "36805/36805 [==============================] - 30s 806us/sample - loss: 0.2912 - acc: 0.9141 - val_loss: 0.2309 - val_acc: 0.9385\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2870 - acc: 0.9149\n",
      "Epoch 00096: val_loss did not improve from 0.22958\n",
      "36805/36805 [==============================] - 30s 803us/sample - loss: 0.2869 - acc: 0.9149 - val_loss: 0.2563 - val_acc: 0.9324\n",
      "Epoch 97/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2850 - acc: 0.9156\n",
      "Epoch 00097: val_loss did not improve from 0.22958\n",
      "36805/36805 [==============================] - 30s 805us/sample - loss: 0.2850 - acc: 0.9156 - val_loss: 0.2945 - val_acc: 0.9210\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2815 - acc: 0.9169\n",
      "Epoch 00098: val_loss did not improve from 0.22958\n",
      "36805/36805 [==============================] - 30s 810us/sample - loss: 0.2815 - acc: 0.9169 - val_loss: 0.2369 - val_acc: 0.9366\n",
      "Epoch 99/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2808 - acc: 0.9174\n",
      "Epoch 00099: val_loss did not improve from 0.22958\n",
      "36805/36805 [==============================] - 30s 810us/sample - loss: 0.2807 - acc: 0.9174 - val_loss: 0.3062 - val_acc: 0.9096\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2763 - acc: 0.9189\n",
      "Epoch 00100: val_loss did not improve from 0.22958\n",
      "36805/36805 [==============================] - 30s 807us/sample - loss: 0.2763 - acc: 0.9190 - val_loss: 0.2557 - val_acc: 0.9308\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2774 - acc: 0.9180\n",
      "Epoch 00101: val_loss did not improve from 0.22958\n",
      "36805/36805 [==============================] - 30s 811us/sample - loss: 0.2774 - acc: 0.9179 - val_loss: 0.2385 - val_acc: 0.9364\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2801 - acc: 0.9168\n",
      "Epoch 00102: val_loss did not improve from 0.22958\n",
      "36805/36805 [==============================] - 30s 820us/sample - loss: 0.2802 - acc: 0.9168 - val_loss: 0.2325 - val_acc: 0.9369\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2770 - acc: 0.9173\n",
      "Epoch 00103: val_loss did not improve from 0.22958\n",
      "36805/36805 [==============================] - 30s 817us/sample - loss: 0.2770 - acc: 0.9173 - val_loss: 0.2726 - val_acc: 0.9231\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2709 - acc: 0.9193\n",
      "Epoch 00104: val_loss improved from 0.22958 to 0.22501, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_6_conv_checkpoint/104-0.2250.hdf5\n",
      "36805/36805 [==============================] - 30s 816us/sample - loss: 0.2709 - acc: 0.9193 - val_loss: 0.2250 - val_acc: 0.9415\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2700 - acc: 0.9185\n",
      "Epoch 00105: val_loss improved from 0.22501 to 0.22079, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_6_conv_checkpoint/105-0.2208.hdf5\n",
      "36805/36805 [==============================] - 30s 811us/sample - loss: 0.2701 - acc: 0.9185 - val_loss: 0.2208 - val_acc: 0.9406\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2673 - acc: 0.9187\n",
      "Epoch 00106: val_loss did not improve from 0.22079\n",
      "36805/36805 [==============================] - 30s 810us/sample - loss: 0.2673 - acc: 0.9187 - val_loss: 0.2458 - val_acc: 0.9371\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2697 - acc: 0.9190\n",
      "Epoch 00107: val_loss did not improve from 0.22079\n",
      "36805/36805 [==============================] - 30s 805us/sample - loss: 0.2696 - acc: 0.9190 - val_loss: 0.2489 - val_acc: 0.9283\n",
      "Epoch 108/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2653 - acc: 0.9203\n",
      "Epoch 00108: val_loss did not improve from 0.22079\n",
      "36805/36805 [==============================] - 30s 808us/sample - loss: 0.2656 - acc: 0.9202 - val_loss: 0.2775 - val_acc: 0.9213\n",
      "Epoch 109/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2644 - acc: 0.9197\n",
      "Epoch 00109: val_loss did not improve from 0.22079\n",
      "36805/36805 [==============================] - 30s 809us/sample - loss: 0.2648 - acc: 0.9195 - val_loss: 0.2306 - val_acc: 0.9404\n",
      "Epoch 110/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2675 - acc: 0.9204\n",
      "Epoch 00110: val_loss did not improve from 0.22079\n",
      "36805/36805 [==============================] - 30s 807us/sample - loss: 0.2674 - acc: 0.9205 - val_loss: 0.2328 - val_acc: 0.9336\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2600 - acc: 0.9221\n",
      "Epoch 00111: val_loss did not improve from 0.22079\n",
      "36805/36805 [==============================] - 30s 807us/sample - loss: 0.2604 - acc: 0.9220 - val_loss: 0.2749 - val_acc: 0.9201\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2658 - acc: 0.9211\n",
      "Epoch 00112: val_loss did not improve from 0.22079\n",
      "36805/36805 [==============================] - 30s 805us/sample - loss: 0.2659 - acc: 0.9210 - val_loss: 0.2375 - val_acc: 0.9408\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2599 - acc: 0.9217\n",
      "Epoch 00113: val_loss improved from 0.22079 to 0.21821, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_6_conv_checkpoint/113-0.2182.hdf5\n",
      "36805/36805 [==============================] - 30s 808us/sample - loss: 0.2599 - acc: 0.9217 - val_loss: 0.2182 - val_acc: 0.9378\n",
      "Epoch 114/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2591 - acc: 0.9222\n",
      "Epoch 00114: val_loss did not improve from 0.21821\n",
      "36805/36805 [==============================] - 30s 806us/sample - loss: 0.2592 - acc: 0.9222 - val_loss: 0.2315 - val_acc: 0.9397\n",
      "Epoch 115/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2597 - acc: 0.9231\n",
      "Epoch 00115: val_loss did not improve from 0.21821\n",
      "36805/36805 [==============================] - 30s 810us/sample - loss: 0.2597 - acc: 0.9230 - val_loss: 0.2368 - val_acc: 0.9399\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2558 - acc: 0.9252\n",
      "Epoch 00116: val_loss did not improve from 0.21821\n",
      "36805/36805 [==============================] - 30s 807us/sample - loss: 0.2558 - acc: 0.9251 - val_loss: 0.2273 - val_acc: 0.9394\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2552 - acc: 0.9248\n",
      "Epoch 00117: val_loss did not improve from 0.21821\n",
      "36805/36805 [==============================] - 30s 813us/sample - loss: 0.2552 - acc: 0.9248 - val_loss: 0.2345 - val_acc: 0.9371\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2555 - acc: 0.9242\n",
      "Epoch 00118: val_loss did not improve from 0.21821\n",
      "36805/36805 [==============================] - 30s 806us/sample - loss: 0.2555 - acc: 0.9241 - val_loss: 0.2319 - val_acc: 0.9345\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2538 - acc: 0.9238\n",
      "Epoch 00119: val_loss did not improve from 0.21821\n",
      "36805/36805 [==============================] - 30s 808us/sample - loss: 0.2539 - acc: 0.9238 - val_loss: 0.2439 - val_acc: 0.9355\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2520 - acc: 0.9247\n",
      "Epoch 00120: val_loss did not improve from 0.21821\n",
      "36805/36805 [==============================] - 30s 811us/sample - loss: 0.2520 - acc: 0.9247 - val_loss: 0.2606 - val_acc: 0.9278\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2466 - acc: 0.9253\n",
      "Epoch 00121: val_loss did not improve from 0.21821\n",
      "36805/36805 [==============================] - 30s 808us/sample - loss: 0.2466 - acc: 0.9253 - val_loss: 0.2451 - val_acc: 0.9357\n",
      "Epoch 122/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2484 - acc: 0.9254\n",
      "Epoch 00122: val_loss did not improve from 0.21821\n",
      "36805/36805 [==============================] - 30s 810us/sample - loss: 0.2483 - acc: 0.9254 - val_loss: 0.2498 - val_acc: 0.9315\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2481 - acc: 0.9248\n",
      "Epoch 00123: val_loss did not improve from 0.21821\n",
      "36805/36805 [==============================] - 30s 807us/sample - loss: 0.2481 - acc: 0.9248 - val_loss: 0.2497 - val_acc: 0.9308\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2483 - acc: 0.9245\n",
      "Epoch 00124: val_loss did not improve from 0.21821\n",
      "36805/36805 [==============================] - 30s 807us/sample - loss: 0.2483 - acc: 0.9245 - val_loss: 0.2330 - val_acc: 0.9364\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2469 - acc: 0.9265\n",
      "Epoch 00125: val_loss did not improve from 0.21821\n",
      "36805/36805 [==============================] - 30s 809us/sample - loss: 0.2470 - acc: 0.9265 - val_loss: 0.2227 - val_acc: 0.9408\n",
      "Epoch 126/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2433 - acc: 0.9273\n",
      "Epoch 00126: val_loss improved from 0.21821 to 0.21705, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_6_conv_checkpoint/126-0.2171.hdf5\n",
      "36805/36805 [==============================] - 30s 808us/sample - loss: 0.2430 - acc: 0.9273 - val_loss: 0.2171 - val_acc: 0.9404\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2450 - acc: 0.9267\n",
      "Epoch 00127: val_loss did not improve from 0.21705\n",
      "36805/36805 [==============================] - 30s 807us/sample - loss: 0.2450 - acc: 0.9267 - val_loss: 0.2254 - val_acc: 0.9383\n",
      "Epoch 128/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2421 - acc: 0.9280\n",
      "Epoch 00128: val_loss did not improve from 0.21705\n",
      "36805/36805 [==============================] - 30s 808us/sample - loss: 0.2421 - acc: 0.9280 - val_loss: 0.2238 - val_acc: 0.9413\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2404 - acc: 0.9271\n",
      "Epoch 00129: val_loss did not improve from 0.21705\n",
      "36805/36805 [==============================] - 30s 806us/sample - loss: 0.2404 - acc: 0.9271 - val_loss: 0.2231 - val_acc: 0.9415\n",
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2395 - acc: 0.9286\n",
      "Epoch 00130: val_loss did not improve from 0.21705\n",
      "36805/36805 [==============================] - 30s 803us/sample - loss: 0.2395 - acc: 0.9286 - val_loss: 0.2584 - val_acc: 0.9255\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 131/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2362 - acc: 0.9294\n",
      "Epoch 00131: val_loss did not improve from 0.21705\n",
      "36805/36805 [==============================] - 30s 807us/sample - loss: 0.2360 - acc: 0.9295 - val_loss: 0.2215 - val_acc: 0.9383\n",
      "Epoch 132/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2338 - acc: 0.9291\n",
      "Epoch 00132: val_loss did not improve from 0.21705\n",
      "36805/36805 [==============================] - 30s 805us/sample - loss: 0.2339 - acc: 0.9290 - val_loss: 0.2291 - val_acc: 0.9378\n",
      "Epoch 133/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2356 - acc: 0.9306\n",
      "Epoch 00133: val_loss did not improve from 0.21705\n",
      "36805/36805 [==============================] - 30s 807us/sample - loss: 0.2357 - acc: 0.9306 - val_loss: 0.2484 - val_acc: 0.9345\n",
      "Epoch 134/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2329 - acc: 0.9321\n",
      "Epoch 00134: val_loss did not improve from 0.21705\n",
      "36805/36805 [==============================] - 30s 808us/sample - loss: 0.2329 - acc: 0.9321 - val_loss: 0.2680 - val_acc: 0.9271\n",
      "Epoch 135/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2344 - acc: 0.9290\n",
      "Epoch 00135: val_loss did not improve from 0.21705\n",
      "36805/36805 [==============================] - 30s 808us/sample - loss: 0.2345 - acc: 0.9290 - val_loss: 0.2753 - val_acc: 0.9199\n",
      "Epoch 136/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2325 - acc: 0.9304\n",
      "Epoch 00136: val_loss did not improve from 0.21705\n",
      "36805/36805 [==============================] - 30s 802us/sample - loss: 0.2326 - acc: 0.9303 - val_loss: 0.2633 - val_acc: 0.9299\n",
      "Epoch 137/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2323 - acc: 0.9313\n",
      "Epoch 00137: val_loss improved from 0.21705 to 0.21701, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_6_conv_checkpoint/137-0.2170.hdf5\n",
      "36805/36805 [==============================] - 30s 810us/sample - loss: 0.2323 - acc: 0.9313 - val_loss: 0.2170 - val_acc: 0.9408\n",
      "Epoch 138/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2257 - acc: 0.9315- ETA: 0s - loss: 0.2258 - \n",
      "Epoch 00138: val_loss did not improve from 0.21701\n",
      "36805/36805 [==============================] - 30s 809us/sample - loss: 0.2256 - acc: 0.9315 - val_loss: 0.2207 - val_acc: 0.9411\n",
      "Epoch 139/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2272 - acc: 0.9335\n",
      "Epoch 00139: val_loss did not improve from 0.21701\n",
      "36805/36805 [==============================] - 30s 805us/sample - loss: 0.2272 - acc: 0.9335 - val_loss: 0.2342 - val_acc: 0.9385\n",
      "Epoch 140/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2315 - acc: 0.9299\n",
      "Epoch 00140: val_loss did not improve from 0.21701\n",
      "36805/36805 [==============================] - 30s 813us/sample - loss: 0.2315 - acc: 0.9299 - val_loss: 0.2421 - val_acc: 0.9334\n",
      "Epoch 141/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2247 - acc: 0.9333\n",
      "Epoch 00141: val_loss did not improve from 0.21701\n",
      "36805/36805 [==============================] - 30s 809us/sample - loss: 0.2247 - acc: 0.9333 - val_loss: 0.2250 - val_acc: 0.9380\n",
      "Epoch 142/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2265 - acc: 0.9321\n",
      "Epoch 00142: val_loss did not improve from 0.21701\n",
      "36805/36805 [==============================] - 30s 808us/sample - loss: 0.2266 - acc: 0.9320 - val_loss: 0.2177 - val_acc: 0.9385\n",
      "Epoch 143/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2272 - acc: 0.9297\n",
      "Epoch 00143: val_loss improved from 0.21701 to 0.21379, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_6_conv_checkpoint/143-0.2138.hdf5\n",
      "36805/36805 [==============================] - 30s 810us/sample - loss: 0.2272 - acc: 0.9297 - val_loss: 0.2138 - val_acc: 0.9422\n",
      "Epoch 144/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2205 - acc: 0.9336\n",
      "Epoch 00144: val_loss did not improve from 0.21379\n",
      "36805/36805 [==============================] - 30s 809us/sample - loss: 0.2205 - acc: 0.9336 - val_loss: 0.2209 - val_acc: 0.9415\n",
      "Epoch 145/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2201 - acc: 0.9315\n",
      "Epoch 00145: val_loss improved from 0.21379 to 0.21203, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_6_conv_checkpoint/145-0.2120.hdf5\n",
      "36805/36805 [==============================] - 30s 816us/sample - loss: 0.2202 - acc: 0.9315 - val_loss: 0.2120 - val_acc: 0.9441\n",
      "Epoch 146/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2235 - acc: 0.9336\n",
      "Epoch 00146: val_loss did not improve from 0.21203\n",
      "36805/36805 [==============================] - 30s 821us/sample - loss: 0.2237 - acc: 0.9335 - val_loss: 0.2854 - val_acc: 0.9245\n",
      "Epoch 147/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2257 - acc: 0.9319\n",
      "Epoch 00147: val_loss did not improve from 0.21203\n",
      "36805/36805 [==============================] - 30s 808us/sample - loss: 0.2256 - acc: 0.9320 - val_loss: 0.2774 - val_acc: 0.9245\n",
      "Epoch 148/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2196 - acc: 0.9337\n",
      "Epoch 00148: val_loss did not improve from 0.21203\n",
      "36805/36805 [==============================] - 30s 808us/sample - loss: 0.2197 - acc: 0.9337 - val_loss: 0.2579 - val_acc: 0.9290\n",
      "Epoch 149/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2184 - acc: 0.9340\n",
      "Epoch 00149: val_loss did not improve from 0.21203\n",
      "36805/36805 [==============================] - 30s 811us/sample - loss: 0.2185 - acc: 0.9340 - val_loss: 0.2412 - val_acc: 0.9336\n",
      "Epoch 150/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2183 - acc: 0.9327\n",
      "Epoch 00150: val_loss did not improve from 0.21203\n",
      "36805/36805 [==============================] - 30s 811us/sample - loss: 0.2183 - acc: 0.9328 - val_loss: 0.2182 - val_acc: 0.9394\n",
      "Epoch 151/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2178 - acc: 0.9338\n",
      "Epoch 00151: val_loss improved from 0.21203 to 0.21168, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_6_conv_checkpoint/151-0.2117.hdf5\n",
      "36805/36805 [==============================] - 30s 815us/sample - loss: 0.2179 - acc: 0.9338 - val_loss: 0.2117 - val_acc: 0.9450\n",
      "Epoch 152/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2198 - acc: 0.9341\n",
      "Epoch 00152: val_loss did not improve from 0.21168\n",
      "36805/36805 [==============================] - 30s 808us/sample - loss: 0.2200 - acc: 0.9341 - val_loss: 0.2441 - val_acc: 0.9387\n",
      "Epoch 153/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2191 - acc: 0.9342\n",
      "Epoch 00153: val_loss did not improve from 0.21168\n",
      "36805/36805 [==============================] - 30s 809us/sample - loss: 0.2191 - acc: 0.9342 - val_loss: 0.2322 - val_acc: 0.9390\n",
      "Epoch 154/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2138 - acc: 0.9357\n",
      "Epoch 00154: val_loss improved from 0.21168 to 0.20865, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_6_conv_checkpoint/154-0.2087.hdf5\n",
      "36805/36805 [==============================] - 31s 831us/sample - loss: 0.2138 - acc: 0.9357 - val_loss: 0.2087 - val_acc: 0.9439\n",
      "Epoch 155/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2106 - acc: 0.9351\n",
      "Epoch 00155: val_loss did not improve from 0.20865\n",
      "36805/36805 [==============================] - 30s 815us/sample - loss: 0.2107 - acc: 0.9350 - val_loss: 0.2372 - val_acc: 0.9359\n",
      "Epoch 156/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2123 - acc: 0.9362\n",
      "Epoch 00156: val_loss did not improve from 0.20865\n",
      "36805/36805 [==============================] - 30s 812us/sample - loss: 0.2123 - acc: 0.9362 - val_loss: 0.3426 - val_acc: 0.9040\n",
      "Epoch 157/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2121 - acc: 0.9362\n",
      "Epoch 00157: val_loss did not improve from 0.20865\n",
      "36805/36805 [==============================] - 30s 808us/sample - loss: 0.2121 - acc: 0.9362 - val_loss: 0.2488 - val_acc: 0.9334\n",
      "Epoch 158/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2105 - acc: 0.9354\n",
      "Epoch 00158: val_loss did not improve from 0.20865\n",
      "36805/36805 [==============================] - 30s 808us/sample - loss: 0.2108 - acc: 0.9353 - val_loss: 0.2350 - val_acc: 0.9350\n",
      "Epoch 159/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2105 - acc: 0.9369\n",
      "Epoch 00159: val_loss did not improve from 0.20865\n",
      "36805/36805 [==============================] - 30s 809us/sample - loss: 0.2107 - acc: 0.9368 - val_loss: 0.2512 - val_acc: 0.9322\n",
      "Epoch 160/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2072 - acc: 0.9355\n",
      "Epoch 00160: val_loss did not improve from 0.20865\n",
      "36805/36805 [==============================] - 30s 811us/sample - loss: 0.2073 - acc: 0.9355 - val_loss: 0.3646 - val_acc: 0.9019\n",
      "Epoch 161/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2117 - acc: 0.9357\n",
      "Epoch 00161: val_loss did not improve from 0.20865\n",
      "36805/36805 [==============================] - 30s 808us/sample - loss: 0.2117 - acc: 0.9357 - val_loss: 0.2127 - val_acc: 0.9420\n",
      "Epoch 162/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2114 - acc: 0.9365\n",
      "Epoch 00162: val_loss did not improve from 0.20865\n",
      "36805/36805 [==============================] - 30s 811us/sample - loss: 0.2115 - acc: 0.9364 - val_loss: 0.2281 - val_acc: 0.9411\n",
      "Epoch 163/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2022 - acc: 0.9404\n",
      "Epoch 00163: val_loss did not improve from 0.20865\n",
      "36805/36805 [==============================] - 30s 810us/sample - loss: 0.2022 - acc: 0.9404 - val_loss: 0.2227 - val_acc: 0.9397\n",
      "Epoch 164/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2071 - acc: 0.9377\n",
      "Epoch 00164: val_loss did not improve from 0.20865\n",
      "36805/36805 [==============================] - 30s 809us/sample - loss: 0.2072 - acc: 0.9377 - val_loss: 0.2638 - val_acc: 0.9285\n",
      "Epoch 165/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2024 - acc: 0.9386\n",
      "Epoch 00165: val_loss did not improve from 0.20865\n",
      "36805/36805 [==============================] - 30s 820us/sample - loss: 0.2025 - acc: 0.9386 - val_loss: 0.2147 - val_acc: 0.9441\n",
      "Epoch 166/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2047 - acc: 0.9386\n",
      "Epoch 00166: val_loss did not improve from 0.20865\n",
      "36805/36805 [==============================] - 30s 810us/sample - loss: 0.2049 - acc: 0.9386 - val_loss: 0.2356 - val_acc: 0.9397\n",
      "Epoch 167/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2065 - acc: 0.9385\n",
      "Epoch 00167: val_loss did not improve from 0.20865\n",
      "36805/36805 [==============================] - 30s 823us/sample - loss: 0.2065 - acc: 0.9385 - val_loss: 0.4640 - val_acc: 0.8800\n",
      "Epoch 168/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2019 - acc: 0.9390\n",
      "Epoch 00168: val_loss did not improve from 0.20865\n",
      "36805/36805 [==============================] - 30s 822us/sample - loss: 0.2019 - acc: 0.9390 - val_loss: 0.2238 - val_acc: 0.9399\n",
      "Epoch 169/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2033 - acc: 0.9378\n",
      "Epoch 00169: val_loss improved from 0.20865 to 0.20715, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_6_conv_checkpoint/169-0.2071.hdf5\n",
      "36805/36805 [==============================] - 30s 824us/sample - loss: 0.2033 - acc: 0.9378 - val_loss: 0.2071 - val_acc: 0.9434\n",
      "Epoch 170/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1950 - acc: 0.9404\n",
      "Epoch 00170: val_loss did not improve from 0.20715\n",
      "36805/36805 [==============================] - 30s 826us/sample - loss: 0.1950 - acc: 0.9404 - val_loss: 0.2317 - val_acc: 0.9373\n",
      "Epoch 171/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2044 - acc: 0.9377\n",
      "Epoch 00171: val_loss did not improve from 0.20715\n",
      "36805/36805 [==============================] - 30s 821us/sample - loss: 0.2044 - acc: 0.9376 - val_loss: 0.2394 - val_acc: 0.9366\n",
      "Epoch 172/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1981 - acc: 0.9401\n",
      "Epoch 00172: val_loss did not improve from 0.20715\n",
      "36805/36805 [==============================] - 31s 833us/sample - loss: 0.1981 - acc: 0.9401 - val_loss: 0.2329 - val_acc: 0.9362\n",
      "Epoch 173/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1994 - acc: 0.9380\n",
      "Epoch 00173: val_loss did not improve from 0.20715\n",
      "36805/36805 [==============================] - 31s 833us/sample - loss: 0.1995 - acc: 0.9380 - val_loss: 0.2163 - val_acc: 0.9420\n",
      "Epoch 174/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2021 - acc: 0.9384\n",
      "Epoch 00174: val_loss did not improve from 0.20715\n",
      "36805/36805 [==============================] - 30s 827us/sample - loss: 0.2021 - acc: 0.9384 - val_loss: 0.2206 - val_acc: 0.9420\n",
      "Epoch 175/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1975 - acc: 0.9408\n",
      "Epoch 00175: val_loss did not improve from 0.20715\n",
      "36805/36805 [==============================] - 31s 833us/sample - loss: 0.1975 - acc: 0.9408 - val_loss: 0.2272 - val_acc: 0.9432\n",
      "Epoch 176/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1956 - acc: 0.9406\n",
      "Epoch 00176: val_loss did not improve from 0.20715\n",
      "36805/36805 [==============================] - 31s 836us/sample - loss: 0.1956 - acc: 0.9406 - val_loss: 0.2249 - val_acc: 0.9436\n",
      "Epoch 177/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1958 - acc: 0.9411\n",
      "Epoch 00177: val_loss did not improve from 0.20715\n",
      "36805/36805 [==============================] - 31s 834us/sample - loss: 0.1959 - acc: 0.9411 - val_loss: 0.2215 - val_acc: 0.9427\n",
      "Epoch 178/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1995 - acc: 0.9402\n",
      "Epoch 00178: val_loss did not improve from 0.20715\n",
      "36805/36805 [==============================] - 30s 828us/sample - loss: 0.1995 - acc: 0.9402 - val_loss: 0.2726 - val_acc: 0.9271\n",
      "Epoch 179/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1949 - acc: 0.9394\n",
      "Epoch 00179: val_loss did not improve from 0.20715\n",
      "36805/36805 [==============================] - 30s 809us/sample - loss: 0.1950 - acc: 0.9393 - val_loss: 0.2109 - val_acc: 0.9415\n",
      "Epoch 180/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1963 - acc: 0.9392\n",
      "Epoch 00180: val_loss did not improve from 0.20715\n",
      "36805/36805 [==============================] - 30s 807us/sample - loss: 0.1963 - acc: 0.9392 - val_loss: 0.2207 - val_acc: 0.9422\n",
      "Epoch 181/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1907 - acc: 0.9423\n",
      "Epoch 00181: val_loss did not improve from 0.20715\n",
      "36805/36805 [==============================] - 30s 813us/sample - loss: 0.1907 - acc: 0.9423 - val_loss: 0.2592 - val_acc: 0.9317\n",
      "Epoch 182/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1908 - acc: 0.9407\n",
      "Epoch 00182: val_loss did not improve from 0.20715\n",
      "36805/36805 [==============================] - 30s 822us/sample - loss: 0.1909 - acc: 0.9407 - val_loss: 0.2221 - val_acc: 0.9450\n",
      "Epoch 183/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1940 - acc: 0.9396\n",
      "Epoch 00183: val_loss did not improve from 0.20715\n",
      "36805/36805 [==============================] - 30s 816us/sample - loss: 0.1940 - acc: 0.9396 - val_loss: 0.2133 - val_acc: 0.9436\n",
      "Epoch 184/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1887 - acc: 0.9421\n",
      "Epoch 00184: val_loss did not improve from 0.20715\n",
      "36805/36805 [==============================] - 30s 824us/sample - loss: 0.1887 - acc: 0.9421 - val_loss: 0.2245 - val_acc: 0.9392\n",
      "Epoch 185/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1869 - acc: 0.9417\n",
      "Epoch 00185: val_loss did not improve from 0.20715\n",
      "36805/36805 [==============================] - 30s 814us/sample - loss: 0.1871 - acc: 0.9416 - val_loss: 0.3224 - val_acc: 0.9082\n",
      "Epoch 186/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1897 - acc: 0.9409\n",
      "Epoch 00186: val_loss did not improve from 0.20715\n",
      "36805/36805 [==============================] - 30s 824us/sample - loss: 0.1898 - acc: 0.9409 - val_loss: 0.2268 - val_acc: 0.9397\n",
      "Epoch 187/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1888 - acc: 0.9434\n",
      "Epoch 00187: val_loss did not improve from 0.20715\n",
      "36805/36805 [==============================] - 31s 836us/sample - loss: 0.1888 - acc: 0.9434 - val_loss: 0.2200 - val_acc: 0.9415\n",
      "Epoch 188/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1857 - acc: 0.9420\n",
      "Epoch 00188: val_loss did not improve from 0.20715\n",
      "36805/36805 [==============================] - 30s 818us/sample - loss: 0.1858 - acc: 0.9420 - val_loss: 0.2093 - val_acc: 0.9441\n",
      "Epoch 189/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1876 - acc: 0.9429\n",
      "Epoch 00189: val_loss did not improve from 0.20715\n",
      "36805/36805 [==============================] - 30s 811us/sample - loss: 0.1877 - acc: 0.9429 - val_loss: 0.2755 - val_acc: 0.9273\n",
      "Epoch 190/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1860 - acc: 0.9429\n",
      "Epoch 00190: val_loss did not improve from 0.20715\n",
      "36805/36805 [==============================] - 30s 827us/sample - loss: 0.1861 - acc: 0.9429 - val_loss: 0.2145 - val_acc: 0.9450\n",
      "Epoch 191/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1877 - acc: 0.9429\n",
      "Epoch 00191: val_loss did not improve from 0.20715\n",
      "36805/36805 [==============================] - 30s 816us/sample - loss: 0.1880 - acc: 0.9429 - val_loss: 0.2175 - val_acc: 0.9413\n",
      "Epoch 192/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1906 - acc: 0.9423\n",
      "Epoch 00192: val_loss did not improve from 0.20715\n",
      "36805/36805 [==============================] - 30s 809us/sample - loss: 0.1906 - acc: 0.9423 - val_loss: 0.2104 - val_acc: 0.9413\n",
      "Epoch 193/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1833 - acc: 0.9434\n",
      "Epoch 00193: val_loss did not improve from 0.20715\n",
      "36805/36805 [==============================] - 30s 809us/sample - loss: 0.1833 - acc: 0.9434 - val_loss: 0.2303 - val_acc: 0.9453\n",
      "Epoch 194/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1784 - acc: 0.9448\n",
      "Epoch 00194: val_loss did not improve from 0.20715\n",
      "36805/36805 [==============================] - 30s 810us/sample - loss: 0.1787 - acc: 0.9447 - val_loss: 0.2150 - val_acc: 0.9408\n",
      "Epoch 195/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1871 - acc: 0.9429\n",
      "Epoch 00195: val_loss did not improve from 0.20715\n",
      "36805/36805 [==============================] - 30s 823us/sample - loss: 0.1871 - acc: 0.9429 - val_loss: 0.2188 - val_acc: 0.9425\n",
      "Epoch 196/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1815 - acc: 0.9447\n",
      "Epoch 00196: val_loss did not improve from 0.20715\n",
      "36805/36805 [==============================] - 30s 813us/sample - loss: 0.1816 - acc: 0.9446 - val_loss: 0.2207 - val_acc: 0.9415\n",
      "Epoch 197/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1886 - acc: 0.9434\n",
      "Epoch 00197: val_loss did not improve from 0.20715\n",
      "36805/36805 [==============================] - 30s 816us/sample - loss: 0.1886 - acc: 0.9434 - val_loss: 0.2156 - val_acc: 0.9439\n",
      "Epoch 198/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1761 - acc: 0.9464\n",
      "Epoch 00198: val_loss did not improve from 0.20715\n",
      "36805/36805 [==============================] - 30s 820us/sample - loss: 0.1764 - acc: 0.9463 - val_loss: 0.2311 - val_acc: 0.9392\n",
      "Epoch 199/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1834 - acc: 0.9434\n",
      "Epoch 00199: val_loss did not improve from 0.20715\n",
      "36805/36805 [==============================] - 30s 817us/sample - loss: 0.1835 - acc: 0.9434 - val_loss: 0.2408 - val_acc: 0.9376\n",
      "Epoch 200/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1802 - acc: 0.9436\n",
      "Epoch 00200: val_loss did not improve from 0.20715\n",
      "36805/36805 [==============================] - 30s 815us/sample - loss: 0.1803 - acc: 0.9436 - val_loss: 0.2683 - val_acc: 0.9280\n",
      "Epoch 201/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1769 - acc: 0.9446\n",
      "Epoch 00201: val_loss did not improve from 0.20715\n",
      "36805/36805 [==============================] - 30s 818us/sample - loss: 0.1774 - acc: 0.9445 - val_loss: 0.2460 - val_acc: 0.9394\n",
      "Epoch 202/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1864 - acc: 0.9436\n",
      "Epoch 00202: val_loss did not improve from 0.20715\n",
      "36805/36805 [==============================] - 30s 821us/sample - loss: 0.1867 - acc: 0.9435 - val_loss: 0.2344 - val_acc: 0.9397\n",
      "Epoch 203/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1850 - acc: 0.9433\n",
      "Epoch 00203: val_loss did not improve from 0.20715\n",
      "36805/36805 [==============================] - 30s 817us/sample - loss: 0.1850 - acc: 0.9433 - val_loss: 0.2292 - val_acc: 0.9429\n",
      "Epoch 204/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1778 - acc: 0.9449\n",
      "Epoch 00204: val_loss did not improve from 0.20715\n",
      "36805/36805 [==============================] - 30s 815us/sample - loss: 0.1779 - acc: 0.9449 - val_loss: 0.2155 - val_acc: 0.9411\n",
      "Epoch 205/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1781 - acc: 0.9460\n",
      "Epoch 00205: val_loss improved from 0.20715 to 0.20671, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_6_conv_checkpoint/205-0.2067.hdf5\n",
      "36805/36805 [==============================] - 30s 821us/sample - loss: 0.1783 - acc: 0.9460 - val_loss: 0.2067 - val_acc: 0.9436\n",
      "Epoch 206/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1729 - acc: 0.9473\n",
      "Epoch 00206: val_loss did not improve from 0.20671\n",
      "36805/36805 [==============================] - 30s 818us/sample - loss: 0.1728 - acc: 0.9474 - val_loss: 0.2218 - val_acc: 0.9460\n",
      "Epoch 207/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1764 - acc: 0.9463\n",
      "Epoch 00207: val_loss did not improve from 0.20671\n",
      "36805/36805 [==============================] - 30s 815us/sample - loss: 0.1764 - acc: 0.9463 - val_loss: 0.2182 - val_acc: 0.9450\n",
      "Epoch 208/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1759 - acc: 0.9458\n",
      "Epoch 00208: val_loss did not improve from 0.20671\n",
      "36805/36805 [==============================] - 30s 822us/sample - loss: 0.1759 - acc: 0.9458 - val_loss: 0.2326 - val_acc: 0.9411\n",
      "Epoch 209/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1725 - acc: 0.9460\n",
      "Epoch 00209: val_loss did not improve from 0.20671\n",
      "36805/36805 [==============================] - 30s 816us/sample - loss: 0.1726 - acc: 0.9460 - val_loss: 0.2697 - val_acc: 0.9322\n",
      "Epoch 210/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1749 - acc: 0.9462\n",
      "Epoch 00210: val_loss did not improve from 0.20671\n",
      "36805/36805 [==============================] - 30s 816us/sample - loss: 0.1749 - acc: 0.9462 - val_loss: 0.2224 - val_acc: 0.9422\n",
      "Epoch 211/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1720 - acc: 0.9464\n",
      "Epoch 00211: val_loss did not improve from 0.20671\n",
      "36805/36805 [==============================] - 30s 818us/sample - loss: 0.1725 - acc: 0.9463 - val_loss: 0.2178 - val_acc: 0.9460\n",
      "Epoch 212/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1815 - acc: 0.9443\n",
      "Epoch 00212: val_loss did not improve from 0.20671\n",
      "36805/36805 [==============================] - 30s 817us/sample - loss: 0.1815 - acc: 0.9443 - val_loss: 0.2150 - val_acc: 0.9436\n",
      "Epoch 213/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1717 - acc: 0.9467\n",
      "Epoch 00213: val_loss did not improve from 0.20671\n",
      "36805/36805 [==============================] - 30s 815us/sample - loss: 0.1717 - acc: 0.9467 - val_loss: 0.2208 - val_acc: 0.9460\n",
      "Epoch 214/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1731 - acc: 0.9479\n",
      "Epoch 00214: val_loss did not improve from 0.20671\n",
      "36805/36805 [==============================] - 30s 813us/sample - loss: 0.1731 - acc: 0.9479 - val_loss: 0.2508 - val_acc: 0.9331\n",
      "Epoch 215/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1710 - acc: 0.9467\n",
      "Epoch 00215: val_loss did not improve from 0.20671\n",
      "36805/36805 [==============================] - 30s 817us/sample - loss: 0.1710 - acc: 0.9467 - val_loss: 0.2183 - val_acc: 0.9434\n",
      "Epoch 216/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1724 - acc: 0.9481\n",
      "Epoch 00216: val_loss did not improve from 0.20671\n",
      "36805/36805 [==============================] - 30s 818us/sample - loss: 0.1724 - acc: 0.9481 - val_loss: 0.2157 - val_acc: 0.9425\n",
      "Epoch 217/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1748 - acc: 0.9468\n",
      "Epoch 00217: val_loss did not improve from 0.20671\n",
      "36805/36805 [==============================] - 30s 821us/sample - loss: 0.1748 - acc: 0.9467 - val_loss: 0.2262 - val_acc: 0.9397\n",
      "Epoch 218/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1716 - acc: 0.9473\n",
      "Epoch 00218: val_loss did not improve from 0.20671\n",
      "36805/36805 [==============================] - 30s 814us/sample - loss: 0.1717 - acc: 0.9473 - val_loss: 0.2312 - val_acc: 0.9432\n",
      "Epoch 219/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1729 - acc: 0.9465\n",
      "Epoch 00219: val_loss did not improve from 0.20671\n",
      "36805/36805 [==============================] - 30s 817us/sample - loss: 0.1732 - acc: 0.9464 - val_loss: 0.2257 - val_acc: 0.9380\n",
      "Epoch 220/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1709 - acc: 0.9462\n",
      "Epoch 00220: val_loss did not improve from 0.20671\n",
      "36805/36805 [==============================] - 30s 814us/sample - loss: 0.1708 - acc: 0.9462 - val_loss: 0.2099 - val_acc: 0.9453\n",
      "Epoch 221/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1677 - acc: 0.9484\n",
      "Epoch 00221: val_loss did not improve from 0.20671\n",
      "36805/36805 [==============================] - 30s 817us/sample - loss: 0.1678 - acc: 0.9484 - val_loss: 0.2127 - val_acc: 0.9441\n",
      "Epoch 222/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1726 - acc: 0.9470\n",
      "Epoch 00222: val_loss did not improve from 0.20671\n",
      "36805/36805 [==============================] - 30s 824us/sample - loss: 0.1726 - acc: 0.9470 - val_loss: 0.2199 - val_acc: 0.9401\n",
      "Epoch 223/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1680 - acc: 0.9475\n",
      "Epoch 00223: val_loss improved from 0.20671 to 0.20274, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_6_conv_checkpoint/223-0.2027.hdf5\n",
      "36805/36805 [==============================] - 30s 826us/sample - loss: 0.1682 - acc: 0.9475 - val_loss: 0.2027 - val_acc: 0.9460\n",
      "Epoch 224/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1712 - acc: 0.9467\n",
      "Epoch 00224: val_loss improved from 0.20274 to 0.20037, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_6_conv_checkpoint/224-0.2004.hdf5\n",
      "36805/36805 [==============================] - 30s 828us/sample - loss: 0.1712 - acc: 0.9467 - val_loss: 0.2004 - val_acc: 0.9471\n",
      "Epoch 225/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1650 - acc: 0.9486\n",
      "Epoch 00225: val_loss did not improve from 0.20037\n",
      "36805/36805 [==============================] - 30s 825us/sample - loss: 0.1651 - acc: 0.9485 - val_loss: 0.2558 - val_acc: 0.9313\n",
      "Epoch 226/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1689 - acc: 0.9470\n",
      "Epoch 00226: val_loss did not improve from 0.20037\n",
      "36805/36805 [==============================] - 31s 829us/sample - loss: 0.1689 - acc: 0.9470 - val_loss: 0.2205 - val_acc: 0.9446\n",
      "Epoch 227/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1654 - acc: 0.9492\n",
      "Epoch 00227: val_loss did not improve from 0.20037\n",
      "36805/36805 [==============================] - 30s 823us/sample - loss: 0.1654 - acc: 0.9492 - val_loss: 0.2193 - val_acc: 0.9399\n",
      "Epoch 228/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1694 - acc: 0.9483\n",
      "Epoch 00228: val_loss did not improve from 0.20037\n",
      "36805/36805 [==============================] - 30s 823us/sample - loss: 0.1694 - acc: 0.9483 - val_loss: 0.2074 - val_acc: 0.9455\n",
      "Epoch 229/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1661 - acc: 0.9501\n",
      "Epoch 00229: val_loss did not improve from 0.20037\n",
      "36805/36805 [==============================] - 30s 824us/sample - loss: 0.1661 - acc: 0.9501 - val_loss: 0.2458 - val_acc: 0.9359\n",
      "Epoch 230/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1639 - acc: 0.9507\n",
      "Epoch 00230: val_loss did not improve from 0.20037\n",
      "36805/36805 [==============================] - 30s 826us/sample - loss: 0.1639 - acc: 0.9507 - val_loss: 0.2330 - val_acc: 0.9404\n",
      "Epoch 231/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1666 - acc: 0.9481\n",
      "Epoch 00231: val_loss did not improve from 0.20037\n",
      "36805/36805 [==============================] - 30s 825us/sample - loss: 0.1667 - acc: 0.9481 - val_loss: 0.2204 - val_acc: 0.9441\n",
      "Epoch 232/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1651 - acc: 0.9493\n",
      "Epoch 00232: val_loss did not improve from 0.20037\n",
      "36805/36805 [==============================] - 30s 824us/sample - loss: 0.1654 - acc: 0.9492 - val_loss: 0.2186 - val_acc: 0.9399\n",
      "Epoch 233/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1605 - acc: 0.9501\n",
      "Epoch 00233: val_loss did not improve from 0.20037\n",
      "36805/36805 [==============================] - 30s 823us/sample - loss: 0.1605 - acc: 0.9501 - val_loss: 0.2644 - val_acc: 0.9329\n",
      "Epoch 234/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1601 - acc: 0.9506\n",
      "Epoch 00234: val_loss did not improve from 0.20037\n",
      "36805/36805 [==============================] - 30s 823us/sample - loss: 0.1604 - acc: 0.9505 - val_loss: 0.2191 - val_acc: 0.9432\n",
      "Epoch 235/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1639 - acc: 0.9492\n",
      "Epoch 00235: val_loss did not improve from 0.20037\n",
      "36805/36805 [==============================] - 30s 827us/sample - loss: 0.1640 - acc: 0.9492 - val_loss: 0.2442 - val_acc: 0.9378\n",
      "Epoch 236/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1648 - acc: 0.9496\n",
      "Epoch 00236: val_loss did not improve from 0.20037\n",
      "36805/36805 [==============================] - 30s 825us/sample - loss: 0.1649 - acc: 0.9496 - val_loss: 0.2073 - val_acc: 0.9499\n",
      "Epoch 237/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1611 - acc: 0.9490\n",
      "Epoch 00237: val_loss did not improve from 0.20037\n",
      "36805/36805 [==============================] - 30s 822us/sample - loss: 0.1611 - acc: 0.9490 - val_loss: 0.2230 - val_acc: 0.9441\n",
      "Epoch 238/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1576 - acc: 0.9518\n",
      "Epoch 00238: val_loss did not improve from 0.20037\n",
      "36805/36805 [==============================] - 30s 824us/sample - loss: 0.1576 - acc: 0.9519 - val_loss: 0.2177 - val_acc: 0.9455\n",
      "Epoch 239/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1611 - acc: 0.9504\n",
      "Epoch 00239: val_loss did not improve from 0.20037\n",
      "36805/36805 [==============================] - 30s 825us/sample - loss: 0.1617 - acc: 0.9503 - val_loss: 0.2499 - val_acc: 0.9357\n",
      "Epoch 240/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1639 - acc: 0.9489\n",
      "Epoch 00240: val_loss did not improve from 0.20037\n",
      "36805/36805 [==============================] - 30s 822us/sample - loss: 0.1641 - acc: 0.9489 - val_loss: 0.2328 - val_acc: 0.9408\n",
      "Epoch 241/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1646 - acc: 0.9490\n",
      "Epoch 00241: val_loss did not improve from 0.20037\n",
      "36805/36805 [==============================] - 30s 825us/sample - loss: 0.1647 - acc: 0.9490 - val_loss: 0.2196 - val_acc: 0.9476\n",
      "Epoch 242/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1550 - acc: 0.9521\n",
      "Epoch 00242: val_loss did not improve from 0.20037\n",
      "36805/36805 [==============================] - 30s 824us/sample - loss: 0.1551 - acc: 0.9521 - val_loss: 0.2252 - val_acc: 0.9450\n",
      "Epoch 243/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1618 - acc: 0.9505\n",
      "Epoch 00243: val_loss did not improve from 0.20037\n",
      "36805/36805 [==============================] - 30s 816us/sample - loss: 0.1618 - acc: 0.9505 - val_loss: 0.2341 - val_acc: 0.9380\n",
      "Epoch 244/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1547 - acc: 0.9517\n",
      "Epoch 00244: val_loss did not improve from 0.20037\n",
      "36805/36805 [==============================] - 30s 817us/sample - loss: 0.1547 - acc: 0.9517 - val_loss: 0.2322 - val_acc: 0.9432\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 245/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1548 - acc: 0.9530\n",
      "Epoch 00245: val_loss did not improve from 0.20037\n",
      "36805/36805 [==============================] - 30s 813us/sample - loss: 0.1548 - acc: 0.9530 - val_loss: 0.2204 - val_acc: 0.9462\n",
      "Epoch 246/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1544 - acc: 0.9532\n",
      "Epoch 00246: val_loss did not improve from 0.20037\n",
      "36805/36805 [==============================] - 30s 821us/sample - loss: 0.1543 - acc: 0.9532 - val_loss: 0.2391 - val_acc: 0.9397\n",
      "Epoch 247/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1537 - acc: 0.9521\n",
      "Epoch 00247: val_loss did not improve from 0.20037\n",
      "36805/36805 [==============================] - 30s 818us/sample - loss: 0.1537 - acc: 0.9520 - val_loss: 0.2185 - val_acc: 0.9457\n",
      "Epoch 248/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1523 - acc: 0.9523\n",
      "Epoch 00248: val_loss did not improve from 0.20037\n",
      "36805/36805 [==============================] - 30s 817us/sample - loss: 0.1523 - acc: 0.9524 - val_loss: 0.2297 - val_acc: 0.9450\n",
      "Epoch 249/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1581 - acc: 0.9503\n",
      "Epoch 00249: val_loss did not improve from 0.20037\n",
      "36805/36805 [==============================] - 30s 817us/sample - loss: 0.1581 - acc: 0.9503 - val_loss: 0.2996 - val_acc: 0.9292\n",
      "Epoch 250/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1533 - acc: 0.9533\n",
      "Epoch 00250: val_loss did not improve from 0.20037\n",
      "36805/36805 [==============================] - 30s 817us/sample - loss: 0.1532 - acc: 0.9533 - val_loss: 0.2188 - val_acc: 0.9427\n",
      "Epoch 251/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1514 - acc: 0.9528\n",
      "Epoch 00251: val_loss did not improve from 0.20037\n",
      "36805/36805 [==============================] - 30s 817us/sample - loss: 0.1514 - acc: 0.9528 - val_loss: 0.2290 - val_acc: 0.9467\n",
      "Epoch 252/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1521 - acc: 0.9518\n",
      "Epoch 00252: val_loss did not improve from 0.20037\n",
      "36805/36805 [==============================] - 30s 819us/sample - loss: 0.1520 - acc: 0.9518 - val_loss: 0.2415 - val_acc: 0.9371\n",
      "Epoch 253/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1537 - acc: 0.9533\n",
      "Epoch 00253: val_loss did not improve from 0.20037\n",
      "36805/36805 [==============================] - 30s 816us/sample - loss: 0.1537 - acc: 0.9533 - val_loss: 0.2160 - val_acc: 0.9450\n",
      "Epoch 254/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1549 - acc: 0.9515\n",
      "Epoch 00254: val_loss did not improve from 0.20037\n",
      "36805/36805 [==============================] - 30s 815us/sample - loss: 0.1548 - acc: 0.9516 - val_loss: 0.2176 - val_acc: 0.9450\n",
      "Epoch 255/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1551 - acc: 0.9526\n",
      "Epoch 00255: val_loss did not improve from 0.20037\n",
      "36805/36805 [==============================] - 30s 817us/sample - loss: 0.1552 - acc: 0.9526 - val_loss: 0.2403 - val_acc: 0.9427\n",
      "Epoch 256/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1563 - acc: 0.9521\n",
      "Epoch 00256: val_loss did not improve from 0.20037\n",
      "36805/36805 [==============================] - 30s 818us/sample - loss: 0.1563 - acc: 0.9521 - val_loss: 0.2074 - val_acc: 0.9478\n",
      "Epoch 257/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1539 - acc: 0.9524\n",
      "Epoch 00257: val_loss did not improve from 0.20037\n",
      "36805/36805 [==============================] - 30s 814us/sample - loss: 0.1539 - acc: 0.9524 - val_loss: 0.2177 - val_acc: 0.9490\n",
      "Epoch 258/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1505 - acc: 0.9533\n",
      "Epoch 00258: val_loss did not improve from 0.20037\n",
      "36805/36805 [==============================] - 30s 817us/sample - loss: 0.1505 - acc: 0.9532 - val_loss: 0.2365 - val_acc: 0.9422\n",
      "Epoch 259/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1494 - acc: 0.9537\n",
      "Epoch 00259: val_loss did not improve from 0.20037\n",
      "36805/36805 [==============================] - 30s 822us/sample - loss: 0.1494 - acc: 0.9537 - val_loss: 0.2208 - val_acc: 0.9453\n",
      "Epoch 260/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1481 - acc: 0.9541\n",
      "Epoch 00260: val_loss did not improve from 0.20037\n",
      "36805/36805 [==============================] - 30s 823us/sample - loss: 0.1481 - acc: 0.9541 - val_loss: 0.2154 - val_acc: 0.9462\n",
      "Epoch 261/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1485 - acc: 0.9545\n",
      "Epoch 00261: val_loss did not improve from 0.20037\n",
      "36805/36805 [==============================] - 30s 826us/sample - loss: 0.1485 - acc: 0.9545 - val_loss: 0.2165 - val_acc: 0.9474\n",
      "Epoch 262/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1496 - acc: 0.9539\n",
      "Epoch 00262: val_loss did not improve from 0.20037\n",
      "36805/36805 [==============================] - 30s 817us/sample - loss: 0.1496 - acc: 0.9539 - val_loss: 0.2147 - val_acc: 0.9481\n",
      "Epoch 263/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1498 - acc: 0.9528\n",
      "Epoch 00263: val_loss did not improve from 0.20037\n",
      "36805/36805 [==============================] - 30s 826us/sample - loss: 0.1499 - acc: 0.9528 - val_loss: 0.4363 - val_acc: 0.8917\n",
      "Epoch 264/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1504 - acc: 0.9535\n",
      "Epoch 00264: val_loss did not improve from 0.20037\n",
      "36805/36805 [==============================] - 30s 815us/sample - loss: 0.1506 - acc: 0.9534 - val_loss: 0.2263 - val_acc: 0.9429\n",
      "Epoch 265/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1510 - acc: 0.9511\n",
      "Epoch 00265: val_loss did not improve from 0.20037\n",
      "36805/36805 [==============================] - 30s 820us/sample - loss: 0.1512 - acc: 0.9510 - val_loss: 0.2079 - val_acc: 0.9469\n",
      "Epoch 266/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1543 - acc: 0.9523\n",
      "Epoch 00266: val_loss did not improve from 0.20037\n",
      "36805/36805 [==============================] - 30s 816us/sample - loss: 0.1541 - acc: 0.9524 - val_loss: 0.5397 - val_acc: 0.8838\n",
      "Epoch 267/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1407 - acc: 0.9564\n",
      "Epoch 00267: val_loss did not improve from 0.20037\n",
      "36805/36805 [==============================] - 30s 814us/sample - loss: 0.1408 - acc: 0.9564 - val_loss: 0.2138 - val_acc: 0.9497\n",
      "Epoch 268/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1437 - acc: 0.9549\n",
      "Epoch 00268: val_loss did not improve from 0.20037\n",
      "36805/36805 [==============================] - 30s 813us/sample - loss: 0.1437 - acc: 0.9549 - val_loss: 0.2790 - val_acc: 0.9280\n",
      "Epoch 269/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1456 - acc: 0.9546\n",
      "Epoch 00269: val_loss did not improve from 0.20037\n",
      "36805/36805 [==============================] - 30s 817us/sample - loss: 0.1455 - acc: 0.9546 - val_loss: 0.2072 - val_acc: 0.9488\n",
      "Epoch 270/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1406 - acc: 0.9563\n",
      "Epoch 00270: val_loss did not improve from 0.20037\n",
      "36805/36805 [==============================] - 30s 814us/sample - loss: 0.1406 - acc: 0.9563 - val_loss: 0.2268 - val_acc: 0.9443\n",
      "Epoch 271/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1435 - acc: 0.9542\n",
      "Epoch 00271: val_loss did not improve from 0.20037\n",
      "36805/36805 [==============================] - 30s 813us/sample - loss: 0.1438 - acc: 0.9542 - val_loss: 0.2269 - val_acc: 0.9455\n",
      "Epoch 272/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1543 - acc: 0.9513\n",
      "Epoch 00272: val_loss did not improve from 0.20037\n",
      "36805/36805 [==============================] - 30s 819us/sample - loss: 0.1543 - acc: 0.9512 - val_loss: 0.2283 - val_acc: 0.9446\n",
      "Epoch 273/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1391 - acc: 0.9564\n",
      "Epoch 00273: val_loss did not improve from 0.20037\n",
      "36805/36805 [==============================] - 30s 818us/sample - loss: 0.1391 - acc: 0.9564 - val_loss: 0.2291 - val_acc: 0.9446\n",
      "Epoch 274/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1423 - acc: 0.9561\n",
      "Epoch 00274: val_loss did not improve from 0.20037\n",
      "36805/36805 [==============================] - 30s 817us/sample - loss: 0.1423 - acc: 0.9561 - val_loss: 0.2276 - val_acc: 0.9464\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_6_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 428us/sample - loss: 0.2341 - acc: 0.9302\n",
      "Loss: 0.2340986780040856 Accuracy: 0.93021804\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.2961 - acc: 0.2758\n",
      "Epoch 00001: val_loss improved from inf to 1.88348, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_7_conv_checkpoint/001-1.8835.hdf5\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 2.2961 - acc: 0.2758 - val_loss: 1.8835 - val_acc: 0.4927\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6109 - acc: 0.4992\n",
      "Epoch 00002: val_loss improved from 1.88348 to 1.14890, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_7_conv_checkpoint/002-1.1489.hdf5\n",
      "36805/36805 [==============================] - 32s 860us/sample - loss: 1.6108 - acc: 0.4992 - val_loss: 1.1489 - val_acc: 0.7093\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2998 - acc: 0.6125\n",
      "Epoch 00003: val_loss improved from 1.14890 to 0.92175, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_7_conv_checkpoint/003-0.9218.hdf5\n",
      "36805/36805 [==============================] - 32s 867us/sample - loss: 1.2997 - acc: 0.6125 - val_loss: 0.9218 - val_acc: 0.7838\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0892 - acc: 0.6902\n",
      "Epoch 00004: val_loss improved from 0.92175 to 0.76278, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_7_conv_checkpoint/004-0.7628.hdf5\n",
      "36805/36805 [==============================] - 32s 866us/sample - loss: 1.0893 - acc: 0.6902 - val_loss: 0.7628 - val_acc: 0.8204\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9406 - acc: 0.7374\n",
      "Epoch 00005: val_loss improved from 0.76278 to 0.68280, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_7_conv_checkpoint/005-0.6828.hdf5\n",
      "36805/36805 [==============================] - 32s 863us/sample - loss: 0.9406 - acc: 0.7375 - val_loss: 0.6828 - val_acc: 0.8237\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8205 - acc: 0.7729\n",
      "Epoch 00006: val_loss improved from 0.68280 to 0.58742, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_7_conv_checkpoint/006-0.5874.hdf5\n",
      "36805/36805 [==============================] - 32s 864us/sample - loss: 0.8205 - acc: 0.7729 - val_loss: 0.5874 - val_acc: 0.8612\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7346 - acc: 0.8001\n",
      "Epoch 00007: val_loss improved from 0.58742 to 0.47833, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_7_conv_checkpoint/007-0.4783.hdf5\n",
      "36805/36805 [==============================] - 32s 864us/sample - loss: 0.7347 - acc: 0.8001 - val_loss: 0.4783 - val_acc: 0.8845\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6680 - acc: 0.8182\n",
      "Epoch 00008: val_loss improved from 0.47833 to 0.47041, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_7_conv_checkpoint/008-0.4704.hdf5\n",
      "36805/36805 [==============================] - 32s 866us/sample - loss: 0.6680 - acc: 0.8182 - val_loss: 0.4704 - val_acc: 0.8800\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6131 - acc: 0.8309\n",
      "Epoch 00009: val_loss improved from 0.47041 to 0.42028, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_7_conv_checkpoint/009-0.4203.hdf5\n",
      "36805/36805 [==============================] - 32s 868us/sample - loss: 0.6131 - acc: 0.8309 - val_loss: 0.4203 - val_acc: 0.8912\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5703 - acc: 0.8448\n",
      "Epoch 00010: val_loss improved from 0.42028 to 0.35313, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_7_conv_checkpoint/010-0.3531.hdf5\n",
      "36805/36805 [==============================] - 32s 866us/sample - loss: 0.5703 - acc: 0.8448 - val_loss: 0.3531 - val_acc: 0.9099\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5299 - acc: 0.8528\n",
      "Epoch 00011: val_loss improved from 0.35313 to 0.33125, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_7_conv_checkpoint/011-0.3313.hdf5\n",
      "36805/36805 [==============================] - 32s 863us/sample - loss: 0.5299 - acc: 0.8528 - val_loss: 0.3313 - val_acc: 0.9178\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5031 - acc: 0.8613\n",
      "Epoch 00012: val_loss improved from 0.33125 to 0.32173, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_7_conv_checkpoint/012-0.3217.hdf5\n",
      "36805/36805 [==============================] - 32s 864us/sample - loss: 0.5030 - acc: 0.8613 - val_loss: 0.3217 - val_acc: 0.9171\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4733 - acc: 0.8695\n",
      "Epoch 00013: val_loss improved from 0.32173 to 0.30102, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_7_conv_checkpoint/013-0.3010.hdf5\n",
      "36805/36805 [==============================] - 32s 869us/sample - loss: 0.4733 - acc: 0.8695 - val_loss: 0.3010 - val_acc: 0.9320\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4519 - acc: 0.8738\n",
      "Epoch 00014: val_loss did not improve from 0.30102\n",
      "36805/36805 [==============================] - 32s 860us/sample - loss: 0.4518 - acc: 0.8738 - val_loss: 0.3014 - val_acc: 0.9217\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4313 - acc: 0.8787\n",
      "Epoch 00015: val_loss improved from 0.30102 to 0.25888, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_7_conv_checkpoint/015-0.2589.hdf5\n",
      "36805/36805 [==============================] - 32s 865us/sample - loss: 0.4312 - acc: 0.8787 - val_loss: 0.2589 - val_acc: 0.9324\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4125 - acc: 0.8838\n",
      "Epoch 00016: val_loss improved from 0.25888 to 0.24406, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_7_conv_checkpoint/016-0.2441.hdf5\n",
      "36805/36805 [==============================] - 32s 867us/sample - loss: 0.4126 - acc: 0.8838 - val_loss: 0.2441 - val_acc: 0.9364\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3899 - acc: 0.8919\n",
      "Epoch 00017: val_loss did not improve from 0.24406\n",
      "36805/36805 [==============================] - 32s 859us/sample - loss: 0.3899 - acc: 0.8919 - val_loss: 0.2781 - val_acc: 0.9264\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3803 - acc: 0.8923\n",
      "Epoch 00018: val_loss improved from 0.24406 to 0.24219, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_7_conv_checkpoint/018-0.2422.hdf5\n",
      "36805/36805 [==============================] - 32s 867us/sample - loss: 0.3804 - acc: 0.8922 - val_loss: 0.2422 - val_acc: 0.9355\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3702 - acc: 0.8974\n",
      "Epoch 00019: val_loss improved from 0.24219 to 0.22688, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_7_conv_checkpoint/019-0.2269.hdf5\n",
      "36805/36805 [==============================] - 32s 864us/sample - loss: 0.3701 - acc: 0.8974 - val_loss: 0.2269 - val_acc: 0.9392\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3565 - acc: 0.8989\n",
      "Epoch 00020: val_loss improved from 0.22688 to 0.22541, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_7_conv_checkpoint/020-0.2254.hdf5\n",
      "36805/36805 [==============================] - 32s 867us/sample - loss: 0.3566 - acc: 0.8989 - val_loss: 0.2254 - val_acc: 0.9380\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3489 - acc: 0.8992\n",
      "Epoch 00021: val_loss did not improve from 0.22541\n",
      "36805/36805 [==============================] - 32s 860us/sample - loss: 0.3490 - acc: 0.8992 - val_loss: 0.2361 - val_acc: 0.9357\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3400 - acc: 0.9021\n",
      "Epoch 00022: val_loss improved from 0.22541 to 0.20389, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_7_conv_checkpoint/022-0.2039.hdf5\n",
      "36805/36805 [==============================] - 32s 859us/sample - loss: 0.3399 - acc: 0.9021 - val_loss: 0.2039 - val_acc: 0.9460\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3277 - acc: 0.9051\n",
      "Epoch 00023: val_loss did not improve from 0.20389\n",
      "36805/36805 [==============================] - 32s 860us/sample - loss: 0.3277 - acc: 0.9051 - val_loss: 0.2201 - val_acc: 0.9427\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3159 - acc: 0.9096\n",
      "Epoch 00024: val_loss improved from 0.20389 to 0.19452, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_7_conv_checkpoint/024-0.1945.hdf5\n",
      "36805/36805 [==============================] - 32s 865us/sample - loss: 0.3160 - acc: 0.9096 - val_loss: 0.1945 - val_acc: 0.9462\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3086 - acc: 0.9127\n",
      "Epoch 00025: val_loss improved from 0.19452 to 0.18794, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_7_conv_checkpoint/025-0.1879.hdf5\n",
      "36805/36805 [==============================] - 32s 861us/sample - loss: 0.3086 - acc: 0.9127 - val_loss: 0.1879 - val_acc: 0.9495\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2958 - acc: 0.9142\n",
      "Epoch 00026: val_loss did not improve from 0.18794\n",
      "36805/36805 [==============================] - 32s 859us/sample - loss: 0.2961 - acc: 0.9141 - val_loss: 0.2008 - val_acc: 0.9443\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2976 - acc: 0.9136\n",
      "Epoch 00027: val_loss did not improve from 0.18794\n",
      "36805/36805 [==============================] - 32s 862us/sample - loss: 0.2977 - acc: 0.9135 - val_loss: 0.1900 - val_acc: 0.9460\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2953 - acc: 0.9152\n",
      "Epoch 00028: val_loss did not improve from 0.18794\n",
      "36805/36805 [==============================] - 32s 859us/sample - loss: 0.2953 - acc: 0.9152 - val_loss: 0.1888 - val_acc: 0.9478\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2816 - acc: 0.9185\n",
      "Epoch 00029: val_loss did not improve from 0.18794\n",
      "36805/36805 [==============================] - 32s 864us/sample - loss: 0.2817 - acc: 0.9184 - val_loss: 0.1927 - val_acc: 0.9460\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2766 - acc: 0.9202\n",
      "Epoch 00030: val_loss did not improve from 0.18794\n",
      "36805/36805 [==============================] - 32s 861us/sample - loss: 0.2766 - acc: 0.9202 - val_loss: 0.1930 - val_acc: 0.9448\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2684 - acc: 0.9221\n",
      "Epoch 00031: val_loss did not improve from 0.18794\n",
      "36805/36805 [==============================] - 32s 857us/sample - loss: 0.2684 - acc: 0.9221 - val_loss: 0.1994 - val_acc: 0.9441\n",
      "Epoch 32/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2672 - acc: 0.9221\n",
      "Epoch 00032: val_loss did not improve from 0.18794\n",
      "36805/36805 [==============================] - 32s 859us/sample - loss: 0.2674 - acc: 0.9220 - val_loss: 0.1955 - val_acc: 0.9432\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2582 - acc: 0.9253\n",
      "Epoch 00033: val_loss did not improve from 0.18794\n",
      "36805/36805 [==============================] - 32s 860us/sample - loss: 0.2583 - acc: 0.9253 - val_loss: 0.1978 - val_acc: 0.9413\n",
      "Epoch 34/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2535 - acc: 0.9274\n",
      "Epoch 00034: val_loss did not improve from 0.18794\n",
      "36805/36805 [==============================] - 32s 863us/sample - loss: 0.2534 - acc: 0.9275 - val_loss: 0.1899 - val_acc: 0.9478\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2512 - acc: 0.9269\n",
      "Epoch 00035: val_loss did not improve from 0.18794\n",
      "36805/36805 [==============================] - 32s 862us/sample - loss: 0.2511 - acc: 0.9269 - val_loss: 0.3177 - val_acc: 0.9080\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2457 - acc: 0.9286\n",
      "Epoch 00036: val_loss improved from 0.18794 to 0.18363, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_7_conv_checkpoint/036-0.1836.hdf5\n",
      "36805/36805 [==============================] - 32s 863us/sample - loss: 0.2457 - acc: 0.9286 - val_loss: 0.1836 - val_acc: 0.9460\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2439 - acc: 0.9282\n",
      "Epoch 00037: val_loss did not improve from 0.18363\n",
      "36805/36805 [==============================] - 32s 860us/sample - loss: 0.2438 - acc: 0.9282 - val_loss: 0.1874 - val_acc: 0.9429\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2349 - acc: 0.9318\n",
      "Epoch 00038: val_loss improved from 0.18363 to 0.17464, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_7_conv_checkpoint/038-0.1746.hdf5\n",
      "36805/36805 [==============================] - 32s 863us/sample - loss: 0.2349 - acc: 0.9318 - val_loss: 0.1746 - val_acc: 0.9471\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2305 - acc: 0.9328\n",
      "Epoch 00039: val_loss improved from 0.17464 to 0.16635, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_7_conv_checkpoint/039-0.1664.hdf5\n",
      "36805/36805 [==============================] - 32s 865us/sample - loss: 0.2306 - acc: 0.9328 - val_loss: 0.1664 - val_acc: 0.9527\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2322 - acc: 0.9315\n",
      "Epoch 00040: val_loss improved from 0.16635 to 0.16268, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_7_conv_checkpoint/040-0.1627.hdf5\n",
      "36805/36805 [==============================] - 32s 866us/sample - loss: 0.2323 - acc: 0.9315 - val_loss: 0.1627 - val_acc: 0.9527\n",
      "Epoch 41/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2282 - acc: 0.9338\n",
      "Epoch 00041: val_loss did not improve from 0.16268\n",
      "36805/36805 [==============================] - 32s 860us/sample - loss: 0.2282 - acc: 0.9338 - val_loss: 0.2034 - val_acc: 0.9383\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2198 - acc: 0.9363\n",
      "Epoch 00042: val_loss did not improve from 0.16268\n",
      "36805/36805 [==============================] - 32s 861us/sample - loss: 0.2198 - acc: 0.9363 - val_loss: 0.1975 - val_acc: 0.9422\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2168 - acc: 0.9375\n",
      "Epoch 00043: val_loss did not improve from 0.16268\n",
      "36805/36805 [==============================] - 32s 858us/sample - loss: 0.2169 - acc: 0.9375 - val_loss: 0.1789 - val_acc: 0.9467\n",
      "Epoch 44/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2195 - acc: 0.9356\n",
      "Epoch 00044: val_loss did not improve from 0.16268\n",
      "36805/36805 [==============================] - 32s 859us/sample - loss: 0.2193 - acc: 0.9357 - val_loss: 0.1752 - val_acc: 0.9492\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2120 - acc: 0.9376\n",
      "Epoch 00045: val_loss did not improve from 0.16268\n",
      "36805/36805 [==============================] - 32s 862us/sample - loss: 0.2120 - acc: 0.9376 - val_loss: 0.1705 - val_acc: 0.9497\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2094 - acc: 0.9379\n",
      "Epoch 00046: val_loss did not improve from 0.16268\n",
      "36805/36805 [==============================] - 32s 860us/sample - loss: 0.2094 - acc: 0.9379 - val_loss: 0.1666 - val_acc: 0.9497\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2037 - acc: 0.9392\n",
      "Epoch 00047: val_loss did not improve from 0.16268\n",
      "36805/36805 [==============================] - 32s 859us/sample - loss: 0.2037 - acc: 0.9392 - val_loss: 0.1724 - val_acc: 0.9462\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2022 - acc: 0.9389\n",
      "Epoch 00048: val_loss improved from 0.16268 to 0.15098, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_7_conv_checkpoint/048-0.1510.hdf5\n",
      "36805/36805 [==============================] - 32s 866us/sample - loss: 0.2024 - acc: 0.9388 - val_loss: 0.1510 - val_acc: 0.9522\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1995 - acc: 0.9404\n",
      "Epoch 00049: val_loss did not improve from 0.15098\n",
      "36805/36805 [==============================] - 32s 863us/sample - loss: 0.1995 - acc: 0.9404 - val_loss: 0.1674 - val_acc: 0.9488\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1960 - acc: 0.9425\n",
      "Epoch 00050: val_loss did not improve from 0.15098\n",
      "36805/36805 [==============================] - 32s 860us/sample - loss: 0.1960 - acc: 0.9425 - val_loss: 0.1596 - val_acc: 0.9553\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1940 - acc: 0.9425\n",
      "Epoch 00051: val_loss did not improve from 0.15098\n",
      "36805/36805 [==============================] - 32s 860us/sample - loss: 0.1941 - acc: 0.9425 - val_loss: 0.1550 - val_acc: 0.9534\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1921 - acc: 0.9435\n",
      "Epoch 00052: val_loss did not improve from 0.15098\n",
      "36805/36805 [==============================] - 32s 859us/sample - loss: 0.1924 - acc: 0.9435 - val_loss: 0.1605 - val_acc: 0.9527\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1922 - acc: 0.9435\n",
      "Epoch 00053: val_loss did not improve from 0.15098\n",
      "36805/36805 [==============================] - 32s 861us/sample - loss: 0.1922 - acc: 0.9435 - val_loss: 0.1522 - val_acc: 0.9546\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1838 - acc: 0.9447\n",
      "Epoch 00054: val_loss did not improve from 0.15098\n",
      "36805/36805 [==============================] - 32s 859us/sample - loss: 0.1838 - acc: 0.9447 - val_loss: 0.1644 - val_acc: 0.9502\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1797 - acc: 0.9472\n",
      "Epoch 00055: val_loss did not improve from 0.15098\n",
      "36805/36805 [==============================] - 32s 861us/sample - loss: 0.1797 - acc: 0.9472 - val_loss: 0.1549 - val_acc: 0.9553\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1815 - acc: 0.9468\n",
      "Epoch 00056: val_loss did not improve from 0.15098\n",
      "36805/36805 [==============================] - 32s 863us/sample - loss: 0.1814 - acc: 0.9468 - val_loss: 0.1510 - val_acc: 0.9557\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1794 - acc: 0.9463\n",
      "Epoch 00057: val_loss did not improve from 0.15098\n",
      "36805/36805 [==============================] - 32s 865us/sample - loss: 0.1793 - acc: 0.9463 - val_loss: 0.1606 - val_acc: 0.9497\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1769 - acc: 0.9468\n",
      "Epoch 00058: val_loss did not improve from 0.15098\n",
      "36805/36805 [==============================] - 32s 863us/sample - loss: 0.1768 - acc: 0.9468 - val_loss: 0.1662 - val_acc: 0.9511\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1720 - acc: 0.9487\n",
      "Epoch 00059: val_loss did not improve from 0.15098\n",
      "36805/36805 [==============================] - 32s 861us/sample - loss: 0.1719 - acc: 0.9487 - val_loss: 0.1693 - val_acc: 0.9485\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1713 - acc: 0.9485\n",
      "Epoch 00060: val_loss improved from 0.15098 to 0.14416, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_7_conv_checkpoint/060-0.1442.hdf5\n",
      "36805/36805 [==============================] - 32s 867us/sample - loss: 0.1713 - acc: 0.9485 - val_loss: 0.1442 - val_acc: 0.9548\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1690 - acc: 0.9494\n",
      "Epoch 00061: val_loss did not improve from 0.14416\n",
      "36805/36805 [==============================] - 32s 871us/sample - loss: 0.1693 - acc: 0.9494 - val_loss: 0.1641 - val_acc: 0.9513\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1678 - acc: 0.9505\n",
      "Epoch 00062: val_loss improved from 0.14416 to 0.14282, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_7_conv_checkpoint/062-0.1428.hdf5\n",
      "36805/36805 [==============================] - 32s 867us/sample - loss: 0.1678 - acc: 0.9504 - val_loss: 0.1428 - val_acc: 0.9562\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1606 - acc: 0.9518\n",
      "Epoch 00063: val_loss did not improve from 0.14282\n",
      "36805/36805 [==============================] - 32s 864us/sample - loss: 0.1606 - acc: 0.9518 - val_loss: 0.1692 - val_acc: 0.9502\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1638 - acc: 0.9511\n",
      "Epoch 00064: val_loss did not improve from 0.14282\n",
      "36805/36805 [==============================] - 32s 860us/sample - loss: 0.1638 - acc: 0.9510 - val_loss: 0.1528 - val_acc: 0.9520\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1591 - acc: 0.9528\n",
      "Epoch 00065: val_loss did not improve from 0.14282\n",
      "36805/36805 [==============================] - 32s 866us/sample - loss: 0.1591 - acc: 0.9528 - val_loss: 0.1519 - val_acc: 0.9515\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1600 - acc: 0.9516\n",
      "Epoch 00066: val_loss improved from 0.14282 to 0.14230, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_7_conv_checkpoint/066-0.1423.hdf5\n",
      "36805/36805 [==============================] - 32s 869us/sample - loss: 0.1601 - acc: 0.9516 - val_loss: 0.1423 - val_acc: 0.9567\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1596 - acc: 0.9516\n",
      "Epoch 00067: val_loss did not improve from 0.14230\n",
      "36805/36805 [==============================] - 32s 864us/sample - loss: 0.1596 - acc: 0.9516 - val_loss: 0.1545 - val_acc: 0.9557\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1519 - acc: 0.9549\n",
      "Epoch 00068: val_loss did not improve from 0.14230\n",
      "36805/36805 [==============================] - 32s 862us/sample - loss: 0.1520 - acc: 0.9548 - val_loss: 0.1641 - val_acc: 0.9527\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1527 - acc: 0.9546\n",
      "Epoch 00069: val_loss did not improve from 0.14230\n",
      "36805/36805 [==============================] - 32s 859us/sample - loss: 0.1528 - acc: 0.9546 - val_loss: 0.1568 - val_acc: 0.9520\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1514 - acc: 0.9546\n",
      "Epoch 00070: val_loss did not improve from 0.14230\n",
      "36805/36805 [==============================] - 32s 864us/sample - loss: 0.1515 - acc: 0.9546 - val_loss: 0.1952 - val_acc: 0.9397\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1476 - acc: 0.9563\n",
      "Epoch 00071: val_loss improved from 0.14230 to 0.13901, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_7_conv_checkpoint/071-0.1390.hdf5\n",
      "36805/36805 [==============================] - 32s 865us/sample - loss: 0.1477 - acc: 0.9563 - val_loss: 0.1390 - val_acc: 0.9590\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1472 - acc: 0.9547\n",
      "Epoch 00072: val_loss did not improve from 0.13901\n",
      "36805/36805 [==============================] - 32s 861us/sample - loss: 0.1471 - acc: 0.9547 - val_loss: 0.1462 - val_acc: 0.9543\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1446 - acc: 0.9566\n",
      "Epoch 00073: val_loss did not improve from 0.13901\n",
      "36805/36805 [==============================] - 32s 864us/sample - loss: 0.1446 - acc: 0.9566 - val_loss: 0.1499 - val_acc: 0.9539\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1416 - acc: 0.9573\n",
      "Epoch 00074: val_loss did not improve from 0.13901\n",
      "36805/36805 [==============================] - 32s 862us/sample - loss: 0.1416 - acc: 0.9573 - val_loss: 0.1789 - val_acc: 0.9497\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1456 - acc: 0.9555\n",
      "Epoch 00075: val_loss did not improve from 0.13901\n",
      "36805/36805 [==============================] - 32s 861us/sample - loss: 0.1456 - acc: 0.9555 - val_loss: 0.1909 - val_acc: 0.9462\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1406 - acc: 0.9565\n",
      "Epoch 00076: val_loss did not improve from 0.13901\n",
      "36805/36805 [==============================] - 32s 863us/sample - loss: 0.1406 - acc: 0.9565 - val_loss: 0.1394 - val_acc: 0.9599\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1412 - acc: 0.9575\n",
      "Epoch 00077: val_loss did not improve from 0.13901\n",
      "36805/36805 [==============================] - 32s 860us/sample - loss: 0.1411 - acc: 0.9575 - val_loss: 0.1503 - val_acc: 0.9534\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1385 - acc: 0.9569\n",
      "Epoch 00078: val_loss did not improve from 0.13901\n",
      "36805/36805 [==============================] - 32s 862us/sample - loss: 0.1387 - acc: 0.9569 - val_loss: 0.1479 - val_acc: 0.9543\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1357 - acc: 0.9579\n",
      "Epoch 00079: val_loss did not improve from 0.13901\n",
      "36805/36805 [==============================] - 32s 865us/sample - loss: 0.1358 - acc: 0.9578 - val_loss: 0.1710 - val_acc: 0.9495\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1388 - acc: 0.9581\n",
      "Epoch 00080: val_loss did not improve from 0.13901\n",
      "36805/36805 [==============================] - 32s 863us/sample - loss: 0.1388 - acc: 0.9581 - val_loss: 0.1449 - val_acc: 0.9578\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1303 - acc: 0.9607\n",
      "Epoch 00081: val_loss did not improve from 0.13901\n",
      "36805/36805 [==============================] - 32s 865us/sample - loss: 0.1305 - acc: 0.9607 - val_loss: 0.1662 - val_acc: 0.9497\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1357 - acc: 0.9588\n",
      "Epoch 00082: val_loss did not improve from 0.13901\n",
      "36805/36805 [==============================] - 32s 864us/sample - loss: 0.1358 - acc: 0.9587 - val_loss: 0.1494 - val_acc: 0.9562\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1291 - acc: 0.9609\n",
      "Epoch 00083: val_loss did not improve from 0.13901\n",
      "36805/36805 [==============================] - 32s 863us/sample - loss: 0.1291 - acc: 0.9609 - val_loss: 0.1641 - val_acc: 0.9515\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1278 - acc: 0.9611\n",
      "Epoch 00084: val_loss did not improve from 0.13901\n",
      "36805/36805 [==============================] - 32s 861us/sample - loss: 0.1277 - acc: 0.9611 - val_loss: 0.1531 - val_acc: 0.9548\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1248 - acc: 0.9624\n",
      "Epoch 00085: val_loss did not improve from 0.13901\n",
      "36805/36805 [==============================] - 32s 860us/sample - loss: 0.1250 - acc: 0.9623 - val_loss: 0.1896 - val_acc: 0.9474\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1294 - acc: 0.9611\n",
      "Epoch 00086: val_loss did not improve from 0.13901\n",
      "36805/36805 [==============================] - 32s 860us/sample - loss: 0.1295 - acc: 0.9611 - val_loss: 0.1861 - val_acc: 0.9488\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1285 - acc: 0.9614\n",
      "Epoch 00087: val_loss did not improve from 0.13901\n",
      "36805/36805 [==============================] - 32s 863us/sample - loss: 0.1288 - acc: 0.9614 - val_loss: 0.2543 - val_acc: 0.9271\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1281 - acc: 0.9609\n",
      "Epoch 00088: val_loss did not improve from 0.13901\n",
      "36805/36805 [==============================] - 32s 862us/sample - loss: 0.1285 - acc: 0.9608 - val_loss: 0.1880 - val_acc: 0.9434\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1248 - acc: 0.9623\n",
      "Epoch 00089: val_loss did not improve from 0.13901\n",
      "36805/36805 [==============================] - 32s 864us/sample - loss: 0.1248 - acc: 0.9623 - val_loss: 0.1737 - val_acc: 0.9490\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1200 - acc: 0.9632\n",
      "Epoch 00090: val_loss did not improve from 0.13901\n",
      "36805/36805 [==============================] - 32s 865us/sample - loss: 0.1201 - acc: 0.9631 - val_loss: 0.1699 - val_acc: 0.9513\n",
      "Epoch 91/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1162 - acc: 0.9649\n",
      "Epoch 00091: val_loss did not improve from 0.13901\n",
      "36805/36805 [==============================] - 32s 863us/sample - loss: 0.1161 - acc: 0.9649 - val_loss: 0.1415 - val_acc: 0.9567\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1185 - acc: 0.9631\n",
      "Epoch 00092: val_loss did not improve from 0.13901\n",
      "36805/36805 [==============================] - 32s 862us/sample - loss: 0.1187 - acc: 0.9630 - val_loss: 0.1572 - val_acc: 0.9539\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1186 - acc: 0.9641\n",
      "Epoch 00093: val_loss did not improve from 0.13901\n",
      "36805/36805 [==============================] - 32s 861us/sample - loss: 0.1186 - acc: 0.9641 - val_loss: 0.1717 - val_acc: 0.9513\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1167 - acc: 0.9640\n",
      "Epoch 00094: val_loss did not improve from 0.13901\n",
      "36805/36805 [==============================] - 32s 863us/sample - loss: 0.1167 - acc: 0.9640 - val_loss: 0.1698 - val_acc: 0.9504\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1148 - acc: 0.9649\n",
      "Epoch 00095: val_loss did not improve from 0.13901\n",
      "36805/36805 [==============================] - 32s 862us/sample - loss: 0.1148 - acc: 0.9650 - val_loss: 0.1618 - val_acc: 0.9562\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1160 - acc: 0.9646\n",
      "Epoch 00096: val_loss did not improve from 0.13901\n",
      "36805/36805 [==============================] - 32s 863us/sample - loss: 0.1160 - acc: 0.9647 - val_loss: 0.1451 - val_acc: 0.9560\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1120 - acc: 0.9657\n",
      "Epoch 00097: val_loss did not improve from 0.13901\n",
      "36805/36805 [==============================] - 32s 861us/sample - loss: 0.1121 - acc: 0.9657 - val_loss: 0.1463 - val_acc: 0.9567\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1142 - acc: 0.9643\n",
      "Epoch 00098: val_loss did not improve from 0.13901\n",
      "36805/36805 [==============================] - 32s 861us/sample - loss: 0.1142 - acc: 0.9643 - val_loss: 0.1491 - val_acc: 0.9548\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1114 - acc: 0.9665\n",
      "Epoch 00099: val_loss did not improve from 0.13901\n",
      "36805/36805 [==============================] - 32s 863us/sample - loss: 0.1114 - acc: 0.9665 - val_loss: 0.1713 - val_acc: 0.9495\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1098 - acc: 0.9668\n",
      "Epoch 00100: val_loss did not improve from 0.13901\n",
      "36805/36805 [==============================] - 32s 865us/sample - loss: 0.1098 - acc: 0.9668 - val_loss: 0.1536 - val_acc: 0.9504\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1082 - acc: 0.9667\n",
      "Epoch 00101: val_loss did not improve from 0.13901\n",
      "36805/36805 [==============================] - 32s 860us/sample - loss: 0.1082 - acc: 0.9667 - val_loss: 0.1392 - val_acc: 0.9581\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1098 - acc: 0.9666\n",
      "Epoch 00102: val_loss did not improve from 0.13901\n",
      "36805/36805 [==============================] - 32s 859us/sample - loss: 0.1098 - acc: 0.9666 - val_loss: 0.1672 - val_acc: 0.9539\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1065 - acc: 0.9675\n",
      "Epoch 00103: val_loss did not improve from 0.13901\n",
      "36805/36805 [==============================] - 32s 863us/sample - loss: 0.1066 - acc: 0.9675 - val_loss: 0.1765 - val_acc: 0.9522\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1054 - acc: 0.9680\n",
      "Epoch 00104: val_loss did not improve from 0.13901\n",
      "36805/36805 [==============================] - 32s 861us/sample - loss: 0.1055 - acc: 0.9679 - val_loss: 0.1501 - val_acc: 0.9567\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1059 - acc: 0.9679\n",
      "Epoch 00105: val_loss did not improve from 0.13901\n",
      "36805/36805 [==============================] - 32s 861us/sample - loss: 0.1059 - acc: 0.9679 - val_loss: 0.1582 - val_acc: 0.9576\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1054 - acc: 0.9678\n",
      "Epoch 00106: val_loss did not improve from 0.13901\n",
      "36805/36805 [==============================] - 32s 860us/sample - loss: 0.1054 - acc: 0.9678 - val_loss: 0.1816 - val_acc: 0.9495\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1027 - acc: 0.9690\n",
      "Epoch 00107: val_loss did not improve from 0.13901\n",
      "36805/36805 [==============================] - 32s 863us/sample - loss: 0.1027 - acc: 0.9690 - val_loss: 0.1494 - val_acc: 0.9560\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1035 - acc: 0.9690\n",
      "Epoch 00108: val_loss did not improve from 0.13901\n",
      "36805/36805 [==============================] - 32s 860us/sample - loss: 0.1035 - acc: 0.9689 - val_loss: 0.1649 - val_acc: 0.9548\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0999 - acc: 0.9695\n",
      "Epoch 00109: val_loss did not improve from 0.13901\n",
      "36805/36805 [==============================] - 32s 862us/sample - loss: 0.0999 - acc: 0.9695 - val_loss: 0.1582 - val_acc: 0.9509\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0980 - acc: 0.9698\n",
      "Epoch 00110: val_loss did not improve from 0.13901\n",
      "36805/36805 [==============================] - 32s 867us/sample - loss: 0.0980 - acc: 0.9698 - val_loss: 0.1499 - val_acc: 0.9576\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0989 - acc: 0.9695\n",
      "Epoch 00111: val_loss did not improve from 0.13901\n",
      "36805/36805 [==============================] - 32s 866us/sample - loss: 0.0989 - acc: 0.9695 - val_loss: 0.1492 - val_acc: 0.9539\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0994 - acc: 0.9695\n",
      "Epoch 00112: val_loss did not improve from 0.13901\n",
      "36805/36805 [==============================] - 32s 864us/sample - loss: 0.0995 - acc: 0.9695 - val_loss: 0.1787 - val_acc: 0.9504\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0958 - acc: 0.9705\n",
      "Epoch 00113: val_loss did not improve from 0.13901\n",
      "36805/36805 [==============================] - 32s 861us/sample - loss: 0.0959 - acc: 0.9705 - val_loss: 0.1586 - val_acc: 0.9557\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0934 - acc: 0.9702\n",
      "Epoch 00114: val_loss did not improve from 0.13901\n",
      "36805/36805 [==============================] - 32s 866us/sample - loss: 0.0934 - acc: 0.9702 - val_loss: 0.1605 - val_acc: 0.9509\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0965 - acc: 0.9704\n",
      "Epoch 00115: val_loss did not improve from 0.13901\n",
      "36805/36805 [==============================] - 32s 862us/sample - loss: 0.0965 - acc: 0.9704 - val_loss: 0.1502 - val_acc: 0.9550\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0939 - acc: 0.9709\n",
      "Epoch 00116: val_loss did not improve from 0.13901\n",
      "36805/36805 [==============================] - 32s 864us/sample - loss: 0.0940 - acc: 0.9709 - val_loss: 0.1665 - val_acc: 0.9536\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0933 - acc: 0.9717\n",
      "Epoch 00117: val_loss did not improve from 0.13901\n",
      "36805/36805 [==============================] - 32s 863us/sample - loss: 0.0933 - acc: 0.9717 - val_loss: 0.1539 - val_acc: 0.9583\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0924 - acc: 0.9722\n",
      "Epoch 00118: val_loss did not improve from 0.13901\n",
      "36805/36805 [==============================] - 32s 861us/sample - loss: 0.0924 - acc: 0.9722 - val_loss: 0.1582 - val_acc: 0.9522\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0908 - acc: 0.9726\n",
      "Epoch 00119: val_loss did not improve from 0.13901\n",
      "36805/36805 [==============================] - 32s 860us/sample - loss: 0.0907 - acc: 0.9726 - val_loss: 0.1627 - val_acc: 0.9550\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0913 - acc: 0.9720\n",
      "Epoch 00120: val_loss did not improve from 0.13901\n",
      "36805/36805 [==============================] - 32s 861us/sample - loss: 0.0913 - acc: 0.9720 - val_loss: 0.1535 - val_acc: 0.9557\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0891 - acc: 0.9731\n",
      "Epoch 00121: val_loss did not improve from 0.13901\n",
      "36805/36805 [==============================] - 32s 864us/sample - loss: 0.0891 - acc: 0.9731 - val_loss: 0.1484 - val_acc: 0.9611\n",
      "\n",
      "1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_7_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 436us/sample - loss: 0.1892 - acc: 0.9452\n",
      "Loss: 0.18921934992446335 Accuracy: 0.94517136\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.3531 - acc: 0.2720\n",
      "Epoch 00001: val_loss improved from inf to 1.95835, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_8_conv_checkpoint/001-1.9583.hdf5\n",
      "36805/36805 [==============================] - 46s 1ms/sample - loss: 2.3532 - acc: 0.2720 - val_loss: 1.9583 - val_acc: 0.3809\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4407 - acc: 0.5468\n",
      "Epoch 00002: val_loss improved from 1.95835 to 0.88127, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_8_conv_checkpoint/002-0.8813.hdf5\n",
      "36805/36805 [==============================] - 34s 921us/sample - loss: 1.4408 - acc: 0.5468 - val_loss: 0.8813 - val_acc: 0.7549\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0278 - acc: 0.6885\n",
      "Epoch 00003: val_loss improved from 0.88127 to 0.63358, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_8_conv_checkpoint/003-0.6336.hdf5\n",
      "36805/36805 [==============================] - 33s 907us/sample - loss: 1.0280 - acc: 0.6885 - val_loss: 0.6336 - val_acc: 0.8311\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8079 - acc: 0.7609\n",
      "Epoch 00004: val_loss improved from 0.63358 to 0.49124, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_8_conv_checkpoint/004-0.4912.hdf5\n",
      "36805/36805 [==============================] - 33s 903us/sample - loss: 0.8080 - acc: 0.7609 - val_loss: 0.4912 - val_acc: 0.8689\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6626 - acc: 0.8062\n",
      "Epoch 00005: val_loss improved from 0.49124 to 0.38237, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_8_conv_checkpoint/005-0.3824.hdf5\n",
      "36805/36805 [==============================] - 33s 908us/sample - loss: 0.6626 - acc: 0.8062 - val_loss: 0.3824 - val_acc: 0.8989\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5709 - acc: 0.8339\n",
      "Epoch 00006: val_loss improved from 0.38237 to 0.34051, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_8_conv_checkpoint/006-0.3405.hdf5\n",
      "36805/36805 [==============================] - 33s 906us/sample - loss: 0.5711 - acc: 0.8339 - val_loss: 0.3405 - val_acc: 0.9126\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5138 - acc: 0.8512\n",
      "Epoch 00007: val_loss improved from 0.34051 to 0.30792, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_8_conv_checkpoint/007-0.3079.hdf5\n",
      "36805/36805 [==============================] - 33s 906us/sample - loss: 0.5139 - acc: 0.8512 - val_loss: 0.3079 - val_acc: 0.9182\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4587 - acc: 0.8674\n",
      "Epoch 00008: val_loss improved from 0.30792 to 0.29931, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_8_conv_checkpoint/008-0.2993.hdf5\n",
      "36805/36805 [==============================] - 34s 911us/sample - loss: 0.4587 - acc: 0.8675 - val_loss: 0.2993 - val_acc: 0.9119\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4214 - acc: 0.8796\n",
      "Epoch 00009: val_loss improved from 0.29931 to 0.26481, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_8_conv_checkpoint/009-0.2648.hdf5\n",
      "36805/36805 [==============================] - 33s 909us/sample - loss: 0.4215 - acc: 0.8796 - val_loss: 0.2648 - val_acc: 0.9290\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3880 - acc: 0.8889\n",
      "Epoch 00010: val_loss improved from 0.26481 to 0.23448, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_8_conv_checkpoint/010-0.2345.hdf5\n",
      "36805/36805 [==============================] - 33s 907us/sample - loss: 0.3882 - acc: 0.8888 - val_loss: 0.2345 - val_acc: 0.9357\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3634 - acc: 0.8942\n",
      "Epoch 00011: val_loss did not improve from 0.23448\n",
      "36805/36805 [==============================] - 33s 905us/sample - loss: 0.3635 - acc: 0.8942 - val_loss: 0.2385 - val_acc: 0.9343\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3388 - acc: 0.9020\n",
      "Epoch 00012: val_loss did not improve from 0.23448\n",
      "36805/36805 [==============================] - 33s 907us/sample - loss: 0.3388 - acc: 0.9019 - val_loss: 0.2388 - val_acc: 0.9285\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3258 - acc: 0.9052\n",
      "Epoch 00013: val_loss improved from 0.23448 to 0.20449, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_8_conv_checkpoint/013-0.2045.hdf5\n",
      "36805/36805 [==============================] - 33s 906us/sample - loss: 0.3258 - acc: 0.9052 - val_loss: 0.2045 - val_acc: 0.9443\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3019 - acc: 0.9118\n",
      "Epoch 00014: val_loss improved from 0.20449 to 0.19717, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_8_conv_checkpoint/014-0.1972.hdf5\n",
      "36805/36805 [==============================] - 34s 911us/sample - loss: 0.3019 - acc: 0.9117 - val_loss: 0.1972 - val_acc: 0.9453\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2869 - acc: 0.9172\n",
      "Epoch 00015: val_loss improved from 0.19717 to 0.19276, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_8_conv_checkpoint/015-0.1928.hdf5\n",
      "36805/36805 [==============================] - 33s 904us/sample - loss: 0.2869 - acc: 0.9172 - val_loss: 0.1928 - val_acc: 0.9464\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2714 - acc: 0.9219\n",
      "Epoch 00016: val_loss improved from 0.19276 to 0.18990, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_8_conv_checkpoint/016-0.1899.hdf5\n",
      "36805/36805 [==============================] - 33s 905us/sample - loss: 0.2715 - acc: 0.9218 - val_loss: 0.1899 - val_acc: 0.9455\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2606 - acc: 0.9232\n",
      "Epoch 00017: val_loss did not improve from 0.18990\n",
      "36805/36805 [==============================] - 33s 899us/sample - loss: 0.2606 - acc: 0.9231 - val_loss: 0.1912 - val_acc: 0.9441\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2504 - acc: 0.9257\n",
      "Epoch 00018: val_loss improved from 0.18990 to 0.18543, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_8_conv_checkpoint/018-0.1854.hdf5\n",
      "36805/36805 [==============================] - 33s 901us/sample - loss: 0.2505 - acc: 0.9257 - val_loss: 0.1854 - val_acc: 0.9492\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2391 - acc: 0.9294\n",
      "Epoch 00019: val_loss did not improve from 0.18543\n",
      "36805/36805 [==============================] - 33s 903us/sample - loss: 0.2392 - acc: 0.9294 - val_loss: 0.2086 - val_acc: 0.9394\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2363 - acc: 0.9307\n",
      "Epoch 00020: val_loss improved from 0.18543 to 0.16750, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_8_conv_checkpoint/020-0.1675.hdf5\n",
      "36805/36805 [==============================] - 33s 906us/sample - loss: 0.2364 - acc: 0.9306 - val_loss: 0.1675 - val_acc: 0.9529\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2242 - acc: 0.9338\n",
      "Epoch 00021: val_loss improved from 0.16750 to 0.16610, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_8_conv_checkpoint/021-0.1661.hdf5\n",
      "36805/36805 [==============================] - 33s 904us/sample - loss: 0.2244 - acc: 0.9337 - val_loss: 0.1661 - val_acc: 0.9502\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2156 - acc: 0.9362\n",
      "Epoch 00022: val_loss improved from 0.16610 to 0.15502, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_8_conv_checkpoint/022-0.1550.hdf5\n",
      "36805/36805 [==============================] - 33s 904us/sample - loss: 0.2156 - acc: 0.9362 - val_loss: 0.1550 - val_acc: 0.9548\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2048 - acc: 0.9401\n",
      "Epoch 00023: val_loss did not improve from 0.15502\n",
      "36805/36805 [==============================] - 33s 903us/sample - loss: 0.2048 - acc: 0.9401 - val_loss: 0.1557 - val_acc: 0.9555\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1988 - acc: 0.9403\n",
      "Epoch 00024: val_loss did not improve from 0.15502\n",
      "36805/36805 [==============================] - 33s 901us/sample - loss: 0.1988 - acc: 0.9403 - val_loss: 0.1658 - val_acc: 0.9525\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1955 - acc: 0.9425\n",
      "Epoch 00025: val_loss did not improve from 0.15502\n",
      "36805/36805 [==============================] - 33s 904us/sample - loss: 0.1955 - acc: 0.9425 - val_loss: 0.1781 - val_acc: 0.9497\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1845 - acc: 0.9457\n",
      "Epoch 00026: val_loss did not improve from 0.15502\n",
      "36805/36805 [==============================] - 33s 900us/sample - loss: 0.1847 - acc: 0.9456 - val_loss: 0.1681 - val_acc: 0.9527\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1909 - acc: 0.9416\n",
      "Epoch 00027: val_loss improved from 0.15502 to 0.15474, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_8_conv_checkpoint/027-0.1547.hdf5\n",
      "36805/36805 [==============================] - 33s 909us/sample - loss: 0.1909 - acc: 0.9416 - val_loss: 0.1547 - val_acc: 0.9562\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1738 - acc: 0.9490\n",
      "Epoch 00028: val_loss improved from 0.15474 to 0.14983, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_8_conv_checkpoint/028-0.1498.hdf5\n",
      "36805/36805 [==============================] - 33s 908us/sample - loss: 0.1740 - acc: 0.9490 - val_loss: 0.1498 - val_acc: 0.9562\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1739 - acc: 0.9474\n",
      "Epoch 00029: val_loss improved from 0.14983 to 0.14175, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_8_conv_checkpoint/029-0.1418.hdf5\n",
      "36805/36805 [==============================] - 33s 905us/sample - loss: 0.1740 - acc: 0.9473 - val_loss: 0.1418 - val_acc: 0.9616\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1644 - acc: 0.9510\n",
      "Epoch 00030: val_loss improved from 0.14175 to 0.13477, saving model to model/checkpoint/1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_8_conv_checkpoint/030-0.1348.hdf5\n",
      "36805/36805 [==============================] - 33s 903us/sample - loss: 0.1646 - acc: 0.9509 - val_loss: 0.1348 - val_acc: 0.9592\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1649 - acc: 0.9516\n",
      "Epoch 00031: val_loss did not improve from 0.13477\n",
      "36805/36805 [==============================] - 33s 905us/sample - loss: 0.1649 - acc: 0.9516 - val_loss: 0.1488 - val_acc: 0.9574\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1552 - acc: 0.9541\n",
      "Epoch 00032: val_loss did not improve from 0.13477\n",
      "36805/36805 [==============================] - 33s 905us/sample - loss: 0.1552 - acc: 0.9541 - val_loss: 0.1569 - val_acc: 0.9539\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1525 - acc: 0.9542\n",
      "Epoch 00033: val_loss did not improve from 0.13477\n",
      "36805/36805 [==============================] - 33s 902us/sample - loss: 0.1525 - acc: 0.9542 - val_loss: 0.1557 - val_acc: 0.9550\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1464 - acc: 0.9555\n",
      "Epoch 00034: val_loss did not improve from 0.13477\n",
      "36805/36805 [==============================] - 33s 904us/sample - loss: 0.1463 - acc: 0.9555 - val_loss: 0.1498 - val_acc: 0.9571\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1416 - acc: 0.9566\n",
      "Epoch 00035: val_loss did not improve from 0.13477\n",
      "36805/36805 [==============================] - 33s 903us/sample - loss: 0.1417 - acc: 0.9566 - val_loss: 0.1408 - val_acc: 0.9602\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1413 - acc: 0.9573\n",
      "Epoch 00036: val_loss did not improve from 0.13477\n",
      "36805/36805 [==============================] - 33s 904us/sample - loss: 0.1413 - acc: 0.9573 - val_loss: 0.1615 - val_acc: 0.9536\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1391 - acc: 0.9574\n",
      "Epoch 00037: val_loss did not improve from 0.13477\n",
      "36805/36805 [==============================] - 33s 902us/sample - loss: 0.1394 - acc: 0.9574 - val_loss: 0.1415 - val_acc: 0.9576\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1418 - acc: 0.9565\n",
      "Epoch 00038: val_loss did not improve from 0.13477\n",
      "36805/36805 [==============================] - 33s 896us/sample - loss: 0.1418 - acc: 0.9565 - val_loss: 0.1377 - val_acc: 0.9611\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1305 - acc: 0.9603\n",
      "Epoch 00039: val_loss did not improve from 0.13477\n",
      "36805/36805 [==============================] - 33s 904us/sample - loss: 0.1308 - acc: 0.9602 - val_loss: 0.1595 - val_acc: 0.9534\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1295 - acc: 0.9611\n",
      "Epoch 00040: val_loss did not improve from 0.13477\n",
      "36805/36805 [==============================] - 33s 899us/sample - loss: 0.1295 - acc: 0.9611 - val_loss: 0.1497 - val_acc: 0.9532\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1208 - acc: 0.9638\n",
      "Epoch 00041: val_loss did not improve from 0.13477\n",
      "36805/36805 [==============================] - 33s 898us/sample - loss: 0.1208 - acc: 0.9638 - val_loss: 0.1405 - val_acc: 0.9597\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1197 - acc: 0.9640\n",
      "Epoch 00042: val_loss did not improve from 0.13477\n",
      "36805/36805 [==============================] - 33s 900us/sample - loss: 0.1197 - acc: 0.9640 - val_loss: 0.1480 - val_acc: 0.9564\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1142 - acc: 0.9660\n",
      "Epoch 00043: val_loss did not improve from 0.13477\n",
      "36805/36805 [==============================] - 33s 903us/sample - loss: 0.1142 - acc: 0.9660 - val_loss: 0.1397 - val_acc: 0.9588\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1150 - acc: 0.9645\n",
      "Epoch 00044: val_loss did not improve from 0.13477\n",
      "36805/36805 [==============================] - 33s 900us/sample - loss: 0.1150 - acc: 0.9645 - val_loss: 0.1698 - val_acc: 0.9513\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1132 - acc: 0.9654\n",
      "Epoch 00045: val_loss did not improve from 0.13477\n",
      "36805/36805 [==============================] - 33s 903us/sample - loss: 0.1132 - acc: 0.9654 - val_loss: 0.1503 - val_acc: 0.9595\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1061 - acc: 0.9679\n",
      "Epoch 00046: val_loss did not improve from 0.13477\n",
      "36805/36805 [==============================] - 33s 899us/sample - loss: 0.1067 - acc: 0.9678 - val_loss: 0.1473 - val_acc: 0.9557\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1172 - acc: 0.9640\n",
      "Epoch 00047: val_loss did not improve from 0.13477\n",
      "36805/36805 [==============================] - 33s 896us/sample - loss: 0.1172 - acc: 0.9639 - val_loss: 0.1412 - val_acc: 0.9611\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1066 - acc: 0.9669\n",
      "Epoch 00048: val_loss did not improve from 0.13477\n",
      "36805/36805 [==============================] - 33s 901us/sample - loss: 0.1066 - acc: 0.9669 - val_loss: 0.1445 - val_acc: 0.9585\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0973 - acc: 0.9704\n",
      "Epoch 00049: val_loss did not improve from 0.13477\n",
      "36805/36805 [==============================] - 33s 903us/sample - loss: 0.0973 - acc: 0.9704 - val_loss: 0.1488 - val_acc: 0.9599\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0987 - acc: 0.9700\n",
      "Epoch 00050: val_loss did not improve from 0.13477\n",
      "36805/36805 [==============================] - 33s 904us/sample - loss: 0.0987 - acc: 0.9700 - val_loss: 0.1521 - val_acc: 0.9536\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0992 - acc: 0.9690\n",
      "Epoch 00051: val_loss did not improve from 0.13477\n",
      "36805/36805 [==============================] - 33s 899us/sample - loss: 0.0994 - acc: 0.9689 - val_loss: 0.1392 - val_acc: 0.9606\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0992 - acc: 0.9692\n",
      "Epoch 00052: val_loss did not improve from 0.13477\n",
      "36805/36805 [==============================] - 33s 904us/sample - loss: 0.0993 - acc: 0.9692 - val_loss: 0.1580 - val_acc: 0.9529\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0973 - acc: 0.9708\n",
      "Epoch 00053: val_loss did not improve from 0.13477\n",
      "36805/36805 [==============================] - 33s 902us/sample - loss: 0.0974 - acc: 0.9708 - val_loss: 0.1454 - val_acc: 0.9548\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0956 - acc: 0.9697\n",
      "Epoch 00054: val_loss did not improve from 0.13477\n",
      "36805/36805 [==============================] - 33s 905us/sample - loss: 0.0956 - acc: 0.9697 - val_loss: 0.1591 - val_acc: 0.9541\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0934 - acc: 0.9715\n",
      "Epoch 00055: val_loss did not improve from 0.13477\n",
      "36805/36805 [==============================] - 33s 903us/sample - loss: 0.0936 - acc: 0.9715 - val_loss: 0.1594 - val_acc: 0.9571\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0885 - acc: 0.9740\n",
      "Epoch 00056: val_loss did not improve from 0.13477\n",
      "36805/36805 [==============================] - 33s 900us/sample - loss: 0.0885 - acc: 0.9740 - val_loss: 0.1491 - val_acc: 0.9550\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0850 - acc: 0.9733\n",
      "Epoch 00057: val_loss did not improve from 0.13477\n",
      "36805/36805 [==============================] - 33s 903us/sample - loss: 0.0850 - acc: 0.9733 - val_loss: 0.1412 - val_acc: 0.9604\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0831 - acc: 0.9750\n",
      "Epoch 00058: val_loss did not improve from 0.13477\n",
      "36805/36805 [==============================] - 33s 906us/sample - loss: 0.0832 - acc: 0.9750 - val_loss: 0.1601 - val_acc: 0.9541\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0902 - acc: 0.9718\n",
      "Epoch 00059: val_loss did not improve from 0.13477\n",
      "36805/36805 [==============================] - 33s 900us/sample - loss: 0.0903 - acc: 0.9718 - val_loss: 0.1434 - val_acc: 0.9602\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0820 - acc: 0.9754\n",
      "Epoch 00060: val_loss did not improve from 0.13477\n",
      "36805/36805 [==============================] - 33s 899us/sample - loss: 0.0821 - acc: 0.9753 - val_loss: 0.1546 - val_acc: 0.9571\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0864 - acc: 0.9735\n",
      "Epoch 00061: val_loss did not improve from 0.13477\n",
      "36805/36805 [==============================] - 33s 900us/sample - loss: 0.0864 - acc: 0.9735 - val_loss: 0.1476 - val_acc: 0.9609\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0768 - acc: 0.9768\n",
      "Epoch 00062: val_loss did not improve from 0.13477\n",
      "36805/36805 [==============================] - 33s 897us/sample - loss: 0.0768 - acc: 0.9768 - val_loss: 0.1498 - val_acc: 0.9567\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0801 - acc: 0.9759\n",
      "Epoch 00063: val_loss did not improve from 0.13477\n",
      "36805/36805 [==============================] - 33s 900us/sample - loss: 0.0801 - acc: 0.9759 - val_loss: 0.1563 - val_acc: 0.9550\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0705 - acc: 0.9791\n",
      "Epoch 00064: val_loss did not improve from 0.13477\n",
      "36805/36805 [==============================] - 33s 905us/sample - loss: 0.0705 - acc: 0.9791 - val_loss: 0.1561 - val_acc: 0.9567\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0745 - acc: 0.9776\n",
      "Epoch 00065: val_loss did not improve from 0.13477\n",
      "36805/36805 [==============================] - 33s 899us/sample - loss: 0.0747 - acc: 0.9776 - val_loss: 0.1436 - val_acc: 0.9604\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0748 - acc: 0.9772\n",
      "Epoch 00066: val_loss did not improve from 0.13477\n",
      "36805/36805 [==============================] - 33s 908us/sample - loss: 0.0750 - acc: 0.9771 - val_loss: 0.1629 - val_acc: 0.9532\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0728 - acc: 0.9778\n",
      "Epoch 00067: val_loss did not improve from 0.13477\n",
      "36805/36805 [==============================] - 33s 908us/sample - loss: 0.0728 - acc: 0.9778 - val_loss: 0.1388 - val_acc: 0.9609\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0666 - acc: 0.9795\n",
      "Epoch 00068: val_loss did not improve from 0.13477\n",
      "36805/36805 [==============================] - 33s 904us/sample - loss: 0.0666 - acc: 0.9795 - val_loss: 0.1539 - val_acc: 0.9595\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0665 - acc: 0.9794\n",
      "Epoch 00069: val_loss did not improve from 0.13477\n",
      "36805/36805 [==============================] - 33s 904us/sample - loss: 0.0665 - acc: 0.9794 - val_loss: 0.1556 - val_acc: 0.9543\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0680 - acc: 0.9793\n",
      "Epoch 00070: val_loss did not improve from 0.13477\n",
      "36805/36805 [==============================] - 33s 899us/sample - loss: 0.0681 - acc: 0.9793 - val_loss: 0.1526 - val_acc: 0.9583\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0686 - acc: 0.9785\n",
      "Epoch 00071: val_loss did not improve from 0.13477\n",
      "36805/36805 [==============================] - 33s 903us/sample - loss: 0.0686 - acc: 0.9785 - val_loss: 0.1480 - val_acc: 0.9599\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0645 - acc: 0.9799\n",
      "Epoch 00072: val_loss did not improve from 0.13477\n",
      "36805/36805 [==============================] - 33s 904us/sample - loss: 0.0645 - acc: 0.9799 - val_loss: 0.1512 - val_acc: 0.9609\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0629 - acc: 0.9817\n",
      "Epoch 00073: val_loss did not improve from 0.13477\n",
      "36805/36805 [==============================] - 33s 902us/sample - loss: 0.0630 - acc: 0.9817 - val_loss: 0.1842 - val_acc: 0.9562\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0665 - acc: 0.9792\n",
      "Epoch 00074: val_loss did not improve from 0.13477\n",
      "36805/36805 [==============================] - 33s 899us/sample - loss: 0.0665 - acc: 0.9792 - val_loss: 0.1562 - val_acc: 0.9599\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0619 - acc: 0.9803\n",
      "Epoch 00075: val_loss did not improve from 0.13477\n",
      "36805/36805 [==============================] - 33s 907us/sample - loss: 0.0619 - acc: 0.9803 - val_loss: 0.1456 - val_acc: 0.9623\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0620 - acc: 0.9801\n",
      "Epoch 00076: val_loss did not improve from 0.13477\n",
      "36805/36805 [==============================] - 33s 903us/sample - loss: 0.0620 - acc: 0.9801 - val_loss: 0.1712 - val_acc: 0.9529\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0571 - acc: 0.9828\n",
      "Epoch 00077: val_loss did not improve from 0.13477\n",
      "36805/36805 [==============================] - 33s 899us/sample - loss: 0.0575 - acc: 0.9827 - val_loss: 0.1617 - val_acc: 0.9583\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0790 - acc: 0.9761\n",
      "Epoch 00078: val_loss did not improve from 0.13477\n",
      "36805/36805 [==============================] - 32s 880us/sample - loss: 0.0790 - acc: 0.9761 - val_loss: 0.1702 - val_acc: 0.9574\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0535 - acc: 0.9838\n",
      "Epoch 00079: val_loss did not improve from 0.13477\n",
      "36805/36805 [==============================] - 32s 882us/sample - loss: 0.0535 - acc: 0.9838 - val_loss: 0.1619 - val_acc: 0.9595\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0552 - acc: 0.9835\n",
      "Epoch 00080: val_loss did not improve from 0.13477\n",
      "36805/36805 [==============================] - 32s 878us/sample - loss: 0.0552 - acc: 0.9835 - val_loss: 0.1579 - val_acc: 0.9581\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_8_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 465us/sample - loss: 0.1823 - acc: 0.9433\n",
      "Loss: 0.1822829036976318 Accuracy: 0.94330215\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base = '1D_CNN_custom_multi_2_GAP_ch_32_DO_BN'\n",
    "\n",
    "for i in range(3, 9):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    model = build_cnn(conv_num=i)\n",
    "#         model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4),\n",
    "          metrics=['accuracy'])\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model_filename = model_path+'{epoch:03d}-{val_loss:.4f}.hdf5'\n",
    "    checkpointer = ModelCheckpoint(filepath = model_filename, monitor = \"val_loss\", \n",
    "                                   verbose=1, save_best_only=True)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=50)\n",
    "    hist = model.fit(x_train_abs, y_train_onehot, batch_size=64, epochs=500, \n",
    "                     validation_data=[x_val_abs, y_val_onehot], shuffle=True, \n",
    "                     callbacks = [checkpointer, early_stopping])\n",
    "\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "    ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "    ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "    ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    png_path = 'visualization/learning_curve/'\n",
    "    filename = model_name+'.png'\n",
    "    os.makedirs(png_path, exist_ok=True)\n",
    "    fig.savefig(png_path+filename, transparent=True)\n",
    "\n",
    "    model.save(model_path+'000_last.hdf5')\n",
    "    del(model)\n",
    "\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "    model = load_model(model_filename)\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "    print()\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_3_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_33 (Conv1D)              (None, 16000, 32)    192         input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_33 (Batc (None, 16000, 32)    128         conv1d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_33[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling1D) (None, 5333, 32)     0           activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_34 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_33[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_34 (Batc (None, 5333, 32)     128         conv1d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_34[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling1D) (None, 1777, 32)     0           activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_35 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_35 (Batc (None, 1777, 32)     128         conv1d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_35[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling1D) (None, 592, 32)      0           activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_12 (Gl (None, 32)           0           max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_13 (Gl (None, 32)           0           max_pooling1d_35[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 64)           0           global_average_pooling1d_12[0][0]\n",
      "                                                                 global_average_pooling1d_13[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 64)           0           concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 16)           1040        dropout_6[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 11,920\n",
      "Trainable params: 11,728\n",
      "Non-trainable params: 192\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 2s 436us/sample - loss: 0.9860 - acc: 0.7092\n",
      "Loss: 0.98602715012688 Accuracy: 0.7092419\n",
      "\n",
      "1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_4_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_8 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_36 (Conv1D)              (None, 16000, 32)    192         input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_36 (Batc (None, 16000, 32)    128         conv1d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_36[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling1D) (None, 5333, 32)     0           activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_37 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_36[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_37 (Batc (None, 5333, 32)     128         conv1d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_37[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling1D) (None, 1777, 32)     0           activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_38 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_37[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_38 (Batc (None, 1777, 32)     128         conv1d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_38[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling1D) (None, 592, 32)      0           activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_39 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_39 (Batc (None, 592, 32)      128         conv1d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 592, 32)      0           batch_normalization_v1_39[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling1D) (None, 197, 32)      0           activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_14 (Gl (None, 32)           0           max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_15 (Gl (None, 32)           0           max_pooling1d_39[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 64)           0           global_average_pooling1d_14[0][0]\n",
      "                                                                 global_average_pooling1d_15[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 64)           0           concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 16)           1040        dropout_7[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 17,200\n",
      "Trainable params: 16,944\n",
      "Non-trainable params: 256\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 458us/sample - loss: 0.6497 - acc: 0.8064\n",
      "Loss: 0.649664846313334 Accuracy: 0.8064382\n",
      "\n",
      "1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_5_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_40 (Conv1D)              (None, 16000, 32)    192         input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_40 (Batc (None, 16000, 32)    128         conv1d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_40[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling1D) (None, 5333, 32)     0           activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_41 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_40[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_41 (Batc (None, 5333, 32)     128         conv1d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_41[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling1D) (None, 1777, 32)     0           activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_42 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_41[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_42 (Batc (None, 1777, 32)     128         conv1d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_42[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling1D) (None, 592, 32)      0           activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_43 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_42[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_43 (Batc (None, 592, 32)      128         conv1d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 592, 32)      0           batch_normalization_v1_43[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling1D) (None, 197, 32)      0           activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_44 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_44 (Batc (None, 197, 64)      256         conv1d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 197, 64)      0           batch_normalization_v1_44[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling1D) (None, 65, 64)       0           activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_16 (Gl (None, 32)           0           max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_17 (Gl (None, 64)           0           max_pooling1d_44[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 96)           0           global_average_pooling1d_16[0][0]\n",
      "                                                                 global_average_pooling1d_17[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 96)           0           concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 16)           1552        dropout_8[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 28,272\n",
      "Trainable params: 27,888\n",
      "Non-trainable params: 384\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 2s 475us/sample - loss: 0.3813 - acc: 0.8916\n",
      "Loss: 0.38126937850984827 Accuracy: 0.8915888\n",
      "\n",
      "1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_6_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_10 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_45 (Conv1D)              (None, 16000, 32)    192         input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_45 (Batc (None, 16000, 32)    128         conv1d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_45[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling1D) (None, 5333, 32)     0           activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_46 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_45[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_46 (Batc (None, 5333, 32)     128         conv1d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_46[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling1D) (None, 1777, 32)     0           activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_47 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_46[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_47 (Batc (None, 1777, 32)     128         conv1d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_47[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling1D) (None, 592, 32)      0           activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_48 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_47[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_48 (Batc (None, 592, 32)      128         conv1d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 592, 32)      0           batch_normalization_v1_48[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling1D) (None, 197, 32)      0           activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_49 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_48[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_49 (Batc (None, 197, 64)      256         conv1d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 197, 64)      0           batch_normalization_v1_49[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling1D) (None, 65, 64)       0           activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_50 (Conv1D)              (None, 65, 64)       20544       max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_50 (Batc (None, 65, 64)       256         conv1d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 65, 64)       0           batch_normalization_v1_50[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling1D) (None, 21, 64)       0           activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_18 (Gl (None, 64)           0           max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_19 (Gl (None, 64)           0           max_pooling1d_50[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 128)          0           global_average_pooling1d_18[0][0]\n",
      "                                                                 global_average_pooling1d_19[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 128)          0           concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 16)           2064        dropout_9[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 49,584\n",
      "Trainable params: 49,072\n",
      "Non-trainable params: 512\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 517us/sample - loss: 0.2341 - acc: 0.9302\n",
      "Loss: 0.2340986780040856 Accuracy: 0.93021804\n",
      "\n",
      "1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_7_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_51 (Conv1D)              (None, 16000, 32)    192         input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_51 (Batc (None, 16000, 32)    128         conv1d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_51[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling1D) (None, 5333, 32)     0           activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_52 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_51[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_52 (Batc (None, 5333, 32)     128         conv1d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_52[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling1D) (None, 1777, 32)     0           activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_53 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_52[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_53 (Batc (None, 1777, 32)     128         conv1d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_53[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling1D) (None, 592, 32)      0           activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_54 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_53[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_54 (Batc (None, 592, 32)      128         conv1d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 592, 32)      0           batch_normalization_v1_54[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling1D) (None, 197, 32)      0           activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_55 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_54[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_55 (Batc (None, 197, 64)      256         conv1d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 197, 64)      0           batch_normalization_v1_55[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling1D) (None, 65, 64)       0           activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_56 (Conv1D)              (None, 65, 64)       20544       max_pooling1d_55[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_56 (Batc (None, 65, 64)       256         conv1d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 65, 64)       0           batch_normalization_v1_56[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling1D) (None, 21, 64)       0           activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_57 (Conv1D)              (None, 21, 64)       20544       max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_57 (Batc (None, 21, 64)       256         conv1d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 21, 64)       0           batch_normalization_v1_57[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling1D) (None, 7, 64)        0           activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_20 (Gl (None, 64)           0           max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_21 (Gl (None, 64)           0           max_pooling1d_57[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 128)          0           global_average_pooling1d_20[0][0]\n",
      "                                                                 global_average_pooling1d_21[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 128)          0           concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 16)           2064        dropout_10[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 70,384\n",
      "Trainable params: 69,744\n",
      "Non-trainable params: 640\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 521us/sample - loss: 0.1892 - acc: 0.9452\n",
      "Loss: 0.18921934992446335 Accuracy: 0.94517136\n",
      "\n",
      "1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_8_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_12 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_58 (Conv1D)              (None, 16000, 32)    192         input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_58 (Batc (None, 16000, 32)    128         conv1d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_58[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling1D) (None, 5333, 32)     0           activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_59 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_58[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_59 (Batc (None, 5333, 32)     128         conv1d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_59[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling1D) (None, 1777, 32)     0           activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_60 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_59[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_60 (Batc (None, 1777, 32)     128         conv1d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_60[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling1D) (None, 592, 32)      0           activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_61 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_60[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_61 (Batc (None, 592, 32)      128         conv1d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 592, 32)      0           batch_normalization_v1_61[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling1D) (None, 197, 32)      0           activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_62 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_61[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_62 (Batc (None, 197, 64)      256         conv1d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 197, 64)      0           batch_normalization_v1_62[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling1D) (None, 65, 64)       0           activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_63 (Conv1D)              (None, 65, 64)       20544       max_pooling1d_62[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_63 (Batc (None, 65, 64)       256         conv1d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 65, 64)       0           batch_normalization_v1_63[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling1D) (None, 21, 64)       0           activation_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_64 (Conv1D)              (None, 21, 64)       20544       max_pooling1d_63[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_64 (Batc (None, 21, 64)       256         conv1d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 21, 64)       0           batch_normalization_v1_64[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling1D) (None, 7, 64)        0           activation_64[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_65 (Conv1D)              (None, 7, 64)        20544       max_pooling1d_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_65 (Batc (None, 7, 64)        256         conv1d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 7, 64)        0           batch_normalization_v1_65[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling1D) (None, 2, 64)        0           activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_22 (Gl (None, 64)           0           max_pooling1d_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_23 (Gl (None, 64)           0           max_pooling1d_65[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 128)          0           global_average_pooling1d_22[0][0]\n",
      "                                                                 global_average_pooling1d_23[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 128)          0           concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 16)           2064        dropout_11[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 91,184\n",
      "Trainable params: 90,416\n",
      "Non-trainable params: 768\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 536us/sample - loss: 0.1823 - acc: 0.9433\n",
      "Loss: 0.1822829036976318 Accuracy: 0.94330215\n"
     ]
    }
   ],
   "source": [
    "log_dir = 'log'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "base = '1D_CNN_custom_multi_2_GAP_ch_32_DO_BN'\n",
    "\n",
    "with open(path.join(log_dir, base), 'w') as log_file:\n",
    "    for i in range(3, 9):\n",
    "        model_name = base+'_{}_conv'.format(i)\n",
    "        print()\n",
    "        print(model_name, 'Model')\n",
    "        model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "        model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "\n",
    "        model = load_model(model_filename)\n",
    "        model.summary()\n",
    "\n",
    "        [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "        print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "        del(model)\n",
    "\n",
    "        log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_3_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_33 (Conv1D)              (None, 16000, 32)    192         input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_33 (Batc (None, 16000, 32)    128         conv1d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_33[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling1D) (None, 5333, 32)     0           activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_34 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_33[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_34 (Batc (None, 5333, 32)     128         conv1d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_34[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling1D) (None, 1777, 32)     0           activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_35 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_35 (Batc (None, 1777, 32)     128         conv1d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_35[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling1D) (None, 592, 32)      0           activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_12 (Gl (None, 32)           0           max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_13 (Gl (None, 32)           0           max_pooling1d_35[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 64)           0           global_average_pooling1d_12[0][0]\n",
      "                                                                 global_average_pooling1d_13[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 64)           0           concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 16)           1040        dropout_6[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 11,920\n",
      "Trainable params: 11,728\n",
      "Non-trainable params: 192\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 528us/sample - loss: 1.1276 - acc: 0.6455\n",
      "Loss: 1.1275911255417583 Accuracy: 0.64548284\n",
      "\n",
      "1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_4_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_8 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_36 (Conv1D)              (None, 16000, 32)    192         input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_36 (Batc (None, 16000, 32)    128         conv1d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_36[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling1D) (None, 5333, 32)     0           activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_37 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_36[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_37 (Batc (None, 5333, 32)     128         conv1d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_37[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling1D) (None, 1777, 32)     0           activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_38 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_37[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_38 (Batc (None, 1777, 32)     128         conv1d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_38[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling1D) (None, 592, 32)      0           activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_39 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_39 (Batc (None, 592, 32)      128         conv1d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 592, 32)      0           batch_normalization_v1_39[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling1D) (None, 197, 32)      0           activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_14 (Gl (None, 32)           0           max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_15 (Gl (None, 32)           0           max_pooling1d_39[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 64)           0           global_average_pooling1d_14[0][0]\n",
      "                                                                 global_average_pooling1d_15[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 64)           0           concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 16)           1040        dropout_7[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 17,200\n",
      "Trainable params: 16,944\n",
      "Non-trainable params: 256\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 544us/sample - loss: 0.6646 - acc: 0.8023\n",
      "Loss: 0.6645504071952646 Accuracy: 0.80228454\n",
      "\n",
      "1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_5_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_40 (Conv1D)              (None, 16000, 32)    192         input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_40 (Batc (None, 16000, 32)    128         conv1d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_40[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling1D) (None, 5333, 32)     0           activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_41 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_40[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_41 (Batc (None, 5333, 32)     128         conv1d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_41[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling1D) (None, 1777, 32)     0           activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_42 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_41[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_42 (Batc (None, 1777, 32)     128         conv1d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_42[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling1D) (None, 592, 32)      0           activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_43 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_42[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_43 (Batc (None, 592, 32)      128         conv1d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 592, 32)      0           batch_normalization_v1_43[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling1D) (None, 197, 32)      0           activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_44 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_44 (Batc (None, 197, 64)      256         conv1d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 197, 64)      0           batch_normalization_v1_44[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling1D) (None, 65, 64)       0           activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_16 (Gl (None, 32)           0           max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_17 (Gl (None, 64)           0           max_pooling1d_44[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 96)           0           global_average_pooling1d_16[0][0]\n",
      "                                                                 global_average_pooling1d_17[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 96)           0           concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 16)           1552        dropout_8[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 28,272\n",
      "Trainable params: 27,888\n",
      "Non-trainable params: 384\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 564us/sample - loss: 0.4027 - acc: 0.8872\n",
      "Loss: 0.4027311397366311 Accuracy: 0.8872274\n",
      "\n",
      "1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_6_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_10 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_45 (Conv1D)              (None, 16000, 32)    192         input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_45 (Batc (None, 16000, 32)    128         conv1d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_45[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling1D) (None, 5333, 32)     0           activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_46 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_45[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_46 (Batc (None, 5333, 32)     128         conv1d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_46[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling1D) (None, 1777, 32)     0           activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_47 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_46[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_47 (Batc (None, 1777, 32)     128         conv1d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_47[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling1D) (None, 592, 32)      0           activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_48 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_47[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_48 (Batc (None, 592, 32)      128         conv1d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 592, 32)      0           batch_normalization_v1_48[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling1D) (None, 197, 32)      0           activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_49 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_48[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_49 (Batc (None, 197, 64)      256         conv1d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 197, 64)      0           batch_normalization_v1_49[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling1D) (None, 65, 64)       0           activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_50 (Conv1D)              (None, 65, 64)       20544       max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_50 (Batc (None, 65, 64)       256         conv1d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 65, 64)       0           batch_normalization_v1_50[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling1D) (None, 21, 64)       0           activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_18 (Gl (None, 64)           0           max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_19 (Gl (None, 64)           0           max_pooling1d_50[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 128)          0           global_average_pooling1d_18[0][0]\n",
      "                                                                 global_average_pooling1d_19[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 128)          0           concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 16)           2064        dropout_9[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 49,584\n",
      "Trainable params: 49,072\n",
      "Non-trainable params: 512\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 597us/sample - loss: 0.2667 - acc: 0.9211\n",
      "Loss: 0.2666606889397432 Accuracy: 0.92107993\n",
      "\n",
      "1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_7_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_51 (Conv1D)              (None, 16000, 32)    192         input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_51 (Batc (None, 16000, 32)    128         conv1d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_51[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling1D) (None, 5333, 32)     0           activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_52 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_51[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_52 (Batc (None, 5333, 32)     128         conv1d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_52[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling1D) (None, 1777, 32)     0           activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_53 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_52[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_53 (Batc (None, 1777, 32)     128         conv1d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_53[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling1D) (None, 592, 32)      0           activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_54 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_53[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_54 (Batc (None, 592, 32)      128         conv1d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 592, 32)      0           batch_normalization_v1_54[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling1D) (None, 197, 32)      0           activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_55 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_54[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_55 (Batc (None, 197, 64)      256         conv1d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 197, 64)      0           batch_normalization_v1_55[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling1D) (None, 65, 64)       0           activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_56 (Conv1D)              (None, 65, 64)       20544       max_pooling1d_55[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_56 (Batc (None, 65, 64)       256         conv1d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 65, 64)       0           batch_normalization_v1_56[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling1D) (None, 21, 64)       0           activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_57 (Conv1D)              (None, 21, 64)       20544       max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_57 (Batc (None, 21, 64)       256         conv1d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 21, 64)       0           batch_normalization_v1_57[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling1D) (None, 7, 64)        0           activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_20 (Gl (None, 64)           0           max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_21 (Gl (None, 64)           0           max_pooling1d_57[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 128)          0           global_average_pooling1d_20[0][0]\n",
      "                                                                 global_average_pooling1d_21[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 128)          0           concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 16)           2064        dropout_10[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 70,384\n",
      "Trainable params: 69,744\n",
      "Non-trainable params: 640\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 613us/sample - loss: 0.1840 - acc: 0.9512\n",
      "Loss: 0.18396019145830894 Accuracy: 0.95119417\n",
      "\n",
      "1D_CNN_custom_multi_2_GAP_ch_32_DO_BN_8_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_12 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_58 (Conv1D)              (None, 16000, 32)    192         input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_58 (Batc (None, 16000, 32)    128         conv1d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 16000, 32)    0           batch_normalization_v1_58[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling1D) (None, 5333, 32)     0           activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_59 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_58[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_59 (Batc (None, 5333, 32)     128         conv1d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 5333, 32)     0           batch_normalization_v1_59[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling1D) (None, 1777, 32)     0           activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_60 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_59[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_60 (Batc (None, 1777, 32)     128         conv1d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 1777, 32)     0           batch_normalization_v1_60[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling1D) (None, 592, 32)      0           activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_61 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_60[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_61 (Batc (None, 592, 32)      128         conv1d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 592, 32)      0           batch_normalization_v1_61[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling1D) (None, 197, 32)      0           activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_62 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_61[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_62 (Batc (None, 197, 64)      256         conv1d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 197, 64)      0           batch_normalization_v1_62[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling1D) (None, 65, 64)       0           activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_63 (Conv1D)              (None, 65, 64)       20544       max_pooling1d_62[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_63 (Batc (None, 65, 64)       256         conv1d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 65, 64)       0           batch_normalization_v1_63[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling1D) (None, 21, 64)       0           activation_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_64 (Conv1D)              (None, 21, 64)       20544       max_pooling1d_63[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_64 (Batc (None, 21, 64)       256         conv1d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 21, 64)       0           batch_normalization_v1_64[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling1D) (None, 7, 64)        0           activation_64[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_65 (Conv1D)              (None, 7, 64)        20544       max_pooling1d_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_65 (Batc (None, 7, 64)        256         conv1d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 7, 64)        0           batch_normalization_v1_65[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling1D) (None, 2, 64)        0           activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_22 (Gl (None, 64)           0           max_pooling1d_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_23 (Gl (None, 64)           0           max_pooling1d_65[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 128)          0           global_average_pooling1d_22[0][0]\n",
      "                                                                 global_average_pooling1d_23[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 128)          0           concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 16)           2064        dropout_11[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 91,184\n",
      "Trainable params: 90,416\n",
      "Non-trainable params: 768\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 663us/sample - loss: 0.2015 - acc: 0.9448\n",
      "Loss: 0.20153303648638204 Accuracy: 0.944756\n"
     ]
    }
   ],
   "source": [
    "for i in range(3, 9):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + '000_last.hdf5'\n",
    "\n",
    "    model = load_model(model_filename)\n",
    "    model.summary()\n",
    "\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
