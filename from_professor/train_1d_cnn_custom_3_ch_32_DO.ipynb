{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path as path\n",
    "import itertools\n",
    "from sklearn.preprocessing import maxabs_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Input,InputLayer, Dense, Activation, BatchNormalization, Flatten, Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D, Dropout\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler, \\\n",
    "                                        EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = path.join('..', 'data')\n",
    "data_dir = path.join(base_dir, 'data_speech_commands_v0.02')\n",
    " \n",
    "train_txt = path.join(data_dir, 'wav_train_16words.txt')\n",
    "val_txt = path.join(data_dir, 'wav_validation_16words.txt')\n",
    "test_txt = path.join(data_dir, 'wav_test_16words.txt')\n",
    "\n",
    "train_data = np.load(path.join(data_dir, 'wav_train_data.npz'))\n",
    "val_data = np.load(path.join(data_dir, 'wav_validation_data.npz'))\n",
    "test_data = np.load(path.join(data_dir, 'wav_test_data.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36805, 16000, 1),\n",
       " (36805,),\n",
       " (4293, 16000, 1),\n",
       " (4293,),\n",
       " (4815, 16000, 1),\n",
       " (4815,),\n",
       " (16, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = train_data['x_train']\n",
    "y_train = train_data['y_train']\n",
    "x_val = val_data['x_val']\n",
    "y_val = val_data['y_val']\n",
    "x_test = test_data['x_test']\n",
    "y_test = test_data['y_test']\n",
    "y_table = test_data['table']\n",
    "\n",
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape, y_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_test[0].shape\n",
    "output_size = y_table.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_abs = np.asarray([maxabs_scale(wav) for wav in x_train])\n",
    "y_train_onehot = np.asarray([to_categorical(label, output_size) for label in y_train])\n",
    "del x_train, y_train\n",
    "\n",
    "x_val_abs = np.asarray([maxabs_scale(wav) for wav in x_val])\n",
    "y_val_onehot = np.asarray([to_categorical(label, output_size) for label in y_val])\n",
    "del x_val, y_val\n",
    "\n",
    "x_test_abs = np.asarray([maxabs_scale(wav) for wav in x_test])\n",
    "y_test_onehot = np.asarray([to_categorical(label, output_size) for label in y_test])\n",
    "del x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_1d_cnn_custom_DO(conv_num=1):\n",
    "    init_channel = 32\n",
    "    \n",
    "    model=Sequential()\n",
    "    model.add(Conv1D (kernel_size=5, filters=init_channel, strides=1, padding='same', \n",
    "                      activation='relu', input_shape=input_shape)) \n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(MaxPooling1D(pool_size=3, strides=3, padding='same'))\n",
    "    \n",
    "    for i in range(conv_num-1):\n",
    "        model.add(Conv1D (kernel_size=5, filters=int(init_channel*(2**int((i+1)/3))), \n",
    "                          strides=1, padding='same', activation='relu'))\n",
    "#         model.add(BatchNormalization())\n",
    "        model.add(MaxPooling1D(pool_size=3, strides=3))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(output_size, activation='softmax' ))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 512000)            0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 512000)            0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                8192016   \n",
      "=================================================================\n",
      "Total params: 8,192,208\n",
      "Trainable params: 8,192,208\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 170656)            0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 170656)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                2730512   \n",
      "=================================================================\n",
      "Total params: 2,735,856\n",
      "Trainable params: 2,735,856\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_3 (Conv1D)            (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 56864)             0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 56864)             0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                909840    \n",
      "=================================================================\n",
      "Total params: 920,336\n",
      "Trainable params: 920,336\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_6 (Conv1D)            (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 1777, 64)          10304     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                606224    \n",
      "=================================================================\n",
      "Total params: 627,024\n",
      "Trainable params: 627,024\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_10 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_12 (Conv1D)           (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 1777, 64)          10304     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 592, 64)           20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1 (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 12608)             0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 12608)             0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                201744    \n",
      "=================================================================\n",
      "Total params: 243,088\n",
      "Trainable params: 243,088\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_15 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "conv1d_16 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_17 (Conv1D)           (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_18 (Conv1D)           (None, 1777, 64)          10304     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_19 (Conv1D)           (None, 592, 64)           20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_20 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 4160)              0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 4160)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 16)                66576     \n",
      "=================================================================\n",
      "Total params: 128,464\n",
      "Trainable params: 128,464\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_21 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "conv1d_22 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_23 (Conv1D)           (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_24 (Conv1D)           (None, 1777, 64)          10304     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_25 (Conv1D)           (None, 592, 64)           20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_26 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_27 (Conv1D)           (None, 65, 128)           41088     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 16)                43024     \n",
      "=================================================================\n",
      "Total params: 146,000\n",
      "Trainable params: 146,000\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_28 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "conv1d_29 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_30 (Conv1D)           (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_31 (Conv1D)           (None, 1777, 64)          10304     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_32 (Conv1D)           (None, 592, 64)           20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_33 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_34 (Conv1D)           (None, 65, 128)           41088     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_26 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_35 (Conv1D)           (None, 21, 128)           82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_27 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 16)                14352     \n",
      "=================================================================\n",
      "Total params: 199,376\n",
      "Trainable params: 199,376\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_36 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "conv1d_37 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_28 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_38 (Conv1D)           (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_29 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_39 (Conv1D)           (None, 1777, 64)          10304     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_30 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_40 (Conv1D)           (None, 592, 64)           20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_31 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_41 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_32 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_42 (Conv1D)           (None, 65, 128)           41088     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_43 (Conv1D)           (None, 21, 128)           82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_44 (Conv1D)           (None, 7, 128)            82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling (None, 2, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 16)                4112      \n",
      "=================================================================\n",
      "Total params: 271,184\n",
      "Trainable params: 271,184\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 10):\n",
    "    model = build_1d_cnn_custom_DO(conv_num=i)\n",
    "    model.summary()\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36805 samples, validate on 4293 samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 2.2855 - acc: 0.2659\n",
      "Epoch 00001: val_loss improved from inf to 1.81828, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_4_conv_checkpoint/001-1.8183.hdf5\n",
      "36805/36805 [==============================] - 27s 740us/sample - loss: 2.2843 - acc: 0.2665 - val_loss: 1.8183 - val_acc: 0.4442\n",
      "Epoch 2/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.6729 - acc: 0.4720\n",
      "Epoch 00002: val_loss improved from 1.81828 to 1.51419, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_4_conv_checkpoint/002-1.5142.hdf5\n",
      "36805/36805 [==============================] - 25s 680us/sample - loss: 1.6724 - acc: 0.4722 - val_loss: 1.5142 - val_acc: 0.5167\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4608 - acc: 0.5468\n",
      "Epoch 00003: val_loss improved from 1.51419 to 1.37636, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_4_conv_checkpoint/003-1.3764.hdf5\n",
      "36805/36805 [==============================] - 25s 669us/sample - loss: 1.4607 - acc: 0.5468 - val_loss: 1.3764 - val_acc: 0.5826\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3185 - acc: 0.5962\n",
      "Epoch 00004: val_loss improved from 1.37636 to 1.31569, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_4_conv_checkpoint/004-1.3157.hdf5\n",
      "36805/36805 [==============================] - 25s 667us/sample - loss: 1.3185 - acc: 0.5963 - val_loss: 1.3157 - val_acc: 0.5947\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2214 - acc: 0.6262\n",
      "Epoch 00005: val_loss improved from 1.31569 to 1.24551, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_4_conv_checkpoint/005-1.2455.hdf5\n",
      "36805/36805 [==============================] - 25s 671us/sample - loss: 1.2213 - acc: 0.6262 - val_loss: 1.2455 - val_acc: 0.6091\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1380 - acc: 0.6530\n",
      "Epoch 00006: val_loss improved from 1.24551 to 1.21760, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_4_conv_checkpoint/006-1.2176.hdf5\n",
      "36805/36805 [==============================] - 25s 671us/sample - loss: 1.1382 - acc: 0.6530 - val_loss: 1.2176 - val_acc: 0.6182\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0656 - acc: 0.6757\n",
      "Epoch 00007: val_loss improved from 1.21760 to 1.18272, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_4_conv_checkpoint/007-1.1827.hdf5\n",
      "36805/36805 [==============================] - 25s 666us/sample - loss: 1.0656 - acc: 0.6756 - val_loss: 1.1827 - val_acc: 0.6268\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9978 - acc: 0.6974\n",
      "Epoch 00008: val_loss improved from 1.18272 to 1.13863, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_4_conv_checkpoint/008-1.1386.hdf5\n",
      "36805/36805 [==============================] - 24s 665us/sample - loss: 0.9979 - acc: 0.6974 - val_loss: 1.1386 - val_acc: 0.6536\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9356 - acc: 0.7140\n",
      "Epoch 00009: val_loss did not improve from 1.13863\n",
      "36805/36805 [==============================] - 24s 665us/sample - loss: 0.9356 - acc: 0.7140 - val_loss: 1.1636 - val_acc: 0.6406\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8814 - acc: 0.7321\n",
      "Epoch 00010: val_loss improved from 1.13863 to 1.12753, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_4_conv_checkpoint/010-1.1275.hdf5\n",
      "36805/36805 [==============================] - 25s 669us/sample - loss: 0.8813 - acc: 0.7322 - val_loss: 1.1275 - val_acc: 0.6427\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8299 - acc: 0.7492\n",
      "Epoch 00011: val_loss did not improve from 1.12753\n",
      "36805/36805 [==============================] - 24s 663us/sample - loss: 0.8303 - acc: 0.7492 - val_loss: 1.1513 - val_acc: 0.6464\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7779 - acc: 0.7632\n",
      "Epoch 00012: val_loss did not improve from 1.12753\n",
      "36805/36805 [==============================] - 25s 666us/sample - loss: 0.7778 - acc: 0.7632 - val_loss: 1.1620 - val_acc: 0.6422\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7315 - acc: 0.7778\n",
      "Epoch 00013: val_loss improved from 1.12753 to 1.10260, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_4_conv_checkpoint/013-1.1026.hdf5\n",
      "36805/36805 [==============================] - 25s 667us/sample - loss: 0.7315 - acc: 0.7778 - val_loss: 1.1026 - val_acc: 0.6576\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6924 - acc: 0.7890\n",
      "Epoch 00014: val_loss did not improve from 1.10260\n",
      "36805/36805 [==============================] - 24s 664us/sample - loss: 0.6924 - acc: 0.7890 - val_loss: 1.1065 - val_acc: 0.6648\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6465 - acc: 0.8032\n",
      "Epoch 00015: val_loss did not improve from 1.10260\n",
      "36805/36805 [==============================] - 25s 667us/sample - loss: 0.6466 - acc: 0.8032 - val_loss: 1.1393 - val_acc: 0.6567\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6136 - acc: 0.8123\n",
      "Epoch 00016: val_loss did not improve from 1.10260\n",
      "36805/36805 [==============================] - 25s 668us/sample - loss: 0.6135 - acc: 0.8124 - val_loss: 1.1525 - val_acc: 0.6608\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5749 - acc: 0.8247\n",
      "Epoch 00017: val_loss did not improve from 1.10260\n",
      "36805/36805 [==============================] - 24s 663us/sample - loss: 0.5749 - acc: 0.8247 - val_loss: 1.1191 - val_acc: 0.6711\n",
      "Epoch 18/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5380 - acc: 0.8346\n",
      "Epoch 00018: val_loss did not improve from 1.10260\n",
      "36805/36805 [==============================] - 25s 666us/sample - loss: 0.5379 - acc: 0.8347 - val_loss: 1.1055 - val_acc: 0.6767\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5109 - acc: 0.8420\n",
      "Epoch 00019: val_loss did not improve from 1.10260\n",
      "36805/36805 [==============================] - 25s 666us/sample - loss: 0.5110 - acc: 0.8420 - val_loss: 1.1316 - val_acc: 0.6792\n",
      "Epoch 20/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4851 - acc: 0.8498\n",
      "Epoch 00020: val_loss did not improve from 1.10260\n",
      "36805/36805 [==============================] - 24s 664us/sample - loss: 0.4850 - acc: 0.8499 - val_loss: 1.1353 - val_acc: 0.6783\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4530 - acc: 0.8598\n",
      "Epoch 00021: val_loss did not improve from 1.10260\n",
      "36805/36805 [==============================] - 24s 661us/sample - loss: 0.4530 - acc: 0.8598 - val_loss: 1.1337 - val_acc: 0.6804\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4288 - acc: 0.8655\n",
      "Epoch 00022: val_loss did not improve from 1.10260\n",
      "36805/36805 [==============================] - 25s 668us/sample - loss: 0.4288 - acc: 0.8655 - val_loss: 1.1439 - val_acc: 0.6706\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4088 - acc: 0.8723\n",
      "Epoch 00023: val_loss did not improve from 1.10260\n",
      "36805/36805 [==============================] - 24s 663us/sample - loss: 0.4088 - acc: 0.8723 - val_loss: 1.1589 - val_acc: 0.6823\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3836 - acc: 0.8791\n",
      "Epoch 00024: val_loss did not improve from 1.10260\n",
      "36805/36805 [==============================] - 24s 665us/sample - loss: 0.3835 - acc: 0.8791 - val_loss: 1.1608 - val_acc: 0.6844\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3614 - acc: 0.8877\n",
      "Epoch 00025: val_loss did not improve from 1.10260\n",
      "36805/36805 [==============================] - 25s 667us/sample - loss: 0.3614 - acc: 0.8877 - val_loss: 1.2238 - val_acc: 0.6823\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3507 - acc: 0.8891\n",
      "Epoch 00026: val_loss did not improve from 1.10260\n",
      "36805/36805 [==============================] - 24s 662us/sample - loss: 0.3507 - acc: 0.8891 - val_loss: 1.2128 - val_acc: 0.6827\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3331 - acc: 0.8932\n",
      "Epoch 00027: val_loss did not improve from 1.10260\n",
      "36805/36805 [==============================] - 24s 663us/sample - loss: 0.3331 - acc: 0.8931 - val_loss: 1.2901 - val_acc: 0.6660\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3156 - acc: 0.9007\n",
      "Epoch 00028: val_loss did not improve from 1.10260\n",
      "36805/36805 [==============================] - 24s 661us/sample - loss: 0.3156 - acc: 0.9007 - val_loss: 1.2076 - val_acc: 0.6937\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2992 - acc: 0.9067\n",
      "Epoch 00029: val_loss did not improve from 1.10260\n",
      "36805/36805 [==============================] - 25s 666us/sample - loss: 0.2992 - acc: 0.9066 - val_loss: 1.1965 - val_acc: 0.6928\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2865 - acc: 0.9097\n",
      "Epoch 00030: val_loss did not improve from 1.10260\n",
      "36805/36805 [==============================] - 24s 661us/sample - loss: 0.2865 - acc: 0.9097 - val_loss: 1.2627 - val_acc: 0.6846\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2737 - acc: 0.9140\n",
      "Epoch 00031: val_loss did not improve from 1.10260\n",
      "36805/36805 [==============================] - 24s 662us/sample - loss: 0.2737 - acc: 0.9140 - val_loss: 1.2445 - val_acc: 0.6946\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2623 - acc: 0.9174\n",
      "Epoch 00032: val_loss did not improve from 1.10260\n",
      "36805/36805 [==============================] - 24s 665us/sample - loss: 0.2622 - acc: 0.9174 - val_loss: 1.2957 - val_acc: 0.6928\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2543 - acc: 0.9190\n",
      "Epoch 00033: val_loss did not improve from 1.10260\n",
      "36805/36805 [==============================] - 24s 663us/sample - loss: 0.2545 - acc: 0.9190 - val_loss: 1.2656 - val_acc: 0.6918\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2472 - acc: 0.9211\n",
      "Epoch 00034: val_loss did not improve from 1.10260\n",
      "36805/36805 [==============================] - 24s 663us/sample - loss: 0.2472 - acc: 0.9212 - val_loss: 1.2746 - val_acc: 0.7007\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2396 - acc: 0.9237\n",
      "Epoch 00035: val_loss did not improve from 1.10260\n",
      "36805/36805 [==============================] - 24s 663us/sample - loss: 0.2396 - acc: 0.9237 - val_loss: 1.2472 - val_acc: 0.7044\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2272 - acc: 0.9278\n",
      "Epoch 00036: val_loss did not improve from 1.10260\n",
      "36805/36805 [==============================] - 25s 667us/sample - loss: 0.2272 - acc: 0.9278 - val_loss: 1.2830 - val_acc: 0.6993\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2221 - acc: 0.9290\n",
      "Epoch 00037: val_loss did not improve from 1.10260\n",
      "36805/36805 [==============================] - 24s 663us/sample - loss: 0.2220 - acc: 0.9290 - val_loss: 1.2652 - val_acc: 0.7056\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2112 - acc: 0.9334\n",
      "Epoch 00038: val_loss did not improve from 1.10260\n",
      "36805/36805 [==============================] - 24s 664us/sample - loss: 0.2112 - acc: 0.9334 - val_loss: 1.3140 - val_acc: 0.6946\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2068 - acc: 0.9344\n",
      "Epoch 00039: val_loss did not improve from 1.10260\n",
      "36805/36805 [==============================] - 25s 666us/sample - loss: 0.2068 - acc: 0.9344 - val_loss: 1.2824 - val_acc: 0.7037\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2005 - acc: 0.9369\n",
      "Epoch 00040: val_loss did not improve from 1.10260\n",
      "36805/36805 [==============================] - 24s 665us/sample - loss: 0.2007 - acc: 0.9369 - val_loss: 1.3149 - val_acc: 0.7114\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1940 - acc: 0.9375\n",
      "Epoch 00041: val_loss did not improve from 1.10260\n",
      "36805/36805 [==============================] - 24s 662us/sample - loss: 0.1940 - acc: 0.9375 - val_loss: 1.3430 - val_acc: 0.6969\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1887 - acc: 0.9404\n",
      "Epoch 00042: val_loss did not improve from 1.10260\n",
      "36805/36805 [==============================] - 24s 661us/sample - loss: 0.1887 - acc: 0.9404 - val_loss: 1.3236 - val_acc: 0.7037\n",
      "Epoch 43/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1758 - acc: 0.9453\n",
      "Epoch 00043: val_loss did not improve from 1.10260\n",
      "36805/36805 [==============================] - 24s 666us/sample - loss: 0.1756 - acc: 0.9453 - val_loss: 1.3250 - val_acc: 0.7109\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1790 - acc: 0.9442\n",
      "Epoch 00044: val_loss did not improve from 1.10260\n",
      "36805/36805 [==============================] - 24s 664us/sample - loss: 0.1789 - acc: 0.9442 - val_loss: 1.3371 - val_acc: 0.7119\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1707 - acc: 0.9460\n",
      "Epoch 00045: val_loss did not improve from 1.10260\n",
      "36805/36805 [==============================] - 24s 662us/sample - loss: 0.1707 - acc: 0.9460 - val_loss: 1.3580 - val_acc: 0.7030\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1663 - acc: 0.9468\n",
      "Epoch 00046: val_loss did not improve from 1.10260\n",
      "36805/36805 [==============================] - 24s 662us/sample - loss: 0.1663 - acc: 0.9468 - val_loss: 1.3600 - val_acc: 0.7084\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1583 - acc: 0.9510\n",
      "Epoch 00047: val_loss did not improve from 1.10260\n",
      "36805/36805 [==============================] - 24s 665us/sample - loss: 0.1582 - acc: 0.9510 - val_loss: 1.3762 - val_acc: 0.7102\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1601 - acc: 0.9486\n",
      "Epoch 00048: val_loss did not improve from 1.10260\n",
      "36805/36805 [==============================] - 24s 662us/sample - loss: 0.1601 - acc: 0.9486 - val_loss: 1.3732 - val_acc: 0.7074\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1527 - acc: 0.9518\n",
      "Epoch 00049: val_loss did not improve from 1.10260\n",
      "36805/36805 [==============================] - 24s 664us/sample - loss: 0.1526 - acc: 0.9518 - val_loss: 1.3860 - val_acc: 0.7053\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1512 - acc: 0.9540\n",
      "Epoch 00050: val_loss did not improve from 1.10260\n",
      "36805/36805 [==============================] - 24s 663us/sample - loss: 0.1512 - acc: 0.9539 - val_loss: 1.3613 - val_acc: 0.7205\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1421 - acc: 0.9551\n",
      "Epoch 00051: val_loss did not improve from 1.10260\n",
      "36805/36805 [==============================] - 25s 666us/sample - loss: 0.1421 - acc: 0.9551 - val_loss: 1.4370 - val_acc: 0.7174\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1453 - acc: 0.9539\n",
      "Epoch 00052: val_loss did not improve from 1.10260\n",
      "36805/36805 [==============================] - 24s 662us/sample - loss: 0.1454 - acc: 0.9539 - val_loss: 1.4135 - val_acc: 0.7058\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1385 - acc: 0.9565\n",
      "Epoch 00053: val_loss did not improve from 1.10260\n",
      "36805/36805 [==============================] - 24s 664us/sample - loss: 0.1385 - acc: 0.9565 - val_loss: 1.4095 - val_acc: 0.7093\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1331 - acc: 0.9582\n",
      "Epoch 00054: val_loss did not improve from 1.10260\n",
      "36805/36805 [==============================] - 24s 661us/sample - loss: 0.1331 - acc: 0.9581 - val_loss: 1.4220 - val_acc: 0.7079\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1374 - acc: 0.9571\n",
      "Epoch 00055: val_loss did not improve from 1.10260\n",
      "36805/36805 [==============================] - 25s 666us/sample - loss: 0.1374 - acc: 0.9571 - val_loss: 1.3902 - val_acc: 0.7174\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1283 - acc: 0.9598\n",
      "Epoch 00056: val_loss did not improve from 1.10260\n",
      "36805/36805 [==============================] - 24s 664us/sample - loss: 0.1283 - acc: 0.9598 - val_loss: 1.4131 - val_acc: 0.7179\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1355 - acc: 0.9578\n",
      "Epoch 00057: val_loss did not improve from 1.10260\n",
      "36805/36805 [==============================] - 24s 663us/sample - loss: 0.1355 - acc: 0.9578 - val_loss: 1.4038 - val_acc: 0.7088\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1271 - acc: 0.9601\n",
      "Epoch 00058: val_loss did not improve from 1.10260\n",
      "36805/36805 [==============================] - 25s 668us/sample - loss: 0.1271 - acc: 0.9600 - val_loss: 1.4253 - val_acc: 0.7130\n",
      "Epoch 59/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1239 - acc: 0.9618\n",
      "Epoch 00059: val_loss did not improve from 1.10260\n",
      "36805/36805 [==============================] - 25s 670us/sample - loss: 0.1239 - acc: 0.9619 - val_loss: 1.4512 - val_acc: 0.7070\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1200 - acc: 0.9637\n",
      "Epoch 00060: val_loss did not improve from 1.10260\n",
      "36805/36805 [==============================] - 24s 664us/sample - loss: 0.1200 - acc: 0.9637 - val_loss: 1.4612 - val_acc: 0.7053\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1195 - acc: 0.9630\n",
      "Epoch 00061: val_loss did not improve from 1.10260\n",
      "36805/36805 [==============================] - 24s 663us/sample - loss: 0.1196 - acc: 0.9630 - val_loss: 1.4129 - val_acc: 0.7158\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1154 - acc: 0.9637\n",
      "Epoch 00062: val_loss did not improve from 1.10260\n",
      "36805/36805 [==============================] - 24s 664us/sample - loss: 0.1154 - acc: 0.9637 - val_loss: 1.4351 - val_acc: 0.7209\n",
      "Epoch 63/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1124 - acc: 0.9661\n",
      "Epoch 00063: val_loss did not improve from 1.10260\n",
      "36805/36805 [==============================] - 25s 668us/sample - loss: 0.1123 - acc: 0.9661 - val_loss: 1.4500 - val_acc: 0.7151\n",
      "\n",
      "1D_CNN_custom_3_ch_32_DO_4_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 360us/sample - loss: 1.1983 - acc: 0.6339\n",
      "Loss: 1.1983275781168001 Accuracy: 0.63385254\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.3008 - acc: 0.2468\n",
      "Epoch 00001: val_loss improved from inf to 1.72951, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_5_conv_checkpoint/001-1.7295.hdf5\n",
      "36805/36805 [==============================] - 27s 723us/sample - loss: 2.3007 - acc: 0.2468 - val_loss: 1.7295 - val_acc: 0.4491\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6506 - acc: 0.4657\n",
      "Epoch 00002: val_loss improved from 1.72951 to 1.47672, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_5_conv_checkpoint/002-1.4767.hdf5\n",
      "36805/36805 [==============================] - 26s 697us/sample - loss: 1.6506 - acc: 0.4657 - val_loss: 1.4767 - val_acc: 0.5367\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4579 - acc: 0.5356\n",
      "Epoch 00003: val_loss improved from 1.47672 to 1.31964, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_5_conv_checkpoint/003-1.3196.hdf5\n",
      "36805/36805 [==============================] - 26s 698us/sample - loss: 1.4578 - acc: 0.5356 - val_loss: 1.3196 - val_acc: 0.5931\n",
      "Epoch 4/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3289 - acc: 0.5830\n",
      "Epoch 00004: val_loss improved from 1.31964 to 1.23872, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_5_conv_checkpoint/004-1.2387.hdf5\n",
      "36805/36805 [==============================] - 26s 699us/sample - loss: 1.3284 - acc: 0.5830 - val_loss: 1.2387 - val_acc: 0.6282\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2276 - acc: 0.6157\n",
      "Epoch 00005: val_loss improved from 1.23872 to 1.17650, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_5_conv_checkpoint/005-1.1765.hdf5\n",
      "36805/36805 [==============================] - 26s 694us/sample - loss: 1.2276 - acc: 0.6157 - val_loss: 1.1765 - val_acc: 0.6420\n",
      "Epoch 6/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1523 - acc: 0.6406\n",
      "Epoch 00006: val_loss improved from 1.17650 to 1.08940, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_5_conv_checkpoint/006-1.0894.hdf5\n",
      "36805/36805 [==============================] - 26s 696us/sample - loss: 1.1521 - acc: 0.6406 - val_loss: 1.0894 - val_acc: 0.6755\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0868 - acc: 0.6639\n",
      "Epoch 00007: val_loss did not improve from 1.08940\n",
      "36805/36805 [==============================] - 26s 694us/sample - loss: 1.0870 - acc: 0.6639 - val_loss: 1.1268 - val_acc: 0.6387\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0272 - acc: 0.6837\n",
      "Epoch 00008: val_loss improved from 1.08940 to 1.00941, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_5_conv_checkpoint/008-1.0094.hdf5\n",
      "36805/36805 [==============================] - 26s 697us/sample - loss: 1.0272 - acc: 0.6837 - val_loss: 1.0094 - val_acc: 0.6958\n",
      "Epoch 9/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9811 - acc: 0.6981\n",
      "Epoch 00009: val_loss improved from 1.00941 to 0.97316, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_5_conv_checkpoint/009-0.9732.hdf5\n",
      "36805/36805 [==============================] - 26s 695us/sample - loss: 0.9812 - acc: 0.6982 - val_loss: 0.9732 - val_acc: 0.7011\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9338 - acc: 0.7116\n",
      "Epoch 00010: val_loss improved from 0.97316 to 0.94094, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_5_conv_checkpoint/010-0.9409.hdf5\n",
      "36805/36805 [==============================] - 26s 694us/sample - loss: 0.9339 - acc: 0.7116 - val_loss: 0.9409 - val_acc: 0.7058\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8918 - acc: 0.7260\n",
      "Epoch 00011: val_loss improved from 0.94094 to 0.90830, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_5_conv_checkpoint/011-0.9083.hdf5\n",
      "36805/36805 [==============================] - 26s 695us/sample - loss: 0.8918 - acc: 0.7260 - val_loss: 0.9083 - val_acc: 0.7242\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8527 - acc: 0.7380\n",
      "Epoch 00012: val_loss improved from 0.90830 to 0.90016, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_5_conv_checkpoint/012-0.9002.hdf5\n",
      "36805/36805 [==============================] - 26s 694us/sample - loss: 0.8527 - acc: 0.7380 - val_loss: 0.9002 - val_acc: 0.7282\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8145 - acc: 0.7510\n",
      "Epoch 00013: val_loss improved from 0.90016 to 0.84429, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_5_conv_checkpoint/013-0.8443.hdf5\n",
      "36805/36805 [==============================] - 26s 700us/sample - loss: 0.8145 - acc: 0.7510 - val_loss: 0.8443 - val_acc: 0.7438\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7834 - acc: 0.7615\n",
      "Epoch 00014: val_loss did not improve from 0.84429\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.7835 - acc: 0.7614 - val_loss: 0.8575 - val_acc: 0.7421\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7583 - acc: 0.7680\n",
      "Epoch 00015: val_loss improved from 0.84429 to 0.82026, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_5_conv_checkpoint/015-0.8203.hdf5\n",
      "36805/36805 [==============================] - 26s 694us/sample - loss: 0.7583 - acc: 0.7680 - val_loss: 0.8203 - val_acc: 0.7487\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7320 - acc: 0.7772\n",
      "Epoch 00016: val_loss improved from 0.82026 to 0.80732, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_5_conv_checkpoint/016-0.8073.hdf5\n",
      "36805/36805 [==============================] - 25s 693us/sample - loss: 0.7320 - acc: 0.7772 - val_loss: 0.8073 - val_acc: 0.7652\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7039 - acc: 0.7852\n",
      "Epoch 00017: val_loss improved from 0.80732 to 0.79819, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_5_conv_checkpoint/017-0.7982.hdf5\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.7038 - acc: 0.7852 - val_loss: 0.7982 - val_acc: 0.7636\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6778 - acc: 0.7937\n",
      "Epoch 00018: val_loss improved from 0.79819 to 0.77344, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_5_conv_checkpoint/018-0.7734.hdf5\n",
      "36805/36805 [==============================] - 26s 697us/sample - loss: 0.6779 - acc: 0.7937 - val_loss: 0.7734 - val_acc: 0.7729\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6530 - acc: 0.7998\n",
      "Epoch 00019: val_loss did not improve from 0.77344\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.6530 - acc: 0.7999 - val_loss: 0.7898 - val_acc: 0.7796\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6288 - acc: 0.8070\n",
      "Epoch 00020: val_loss improved from 0.77344 to 0.77252, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_5_conv_checkpoint/020-0.7725.hdf5\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.6288 - acc: 0.8070 - val_loss: 0.7725 - val_acc: 0.7734\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6102 - acc: 0.8127\n",
      "Epoch 00021: val_loss improved from 0.77252 to 0.75421, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_5_conv_checkpoint/021-0.7542.hdf5\n",
      "36805/36805 [==============================] - 26s 696us/sample - loss: 0.6101 - acc: 0.8128 - val_loss: 0.7542 - val_acc: 0.7761\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5860 - acc: 0.8195\n",
      "Epoch 00022: val_loss improved from 0.75421 to 0.74847, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_5_conv_checkpoint/022-0.7485.hdf5\n",
      "36805/36805 [==============================] - 26s 694us/sample - loss: 0.5859 - acc: 0.8195 - val_loss: 0.7485 - val_acc: 0.7806\n",
      "Epoch 23/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5711 - acc: 0.8235\n",
      "Epoch 00023: val_loss improved from 0.74847 to 0.74618, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_5_conv_checkpoint/023-0.7462.hdf5\n",
      "36805/36805 [==============================] - 26s 699us/sample - loss: 0.5708 - acc: 0.8236 - val_loss: 0.7462 - val_acc: 0.7792\n",
      "Epoch 24/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5482 - acc: 0.8333\n",
      "Epoch 00024: val_loss improved from 0.74618 to 0.73775, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_5_conv_checkpoint/024-0.7378.hdf5\n",
      "36805/36805 [==============================] - 26s 695us/sample - loss: 0.5484 - acc: 0.8333 - val_loss: 0.7378 - val_acc: 0.7841\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5371 - acc: 0.8333\n",
      "Epoch 00025: val_loss improved from 0.73775 to 0.72146, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_5_conv_checkpoint/025-0.7215.hdf5\n",
      "36805/36805 [==============================] - 26s 696us/sample - loss: 0.5370 - acc: 0.8333 - val_loss: 0.7215 - val_acc: 0.7908\n",
      "Epoch 26/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5139 - acc: 0.8428\n",
      "Epoch 00026: val_loss did not improve from 0.72146\n",
      "36805/36805 [==============================] - 26s 694us/sample - loss: 0.5137 - acc: 0.8429 - val_loss: 0.7391 - val_acc: 0.7890\n",
      "Epoch 27/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4995 - acc: 0.8449\n",
      "Epoch 00027: val_loss did not improve from 0.72146\n",
      "36805/36805 [==============================] - 26s 696us/sample - loss: 0.5000 - acc: 0.8449 - val_loss: 0.7470 - val_acc: 0.7920\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4823 - acc: 0.8501\n",
      "Epoch 00028: val_loss did not improve from 0.72146\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.4826 - acc: 0.8500 - val_loss: 0.9337 - val_acc: 0.7405\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4753 - acc: 0.8515\n",
      "Epoch 00029: val_loss improved from 0.72146 to 0.71768, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_5_conv_checkpoint/029-0.7177.hdf5\n",
      "36805/36805 [==============================] - 26s 694us/sample - loss: 0.4752 - acc: 0.8515 - val_loss: 0.7177 - val_acc: 0.7915\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4561 - acc: 0.8586\n",
      "Epoch 00030: val_loss did not improve from 0.71768\n",
      "36805/36805 [==============================] - 26s 693us/sample - loss: 0.4563 - acc: 0.8586 - val_loss: 0.7590 - val_acc: 0.7880\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4438 - acc: 0.8645\n",
      "Epoch 00031: val_loss improved from 0.71768 to 0.71517, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_5_conv_checkpoint/031-0.7152.hdf5\n",
      "36805/36805 [==============================] - 26s 693us/sample - loss: 0.4438 - acc: 0.8644 - val_loss: 0.7152 - val_acc: 0.7945\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4295 - acc: 0.8656\n",
      "Epoch 00032: val_loss improved from 0.71517 to 0.71349, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_5_conv_checkpoint/032-0.7135.hdf5\n",
      "36805/36805 [==============================] - 25s 693us/sample - loss: 0.4295 - acc: 0.8656 - val_loss: 0.7135 - val_acc: 0.7985\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4161 - acc: 0.8710\n",
      "Epoch 00033: val_loss improved from 0.71349 to 0.70595, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_5_conv_checkpoint/033-0.7059.hdf5\n",
      "36805/36805 [==============================] - 26s 696us/sample - loss: 0.4161 - acc: 0.8710 - val_loss: 0.7059 - val_acc: 0.8004\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4072 - acc: 0.8721\n",
      "Epoch 00034: val_loss did not improve from 0.70595\n",
      "36805/36805 [==============================] - 26s 696us/sample - loss: 0.4072 - acc: 0.8721 - val_loss: 0.7388 - val_acc: 0.7911\n",
      "Epoch 35/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3968 - acc: 0.8739\n",
      "Epoch 00035: val_loss did not improve from 0.70595\n",
      "36805/36805 [==============================] - 26s 695us/sample - loss: 0.3966 - acc: 0.8739 - val_loss: 0.7267 - val_acc: 0.8001\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3851 - acc: 0.8769\n",
      "Epoch 00036: val_loss did not improve from 0.70595\n",
      "36805/36805 [==============================] - 26s 700us/sample - loss: 0.3851 - acc: 0.8769 - val_loss: 0.7301 - val_acc: 0.8006\n",
      "Epoch 37/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3786 - acc: 0.8814\n",
      "Epoch 00037: val_loss did not improve from 0.70595\n",
      "36805/36805 [==============================] - 26s 700us/sample - loss: 0.3786 - acc: 0.8813 - val_loss: 0.7237 - val_acc: 0.7950\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3666 - acc: 0.8843\n",
      "Epoch 00038: val_loss did not improve from 0.70595\n",
      "36805/36805 [==============================] - 25s 693us/sample - loss: 0.3666 - acc: 0.8844 - val_loss: 0.7174 - val_acc: 0.8085\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3578 - acc: 0.8874\n",
      "Epoch 00039: val_loss did not improve from 0.70595\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.3578 - acc: 0.8874 - val_loss: 0.7324 - val_acc: 0.8001\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3445 - acc: 0.8909\n",
      "Epoch 00040: val_loss did not improve from 0.70595\n",
      "36805/36805 [==============================] - 26s 694us/sample - loss: 0.3445 - acc: 0.8909 - val_loss: 0.7345 - val_acc: 0.8046\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3343 - acc: 0.8923\n",
      "Epoch 00041: val_loss did not improve from 0.70595\n",
      "36805/36805 [==============================] - 26s 705us/sample - loss: 0.3342 - acc: 0.8923 - val_loss: 0.7335 - val_acc: 0.8022\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3255 - acc: 0.8954\n",
      "Epoch 00042: val_loss did not improve from 0.70595\n",
      "36805/36805 [==============================] - 26s 701us/sample - loss: 0.3254 - acc: 0.8954 - val_loss: 0.7174 - val_acc: 0.8104\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3213 - acc: 0.8973\n",
      "Epoch 00043: val_loss did not improve from 0.70595\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.3213 - acc: 0.8974 - val_loss: 0.7535 - val_acc: 0.7980\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3121 - acc: 0.9010\n",
      "Epoch 00044: val_loss did not improve from 0.70595\n",
      "36805/36805 [==============================] - 26s 698us/sample - loss: 0.3121 - acc: 0.9010 - val_loss: 0.7271 - val_acc: 0.8057\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3078 - acc: 0.9021\n",
      "Epoch 00045: val_loss did not improve from 0.70595\n",
      "36805/36805 [==============================] - 26s 695us/sample - loss: 0.3079 - acc: 0.9021 - val_loss: 0.7394 - val_acc: 0.8053\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2994 - acc: 0.9023\n",
      "Epoch 00046: val_loss did not improve from 0.70595\n",
      "36805/36805 [==============================] - 26s 693us/sample - loss: 0.2996 - acc: 0.9023 - val_loss: 0.7345 - val_acc: 0.8130\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2937 - acc: 0.9056\n",
      "Epoch 00047: val_loss did not improve from 0.70595\n",
      "36805/36805 [==============================] - 26s 697us/sample - loss: 0.2937 - acc: 0.9056 - val_loss: 0.7250 - val_acc: 0.8069\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2871 - acc: 0.9063\n",
      "Epoch 00048: val_loss did not improve from 0.70595\n",
      "36805/36805 [==============================] - 26s 693us/sample - loss: 0.2872 - acc: 0.9063 - val_loss: 0.7537 - val_acc: 0.8034\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2856 - acc: 0.9086\n",
      "Epoch 00049: val_loss did not improve from 0.70595\n",
      "36805/36805 [==============================] - 26s 699us/sample - loss: 0.2856 - acc: 0.9085 - val_loss: 0.7345 - val_acc: 0.8039\n",
      "Epoch 50/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2766 - acc: 0.9101\n",
      "Epoch 00050: val_loss did not improve from 0.70595\n",
      "36805/36805 [==============================] - 26s 697us/sample - loss: 0.2766 - acc: 0.9101 - val_loss: 0.7312 - val_acc: 0.8097\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2699 - acc: 0.9109\n",
      "Epoch 00051: val_loss did not improve from 0.70595\n",
      "36805/36805 [==============================] - 25s 693us/sample - loss: 0.2699 - acc: 0.9109 - val_loss: 0.7429 - val_acc: 0.8143\n",
      "Epoch 52/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2655 - acc: 0.9148\n",
      "Epoch 00052: val_loss did not improve from 0.70595\n",
      "36805/36805 [==============================] - 26s 697us/sample - loss: 0.2656 - acc: 0.9147 - val_loss: 0.7435 - val_acc: 0.8120\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2531 - acc: 0.9185\n",
      "Epoch 00053: val_loss did not improve from 0.70595\n",
      "36805/36805 [==============================] - 26s 696us/sample - loss: 0.2531 - acc: 0.9185 - val_loss: 0.7500 - val_acc: 0.8120\n",
      "Epoch 54/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2497 - acc: 0.9183\n",
      "Epoch 00054: val_loss did not improve from 0.70595\n",
      "36805/36805 [==============================] - 26s 699us/sample - loss: 0.2495 - acc: 0.9184 - val_loss: 0.7648 - val_acc: 0.8076\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2528 - acc: 0.9171\n",
      "Epoch 00055: val_loss did not improve from 0.70595\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.2528 - acc: 0.9171 - val_loss: 0.7803 - val_acc: 0.8057\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2451 - acc: 0.9201\n",
      "Epoch 00056: val_loss did not improve from 0.70595\n",
      "36805/36805 [==============================] - 26s 696us/sample - loss: 0.2451 - acc: 0.9201 - val_loss: 0.7680 - val_acc: 0.8092\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2419 - acc: 0.9212\n",
      "Epoch 00057: val_loss did not improve from 0.70595\n",
      "36805/36805 [==============================] - 26s 693us/sample - loss: 0.2419 - acc: 0.9212 - val_loss: 0.7662 - val_acc: 0.8123\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2336 - acc: 0.9233\n",
      "Epoch 00058: val_loss did not improve from 0.70595\n",
      "36805/36805 [==============================] - 26s 694us/sample - loss: 0.2337 - acc: 0.9233 - val_loss: 0.7605 - val_acc: 0.8109\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2310 - acc: 0.9238\n",
      "Epoch 00059: val_loss did not improve from 0.70595\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.2310 - acc: 0.9238 - val_loss: 0.7818 - val_acc: 0.8078\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2389 - acc: 0.9211\n",
      "Epoch 00060: val_loss did not improve from 0.70595\n",
      "36805/36805 [==============================] - 26s 695us/sample - loss: 0.2389 - acc: 0.9211 - val_loss: 0.7609 - val_acc: 0.8157\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2255 - acc: 0.9273\n",
      "Epoch 00061: val_loss did not improve from 0.70595\n",
      "36805/36805 [==============================] - 26s 694us/sample - loss: 0.2255 - acc: 0.9273 - val_loss: 0.7906 - val_acc: 0.8099\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2240 - acc: 0.9282\n",
      "Epoch 00062: val_loss did not improve from 0.70595\n",
      "36805/36805 [==============================] - 25s 692us/sample - loss: 0.2241 - acc: 0.9281 - val_loss: 0.7631 - val_acc: 0.8130\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2196 - acc: 0.9294\n",
      "Epoch 00063: val_loss did not improve from 0.70595\n",
      "36805/36805 [==============================] - 26s 696us/sample - loss: 0.2195 - acc: 0.9294 - val_loss: 0.7985 - val_acc: 0.8018\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2170 - acc: 0.9287\n",
      "Epoch 00064: val_loss did not improve from 0.70595\n",
      "36805/36805 [==============================] - 25s 689us/sample - loss: 0.2171 - acc: 0.9287 - val_loss: 0.8025 - val_acc: 0.8067\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2159 - acc: 0.9283\n",
      "Epoch 00065: val_loss did not improve from 0.70595\n",
      "36805/36805 [==============================] - 25s 691us/sample - loss: 0.2159 - acc: 0.9283 - val_loss: 0.8557 - val_acc: 0.8029\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2098 - acc: 0.9311\n",
      "Epoch 00066: val_loss did not improve from 0.70595\n",
      "36805/36805 [==============================] - 26s 701us/sample - loss: 0.2098 - acc: 0.9311 - val_loss: 0.7702 - val_acc: 0.8160\n",
      "Epoch 67/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2035 - acc: 0.9342\n",
      "Epoch 00067: val_loss did not improve from 0.70595\n",
      "36805/36805 [==============================] - 26s 705us/sample - loss: 0.2035 - acc: 0.9342 - val_loss: 0.7917 - val_acc: 0.8090\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2041 - acc: 0.9329\n",
      "Epoch 00068: val_loss did not improve from 0.70595\n",
      "36805/36805 [==============================] - 26s 698us/sample - loss: 0.2041 - acc: 0.9329 - val_loss: 0.7560 - val_acc: 0.8188\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1957 - acc: 0.9366\n",
      "Epoch 00069: val_loss did not improve from 0.70595\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.1957 - acc: 0.9366 - val_loss: 0.7945 - val_acc: 0.8123\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1986 - acc: 0.9349\n",
      "Epoch 00070: val_loss did not improve from 0.70595\n",
      "36805/36805 [==============================] - 25s 690us/sample - loss: 0.1986 - acc: 0.9349 - val_loss: 0.7830 - val_acc: 0.8195\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1991 - acc: 0.9364\n",
      "Epoch 00071: val_loss did not improve from 0.70595\n",
      "36805/36805 [==============================] - 26s 702us/sample - loss: 0.1990 - acc: 0.9364 - val_loss: 0.8041 - val_acc: 0.8176\n",
      "Epoch 72/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1953 - acc: 0.9363\n",
      "Epoch 00072: val_loss did not improve from 0.70595\n",
      "36805/36805 [==============================] - 26s 697us/sample - loss: 0.1961 - acc: 0.9362 - val_loss: 0.7879 - val_acc: 0.8160\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1913 - acc: 0.9366\n",
      "Epoch 00073: val_loss did not improve from 0.70595\n",
      "36805/36805 [==============================] - 26s 697us/sample - loss: 0.1913 - acc: 0.9366 - val_loss: 0.7709 - val_acc: 0.8206\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1916 - acc: 0.9366\n",
      "Epoch 00074: val_loss did not improve from 0.70595\n",
      "36805/36805 [==============================] - 26s 701us/sample - loss: 0.1915 - acc: 0.9366 - val_loss: 0.8176 - val_acc: 0.8081\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1818 - acc: 0.9406\n",
      "Epoch 00075: val_loss did not improve from 0.70595\n",
      "36805/36805 [==============================] - 26s 697us/sample - loss: 0.1817 - acc: 0.9406 - val_loss: 0.8184 - val_acc: 0.8204\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1815 - acc: 0.9398\n",
      "Epoch 00076: val_loss did not improve from 0.70595\n",
      "36805/36805 [==============================] - 26s 695us/sample - loss: 0.1815 - acc: 0.9398 - val_loss: 0.7943 - val_acc: 0.8218\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1873 - acc: 0.9386\n",
      "Epoch 00077: val_loss did not improve from 0.70595\n",
      "36805/36805 [==============================] - 26s 696us/sample - loss: 0.1874 - acc: 0.9385 - val_loss: 0.7927 - val_acc: 0.8153\n",
      "Epoch 78/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1775 - acc: 0.9418\n",
      "Epoch 00078: val_loss did not improve from 0.70595\n",
      "36805/36805 [==============================] - 26s 703us/sample - loss: 0.1774 - acc: 0.9419 - val_loss: 0.7826 - val_acc: 0.8192\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1773 - acc: 0.9416\n",
      "Epoch 00079: val_loss did not improve from 0.70595\n",
      "36805/36805 [==============================] - 26s 698us/sample - loss: 0.1773 - acc: 0.9416 - val_loss: 0.7782 - val_acc: 0.8248\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1746 - acc: 0.9432\n",
      "Epoch 00080: val_loss did not improve from 0.70595\n",
      "36805/36805 [==============================] - 26s 696us/sample - loss: 0.1746 - acc: 0.9432 - val_loss: 0.7809 - val_acc: 0.8183\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1687 - acc: 0.9448\n",
      "Epoch 00081: val_loss did not improve from 0.70595\n",
      "36805/36805 [==============================] - 26s 701us/sample - loss: 0.1687 - acc: 0.9448 - val_loss: 0.8028 - val_acc: 0.8206\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1686 - acc: 0.9447\n",
      "Epoch 00082: val_loss did not improve from 0.70595\n",
      "36805/36805 [==============================] - 26s 699us/sample - loss: 0.1686 - acc: 0.9447 - val_loss: 0.8378 - val_acc: 0.8097\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1686 - acc: 0.9446\n",
      "Epoch 00083: val_loss did not improve from 0.70595\n",
      "36805/36805 [==============================] - 26s 698us/sample - loss: 0.1687 - acc: 0.9446 - val_loss: 0.8014 - val_acc: 0.8237\n",
      "\n",
      "1D_CNN_custom_3_ch_32_DO_5_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzs3Xl8VNXd+PHPmSWTZbIvJECAsAghBMKOorhVBW1xRVxRW7X+6qO1tlaqto/2aetSbRVr6+PWuq9ocXvAqiCKILJvCULYEghZyTqZySzn98fJhhAIkGFC5vt+ve5rmMyde78zJOd7tnuu0lojhBBCAFhCHYAQQojuQ5KCEEKIVpIUhBBCtJKkIIQQopUkBSGEEK0kKQghhGglSUEIIUQrSQpCCCFaSVIQQgjRyhbqAI5USkqKHjBgQKjDEEKIE8rKlSsrtNaph9vvhEsKAwYMYMWKFaEOQwghTihKqZ2d2U+6j4QQQrSSpCCEEKKVJAUhhBCtTrgxhYPxer0UFxfjdrtDHcoJKzIykr59+2K320MdihAihHpEUiguLiY2NpYBAwaglAp1OCccrTWVlZUUFxeTlZUV6nCEECHUI7qP3G43ycnJkhCOklKK5ORkaWkJIXpGUgAkIRwj+f6EENCDksLh+P2NeDy7CQS8oQ5FCCG6rbBJCoGAm6amErTu+qRQXV3N3//+96N67/nnn091dXWn97///vt59NFHj+pcQghxOGGTFJSyAqC1v8uPfaik4PP5Dvnejz/+mISEhC6PSQghjoYkhS4we/ZsCgsLycvL46677mLRokWcdtppTJ8+neHDhwNw0UUXMXbsWHJycnjmmWda3ztgwAAqKirYsWMH2dnZ3HTTTeTk5HDuuefS2Nh4yPOuWbOGSZMmMXLkSC6++GL27dsHwJw5cxg+fDgjR47kiiuuAOCLL74gLy+PvLw8Ro8eTV1dXZd/D0KIE1+PmJLa3pYtd1Bfv+YgrwTw+xuwWCJR6sjm4judeQwZ8niHrz/00ENs2LCBNWvMeRctWsSqVavYsGFD6xTPF154gaSkJBobGxk/fjyXXnopycnJ34t9C6+//jrPPvssl19+OXPnzuWaa67p8LyzZs3iySef5PTTT+d3v/sdDzzwAI8//jgPPfQQ27dvx+FwtHZNPfroozz11FNMnjyZ+vp6IiMjj+g7EEKEh7BpKUDL7Bp9XM42YcKE/eb8z5kzh1GjRjFp0iSKiorYsmXLAe/JysoiLy8PgLFjx7Jjx44Oj19TU0N1dTWnn346ANdddx2LFy8GYOTIkVx99dW88sor2Gwm70+ePJk777yTOXPmUF1d3fpzIYRor8eVDB3V6LXW1NevJCKiNw5H76DHERMT0/rvRYsW8emnn7J06VKio6M544wzDnpNgMPhaP231Wo9bPdRRz766CMWL17MBx98wB//+EfWr1/P7NmzueCCC/j444+ZPHkyCxYsYNiwYUd1fCFEzxU2LQUzD98SlDGF2NjYQ/bR19TUkJiYSHR0NAUFBSxbtuyYzxkfH09iYiJffvklAC+//DKnn346gUCAoqIizjzzTB5++GFqamqor6+nsLCQ3Nxc7r77bsaPH09BQcExxyCE6Hl6XEvhUJSyBSUpJCcnM3nyZEaMGMG0adO44IIL9nt96tSpPP3002RnZzN06FAmTZrUJed98cUXueWWW3C5XAwcOJB//vOf+P1+rrnmGmpqatBac/vtt5OQkMBvf/tbFi5ciMViIScnh2nTpnVJDEKInkVpfXz62LvKuHHj9PdvspOfn092dvZh39vQsBGLxUFU1OBghXdC6+z3KIQ48SilVmqtxx1uv7DpPjKsQWkpCCFETxFWSUEpSQpCCHEokhSEEEK0CrOkEJyBZiGE6CnCLClYAR8n2uC6EEIcL2GVFMDa/BgIaRRCCNFdhVVSCOaieEfK6XQe0c+FEOJ4kKQghBCilSSFLjB79myeeuqp1uctN8Kpr6/n7LPPZsyYMeTm5jJv3rxOH1NrzV133cWIESPIzc3lzTffBKCkpIQpU6aQl5fHiBEj+PLLL/H7/Vx//fWt+/71r3/t0s8nhAgfPW+ZizvugDUHWzobrNpPVMCF1RIF6gg+el4ePN7x0tkzZ87kjjvu4NZbbwXgrbfeYsGCBURGRvLee+8RFxdHRUUFkyZNYvr06Z26H/K7777LmjVrWLt2LRUVFYwfP54pU6bw2muvcd5553Hvvffi9/txuVysWbOG3bt3s2HDBoAjupObEEK01/OSwiGZwlij6crb1I8ePZqysjL27NlDeXk5iYmJZGZm4vV6ueeee1i8eDEWi4Xdu3dTWlpKenr6YY/51VdfceWVV2K1WunVqxenn3463377LePHj+fHP/4xXq+Xiy66iLy8PAYOHMi2bdu47bbbuOCCCzj33HO78NMJIcJJz0sKh6jR64CXxoa1OBz9iIhI69LTzpgxg3feeYe9e/cyc+ZMAF599VXKy8tZuXIldrudAQMGHHTJ7CMxZcoUFi9ezEcffcT111/PnXfeyaxZs1i7di0LFizg6aef5q233uKFF17oio8lhAgzMqbQRWbOnMkbb7zBO++8w4wZMwCzZHZaWhp2u52FCxeyc+fOTh/vtNNO480338Tv91NeXs7ixYuZMGECO3fupFevXtx0003ceOONrFq1ioqKCgKBAJdeeil/+MMfWLVqVZd/PiFEeOh5LYVDUMoCqKAkhZycHOrq6ujTpw8ZGRkAXH311fzoRz8iNzeXcePGHdFNbS6++GKWLl3KqFGjUErxyCOPkJ6ezosvvsif//xn7HY7TqeTl156id27d3PDDTcQCJjrLx588MEu/3xCiPAQVktnA9TXr8VmiycyckAQojuxydLZQvRcIV86WymVqZRaqJTapJTaqJT6+UH2UUqpOUqprUqpdUqpMcGKp+2csiieEEJ0JJjdRz7gl1rrVUqpWGClUuo/WutN7faZBgxp3iYC/2h+DCJJCkII0ZGgtRS01iVa61XN/64D8oE+39vtQuAlbSwDEpRSGcGKCaSlIIQQh3JcZh8ppQYAo4FvvvdSH6Co3fNiDkwcXRyLJAUhhOhI0JOCUsoJzAXu0FrXHuUxblZKrVBKrSgvLz/GeGyYni0hhBDfF9SkoJSyYxLCq1rrdw+yy24gs93zvs0/24/W+hmt9Tit9bjU1NRjjEpaCkII0ZFgzj5SwPNAvtb6Lx3s9j4wq3kW0iSgRmtdEqyYTFxWQKN1191Tobq6mr///e9H9d7zzz9f1ioSQnQbwWwpTAauBc5SSq1p3s5XSt2ilLqleZ+PgW3AVuBZ4GdBjAcIzlXNh0oKPt+hu6o+/vhjEhISuiwWIYQ4FsGcffSV1lpprUdqrfOat4+11k9rrZ9u3kdrrW/VWg/SWudqrVcc7rjHqi0pdN24wuzZsyksLCQvL4+77rqLRYsWcdpppzF9+nSGDx8OwEUXXcTYsWPJycnhmWeeaX3vgAEDqKioYMeOHWRnZ3PTTTeRk5PDueeeS2Nj4wHn+uCDD5g4cSKjR4/mBz/4AaWlpQDU19dzww03kJuby8iRI5k7dy4A8+fPZ8yYMYwaNYqzzz67yz6zEKJn6nHLXBxi5WwAtI4nEBiKxRJBJ1awBg67cjYPPfQQGzZsYE3ziRctWsSqVavYsGEDWVlZALzwwgskJSXR2NjI+PHjufTSS0lOTt7vOFu2bOH111/n2Wef5fLLL2fu3Llcc801++1z6qmnsmzZMpRSPPfcczzyyCM89thj/M///A/x8fGsX78egH379lFeXs5NN93E4sWLycrKoqqqqnMfWAgRtnpcUji8lkwQ3OU9JkyY0JoQAObMmcN7770HQFFREVu2bDkgKWRlZZGXlwfA2LFj2bFjxwHHLS4uZubMmZSUlNDU1NR6jk8//ZQ33nijdb/ExEQ++OADpkyZ0rpPUlJSl35GIUTP0+OSwqFq9AB+fxMu12YiIwditwevkIyJiWn996JFi/j0009ZunQp0dHRnHHGGQddQtvhcLT+22q1HrT76LbbbuPOO+9k+vTpLFq0iPvvvz8o8QshwlNYLZ0NwRlojo2Npa6ursPXa2pqSExMJDo6moKCApYtW3bU56qpqaFPH3N934svvtj683POOWe/W4Lu27ePSZMmsXjxYrZv3w4g3UdCiMOSpNAFkpOTmTx5MiNGjOCuu+464PWpU6fi8/nIzs5m9uzZTJo06ajPdf/99zNjxgzGjh1LSkpK68/vu+8+9u3bx4gRIxg1ahQLFy4kNTWVZ555hksuuYRRo0a13vxHCCE6EnZLZ2utqa9fRURELxyOvsEI8YQlS2cL0XOFfOns7kopJesfCSFEB8IuKRiSFIQQ4mDCMilIS0EIIQ4uTJOCTZKCEEIcRJgmBSuyfLYQQhwoLJOCjCkIIcTBhWVS6A5jCk6nM6TnF0KIgwnbpACBLr2nghBC9ARhnBS67qrm2bNn77fExP3338+jjz5KfX09Z599NmPGjCE3N5d58+Yd9lgdLbF9sCWwO1ouWwghjlaPWxDvjvl3sGbvIdbOBrT2Egi4sVhiUOrweTEvPY/Hp3a80t7MmTO54447uPXWWwF46623WLBgAZGRkbz33nvExcVRUVHBpEmTmD59OuoQa3YfbIntQCBw0CWwD7ZcthBCHIselxQ6p2uXzx49ejRlZWXs2bOH8vJyEhMTyczMxOv1cs8997B48WIsFgu7d++mtLSU9PT0Do91sCW2y8vLD7oE9sGWyxZCiGPR45LCoWr0LXy+OhobNxMVdRI2W1yXnHfGjBm888477N27t3XhuVdffZXy8nJWrlyJ3W5nwIABB10yu0Vnl9gWQohgkTGFLjJz5kzeeOMN3nnnHWbMmAGYZa7T0tKw2+0sXLiQnTt3HvIYHS2x3dES2AdbLlsIIY5F+CQFlwt27QKfLyj3ac7JyaGuro4+ffqQkZEBwNVXX82KFSvIzc3lpZdeYtiwYYc8RkdLbHe0BPbBlssWQohjET5LZ1dXw9atMGwYOiaK+vrVOBx9iYjouH8/3MjS2UL0XLJ09vdFRppHt5uWjx3qC9iEEKK7CZ+k4HCAUtDY2DwlNPRXNQshRHfTY5LCYbvBlDKthebZPN1hqYvu5ETrRhRCBEePSAqRkZFUVlYevmDbLynYunSg+USmtaayspLIli42IUTY6hHXKfTt25fi4mLKy8sPvWN1NdTUgNVKk7cM0ERESGIAk1j79pV7VgsR7npEUrDb7a1X+x7S66/DVVfBunVssD5BY+M2Ro1aG/wAhRDiBNEjuo86rWW6ZX4+NlsSTU2loY1HCCG6mfBKCiedZAacCwqIicnF6y3F49kT6qiEEKLbCK+kEB0N/ftDfj5xceMBqKv7NsRBCSFE9xFeSQFg2DAoKMDpHA1Yqa2VpCCEEC3CLylkZ8PmzVhVJDExI6SlIIQQ7YRnUmhshF27iIsbT13dCrlwSwghmoVfUmhZqTQ/n9jY8fh8Vbjd20IbkxBCdBPhlxRapqUWFBAbawaba2uXhzAgIYToPsIvKaSkQHIy5OcTEzMCiyVSxhWEEKJZ+CUFMK2F/HwsFjtOZ54kBSGEaBaeSaF5WipAbOx46upWEQjIGkhCCBG0pKCUekEpVaaU2tDB62copWqUUmuat98FK5YDZGdDRQVUVBAbO4FAwIXLlX/cTi+EEN1VMFsK/wKmHmafL7XWec3b74MYy/7aDTbLlc1CCNEmaElBa70YqArW8Y9Ju2mpUVFDsFrjZAaSEEIQ+jGFk5VSa5VS/6eUyuloJ6XUzUqpFUqpFYe9Z0Jn9O9vbrhTUIBSFmJjx0lLQQghCG1SWAX011qPAp4E/t3RjlrrZ7TW47TW41JTU4/9zBYLDB0K+WYcITZ2PA0N6/D73cd+bCGEOIGFLClorWu11vXN//4YsCulUo5bANnZsMGMgcfFjUdrHw0NcsMdIUR4C1lSUEqlK6VU878nNMdSedwCOPlkKCqCHTvaXdksXUhCiPAWzCmprwNLgaFKqWKl1E+UUrcopW5p3uUyYINSai0wB7hCH8+V6c480zwuXIjDkUlERAY1NYuP2+mFEKI7Cto9mrXWVx7m9b8BfwvW+Q8rJwdSU+Hzz1E33EBy8gWUlb1JIODBYnGELCwhhAilUM8+Ch2LBc44AxYuBK1JTr4Qv7+OffsWhjoyIYQImfBNCmC6kHbvhq1bSUw8G4slhsrKeaGOSgghQkaSAsDChVitUSQlnUdFxftoHQhtXEIIESLhnRSGDoWMDPj8cwBSUi6kqWkPdXUrQxyYEEKERngnBaVMa2HRouZxhQsAKxUV0oUkhAhP4Z0UwCSF0lLIz8duTyY+/lQqKjq8uFoIIXo0SQrtxhUAUlIuwuXaSGNjYQiDEkKI0JCkMHAg9Ou337gCIF1IQoiwJEmh/bhCIEBUVBYxMbmSFIQQYUmSApikUFUF69cDprVQU/MVTU0VIQ5MCCGOL0kK0Dau0NqFdDEQoLz8ndDFJIQQISBJAcyYwogR8MILEAjgdI4mJmYkJSXPhjoyIYQ4rjqVFJRSP1dKxSnjeaXUKqXUucEO7ri6+25zf4X33kMpRe/eN1Nfv0ouZBNChJXOthR+rLWuBc4FEoFrgYeCFlUoXHEFDBkCv/89BAKkpV2NxRLFnj3SWhBChI/OJgXV/Hg+8LLWemO7n/UMNhvcdx+sWwfvv4/dnkBq6uWUlb2Kz1cf6uiEEOK46GxSWKmU+gSTFBYopWKBnrdq3FVXweDBprWgNb1734zfX095+ZuhjkwIIY6LziaFnwCzgfFaaxdgB24IWlShYrPBvffC6tXw4YfExZ1MdPRw9ux5JtSRCSHEcdHZpHAysFlrXa2Uuga4D6gJXlghdPXVkJUFv/89Cujd+2bq6pZTX7821JEJIUTQdTYp/ANwKaVGAb8ECoGXghZVKNntprWwYgW8+y69el2LUg4ZcBZChIXOJgWf1loDFwJ/01o/BcQGL6wQmzULRo2C227D3mAhNfUySktfxuutDnVkQggRVJ1NCnVKqd9gpqJ+pJSyYMYVeia7HZ57ziyp/etfk5n5K/z+WoqKHg11ZEIIEVSdTQozAQ/meoW9QF/gz0GLqjsYNw7uvBOefZbYldWkpV1BcfFf8Xj2hjoyIYQImk4lheZE8CoQr5T6IeDWWvfMMYX2HngABg2Cm25iQK970bqJnTv/EOqohBAiaDq7zMXlwHJgBnA58I1S6rJgBtYtREfDM8/A1q1E//kVMjJupKTkf2ls3BbqyIQQIig62310L+Yaheu01rOACcBvgxdWN3LWWfDjH8Ojj9KfG1DKzo4d/x3qqIQQIig6mxQsWuuyds8rj+C9J7777welcPzjdfr0uZ3S0lepr18X6qiEEKLLdbZgn6+UWqCUul4pdT3wEfBx8MLqZjIzzRIYzz5LP+fN2GzxFBb+GjNLVwgheo7ODjTfBTwDjGzentFa3x3MwLqdX/0KGhqwP/c6/fv/N/v2LaC8fG6ooxJCiC6lTrTa7rhx4/SKFStCc/Lzz4eVKwlsL2TVptNoaipjwrDl2B56Am68EU46KTRxCSHEYSilVmqtxx1uv0O2FJRSdUqp2oNsdUqp2q4L9wTx619DWRmWV17jpJOeRpfvwXfGWPjzn+FPfwp1dEIIccykpXAktIaJE6G6Gj77DM+Zo7AV7YOcbKyFu6GsDByO0MQmhBCH0CUtBfE9SsFdd8GWLZCbS0SZl02PJbDthgDU1sL8+aGOUAghjokkhSN1ySXmtp02G+rzhaRe9iR7sjfjT4yBN+VmPEKIE5st1AGccKxW+OIL85iWRi89ltLSlymb/Dnp789DuVzmSmghhDgBSUvhaGRkQFoaAEophg59lvKz7agGF/qjD0McnBBCHD1JCl0gMrIfqZc9QVMiuP/1YKjDEUKIoyZJoYuk97mR2nP7EfHpGhrL5NadQogTU9CSglLqBaVUmVJqQwevK6XUHKXUVqXUOqXUmGDFcjwopYi96a9Ym6D0uSvQ2h/qkIQQ4ogFs6XwL2DqIV6fBgxp3m7G3Af6hOY48yL8GYk4PyqgsPCuUIcjhBBHLGhJQWu9GKg6xC4XAi9pYxmQoJTKCFY8x4XFgvWK60n+1krlsr9SXPxkqCMSQogjEsoxhT5AUbvnxc0/O7H97GcQn8jYn0ew96PbqaiYF+qIhBCi006IgWal1M1KqRVKqRXl5eWhDufQBg9GLVmCNTad0Xda2PPS5dTWfhvqqIQQolNCmRR2A5ntnvdt/tkBtNbPaK3Haa3HpaamHpfgjslJJ6G+XooaNIwRdzdR8pcf4HbvDHVUQghxWKFMCu8Ds5pnIU0CarTWJSGMp2v17o1l8RL0xHEMfaCWqv83Hp9nX6ijEkKIQwrmlNTXgaXAUKVUsVLqJ0qpW5RStzTv8jGwDdgKPAv8LFixhExCAtbPvsI963x6/6sc1zlDCVRVhDoqIYToUNDWPtJaX3mY1zVwa7DO3204HES++BHVw68m7t7X8I0dQsTHX0N2dqgjE6JH8/nMwsZW64GvaQ1+v3m9ZWv5md8PgcD++zc1wd69sGcPlJRAfT1ERIDdbh4dDoiMNJvNBlVVZiX98nKzgHLLOSwW8HrB5Tr45nYfGGv7GGfNMnNZgkkWxDtOEu5+leL+PtJ+9haBCaOxvP4O/PCHoQ4rdDweuPlmuOceGDo01NGIINHaFI4lJaYwbNk8HmhsNIWgx2MKbrvdFKh2uyk8Wza32xSyLVtlpdkqKqCmxhTKkZEQFWUK79JSU4BXVpoYHA5wOs3rHk9bAXy8biXTkpS0NskmIsKsmRkdbWKKiWl7HhtrCv/231/7LTIy+PFKUjiO+sx8je9Squl96yc4p09H/eEP8Jvf7P9bEC6WLYOXXoJBg+B3vwt1ND1WIGAKz5ISU1BWVJjCpaXmabO11XAdDlNYthS6VVWm4PZ4zOZ2Q12dqfnW1Znn7WvWTickJpotIgIKC6GgwNyTqqtYrZCcbLaUFOjTxySZxkYTr81m7oo7ZYpZs9JigYYGU7NvbGxLHtHRJsaWwjYQMPtarWazWPb/s7TZID3drIXZu7cpvL1ek4RaNrfbbF4vJCVBaqrZTrRFkyUpHEdKWRly1vtseOl8et37Ob3uvRfWrIHbb4fRo02VIVx88415XCvrRLXndpsuh5oa0/3h9ZrHhgZTGNfUmMf2XQ6NjW0FlNdrCuG9e81WVmYK7aPVkiwcDlOYxsaaLT4eevXavxCtr4d9+6C42MQ0aBBceSUMGwZ9+7Z1t9jtbYmo5fh+f9vn9XrbCmq/3+yTlGS279ekRdeTpHCcWSwOcsa9z7pHzqP+2aUMfGYu6u23zV/V8OFw/vnw4IPmeU/WkhTWrAltHMdIa1NIt3Rn1NWZrb7eFM6lpW3dGTU1bbVuj8cUeC2Fn89njlFf3/lzK2VqoS0Fa0uBGxdnarNjxrTVbtPTzZaSYn61WmrIPl9bK8DtNgV/S008IeHg/fGiZ5OkEAJWawy5Iz9izU/OZum568l130fsZj8sWQKPPGL+In/961CHGVwtSWHbNlOqxsWFNBy3u63bpKWAb9nKytq2fftMLbhlq642BWtHLBbThZCebgrZ+Pi2mnf7boqWbpGWLof4+LZC3m43jci4uLYtJsbUvKXWLLqa0sdrtKWLjBs3Tq9YsSLUYXSJpqYK1q49G5drMyNGvEty0jSYMQPmzYOlS2HcYe+xfWLavdv0J5xzDvznP/Dll3Dqqcd8WK3bulFcLlPrbmgwNff2A5Tl5aaLo6jIPO7da/brSEKC6Z9OSzNdGFFRbVtCgql9p6SY1+LjTd96bKwpvJOTpbYtugel1Eqt9WELFWkphFBERAp5eZ+zdu25bNhwETk5b5PyzDOmFn3VVbBqlSlheprly83jT39qksLatR0mhcZGM2C5davpfmnp5qivhx07TENj2zYzkNrU1LkZJTabGaDMzIQJE9q6VdoPYLZ/HhHRdR9diO5OkkKI2e3JjBr1GevWTWXjxsvIzn6NtJdfhrPOgjvugOeeM6Xhhx/C4sVw3XVwyimhDvvYfPMN2O0Epl1AY1ImrqVbcf3Q1OI3bID1682Wn29q8x0V9GlpMHCg+Tr69Nl/UDQ62nSxOJ3mMSnJFPAttfmePmTTY3z2mfnPPPnkUEcSNqT7qJvw+WpZt+58amuXMmzYv0ifk28GnE8/3XQlNTW1TeZ+5x244IJQh3xIPp9pAOTnt01hrKszF/9sf2812129KPb3PujMGIfDXNs3fLi5hGHIELMlJbXNWGnpvhE92PLlpgWZnAzbtx+fSfo9mHQfnWBstjhGjpzPhg0XUlBwHYGf/o3eS6aYP4Zbb4XLLoPBg83spIsughdfNF1MIRIImAJ/yRLTkGmpWzQ0mEsQvv76wJk0Fgukp2sG1Ho4dXAx/S/rTcKXHxC19HOi//4o8UlWhg83CcAmv5nhraoKLr/cNPX27jXXtNx8c6ijOn6amuDOO80A2HPPmUGq40RaCt2M39/Ixo0zqKr6iEGD/kJm5i/236G2FqZPN11Jf/2rSRhBLEFra2HLFjM2vGeP2datg6++artitD2lYMQIOO00s40ZY7pr4uJMRU9tWA8jR5o/8muvNY/XXQebNsnSH8LQ2lR8/u//zC/af/2XSRKbNx9y1L7J34Tb5ybOceBMNo/Pw47qHditdmLsMTgjnETbo1Hfm77V0NRAUW0RJXUl5PbKJSU6xbwQCLReCLKzchv57iIatZdGXyNun5t0ZzrZKdn0T+iPRZm+yYAOUN5QTlFtEVurtrK1aiuF+wpxWB0MTx3O8NThDE0eSkAHqPHUUO02V/mNd55E1BXXwqefmprUmDFseuWvvLZ7Pqf2O5Wpgw91Q8uOSUvhBGW1RjFixLvk519DYeGdeL2VZGX9HtX8i0ZcnPljueIKM+bwhz+YP6BLLoGzzz6qUdFAwMzC2brVJIAtW0wZvWGD6dNvz2KBrCyTl1oK/ozm++W1TK10OA5xspapqBMnmsdRo8zjmjXdIimU1pcS64gl2t75y1C11uyo3kGzd8jJAAAgAElEQVR+RT6j00eTEdu5Gwi6vC7Wl65n9d7VbNu3jX7x/Tgp+SSGJg8lMz6ztXDpiNvnZtu+bWzbt41dNbtaN4/fQ7wjnnhHPHGOOOxWe+t7rMpKRmwGfeP6khmXSS9nL5wRTmwWW+tnqW+qp6yhjLKGMipcFVQ2VlLhqqC+qZ5IWyRRtiii7FHEOeJIiU4hOSqZ+Mh4tlZtZVXJKlaWrGz9PEOShjAkaQiJUYlUuiqpbKykqrGKrIQspg6eyqCkQQd+sL/8Bd5/Hx5/HCZMQN99N+tuvYx3/nE58x27SIpKIjctlxFpI0h3prOseBmLdy5mafFS3D43Gc4MhqcOZ1jKMPa597GudB0FFQX4AgfOHY62RxNtjybGHkNdUx1VjW03i7QoC1P6T+Gifb0Y87e5fJrpY94wWJve8f9JtD2arIQsqt3V7K3fi/9792rvHdubRm8j+9wdr5js8Cum9IZzHr8cf0wkr69+mXVvnIZFWfjtlN8edVLoLGkpdFOBgI8tW/4fJSXPkZQ0lezsV7Dbk9t28PnM1NW5c80gdF2d6WSfMMGMvE6ebAar23W8a21m6RQUmEJ/3Tqzbdiw/5RMh8NchZqTY7bsbDODtHdvcxXrMTVMbrrJxFxZabJIU5PpIrjzTnjoocO+vaGpgZL6EkrrS6lrqqPOU9daWJ2ceTL94/u31v601nxX+R0r9qygwduAP+DHr/3YLXayErMYnDSYfvH9KKkr4e1Nb/PWxrf4Zvc3WJWVnLQcJvSeQE5aDlWNVRTXFlNcW0xdUx1xjrjWAre4tpgVe1ZQ2djWbJrQZwIXDr2Qcb3HkV+ez6q9q1hdsppyVzlWZcVqsaJQFNUWEdBm5TWbxbZfoWWz2EiNTiUtJo20mDQcNkdr/C213l01u9C0/f1GWCPIjMsk0hZJjaeGGncNdU11nfpvcVgdOCOcuLwuGn2NnXpPRwYmDmRw0mCKaooo3FdIk7/pgHN5/B4ABiUO4ozYEdRVl7GzYQ87vOXU+F2k6SjSB+TSy9mLTeWbKNxXiEXDKf1OxeVzsal8E26fWT3Ooizkpecxpd8Uejl7UVBRwKbyTRRUFJAYlcjIXiMZmTaSYSnD0Gjq66uoX/QJ9aVFuLIH09A3jQZfIzH2GPrF96NffD9SolP4cteX/HvFK2xsNPdCsaA4xTKACyNyOfnz74hZV0BU3ngcD/yB3ekxbCrfRH5FPturt5MYmUiGM4Pesb3pE9eHwUmDGZg4kGh7NFpryhrK2FS+ie8qv8NutZskvqkQ918e5vPkOj6Z0oeNrh0AnBw/givnFXL5rlh6vfcf09I+Cp1tKUhS6Ma01pSUPMOWLbcTEZHBiBFziY0de+CObrdpan72menkX70avy/A5t5n8c3lj7HcPZKVK00yqGtXRiQlaUaOVIwcaQr+lgHdvn2PbHaOy+sivzyfjeUb2bZvGxHWCJwRTpwRTizK0lpDrHRVcvGcT5hqHQrz57cdIC+Pnf0T+Mdtk4ixx7QWhB6/h03lm1r/2Ipqig5byPWJ7cPkfpNxeV18XfT1fjW/g2lfGI/JGMOl2Zfi9rlZvns5y3cvZ597HxZlIcNpatexjlhqPbXUuGuo8dTQK6YX43uPZ1zvcQxNGcrXRV8zb/M8lu9e3nqOdGc6o9NH0ye2DwEdwK/9BHSAAQkDGJ0+mjEZY+gX34/ShlI2V2xmc+VmdlTvaK2tlzaU4vV7sVqsWJUVu9VuWhVJJzEkeUhrckuLSTugdRHQgdbEA6aLpaSupDXJlTWUUd9U37pF26Pp5ezV+n/Q0hJIiU4hJiIGj89Do6+RRm8jNZ6a/Wr//eP7MyZjDIlRia3n8wf8FH3xPjW7tpBy0VUkx6TgsDrYWrWVBVvnM/+Dv7LUu52kRhhQDf3rLCQkpFM+bQqlTfvYW7+XdGc6l1T14qJ7XiLtvU/gnHPwB/xs/fQtSr78mNG3/ZH4tH77/8dWVZkxCI/HjMNNm2ZqNc8/D//zP6Z2lJBgrj7s2xduvNFsfdrdEXj9epg8mS25fVj/5H2cNvRcUmOab/Ll95tjzZ5t/qiGD29b1S4uzlTOzjzTXGt0uFrU2rXmYtVPPjHN8H//G0aOZE/dHvwBP5nxmbBxI5x3nlkm9U9/OvTxOiBJoQeprf2WjRsvo6lpL1lZf6Bv3zuwWNq6BLQ2XT/ffAMrV8LKb/2sXqWpbzS/jHE2F+PGw0mx+cRWvgj+T4mO3M04L2RPu4YBP52NtW8mXr+Xanc1lY2V7K7dTXFtMUW1RVS4KnBYHUTZo4iyRdHoa2RnzU52VO9gR/UOdlbv3K/GejA2iw2H1YGvsYEllhsZ+8Czra/V33A1k+LeIj95/wIMTC1wUOIgslOzGRA/gN6xvcmIzaBXTC/iI+Nbk0+1u5olu5awpGgJXxd9TaQtksmZkzkl8xQm9p1IYmRia6Hq8XvYtm+b6eOtKiTWEctlwy9jcNLg/c6ttabcVU5iZOJ+XTCdsaduD/nl+QxPHd7p7qSQKimBN980A5o//KFpEnZWfj787/+aAvDKK9v6/f1+M4Puv//b9FFOm2YGTXv3NhWZa681M+luv920IDMyzBSzg12m7fGY+cfDhsGcOWZ13fffN6+NHGm6VHv3Ns/LysyFkQUFppDfvt38PDbWFOCnnmoK1kmTTCv76adNgWyzmW7Y224zCzdNnGg+wzffmMRxMBUV5jMWFrYtRlVRYcY/wLSCJ0yAfv1MLH37mu+nqspcHr9lC7z3nklQ991nxgg76n8tLTWXux/lfGpJCj1MU1MFmzffSEXFPFyuc/B657B16zCWLDEzfcqq3OAswWGJZmR2NGPzIuk7YgNN2//Mlu1v8U2Gn21JBz92hA8cykad9eDrNcRGxNLkb2pt8isUvWN70z+hP/3j+zM0eSg5aTnkpOYwOGkwAR1orXkGdIDk6GRiI2Kp/PR9xn58ESotjZU/30RydDJaa6780xjeblrDgulvMSXvQsobyiltKMVusTMkeQiRNpmKGBReL3z8sanxfvxx28p5SpkC84c/NIWY09m2BOqgQabgBti1C+6/38yEA1PwDx9uauInn2wK/c8+M4li4kSzInBkJDz2GLzwghlE/stf4Be/OGh4B3jsMfjVr0yh6HTC3XebWQ1XX21imj/f1NLPPtvENm8e/OAH8N13JmmsWmVimTr1wMRTWAj/+If5LqqrzcUtWpur7ceMOfLvtqwMvvgCFi4059292yTe9nOw7XZzleS115oWR2Jix8frApIUegi321ym8NlnsGiRZt06H3V1bbXWzLzNxJ7xDDsS/4lLH3zwKiMqjVNcyYzKmkTO+AvISRtBcnQyWyq3UJD/JfkL38K7YR1JtV6S+g8l8byL6D3+LDJTBtEnrk9roRzQAdw+NzaLjQjrUVzm+/DDfDtnNqfeEsFZA8/iwys/ZM43c7jzkzt58FOY/dsFcO65Zt9PPjFzXWfMOPLzdBd+f9etcdGy3nVXWLvWFOSvvmoKr/R0MwPsxz82l5DPm2e2VasO/v7ERFNrX7/ePL/1VlOoffGFWQa9oKDt5ghPPmmOq5QpnGfNMjXviAh4+WUz7bSz6upMoho7Fu6911y/ALB6tWmFNDWZqW4VFSbJnXbakX83DQ3me3ntNdOlc/75R36Mjvj9prYfCJjvMDr6uC5eJUnhBFZebroV35xXw5crK2iqj8bij2FsXgRDxu8kqt9qauyvsMX7CWvrvdgsVi4edgnnDToPj9+Dy+vC5XWRlZDF5H6TyUrIOmDq3QGqq00XwBNPmBqNxWKavEOGmH7OxERTC4uPN7XHCRPaph0dTlOTGc2+4w7Ys4f/feMubvnoFmYMn8G7+e8yPWsqc2d9hHrkEbjrLnj3XVNY+P3wz3/C9dcf83d6XPn9pkvk5Zfhj380hebRXkI9f77p646ONgXvNdcceoaZ12u6JtqvRb1tm+ni2bTJ1DDWrTOF9o9+ZJLB+ecfvN+7pYujZRGpigpTo96yxfRXZmWZwrlfu/58nw9eecV0yzzwgJmp0J7PZ7qQ8vJMa6SrbN9uWgBlZeY7a5ndJlpJUjjBFBbCBx/Av+cFWLzrC3Tec6icuWirp8P3ZMX34ZxUN+emVJLT/ycMGvQodnvCsQXi8ZhA1q9vm6O6c6dJGk37zyJpTQ7Tp5tCvP0lxhUVJsm8+645ltdrfn7rregnn+SGeTfw4toXOSn5JL696VvihjRf3HDVVXDxxaZ/OibGNL/fe88UYJ3h9ZpZTNXV5j2TJ5sC8GiVlprCseWy7KYmUyi33INx2DCTKFu43eYzvPeeKRA3bjSf6/nnTYItLjZ94QsWmGMMGAD9+5ua98SJbbVfj8ckgccfN8dxOEzNvV8/+OUvzffTsqSq2226Rz76yLSw6joYjE9MhNxc0/q68sq2c/UUjY0meaWkhDqSbkmSQjenNSxf3chDcz9m6aadlDaUQOwe7AOX4XVuI9aewKy8a5jQZzyN3kYavA00ehvJjM9snYPtjHDi9zeyY8cDFBX9GYejN8OGvUhi4lnBCdrjMV06hYVmCYLly81sp507TYFz/fVm+Y033zS1ZLfbFIgnn2ya/GPGmD5ppWj0NvLHL//IdaOuY0jyEFOAL1tmCrTc3LYLd846y7Qy/vOfw6+kWlFhrvz+4guTCLxeM4B3zjmmG6dl/Wu321wdnp1tCvWWx9TmmSWVlfD66/Cvf5mR+0OJijLXjPz0p2ZNjunTTV/544+bAcsXXzQtJI/HnGf1avO+QYNM7XzHDvNai+HDzef85hvTzXPbbfDww6Yvfv58c13K118fPJY+fUytf9Qoc+yWrX9/c+60NFlrO4xJUuimdu2C51+r4umVf6csaw7ElANgJ5J0ZwbDe53EtSOv5ZLsS4iyd35xn9rab8nPv4bGxi1kZv6SrKw/YLEc6iqyLqK1KYSfftq0CrxeU4DNmgU//7kp5DrjvvtMV8vIkaZ10DKYWV5uCsmyMlNDdjpNV0p8vDn2sGEmAaxbBxdeaLq+nn/eFM7/+Y+plX/+ualpt6x/HRFhWkAFBaZ22SIpyXSJrFtnPsfo0aZGnZlpus5iY817W24y7HKZVtVrr5kaqtNpfv7yyzBzZttx9+wx12EUF5s+8QsvNHErZfqXy8rMbJUlS0xCWbLEfIfPP3/gfby1Nq2PoiLz3ZSb3x9+8APz3UmhLzogSaEb8Pq9FNUW8e2WHcxbtIuv1hVR5NoC2XMhwkVOxPn8fuqdnJU9lnhH/OH7/Q/D72+gsPBX7NnzNDExo8jOfhmnM7eLPk0nlJaaBHHWWUfehF+3znT7PPFEW429xc6dpra/ZcuB73M4zAyUggJTcP/736ZLqzMCAZOlCwratq1bTeF6/fWdv0iottYkhn//2wxOnnWMLbWWGSpyIwbRhSQphEhBRQF3LriTTWUFFNXuIsD+l7k7SefcgefxwHm/YkTaiKDEUFHxIZs3/xifr5p+/X5D//73HJ9WQ7A1NZkauctl+vnXrzfLY6xebVoP//hH21x1IcR+JCmEgMfnYfgT4ymqLsa/eSqByoGkOwYy7eQBXHV+f04b1ReH7fgUzk1NFRQW/oLS0leIjh7O0KHPEx/fhbM9hBAnFFkQL0gavY3cMf8Oou3R/PncP7cuJPbFF3Dty/dRlLmeuP/7kB+fdgHX3G3GVkPRzRsRkUJ29sukpV3Fd9/9lNWrT6F3758xcOAfsdniD38AIURYkqRwBErqSrjwjQv5ds+3AGyv3s7swW9wz68jWbh9IVz3GKfYb+E/iy8guvOLbAZVcvI0xo/fyPbt97F799+oqHiXwYOfIDX1smMewxBC9DzSfdRJq0tW86PXf0S1u5rXLn2NjcU7ueer22HH6SR+8SL6+lNJTYhm9S2riImIOe7xdUZt7Qq+++6n1NevIilpGkOGPElU1EGWLhZC9Did7T6SO9UeRn55Pr9b+DtO/eepWJSFxdcvYceC6Tx86W1Y3nsNS/8lNP54GPVqL69d9mq3TQgAcXHjGDPmGwYN+is1NV+yfHkO27ffj99/bEslCyF6Duk+OgiPz8MT3zzBq+tfZV3pOhSKaUOmcVu/57npR+msWmVmSD7xxJXstCcy852ZPHDaA4zrfdgkHHIWi43MzDtIS7ucwsJfsXPnA5SWvszAgQ+TmnpJ2818hBBhSbqPvqfaXc3Fb17Moh2LOCXzFK7IuYKp/S7jyQczeOopc+3TE0+YlQJauuR9AV/rgPOJZt++z9my5b9wufKJihpKv3530avXNT1jCqsQopVMST0Ku2p2Me3VaWyp3MK/LvoXV+Vexdat5gLU/Hz42c/MRbfxPWzyjtZ+ysvnsmvXw9TXryIiojdZWf9Devr10nIQooeQMYUjtGbvGiY9N4ndtbtZcM0Crsq9ik8/NRfH7t1rluL52996XkIAUMpKWtrljB27gpEjPyEycgCbN/+E1atPpa5uTajDE0IcR5IUgOLaYs568SxsFhtLfryEMwacyRNPmLvf9ekD33577CsXnAiUUiQlncPo0V8xbNi/aGzcysqVY/nuu1txub4LdXhCiOMg7JNCQAeY9d4smvxNfDbrM3LScnjiCbOw5fTpZkHKgQNDHeXxpZQiPf06JkzYTO/et7Bnz/+yfPlQVq2azJ49z+Hz1YY6RCFEkIR9Unjs68dYuGMhT0x9giHJQ/jwQ7Og5cUXw9y5ZmHMcGW3J3LSSU9x8slFDBz4MD5fFd99dxPLlmVRXPwEgUDT4Q8ihDihhPVA8+qS1Ux8biI/Gvoj3pnxDuvXKyZPhpNOgsWLzT1eRButNbW1y9ix47/Zt+8/REUNZuDAR0hJuUiujhaim5PZR4fh8roY+8xYaj21rLtlHd7aZCZONHcLXL7cjCWIg9NaU1U1n8LCX+FybcLh6E9y8jSSkqaRkHAWNpsz1CEKIb5HFsQ7jPsX3U9BRQGfXvspiZHJTDnX3Ljryy8lIRyOUork5GkkJp5DWdlrlJe/S2npK+zZ8zRKOcjM/AX9+t2DzRbGfW9CnKCCOqaglJqqlNqslNqqlJp9kNevV0qVK6XWNG83BjOeFvsa9/H3b//OtSOv5eyBZ/PSS+ZmV3//u1nVVHSOxWIjPX0Wubn/ZvLkSkaN+oy0tBns2vUQy5cPZe/el9E6EOowhRBHIGhJQSllBZ4CpgHDgSuVUge7N+ObWuu85u25YMXT3j9W/IMGbwN3nXIX9fXwm9/ApEnmDpLi6FgsESQmnkV29suMHr0Uh6MvBQWzWLlyHMXFT+Dx7Al1iEKITghmS2ECsFVrvU1r3QS8AVwYxPN1itvnZs43c5g6eCq5vXJ58EFzcdrjj8vtbbtKfPwkxoxZxtCh/wQ0W7fewdKlfVm9+gxKSv6F3+8OdYhCiA4EMyn0AYraPS9u/tn3XaqUWqeUekcplXmwAymlblZKrVBKrShvuVH5UXpl3SuUNpRy1yl3sWMHPPYYXHMNTJx4TIcV36OUhYyM6xk3bjXjx+czYMB/09RUwubNN7BsWT+2b/8dHk9JqMMUQnxP0GYfKaUuA6ZqrW9sfn4tMFFr/V/t9kkG6rXWHqXUT4GZWutDXjt8LLOPAjrA8KeGExMRw4qbVjBzpuKjj2DzZujb96gOKY6A1prq6s8pLn6cysqPUMpKTMxInM5ROJ2jiI2dQFzcJJneKkQQdIfZR7uB9jX/vs0/a6W1rmz39DngkSDGw4fffcjmys28funrfPWV4u234YEHJCEcL0opEhPPJjHxbFyurezd+wJ1dd9SWfkhe/f+EwCncyz9+/+GlJSLZTE+IUIgmEnhW2CIUioLkwyuAK5qv4NSKkNr3dKHMB3ID2I8PLLkEQYkDOCy4Zdx7dVmGexf/SqYZxQdiY4ezMCBfwJMC6KpqZTKyg8oKnqEjRsvIypqKH373k5KysU4HBkhjlaI8BG0qpjW2gf8F7AAU9i/pbXeqJT6vVJqevNutyulNiql1gK3A9cHK56lRUtZUrSEX0z6BTaLjdWr4ZRT6Db3Ug5nSikcjnR6976JCRMKGD78DazWKLZsuZWlS3uzcuUkdu58iMbGwlCHKkSPFzZXNC8tWsrvF/+et2e8jcXnJDYWfvtbuP/+ro9RHDutNQ0NG6msnEdFxb+pqzP/53Fxk+jV6xpSUy8nIiI1xFEKceKQZS4OYflyM9to7ly45JIuCkwEldtdRFnZ65SWvkpDwzrASmLi2aSlXU5KykXY7cmhDlGIbq07DDR3W+vWmcdRo0Ibh+i8yMhM+vX7Nf36/Zr6+nWUlr5GefnbbN58I5s3/5SEhCnEx08mLu4U4uImYbcnhjpkIU5IYZkU1q4FpxOyskIdiTgaTudInM6RDBz4IPX1qykvf5uqqgXs3Pkg4AcgJmYECQlnk5j4AxISpmCzxYU2aCFOEGHZfTRlCvj9Zr0j0XP4fPXU1X1Lbe3X7Nu3kJqar9DaA1iJj59McvL5JCWdT0zMCLkWQoQdGVPogNaQmAhXXWUWwBM9l9/vbk4Q/6Gqaj719eZ+0xER6cTE5BIdnU10dDZxceNxOkfLdRGiR5MxhQ7s2gU1NTByZKgjEcFmtUaSmHgWiYlnMXDgg3g8u6mqmk919Re4XPmUlDxPINAAQEREBsnJPyQ5+UckJJwu3U0ibIVdUli71jzKIHP4cTj6kJHxEzIyfgKA1gE8nmKqq7+gsvJ9ysreoKTkWUARE5NDXNwk4uJOITn5AiIi0kIbvBDHSVgmBaUgNzfUkYhQU8pCZGQ/0tOvJT39WgKBJmpqvqSmZgm1tcsoL59LSclzgIWEhDNITZ1BaurFRET0CnXoQgRNWCaFQYPM7CMh2jP3hDBrM4FpSTQ0rKe8/B3Kyt5my5b/x5Yt/695ZtOZJCScSXz8aUREpIQ4ciG6TlgmBek6Ep2hlKV1BdcBA35PQ8MGKis/orp6ISUlz7F795MAREUNJi7uZOLiJhIZORCHozcREX2w25NllpM44YRVUqivh8JCucOaOHJKKZzOXJzOXPr3n00g4KG2djm1tV9TW7uMqqpPKC19eb/3WCwxxMVNbL2ozunMIyIiTWY5iW4trJLChg1mSqrMPBLHymJxkJBwGgkJpwFmrSaPZzcezy48nj00Ne3G5dpCbe3X7Nz5R8Dcq1opGxERGTgcfYiJGUFs7HhiY8cTEzMCi8Uewk8khBFWSUFmHolgUUoRGdmXyMgDb85hLqr7BperoDlx7MbjKWo3kA1KRRAVNZjo6JOIihpCdPRQoqOHEx2djd2ecLw/jghjYZcU4uOhf/9QRyLCic3m3G8Au4XWGrd7G7W131JfvwqX6ztcrs1UVn6Mua25ERGRQUREOhZLNFZrNFZrLE7nqNZxDLmmQnSlsEsKI0eaKalChJpSiqioQURFDaJXrytaf661H7d7Bw0N+bhcm3C58vF6K/H7Xfj9dbjdu6ioeA/QgCI6ejgxMSOIiRlOTEwOUVFDcDj6YrMlykC3OGJhkxQCAVi/Hq67LtSRCHFoSllbkwX88KD7+Hw1zQPdS6mr+5a6uuWUl7+53z4WSyQORybx8aeSknIRiYnnYLVGHYdPIE5kYZMUduyAujoZTxA9g80WT1LSOSQlndP6M7+/gYaGfNzu7Xg8xXg8u3G7t1Fe/i579/4TiyWahIQziYwcQERELyIiemGxOGhqKsfrrcDrrSAiIhWnczRO52iiogbJTKkwFDZJoWWQWWYeiZ7Kao0hLm4ccXH7r3kWCDRRXb2Yiop/U139ObW1S/D5qvfbRyk7dnsyXm8F5k66Zkqtw5GB3Z6K3Z6Cw9EHp3MMsbHjOpwtFQh4aWjYiMdTTELCadhs8cH7wCIowiYpjBwJjz0GI0aEOhIhji+LJYKkpB+QlPSD1p8FAk00NZWhtQe7PRWrNRalFIGAh4aGjdTXr6a+fj1ebylNTeW43buoqfmSPXuebj5mJJGRWdhsic1bLI2NW6mvX9c6SK5UBElJ55GaejkJCWdgtTqxWqNQKkLGOrqxsFs6WwhxdLTWNDYWUle3grq6b3G7d+Dz7cPnq8bnqyEysj+xsWNxOscSEZFGZeWHlJe/jcdT/L0jWbDbU4mMzMThyGweFE/CZovHZkvAZovDYols3cy+WXIdxzGS+ykIIUJO6wC1td9QX7+WQKCRQKARv9+F11uK212Ex1OEx7Mbv7/mkMdRykZk5KDm6zeGNW/ZREVlAQqtfWjtw2qNk+s6OiD3UxBChJxSFuLjTyY+/uRD7qe1H5+vDp+vGr+/lkDA3bp5PHtobNyMy2W2qqr/Q2tvh8eKihrSeqW41RqF11uFz1eF17uvebzEj9Z+lLJisyVhtydjsyXhcGTgcPQnMrI/dntK2HZxSVIQQoScUlbs9oRO1fIDAR9u93ZcrgLc7p0oZUEpG0rZaGoqpa7uW6qrv6Cs7LXW91gsUdhsCc3jGRaUsqK1D6+3Cr+/9oBzWCyRKBUBBNA6gMUSSVzcJBISphAffzoxMTkoZQVM4vB4dtPY+B0u13c0Ne3G6cwjIeEMHI4+XfUVHTeSFIQQJxSLxUZ09BCio4cccj+PZy8QwGZLPOT1GYGAF5+vCo9nD273TjyenbjdRYAfsKCUBZ+vhpqar9i27ePDxqeUvbUlExU1uPlWry1FrcJiicJuT8JmS8RuTyIyMouoqJOIjMxsTjShJUlBCNEjORzpndrPYrG3XrcRGzv6kPs2NZVRXb0Yt3sH5opy05KIiEhvXrdqKHZ7IvX166iuXkR19SLq69c276vRWhMIuPB6q9Das9+xlXI0J4a2BGKY94EmI+Mm+vX7Vae/g6MhSUEIITopIiKNtLTLDrtfbOxoYmNHk5n5iw738fsb8Xorcbu34XJ9R2Pjd7jdu2hJIIZZyqRlczh6H/uHOAxJCkIIEeqrIm8AAAW4SURBVAJWaxRWq1lZNyFhSqjDaSXXsAshhGglSUEIIUQrSQpCCCFaSVIQQgjRSpKCEEKIVpIUhBBCtJKkIIQQopUkBSGEEK1OuKWzlVLlwM6jfHsKUNGF4fRk8l11jnxPnSPfU+cE83vqr7VOPdxOJ1xSOBZKqRWdWU9cyHfVWfI9dY58T53THb4n6T4SQgjRSpKCEEKIVuGWFJ4JdQAnEPmuOke+p86R76lzQv49hdWYghBCiEMLt5aCEEKIQwibpKCUmqqU2qyU2qqUmh3qeLoLpVSmUmqhUmqTUmqjUurnzT9PUkr9Rym1pfkxMdSxdgdKKatSarVS6sPm51lKqW+af6/eVObGvmFPKZWglHpHKVWglMpXSp0sv1MHUkr9ovnvboNS6nWlVGSof6fCIikoc+PTp4BpwHDgSqXU8NBG1W34gF9qrYcDk4Bbm7+b2cBnWushwGfNzwX8HMhv9/xh4K9a68HAPuAnIYmq+3kCmK+1HgaMwnxn8jvVjlKqD3A7ME5rPQKwAlcQ4t+psEgKwARgq9Z6m9a6CXgDuDDEMXULWusSrfWq5n/XYf54+2C+nxebd3sRuCg0EXYfSqm+wAXAc83PFXAW8E7zLvI9AUqpeGAK8DyA1rpJa12N/E4djA2IUubGzNFACSH+nQqXpNAHKGr3vLj5Z6IdpdQAYDTwDdBLa13S/NJeoFeIwupOHgd+DQSanycD1VprX/Nz+b0ysoBy4J/NXW3PKaVikN+p/WitdwOPArswyaAGWEmIf6fCJSmIw1BKOYG5wB1a69r/3979hFpRxmEc/z5mRqYggoFlJf5BItBbQYQmSLaIkHJhCWlI0K6NC0GMIgra1irKRYLRXZhxJZeRxSUXpZWmYDsLu0FeoQgMErGnxfue8XgMrlzwzsR5Pqsz78wZ3jm8c34z7zvze/vXuTyiNtSPqUnaBEza/q7tuvwPzAYeAt6z/SDwFwNdRWlTUMdUnqEE0buAO4AnW60UwxMUfgXu6VteUssCkHQrJSCM2h6rxeclLa7rFwOTbdWvI9YBT0v6mdL9+Dil33xBvfWHtKueCWDC9jd1+RNKkEibutYTwE+2L9i+DIxR2lmrbWpYgsJxYGUd1Z9DGcw53HKdOqH2i38A/Gj77b5Vh4Ed9fMO4NOZrluX2N5je4ntpZT284XtbcCXwJa62dD/TgC2fwN+kbSqFm0EzpA2Negc8KikufU87P1OrbapoXl5TdJTlD7hW4B9tt9quUqdIOkx4CvgNFf7yl+hjCt8DNxLyUr7nO3fW6lkx0jaAOyyvUnSMsqdw0LgBLDd9qU269cFkkYoA/JzgLPAi5SL0LSpPpLeALZSngI8AbxEGUNorU0NTVCIiIipDUv3UURE3IAEhYiIaCQoREREI0EhIiIaCQoREdFIUIiYQZI29DKsRnRRgkJERDQSFCL+g6Ttko5JOilpb51H4aKkd2r++yOSFtVtRyR9LemUpEO9eQIkrZD0uaQfJH0vaXnd/by+uQZG69usEZ2QoBAxQNL9lLdM19keAa4A2ygJy761/QAwDrxev/IhsNv2asqb4b3yUeBd22uAtZRMmFAy0e6kzO2xjJLvJqITZk+9ScTQ2Qg8DByvF/G3U5K3/QMcqNt8BIzVuQMW2B6v5fuBg5LmA3fbPgRg+2+Aur9jtifq8klgKXD05h9WxNQSFCKuJ2C/7T3XFEqvDWw33Rwx/XlsrpDzMDok3UcR1zsCbJF0JzTzVd9HOV962SufB47a/hP4Q9L6Wv4CMF5nsZuQtLnu4zZJc2f0KCKmIVcoEQNsn5H0KvCZpFnAZeBlymQxj9R1k5RxByjpjd+vf/q9jKBQAsReSW/WfTw7g4cRMS3JkhpxgyRdtD2v7XpE3EzpPoqIiEbuFCIiopE7hYiIaCQoREREI0EhIiIaCQoREdFIUIiIiEaCQkRENP4FcTH5BFwhW2AAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 381us/sample - loss: 0.8117 - acc: 0.7701\n",
      "Loss: 0.8116677548655097 Accuracy: 0.77009344\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.3715 - acc: 0.2211\n",
      "Epoch 00001: val_loss improved from inf to 1.83005, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_6_conv_checkpoint/001-1.8301.hdf5\n",
      "36805/36805 [==============================] - 28s 772us/sample - loss: 2.3715 - acc: 0.2212 - val_loss: 1.8301 - val_acc: 0.4156\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.7145 - acc: 0.4390\n",
      "Epoch 00002: val_loss improved from 1.83005 to 1.46157, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_6_conv_checkpoint/002-1.4616.hdf5\n",
      "36805/36805 [==============================] - 26s 717us/sample - loss: 1.7145 - acc: 0.4390 - val_loss: 1.4616 - val_acc: 0.5418\n",
      "Epoch 3/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.5224 - acc: 0.5003\n",
      "Epoch 00003: val_loss improved from 1.46157 to 1.39012, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_6_conv_checkpoint/003-1.3901.hdf5\n",
      "36805/36805 [==============================] - 26s 714us/sample - loss: 1.5219 - acc: 0.5006 - val_loss: 1.3901 - val_acc: 0.5677\n",
      "Epoch 4/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.4151 - acc: 0.5390\n",
      "Epoch 00004: val_loss improved from 1.39012 to 1.25350, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_6_conv_checkpoint/004-1.2535.hdf5\n",
      "36805/36805 [==============================] - 26s 717us/sample - loss: 1.4146 - acc: 0.5391 - val_loss: 1.2535 - val_acc: 0.6196\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3298 - acc: 0.5714\n",
      "Epoch 00005: val_loss improved from 1.25350 to 1.18229, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_6_conv_checkpoint/005-1.1823.hdf5\n",
      "36805/36805 [==============================] - 26s 717us/sample - loss: 1.3298 - acc: 0.5714 - val_loss: 1.1823 - val_acc: 0.6382\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2517 - acc: 0.6036\n",
      "Epoch 00006: val_loss improved from 1.18229 to 1.09341, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_6_conv_checkpoint/006-1.0934.hdf5\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 1.2518 - acc: 0.6036 - val_loss: 1.0934 - val_acc: 0.6704\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1777 - acc: 0.6290\n",
      "Epoch 00007: val_loss improved from 1.09341 to 1.06524, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_6_conv_checkpoint/007-1.0652.hdf5\n",
      "36805/36805 [==============================] - 26s 710us/sample - loss: 1.1778 - acc: 0.6290 - val_loss: 1.0652 - val_acc: 0.6765\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1130 - acc: 0.6538\n",
      "Epoch 00008: val_loss improved from 1.06524 to 0.96830, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_6_conv_checkpoint/008-0.9683.hdf5\n",
      "36805/36805 [==============================] - 26s 717us/sample - loss: 1.1130 - acc: 0.6539 - val_loss: 0.9683 - val_acc: 0.7109\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0511 - acc: 0.6754\n",
      "Epoch 00009: val_loss improved from 0.96830 to 0.93019, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_6_conv_checkpoint/009-0.9302.hdf5\n",
      "36805/36805 [==============================] - 26s 714us/sample - loss: 1.0512 - acc: 0.6753 - val_loss: 0.9302 - val_acc: 0.7335\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0002 - acc: 0.6931\n",
      "Epoch 00010: val_loss improved from 0.93019 to 0.88234, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_6_conv_checkpoint/010-0.8823.hdf5\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 1.0003 - acc: 0.6931 - val_loss: 0.8823 - val_acc: 0.7398\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9604 - acc: 0.7058\n",
      "Epoch 00011: val_loss improved from 0.88234 to 0.84768, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_6_conv_checkpoint/011-0.8477.hdf5\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.9604 - acc: 0.7057 - val_loss: 0.8477 - val_acc: 0.7498\n",
      "Epoch 12/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9172 - acc: 0.7202\n",
      "Epoch 00012: val_loss improved from 0.84768 to 0.80638, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_6_conv_checkpoint/012-0.8064.hdf5\n",
      "36805/36805 [==============================] - 26s 715us/sample - loss: 0.9169 - acc: 0.7203 - val_loss: 0.8064 - val_acc: 0.7724\n",
      "Epoch 13/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8732 - acc: 0.7344\n",
      "Epoch 00013: val_loss did not improve from 0.80638\n",
      "36805/36805 [==============================] - 26s 710us/sample - loss: 0.8730 - acc: 0.7342 - val_loss: 0.8284 - val_acc: 0.7449\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8323 - acc: 0.7471\n",
      "Epoch 00014: val_loss improved from 0.80638 to 0.73924, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_6_conv_checkpoint/014-0.7392.hdf5\n",
      "36805/36805 [==============================] - 26s 714us/sample - loss: 0.8325 - acc: 0.7470 - val_loss: 0.7392 - val_acc: 0.7873\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8000 - acc: 0.7598\n",
      "Epoch 00015: val_loss did not improve from 0.73924\n",
      "36805/36805 [==============================] - 26s 711us/sample - loss: 0.8001 - acc: 0.7598 - val_loss: 0.7582 - val_acc: 0.7824\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7727 - acc: 0.7673\n",
      "Epoch 00016: val_loss improved from 0.73924 to 0.70294, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_6_conv_checkpoint/016-0.7029.hdf5\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.7728 - acc: 0.7673 - val_loss: 0.7029 - val_acc: 0.8011\n",
      "Epoch 17/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7452 - acc: 0.7773\n",
      "Epoch 00017: val_loss improved from 0.70294 to 0.67935, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_6_conv_checkpoint/017-0.6793.hdf5\n",
      "36805/36805 [==============================] - 26s 718us/sample - loss: 0.7456 - acc: 0.7771 - val_loss: 0.6793 - val_acc: 0.8001\n",
      "Epoch 18/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7112 - acc: 0.7851\n",
      "Epoch 00018: val_loss improved from 0.67935 to 0.63969, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_6_conv_checkpoint/018-0.6397.hdf5\n",
      "36805/36805 [==============================] - 27s 720us/sample - loss: 0.7110 - acc: 0.7851 - val_loss: 0.6397 - val_acc: 0.8190\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6853 - acc: 0.7929\n",
      "Epoch 00019: val_loss did not improve from 0.63969\n",
      "36805/36805 [==============================] - 26s 710us/sample - loss: 0.6853 - acc: 0.7929 - val_loss: 0.6580 - val_acc: 0.8095\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6646 - acc: 0.8008\n",
      "Epoch 00020: val_loss improved from 0.63969 to 0.60614, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_6_conv_checkpoint/020-0.6061.hdf5\n",
      "36805/36805 [==============================] - 26s 716us/sample - loss: 0.6645 - acc: 0.8009 - val_loss: 0.6061 - val_acc: 0.8260\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6371 - acc: 0.8087\n",
      "Epoch 00021: val_loss did not improve from 0.60614\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.6371 - acc: 0.8087 - val_loss: 0.6509 - val_acc: 0.8039\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6212 - acc: 0.8121\n",
      "Epoch 00022: val_loss improved from 0.60614 to 0.57179, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_6_conv_checkpoint/022-0.5718.hdf5\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.6212 - acc: 0.8121 - val_loss: 0.5718 - val_acc: 0.8369\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6000 - acc: 0.8203\n",
      "Epoch 00023: val_loss did not improve from 0.57179\n",
      "36805/36805 [==============================] - 26s 711us/sample - loss: 0.6001 - acc: 0.8203 - val_loss: 0.5788 - val_acc: 0.8332\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5833 - acc: 0.8221\n",
      "Epoch 00024: val_loss improved from 0.57179 to 0.54720, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_6_conv_checkpoint/024-0.5472.hdf5\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.5834 - acc: 0.8220 - val_loss: 0.5472 - val_acc: 0.8421\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5685 - acc: 0.8281\n",
      "Epoch 00025: val_loss improved from 0.54720 to 0.54035, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_6_conv_checkpoint/025-0.5403.hdf5\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.5684 - acc: 0.8281 - val_loss: 0.5403 - val_acc: 0.8423\n",
      "Epoch 26/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5467 - acc: 0.8335\n",
      "Epoch 00026: val_loss improved from 0.54035 to 0.53102, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_6_conv_checkpoint/026-0.5310.hdf5\n",
      "36805/36805 [==============================] - 26s 717us/sample - loss: 0.5466 - acc: 0.8334 - val_loss: 0.5310 - val_acc: 0.8495\n",
      "Epoch 27/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5316 - acc: 0.8407\n",
      "Epoch 00027: val_loss did not improve from 0.53102\n",
      "36805/36805 [==============================] - 26s 711us/sample - loss: 0.5322 - acc: 0.8406 - val_loss: 0.5575 - val_acc: 0.8437\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5231 - acc: 0.8412\n",
      "Epoch 00028: val_loss improved from 0.53102 to 0.52729, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_6_conv_checkpoint/028-0.5273.hdf5\n",
      "36805/36805 [==============================] - 26s 714us/sample - loss: 0.5231 - acc: 0.8412 - val_loss: 0.5273 - val_acc: 0.8521\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5111 - acc: 0.8462\n",
      "Epoch 00029: val_loss improved from 0.52729 to 0.50086, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_6_conv_checkpoint/029-0.5009.hdf5\n",
      "36805/36805 [==============================] - 26s 710us/sample - loss: 0.5111 - acc: 0.8462 - val_loss: 0.5009 - val_acc: 0.8584\n",
      "Epoch 30/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4920 - acc: 0.8514\n",
      "Epoch 00030: val_loss did not improve from 0.50086\n",
      "36805/36805 [==============================] - 26s 715us/sample - loss: 0.4920 - acc: 0.8514 - val_loss: 0.5034 - val_acc: 0.8577\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4836 - acc: 0.8529\n",
      "Epoch 00031: val_loss did not improve from 0.50086\n",
      "36805/36805 [==============================] - 26s 709us/sample - loss: 0.4837 - acc: 0.8528 - val_loss: 0.5042 - val_acc: 0.8593\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4731 - acc: 0.8561\n",
      "Epoch 00032: val_loss improved from 0.50086 to 0.47175, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_6_conv_checkpoint/032-0.4717.hdf5\n",
      "36805/36805 [==============================] - 26s 714us/sample - loss: 0.4731 - acc: 0.8561 - val_loss: 0.4717 - val_acc: 0.8719\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4599 - acc: 0.8602\n",
      "Epoch 00033: val_loss improved from 0.47175 to 0.46434, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_6_conv_checkpoint/033-0.4643.hdf5\n",
      "36805/36805 [==============================] - 26s 714us/sample - loss: 0.4599 - acc: 0.8602 - val_loss: 0.4643 - val_acc: 0.8724\n",
      "Epoch 34/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4488 - acc: 0.8634\n",
      "Epoch 00034: val_loss improved from 0.46434 to 0.45993, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_6_conv_checkpoint/034-0.4599.hdf5\n",
      "36805/36805 [==============================] - 26s 714us/sample - loss: 0.4495 - acc: 0.8634 - val_loss: 0.4599 - val_acc: 0.8737\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4424 - acc: 0.8643\n",
      "Epoch 00035: val_loss did not improve from 0.45993\n",
      "36805/36805 [==============================] - 26s 710us/sample - loss: 0.4424 - acc: 0.8643 - val_loss: 0.4876 - val_acc: 0.8649\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4357 - acc: 0.8652\n",
      "Epoch 00036: val_loss improved from 0.45993 to 0.44928, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_6_conv_checkpoint/036-0.4493.hdf5\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.4357 - acc: 0.8653 - val_loss: 0.4493 - val_acc: 0.8728\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4213 - acc: 0.8721\n",
      "Epoch 00037: val_loss improved from 0.44928 to 0.43764, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_6_conv_checkpoint/037-0.4376.hdf5\n",
      "36805/36805 [==============================] - 26s 711us/sample - loss: 0.4214 - acc: 0.8720 - val_loss: 0.4376 - val_acc: 0.8793\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4137 - acc: 0.8758\n",
      "Epoch 00038: val_loss did not improve from 0.43764\n",
      "36805/36805 [==============================] - 26s 711us/sample - loss: 0.4138 - acc: 0.8758 - val_loss: 0.4485 - val_acc: 0.8740\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4106 - acc: 0.8749\n",
      "Epoch 00039: val_loss improved from 0.43764 to 0.42607, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_6_conv_checkpoint/039-0.4261.hdf5\n",
      "36805/36805 [==============================] - 26s 711us/sample - loss: 0.4106 - acc: 0.8749 - val_loss: 0.4261 - val_acc: 0.8859\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4046 - acc: 0.8748\n",
      "Epoch 00040: val_loss did not improve from 0.42607\n",
      "36805/36805 [==============================] - 26s 715us/sample - loss: 0.4046 - acc: 0.8748 - val_loss: 0.4409 - val_acc: 0.8803\n",
      "Epoch 41/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3931 - acc: 0.8792\n",
      "Epoch 00041: val_loss improved from 0.42607 to 0.42444, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_6_conv_checkpoint/041-0.4244.hdf5\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.3929 - acc: 0.8792 - val_loss: 0.4244 - val_acc: 0.8842\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3861 - acc: 0.8829\n",
      "Epoch 00042: val_loss did not improve from 0.42444\n",
      "36805/36805 [==============================] - 26s 710us/sample - loss: 0.3861 - acc: 0.8829 - val_loss: 0.4268 - val_acc: 0.8868\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3811 - acc: 0.8835\n",
      "Epoch 00043: val_loss improved from 0.42444 to 0.40987, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_6_conv_checkpoint/043-0.4099.hdf5\n",
      "36805/36805 [==============================] - 26s 707us/sample - loss: 0.3811 - acc: 0.8835 - val_loss: 0.4099 - val_acc: 0.8894\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3677 - acc: 0.8877\n",
      "Epoch 00044: val_loss did not improve from 0.40987\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.3676 - acc: 0.8878 - val_loss: 0.4115 - val_acc: 0.8903\n",
      "Epoch 45/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3671 - acc: 0.8877\n",
      "Epoch 00045: val_loss improved from 0.40987 to 0.39740, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_6_conv_checkpoint/045-0.3974.hdf5\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.3669 - acc: 0.8878 - val_loss: 0.3974 - val_acc: 0.8931\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3579 - acc: 0.8888\n",
      "Epoch 00046: val_loss did not improve from 0.39740\n",
      "36805/36805 [==============================] - 26s 707us/sample - loss: 0.3579 - acc: 0.8888 - val_loss: 0.4037 - val_acc: 0.8940\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3556 - acc: 0.8898\n",
      "Epoch 00047: val_loss did not improve from 0.39740\n",
      "36805/36805 [==============================] - 26s 714us/sample - loss: 0.3555 - acc: 0.8898 - val_loss: 0.4138 - val_acc: 0.8833\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3452 - acc: 0.8942\n",
      "Epoch 00048: val_loss did not improve from 0.39740\n",
      "36805/36805 [==============================] - 26s 708us/sample - loss: 0.3452 - acc: 0.8942 - val_loss: 0.4076 - val_acc: 0.8898\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3410 - acc: 0.8950\n",
      "Epoch 00049: val_loss did not improve from 0.39740\n",
      "36805/36805 [==============================] - 26s 709us/sample - loss: 0.3409 - acc: 0.8950 - val_loss: 0.3997 - val_acc: 0.8861\n",
      "Epoch 50/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3326 - acc: 0.8965\n",
      "Epoch 00050: val_loss did not improve from 0.39740\n",
      "36805/36805 [==============================] - 26s 714us/sample - loss: 0.3324 - acc: 0.8966 - val_loss: 0.4155 - val_acc: 0.8896\n",
      "Epoch 51/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3266 - acc: 0.8985\n",
      "Epoch 00051: val_loss did not improve from 0.39740\n",
      "36805/36805 [==============================] - 26s 710us/sample - loss: 0.3265 - acc: 0.8985 - val_loss: 0.3998 - val_acc: 0.8924\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3236 - acc: 0.8984\n",
      "Epoch 00052: val_loss improved from 0.39740 to 0.38407, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_6_conv_checkpoint/052-0.3841.hdf5\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.3236 - acc: 0.8984 - val_loss: 0.3841 - val_acc: 0.8910\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3195 - acc: 0.9006\n",
      "Epoch 00053: val_loss did not improve from 0.38407\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.3197 - acc: 0.9006 - val_loss: 0.4272 - val_acc: 0.8887\n",
      "Epoch 54/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3272 - acc: 0.8970\n",
      "Epoch 00054: val_loss did not improve from 0.38407\n",
      "36805/36805 [==============================] - 26s 714us/sample - loss: 0.3269 - acc: 0.8971 - val_loss: 0.3871 - val_acc: 0.8980\n",
      "Epoch 55/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3129 - acc: 0.9022\n",
      "Epoch 00055: val_loss did not improve from 0.38407\n",
      "36805/36805 [==============================] - 26s 710us/sample - loss: 0.3128 - acc: 0.9022 - val_loss: 0.3999 - val_acc: 0.8940\n",
      "Epoch 56/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3013 - acc: 0.9042\n",
      "Epoch 00056: val_loss did not improve from 0.38407\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.3012 - acc: 0.9042 - val_loss: 0.3886 - val_acc: 0.9001\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3028 - acc: 0.9050\n",
      "Epoch 00057: val_loss did not improve from 0.38407\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.3028 - acc: 0.9050 - val_loss: 0.3882 - val_acc: 0.8975\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2989 - acc: 0.9059\n",
      "Epoch 00058: val_loss improved from 0.38407 to 0.38149, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_6_conv_checkpoint/058-0.3815.hdf5\n",
      "36805/36805 [==============================] - 26s 716us/sample - loss: 0.2990 - acc: 0.9059 - val_loss: 0.3815 - val_acc: 0.8963\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3015 - acc: 0.9049\n",
      "Epoch 00059: val_loss did not improve from 0.38149\n",
      "36805/36805 [==============================] - 26s 711us/sample - loss: 0.3014 - acc: 0.9049 - val_loss: 0.3872 - val_acc: 0.8959\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2919 - acc: 0.9084\n",
      "Epoch 00060: val_loss did not improve from 0.38149\n",
      "36805/36805 [==============================] - 26s 711us/sample - loss: 0.2918 - acc: 0.9084 - val_loss: 0.3933 - val_acc: 0.8921\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2851 - acc: 0.9099\n",
      "Epoch 00061: val_loss did not improve from 0.38149\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.2850 - acc: 0.9099 - val_loss: 0.4134 - val_acc: 0.8931\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2777 - acc: 0.9142\n",
      "Epoch 00062: val_loss did not improve from 0.38149\n",
      "36805/36805 [==============================] - 26s 711us/sample - loss: 0.2777 - acc: 0.9142 - val_loss: 0.4122 - val_acc: 0.8938\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2810 - acc: 0.9103\n",
      "Epoch 00063: val_loss improved from 0.38149 to 0.37496, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_6_conv_checkpoint/063-0.3750.hdf5\n",
      "36805/36805 [==============================] - 26s 710us/sample - loss: 0.2809 - acc: 0.9103 - val_loss: 0.3750 - val_acc: 0.9031\n",
      "Epoch 64/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2749 - acc: 0.9139\n",
      "Epoch 00064: val_loss did not improve from 0.37496\n",
      "36805/36805 [==============================] - 26s 716us/sample - loss: 0.2750 - acc: 0.9139 - val_loss: 0.4067 - val_acc: 0.8908\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2743 - acc: 0.9125\n",
      "Epoch 00065: val_loss did not improve from 0.37496\n",
      "36805/36805 [==============================] - 26s 711us/sample - loss: 0.2742 - acc: 0.9125 - val_loss: 0.3787 - val_acc: 0.9031\n",
      "Epoch 66/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2679 - acc: 0.9152\n",
      "Epoch 00066: val_loss did not improve from 0.37496\n",
      "36805/36805 [==============================] - 26s 709us/sample - loss: 0.2681 - acc: 0.9151 - val_loss: 0.4038 - val_acc: 0.8952\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2649 - acc: 0.9150\n",
      "Epoch 00067: val_loss did not improve from 0.37496\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.2648 - acc: 0.9150 - val_loss: 0.3842 - val_acc: 0.8994\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2597 - acc: 0.9182\n",
      "Epoch 00068: val_loss did not improve from 0.37496\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.2597 - acc: 0.9182 - val_loss: 0.3825 - val_acc: 0.9026\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2639 - acc: 0.9146\n",
      "Epoch 00069: val_loss did not improve from 0.37496\n",
      "36805/36805 [==============================] - 26s 710us/sample - loss: 0.2638 - acc: 0.9147 - val_loss: 0.3924 - val_acc: 0.9015\n",
      "Epoch 70/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2569 - acc: 0.9179\n",
      "Epoch 00070: val_loss did not improve from 0.37496\n",
      "36805/36805 [==============================] - 26s 714us/sample - loss: 0.2569 - acc: 0.9178 - val_loss: 0.3914 - val_acc: 0.9022\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2489 - acc: 0.9204\n",
      "Epoch 00071: val_loss improved from 0.37496 to 0.36937, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_6_conv_checkpoint/071-0.3694.hdf5\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.2488 - acc: 0.9204 - val_loss: 0.3694 - val_acc: 0.9078\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2459 - acc: 0.9202\n",
      "Epoch 00072: val_loss did not improve from 0.36937\n",
      "36805/36805 [==============================] - 26s 711us/sample - loss: 0.2459 - acc: 0.9202 - val_loss: 0.3717 - val_acc: 0.9066\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2444 - acc: 0.9208\n",
      "Epoch 00073: val_loss did not improve from 0.36937\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.2444 - acc: 0.9208 - val_loss: 0.3960 - val_acc: 0.8966\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2409 - acc: 0.9229\n",
      "Epoch 00074: val_loss did not improve from 0.36937\n",
      "36805/36805 [==============================] - 26s 711us/sample - loss: 0.2409 - acc: 0.9229 - val_loss: 0.3916 - val_acc: 0.9015\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2454 - acc: 0.9206\n",
      "Epoch 00075: val_loss improved from 0.36937 to 0.36610, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_6_conv_checkpoint/075-0.3661.hdf5\n",
      "36805/36805 [==============================] - 26s 715us/sample - loss: 0.2454 - acc: 0.9206 - val_loss: 0.3661 - val_acc: 0.9045\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2408 - acc: 0.9237\n",
      "Epoch 00076: val_loss did not improve from 0.36610\n",
      "36805/36805 [==============================] - 26s 715us/sample - loss: 0.2408 - acc: 0.9238 - val_loss: 0.3986 - val_acc: 0.9022\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2353 - acc: 0.9246\n",
      "Epoch 00077: val_loss did not improve from 0.36610\n",
      "36805/36805 [==============================] - 26s 710us/sample - loss: 0.2353 - acc: 0.9246 - val_loss: 0.3743 - val_acc: 0.9010\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2311 - acc: 0.9249\n",
      "Epoch 00078: val_loss did not improve from 0.36610\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.2311 - acc: 0.9249 - val_loss: 0.3773 - val_acc: 0.8996\n",
      "Epoch 79/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2315 - acc: 0.9245\n",
      "Epoch 00079: val_loss did not improve from 0.36610\n",
      "36805/36805 [==============================] - 26s 714us/sample - loss: 0.2316 - acc: 0.9245 - val_loss: 0.3743 - val_acc: 0.9052\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2296 - acc: 0.9262\n",
      "Epoch 00080: val_loss did not improve from 0.36610\n",
      "36805/36805 [==============================] - 26s 710us/sample - loss: 0.2296 - acc: 0.9263 - val_loss: 0.3710 - val_acc: 0.9073\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2283 - acc: 0.9258\n",
      "Epoch 00081: val_loss did not improve from 0.36610\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.2284 - acc: 0.9258 - val_loss: 0.3796 - val_acc: 0.9057\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2292 - acc: 0.9257\n",
      "Epoch 00082: val_loss did not improve from 0.36610\n",
      "36805/36805 [==============================] - 26s 710us/sample - loss: 0.2292 - acc: 0.9257 - val_loss: 0.3718 - val_acc: 0.9082\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2208 - acc: 0.9285\n",
      "Epoch 00083: val_loss did not improve from 0.36610\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.2209 - acc: 0.9284 - val_loss: 0.3715 - val_acc: 0.9087\n",
      "Epoch 84/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2185 - acc: 0.9287\n",
      "Epoch 00084: val_loss improved from 0.36610 to 0.36554, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_6_conv_checkpoint/084-0.3655.hdf5\n",
      "36805/36805 [==============================] - 26s 709us/sample - loss: 0.2183 - acc: 0.9287 - val_loss: 0.3655 - val_acc: 0.9078\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2135 - acc: 0.9304\n",
      "Epoch 00085: val_loss did not improve from 0.36554\n",
      "36805/36805 [==============================] - 26s 707us/sample - loss: 0.2135 - acc: 0.9304 - val_loss: 0.3923 - val_acc: 0.8984\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2186 - acc: 0.9287\n",
      "Epoch 00086: val_loss did not improve from 0.36554\n",
      "36805/36805 [==============================] - 26s 709us/sample - loss: 0.2186 - acc: 0.9287 - val_loss: 0.4001 - val_acc: 0.9045\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2085 - acc: 0.9329\n",
      "Epoch 00087: val_loss did not improve from 0.36554\n",
      "36805/36805 [==============================] - 26s 707us/sample - loss: 0.2085 - acc: 0.9329 - val_loss: 0.3675 - val_acc: 0.9092\n",
      "Epoch 88/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2140 - acc: 0.9306\n",
      "Epoch 00088: val_loss did not improve from 0.36554\n",
      "36805/36805 [==============================] - 26s 706us/sample - loss: 0.2140 - acc: 0.9306 - val_loss: 0.3785 - val_acc: 0.9092\n",
      "Epoch 89/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2063 - acc: 0.9340\n",
      "Epoch 00089: val_loss did not improve from 0.36554\n",
      "36805/36805 [==============================] - 26s 708us/sample - loss: 0.2063 - acc: 0.9339 - val_loss: 0.3739 - val_acc: 0.9106\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2097 - acc: 0.9314\n",
      "Epoch 00090: val_loss did not improve from 0.36554\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.2096 - acc: 0.9314 - val_loss: 0.3758 - val_acc: 0.9082\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2054 - acc: 0.9315\n",
      "Epoch 00091: val_loss did not improve from 0.36554\n",
      "36805/36805 [==============================] - 26s 707us/sample - loss: 0.2054 - acc: 0.9315 - val_loss: 0.3714 - val_acc: 0.9094\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1998 - acc: 0.9345\n",
      "Epoch 00092: val_loss did not improve from 0.36554\n",
      "36805/36805 [==============================] - 26s 707us/sample - loss: 0.1998 - acc: 0.9345 - val_loss: 0.3854 - val_acc: 0.9054\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1967 - acc: 0.9354\n",
      "Epoch 00093: val_loss did not improve from 0.36554\n",
      "36805/36805 [==============================] - 26s 710us/sample - loss: 0.1967 - acc: 0.9354 - val_loss: 0.3766 - val_acc: 0.9066\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2011 - acc: 0.9343\n",
      "Epoch 00094: val_loss improved from 0.36554 to 0.35274, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_6_conv_checkpoint/094-0.3527.hdf5\n",
      "36805/36805 [==============================] - 26s 720us/sample - loss: 0.2010 - acc: 0.9343 - val_loss: 0.3527 - val_acc: 0.9145\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1986 - acc: 0.9354\n",
      "Epoch 00095: val_loss did not improve from 0.35274\n",
      "36805/36805 [==============================] - 26s 711us/sample - loss: 0.1987 - acc: 0.9354 - val_loss: 0.3838 - val_acc: 0.9054\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1997 - acc: 0.9354\n",
      "Epoch 00096: val_loss did not improve from 0.35274\n",
      "36805/36805 [==============================] - 26s 718us/sample - loss: 0.1997 - acc: 0.9354 - val_loss: 0.3694 - val_acc: 0.9099\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1942 - acc: 0.9354\n",
      "Epoch 00097: val_loss did not improve from 0.35274\n",
      "36805/36805 [==============================] - 26s 714us/sample - loss: 0.1942 - acc: 0.9354 - val_loss: 0.3595 - val_acc: 0.9117\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1906 - acc: 0.9395\n",
      "Epoch 00098: val_loss did not improve from 0.35274\n",
      "36805/36805 [==============================] - 26s 714us/sample - loss: 0.1906 - acc: 0.9394 - val_loss: 0.3565 - val_acc: 0.9099\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1885 - acc: 0.9389\n",
      "Epoch 00099: val_loss did not improve from 0.35274\n",
      "36805/36805 [==============================] - 26s 708us/sample - loss: 0.1889 - acc: 0.9389 - val_loss: 0.3720 - val_acc: 0.9082\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1877 - acc: 0.9385\n",
      "Epoch 00100: val_loss did not improve from 0.35274\n",
      "36805/36805 [==============================] - 26s 709us/sample - loss: 0.1878 - acc: 0.9385 - val_loss: 0.3829 - val_acc: 0.9073\n",
      "Epoch 101/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1853 - acc: 0.9413\n",
      "Epoch 00101: val_loss did not improve from 0.35274\n",
      "36805/36805 [==============================] - 26s 716us/sample - loss: 0.1854 - acc: 0.9412 - val_loss: 0.3653 - val_acc: 0.9124\n",
      "Epoch 102/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1840 - acc: 0.9408\n",
      "Epoch 00102: val_loss did not improve from 0.35274\n",
      "36805/36805 [==============================] - 26s 709us/sample - loss: 0.1839 - acc: 0.9408 - val_loss: 0.3715 - val_acc: 0.9101\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1884 - acc: 0.9373\n",
      "Epoch 00103: val_loss did not improve from 0.35274\n",
      "36805/36805 [==============================] - 26s 708us/sample - loss: 0.1884 - acc: 0.9373 - val_loss: 0.3652 - val_acc: 0.9138\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1820 - acc: 0.9394\n",
      "Epoch 00104: val_loss did not improve from 0.35274\n",
      "36805/36805 [==============================] - 26s 706us/sample - loss: 0.1820 - acc: 0.9394 - val_loss: 0.3706 - val_acc: 0.9136\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1820 - acc: 0.9414\n",
      "Epoch 00105: val_loss did not improve from 0.35274\n",
      "36805/36805 [==============================] - 26s 708us/sample - loss: 0.1820 - acc: 0.9414 - val_loss: 0.3717 - val_acc: 0.9068\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1779 - acc: 0.9418\n",
      "Epoch 00106: val_loss did not improve from 0.35274\n",
      "36805/36805 [==============================] - 26s 710us/sample - loss: 0.1779 - acc: 0.9418 - val_loss: 0.3753 - val_acc: 0.9124\n",
      "Epoch 107/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1770 - acc: 0.9418\n",
      "Epoch 00107: val_loss did not improve from 0.35274\n",
      "36805/36805 [==============================] - 26s 710us/sample - loss: 0.1771 - acc: 0.9417 - val_loss: 0.3992 - val_acc: 0.9085\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1769 - acc: 0.9429\n",
      "Epoch 00108: val_loss did not improve from 0.35274\n",
      "36805/36805 [==============================] - 26s 711us/sample - loss: 0.1769 - acc: 0.9429 - val_loss: 0.3733 - val_acc: 0.9108\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1747 - acc: 0.9416\n",
      "Epoch 00109: val_loss did not improve from 0.35274\n",
      "36805/36805 [==============================] - 26s 708us/sample - loss: 0.1748 - acc: 0.9416 - val_loss: 0.3719 - val_acc: 0.9140\n",
      "Epoch 110/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1769 - acc: 0.9424\n",
      "Epoch 00110: val_loss did not improve from 0.35274\n",
      "36805/36805 [==============================] - 26s 710us/sample - loss: 0.1771 - acc: 0.9423 - val_loss: 0.3790 - val_acc: 0.9103\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1694 - acc: 0.9440\n",
      "Epoch 00111: val_loss did not improve from 0.35274\n",
      "36805/36805 [==============================] - 26s 711us/sample - loss: 0.1694 - acc: 0.9440 - val_loss: 0.3831 - val_acc: 0.9113\n",
      "Epoch 112/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1724 - acc: 0.9432\n",
      "Epoch 00112: val_loss did not improve from 0.35274\n",
      "36805/36805 [==============================] - 26s 710us/sample - loss: 0.1724 - acc: 0.9432 - val_loss: 0.3608 - val_acc: 0.9131\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1695 - acc: 0.9451\n",
      "Epoch 00113: val_loss did not improve from 0.35274\n",
      "36805/36805 [==============================] - 26s 711us/sample - loss: 0.1695 - acc: 0.9451 - val_loss: 0.3595 - val_acc: 0.9119\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1676 - acc: 0.9445\n",
      "Epoch 00114: val_loss did not improve from 0.35274\n",
      "36805/36805 [==============================] - 26s 708us/sample - loss: 0.1675 - acc: 0.9445 - val_loss: 0.3890 - val_acc: 0.9071\n",
      "Epoch 115/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1663 - acc: 0.9444\n",
      "Epoch 00115: val_loss did not improve from 0.35274\n",
      "36805/36805 [==============================] - 26s 714us/sample - loss: 0.1662 - acc: 0.9444 - val_loss: 0.3773 - val_acc: 0.9099\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1600 - acc: 0.9471\n",
      "Epoch 00116: val_loss did not improve from 0.35274\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.1601 - acc: 0.9471 - val_loss: 0.3756 - val_acc: 0.9150\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1636 - acc: 0.9462\n",
      "Epoch 00117: val_loss did not improve from 0.35274\n",
      "36805/36805 [==============================] - 26s 714us/sample - loss: 0.1635 - acc: 0.9462 - val_loss: 0.4253 - val_acc: 0.9050\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1674 - acc: 0.9449\n",
      "Epoch 00118: val_loss did not improve from 0.35274\n",
      "36805/36805 [==============================] - 26s 714us/sample - loss: 0.1673 - acc: 0.9449 - val_loss: 0.3647 - val_acc: 0.9126\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1595 - acc: 0.9470\n",
      "Epoch 00119: val_loss did not improve from 0.35274\n",
      "36805/36805 [==============================] - 26s 708us/sample - loss: 0.1595 - acc: 0.9470 - val_loss: 0.3763 - val_acc: 0.9161\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1653 - acc: 0.9460\n",
      "Epoch 00120: val_loss did not improve from 0.35274\n",
      "36805/36805 [==============================] - 26s 708us/sample - loss: 0.1653 - acc: 0.9460 - val_loss: 0.3624 - val_acc: 0.9178\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1604 - acc: 0.9474\n",
      "Epoch 00121: val_loss did not improve from 0.35274\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.1604 - acc: 0.9473 - val_loss: 0.3715 - val_acc: 0.9161\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1548 - acc: 0.9475\n",
      "Epoch 00122: val_loss did not improve from 0.35274\n",
      "36805/36805 [==============================] - 26s 710us/sample - loss: 0.1549 - acc: 0.9475 - val_loss: 0.3899 - val_acc: 0.9131\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1622 - acc: 0.9465\n",
      "Epoch 00123: val_loss did not improve from 0.35274\n",
      "36805/36805 [==============================] - 26s 708us/sample - loss: 0.1622 - acc: 0.9465 - val_loss: 0.3711 - val_acc: 0.9143\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1532 - acc: 0.9497\n",
      "Epoch 00124: val_loss did not improve from 0.35274\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.1532 - acc: 0.9497 - val_loss: 0.3898 - val_acc: 0.9161\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1594 - acc: 0.9481\n",
      "Epoch 00125: val_loss did not improve from 0.35274\n",
      "36805/36805 [==============================] - 26s 704us/sample - loss: 0.1594 - acc: 0.9481 - val_loss: 0.3552 - val_acc: 0.9152\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1527 - acc: 0.9501\n",
      "Epoch 00126: val_loss did not improve from 0.35274\n",
      "36805/36805 [==============================] - 26s 708us/sample - loss: 0.1527 - acc: 0.9501 - val_loss: 0.3711 - val_acc: 0.9113\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1528 - acc: 0.9499\n",
      "Epoch 00127: val_loss did not improve from 0.35274\n",
      "36805/36805 [==============================] - 26s 709us/sample - loss: 0.1528 - acc: 0.9499 - val_loss: 0.3965 - val_acc: 0.9119\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1534 - acc: 0.9491\n",
      "Epoch 00128: val_loss did not improve from 0.35274\n",
      "36805/36805 [==============================] - 26s 707us/sample - loss: 0.1534 - acc: 0.9491 - val_loss: 0.3704 - val_acc: 0.9115\n",
      "Epoch 129/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1486 - acc: 0.9508\n",
      "Epoch 00129: val_loss did not improve from 0.35274\n",
      "36805/36805 [==============================] - 26s 706us/sample - loss: 0.1486 - acc: 0.9508 - val_loss: 0.4026 - val_acc: 0.9059\n",
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1529 - acc: 0.9492\n",
      "Epoch 00130: val_loss did not improve from 0.35274\n",
      "36805/36805 [==============================] - 26s 705us/sample - loss: 0.1529 - acc: 0.9492 - val_loss: 0.3900 - val_acc: 0.9166\n",
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1471 - acc: 0.9515\n",
      "Epoch 00131: val_loss did not improve from 0.35274\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.1471 - acc: 0.9515 - val_loss: 0.3635 - val_acc: 0.9203\n",
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1509 - acc: 0.9503\n",
      "Epoch 00132: val_loss did not improve from 0.35274\n",
      "36805/36805 [==============================] - 26s 706us/sample - loss: 0.1509 - acc: 0.9503 - val_loss: 0.3713 - val_acc: 0.9147\n",
      "Epoch 133/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1469 - acc: 0.9509\n",
      "Epoch 00133: val_loss did not improve from 0.35274\n",
      "36805/36805 [==============================] - 26s 709us/sample - loss: 0.1469 - acc: 0.9509 - val_loss: 0.3858 - val_acc: 0.9099\n",
      "Epoch 134/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1451 - acc: 0.9508\n",
      "Epoch 00134: val_loss did not improve from 0.35274\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.1451 - acc: 0.9508 - val_loss: 0.3771 - val_acc: 0.9157\n",
      "Epoch 135/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1421 - acc: 0.9527\n",
      "Epoch 00135: val_loss did not improve from 0.35274\n",
      "36805/36805 [==============================] - 26s 713us/sample - loss: 0.1421 - acc: 0.9527 - val_loss: 0.3761 - val_acc: 0.9166\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 136/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1424 - acc: 0.9523\n",
      "Epoch 00136: val_loss did not improve from 0.35274\n",
      "36805/36805 [==============================] - 26s 711us/sample - loss: 0.1424 - acc: 0.9523 - val_loss: 0.3778 - val_acc: 0.9136\n",
      "Epoch 137/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1438 - acc: 0.9518\n",
      "Epoch 00137: val_loss did not improve from 0.35274\n",
      "36805/36805 [==============================] - 26s 708us/sample - loss: 0.1437 - acc: 0.9519 - val_loss: 0.3879 - val_acc: 0.9154\n",
      "Epoch 138/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1376 - acc: 0.9552\n",
      "Epoch 00138: val_loss did not improve from 0.35274\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.1376 - acc: 0.9552 - val_loss: 0.3775 - val_acc: 0.9122\n",
      "Epoch 139/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1452 - acc: 0.9509\n",
      "Epoch 00139: val_loss did not improve from 0.35274\n",
      "36805/36805 [==============================] - 26s 714us/sample - loss: 0.1451 - acc: 0.9510 - val_loss: 0.3770 - val_acc: 0.9150\n",
      "Epoch 140/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1416 - acc: 0.9524\n",
      "Epoch 00140: val_loss did not improve from 0.35274\n",
      "36805/36805 [==============================] - 26s 712us/sample - loss: 0.1416 - acc: 0.9525 - val_loss: 0.3616 - val_acc: 0.9173\n",
      "Epoch 141/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1397 - acc: 0.9535\n",
      "Epoch 00141: val_loss did not improve from 0.35274\n",
      "36805/36805 [==============================] - 26s 711us/sample - loss: 0.1397 - acc: 0.9535 - val_loss: 0.3840 - val_acc: 0.9143\n",
      "Epoch 142/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1343 - acc: 0.9552\n",
      "Epoch 00142: val_loss did not improve from 0.35274\n",
      "36805/36805 [==============================] - 26s 714us/sample - loss: 0.1343 - acc: 0.9553 - val_loss: 0.3901 - val_acc: 0.9115\n",
      "Epoch 143/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1372 - acc: 0.9533\n",
      "Epoch 00143: val_loss did not improve from 0.35274\n",
      "36805/36805 [==============================] - 26s 710us/sample - loss: 0.1372 - acc: 0.9533 - val_loss: 0.3937 - val_acc: 0.9192\n",
      "Epoch 144/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1357 - acc: 0.9545\n",
      "Epoch 00144: val_loss did not improve from 0.35274\n",
      "36805/36805 [==============================] - 26s 709us/sample - loss: 0.1357 - acc: 0.9545 - val_loss: 0.3834 - val_acc: 0.9150\n",
      "\n",
      "1D_CNN_custom_3_ch_32_DO_6_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 397us/sample - loss: 0.4294 - acc: 0.8856\n",
      "Loss: 0.42937957885233163 Accuracy: 0.88556594\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.4797 - acc: 0.1874\n",
      "Epoch 00001: val_loss improved from inf to 1.89325, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_7_conv_checkpoint/001-1.8932.hdf5\n",
      "36805/36805 [==============================] - 30s 811us/sample - loss: 2.4796 - acc: 0.1875 - val_loss: 1.8932 - val_acc: 0.4030\n",
      "Epoch 2/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.6840 - acc: 0.4456\n",
      "Epoch 00002: val_loss improved from 1.89325 to 1.31142, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_7_conv_checkpoint/002-1.3114.hdf5\n",
      "36805/36805 [==============================] - 27s 726us/sample - loss: 1.6834 - acc: 0.4459 - val_loss: 1.3114 - val_acc: 0.5763\n",
      "Epoch 3/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3902 - acc: 0.5418\n",
      "Epoch 00003: val_loss improved from 1.31142 to 1.13800, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_7_conv_checkpoint/003-1.1380.hdf5\n",
      "36805/36805 [==============================] - 27s 728us/sample - loss: 1.3903 - acc: 0.5418 - val_loss: 1.1380 - val_acc: 0.6541\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2295 - acc: 0.6040\n",
      "Epoch 00004: val_loss improved from 1.13800 to 1.03699, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_7_conv_checkpoint/004-1.0370.hdf5\n",
      "36805/36805 [==============================] - 27s 727us/sample - loss: 1.2297 - acc: 0.6040 - val_loss: 1.0370 - val_acc: 0.6792\n",
      "Epoch 5/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0966 - acc: 0.6491\n",
      "Epoch 00005: val_loss improved from 1.03699 to 0.89980, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_7_conv_checkpoint/005-0.8998.hdf5\n",
      "36805/36805 [==============================] - 27s 729us/sample - loss: 1.0967 - acc: 0.6491 - val_loss: 0.8998 - val_acc: 0.7275\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9831 - acc: 0.6884\n",
      "Epoch 00006: val_loss improved from 0.89980 to 0.79856, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_7_conv_checkpoint/006-0.7986.hdf5\n",
      "36805/36805 [==============================] - 26s 719us/sample - loss: 0.9831 - acc: 0.6884 - val_loss: 0.7986 - val_acc: 0.7615\n",
      "Epoch 7/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8942 - acc: 0.7183\n",
      "Epoch 00007: val_loss improved from 0.79856 to 0.70991, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_7_conv_checkpoint/007-0.7099.hdf5\n",
      "36805/36805 [==============================] - 27s 729us/sample - loss: 0.8942 - acc: 0.7184 - val_loss: 0.7099 - val_acc: 0.8020\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8066 - acc: 0.7490\n",
      "Epoch 00008: val_loss improved from 0.70991 to 0.66464, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_7_conv_checkpoint/008-0.6646.hdf5\n",
      "36805/36805 [==============================] - 27s 724us/sample - loss: 0.8066 - acc: 0.7491 - val_loss: 0.6646 - val_acc: 0.8041\n",
      "Epoch 9/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7436 - acc: 0.7692\n",
      "Epoch 00009: val_loss improved from 0.66464 to 0.64215, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_7_conv_checkpoint/009-0.6422.hdf5\n",
      "36805/36805 [==============================] - 27s 727us/sample - loss: 0.7434 - acc: 0.7693 - val_loss: 0.6422 - val_acc: 0.8164\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6919 - acc: 0.7834\n",
      "Epoch 00010: val_loss improved from 0.64215 to 0.53593, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_7_conv_checkpoint/010-0.5359.hdf5\n",
      "36805/36805 [==============================] - 27s 726us/sample - loss: 0.6920 - acc: 0.7834 - val_loss: 0.5359 - val_acc: 0.8491\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6437 - acc: 0.8016\n",
      "Epoch 00011: val_loss improved from 0.53593 to 0.52740, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_7_conv_checkpoint/011-0.5274.hdf5\n",
      "36805/36805 [==============================] - 27s 726us/sample - loss: 0.6436 - acc: 0.8017 - val_loss: 0.5274 - val_acc: 0.8535\n",
      "Epoch 12/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6064 - acc: 0.8129\n",
      "Epoch 00012: val_loss improved from 0.52740 to 0.49601, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_7_conv_checkpoint/012-0.4960.hdf5\n",
      "36805/36805 [==============================] - 27s 722us/sample - loss: 0.6065 - acc: 0.8129 - val_loss: 0.4960 - val_acc: 0.8637\n",
      "Epoch 13/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5718 - acc: 0.8240\n",
      "Epoch 00013: val_loss improved from 0.49601 to 0.45259, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_7_conv_checkpoint/013-0.4526.hdf5\n",
      "36805/36805 [==============================] - 27s 729us/sample - loss: 0.5712 - acc: 0.8242 - val_loss: 0.4526 - val_acc: 0.8765\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5404 - acc: 0.8325\n",
      "Epoch 00014: val_loss improved from 0.45259 to 0.44795, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_7_conv_checkpoint/014-0.4479.hdf5\n",
      "36805/36805 [==============================] - 27s 723us/sample - loss: 0.5404 - acc: 0.8325 - val_loss: 0.4479 - val_acc: 0.8684\n",
      "Epoch 15/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5045 - acc: 0.8432\n",
      "Epoch 00015: val_loss did not improve from 0.44795\n",
      "36805/36805 [==============================] - 27s 725us/sample - loss: 0.5045 - acc: 0.8433 - val_loss: 0.4555 - val_acc: 0.8686\n",
      "Epoch 16/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4828 - acc: 0.8503\n",
      "Epoch 00016: val_loss improved from 0.44795 to 0.41021, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_7_conv_checkpoint/016-0.4102.hdf5\n",
      "36805/36805 [==============================] - 27s 722us/sample - loss: 0.4825 - acc: 0.8504 - val_loss: 0.4102 - val_acc: 0.8833\n",
      "Epoch 17/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4711 - acc: 0.8547\n",
      "Epoch 00017: val_loss improved from 0.41021 to 0.38522, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_7_conv_checkpoint/017-0.3852.hdf5\n",
      "36805/36805 [==============================] - 27s 725us/sample - loss: 0.4714 - acc: 0.8547 - val_loss: 0.3852 - val_acc: 0.8919\n",
      "Epoch 18/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4429 - acc: 0.8628\n",
      "Epoch 00018: val_loss improved from 0.38522 to 0.37183, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_7_conv_checkpoint/018-0.3718.hdf5\n",
      "36805/36805 [==============================] - 27s 730us/sample - loss: 0.4430 - acc: 0.8627 - val_loss: 0.3718 - val_acc: 0.8956\n",
      "Epoch 19/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4223 - acc: 0.8684\n",
      "Epoch 00019: val_loss improved from 0.37183 to 0.36718, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_7_conv_checkpoint/019-0.3672.hdf5\n",
      "36805/36805 [==============================] - 27s 723us/sample - loss: 0.4227 - acc: 0.8683 - val_loss: 0.3672 - val_acc: 0.9024\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4047 - acc: 0.8740\n",
      "Epoch 00020: val_loss improved from 0.36718 to 0.33413, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_7_conv_checkpoint/020-0.3341.hdf5\n",
      "36805/36805 [==============================] - 27s 725us/sample - loss: 0.4046 - acc: 0.8740 - val_loss: 0.3341 - val_acc: 0.9096\n",
      "Epoch 21/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3993 - acc: 0.8785\n",
      "Epoch 00021: val_loss improved from 0.33413 to 0.32602, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_7_conv_checkpoint/021-0.3260.hdf5\n",
      "36805/36805 [==============================] - 27s 723us/sample - loss: 0.3992 - acc: 0.8784 - val_loss: 0.3260 - val_acc: 0.9103\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3841 - acc: 0.8795\n",
      "Epoch 00022: val_loss improved from 0.32602 to 0.30993, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_7_conv_checkpoint/022-0.3099.hdf5\n",
      "36805/36805 [==============================] - 27s 724us/sample - loss: 0.3841 - acc: 0.8795 - val_loss: 0.3099 - val_acc: 0.9143\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3701 - acc: 0.8851\n",
      "Epoch 00023: val_loss did not improve from 0.30993\n",
      "36805/36805 [==============================] - 27s 726us/sample - loss: 0.3700 - acc: 0.8851 - val_loss: 0.3125 - val_acc: 0.9161\n",
      "Epoch 24/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3579 - acc: 0.8881\n",
      "Epoch 00024: val_loss did not improve from 0.30993\n",
      "36805/36805 [==============================] - 26s 719us/sample - loss: 0.3578 - acc: 0.8881 - val_loss: 0.3151 - val_acc: 0.9099\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3508 - acc: 0.8905\n",
      "Epoch 00025: val_loss improved from 0.30993 to 0.30483, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_7_conv_checkpoint/025-0.3048.hdf5\n",
      "36805/36805 [==============================] - 27s 723us/sample - loss: 0.3507 - acc: 0.8905 - val_loss: 0.3048 - val_acc: 0.9124\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3380 - acc: 0.8946\n",
      "Epoch 00026: val_loss did not improve from 0.30483\n",
      "36805/36805 [==============================] - 27s 724us/sample - loss: 0.3379 - acc: 0.8947 - val_loss: 0.3109 - val_acc: 0.9131\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3282 - acc: 0.8966\n",
      "Epoch 00027: val_loss improved from 0.30483 to 0.29777, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_7_conv_checkpoint/027-0.2978.hdf5\n",
      "36805/36805 [==============================] - 26s 719us/sample - loss: 0.3282 - acc: 0.8966 - val_loss: 0.2978 - val_acc: 0.9129\n",
      "Epoch 28/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3173 - acc: 0.9002\n",
      "Epoch 00028: val_loss improved from 0.29777 to 0.28689, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_7_conv_checkpoint/028-0.2869.hdf5\n",
      "36805/36805 [==============================] - 27s 723us/sample - loss: 0.3173 - acc: 0.9003 - val_loss: 0.2869 - val_acc: 0.9192\n",
      "Epoch 29/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3120 - acc: 0.9013\n",
      "Epoch 00029: val_loss did not improve from 0.28689\n",
      "36805/36805 [==============================] - 27s 721us/sample - loss: 0.3122 - acc: 0.9011 - val_loss: 0.2875 - val_acc: 0.9199\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3001 - acc: 0.9043\n",
      "Epoch 00030: val_loss improved from 0.28689 to 0.26351, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_7_conv_checkpoint/030-0.2635.hdf5\n",
      "36805/36805 [==============================] - 27s 724us/sample - loss: 0.3000 - acc: 0.9043 - val_loss: 0.2635 - val_acc: 0.9297\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2892 - acc: 0.9068\n",
      "Epoch 00031: val_loss did not improve from 0.26351\n",
      "36805/36805 [==============================] - 27s 723us/sample - loss: 0.2892 - acc: 0.9068 - val_loss: 0.2688 - val_acc: 0.9245\n",
      "Epoch 32/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2811 - acc: 0.9093\n",
      "Epoch 00032: val_loss improved from 0.26351 to 0.24355, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_7_conv_checkpoint/032-0.2435.hdf5\n",
      "36805/36805 [==============================] - 27s 721us/sample - loss: 0.2808 - acc: 0.9094 - val_loss: 0.2435 - val_acc: 0.9348\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2831 - acc: 0.9097\n",
      "Epoch 00033: val_loss did not improve from 0.24355\n",
      "36805/36805 [==============================] - 27s 724us/sample - loss: 0.2831 - acc: 0.9097 - val_loss: 0.2488 - val_acc: 0.9317\n",
      "Epoch 34/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2703 - acc: 0.9144\n",
      "Epoch 00034: val_loss did not improve from 0.24355\n",
      "36805/36805 [==============================] - 27s 726us/sample - loss: 0.2703 - acc: 0.9143 - val_loss: 0.2525 - val_acc: 0.9290\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2631 - acc: 0.9151\n",
      "Epoch 00035: val_loss did not improve from 0.24355\n",
      "36805/36805 [==============================] - 27s 720us/sample - loss: 0.2630 - acc: 0.9151 - val_loss: 0.2437 - val_acc: 0.9352\n",
      "Epoch 36/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2593 - acc: 0.9174\n",
      "Epoch 00036: val_loss did not improve from 0.24355\n",
      "36805/36805 [==============================] - 27s 726us/sample - loss: 0.2592 - acc: 0.9174 - val_loss: 0.2453 - val_acc: 0.9311\n",
      "Epoch 37/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2503 - acc: 0.9185\n",
      "Epoch 00037: val_loss improved from 0.24355 to 0.23846, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_7_conv_checkpoint/037-0.2385.hdf5\n",
      "36805/36805 [==============================] - 27s 725us/sample - loss: 0.2509 - acc: 0.9185 - val_loss: 0.2385 - val_acc: 0.9357\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2421 - acc: 0.9217\n",
      "Epoch 00038: val_loss did not improve from 0.23846\n",
      "36805/36805 [==============================] - 26s 719us/sample - loss: 0.2422 - acc: 0.9217 - val_loss: 0.2535 - val_acc: 0.9250\n",
      "Epoch 39/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2470 - acc: 0.9197\n",
      "Epoch 00039: val_loss improved from 0.23846 to 0.22250, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_7_conv_checkpoint/039-0.2225.hdf5\n",
      "36805/36805 [==============================] - 27s 726us/sample - loss: 0.2468 - acc: 0.9197 - val_loss: 0.2225 - val_acc: 0.9383\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2392 - acc: 0.9225\n",
      "Epoch 00040: val_loss did not improve from 0.22250\n",
      "36805/36805 [==============================] - 26s 719us/sample - loss: 0.2393 - acc: 0.9225 - val_loss: 0.2369 - val_acc: 0.9336\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2338 - acc: 0.9242\n",
      "Epoch 00041: val_loss did not improve from 0.22250\n",
      "36805/36805 [==============================] - 27s 724us/sample - loss: 0.2338 - acc: 0.9242 - val_loss: 0.2393 - val_acc: 0.9292\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2259 - acc: 0.9272\n",
      "Epoch 00042: val_loss did not improve from 0.22250\n",
      "36805/36805 [==============================] - 27s 725us/sample - loss: 0.2259 - acc: 0.9272 - val_loss: 0.2299 - val_acc: 0.9366\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2197 - acc: 0.9282\n",
      "Epoch 00043: val_loss did not improve from 0.22250\n",
      "36805/36805 [==============================] - 27s 721us/sample - loss: 0.2197 - acc: 0.9281 - val_loss: 0.2607 - val_acc: 0.9290\n",
      "Epoch 44/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2210 - acc: 0.9279\n",
      "Epoch 00044: val_loss did not improve from 0.22250\n",
      "36805/36805 [==============================] - 27s 721us/sample - loss: 0.2209 - acc: 0.9280 - val_loss: 0.2332 - val_acc: 0.9355\n",
      "Epoch 45/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2215 - acc: 0.9301\n",
      "Epoch 00045: val_loss did not improve from 0.22250\n",
      "36805/36805 [==============================] - 27s 725us/sample - loss: 0.2216 - acc: 0.9300 - val_loss: 0.2503 - val_acc: 0.9285\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2075 - acc: 0.9333\n",
      "Epoch 00046: val_loss did not improve from 0.22250\n",
      "36805/36805 [==============================] - 27s 722us/sample - loss: 0.2075 - acc: 0.9333 - val_loss: 0.2403 - val_acc: 0.9364\n",
      "Epoch 47/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2136 - acc: 0.9303\n",
      "Epoch 00047: val_loss improved from 0.22250 to 0.21837, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_7_conv_checkpoint/047-0.2184.hdf5\n",
      "36805/36805 [==============================] - 27s 725us/sample - loss: 0.2139 - acc: 0.9302 - val_loss: 0.2184 - val_acc: 0.9369\n",
      "Epoch 48/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2048 - acc: 0.9335\n",
      "Epoch 00048: val_loss did not improve from 0.21837\n",
      "36805/36805 [==============================] - 27s 723us/sample - loss: 0.2047 - acc: 0.9336 - val_loss: 0.2196 - val_acc: 0.9450\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1999 - acc: 0.9352\n",
      "Epoch 00049: val_loss improved from 0.21837 to 0.21732, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_7_conv_checkpoint/049-0.2173.hdf5\n",
      "36805/36805 [==============================] - 27s 727us/sample - loss: 0.1999 - acc: 0.9353 - val_loss: 0.2173 - val_acc: 0.9408\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1988 - acc: 0.9345\n",
      "Epoch 00050: val_loss did not improve from 0.21732\n",
      "36805/36805 [==============================] - 27s 723us/sample - loss: 0.1987 - acc: 0.9345 - val_loss: 0.2299 - val_acc: 0.9366\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1956 - acc: 0.9345\n",
      "Epoch 00051: val_loss did not improve from 0.21732\n",
      "36805/36805 [==============================] - 27s 722us/sample - loss: 0.1955 - acc: 0.9345 - val_loss: 0.2325 - val_acc: 0.9394\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1930 - acc: 0.9360\n",
      "Epoch 00052: val_loss improved from 0.21732 to 0.21635, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_7_conv_checkpoint/052-0.2163.hdf5\n",
      "36805/36805 [==============================] - 27s 729us/sample - loss: 0.1929 - acc: 0.9360 - val_loss: 0.2163 - val_acc: 0.9406\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1848 - acc: 0.9393\n",
      "Epoch 00053: val_loss improved from 0.21635 to 0.21449, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_7_conv_checkpoint/053-0.2145.hdf5\n",
      "36805/36805 [==============================] - 27s 725us/sample - loss: 0.1849 - acc: 0.9393 - val_loss: 0.2145 - val_acc: 0.9406\n",
      "Epoch 54/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1830 - acc: 0.9399\n",
      "Epoch 00054: val_loss improved from 0.21449 to 0.20789, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_7_conv_checkpoint/054-0.2079.hdf5\n",
      "36805/36805 [==============================] - 27s 722us/sample - loss: 0.1832 - acc: 0.9399 - val_loss: 0.2079 - val_acc: 0.9436\n",
      "Epoch 55/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1847 - acc: 0.9398\n",
      "Epoch 00055: val_loss did not improve from 0.20789\n",
      "36805/36805 [==============================] - 27s 724us/sample - loss: 0.1846 - acc: 0.9398 - val_loss: 0.2230 - val_acc: 0.9369\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1799 - acc: 0.9389\n",
      "Epoch 00056: val_loss improved from 0.20789 to 0.20015, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_7_conv_checkpoint/056-0.2002.hdf5\n",
      "36805/36805 [==============================] - 27s 729us/sample - loss: 0.1799 - acc: 0.9389 - val_loss: 0.2002 - val_acc: 0.9460\n",
      "Epoch 57/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1802 - acc: 0.9399\n",
      "Epoch 00057: val_loss did not improve from 0.20015\n",
      "36805/36805 [==============================] - 27s 720us/sample - loss: 0.1802 - acc: 0.9399 - val_loss: 0.2397 - val_acc: 0.9348\n",
      "Epoch 58/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1782 - acc: 0.9414\n",
      "Epoch 00058: val_loss did not improve from 0.20015\n",
      "36805/36805 [==============================] - 27s 722us/sample - loss: 0.1781 - acc: 0.9415 - val_loss: 0.2009 - val_acc: 0.9450\n",
      "Epoch 59/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1706 - acc: 0.9443\n",
      "Epoch 00059: val_loss did not improve from 0.20015\n",
      "36805/36805 [==============================] - 27s 725us/sample - loss: 0.1705 - acc: 0.9443 - val_loss: 0.2010 - val_acc: 0.9471\n",
      "Epoch 60/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1691 - acc: 0.9446\n",
      "Epoch 00060: val_loss did not improve from 0.20015\n",
      "36805/36805 [==============================] - 27s 723us/sample - loss: 0.1696 - acc: 0.9445 - val_loss: 0.2152 - val_acc: 0.9415\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1685 - acc: 0.9439\n",
      "Epoch 00061: val_loss did not improve from 0.20015\n",
      "36805/36805 [==============================] - 26s 718us/sample - loss: 0.1685 - acc: 0.9439 - val_loss: 0.2240 - val_acc: 0.9392\n",
      "Epoch 62/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1694 - acc: 0.9433\n",
      "Epoch 00062: val_loss improved from 0.20015 to 0.19890, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_7_conv_checkpoint/062-0.1989.hdf5\n",
      "36805/36805 [==============================] - 27s 721us/sample - loss: 0.1692 - acc: 0.9433 - val_loss: 0.1989 - val_acc: 0.9464\n",
      "Epoch 63/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1617 - acc: 0.9460\n",
      "Epoch 00063: val_loss did not improve from 0.19890\n",
      "36805/36805 [==============================] - 27s 722us/sample - loss: 0.1617 - acc: 0.9460 - val_loss: 0.1998 - val_acc: 0.9462\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1578 - acc: 0.9474\n",
      "Epoch 00064: val_loss did not improve from 0.19890\n",
      "36805/36805 [==============================] - 26s 719us/sample - loss: 0.1578 - acc: 0.9474 - val_loss: 0.2004 - val_acc: 0.9413\n",
      "Epoch 65/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1577 - acc: 0.9476\n",
      "Epoch 00065: val_loss did not improve from 0.19890\n",
      "36805/36805 [==============================] - 27s 722us/sample - loss: 0.1577 - acc: 0.9476 - val_loss: 0.2023 - val_acc: 0.9427\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1552 - acc: 0.9480\n",
      "Epoch 00066: val_loss did not improve from 0.19890\n",
      "36805/36805 [==============================] - 26s 716us/sample - loss: 0.1552 - acc: 0.9480 - val_loss: 0.1992 - val_acc: 0.9481\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1523 - acc: 0.9492\n",
      "Epoch 00067: val_loss did not improve from 0.19890\n",
      "36805/36805 [==============================] - 27s 726us/sample - loss: 0.1523 - acc: 0.9492 - val_loss: 0.2074 - val_acc: 0.9429\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1475 - acc: 0.9502\n",
      "Epoch 00068: val_loss improved from 0.19890 to 0.19655, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_7_conv_checkpoint/068-0.1965.hdf5\n",
      "36805/36805 [==============================] - 27s 725us/sample - loss: 0.1475 - acc: 0.9502 - val_loss: 0.1965 - val_acc: 0.9478\n",
      "Epoch 69/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1500 - acc: 0.9505\n",
      "Epoch 00069: val_loss did not improve from 0.19655\n",
      "36805/36805 [==============================] - 27s 722us/sample - loss: 0.1502 - acc: 0.9504 - val_loss: 0.2061 - val_acc: 0.9443\n",
      "Epoch 70/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1458 - acc: 0.9518\n",
      "Epoch 00070: val_loss did not improve from 0.19655\n",
      "36805/36805 [==============================] - 27s 722us/sample - loss: 0.1457 - acc: 0.9519 - val_loss: 0.2084 - val_acc: 0.9471\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1484 - acc: 0.9501\n",
      "Epoch 00071: val_loss improved from 0.19655 to 0.19619, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_7_conv_checkpoint/071-0.1962.hdf5\n",
      "36805/36805 [==============================] - 27s 726us/sample - loss: 0.1484 - acc: 0.9501 - val_loss: 0.1962 - val_acc: 0.9457\n",
      "Epoch 72/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1393 - acc: 0.9529\n",
      "Epoch 00072: val_loss improved from 0.19619 to 0.18941, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_7_conv_checkpoint/072-0.1894.hdf5\n",
      "36805/36805 [==============================] - 26s 720us/sample - loss: 0.1393 - acc: 0.9529 - val_loss: 0.1894 - val_acc: 0.9506\n",
      "Epoch 73/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1419 - acc: 0.9524\n",
      "Epoch 00073: val_loss did not improve from 0.18941\n",
      "36805/36805 [==============================] - 27s 725us/sample - loss: 0.1419 - acc: 0.9524 - val_loss: 0.2247 - val_acc: 0.9376\n",
      "Epoch 74/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1393 - acc: 0.9526\n",
      "Epoch 00074: val_loss did not improve from 0.18941\n",
      "36805/36805 [==============================] - 27s 722us/sample - loss: 0.1392 - acc: 0.9526 - val_loss: 0.1944 - val_acc: 0.9476\n",
      "Epoch 75/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1374 - acc: 0.9532\n",
      "Epoch 00075: val_loss did not improve from 0.18941\n",
      "36805/36805 [==============================] - 27s 723us/sample - loss: 0.1373 - acc: 0.9532 - val_loss: 0.2031 - val_acc: 0.9474\n",
      "Epoch 76/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1363 - acc: 0.9539\n",
      "Epoch 00076: val_loss did not improve from 0.18941\n",
      "36805/36805 [==============================] - 27s 723us/sample - loss: 0.1365 - acc: 0.9538 - val_loss: 0.2003 - val_acc: 0.9476\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1331 - acc: 0.9550\n",
      "Epoch 00077: val_loss did not improve from 0.18941\n",
      "36805/36805 [==============================] - 26s 716us/sample - loss: 0.1333 - acc: 0.9550 - val_loss: 0.2042 - val_acc: 0.9457\n",
      "Epoch 78/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1313 - acc: 0.9559\n",
      "Epoch 00078: val_loss did not improve from 0.18941\n",
      "36805/36805 [==============================] - 27s 726us/sample - loss: 0.1312 - acc: 0.9559 - val_loss: 0.2047 - val_acc: 0.9471\n",
      "Epoch 79/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1311 - acc: 0.9557\n",
      "Epoch 00079: val_loss did not improve from 0.18941\n",
      "36805/36805 [==============================] - 27s 721us/sample - loss: 0.1313 - acc: 0.9557 - val_loss: 0.1997 - val_acc: 0.9509\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1277 - acc: 0.9571\n",
      "Epoch 00080: val_loss did not improve from 0.18941\n",
      "36805/36805 [==============================] - 27s 722us/sample - loss: 0.1277 - acc: 0.9571 - val_loss: 0.2025 - val_acc: 0.9476\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1258 - acc: 0.9570\n",
      "Epoch 00081: val_loss did not improve from 0.18941\n",
      "36805/36805 [==============================] - 27s 723us/sample - loss: 0.1259 - acc: 0.9569 - val_loss: 0.1926 - val_acc: 0.9497\n",
      "Epoch 82/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1295 - acc: 0.9550\n",
      "Epoch 00082: val_loss did not improve from 0.18941\n",
      "36805/36805 [==============================] - 27s 725us/sample - loss: 0.1294 - acc: 0.9551 - val_loss: 0.2053 - val_acc: 0.9436\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1250 - acc: 0.9592\n",
      "Epoch 00083: val_loss improved from 0.18941 to 0.18817, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_7_conv_checkpoint/083-0.1882.hdf5\n",
      "36805/36805 [==============================] - 27s 722us/sample - loss: 0.1250 - acc: 0.9592 - val_loss: 0.1882 - val_acc: 0.9485\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1186 - acc: 0.9596\n",
      "Epoch 00084: val_loss did not improve from 0.18817\n",
      "36805/36805 [==============================] - 27s 720us/sample - loss: 0.1185 - acc: 0.9596 - val_loss: 0.1976 - val_acc: 0.9506\n",
      "Epoch 85/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1190 - acc: 0.9596\n",
      "Epoch 00085: val_loss did not improve from 0.18817\n",
      "36805/36805 [==============================] - 27s 727us/sample - loss: 0.1189 - acc: 0.9597 - val_loss: 0.1991 - val_acc: 0.9499\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1207 - acc: 0.9606\n",
      "Epoch 00086: val_loss improved from 0.18817 to 0.18597, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_7_conv_checkpoint/086-0.1860.hdf5\n",
      "36805/36805 [==============================] - 27s 721us/sample - loss: 0.1207 - acc: 0.9606 - val_loss: 0.1860 - val_acc: 0.9506\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1166 - acc: 0.9611\n",
      "Epoch 00087: val_loss did not improve from 0.18597\n",
      "36805/36805 [==============================] - 27s 720us/sample - loss: 0.1167 - acc: 0.9610 - val_loss: 0.1873 - val_acc: 0.9509\n",
      "Epoch 88/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1246 - acc: 0.9574\n",
      "Epoch 00088: val_loss did not improve from 0.18597\n",
      "36805/36805 [==============================] - 27s 722us/sample - loss: 0.1245 - acc: 0.9574 - val_loss: 0.2002 - val_acc: 0.9483\n",
      "Epoch 89/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1157 - acc: 0.9601\n",
      "Epoch 00089: val_loss did not improve from 0.18597\n",
      "36805/36805 [==============================] - 27s 723us/sample - loss: 0.1157 - acc: 0.9601 - val_loss: 0.1912 - val_acc: 0.9515\n",
      "Epoch 90/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1116 - acc: 0.9613\n",
      "Epoch 00090: val_loss did not improve from 0.18597\n",
      "36805/36805 [==============================] - 27s 720us/sample - loss: 0.1117 - acc: 0.9613 - val_loss: 0.1872 - val_acc: 0.9502\n",
      "Epoch 91/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1103 - acc: 0.9630\n",
      "Epoch 00091: val_loss did not improve from 0.18597\n",
      "36805/36805 [==============================] - 27s 722us/sample - loss: 0.1104 - acc: 0.9629 - val_loss: 0.1876 - val_acc: 0.9497\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1091 - acc: 0.9625\n",
      "Epoch 00092: val_loss did not improve from 0.18597\n",
      "36805/36805 [==============================] - 27s 722us/sample - loss: 0.1091 - acc: 0.9625 - val_loss: 0.1868 - val_acc: 0.9532\n",
      "Epoch 93/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1112 - acc: 0.9627\n",
      "Epoch 00093: val_loss did not improve from 0.18597\n",
      "36805/36805 [==============================] - 26s 719us/sample - loss: 0.1111 - acc: 0.9627 - val_loss: 0.1918 - val_acc: 0.9509\n",
      "Epoch 94/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1082 - acc: 0.9626\n",
      "Epoch 00094: val_loss did not improve from 0.18597\n",
      "36805/36805 [==============================] - 27s 728us/sample - loss: 0.1083 - acc: 0.9626 - val_loss: 0.1888 - val_acc: 0.9502\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1117 - acc: 0.9625\n",
      "Epoch 00095: val_loss did not improve from 0.18597\n",
      "36805/36805 [==============================] - 27s 723us/sample - loss: 0.1117 - acc: 0.9625 - val_loss: 0.1917 - val_acc: 0.9495\n",
      "Epoch 96/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1071 - acc: 0.9630\n",
      "Epoch 00096: val_loss did not improve from 0.18597\n",
      "36805/36805 [==============================] - 26s 720us/sample - loss: 0.1073 - acc: 0.9629 - val_loss: 0.1898 - val_acc: 0.9518\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1075 - acc: 0.9631\n",
      "Epoch 00097: val_loss did not improve from 0.18597\n",
      "36805/36805 [==============================] - 27s 723us/sample - loss: 0.1075 - acc: 0.9631 - val_loss: 0.2108 - val_acc: 0.9511\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1039 - acc: 0.9640\n",
      "Epoch 00098: val_loss did not improve from 0.18597\n",
      "36805/36805 [==============================] - 27s 725us/sample - loss: 0.1039 - acc: 0.9640 - val_loss: 0.1997 - val_acc: 0.9536\n",
      "Epoch 99/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1079 - acc: 0.9624\n",
      "Epoch 00099: val_loss did not improve from 0.18597\n",
      "36805/36805 [==============================] - 27s 721us/sample - loss: 0.1079 - acc: 0.9624 - val_loss: 0.1932 - val_acc: 0.9504\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1008 - acc: 0.9645\n",
      "Epoch 00100: val_loss did not improve from 0.18597\n",
      "36805/36805 [==============================] - 27s 725us/sample - loss: 0.1008 - acc: 0.9645 - val_loss: 0.2241 - val_acc: 0.9467\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1070 - acc: 0.9642\n",
      "Epoch 00101: val_loss did not improve from 0.18597\n",
      "36805/36805 [==============================] - 27s 723us/sample - loss: 0.1069 - acc: 0.9642 - val_loss: 0.1977 - val_acc: 0.9488\n",
      "Epoch 102/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1010 - acc: 0.9647\n",
      "Epoch 00102: val_loss did not improve from 0.18597\n",
      "36805/36805 [==============================] - 27s 725us/sample - loss: 0.1010 - acc: 0.9647 - val_loss: 0.1959 - val_acc: 0.9502\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0984 - acc: 0.9660\n",
      "Epoch 00103: val_loss did not improve from 0.18597\n",
      "36805/36805 [==============================] - 27s 723us/sample - loss: 0.0984 - acc: 0.9660 - val_loss: 0.2050 - val_acc: 0.9515\n",
      "Epoch 104/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0974 - acc: 0.9664\n",
      "Epoch 00104: val_loss did not improve from 0.18597\n",
      "36805/36805 [==============================] - 26s 717us/sample - loss: 0.0973 - acc: 0.9664 - val_loss: 0.1884 - val_acc: 0.9525\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0952 - acc: 0.9673\n",
      "Epoch 00105: val_loss did not improve from 0.18597\n",
      "36805/36805 [==============================] - 27s 723us/sample - loss: 0.0952 - acc: 0.9673 - val_loss: 0.1975 - val_acc: 0.9488\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 106/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0959 - acc: 0.9672\n",
      "Epoch 00106: val_loss did not improve from 0.18597\n",
      "36805/36805 [==============================] - 27s 723us/sample - loss: 0.0959 - acc: 0.9672 - val_loss: 0.1976 - val_acc: 0.9495\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0936 - acc: 0.9671\n",
      "Epoch 00107: val_loss did not improve from 0.18597\n",
      "36805/36805 [==============================] - 26s 717us/sample - loss: 0.0936 - acc: 0.9671 - val_loss: 0.1894 - val_acc: 0.9541\n",
      "Epoch 108/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0975 - acc: 0.9662\n",
      "Epoch 00108: val_loss did not improve from 0.18597\n",
      "36805/36805 [==============================] - 27s 724us/sample - loss: 0.0976 - acc: 0.9662 - val_loss: 0.1942 - val_acc: 0.9532\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0944 - acc: 0.9677\n",
      "Epoch 00109: val_loss did not improve from 0.18597\n",
      "36805/36805 [==============================] - 26s 719us/sample - loss: 0.0944 - acc: 0.9677 - val_loss: 0.1988 - val_acc: 0.9518\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0942 - acc: 0.9677\n",
      "Epoch 00110: val_loss did not improve from 0.18597\n",
      "36805/36805 [==============================] - 27s 722us/sample - loss: 0.0942 - acc: 0.9677 - val_loss: 0.2066 - val_acc: 0.9471\n",
      "Epoch 111/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0919 - acc: 0.9692\n",
      "Epoch 00111: val_loss did not improve from 0.18597\n",
      "36805/36805 [==============================] - 26s 719us/sample - loss: 0.0918 - acc: 0.9692 - val_loss: 0.2000 - val_acc: 0.9515\n",
      "Epoch 112/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0922 - acc: 0.9679\n",
      "Epoch 00112: val_loss did not improve from 0.18597\n",
      "36805/36805 [==============================] - 27s 723us/sample - loss: 0.0921 - acc: 0.9680 - val_loss: 0.1916 - val_acc: 0.9546\n",
      "Epoch 113/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0857 - acc: 0.9704\n",
      "Epoch 00113: val_loss did not improve from 0.18597\n",
      "36805/36805 [==============================] - 26s 720us/sample - loss: 0.0855 - acc: 0.9704 - val_loss: 0.2110 - val_acc: 0.9525\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0872 - acc: 0.9697\n",
      "Epoch 00114: val_loss did not improve from 0.18597\n",
      "36805/36805 [==============================] - 26s 720us/sample - loss: 0.0872 - acc: 0.9697 - val_loss: 0.1911 - val_acc: 0.9518\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0831 - acc: 0.9719\n",
      "Epoch 00115: val_loss did not improve from 0.18597\n",
      "36805/36805 [==============================] - 27s 723us/sample - loss: 0.0832 - acc: 0.9719 - val_loss: 0.1975 - val_acc: 0.9534\n",
      "Epoch 116/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0894 - acc: 0.9685\n",
      "Epoch 00116: val_loss did not improve from 0.18597\n",
      "36805/36805 [==============================] - 26s 720us/sample - loss: 0.0893 - acc: 0.9685 - val_loss: 0.1918 - val_acc: 0.9522\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0851 - acc: 0.9704\n",
      "Epoch 00117: val_loss did not improve from 0.18597\n",
      "36805/36805 [==============================] - 27s 721us/sample - loss: 0.0852 - acc: 0.9704 - val_loss: 0.2109 - val_acc: 0.9504\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0865 - acc: 0.9699\n",
      "Epoch 00118: val_loss did not improve from 0.18597\n",
      "36805/36805 [==============================] - 27s 723us/sample - loss: 0.0865 - acc: 0.9699 - val_loss: 0.2038 - val_acc: 0.9504\n",
      "Epoch 119/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0831 - acc: 0.9719\n",
      "Epoch 00119: val_loss did not improve from 0.18597\n",
      "36805/36805 [==============================] - 27s 723us/sample - loss: 0.0831 - acc: 0.9718 - val_loss: 0.1989 - val_acc: 0.9529\n",
      "Epoch 120/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0897 - acc: 0.9696\n",
      "Epoch 00120: val_loss did not improve from 0.18597\n",
      "36805/36805 [==============================] - 26s 720us/sample - loss: 0.0895 - acc: 0.9697 - val_loss: 0.1940 - val_acc: 0.9534\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0815 - acc: 0.9722\n",
      "Epoch 00121: val_loss did not improve from 0.18597\n",
      "36805/36805 [==============================] - 26s 719us/sample - loss: 0.0814 - acc: 0.9722 - val_loss: 0.2099 - val_acc: 0.9492\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0819 - acc: 0.9715\n",
      "Epoch 00122: val_loss did not improve from 0.18597\n",
      "36805/36805 [==============================] - 27s 723us/sample - loss: 0.0819 - acc: 0.9715 - val_loss: 0.2002 - val_acc: 0.9557\n",
      "Epoch 123/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0838 - acc: 0.9706\n",
      "Epoch 00123: val_loss did not improve from 0.18597\n",
      "36805/36805 [==============================] - 27s 721us/sample - loss: 0.0838 - acc: 0.9706 - val_loss: 0.2210 - val_acc: 0.9513\n",
      "Epoch 124/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0830 - acc: 0.9711\n",
      "Epoch 00124: val_loss did not improve from 0.18597\n",
      "36805/36805 [==============================] - 26s 719us/sample - loss: 0.0830 - acc: 0.9711 - val_loss: 0.1989 - val_acc: 0.9550\n",
      "Epoch 125/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0845 - acc: 0.9710\n",
      "Epoch 00125: val_loss did not improve from 0.18597\n",
      "36805/36805 [==============================] - 27s 723us/sample - loss: 0.0844 - acc: 0.9710 - val_loss: 0.2207 - val_acc: 0.9536\n",
      "Epoch 126/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0811 - acc: 0.9714\n",
      "Epoch 00126: val_loss did not improve from 0.18597\n",
      "36805/36805 [==============================] - 27s 721us/sample - loss: 0.0812 - acc: 0.9714 - val_loss: 0.2054 - val_acc: 0.9520\n",
      "Epoch 127/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0832 - acc: 0.9713\n",
      "Epoch 00127: val_loss did not improve from 0.18597\n",
      "36805/36805 [==============================] - 26s 719us/sample - loss: 0.0832 - acc: 0.9713 - val_loss: 0.1986 - val_acc: 0.9532\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0799 - acc: 0.9725\n",
      "Epoch 00128: val_loss did not improve from 0.18597\n",
      "36805/36805 [==============================] - 27s 721us/sample - loss: 0.0799 - acc: 0.9725 - val_loss: 0.2075 - val_acc: 0.9522\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0742 - acc: 0.9739\n",
      "Epoch 00129: val_loss did not improve from 0.18597\n",
      "36805/36805 [==============================] - 27s 725us/sample - loss: 0.0742 - acc: 0.9739 - val_loss: 0.1889 - val_acc: 0.9543\n",
      "Epoch 130/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0787 - acc: 0.9732\n",
      "Epoch 00130: val_loss did not improve from 0.18597\n",
      "36805/36805 [==============================] - 26s 720us/sample - loss: 0.0788 - acc: 0.9732 - val_loss: 0.1968 - val_acc: 0.9518\n",
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0820 - acc: 0.9729\n",
      "Epoch 00131: val_loss did not improve from 0.18597\n",
      "36805/36805 [==============================] - 27s 724us/sample - loss: 0.0820 - acc: 0.9729 - val_loss: 0.2015 - val_acc: 0.9529\n",
      "Epoch 132/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0744 - acc: 0.9733\n",
      "Epoch 00132: val_loss did not improve from 0.18597\n",
      "36805/36805 [==============================] - 27s 721us/sample - loss: 0.0744 - acc: 0.9733 - val_loss: 0.1959 - val_acc: 0.9532\n",
      "Epoch 133/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0754 - acc: 0.9733\n",
      "Epoch 00133: val_loss did not improve from 0.18597\n",
      "36805/36805 [==============================] - 27s 726us/sample - loss: 0.0754 - acc: 0.9733 - val_loss: 0.2125 - val_acc: 0.9527\n",
      "Epoch 134/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0724 - acc: 0.9758\n",
      "Epoch 00134: val_loss did not improve from 0.18597\n",
      "36805/36805 [==============================] - 26s 719us/sample - loss: 0.0726 - acc: 0.9758 - val_loss: 0.1928 - val_acc: 0.9522\n",
      "Epoch 135/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0713 - acc: 0.9754\n",
      "Epoch 00135: val_loss did not improve from 0.18597\n",
      "36805/36805 [==============================] - 26s 718us/sample - loss: 0.0714 - acc: 0.9754 - val_loss: 0.2064 - val_acc: 0.9509\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 136/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0716 - acc: 0.9752\n",
      "Epoch 00136: val_loss did not improve from 0.18597\n",
      "36805/36805 [==============================] - 26s 716us/sample - loss: 0.0716 - acc: 0.9752 - val_loss: 0.2115 - val_acc: 0.9546\n",
      "\n",
      "1D_CNN_custom_3_ch_32_DO_7_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 411us/sample - loss: 0.2336 - acc: 0.9319\n",
      "Loss: 0.23356248641682562 Accuracy: 0.9318795\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.5101 - acc: 0.1821\n",
      "Epoch 00001: val_loss improved from inf to 1.75631, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_8_conv_checkpoint/001-1.7563.hdf5\n",
      "36805/36805 [==============================] - 31s 847us/sample - loss: 2.5099 - acc: 0.1822 - val_loss: 1.7563 - val_acc: 0.4673\n",
      "Epoch 2/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.5951 - acc: 0.4845\n",
      "Epoch 00002: val_loss improved from 1.75631 to 1.09105, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_8_conv_checkpoint/002-1.0911.hdf5\n",
      "36805/36805 [==============================] - 27s 747us/sample - loss: 1.5944 - acc: 0.4847 - val_loss: 1.0911 - val_acc: 0.6553\n",
      "Epoch 3/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2067 - acc: 0.6043\n",
      "Epoch 00003: val_loss improved from 1.09105 to 0.89678, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_8_conv_checkpoint/003-0.8968.hdf5\n",
      "36805/36805 [==============================] - 27s 747us/sample - loss: 1.2061 - acc: 0.6045 - val_loss: 0.8968 - val_acc: 0.7384\n",
      "Epoch 4/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0139 - acc: 0.6726\n",
      "Epoch 00004: val_loss improved from 0.89678 to 0.72454, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_8_conv_checkpoint/004-0.7245.hdf5\n",
      "36805/36805 [==============================] - 27s 745us/sample - loss: 1.0133 - acc: 0.6728 - val_loss: 0.7245 - val_acc: 0.7806\n",
      "Epoch 5/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8646 - acc: 0.7242\n",
      "Epoch 00005: val_loss improved from 0.72454 to 0.62384, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_8_conv_checkpoint/005-0.6238.hdf5\n",
      "36805/36805 [==============================] - 27s 745us/sample - loss: 0.8642 - acc: 0.7243 - val_loss: 0.6238 - val_acc: 0.8088\n",
      "Epoch 6/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7617 - acc: 0.7579\n",
      "Epoch 00006: val_loss improved from 0.62384 to 0.53225, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_8_conv_checkpoint/006-0.5323.hdf5\n",
      "36805/36805 [==============================] - 27s 743us/sample - loss: 0.7616 - acc: 0.7579 - val_loss: 0.5323 - val_acc: 0.8390\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6684 - acc: 0.7849\n",
      "Epoch 00007: val_loss improved from 0.53225 to 0.47623, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_8_conv_checkpoint/007-0.4762.hdf5\n",
      "36805/36805 [==============================] - 28s 748us/sample - loss: 0.6685 - acc: 0.7849 - val_loss: 0.4762 - val_acc: 0.8581\n",
      "Epoch 8/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6136 - acc: 0.8045\n",
      "Epoch 00008: val_loss improved from 0.47623 to 0.41660, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_8_conv_checkpoint/008-0.4166.hdf5\n",
      "36805/36805 [==============================] - 27s 742us/sample - loss: 0.6132 - acc: 0.8045 - val_loss: 0.4166 - val_acc: 0.8798\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5561 - acc: 0.8227\n",
      "Epoch 00009: val_loss improved from 0.41660 to 0.40139, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_8_conv_checkpoint/009-0.4014.hdf5\n",
      "36805/36805 [==============================] - 27s 745us/sample - loss: 0.5561 - acc: 0.8227 - val_loss: 0.4014 - val_acc: 0.8779\n",
      "Epoch 10/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5143 - acc: 0.8363\n",
      "Epoch 00010: val_loss improved from 0.40139 to 0.35528, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_8_conv_checkpoint/010-0.3553.hdf5\n",
      "36805/36805 [==============================] - 27s 745us/sample - loss: 0.5143 - acc: 0.8363 - val_loss: 0.3553 - val_acc: 0.8896\n",
      "Epoch 11/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4768 - acc: 0.8482\n",
      "Epoch 00011: val_loss improved from 0.35528 to 0.32731, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_8_conv_checkpoint/011-0.3273.hdf5\n",
      "36805/36805 [==============================] - 27s 745us/sample - loss: 0.4769 - acc: 0.8481 - val_loss: 0.3273 - val_acc: 0.8996\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4500 - acc: 0.8572\n",
      "Epoch 00012: val_loss improved from 0.32731 to 0.30923, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_8_conv_checkpoint/012-0.3092.hdf5\n",
      "36805/36805 [==============================] - 28s 748us/sample - loss: 0.4500 - acc: 0.8572 - val_loss: 0.3092 - val_acc: 0.9071\n",
      "Epoch 13/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4220 - acc: 0.8665\n",
      "Epoch 00013: val_loss improved from 0.30923 to 0.29444, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_8_conv_checkpoint/013-0.2944.hdf5\n",
      "36805/36805 [==============================] - 27s 744us/sample - loss: 0.4221 - acc: 0.8664 - val_loss: 0.2944 - val_acc: 0.9103\n",
      "Epoch 14/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3960 - acc: 0.8741\n",
      "Epoch 00014: val_loss improved from 0.29444 to 0.26987, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_8_conv_checkpoint/014-0.2699.hdf5\n",
      "36805/36805 [==============================] - 28s 748us/sample - loss: 0.3962 - acc: 0.8740 - val_loss: 0.2699 - val_acc: 0.9164\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3752 - acc: 0.8806\n",
      "Epoch 00015: val_loss improved from 0.26987 to 0.25685, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_8_conv_checkpoint/015-0.2569.hdf5\n",
      "36805/36805 [==============================] - 27s 741us/sample - loss: 0.3751 - acc: 0.8806 - val_loss: 0.2569 - val_acc: 0.9238\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3594 - acc: 0.8855\n",
      "Epoch 00016: val_loss improved from 0.25685 to 0.24206, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_8_conv_checkpoint/016-0.2421.hdf5\n",
      "36805/36805 [==============================] - 27s 745us/sample - loss: 0.3594 - acc: 0.8855 - val_loss: 0.2421 - val_acc: 0.9283\n",
      "Epoch 17/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3390 - acc: 0.8916\n",
      "Epoch 00017: val_loss improved from 0.24206 to 0.23934, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_8_conv_checkpoint/017-0.2393.hdf5\n",
      "36805/36805 [==============================] - 28s 751us/sample - loss: 0.3389 - acc: 0.8917 - val_loss: 0.2393 - val_acc: 0.9257\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3248 - acc: 0.8955\n",
      "Epoch 00018: val_loss did not improve from 0.23934\n",
      "36805/36805 [==============================] - 27s 745us/sample - loss: 0.3247 - acc: 0.8955 - val_loss: 0.2438 - val_acc: 0.9210\n",
      "Epoch 19/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3083 - acc: 0.8994\n",
      "Epoch 00019: val_loss improved from 0.23934 to 0.22440, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_8_conv_checkpoint/019-0.2244.hdf5\n",
      "36805/36805 [==============================] - 27s 747us/sample - loss: 0.3081 - acc: 0.8994 - val_loss: 0.2244 - val_acc: 0.9287\n",
      "Epoch 20/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2954 - acc: 0.9053\n",
      "Epoch 00020: val_loss improved from 0.22440 to 0.21253, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_8_conv_checkpoint/020-0.2125.hdf5\n",
      "36805/36805 [==============================] - 28s 750us/sample - loss: 0.2953 - acc: 0.9054 - val_loss: 0.2125 - val_acc: 0.9350\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2805 - acc: 0.9102\n",
      "Epoch 00021: val_loss improved from 0.21253 to 0.20514, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_8_conv_checkpoint/021-0.2051.hdf5\n",
      "36805/36805 [==============================] - 27s 744us/sample - loss: 0.2805 - acc: 0.9102 - val_loss: 0.2051 - val_acc: 0.9366\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2778 - acc: 0.9101\n",
      "Epoch 00022: val_loss improved from 0.20514 to 0.20413, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_8_conv_checkpoint/022-0.2041.hdf5\n",
      "36805/36805 [==============================] - 28s 749us/sample - loss: 0.2778 - acc: 0.9101 - val_loss: 0.2041 - val_acc: 0.9387\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2627 - acc: 0.9156\n",
      "Epoch 00023: val_loss improved from 0.20413 to 0.19708, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_8_conv_checkpoint/023-0.1971.hdf5\n",
      "36805/36805 [==============================] - 27s 746us/sample - loss: 0.2628 - acc: 0.9156 - val_loss: 0.1971 - val_acc: 0.9380\n",
      "Epoch 24/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2555 - acc: 0.9191\n",
      "Epoch 00024: val_loss improved from 0.19708 to 0.18646, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_8_conv_checkpoint/024-0.1865.hdf5\n",
      "36805/36805 [==============================] - 27s 740us/sample - loss: 0.2554 - acc: 0.9191 - val_loss: 0.1865 - val_acc: 0.9392\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2464 - acc: 0.9210\n",
      "Epoch 00025: val_loss improved from 0.18646 to 0.18028, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_8_conv_checkpoint/025-0.1803.hdf5\n",
      "36805/36805 [==============================] - 28s 749us/sample - loss: 0.2464 - acc: 0.9210 - val_loss: 0.1803 - val_acc: 0.9441\n",
      "Epoch 26/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2386 - acc: 0.9231\n",
      "Epoch 00026: val_loss did not improve from 0.18028\n",
      "36805/36805 [==============================] - 27s 739us/sample - loss: 0.2387 - acc: 0.9231 - val_loss: 0.1817 - val_acc: 0.9455\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2316 - acc: 0.9248\n",
      "Epoch 00027: val_loss improved from 0.18028 to 0.17096, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_8_conv_checkpoint/027-0.1710.hdf5\n",
      "36805/36805 [==============================] - 27s 742us/sample - loss: 0.2317 - acc: 0.9248 - val_loss: 0.1710 - val_acc: 0.9467\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2261 - acc: 0.9273\n",
      "Epoch 00028: val_loss improved from 0.17096 to 0.17095, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_8_conv_checkpoint/028-0.1709.hdf5\n",
      "36805/36805 [==============================] - 28s 748us/sample - loss: 0.2261 - acc: 0.9273 - val_loss: 0.1709 - val_acc: 0.9481\n",
      "Epoch 29/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2149 - acc: 0.9310\n",
      "Epoch 00029: val_loss did not improve from 0.17095\n",
      "36805/36805 [==============================] - 27s 744us/sample - loss: 0.2151 - acc: 0.9309 - val_loss: 0.1793 - val_acc: 0.9469\n",
      "Epoch 30/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2133 - acc: 0.9312\n",
      "Epoch 00030: val_loss did not improve from 0.17095\n",
      "36805/36805 [==============================] - 27s 745us/sample - loss: 0.2131 - acc: 0.9313 - val_loss: 0.1829 - val_acc: 0.9443\n",
      "Epoch 31/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2085 - acc: 0.9319\n",
      "Epoch 00031: val_loss did not improve from 0.17095\n",
      "36805/36805 [==============================] - 27s 745us/sample - loss: 0.2084 - acc: 0.9320 - val_loss: 0.1723 - val_acc: 0.9478\n",
      "Epoch 32/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1960 - acc: 0.9359\n",
      "Epoch 00032: val_loss did not improve from 0.17095\n",
      "36805/36805 [==============================] - 27s 741us/sample - loss: 0.1961 - acc: 0.9359 - val_loss: 0.1856 - val_acc: 0.9411\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1940 - acc: 0.9370\n",
      "Epoch 00033: val_loss improved from 0.17095 to 0.16621, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_8_conv_checkpoint/033-0.1662.hdf5\n",
      "36805/36805 [==============================] - 27s 747us/sample - loss: 0.1940 - acc: 0.9370 - val_loss: 0.1662 - val_acc: 0.9485\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1932 - acc: 0.9380\n",
      "Epoch 00034: val_loss improved from 0.16621 to 0.15997, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_8_conv_checkpoint/034-0.1600.hdf5\n",
      "36805/36805 [==============================] - 27s 746us/sample - loss: 0.1933 - acc: 0.9380 - val_loss: 0.1600 - val_acc: 0.9506\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1876 - acc: 0.9387\n",
      "Epoch 00035: val_loss did not improve from 0.15997\n",
      "36805/36805 [==============================] - 27s 744us/sample - loss: 0.1876 - acc: 0.9387 - val_loss: 0.1672 - val_acc: 0.9509\n",
      "Epoch 36/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1817 - acc: 0.9388\n",
      "Epoch 00036: val_loss improved from 0.15997 to 0.15123, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_8_conv_checkpoint/036-0.1512.hdf5\n",
      "36805/36805 [==============================] - 27s 744us/sample - loss: 0.1816 - acc: 0.9389 - val_loss: 0.1512 - val_acc: 0.9541\n",
      "Epoch 37/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1743 - acc: 0.9437\n",
      "Epoch 00037: val_loss did not improve from 0.15123\n",
      "36805/36805 [==============================] - 27s 742us/sample - loss: 0.1743 - acc: 0.9436 - val_loss: 0.1570 - val_acc: 0.9557\n",
      "Epoch 38/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1741 - acc: 0.9418\n",
      "Epoch 00038: val_loss did not improve from 0.15123\n",
      "36805/36805 [==============================] - 28s 748us/sample - loss: 0.1741 - acc: 0.9418 - val_loss: 0.1540 - val_acc: 0.9515\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1706 - acc: 0.9442\n",
      "Epoch 00039: val_loss did not improve from 0.15123\n",
      "36805/36805 [==============================] - 28s 748us/sample - loss: 0.1706 - acc: 0.9442 - val_loss: 0.1536 - val_acc: 0.9515\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1648 - acc: 0.9457\n",
      "Epoch 00040: val_loss improved from 0.15123 to 0.14985, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_8_conv_checkpoint/040-0.1499.hdf5\n",
      "36805/36805 [==============================] - 27s 744us/sample - loss: 0.1648 - acc: 0.9457 - val_loss: 0.1499 - val_acc: 0.9532\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1547 - acc: 0.9480\n",
      "Epoch 00041: val_loss did not improve from 0.14985\n",
      "36805/36805 [==============================] - 27s 743us/sample - loss: 0.1547 - acc: 0.9480 - val_loss: 0.1503 - val_acc: 0.9548\n",
      "Epoch 42/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1571 - acc: 0.9479\n",
      "Epoch 00042: val_loss improved from 0.14985 to 0.14320, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_8_conv_checkpoint/042-0.1432.hdf5\n",
      "36805/36805 [==============================] - 27s 744us/sample - loss: 0.1571 - acc: 0.9479 - val_loss: 0.1432 - val_acc: 0.9553\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1549 - acc: 0.9480\n",
      "Epoch 00043: val_loss did not improve from 0.14320\n",
      "36805/36805 [==============================] - 27s 746us/sample - loss: 0.1549 - acc: 0.9480 - val_loss: 0.1610 - val_acc: 0.9539\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1498 - acc: 0.9483\n",
      "Epoch 00044: val_loss did not improve from 0.14320\n",
      "36805/36805 [==============================] - 27s 744us/sample - loss: 0.1497 - acc: 0.9483 - val_loss: 0.1489 - val_acc: 0.9550\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1444 - acc: 0.9513\n",
      "Epoch 00045: val_loss did not improve from 0.14320\n",
      "36805/36805 [==============================] - 27s 742us/sample - loss: 0.1444 - acc: 0.9513 - val_loss: 0.1684 - val_acc: 0.9485\n",
      "Epoch 46/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1479 - acc: 0.9508\n",
      "Epoch 00046: val_loss did not improve from 0.14320\n",
      "36805/36805 [==============================] - 27s 743us/sample - loss: 0.1478 - acc: 0.9508 - val_loss: 0.1543 - val_acc: 0.9541\n",
      "Epoch 47/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1405 - acc: 0.9527\n",
      "Epoch 00047: val_loss did not improve from 0.14320\n",
      "36805/36805 [==============================] - 27s 742us/sample - loss: 0.1404 - acc: 0.9527 - val_loss: 0.1494 - val_acc: 0.9557\n",
      "Epoch 48/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1347 - acc: 0.9548\n",
      "Epoch 00048: val_loss improved from 0.14320 to 0.14318, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_8_conv_checkpoint/048-0.1432.hdf5\n",
      "36805/36805 [==============================] - 27s 741us/sample - loss: 0.1346 - acc: 0.9549 - val_loss: 0.1432 - val_acc: 0.9567\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1321 - acc: 0.9549\n",
      "Epoch 00049: val_loss did not improve from 0.14318\n",
      "36805/36805 [==============================] - 27s 744us/sample - loss: 0.1321 - acc: 0.9550 - val_loss: 0.1510 - val_acc: 0.9562\n",
      "Epoch 50/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1349 - acc: 0.9550\n",
      "Epoch 00050: val_loss did not improve from 0.14318\n",
      "36805/36805 [==============================] - 27s 743us/sample - loss: 0.1349 - acc: 0.9551 - val_loss: 0.1597 - val_acc: 0.9543\n",
      "Epoch 51/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1336 - acc: 0.9561\n",
      "Epoch 00051: val_loss did not improve from 0.14318\n",
      "36805/36805 [==============================] - 27s 740us/sample - loss: 0.1336 - acc: 0.9561 - val_loss: 0.1553 - val_acc: 0.9555\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1284 - acc: 0.9565\n",
      "Epoch 00052: val_loss did not improve from 0.14318\n",
      "36805/36805 [==============================] - 27s 744us/sample - loss: 0.1284 - acc: 0.9565 - val_loss: 0.1505 - val_acc: 0.9571\n",
      "Epoch 53/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1272 - acc: 0.9560\n",
      "Epoch 00053: val_loss did not improve from 0.14318\n",
      "36805/36805 [==============================] - 27s 737us/sample - loss: 0.1273 - acc: 0.9559 - val_loss: 0.1571 - val_acc: 0.9550\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1233 - acc: 0.9580\n",
      "Epoch 00054: val_loss did not improve from 0.14318\n",
      "36805/36805 [==============================] - 27s 740us/sample - loss: 0.1235 - acc: 0.9580 - val_loss: 0.1637 - val_acc: 0.9539\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1239 - acc: 0.9582\n",
      "Epoch 00055: val_loss did not improve from 0.14318\n",
      "36805/36805 [==============================] - 27s 740us/sample - loss: 0.1239 - acc: 0.9582 - val_loss: 0.1482 - val_acc: 0.9574\n",
      "Epoch 56/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1160 - acc: 0.9597\n",
      "Epoch 00056: val_loss did not improve from 0.14318\n",
      "36805/36805 [==============================] - 27s 740us/sample - loss: 0.1160 - acc: 0.9597 - val_loss: 0.1513 - val_acc: 0.9536\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1188 - acc: 0.9605\n",
      "Epoch 00057: val_loss did not improve from 0.14318\n",
      "36805/36805 [==============================] - 27s 737us/sample - loss: 0.1187 - acc: 0.9605 - val_loss: 0.1473 - val_acc: 0.9576\n",
      "Epoch 58/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1126 - acc: 0.9612\n",
      "Epoch 00058: val_loss improved from 0.14318 to 0.13824, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_8_conv_checkpoint/058-0.1382.hdf5\n",
      "36805/36805 [==============================] - 27s 746us/sample - loss: 0.1126 - acc: 0.9612 - val_loss: 0.1382 - val_acc: 0.9574\n",
      "Epoch 59/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1105 - acc: 0.9625\n",
      "Epoch 00059: val_loss did not improve from 0.13824\n",
      "36805/36805 [==============================] - 28s 750us/sample - loss: 0.1106 - acc: 0.9625 - val_loss: 0.1400 - val_acc: 0.9602\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1111 - acc: 0.9626\n",
      "Epoch 00060: val_loss did not improve from 0.13824\n",
      "36805/36805 [==============================] - 28s 748us/sample - loss: 0.1111 - acc: 0.9626 - val_loss: 0.1414 - val_acc: 0.9585\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1060 - acc: 0.9646\n",
      "Epoch 00061: val_loss did not improve from 0.13824\n",
      "36805/36805 [==============================] - 27s 740us/sample - loss: 0.1060 - acc: 0.9647 - val_loss: 0.1458 - val_acc: 0.9581\n",
      "Epoch 62/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1057 - acc: 0.9649\n",
      "Epoch 00062: val_loss did not improve from 0.13824\n",
      "36805/36805 [==============================] - 27s 745us/sample - loss: 0.1056 - acc: 0.9649 - val_loss: 0.1471 - val_acc: 0.9571\n",
      "Epoch 63/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1034 - acc: 0.9643\n",
      "Epoch 00063: val_loss did not improve from 0.13824\n",
      "36805/36805 [==============================] - 27s 745us/sample - loss: 0.1033 - acc: 0.9644 - val_loss: 0.1571 - val_acc: 0.9606\n",
      "Epoch 64/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1026 - acc: 0.9649\n",
      "Epoch 00064: val_loss improved from 0.13824 to 0.13771, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_8_conv_checkpoint/064-0.1377.hdf5\n",
      "36805/36805 [==============================] - 28s 748us/sample - loss: 0.1026 - acc: 0.9649 - val_loss: 0.1377 - val_acc: 0.9602\n",
      "Epoch 65/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0981 - acc: 0.9668\n",
      "Epoch 00065: val_loss did not improve from 0.13771\n",
      "36805/36805 [==============================] - 27s 744us/sample - loss: 0.0980 - acc: 0.9669 - val_loss: 0.1400 - val_acc: 0.9618\n",
      "Epoch 66/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1018 - acc: 0.9654\n",
      "Epoch 00066: val_loss did not improve from 0.13771\n",
      "36805/36805 [==============================] - 27s 746us/sample - loss: 0.1019 - acc: 0.9655 - val_loss: 0.1435 - val_acc: 0.9597\n",
      "Epoch 67/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0916 - acc: 0.9686\n",
      "Epoch 00067: val_loss did not improve from 0.13771\n",
      "36805/36805 [==============================] - 27s 745us/sample - loss: 0.0915 - acc: 0.9686 - val_loss: 0.1474 - val_acc: 0.9578\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0968 - acc: 0.9670\n",
      "Epoch 00068: val_loss did not improve from 0.13771\n",
      "36805/36805 [==============================] - 27s 742us/sample - loss: 0.0968 - acc: 0.9670 - val_loss: 0.1522 - val_acc: 0.9576\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0936 - acc: 0.9670\n",
      "Epoch 00069: val_loss did not improve from 0.13771\n",
      "36805/36805 [==============================] - 27s 741us/sample - loss: 0.0936 - acc: 0.9670 - val_loss: 0.1419 - val_acc: 0.9599\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0892 - acc: 0.9689\n",
      "Epoch 00070: val_loss did not improve from 0.13771\n",
      "36805/36805 [==============================] - 27s 741us/sample - loss: 0.0892 - acc: 0.9689 - val_loss: 0.1515 - val_acc: 0.9592\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0905 - acc: 0.9685\n",
      "Epoch 00071: val_loss did not improve from 0.13771\n",
      "36805/36805 [==============================] - 27s 745us/sample - loss: 0.0905 - acc: 0.9685 - val_loss: 0.1563 - val_acc: 0.9562\n",
      "Epoch 72/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0904 - acc: 0.9697\n",
      "Epoch 00072: val_loss improved from 0.13771 to 0.13715, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_8_conv_checkpoint/072-0.1371.hdf5\n",
      "36805/36805 [==============================] - 27s 741us/sample - loss: 0.0904 - acc: 0.9697 - val_loss: 0.1371 - val_acc: 0.9611\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0889 - acc: 0.9705\n",
      "Epoch 00073: val_loss did not improve from 0.13715\n",
      "36805/36805 [==============================] - 27s 746us/sample - loss: 0.0889 - acc: 0.9705 - val_loss: 0.1708 - val_acc: 0.9546\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0856 - acc: 0.9704\n",
      "Epoch 00074: val_loss did not improve from 0.13715\n",
      "36805/36805 [==============================] - 27s 742us/sample - loss: 0.0857 - acc: 0.9704 - val_loss: 0.1441 - val_acc: 0.9606\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0850 - acc: 0.9702\n",
      "Epoch 00075: val_loss did not improve from 0.13715\n",
      "36805/36805 [==============================] - 27s 744us/sample - loss: 0.0850 - acc: 0.9702 - val_loss: 0.1560 - val_acc: 0.9604\n",
      "Epoch 76/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0826 - acc: 0.9715\n",
      "Epoch 00076: val_loss did not improve from 0.13715\n",
      "36805/36805 [==============================] - 27s 745us/sample - loss: 0.0826 - acc: 0.9716 - val_loss: 0.1611 - val_acc: 0.9571\n",
      "Epoch 77/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0830 - acc: 0.9716\n",
      "Epoch 00077: val_loss did not improve from 0.13715\n",
      "36805/36805 [==============================] - 27s 742us/sample - loss: 0.0829 - acc: 0.9717 - val_loss: 0.1466 - val_acc: 0.9597\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0812 - acc: 0.9722\n",
      "Epoch 00078: val_loss did not improve from 0.13715\n",
      "36805/36805 [==============================] - 27s 744us/sample - loss: 0.0812 - acc: 0.9722 - val_loss: 0.1437 - val_acc: 0.9590\n",
      "Epoch 79/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0818 - acc: 0.9723\n",
      "Epoch 00079: val_loss did not improve from 0.13715\n",
      "36805/36805 [==============================] - 28s 747us/sample - loss: 0.0817 - acc: 0.9723 - val_loss: 0.1472 - val_acc: 0.9595\n",
      "Epoch 80/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0788 - acc: 0.9726\n",
      "Epoch 00080: val_loss did not improve from 0.13715\n",
      "36805/36805 [==============================] - 27s 741us/sample - loss: 0.0788 - acc: 0.9726 - val_loss: 0.1523 - val_acc: 0.9560\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0779 - acc: 0.9724\n",
      "Epoch 00081: val_loss did not improve from 0.13715\n",
      "36805/36805 [==============================] - 27s 746us/sample - loss: 0.0779 - acc: 0.9724 - val_loss: 0.1470 - val_acc: 0.9611\n",
      "Epoch 82/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0735 - acc: 0.9745\n",
      "Epoch 00082: val_loss did not improve from 0.13715\n",
      "36805/36805 [==============================] - 27s 738us/sample - loss: 0.0735 - acc: 0.9745 - val_loss: 0.1432 - val_acc: 0.9606\n",
      "Epoch 83/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0772 - acc: 0.9733\n",
      "Epoch 00083: val_loss did not improve from 0.13715\n",
      "36805/36805 [==============================] - 27s 742us/sample - loss: 0.0771 - acc: 0.9733 - val_loss: 0.1651 - val_acc: 0.9543\n",
      "Epoch 84/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0731 - acc: 0.9749\n",
      "Epoch 00084: val_loss did not improve from 0.13715\n",
      "36805/36805 [==============================] - 27s 743us/sample - loss: 0.0733 - acc: 0.9749 - val_loss: 0.1680 - val_acc: 0.9592\n",
      "Epoch 85/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0762 - acc: 0.9731\n",
      "Epoch 00085: val_loss did not improve from 0.13715\n",
      "36805/36805 [==============================] - 27s 741us/sample - loss: 0.0762 - acc: 0.9731 - val_loss: 0.1703 - val_acc: 0.9550\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0704 - acc: 0.9755\n",
      "Epoch 00086: val_loss did not improve from 0.13715\n",
      "36805/36805 [==============================] - 27s 742us/sample - loss: 0.0704 - acc: 0.9755 - val_loss: 0.1421 - val_acc: 0.9618\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0715 - acc: 0.9753\n",
      "Epoch 00087: val_loss did not improve from 0.13715\n",
      "36805/36805 [==============================] - 27s 742us/sample - loss: 0.0715 - acc: 0.9753 - val_loss: 0.1474 - val_acc: 0.9639\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0734 - acc: 0.9752\n",
      "Epoch 00088: val_loss did not improve from 0.13715\n",
      "36805/36805 [==============================] - 27s 739us/sample - loss: 0.0733 - acc: 0.9752 - val_loss: 0.1392 - val_acc: 0.9623\n",
      "Epoch 89/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0667 - acc: 0.9769\n",
      "Epoch 00089: val_loss did not improve from 0.13715\n",
      "36805/36805 [==============================] - 27s 740us/sample - loss: 0.0667 - acc: 0.9768 - val_loss: 0.1615 - val_acc: 0.9576\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0661 - acc: 0.9772\n",
      "Epoch 00090: val_loss did not improve from 0.13715\n",
      "36805/36805 [==============================] - 27s 740us/sample - loss: 0.0661 - acc: 0.9772 - val_loss: 0.1618 - val_acc: 0.9597\n",
      "Epoch 91/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0670 - acc: 0.9769\n",
      "Epoch 00091: val_loss did not improve from 0.13715\n",
      "36805/36805 [==============================] - 27s 744us/sample - loss: 0.0669 - acc: 0.9769 - val_loss: 0.1593 - val_acc: 0.9581\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0671 - acc: 0.9770\n",
      "Epoch 00092: val_loss did not improve from 0.13715\n",
      "36805/36805 [==============================] - 27s 739us/sample - loss: 0.0671 - acc: 0.9770 - val_loss: 0.1549 - val_acc: 0.9592\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0674 - acc: 0.9768\n",
      "Epoch 00093: val_loss did not improve from 0.13715\n",
      "36805/36805 [==============================] - 27s 742us/sample - loss: 0.0674 - acc: 0.9768 - val_loss: 0.1540 - val_acc: 0.9620\n",
      "Epoch 94/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0647 - acc: 0.9778\n",
      "Epoch 00094: val_loss did not improve from 0.13715\n",
      "36805/36805 [==============================] - 27s 746us/sample - loss: 0.0647 - acc: 0.9778 - val_loss: 0.1504 - val_acc: 0.9632\n",
      "Epoch 95/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0644 - acc: 0.9778\n",
      "Epoch 00095: val_loss did not improve from 0.13715\n",
      "36805/36805 [==============================] - 27s 742us/sample - loss: 0.0644 - acc: 0.9778 - val_loss: 0.1741 - val_acc: 0.9543\n",
      "Epoch 96/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0614 - acc: 0.9788\n",
      "Epoch 00096: val_loss did not improve from 0.13715\n",
      "36805/36805 [==============================] - 27s 741us/sample - loss: 0.0613 - acc: 0.9788 - val_loss: 0.1620 - val_acc: 0.9592\n",
      "Epoch 97/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0635 - acc: 0.9775\n",
      "Epoch 00097: val_loss did not improve from 0.13715\n",
      "36805/36805 [==============================] - 27s 743us/sample - loss: 0.0636 - acc: 0.9774 - val_loss: 0.1728 - val_acc: 0.9581\n",
      "Epoch 98/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0651 - acc: 0.9776\n",
      "Epoch 00098: val_loss did not improve from 0.13715\n",
      "36805/36805 [==============================] - 27s 745us/sample - loss: 0.0651 - acc: 0.9776 - val_loss: 0.1505 - val_acc: 0.9585\n",
      "Epoch 99/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0578 - acc: 0.9802\n",
      "Epoch 00099: val_loss did not improve from 0.13715\n",
      "36805/36805 [==============================] - 27s 744us/sample - loss: 0.0578 - acc: 0.9801 - val_loss: 0.1729 - val_acc: 0.9576\n",
      "Epoch 100/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0583 - acc: 0.9794\n",
      "Epoch 00100: val_loss did not improve from 0.13715\n",
      "36805/36805 [==============================] - 27s 741us/sample - loss: 0.0582 - acc: 0.9794 - val_loss: 0.1755 - val_acc: 0.9590\n",
      "Epoch 101/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0577 - acc: 0.9794\n",
      "Epoch 00101: val_loss did not improve from 0.13715\n",
      "36805/36805 [==============================] - 27s 743us/sample - loss: 0.0579 - acc: 0.9794 - val_loss: 0.1761 - val_acc: 0.9599\n",
      "Epoch 102/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0642 - acc: 0.9783\n",
      "Epoch 00102: val_loss did not improve from 0.13715\n",
      "36805/36805 [==============================] - 27s 733us/sample - loss: 0.0642 - acc: 0.9783 - val_loss: 0.1421 - val_acc: 0.9632\n",
      "Epoch 103/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0596 - acc: 0.9795\n",
      "Epoch 00103: val_loss did not improve from 0.13715\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 0.0596 - acc: 0.9795 - val_loss: 0.1422 - val_acc: 0.9646\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0584 - acc: 0.9801\n",
      "Epoch 00104: val_loss did not improve from 0.13715\n",
      "36805/36805 [==============================] - 27s 740us/sample - loss: 0.0584 - acc: 0.9801 - val_loss: 0.1685 - val_acc: 0.9627\n",
      "Epoch 105/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0549 - acc: 0.9814\n",
      "Epoch 00105: val_loss did not improve from 0.13715\n",
      "36805/36805 [==============================] - 27s 739us/sample - loss: 0.0548 - acc: 0.9815 - val_loss: 0.1561 - val_acc: 0.9590\n",
      "Epoch 106/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0556 - acc: 0.9811\n",
      "Epoch 00106: val_loss did not improve from 0.13715\n",
      "36805/36805 [==============================] - 27s 737us/sample - loss: 0.0555 - acc: 0.9811 - val_loss: 0.1569 - val_acc: 0.9618\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0533 - acc: 0.9813\n",
      "Epoch 00107: val_loss did not improve from 0.13715\n",
      "36805/36805 [==============================] - 27s 731us/sample - loss: 0.0533 - acc: 0.9813 - val_loss: 0.1636 - val_acc: 0.9620\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0557 - acc: 0.9815\n",
      "Epoch 00108: val_loss did not improve from 0.13715\n",
      "36805/36805 [==============================] - 27s 741us/sample - loss: 0.0557 - acc: 0.9815 - val_loss: 0.1606 - val_acc: 0.9606\n",
      "Epoch 109/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0541 - acc: 0.9812\n",
      "Epoch 00109: val_loss did not improve from 0.13715\n",
      "36805/36805 [==============================] - 27s 739us/sample - loss: 0.0542 - acc: 0.9811 - val_loss: 0.1499 - val_acc: 0.9644\n",
      "Epoch 110/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0594 - acc: 0.9792\n",
      "Epoch 00110: val_loss did not improve from 0.13715\n",
      "36805/36805 [==============================] - 27s 735us/sample - loss: 0.0593 - acc: 0.9792 - val_loss: 0.1618 - val_acc: 0.9611\n",
      "Epoch 111/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0543 - acc: 0.9808\n",
      "Epoch 00111: val_loss did not improve from 0.13715\n",
      "36805/36805 [==============================] - 27s 738us/sample - loss: 0.0542 - acc: 0.9808 - val_loss: 0.1595 - val_acc: 0.9620\n",
      "Epoch 112/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0516 - acc: 0.9820\n",
      "Epoch 00112: val_loss did not improve from 0.13715\n",
      "36805/36805 [==============================] - 27s 736us/sample - loss: 0.0516 - acc: 0.9820 - val_loss: 0.1657 - val_acc: 0.9611\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0517 - acc: 0.9822\n",
      "Epoch 00113: val_loss did not improve from 0.13715\n",
      "36805/36805 [==============================] - 27s 732us/sample - loss: 0.0517 - acc: 0.9822 - val_loss: 0.1484 - val_acc: 0.9604\n",
      "Epoch 114/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0531 - acc: 0.9814\n",
      "Epoch 00114: val_loss did not improve from 0.13715\n",
      "36805/36805 [==============================] - 27s 740us/sample - loss: 0.0531 - acc: 0.9814 - val_loss: 0.1711 - val_acc: 0.9611\n",
      "Epoch 115/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0504 - acc: 0.9831\n",
      "Epoch 00115: val_loss did not improve from 0.13715\n",
      "36805/36805 [==============================] - 27s 737us/sample - loss: 0.0504 - acc: 0.9831 - val_loss: 0.1568 - val_acc: 0.9641\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0559 - acc: 0.9802\n",
      "Epoch 00116: val_loss did not improve from 0.13715\n",
      "36805/36805 [==============================] - 27s 738us/sample - loss: 0.0560 - acc: 0.9802 - val_loss: 0.1680 - val_acc: 0.9606\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0492 - acc: 0.9839\n",
      "Epoch 00117: val_loss did not improve from 0.13715\n",
      "36805/36805 [==============================] - 27s 742us/sample - loss: 0.0492 - acc: 0.9839 - val_loss: 0.1688 - val_acc: 0.9630\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0480 - acc: 0.9834\n",
      "Epoch 00118: val_loss did not improve from 0.13715\n",
      "36805/36805 [==============================] - 27s 738us/sample - loss: 0.0480 - acc: 0.9834 - val_loss: 0.1706 - val_acc: 0.9604\n",
      "Epoch 119/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0483 - acc: 0.9830\n",
      "Epoch 00119: val_loss did not improve from 0.13715\n",
      "36805/36805 [==============================] - 27s 738us/sample - loss: 0.0483 - acc: 0.9830 - val_loss: 0.1634 - val_acc: 0.9637\n",
      "Epoch 120/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0507 - acc: 0.9833\n",
      "Epoch 00120: val_loss did not improve from 0.13715\n",
      "36805/36805 [==============================] - 27s 737us/sample - loss: 0.0508 - acc: 0.9833 - val_loss: 0.1640 - val_acc: 0.9639\n",
      "Epoch 121/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0489 - acc: 0.9829\n",
      "Epoch 00121: val_loss did not improve from 0.13715\n",
      "36805/36805 [==============================] - 27s 740us/sample - loss: 0.0488 - acc: 0.9829 - val_loss: 0.1536 - val_acc: 0.9630\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0465 - acc: 0.9839\n",
      "Epoch 00122: val_loss did not improve from 0.13715\n",
      "36805/36805 [==============================] - 27s 738us/sample - loss: 0.0465 - acc: 0.9839 - val_loss: 0.1476 - val_acc: 0.9627\n",
      "\n",
      "1D_CNN_custom_3_ch_32_DO_8_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 389us/sample - loss: 0.1771 - acc: 0.9479\n",
      "Loss: 0.17710612370155807 Accuracy: 0.9478712\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 2.2273 - acc: 0.2774\n",
      "Epoch 00001: val_loss improved from inf to 1.44332, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_9_conv_checkpoint/001-1.4433.hdf5\n",
      "36805/36805 [==============================] - 32s 876us/sample - loss: 2.2268 - acc: 0.2775 - val_loss: 1.4433 - val_acc: 0.5572\n",
      "Epoch 2/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.4162 - acc: 0.5429\n",
      "Epoch 00002: val_loss improved from 1.44332 to 1.04406, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_9_conv_checkpoint/002-1.0441.hdf5\n",
      "36805/36805 [==============================] - 27s 744us/sample - loss: 1.4158 - acc: 0.5430 - val_loss: 1.0441 - val_acc: 0.6783\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1378 - acc: 0.6285\n",
      "Epoch 00003: val_loss improved from 1.04406 to 0.85475, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_9_conv_checkpoint/003-0.8547.hdf5\n",
      "36805/36805 [==============================] - 28s 754us/sample - loss: 1.1378 - acc: 0.6285 - val_loss: 0.8547 - val_acc: 0.7298\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9367 - acc: 0.6949\n",
      "Epoch 00004: val_loss improved from 0.85475 to 0.72984, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_9_conv_checkpoint/004-0.7298.hdf5\n",
      "36805/36805 [==============================] - 28s 750us/sample - loss: 0.9367 - acc: 0.6950 - val_loss: 0.7298 - val_acc: 0.7624\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7926 - acc: 0.7435\n",
      "Epoch 00005: val_loss improved from 0.72984 to 0.57659, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_9_conv_checkpoint/005-0.5766.hdf5\n",
      "36805/36805 [==============================] - 28s 750us/sample - loss: 0.7925 - acc: 0.7435 - val_loss: 0.5766 - val_acc: 0.8181\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6987 - acc: 0.7733\n",
      "Epoch 00006: val_loss improved from 0.57659 to 0.52025, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_9_conv_checkpoint/006-0.5202.hdf5\n",
      "36805/36805 [==============================] - 28s 748us/sample - loss: 0.6986 - acc: 0.7733 - val_loss: 0.5202 - val_acc: 0.8388\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6278 - acc: 0.7969\n",
      "Epoch 00007: val_loss improved from 0.52025 to 0.45218, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_9_conv_checkpoint/007-0.4522.hdf5\n",
      "36805/36805 [==============================] - 28s 750us/sample - loss: 0.6278 - acc: 0.7969 - val_loss: 0.4522 - val_acc: 0.8614\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5594 - acc: 0.8196\n",
      "Epoch 00008: val_loss improved from 0.45218 to 0.40739, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_9_conv_checkpoint/008-0.4074.hdf5\n",
      "36805/36805 [==============================] - 27s 746us/sample - loss: 0.5595 - acc: 0.8195 - val_loss: 0.4074 - val_acc: 0.8705\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5120 - acc: 0.8356\n",
      "Epoch 00009: val_loss improved from 0.40739 to 0.36920, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_9_conv_checkpoint/009-0.3692.hdf5\n",
      "36805/36805 [==============================] - 27s 747us/sample - loss: 0.5119 - acc: 0.8356 - val_loss: 0.3692 - val_acc: 0.8842\n",
      "Epoch 10/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4701 - acc: 0.8482\n",
      "Epoch 00010: val_loss improved from 0.36920 to 0.32512, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_9_conv_checkpoint/010-0.3251.hdf5\n",
      "36805/36805 [==============================] - 27s 746us/sample - loss: 0.4698 - acc: 0.8483 - val_loss: 0.3251 - val_acc: 0.9066\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4317 - acc: 0.8610\n",
      "Epoch 00011: val_loss improved from 0.32512 to 0.29815, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_9_conv_checkpoint/011-0.2981.hdf5\n",
      "36805/36805 [==============================] - 28s 748us/sample - loss: 0.4317 - acc: 0.8610 - val_loss: 0.2981 - val_acc: 0.9087\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3970 - acc: 0.8735\n",
      "Epoch 00012: val_loss improved from 0.29815 to 0.27696, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_9_conv_checkpoint/012-0.2770.hdf5\n",
      "36805/36805 [==============================] - 28s 749us/sample - loss: 0.3970 - acc: 0.8735 - val_loss: 0.2770 - val_acc: 0.9133\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3757 - acc: 0.8792\n",
      "Epoch 00013: val_loss improved from 0.27696 to 0.26282, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_9_conv_checkpoint/013-0.2628.hdf5\n",
      "36805/36805 [==============================] - 27s 745us/sample - loss: 0.3756 - acc: 0.8792 - val_loss: 0.2628 - val_acc: 0.9173\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3500 - acc: 0.8883\n",
      "Epoch 00014: val_loss improved from 0.26282 to 0.24548, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_9_conv_checkpoint/014-0.2455.hdf5\n",
      "36805/36805 [==============================] - 27s 746us/sample - loss: 0.3499 - acc: 0.8882 - val_loss: 0.2455 - val_acc: 0.9250\n",
      "Epoch 15/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3321 - acc: 0.8916\n",
      "Epoch 00015: val_loss improved from 0.24548 to 0.23217, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_9_conv_checkpoint/015-0.2322.hdf5\n",
      "36805/36805 [==============================] - 28s 752us/sample - loss: 0.3319 - acc: 0.8916 - val_loss: 0.2322 - val_acc: 0.9285\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3085 - acc: 0.8997\n",
      "Epoch 00016: val_loss improved from 0.23217 to 0.22997, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_9_conv_checkpoint/016-0.2300.hdf5\n",
      "36805/36805 [==============================] - 27s 747us/sample - loss: 0.3085 - acc: 0.8997 - val_loss: 0.2300 - val_acc: 0.9308\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2962 - acc: 0.9038\n",
      "Epoch 00017: val_loss improved from 0.22997 to 0.20972, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_9_conv_checkpoint/017-0.2097.hdf5\n",
      "36805/36805 [==============================] - 28s 747us/sample - loss: 0.2962 - acc: 0.9038 - val_loss: 0.2097 - val_acc: 0.9343\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2782 - acc: 0.9092\n",
      "Epoch 00018: val_loss did not improve from 0.20972\n",
      "36805/36805 [==============================] - 27s 746us/sample - loss: 0.2781 - acc: 0.9092 - val_loss: 0.2286 - val_acc: 0.9334\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2738 - acc: 0.9118\n",
      "Epoch 00019: val_loss improved from 0.20972 to 0.19195, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_9_conv_checkpoint/019-0.1920.hdf5\n",
      "36805/36805 [==============================] - 28s 748us/sample - loss: 0.2738 - acc: 0.9118 - val_loss: 0.1920 - val_acc: 0.9404\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2561 - acc: 0.9181\n",
      "Epoch 00020: val_loss did not improve from 0.19195\n",
      "36805/36805 [==============================] - 27s 744us/sample - loss: 0.2560 - acc: 0.9181 - val_loss: 0.1965 - val_acc: 0.9415\n",
      "Epoch 21/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2423 - acc: 0.9213\n",
      "Epoch 00021: val_loss did not improve from 0.19195\n",
      "36805/36805 [==============================] - 27s 744us/sample - loss: 0.2424 - acc: 0.9213 - val_loss: 0.2177 - val_acc: 0.9317\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2366 - acc: 0.9235\n",
      "Epoch 00022: val_loss did not improve from 0.19195\n",
      "36805/36805 [==============================] - 27s 747us/sample - loss: 0.2366 - acc: 0.9235 - val_loss: 0.2082 - val_acc: 0.9394\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2247 - acc: 0.9279\n",
      "Epoch 00023: val_loss improved from 0.19195 to 0.17645, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_9_conv_checkpoint/023-0.1764.hdf5\n",
      "36805/36805 [==============================] - 27s 745us/sample - loss: 0.2246 - acc: 0.9279 - val_loss: 0.1764 - val_acc: 0.9464\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2197 - acc: 0.9296\n",
      "Epoch 00024: val_loss improved from 0.17645 to 0.17516, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_9_conv_checkpoint/024-0.1752.hdf5\n",
      "36805/36805 [==============================] - 27s 745us/sample - loss: 0.2197 - acc: 0.9296 - val_loss: 0.1752 - val_acc: 0.9455\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2097 - acc: 0.9331\n",
      "Epoch 00025: val_loss improved from 0.17516 to 0.17309, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_9_conv_checkpoint/025-0.1731.hdf5\n",
      "36805/36805 [==============================] - 28s 747us/sample - loss: 0.2097 - acc: 0.9331 - val_loss: 0.1731 - val_acc: 0.9469\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1931 - acc: 0.9368\n",
      "Epoch 00026: val_loss did not improve from 0.17309\n",
      "36805/36805 [==============================] - 28s 747us/sample - loss: 0.1930 - acc: 0.9368 - val_loss: 0.1813 - val_acc: 0.9460\n",
      "Epoch 27/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1895 - acc: 0.9380\n",
      "Epoch 00027: val_loss improved from 0.17309 to 0.16391, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_9_conv_checkpoint/027-0.1639.hdf5\n",
      "36805/36805 [==============================] - 27s 746us/sample - loss: 0.1894 - acc: 0.9381 - val_loss: 0.1639 - val_acc: 0.9513\n",
      "Epoch 28/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1808 - acc: 0.9405\n",
      "Epoch 00028: val_loss did not improve from 0.16391\n",
      "36805/36805 [==============================] - 27s 744us/sample - loss: 0.1808 - acc: 0.9405 - val_loss: 0.1722 - val_acc: 0.9506\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1794 - acc: 0.9407\n",
      "Epoch 00029: val_loss improved from 0.16391 to 0.16081, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_9_conv_checkpoint/029-0.1608.hdf5\n",
      "36805/36805 [==============================] - 28s 748us/sample - loss: 0.1794 - acc: 0.9407 - val_loss: 0.1608 - val_acc: 0.9506\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1716 - acc: 0.9440\n",
      "Epoch 00030: val_loss did not improve from 0.16081\n",
      "36805/36805 [==============================] - 27s 745us/sample - loss: 0.1716 - acc: 0.9440 - val_loss: 0.1759 - val_acc: 0.9457\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1688 - acc: 0.9449\n",
      "Epoch 00031: val_loss did not improve from 0.16081\n",
      "36805/36805 [==============================] - 28s 748us/sample - loss: 0.1688 - acc: 0.9449 - val_loss: 0.1698 - val_acc: 0.9460\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1563 - acc: 0.9488\n",
      "Epoch 00032: val_loss did not improve from 0.16081\n",
      "36805/36805 [==============================] - 27s 745us/sample - loss: 0.1563 - acc: 0.9488 - val_loss: 0.1681 - val_acc: 0.9471\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1548 - acc: 0.9486\n",
      "Epoch 00033: val_loss improved from 0.16081 to 0.16052, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_9_conv_checkpoint/033-0.1605.hdf5\n",
      "36805/36805 [==============================] - 27s 746us/sample - loss: 0.1548 - acc: 0.9486 - val_loss: 0.1605 - val_acc: 0.9518\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1475 - acc: 0.9510\n",
      "Epoch 00034: val_loss did not improve from 0.16052\n",
      "36805/36805 [==============================] - 27s 744us/sample - loss: 0.1475 - acc: 0.9510 - val_loss: 0.1608 - val_acc: 0.9483\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1450 - acc: 0.9505\n",
      "Epoch 00035: val_loss did not improve from 0.16052\n",
      "36805/36805 [==============================] - 27s 745us/sample - loss: 0.1451 - acc: 0.9505 - val_loss: 0.1637 - val_acc: 0.9515\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1423 - acc: 0.9529\n",
      "Epoch 00036: val_loss did not improve from 0.16052\n",
      "36805/36805 [==============================] - 27s 745us/sample - loss: 0.1423 - acc: 0.9529 - val_loss: 0.1818 - val_acc: 0.9478\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1363 - acc: 0.9538\n",
      "Epoch 00037: val_loss did not improve from 0.16052\n",
      "36805/36805 [==============================] - 27s 743us/sample - loss: 0.1363 - acc: 0.9537 - val_loss: 0.1611 - val_acc: 0.9532\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1305 - acc: 0.9563\n",
      "Epoch 00038: val_loss did not improve from 0.16052\n",
      "36805/36805 [==============================] - 27s 745us/sample - loss: 0.1305 - acc: 0.9563 - val_loss: 0.1909 - val_acc: 0.9462\n",
      "Epoch 39/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1321 - acc: 0.9566\n",
      "Epoch 00039: val_loss improved from 0.16052 to 0.15933, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_9_conv_checkpoint/039-0.1593.hdf5\n",
      "36805/36805 [==============================] - 27s 745us/sample - loss: 0.1322 - acc: 0.9566 - val_loss: 0.1593 - val_acc: 0.9513\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1249 - acc: 0.9577\n",
      "Epoch 00040: val_loss improved from 0.15933 to 0.15879, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_9_conv_checkpoint/040-0.1588.hdf5\n",
      "36805/36805 [==============================] - 27s 745us/sample - loss: 0.1249 - acc: 0.9577 - val_loss: 0.1588 - val_acc: 0.9543\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1232 - acc: 0.9593\n",
      "Epoch 00041: val_loss improved from 0.15879 to 0.14810, saving model to model/checkpoint/1D_CNN_custom_3_ch_32_DO_9_conv_checkpoint/041-0.1481.hdf5\n",
      "36805/36805 [==============================] - 28s 749us/sample - loss: 0.1232 - acc: 0.9594 - val_loss: 0.1481 - val_acc: 0.9553\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1161 - acc: 0.9608\n",
      "Epoch 00042: val_loss did not improve from 0.14810\n",
      "36805/36805 [==============================] - 27s 744us/sample - loss: 0.1161 - acc: 0.9608 - val_loss: 0.1631 - val_acc: 0.9541\n",
      "Epoch 43/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1124 - acc: 0.9617\n",
      "Epoch 00043: val_loss did not improve from 0.14810\n",
      "36805/36805 [==============================] - 27s 746us/sample - loss: 0.1124 - acc: 0.9617 - val_loss: 0.1498 - val_acc: 0.9543\n",
      "Epoch 44/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1145 - acc: 0.9598\n",
      "Epoch 00044: val_loss did not improve from 0.14810\n",
      "36805/36805 [==============================] - 27s 744us/sample - loss: 0.1145 - acc: 0.9597 - val_loss: 0.1487 - val_acc: 0.9534\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1092 - acc: 0.9633\n",
      "Epoch 00045: val_loss did not improve from 0.14810\n",
      "36805/36805 [==============================] - 27s 744us/sample - loss: 0.1092 - acc: 0.9633 - val_loss: 0.1522 - val_acc: 0.9569\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1033 - acc: 0.9653\n",
      "Epoch 00046: val_loss did not improve from 0.14810\n",
      "36805/36805 [==============================] - 27s 744us/sample - loss: 0.1033 - acc: 0.9653 - val_loss: 0.1557 - val_acc: 0.9543\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1011 - acc: 0.9652\n",
      "Epoch 00047: val_loss did not improve from 0.14810\n",
      "36805/36805 [==============================] - 27s 746us/sample - loss: 0.1011 - acc: 0.9652 - val_loss: 0.1884 - val_acc: 0.9492\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1015 - acc: 0.9649\n",
      "Epoch 00048: val_loss did not improve from 0.14810\n",
      "36805/36805 [==============================] - 27s 746us/sample - loss: 0.1015 - acc: 0.9650 - val_loss: 0.1512 - val_acc: 0.9574\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0971 - acc: 0.9667\n",
      "Epoch 00049: val_loss did not improve from 0.14810\n",
      "36805/36805 [==============================] - 27s 743us/sample - loss: 0.0971 - acc: 0.9667 - val_loss: 0.1491 - val_acc: 0.9585\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0921 - acc: 0.9678\n",
      "Epoch 00050: val_loss did not improve from 0.14810\n",
      "36805/36805 [==============================] - 27s 745us/sample - loss: 0.0921 - acc: 0.9678 - val_loss: 0.1590 - val_acc: 0.9588\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0930 - acc: 0.9673\n",
      "Epoch 00051: val_loss did not improve from 0.14810\n",
      "36805/36805 [==============================] - 27s 744us/sample - loss: 0.0930 - acc: 0.9673 - val_loss: 0.1708 - val_acc: 0.9539\n",
      "Epoch 52/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0848 - acc: 0.9713\n",
      "Epoch 00052: val_loss did not improve from 0.14810\n",
      "36805/36805 [==============================] - 27s 744us/sample - loss: 0.0848 - acc: 0.9713 - val_loss: 0.1544 - val_acc: 0.9562\n",
      "Epoch 53/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0875 - acc: 0.9708\n",
      "Epoch 00053: val_loss did not improve from 0.14810\n",
      "36805/36805 [==============================] - 27s 746us/sample - loss: 0.0873 - acc: 0.9708 - val_loss: 0.1576 - val_acc: 0.9583\n",
      "Epoch 54/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0834 - acc: 0.9718\n",
      "Epoch 00054: val_loss did not improve from 0.14810\n",
      "36805/36805 [==============================] - 27s 744us/sample - loss: 0.0836 - acc: 0.9717 - val_loss: 0.1632 - val_acc: 0.9555\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0889 - acc: 0.9697\n",
      "Epoch 00055: val_loss did not improve from 0.14810\n",
      "36805/36805 [==============================] - 27s 746us/sample - loss: 0.0889 - acc: 0.9697 - val_loss: 0.1592 - val_acc: 0.9602\n",
      "Epoch 56/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0792 - acc: 0.9728\n",
      "Epoch 00056: val_loss did not improve from 0.14810\n",
      "36805/36805 [==============================] - 27s 745us/sample - loss: 0.0791 - acc: 0.9728 - val_loss: 0.1711 - val_acc: 0.9536\n",
      "Epoch 57/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0763 - acc: 0.9739\n",
      "Epoch 00057: val_loss did not improve from 0.14810\n",
      "36805/36805 [==============================] - 27s 745us/sample - loss: 0.0764 - acc: 0.9739 - val_loss: 0.1667 - val_acc: 0.9571\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0794 - acc: 0.9721\n",
      "Epoch 00058: val_loss did not improve from 0.14810\n",
      "36805/36805 [==============================] - 27s 744us/sample - loss: 0.0794 - acc: 0.9721 - val_loss: 0.1504 - val_acc: 0.9581\n",
      "Epoch 59/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0790 - acc: 0.9724\n",
      "Epoch 00059: val_loss did not improve from 0.14810\n",
      "36805/36805 [==============================] - 27s 745us/sample - loss: 0.0791 - acc: 0.9723 - val_loss: 0.1512 - val_acc: 0.9595\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0740 - acc: 0.9747\n",
      "Epoch 00060: val_loss did not improve from 0.14810\n",
      "36805/36805 [==============================] - 27s 745us/sample - loss: 0.0740 - acc: 0.9747 - val_loss: 0.1724 - val_acc: 0.9564\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0719 - acc: 0.9737\n",
      "Epoch 00061: val_loss did not improve from 0.14810\n",
      "36805/36805 [==============================] - 27s 745us/sample - loss: 0.0719 - acc: 0.9737 - val_loss: 0.1540 - val_acc: 0.9597\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0691 - acc: 0.9755\n",
      "Epoch 00062: val_loss did not improve from 0.14810\n",
      "36805/36805 [==============================] - 27s 746us/sample - loss: 0.0691 - acc: 0.9755 - val_loss: 0.1761 - val_acc: 0.9553\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0668 - acc: 0.9771\n",
      "Epoch 00063: val_loss did not improve from 0.14810\n",
      "36805/36805 [==============================] - 27s 743us/sample - loss: 0.0667 - acc: 0.9771 - val_loss: 0.1629 - val_acc: 0.9606\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0677 - acc: 0.9767\n",
      "Epoch 00064: val_loss did not improve from 0.14810\n",
      "36805/36805 [==============================] - 27s 744us/sample - loss: 0.0677 - acc: 0.9767 - val_loss: 0.1726 - val_acc: 0.9588\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0624 - acc: 0.9785\n",
      "Epoch 00065: val_loss did not improve from 0.14810\n",
      "36805/36805 [==============================] - 27s 743us/sample - loss: 0.0624 - acc: 0.9785 - val_loss: 0.1681 - val_acc: 0.9557\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0645 - acc: 0.9775\n",
      "Epoch 00066: val_loss did not improve from 0.14810\n",
      "36805/36805 [==============================] - 28s 747us/sample - loss: 0.0645 - acc: 0.9775 - val_loss: 0.1576 - val_acc: 0.9604\n",
      "Epoch 67/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0620 - acc: 0.9788\n",
      "Epoch 00067: val_loss did not improve from 0.14810\n",
      "36805/36805 [==============================] - 27s 744us/sample - loss: 0.0620 - acc: 0.9788 - val_loss: 0.1847 - val_acc: 0.9557\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0618 - acc: 0.9792\n",
      "Epoch 00068: val_loss did not improve from 0.14810\n",
      "36805/36805 [==============================] - 27s 746us/sample - loss: 0.0619 - acc: 0.9791 - val_loss: 0.2004 - val_acc: 0.9553\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0671 - acc: 0.9766\n",
      "Epoch 00069: val_loss did not improve from 0.14810\n",
      "36805/36805 [==============================] - 27s 741us/sample - loss: 0.0672 - acc: 0.9766 - val_loss: 0.1785 - val_acc: 0.9564\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0650 - acc: 0.9779\n",
      "Epoch 00070: val_loss did not improve from 0.14810\n",
      "36805/36805 [==============================] - 27s 746us/sample - loss: 0.0650 - acc: 0.9779 - val_loss: 0.1741 - val_acc: 0.9595\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0568 - acc: 0.9803\n",
      "Epoch 00071: val_loss did not improve from 0.14810\n",
      "36805/36805 [==============================] - 27s 743us/sample - loss: 0.0568 - acc: 0.9803 - val_loss: 0.1898 - val_acc: 0.9567\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0610 - acc: 0.9793\n",
      "Epoch 00072: val_loss did not improve from 0.14810\n",
      "36805/36805 [==============================] - 27s 746us/sample - loss: 0.0610 - acc: 0.9793 - val_loss: 0.1588 - val_acc: 0.9609\n",
      "Epoch 73/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0537 - acc: 0.9819\n",
      "Epoch 00073: val_loss did not improve from 0.14810\n",
      "36805/36805 [==============================] - 27s 743us/sample - loss: 0.0538 - acc: 0.9819 - val_loss: 0.1685 - val_acc: 0.9616\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0521 - acc: 0.9821\n",
      "Epoch 00074: val_loss did not improve from 0.14810\n",
      "36805/36805 [==============================] - 27s 743us/sample - loss: 0.0521 - acc: 0.9821 - val_loss: 0.1805 - val_acc: 0.9588\n",
      "Epoch 75/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0522 - acc: 0.9820\n",
      "Epoch 00075: val_loss did not improve from 0.14810\n",
      "36805/36805 [==============================] - 28s 751us/sample - loss: 0.0523 - acc: 0.9820 - val_loss: 0.1727 - val_acc: 0.9595\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0532 - acc: 0.9815\n",
      "Epoch 00076: val_loss did not improve from 0.14810\n",
      "36805/36805 [==============================] - 28s 748us/sample - loss: 0.0532 - acc: 0.9815 - val_loss: 0.1675 - val_acc: 0.9604\n",
      "Epoch 77/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0515 - acc: 0.9829\n",
      "Epoch 00077: val_loss did not improve from 0.14810\n",
      "36805/36805 [==============================] - 27s 747us/sample - loss: 0.0514 - acc: 0.9829 - val_loss: 0.1813 - val_acc: 0.9599\n",
      "Epoch 78/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0523 - acc: 0.9812\n",
      "Epoch 00078: val_loss did not improve from 0.14810\n",
      "36805/36805 [==============================] - 27s 745us/sample - loss: 0.0522 - acc: 0.9813 - val_loss: 0.1764 - val_acc: 0.9597\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0490 - acc: 0.9830\n",
      "Epoch 00079: val_loss did not improve from 0.14810\n",
      "36805/36805 [==============================] - 27s 744us/sample - loss: 0.0489 - acc: 0.9830 - val_loss: 0.1627 - val_acc: 0.9641\n",
      "Epoch 80/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0470 - acc: 0.9842\n",
      "Epoch 00080: val_loss did not improve from 0.14810\n",
      "36805/36805 [==============================] - 27s 743us/sample - loss: 0.0470 - acc: 0.9842 - val_loss: 0.1784 - val_acc: 0.9585\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0478 - acc: 0.9829\n",
      "Epoch 00081: val_loss did not improve from 0.14810\n",
      "36805/36805 [==============================] - 27s 744us/sample - loss: 0.0477 - acc: 0.9829 - val_loss: 0.1852 - val_acc: 0.9576\n",
      "Epoch 82/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0486 - acc: 0.9822\n",
      "Epoch 00082: val_loss did not improve from 0.14810\n",
      "36805/36805 [==============================] - 27s 742us/sample - loss: 0.0487 - acc: 0.9821 - val_loss: 0.1815 - val_acc: 0.9604\n",
      "Epoch 83/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0448 - acc: 0.9849\n",
      "Epoch 00083: val_loss did not improve from 0.14810\n",
      "36805/36805 [==============================] - 27s 745us/sample - loss: 0.0447 - acc: 0.9849 - val_loss: 0.1748 - val_acc: 0.9618\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0465 - acc: 0.9833\n",
      "Epoch 00084: val_loss did not improve from 0.14810\n",
      "36805/36805 [==============================] - 27s 744us/sample - loss: 0.0465 - acc: 0.9833 - val_loss: 0.1791 - val_acc: 0.9611\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0474 - acc: 0.9835\n",
      "Epoch 00085: val_loss did not improve from 0.14810\n",
      "36805/36805 [==============================] - 27s 745us/sample - loss: 0.0474 - acc: 0.9835 - val_loss: 0.1752 - val_acc: 0.9632\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0503 - acc: 0.9829\n",
      "Epoch 00086: val_loss did not improve from 0.14810\n",
      "36805/36805 [==============================] - 27s 743us/sample - loss: 0.0503 - acc: 0.9829 - val_loss: 0.1701 - val_acc: 0.9613\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0407 - acc: 0.9859\n",
      "Epoch 00087: val_loss did not improve from 0.14810\n",
      "36805/36805 [==============================] - 27s 744us/sample - loss: 0.0407 - acc: 0.9859 - val_loss: 0.1868 - val_acc: 0.9616\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0468 - acc: 0.9835\n",
      "Epoch 00088: val_loss did not improve from 0.14810\n",
      "36805/36805 [==============================] - 27s 742us/sample - loss: 0.0468 - acc: 0.9835 - val_loss: 0.1714 - val_acc: 0.9595\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0423 - acc: 0.9852\n",
      "Epoch 00089: val_loss did not improve from 0.14810\n",
      "36805/36805 [==============================] - 27s 744us/sample - loss: 0.0423 - acc: 0.9852 - val_loss: 0.2372 - val_acc: 0.9520\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0472 - acc: 0.9837\n",
      "Epoch 00090: val_loss did not improve from 0.14810\n",
      "36805/36805 [==============================] - 27s 745us/sample - loss: 0.0472 - acc: 0.9837 - val_loss: 0.1816 - val_acc: 0.9618\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0438 - acc: 0.9851\n",
      "Epoch 00091: val_loss did not improve from 0.14810\n",
      "36805/36805 [==============================] - 27s 742us/sample - loss: 0.0438 - acc: 0.9851 - val_loss: 0.1874 - val_acc: 0.9616\n",
      "\n",
      "1D_CNN_custom_3_ch_32_DO_9_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 411us/sample - loss: 0.2086 - acc: 0.9406\n",
      "Loss: 0.20855384672375885 Accuracy: 0.9406023\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(4, 10):\n",
    "    base = '1D_CNN_custom_3_ch_32_DO'\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    model = build_1d_cnn_custom_DO(conv_num=i)\n",
    "#         model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4),\n",
    "          metrics=['accuracy'])\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model_filename = model_path+'{epoch:03d}-{val_loss:.4f}.hdf5'\n",
    "    checkpointer = ModelCheckpoint(filepath = model_filename, monitor = \"val_loss\", \n",
    "                                   verbose=1, save_best_only=True)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=50)\n",
    "    hist = model.fit(x_train_abs, y_train_onehot, batch_size=64, epochs=500, \n",
    "                     validation_data=[x_val_abs, y_val_onehot], shuffle=True, \n",
    "                     callbacks = [checkpointer, early_stopping])\n",
    "\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "    ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "    ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "    ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    png_path = 'visualization/learning_curve/'\n",
    "    filename = model_name+'.png'\n",
    "    os.makedirs(png_path, exist_ok=True)\n",
    "    fig.savefig(png_path+filename, transparent=True)\n",
    "\n",
    "    model.save(model_path+'000_last.hdf5')\n",
    "    del(model)\n",
    "    \n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "    model = load_model(model_filename)\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "    print()\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_3_ch_32_DO_4_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_45 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "conv1d_46 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_47 (Conv1D)           (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_48 (Conv1D)           (None, 1777, 64)          10304     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 16)                606224    \n",
      "=================================================================\n",
      "Total params: 627,024\n",
      "Trainable params: 627,024\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 2s 396us/sample - loss: 1.1983 - acc: 0.6339\n",
      "Loss: 1.1983275781168001 Accuracy: 0.63385254\n",
      "\n",
      "1D_CNN_custom_3_ch_32_DO_5_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_49 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "conv1d_50 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_51 (Conv1D)           (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_52 (Conv1D)           (None, 1777, 64)          10304     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_53 (Conv1D)           (None, 592, 64)           20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 12608)             0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 12608)             0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 16)                201744    \n",
      "=================================================================\n",
      "Total params: 243,088\n",
      "Trainable params: 243,088\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 2s 405us/sample - loss: 0.8117 - acc: 0.7701\n",
      "Loss: 0.8116677548655097 Accuracy: 0.77009344\n",
      "\n",
      "1D_CNN_custom_3_ch_32_DO_6_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_54 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "conv1d_55 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_56 (Conv1D)           (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_57 (Conv1D)           (None, 1777, 64)          10304     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_58 (Conv1D)           (None, 592, 64)           20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_59 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 4160)              0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 4160)              0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 16)                66576     \n",
      "=================================================================\n",
      "Total params: 128,464\n",
      "Trainable params: 128,464\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 2s 418us/sample - loss: 0.4294 - acc: 0.8856\n",
      "Loss: 0.42937957885233163 Accuracy: 0.88556594\n",
      "\n",
      "1D_CNN_custom_3_ch_32_DO_7_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_60 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "conv1d_61 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_62 (Conv1D)           (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_63 (Conv1D)           (None, 1777, 64)          10304     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_64 (Conv1D)           (None, 592, 64)           20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_65 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_66 (Conv1D)           (None, 65, 128)           41088     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 16)                43024     \n",
      "=================================================================\n",
      "Total params: 146,000\n",
      "Trainable params: 146,000\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 431us/sample - loss: 0.2336 - acc: 0.9319\n",
      "Loss: 0.23356248641682562 Accuracy: 0.9318795\n",
      "\n",
      "1D_CNN_custom_3_ch_32_DO_8_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_67 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "conv1d_68 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_69 (Conv1D)           (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_70 (Conv1D)           (None, 1777, 64)          10304     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_71 (Conv1D)           (None, 592, 64)           20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_72 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_73 (Conv1D)           (None, 65, 128)           41088     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_74 (Conv1D)           (None, 21, 128)           82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 16)                14352     \n",
      "=================================================================\n",
      "Total params: 199,376\n",
      "Trainable params: 199,376\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 2s 448us/sample - loss: 0.1771 - acc: 0.9479\n",
      "Loss: 0.17710612370155807 Accuracy: 0.9478712\n",
      "\n",
      "1D_CNN_custom_3_ch_32_DO_9_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_75 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "conv1d_76 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_77 (Conv1D)           (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_78 (Conv1D)           (None, 1777, 64)          10304     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_79 (Conv1D)           (None, 592, 64)           20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_80 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_81 (Conv1D)           (None, 65, 128)           41088     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_66 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_82 (Conv1D)           (None, 21, 128)           82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_67 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_83 (Conv1D)           (None, 7, 128)            82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_68 (MaxPooling (None, 2, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_14 (Flatten)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 16)                4112      \n",
      "=================================================================\n",
      "Total params: 271,184\n",
      "Trainable params: 271,184\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 2s 451us/sample - loss: 0.2086 - acc: 0.9406\n",
      "Loss: 0.20855384672375885 Accuracy: 0.9406023\n"
     ]
    }
   ],
   "source": [
    "log_dir = 'log'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "base = '1D_CNN_custom_3_ch_32_DO'\n",
    "\n",
    "with open(path.join(log_dir, base), 'w') as log_file:\n",
    "    for i in range(4, 10):\n",
    "        model_name = base+'_{}_conv'.format(i)\n",
    "        print()\n",
    "        print(model_name, 'Model')\n",
    "        model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "        model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "\n",
    "        model = load_model(model_filename)\n",
    "        model.summary()\n",
    "\n",
    "        [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "        print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "        del(model)\n",
    "\n",
    "        log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_3_ch_32_DO_4_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_45 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "conv1d_46 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_47 (Conv1D)           (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_48 (Conv1D)           (None, 1777, 64)          10304     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 16)                606224    \n",
      "=================================================================\n",
      "Total params: 627,024\n",
      "Trainable params: 627,024\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 2s 438us/sample - loss: 1.6124 - acc: 0.6845\n",
      "Loss: 1.612410186631905 Accuracy: 0.6845275\n",
      "\n",
      "1D_CNN_custom_3_ch_32_DO_5_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_49 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "conv1d_50 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_51 (Conv1D)           (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_52 (Conv1D)           (None, 1777, 64)          10304     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_53 (Conv1D)           (None, 592, 64)           20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 12608)             0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 12608)             0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 16)                201744    \n",
      "=================================================================\n",
      "Total params: 243,088\n",
      "Trainable params: 243,088\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 2s 452us/sample - loss: 0.9544 - acc: 0.7915\n",
      "Loss: 0.9543717136264219 Accuracy: 0.79148495\n",
      "\n",
      "1D_CNN_custom_3_ch_32_DO_6_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_54 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "conv1d_55 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_56 (Conv1D)           (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_57 (Conv1D)           (None, 1777, 64)          10304     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_58 (Conv1D)           (None, 592, 64)           20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_59 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 4160)              0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 4160)              0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 16)                66576     \n",
      "=================================================================\n",
      "Total params: 128,464\n",
      "Trainable params: 128,464\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 2s 464us/sample - loss: 0.4803 - acc: 0.8841\n",
      "Loss: 0.4803239729669359 Accuracy: 0.8841122\n",
      "\n",
      "1D_CNN_custom_3_ch_32_DO_7_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_60 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "conv1d_61 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_62 (Conv1D)           (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_63 (Conv1D)           (None, 1777, 64)          10304     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_64 (Conv1D)           (None, 592, 64)           20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_65 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_66 (Conv1D)           (None, 65, 128)           41088     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 16)                43024     \n",
      "=================================================================\n",
      "Total params: 146,000\n",
      "Trainable params: 146,000\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 474us/sample - loss: 0.2505 - acc: 0.9381\n",
      "Loss: 0.25048343313941085 Accuracy: 0.93811005\n",
      "\n",
      "1D_CNN_custom_3_ch_32_DO_8_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_67 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "conv1d_68 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_69 (Conv1D)           (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_70 (Conv1D)           (None, 1777, 64)          10304     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_71 (Conv1D)           (None, 592, 64)           20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_72 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_73 (Conv1D)           (None, 65, 128)           41088     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_74 (Conv1D)           (None, 21, 128)           82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 16)                14352     \n",
      "=================================================================\n",
      "Total params: 199,376\n",
      "Trainable params: 199,376\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 2s 502us/sample - loss: 0.1929 - acc: 0.9522\n",
      "Loss: 0.19287333804398374 Accuracy: 0.9522326\n",
      "\n",
      "1D_CNN_custom_3_ch_32_DO_9_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_75 (Conv1D)           (None, 16000, 32)         192       \n",
      "_________________________________________________________________\n",
      "conv1d_76 (Conv1D)           (None, 16000, 32)         5152      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling (None, 5333, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_77 (Conv1D)           (None, 5333, 32)          5152      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling (None, 1777, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_78 (Conv1D)           (None, 1777, 64)          10304     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_79 (Conv1D)           (None, 592, 64)           20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling (None, 197, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_80 (Conv1D)           (None, 197, 64)           20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling (None, 65, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_81 (Conv1D)           (None, 65, 128)           41088     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_66 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_82 (Conv1D)           (None, 21, 128)           82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_67 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_83 (Conv1D)           (None, 7, 128)            82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_68 (MaxPooling (None, 2, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_14 (Flatten)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 16)                4112      \n",
      "=================================================================\n",
      "Total params: 271,184\n",
      "Trainable params: 271,184\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 2s 498us/sample - loss: 0.2349 - acc: 0.9477\n",
      "Loss: 0.23493513631104804 Accuracy: 0.94766355\n"
     ]
    }
   ],
   "source": [
    "# log_dir = 'log'\n",
    "# os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# with open(path.join(log_dir, base), 'w') as log_file:\n",
    "for i in range(4, 10):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + '000_last.hdf5'\n",
    "\n",
    "    model = load_model(model_filename)\n",
    "    model.summary()\n",
    "\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "    del(model)\n",
    "\n",
    "#         log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
