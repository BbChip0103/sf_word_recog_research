{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path as path\n",
    "import itertools\n",
    "from sklearn.preprocessing import maxabs_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Input, Dense, Activation, BatchNormalization, \\\n",
    "                                    Flatten, Conv1D, MaxPooling1D, Dropout, \\\n",
    "                                    Concatenate, GlobalMaxPool1D, GlobalAvgPool1D\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler, \\\n",
    "                                        EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = path.join('..', 'data')\n",
    "data_dir = path.join(base_dir, 'data_speech_commands_v0.02')\n",
    " \n",
    "train_txt = path.join(data_dir, 'wav_train_16words.txt')\n",
    "val_txt = path.join(data_dir, 'wav_validation_16words.txt')\n",
    "test_txt = path.join(data_dir, 'wav_test_16words.txt')\n",
    "\n",
    "train_data = np.load(path.join(data_dir, 'wav_train_data.npz'))\n",
    "val_data = np.load(path.join(data_dir, 'wav_validation_data.npz'))\n",
    "test_data = np.load(path.join(data_dir, 'wav_test_data.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36805, 16000, 1),\n",
       " (36805,),\n",
       " (4293, 16000, 1),\n",
       " (4293,),\n",
       " (4815, 16000, 1),\n",
       " (4815,),\n",
       " (16, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = train_data['x_train']\n",
    "y_train = train_data['y_train']\n",
    "x_val = val_data['x_val']\n",
    "y_val = val_data['y_val']\n",
    "x_test = test_data['x_test']\n",
    "y_test = test_data['y_test']\n",
    "y_table = test_data['table']\n",
    "\n",
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape, y_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_test[0].shape\n",
    "output_size = y_table.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_abs = np.asarray([maxabs_scale(wav) for wav in x_train])\n",
    "y_train_onehot = np.asarray([to_categorical(label, output_size) for label in y_train])\n",
    "del x_train, y_train\n",
    "\n",
    "x_val_abs = np.asarray([maxabs_scale(wav) for wav in x_val])\n",
    "y_val_onehot = np.asarray([to_categorical(label, output_size) for label in y_val])\n",
    "del x_val, y_val\n",
    "\n",
    "x_test_abs = np.asarray([maxabs_scale(wav) for wav in x_test])\n",
    "y_test_onehot = np.asarray([to_categorical(label, output_size) for label in y_test])\n",
    "del x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn(conv_num=1):\n",
    "    filter_size = 32\n",
    "\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    x = input_layer\n",
    "\n",
    "    layer_outputs = []\n",
    "    for i in range(conv_num):\n",
    "        x = Conv1D (kernel_size=5, filters=filter_size*(2**(i//4)), \n",
    "                          strides=1, padding='same')(x)\n",
    "        x = Activation('relu')(x)\n",
    "        x = MaxPooling1D(pool_size=3, strides=3)(x)\n",
    "        layer_outputs.append(x)    \n",
    "    \n",
    "    x = Concatenate()([Flatten()(output) for output in layer_outputs[-3:]])\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(output_size, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 16000, 32)    192         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 16000, 32)    0           conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D)    (None, 5333, 32)     0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 5333, 32)     5152        max_pooling1d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 5333, 32)     0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 1777, 32)     0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 1777, 32)     5152        max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 1777, 32)     0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 592, 32)      0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 170656)       0           max_pooling1d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 56864)        0           max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 18944)        0           max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 246464)       0           flatten[0][0]                    \n",
      "                                                                 flatten_1[0][0]                  \n",
      "                                                                 flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 246464)       0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 16)           3943440     dropout[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 3,953,936\n",
      "Trainable params: 3,953,936\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 16000, 32)    192         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 16000, 32)    0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 5333, 32)     0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 5333, 32)     5152        max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 5333, 32)     0           conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 1777, 32)     0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 1777, 32)     5152        max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 1777, 32)     0           conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 592, 32)      0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 592, 32)      5152        max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 592, 32)      0           conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1D)  (None, 197, 32)      0           activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 56864)        0           max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 18944)        0           max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, 6304)         0           max_pooling1d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 82112)        0           flatten_3[0][0]                  \n",
      "                                                                 flatten_4[0][0]                  \n",
      "                                                                 flatten_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 82112)        0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 16)           1313808     dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,329,456\n",
      "Trainable params: 1,329,456\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 16000, 32)    192         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 16000, 32)    0           conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1D)  (None, 5333, 32)     0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 5333, 32)     5152        max_pooling1d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 5333, 32)     0           conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1D)  (None, 1777, 32)     0           activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 1777, 32)     5152        max_pooling1d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 1777, 32)     0           conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1D)  (None, 592, 32)      0           activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 592, 32)      0           conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling1D) (None, 197, 32)      0           activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 197, 64)      0           conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling1D) (None, 65, 64)       0           activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_6 (Flatten)             (None, 18944)        0           max_pooling1d_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_7 (Flatten)             (None, 6304)         0           max_pooling1d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_8 (Flatten)             (None, 4160)         0           max_pooling1d_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 29408)        0           flatten_6[0][0]                  \n",
      "                                                                 flatten_7[0][0]                  \n",
      "                                                                 flatten_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 29408)        0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 16)           470544      dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 496,496\n",
      "Trainable params: 496,496\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 16000, 32)    192         input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 16000, 32)    0           conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling1D) (None, 5333, 32)     0           activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 5333, 32)     0           conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling1D) (None, 1777, 32)     0           activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_13[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 1777, 32)     0           conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling1D) (None, 592, 32)      0           activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_14[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 592, 32)      0           conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling1D) (None, 197, 32)      0           activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_15[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 197, 64)      0           conv1d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling1D) (None, 65, 64)       0           activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_17 (Conv1D)              (None, 65, 64)       20544       max_pooling1d_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 65, 64)       0           conv1d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling1D) (None, 21, 64)       0           activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_9 (Flatten)             (None, 6304)         0           max_pooling1d_15[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_10 (Flatten)            (None, 4160)         0           max_pooling1d_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_11 (Flatten)            (None, 1344)         0           max_pooling1d_17[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 11808)        0           flatten_9[0][0]                  \n",
      "                                                                 flatten_10[0][0]                 \n",
      "                                                                 flatten_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 11808)        0           concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 16)           188944      dropout_3[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 235,440\n",
      "Trainable params: 235,440\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_18 (Conv1D)              (None, 16000, 32)    192         input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 16000, 32)    0           conv1d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling1D) (None, 5333, 32)     0           activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_19 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_18[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 5333, 32)     0           conv1d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling1D) (None, 1777, 32)     0           activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_20 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_19[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 1777, 32)     0           conv1d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling1D) (None, 592, 32)      0           activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_21 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_20[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 592, 32)      0           conv1d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling1D) (None, 197, 32)      0           activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_22 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_21[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 197, 64)      0           conv1d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling1D) (None, 65, 64)       0           activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_23 (Conv1D)              (None, 65, 64)       20544       max_pooling1d_22[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 65, 64)       0           conv1d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling1D) (None, 21, 64)       0           activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_24 (Conv1D)              (None, 21, 64)       20544       max_pooling1d_23[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 21, 64)       0           conv1d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling1D) (None, 7, 64)        0           activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_12 (Flatten)            (None, 4160)         0           max_pooling1d_22[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_13 (Flatten)            (None, 1344)         0           max_pooling1d_23[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_14 (Flatten)            (None, 448)          0           max_pooling1d_24[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 5952)         0           flatten_12[0][0]                 \n",
      "                                                                 flatten_13[0][0]                 \n",
      "                                                                 flatten_14[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 5952)         0           concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 16)           95248       dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 162,288\n",
      "Trainable params: 162,288\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_25 (Conv1D)              (None, 16000, 32)    192         input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 16000, 32)    0           conv1d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling1D) (None, 5333, 32)     0           activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_26 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_25[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 5333, 32)     0           conv1d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_26 (MaxPooling1D) (None, 1777, 32)     0           activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_27 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_26[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 1777, 32)     0           conv1d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_27 (MaxPooling1D) (None, 592, 32)      0           activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_28 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_27[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 592, 32)      0           conv1d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_28 (MaxPooling1D) (None, 197, 32)      0           activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_29 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_28[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 197, 64)      0           conv1d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_29 (MaxPooling1D) (None, 65, 64)       0           activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_30 (Conv1D)              (None, 65, 64)       20544       max_pooling1d_29[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 65, 64)       0           conv1d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_30 (MaxPooling1D) (None, 21, 64)       0           activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_31 (Conv1D)              (None, 21, 64)       20544       max_pooling1d_30[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 21, 64)       0           conv1d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_31 (MaxPooling1D) (None, 7, 64)        0           activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_32 (Conv1D)              (None, 7, 64)        20544       max_pooling1d_31[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 7, 64)        0           conv1d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_32 (MaxPooling1D) (None, 2, 64)        0           activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_15 (Flatten)            (None, 1344)         0           max_pooling1d_30[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_16 (Flatten)            (None, 448)          0           max_pooling1d_31[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_17 (Flatten)            (None, 128)          0           max_pooling1d_32[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 1920)         0           flatten_15[0][0]                 \n",
      "                                                                 flatten_16[0][0]                 \n",
      "                                                                 flatten_17[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 1920)         0           concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 16)           30736       dropout_5[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 118,320\n",
      "Trainable params: 118,320\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in range(3, 9):\n",
    "    model = build_cnn(conv_num=i)\n",
    "    model.summary()\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36805 samples, validate on 4293 samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.1502 - acc: 0.3295\n",
      "Epoch 00001: val_loss improved from inf to 1.74915, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_3_conv_checkpoint/001-1.7492.hdf5\n",
      "36805/36805 [==============================] - 23s 624us/sample - loss: 2.1502 - acc: 0.3295 - val_loss: 1.7492 - val_acc: 0.4708\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5548 - acc: 0.5204\n",
      "Epoch 00002: val_loss improved from 1.74915 to 1.56127, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_3_conv_checkpoint/002-1.5613.hdf5\n",
      "36805/36805 [==============================] - 20s 550us/sample - loss: 1.5548 - acc: 0.5203 - val_loss: 1.5613 - val_acc: 0.5027\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3276 - acc: 0.5904\n",
      "Epoch 00003: val_loss improved from 1.56127 to 1.49572, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_3_conv_checkpoint/003-1.4957.hdf5\n",
      "36805/36805 [==============================] - 20s 555us/sample - loss: 1.3276 - acc: 0.5904 - val_loss: 1.4957 - val_acc: 0.5241\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1877 - acc: 0.6338\n",
      "Epoch 00004: val_loss did not improve from 1.49572\n",
      "36805/36805 [==============================] - 21s 562us/sample - loss: 1.1877 - acc: 0.6338 - val_loss: 1.5171 - val_acc: 0.5243\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0813 - acc: 0.6682\n",
      "Epoch 00005: val_loss improved from 1.49572 to 1.45568, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_3_conv_checkpoint/005-1.4557.hdf5\n",
      "36805/36805 [==============================] - 21s 561us/sample - loss: 1.0813 - acc: 0.6683 - val_loss: 1.4557 - val_acc: 0.5413\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9832 - acc: 0.7016\n",
      "Epoch 00006: val_loss did not improve from 1.45568\n",
      "36805/36805 [==============================] - 21s 564us/sample - loss: 0.9831 - acc: 0.7016 - val_loss: 1.4958 - val_acc: 0.5278\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8990 - acc: 0.7250\n",
      "Epoch 00007: val_loss did not improve from 1.45568\n",
      "36805/36805 [==============================] - 21s 560us/sample - loss: 0.8990 - acc: 0.7250 - val_loss: 1.4594 - val_acc: 0.5465\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8278 - acc: 0.7511- ETA: 0s - loss: 0.8255 - \n",
      "Epoch 00008: val_loss did not improve from 1.45568\n",
      "36805/36805 [==============================] - 21s 558us/sample - loss: 0.8278 - acc: 0.7511 - val_loss: 1.5055 - val_acc: 0.5381\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7661 - acc: 0.7691\n",
      "Epoch 00009: val_loss did not improve from 1.45568\n",
      "36805/36805 [==============================] - 21s 558us/sample - loss: 0.7661 - acc: 0.7691 - val_loss: 1.4799 - val_acc: 0.5497\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6997 - acc: 0.7913\n",
      "Epoch 00010: val_loss did not improve from 1.45568\n",
      "36805/36805 [==============================] - 21s 558us/sample - loss: 0.6997 - acc: 0.7914 - val_loss: 1.5460 - val_acc: 0.5413\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6492 - acc: 0.8071\n",
      "Epoch 00011: val_loss did not improve from 1.45568\n",
      "36805/36805 [==============================] - 21s 561us/sample - loss: 0.6491 - acc: 0.8071 - val_loss: 1.5360 - val_acc: 0.5441\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5963 - acc: 0.8231\n",
      "Epoch 00012: val_loss did not improve from 1.45568\n",
      "36805/36805 [==============================] - 21s 559us/sample - loss: 0.5964 - acc: 0.8230 - val_loss: 1.5283 - val_acc: 0.5560\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5517 - acc: 0.8368\n",
      "Epoch 00013: val_loss did not improve from 1.45568\n",
      "36805/36805 [==============================] - 21s 561us/sample - loss: 0.5517 - acc: 0.8368 - val_loss: 1.5511 - val_acc: 0.5616\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5084 - acc: 0.8518\n",
      "Epoch 00014: val_loss did not improve from 1.45568\n",
      "36805/36805 [==============================] - 21s 562us/sample - loss: 0.5084 - acc: 0.8518 - val_loss: 1.5681 - val_acc: 0.5646\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4713 - acc: 0.8643\n",
      "Epoch 00015: val_loss did not improve from 1.45568\n",
      "36805/36805 [==============================] - 21s 557us/sample - loss: 0.4713 - acc: 0.8643 - val_loss: 1.5959 - val_acc: 0.5632\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4378 - acc: 0.8726\n",
      "Epoch 00016: val_loss did not improve from 1.45568\n",
      "36805/36805 [==============================] - 21s 561us/sample - loss: 0.4379 - acc: 0.8725 - val_loss: 1.6403 - val_acc: 0.5609\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4112 - acc: 0.8818\n",
      "Epoch 00017: val_loss did not improve from 1.45568\n",
      "36805/36805 [==============================] - 21s 560us/sample - loss: 0.4112 - acc: 0.8818 - val_loss: 1.6221 - val_acc: 0.5700\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3755 - acc: 0.8914\n",
      "Epoch 00018: val_loss did not improve from 1.45568\n",
      "36805/36805 [==============================] - 21s 560us/sample - loss: 0.3755 - acc: 0.8915 - val_loss: 1.6903 - val_acc: 0.5600\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3570 - acc: 0.8970\n",
      "Epoch 00019: val_loss did not improve from 1.45568\n",
      "36805/36805 [==============================] - 21s 559us/sample - loss: 0.3571 - acc: 0.8969 - val_loss: 1.6819 - val_acc: 0.5679\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3371 - acc: 0.9044\n",
      "Epoch 00020: val_loss did not improve from 1.45568\n",
      "36805/36805 [==============================] - 21s 562us/sample - loss: 0.3371 - acc: 0.9043 - val_loss: 1.7067 - val_acc: 0.5712\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3142 - acc: 0.9115\n",
      "Epoch 00021: val_loss did not improve from 1.45568\n",
      "36805/36805 [==============================] - 21s 563us/sample - loss: 0.3142 - acc: 0.9115 - val_loss: 1.7295 - val_acc: 0.5674\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2910 - acc: 0.9184\n",
      "Epoch 00022: val_loss did not improve from 1.45568\n",
      "36805/36805 [==============================] - 21s 559us/sample - loss: 0.2910 - acc: 0.9184 - val_loss: 1.7225 - val_acc: 0.5749\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2767 - acc: 0.9218\n",
      "Epoch 00023: val_loss did not improve from 1.45568\n",
      "36805/36805 [==============================] - 21s 561us/sample - loss: 0.2767 - acc: 0.9218 - val_loss: 1.7887 - val_acc: 0.5656\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2593 - acc: 0.9267\n",
      "Epoch 00024: val_loss did not improve from 1.45568\n",
      "36805/36805 [==============================] - 21s 563us/sample - loss: 0.2593 - acc: 0.9267 - val_loss: 1.8046 - val_acc: 0.5709\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2453 - acc: 0.9326\n",
      "Epoch 00025: val_loss did not improve from 1.45568\n",
      "36805/36805 [==============================] - 21s 561us/sample - loss: 0.2453 - acc: 0.9326 - val_loss: 1.7970 - val_acc: 0.5795\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2322 - acc: 0.9361\n",
      "Epoch 00026: val_loss did not improve from 1.45568\n",
      "36805/36805 [==============================] - 21s 561us/sample - loss: 0.2322 - acc: 0.9361 - val_loss: 1.7729 - val_acc: 0.5926\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2193 - acc: 0.9394\n",
      "Epoch 00027: val_loss did not improve from 1.45568\n",
      "36805/36805 [==============================] - 21s 560us/sample - loss: 0.2194 - acc: 0.9394 - val_loss: 1.8318 - val_acc: 0.5849\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2065 - acc: 0.9440\n",
      "Epoch 00028: val_loss did not improve from 1.45568\n",
      "36805/36805 [==============================] - 21s 561us/sample - loss: 0.2065 - acc: 0.9440 - val_loss: 1.8331 - val_acc: 0.5812\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1991 - acc: 0.9455\n",
      "Epoch 00029: val_loss did not improve from 1.45568\n",
      "36805/36805 [==============================] - 21s 563us/sample - loss: 0.1991 - acc: 0.9455 - val_loss: 1.8355 - val_acc: 0.5893\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1894 - acc: 0.9490\n",
      "Epoch 00030: val_loss did not improve from 1.45568\n",
      "36805/36805 [==============================] - 21s 561us/sample - loss: 0.1895 - acc: 0.9490 - val_loss: 1.8710 - val_acc: 0.5886\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1775 - acc: 0.9540\n",
      "Epoch 00031: val_loss did not improve from 1.45568\n",
      "36805/36805 [==============================] - 21s 560us/sample - loss: 0.1775 - acc: 0.9540 - val_loss: 1.8730 - val_acc: 0.5924\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1663 - acc: 0.9552\n",
      "Epoch 00032: val_loss did not improve from 1.45568\n",
      "36805/36805 [==============================] - 21s 561us/sample - loss: 0.1663 - acc: 0.9552 - val_loss: 1.9320 - val_acc: 0.5931\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1640 - acc: 0.9574\n",
      "Epoch 00033: val_loss did not improve from 1.45568\n",
      "36805/36805 [==============================] - 21s 562us/sample - loss: 0.1640 - acc: 0.9574 - val_loss: 1.9108 - val_acc: 0.5889\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1554 - acc: 0.9585\n",
      "Epoch 00034: val_loss did not improve from 1.45568\n",
      "36805/36805 [==============================] - 21s 563us/sample - loss: 0.1554 - acc: 0.9585 - val_loss: 1.9144 - val_acc: 0.5952\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1504 - acc: 0.9604\n",
      "Epoch 00035: val_loss did not improve from 1.45568\n",
      "36805/36805 [==============================] - 21s 563us/sample - loss: 0.1505 - acc: 0.9604 - val_loss: 1.9980 - val_acc: 0.5903\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1437 - acc: 0.9618\n",
      "Epoch 00036: val_loss did not improve from 1.45568\n",
      "36805/36805 [==============================] - 21s 561us/sample - loss: 0.1437 - acc: 0.9618 - val_loss: 1.9502 - val_acc: 0.5949\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1387 - acc: 0.9634\n",
      "Epoch 00037: val_loss did not improve from 1.45568\n",
      "36805/36805 [==============================] - 20s 554us/sample - loss: 0.1387 - acc: 0.9634 - val_loss: 1.9869 - val_acc: 0.5856\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1279 - acc: 0.9669\n",
      "Epoch 00038: val_loss did not improve from 1.45568\n",
      "36805/36805 [==============================] - 21s 561us/sample - loss: 0.1279 - acc: 0.9669 - val_loss: 1.9714 - val_acc: 0.6012\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1268 - acc: 0.9670\n",
      "Epoch 00039: val_loss did not improve from 1.45568\n",
      "36805/36805 [==============================] - 21s 557us/sample - loss: 0.1267 - acc: 0.9670 - val_loss: 2.0082 - val_acc: 0.5989\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1227 - acc: 0.9684\n",
      "Epoch 00040: val_loss did not improve from 1.45568\n",
      "36805/36805 [==============================] - 21s 560us/sample - loss: 0.1227 - acc: 0.9683 - val_loss: 2.0337 - val_acc: 0.6054\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1179 - acc: 0.9699\n",
      "Epoch 00041: val_loss did not improve from 1.45568\n",
      "36805/36805 [==============================] - 21s 560us/sample - loss: 0.1179 - acc: 0.9699 - val_loss: 2.0273 - val_acc: 0.6052\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1205 - acc: 0.9689\n",
      "Epoch 00042: val_loss did not improve from 1.45568\n",
      "36805/36805 [==============================] - 21s 558us/sample - loss: 0.1206 - acc: 0.9689 - val_loss: 2.0530 - val_acc: 0.5963\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1119 - acc: 0.9705\n",
      "Epoch 00043: val_loss did not improve from 1.45568\n",
      "36805/36805 [==============================] - 21s 560us/sample - loss: 0.1118 - acc: 0.9705 - val_loss: 2.0477 - val_acc: 0.6068\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1071 - acc: 0.9737\n",
      "Epoch 00044: val_loss did not improve from 1.45568\n",
      "36805/36805 [==============================] - 21s 558us/sample - loss: 0.1071 - acc: 0.9738 - val_loss: 2.0744 - val_acc: 0.6054\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1023 - acc: 0.9737\n",
      "Epoch 00045: val_loss did not improve from 1.45568\n",
      "36805/36805 [==============================] - 21s 559us/sample - loss: 0.1023 - acc: 0.9737 - val_loss: 2.0564 - val_acc: 0.6056\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1028 - acc: 0.9746- ETA: 0s - loss: 0.1024 - acc: 0.97\n",
      "Epoch 00046: val_loss did not improve from 1.45568\n",
      "36805/36805 [==============================] - 21s 559us/sample - loss: 0.1028 - acc: 0.9746 - val_loss: 2.1015 - val_acc: 0.5980\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1004 - acc: 0.9733\n",
      "Epoch 00047: val_loss did not improve from 1.45568\n",
      "36805/36805 [==============================] - 21s 559us/sample - loss: 0.1004 - acc: 0.9733 - val_loss: 2.0686 - val_acc: 0.6161\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0947 - acc: 0.9762\n",
      "Epoch 00048: val_loss did not improve from 1.45568\n",
      "36805/36805 [==============================] - 21s 560us/sample - loss: 0.0946 - acc: 0.9763 - val_loss: 2.1470 - val_acc: 0.6033\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0878 - acc: 0.9789\n",
      "Epoch 00049: val_loss did not improve from 1.45568\n",
      "36805/36805 [==============================] - 21s 559us/sample - loss: 0.0878 - acc: 0.9789 - val_loss: 2.1101 - val_acc: 0.6122\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0864 - acc: 0.9791\n",
      "Epoch 00050: val_loss did not improve from 1.45568\n",
      "36805/36805 [==============================] - 21s 561us/sample - loss: 0.0864 - acc: 0.9791 - val_loss: 2.1075 - val_acc: 0.6129\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0897 - acc: 0.9771\n",
      "Epoch 00051: val_loss did not improve from 1.45568\n",
      "36805/36805 [==============================] - 21s 559us/sample - loss: 0.0897 - acc: 0.9771 - val_loss: 2.1078 - val_acc: 0.6133\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0884 - acc: 0.9775\n",
      "Epoch 00052: val_loss did not improve from 1.45568\n",
      "36805/36805 [==============================] - 21s 562us/sample - loss: 0.0884 - acc: 0.9775 - val_loss: 2.1221 - val_acc: 0.6150\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0797 - acc: 0.9809\n",
      "Epoch 00053: val_loss did not improve from 1.45568\n",
      "36805/36805 [==============================] - 21s 562us/sample - loss: 0.0797 - acc: 0.9809 - val_loss: 2.1298 - val_acc: 0.6108\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0841 - acc: 0.9781\n",
      "Epoch 00054: val_loss did not improve from 1.45568\n",
      "36805/36805 [==============================] - 21s 559us/sample - loss: 0.0841 - acc: 0.9781 - val_loss: 2.1364 - val_acc: 0.6224\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0816 - acc: 0.9794\n",
      "Epoch 00055: val_loss did not improve from 1.45568\n",
      "36805/36805 [==============================] - 21s 562us/sample - loss: 0.0816 - acc: 0.9794 - val_loss: 2.1474 - val_acc: 0.6145\n",
      "\n",
      "1D_CNN_custom_multi_3_concat_ch_32_DO_3_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 1s 307us/sample - loss: 1.5206 - acc: 0.5225\n",
      "Loss: 1.5206209133223458 Accuracy: 0.5225338\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.1309 - acc: 0.3220\n",
      "Epoch 00001: val_loss improved from inf to 1.67315, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_4_conv_checkpoint/001-1.6732.hdf5\n",
      "36805/36805 [==============================] - 19s 529us/sample - loss: 2.1308 - acc: 0.3220 - val_loss: 1.6732 - val_acc: 0.4815\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5725 - acc: 0.5084\n",
      "Epoch 00002: val_loss improved from 1.67315 to 1.50189, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_4_conv_checkpoint/002-1.5019.hdf5\n",
      "36805/36805 [==============================] - 18s 501us/sample - loss: 1.5725 - acc: 0.5084 - val_loss: 1.5019 - val_acc: 0.5337\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3931 - acc: 0.5724\n",
      "Epoch 00003: val_loss improved from 1.50189 to 1.40440, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_4_conv_checkpoint/003-1.4044.hdf5\n",
      "36805/36805 [==============================] - 18s 500us/sample - loss: 1.3931 - acc: 0.5723 - val_loss: 1.4044 - val_acc: 0.5677\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2722 - acc: 0.6126\n",
      "Epoch 00004: val_loss improved from 1.40440 to 1.37330, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_4_conv_checkpoint/004-1.3733.hdf5\n",
      "36805/36805 [==============================] - 18s 498us/sample - loss: 1.2722 - acc: 0.6125 - val_loss: 1.3733 - val_acc: 0.5826\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1672 - acc: 0.6457\n",
      "Epoch 00005: val_loss improved from 1.37330 to 1.29278, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_4_conv_checkpoint/005-1.2928.hdf5\n",
      "36805/36805 [==============================] - 18s 499us/sample - loss: 1.1673 - acc: 0.6456 - val_loss: 1.2928 - val_acc: 0.6005\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0725 - acc: 0.6778\n",
      "Epoch 00006: val_loss improved from 1.29278 to 1.26600, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_4_conv_checkpoint/006-1.2660.hdf5\n",
      "36805/36805 [==============================] - 18s 498us/sample - loss: 1.0727 - acc: 0.6778 - val_loss: 1.2660 - val_acc: 0.6056\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9953 - acc: 0.6990\n",
      "Epoch 00007: val_loss improved from 1.26600 to 1.23642, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_4_conv_checkpoint/007-1.2364.hdf5\n",
      "36805/36805 [==============================] - 18s 500us/sample - loss: 0.9952 - acc: 0.6990 - val_loss: 1.2364 - val_acc: 0.6129\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9268 - acc: 0.7209\n",
      "Epoch 00008: val_loss did not improve from 1.23642\n",
      "36805/36805 [==============================] - 18s 494us/sample - loss: 0.9267 - acc: 0.7209 - val_loss: 1.2479 - val_acc: 0.6105\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8581 - acc: 0.7403\n",
      "Epoch 00009: val_loss improved from 1.23642 to 1.22402, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_4_conv_checkpoint/009-1.2240.hdf5\n",
      "36805/36805 [==============================] - 18s 498us/sample - loss: 0.8580 - acc: 0.7404 - val_loss: 1.2240 - val_acc: 0.6254\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8038 - acc: 0.7542\n",
      "Epoch 00010: val_loss improved from 1.22402 to 1.20267, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_4_conv_checkpoint/010-1.2027.hdf5\n",
      "36805/36805 [==============================] - 18s 500us/sample - loss: 0.8040 - acc: 0.7542 - val_loss: 1.2027 - val_acc: 0.6198\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7574 - acc: 0.7697\n",
      "Epoch 00011: val_loss did not improve from 1.20267\n",
      "36805/36805 [==============================] - 18s 495us/sample - loss: 0.7574 - acc: 0.7697 - val_loss: 1.2437 - val_acc: 0.6194\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7152 - acc: 0.7818\n",
      "Epoch 00012: val_loss improved from 1.20267 to 1.20112, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_4_conv_checkpoint/012-1.2011.hdf5\n",
      "36805/36805 [==============================] - 18s 500us/sample - loss: 0.7151 - acc: 0.7818 - val_loss: 1.2011 - val_acc: 0.6352\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6687 - acc: 0.7958\n",
      "Epoch 00013: val_loss did not improve from 1.20112\n",
      "36805/36805 [==============================] - 18s 498us/sample - loss: 0.6687 - acc: 0.7957 - val_loss: 1.2027 - val_acc: 0.6350\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6345 - acc: 0.8052\n",
      "Epoch 00014: val_loss did not improve from 1.20112\n",
      "36805/36805 [==============================] - 18s 495us/sample - loss: 0.6344 - acc: 0.8052 - val_loss: 1.2213 - val_acc: 0.6382\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5983 - acc: 0.8160\n",
      "Epoch 00015: val_loss did not improve from 1.20112\n",
      "36805/36805 [==============================] - 18s 500us/sample - loss: 0.5982 - acc: 0.8160 - val_loss: 1.2574 - val_acc: 0.6331\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5621 - acc: 0.8282\n",
      "Epoch 00016: val_loss did not improve from 1.20112\n",
      "36805/36805 [==============================] - 18s 497us/sample - loss: 0.5620 - acc: 0.8282 - val_loss: 1.2205 - val_acc: 0.6378\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5351 - acc: 0.8333\n",
      "Epoch 00017: val_loss did not improve from 1.20112\n",
      "36805/36805 [==============================] - 18s 497us/sample - loss: 0.5350 - acc: 0.8333 - val_loss: 1.2179 - val_acc: 0.6478\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5043 - acc: 0.8421\n",
      "Epoch 00018: val_loss did not improve from 1.20112\n",
      "36805/36805 [==============================] - 18s 495us/sample - loss: 0.5044 - acc: 0.8421 - val_loss: 1.2417 - val_acc: 0.6434\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4865 - acc: 0.8472\n",
      "Epoch 00019: val_loss did not improve from 1.20112\n",
      "36805/36805 [==============================] - 18s 499us/sample - loss: 0.4865 - acc: 0.8472 - val_loss: 1.2437 - val_acc: 0.6499\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4587 - acc: 0.8574\n",
      "Epoch 00020: val_loss did not improve from 1.20112\n",
      "36805/36805 [==============================] - 18s 495us/sample - loss: 0.4586 - acc: 0.8574 - val_loss: 1.2407 - val_acc: 0.6511\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4388 - acc: 0.8633\n",
      "Epoch 00021: val_loss did not improve from 1.20112\n",
      "36805/36805 [==============================] - 18s 497us/sample - loss: 0.4388 - acc: 0.8633 - val_loss: 1.2287 - val_acc: 0.6590\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4151 - acc: 0.8713\n",
      "Epoch 00022: val_loss did not improve from 1.20112\n",
      "36805/36805 [==============================] - 18s 499us/sample - loss: 0.4152 - acc: 0.8712 - val_loss: 1.2260 - val_acc: 0.6639\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3969 - acc: 0.8743\n",
      "Epoch 00023: val_loss did not improve from 1.20112\n",
      "36805/36805 [==============================] - 18s 497us/sample - loss: 0.3968 - acc: 0.8743 - val_loss: 1.2312 - val_acc: 0.6643\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3744 - acc: 0.8825\n",
      "Epoch 00024: val_loss did not improve from 1.20112\n",
      "36805/36805 [==============================] - 18s 496us/sample - loss: 0.3744 - acc: 0.8825 - val_loss: 1.2567 - val_acc: 0.6690\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3616 - acc: 0.8859\n",
      "Epoch 00025: val_loss did not improve from 1.20112\n",
      "36805/36805 [==============================] - 18s 497us/sample - loss: 0.3615 - acc: 0.8859 - val_loss: 1.2865 - val_acc: 0.6604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3466 - acc: 0.8902\n",
      "Epoch 00026: val_loss did not improve from 1.20112\n",
      "36805/36805 [==============================] - 18s 497us/sample - loss: 0.3466 - acc: 0.8902 - val_loss: 1.2935 - val_acc: 0.6599\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3337 - acc: 0.8938\n",
      "Epoch 00027: val_loss did not improve from 1.20112\n",
      "36805/36805 [==============================] - 18s 498us/sample - loss: 0.3337 - acc: 0.8938 - val_loss: 1.2715 - val_acc: 0.6692\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3211 - acc: 0.8993\n",
      "Epoch 00028: val_loss did not improve from 1.20112\n",
      "36805/36805 [==============================] - 18s 496us/sample - loss: 0.3211 - acc: 0.8993 - val_loss: 1.2790 - val_acc: 0.6702\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3029 - acc: 0.9049\n",
      "Epoch 00029: val_loss did not improve from 1.20112\n",
      "36805/36805 [==============================] - 18s 496us/sample - loss: 0.3029 - acc: 0.9049 - val_loss: 1.3045 - val_acc: 0.6699\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2952 - acc: 0.9070\n",
      "Epoch 00030: val_loss did not improve from 1.20112\n",
      "36805/36805 [==============================] - 18s 499us/sample - loss: 0.2952 - acc: 0.9071 - val_loss: 1.3601 - val_acc: 0.6618\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2879 - acc: 0.9085\n",
      "Epoch 00031: val_loss did not improve from 1.20112\n",
      "36805/36805 [==============================] - 18s 495us/sample - loss: 0.2878 - acc: 0.9085 - val_loss: 1.3218 - val_acc: 0.6771\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2646 - acc: 0.9172\n",
      "Epoch 00032: val_loss did not improve from 1.20112\n",
      "36805/36805 [==============================] - 18s 496us/sample - loss: 0.2646 - acc: 0.9172 - val_loss: 1.2991 - val_acc: 0.6841\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2635 - acc: 0.9166\n",
      "Epoch 00033: val_loss did not improve from 1.20112\n",
      "36805/36805 [==============================] - 18s 497us/sample - loss: 0.2635 - acc: 0.9166 - val_loss: 1.3763 - val_acc: 0.6683\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2533 - acc: 0.9197\n",
      "Epoch 00034: val_loss did not improve from 1.20112\n",
      "36805/36805 [==============================] - 18s 495us/sample - loss: 0.2533 - acc: 0.9197 - val_loss: 1.3360 - val_acc: 0.6783\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2401 - acc: 0.9267\n",
      "Epoch 00035: val_loss did not improve from 1.20112\n",
      "36805/36805 [==============================] - 18s 494us/sample - loss: 0.2401 - acc: 0.9267 - val_loss: 1.3355 - val_acc: 0.6839\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2373 - acc: 0.9278\n",
      "Epoch 00036: val_loss did not improve from 1.20112\n",
      "36805/36805 [==============================] - 18s 493us/sample - loss: 0.2374 - acc: 0.9278 - val_loss: 1.3381 - val_acc: 0.6872\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2288 - acc: 0.9283\n",
      "Epoch 00037: val_loss did not improve from 1.20112\n",
      "36805/36805 [==============================] - 18s 493us/sample - loss: 0.2288 - acc: 0.9283 - val_loss: 1.3560 - val_acc: 0.6883\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2168 - acc: 0.9316\n",
      "Epoch 00038: val_loss did not improve from 1.20112\n",
      "36805/36805 [==============================] - 18s 493us/sample - loss: 0.2168 - acc: 0.9316 - val_loss: 1.3461 - val_acc: 0.6879\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2129 - acc: 0.9328\n",
      "Epoch 00039: val_loss did not improve from 1.20112\n",
      "36805/36805 [==============================] - 18s 498us/sample - loss: 0.2129 - acc: 0.9328 - val_loss: 1.3472 - val_acc: 0.6862\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2069 - acc: 0.9354\n",
      "Epoch 00040: val_loss did not improve from 1.20112\n",
      "36805/36805 [==============================] - 19s 507us/sample - loss: 0.2069 - acc: 0.9354 - val_loss: 1.4146 - val_acc: 0.6776\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1989 - acc: 0.9376\n",
      "Epoch 00041: val_loss did not improve from 1.20112\n",
      "36805/36805 [==============================] - 19s 505us/sample - loss: 0.1989 - acc: 0.9376 - val_loss: 1.3753 - val_acc: 0.6897\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1881 - acc: 0.9405\n",
      "Epoch 00042: val_loss did not improve from 1.20112\n",
      "36805/36805 [==============================] - 18s 495us/sample - loss: 0.1885 - acc: 0.9405 - val_loss: 1.3435 - val_acc: 0.7011\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1866 - acc: 0.9409\n",
      "Epoch 00043: val_loss did not improve from 1.20112\n",
      "36805/36805 [==============================] - 18s 500us/sample - loss: 0.1866 - acc: 0.9409 - val_loss: 1.3634 - val_acc: 0.6979\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1785 - acc: 0.9435\n",
      "Epoch 00044: val_loss did not improve from 1.20112\n",
      "36805/36805 [==============================] - 19s 509us/sample - loss: 0.1785 - acc: 0.9435 - val_loss: 1.4277 - val_acc: 0.6876\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1796 - acc: 0.9439\n",
      "Epoch 00045: val_loss did not improve from 1.20112\n",
      "36805/36805 [==============================] - 18s 498us/sample - loss: 0.1796 - acc: 0.9439 - val_loss: 1.3976 - val_acc: 0.6944\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1724 - acc: 0.9464\n",
      "Epoch 00046: val_loss did not improve from 1.20112\n",
      "36805/36805 [==============================] - 18s 498us/sample - loss: 0.1724 - acc: 0.9464 - val_loss: 1.4135 - val_acc: 0.6942\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1698 - acc: 0.9470\n",
      "Epoch 00047: val_loss did not improve from 1.20112\n",
      "36805/36805 [==============================] - 18s 499us/sample - loss: 0.1698 - acc: 0.9470 - val_loss: 1.4114 - val_acc: 0.6921\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1664 - acc: 0.9489\n",
      "Epoch 00048: val_loss did not improve from 1.20112\n",
      "36805/36805 [==============================] - 18s 495us/sample - loss: 0.1664 - acc: 0.9489 - val_loss: 1.4195 - val_acc: 0.7023\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1561 - acc: 0.9518\n",
      "Epoch 00049: val_loss did not improve from 1.20112\n",
      "36805/36805 [==============================] - 18s 497us/sample - loss: 0.1561 - acc: 0.9518 - val_loss: 1.4058 - val_acc: 0.6997\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1577 - acc: 0.9515\n",
      "Epoch 00050: val_loss did not improve from 1.20112\n",
      "36805/36805 [==============================] - 18s 489us/sample - loss: 0.1577 - acc: 0.9515 - val_loss: 1.4467 - val_acc: 0.7063\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1507 - acc: 0.9526\n",
      "Epoch 00051: val_loss did not improve from 1.20112\n",
      "36805/36805 [==============================] - 18s 499us/sample - loss: 0.1508 - acc: 0.9526 - val_loss: 1.4149 - val_acc: 0.7009\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1519 - acc: 0.9531\n",
      "Epoch 00052: val_loss did not improve from 1.20112\n",
      "36805/36805 [==============================] - 18s 499us/sample - loss: 0.1519 - acc: 0.9531 - val_loss: 1.4255 - val_acc: 0.7042\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1427 - acc: 0.9562\n",
      "Epoch 00053: val_loss did not improve from 1.20112\n",
      "36805/36805 [==============================] - 18s 496us/sample - loss: 0.1427 - acc: 0.9562 - val_loss: 1.4809 - val_acc: 0.7025\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1426 - acc: 0.9555\n",
      "Epoch 00054: val_loss did not improve from 1.20112\n",
      "36805/36805 [==============================] - 18s 495us/sample - loss: 0.1426 - acc: 0.9555 - val_loss: 1.4531 - val_acc: 0.7046\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1360 - acc: 0.9575\n",
      "Epoch 00055: val_loss did not improve from 1.20112\n",
      "36805/36805 [==============================] - 18s 496us/sample - loss: 0.1361 - acc: 0.9575 - val_loss: 1.4560 - val_acc: 0.7109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1326 - acc: 0.9593\n",
      "Epoch 00056: val_loss did not improve from 1.20112\n",
      "36805/36805 [==============================] - 18s 497us/sample - loss: 0.1325 - acc: 0.9593 - val_loss: 1.4741 - val_acc: 0.7091\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1278 - acc: 0.9611\n",
      "Epoch 00057: val_loss did not improve from 1.20112\n",
      "36805/36805 [==============================] - 18s 497us/sample - loss: 0.1278 - acc: 0.9611 - val_loss: 1.4849 - val_acc: 0.7028\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1288 - acc: 0.9605\n",
      "Epoch 00058: val_loss did not improve from 1.20112\n",
      "36805/36805 [==============================] - 18s 499us/sample - loss: 0.1288 - acc: 0.9605 - val_loss: 1.4799 - val_acc: 0.7095\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1274 - acc: 0.9609\n",
      "Epoch 00059: val_loss did not improve from 1.20112\n",
      "36805/36805 [==============================] - 18s 497us/sample - loss: 0.1274 - acc: 0.9609 - val_loss: 1.4439 - val_acc: 0.7107\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1227 - acc: 0.9639\n",
      "Epoch 00060: val_loss did not improve from 1.20112\n",
      "36805/36805 [==============================] - 18s 497us/sample - loss: 0.1227 - acc: 0.9639 - val_loss: 1.4455 - val_acc: 0.7088\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1215 - acc: 0.9633\n",
      "Epoch 00061: val_loss did not improve from 1.20112\n",
      "36805/36805 [==============================] - 19s 507us/sample - loss: 0.1215 - acc: 0.9633 - val_loss: 1.4879 - val_acc: 0.7105\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1178 - acc: 0.9655\n",
      "Epoch 00062: val_loss did not improve from 1.20112\n",
      "36805/36805 [==============================] - 19s 506us/sample - loss: 0.1178 - acc: 0.9655 - val_loss: 1.4658 - val_acc: 0.7177\n",
      "\n",
      "1D_CNN_custom_multi_3_concat_ch_32_DO_4_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 320us/sample - loss: 1.2618 - acc: 0.6098\n",
      "Loss: 1.2618204119297078 Accuracy: 0.6097612\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.2028 - acc: 0.2942\n",
      "Epoch 00001: val_loss improved from inf to 1.74812, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_5_conv_checkpoint/001-1.7481.hdf5\n",
      "36805/36805 [==============================] - 21s 570us/sample - loss: 2.2028 - acc: 0.2942 - val_loss: 1.7481 - val_acc: 0.4531\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6340 - acc: 0.4821\n",
      "Epoch 00002: val_loss improved from 1.74812 to 1.53352, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_5_conv_checkpoint/002-1.5335.hdf5\n",
      "36805/36805 [==============================] - 18s 500us/sample - loss: 1.6340 - acc: 0.4821 - val_loss: 1.5335 - val_acc: 0.5052\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4637 - acc: 0.5436\n",
      "Epoch 00003: val_loss improved from 1.53352 to 1.36678, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_5_conv_checkpoint/003-1.3668.hdf5\n",
      "36805/36805 [==============================] - 18s 493us/sample - loss: 1.4637 - acc: 0.5435 - val_loss: 1.3668 - val_acc: 0.5858\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3465 - acc: 0.5847\n",
      "Epoch 00004: val_loss improved from 1.36678 to 1.28216, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_5_conv_checkpoint/004-1.2822.hdf5\n",
      "36805/36805 [==============================] - 18s 495us/sample - loss: 1.3465 - acc: 0.5847 - val_loss: 1.2822 - val_acc: 0.6117\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2526 - acc: 0.6170\n",
      "Epoch 00005: val_loss improved from 1.28216 to 1.20828, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_5_conv_checkpoint/005-1.2083.hdf5\n",
      "36805/36805 [==============================] - 18s 496us/sample - loss: 1.2527 - acc: 0.6170 - val_loss: 1.2083 - val_acc: 0.6273\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1741 - acc: 0.6433\n",
      "Epoch 00006: val_loss improved from 1.20828 to 1.17406, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_5_conv_checkpoint/006-1.1741.hdf5\n",
      "36805/36805 [==============================] - 18s 497us/sample - loss: 1.1740 - acc: 0.6433 - val_loss: 1.1741 - val_acc: 0.6380\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1076 - acc: 0.6647\n",
      "Epoch 00007: val_loss improved from 1.17406 to 1.10710, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_5_conv_checkpoint/007-1.1071.hdf5\n",
      "36805/36805 [==============================] - 18s 496us/sample - loss: 1.1075 - acc: 0.6647 - val_loss: 1.1071 - val_acc: 0.6564\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0498 - acc: 0.6832\n",
      "Epoch 00008: val_loss improved from 1.10710 to 1.05382, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_5_conv_checkpoint/008-1.0538.hdf5\n",
      "36805/36805 [==============================] - 18s 495us/sample - loss: 1.0498 - acc: 0.6833 - val_loss: 1.0538 - val_acc: 0.6813\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9985 - acc: 0.6997\n",
      "Epoch 00009: val_loss improved from 1.05382 to 1.01288, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_5_conv_checkpoint/009-1.0129.hdf5\n",
      "36805/36805 [==============================] - 18s 495us/sample - loss: 0.9984 - acc: 0.6997 - val_loss: 1.0129 - val_acc: 0.6900\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9457 - acc: 0.7157\n",
      "Epoch 00010: val_loss improved from 1.01288 to 0.99651, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_5_conv_checkpoint/010-0.9965.hdf5\n",
      "36805/36805 [==============================] - 18s 495us/sample - loss: 0.9458 - acc: 0.7157 - val_loss: 0.9965 - val_acc: 0.7070\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9075 - acc: 0.7288\n",
      "Epoch 00011: val_loss improved from 0.99651 to 0.95546, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_5_conv_checkpoint/011-0.9555.hdf5\n",
      "36805/36805 [==============================] - 18s 494us/sample - loss: 0.9076 - acc: 0.7288 - val_loss: 0.9555 - val_acc: 0.7184\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8701 - acc: 0.7386\n",
      "Epoch 00012: val_loss improved from 0.95546 to 0.93216, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_5_conv_checkpoint/012-0.9322.hdf5\n",
      "36805/36805 [==============================] - 18s 498us/sample - loss: 0.8700 - acc: 0.7386 - val_loss: 0.9322 - val_acc: 0.7188\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8294 - acc: 0.7517\n",
      "Epoch 00013: val_loss improved from 0.93216 to 0.91067, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_5_conv_checkpoint/013-0.9107.hdf5\n",
      "36805/36805 [==============================] - 18s 496us/sample - loss: 0.8294 - acc: 0.7516 - val_loss: 0.9107 - val_acc: 0.7251\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7930 - acc: 0.7624\n",
      "Epoch 00014: val_loss improved from 0.91067 to 0.89868, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_5_conv_checkpoint/014-0.8987.hdf5\n",
      "36805/36805 [==============================] - 18s 491us/sample - loss: 0.7930 - acc: 0.7624 - val_loss: 0.8987 - val_acc: 0.7321\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7616 - acc: 0.7713\n",
      "Epoch 00015: val_loss improved from 0.89868 to 0.87311, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_5_conv_checkpoint/015-0.8731.hdf5\n",
      "36805/36805 [==============================] - 18s 495us/sample - loss: 0.7615 - acc: 0.7713 - val_loss: 0.8731 - val_acc: 0.7405\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7332 - acc: 0.7780\n",
      "Epoch 00016: val_loss did not improve from 0.87311\n",
      "36805/36805 [==============================] - 18s 486us/sample - loss: 0.7332 - acc: 0.7780 - val_loss: 0.8784 - val_acc: 0.7349\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6956 - acc: 0.7906\n",
      "Epoch 00017: val_loss did not improve from 0.87311\n",
      "36805/36805 [==============================] - 18s 490us/sample - loss: 0.6956 - acc: 0.7906 - val_loss: 0.8798 - val_acc: 0.7307\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6703 - acc: 0.7981\n",
      "Epoch 00018: val_loss improved from 0.87311 to 0.83678, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_5_conv_checkpoint/018-0.8368.hdf5\n",
      "36805/36805 [==============================] - 18s 486us/sample - loss: 0.6702 - acc: 0.7981 - val_loss: 0.8368 - val_acc: 0.7545\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6500 - acc: 0.8021\n",
      "Epoch 00019: val_loss improved from 0.83678 to 0.82409, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_5_conv_checkpoint/019-0.8241.hdf5\n",
      "36805/36805 [==============================] - 18s 493us/sample - loss: 0.6500 - acc: 0.8021 - val_loss: 0.8241 - val_acc: 0.7575\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6192 - acc: 0.8126\n",
      "Epoch 00020: val_loss did not improve from 0.82409\n",
      "36805/36805 [==============================] - 18s 487us/sample - loss: 0.6192 - acc: 0.8126 - val_loss: 0.8369 - val_acc: 0.7515\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5958 - acc: 0.8202\n",
      "Epoch 00021: val_loss did not improve from 0.82409\n",
      "36805/36805 [==============================] - 18s 488us/sample - loss: 0.5958 - acc: 0.8202 - val_loss: 0.8255 - val_acc: 0.7524\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5697 - acc: 0.8285\n",
      "Epoch 00022: val_loss improved from 0.82409 to 0.80104, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_5_conv_checkpoint/022-0.8010.hdf5\n",
      "36805/36805 [==============================] - 18s 491us/sample - loss: 0.5697 - acc: 0.8285 - val_loss: 0.8010 - val_acc: 0.7661\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5453 - acc: 0.8346\n",
      "Epoch 00023: val_loss improved from 0.80104 to 0.78833, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_5_conv_checkpoint/023-0.7883.hdf5\n",
      "36805/36805 [==============================] - 18s 492us/sample - loss: 0.5453 - acc: 0.8346 - val_loss: 0.7883 - val_acc: 0.7710\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5292 - acc: 0.8381\n",
      "Epoch 00024: val_loss did not improve from 0.78833\n",
      "36805/36805 [==============================] - 18s 484us/sample - loss: 0.5292 - acc: 0.8381 - val_loss: 0.8033 - val_acc: 0.7692\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5083 - acc: 0.8442\n",
      "Epoch 00025: val_loss improved from 0.78833 to 0.76875, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_5_conv_checkpoint/025-0.7687.hdf5\n",
      "36805/36805 [==============================] - 18s 490us/sample - loss: 0.5083 - acc: 0.8442 - val_loss: 0.7687 - val_acc: 0.7799\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4908 - acc: 0.8486\n",
      "Epoch 00026: val_loss did not improve from 0.76875\n",
      "36805/36805 [==============================] - 18s 485us/sample - loss: 0.4908 - acc: 0.8486 - val_loss: 0.7837 - val_acc: 0.7729\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4684 - acc: 0.8562\n",
      "Epoch 00027: val_loss did not improve from 0.76875\n",
      "36805/36805 [==============================] - 18s 488us/sample - loss: 0.4684 - acc: 0.8562 - val_loss: 0.7946 - val_acc: 0.7692\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4557 - acc: 0.8606\n",
      "Epoch 00028: val_loss improved from 0.76875 to 0.76478, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_5_conv_checkpoint/028-0.7648.hdf5\n",
      "36805/36805 [==============================] - 18s 489us/sample - loss: 0.4556 - acc: 0.8606 - val_loss: 0.7648 - val_acc: 0.7778\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4375 - acc: 0.8677\n",
      "Epoch 00029: val_loss did not improve from 0.76478\n",
      "36805/36805 [==============================] - 18s 488us/sample - loss: 0.4375 - acc: 0.8677 - val_loss: 0.7835 - val_acc: 0.7768\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4204 - acc: 0.8701\n",
      "Epoch 00030: val_loss improved from 0.76478 to 0.73365, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_5_conv_checkpoint/030-0.7337.hdf5\n",
      "36805/36805 [==============================] - 18s 495us/sample - loss: 0.4205 - acc: 0.8701 - val_loss: 0.7337 - val_acc: 0.7952\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4084 - acc: 0.8743\n",
      "Epoch 00031: val_loss did not improve from 0.73365\n",
      "36805/36805 [==============================] - 18s 489us/sample - loss: 0.4083 - acc: 0.8743 - val_loss: 0.7405 - val_acc: 0.7901\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3931 - acc: 0.8761\n",
      "Epoch 00032: val_loss did not improve from 0.73365\n",
      "36805/36805 [==============================] - 18s 487us/sample - loss: 0.3931 - acc: 0.8761 - val_loss: 0.7507 - val_acc: 0.7885\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3779 - acc: 0.8834\n",
      "Epoch 00033: val_loss did not improve from 0.73365\n",
      "36805/36805 [==============================] - 18s 489us/sample - loss: 0.3778 - acc: 0.8834 - val_loss: 0.7502 - val_acc: 0.7857\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3710 - acc: 0.8849\n",
      "Epoch 00034: val_loss improved from 0.73365 to 0.73080, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_5_conv_checkpoint/034-0.7308.hdf5\n",
      "36805/36805 [==============================] - 18s 490us/sample - loss: 0.3710 - acc: 0.8849 - val_loss: 0.7308 - val_acc: 0.7997\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3587 - acc: 0.8872\n",
      "Epoch 00035: val_loss did not improve from 0.73080\n",
      "36805/36805 [==============================] - 18s 485us/sample - loss: 0.3589 - acc: 0.8872 - val_loss: 0.7568 - val_acc: 0.7922\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3433 - acc: 0.8933\n",
      "Epoch 00036: val_loss improved from 0.73080 to 0.72808, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_5_conv_checkpoint/036-0.7281.hdf5\n",
      "36805/36805 [==============================] - 18s 488us/sample - loss: 0.3433 - acc: 0.8933 - val_loss: 0.7281 - val_acc: 0.8020\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3324 - acc: 0.8957\n",
      "Epoch 00037: val_loss did not improve from 0.72808\n",
      "36805/36805 [==============================] - 18s 488us/sample - loss: 0.3326 - acc: 0.8957 - val_loss: 0.7452 - val_acc: 0.7941\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3239 - acc: 0.8987\n",
      "Epoch 00038: val_loss improved from 0.72808 to 0.72792, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_5_conv_checkpoint/038-0.7279.hdf5\n",
      "36805/36805 [==============================] - 18s 489us/sample - loss: 0.3239 - acc: 0.8987 - val_loss: 0.7279 - val_acc: 0.8050\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3090 - acc: 0.9030\n",
      "Epoch 00039: val_loss improved from 0.72792 to 0.72545, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_5_conv_checkpoint/039-0.7255.hdf5\n",
      "36805/36805 [==============================] - 18s 493us/sample - loss: 0.3090 - acc: 0.9029 - val_loss: 0.7255 - val_acc: 0.8046\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2964 - acc: 0.9068\n",
      "Epoch 00040: val_loss did not improve from 0.72545\n",
      "36805/36805 [==============================] - 18s 489us/sample - loss: 0.2964 - acc: 0.9068 - val_loss: 0.7391 - val_acc: 0.8083\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2969 - acc: 0.9065\n",
      "Epoch 00041: val_loss improved from 0.72545 to 0.72203, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_5_conv_checkpoint/041-0.7220.hdf5\n",
      "36805/36805 [==============================] - 18s 487us/sample - loss: 0.2969 - acc: 0.9065 - val_loss: 0.7220 - val_acc: 0.8074\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2872 - acc: 0.9093\n",
      "Epoch 00042: val_loss did not improve from 0.72203\n",
      "36805/36805 [==============================] - 18s 491us/sample - loss: 0.2873 - acc: 0.9093 - val_loss: 0.7318 - val_acc: 0.8041\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2753 - acc: 0.9134\n",
      "Epoch 00043: val_loss improved from 0.72203 to 0.71507, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_5_conv_checkpoint/043-0.7151.hdf5\n",
      "36805/36805 [==============================] - 18s 488us/sample - loss: 0.2753 - acc: 0.9134 - val_loss: 0.7151 - val_acc: 0.8088\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2715 - acc: 0.9134\n",
      "Epoch 00044: val_loss did not improve from 0.71507\n",
      "36805/36805 [==============================] - 18s 489us/sample - loss: 0.2715 - acc: 0.9134 - val_loss: 0.7273 - val_acc: 0.8132\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2573 - acc: 0.9178\n",
      "Epoch 00045: val_loss did not improve from 0.71507\n",
      "36805/36805 [==============================] - 18s 489us/sample - loss: 0.2573 - acc: 0.9178 - val_loss: 0.7177 - val_acc: 0.8118\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2554 - acc: 0.9179\n",
      "Epoch 00046: val_loss did not improve from 0.71507\n",
      "36805/36805 [==============================] - 18s 488us/sample - loss: 0.2554 - acc: 0.9179 - val_loss: 0.7713 - val_acc: 0.7964\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2450 - acc: 0.9207\n",
      "Epoch 00047: val_loss did not improve from 0.71507\n",
      "36805/36805 [==============================] - 18s 483us/sample - loss: 0.2451 - acc: 0.9207 - val_loss: 0.7161 - val_acc: 0.8188\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2383 - acc: 0.9256\n",
      "Epoch 00048: val_loss did not improve from 0.71507\n",
      "36805/36805 [==============================] - 18s 485us/sample - loss: 0.2383 - acc: 0.9256 - val_loss: 0.7336 - val_acc: 0.8097\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2335 - acc: 0.9262\n",
      "Epoch 00049: val_loss did not improve from 0.71507\n",
      "36805/36805 [==============================] - 18s 485us/sample - loss: 0.2335 - acc: 0.9262 - val_loss: 0.7246 - val_acc: 0.8097\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2282 - acc: 0.9275\n",
      "Epoch 00050: val_loss did not improve from 0.71507\n",
      "36805/36805 [==============================] - 18s 485us/sample - loss: 0.2282 - acc: 0.9275 - val_loss: 0.7433 - val_acc: 0.8120\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2223 - acc: 0.9304\n",
      "Epoch 00051: val_loss did not improve from 0.71507\n",
      "36805/36805 [==============================] - 18s 487us/sample - loss: 0.2223 - acc: 0.9304 - val_loss: 0.7402 - val_acc: 0.8109\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2204 - acc: 0.9301\n",
      "Epoch 00052: val_loss did not improve from 0.71507\n",
      "36805/36805 [==============================] - 18s 488us/sample - loss: 0.2204 - acc: 0.9301 - val_loss: 0.7546 - val_acc: 0.8150\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2110 - acc: 0.9321\n",
      "Epoch 00053: val_loss did not improve from 0.71507\n",
      "36805/36805 [==============================] - 18s 486us/sample - loss: 0.2109 - acc: 0.9321 - val_loss: 0.7180 - val_acc: 0.8209\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2055 - acc: 0.9340\n",
      "Epoch 00054: val_loss did not improve from 0.71507\n",
      "36805/36805 [==============================] - 18s 490us/sample - loss: 0.2055 - acc: 0.9340 - val_loss: 0.7151 - val_acc: 0.8223\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2009 - acc: 0.9363\n",
      "Epoch 00055: val_loss did not improve from 0.71507\n",
      "36805/36805 [==============================] - 18s 489us/sample - loss: 0.2009 - acc: 0.9363 - val_loss: 0.7435 - val_acc: 0.8220\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2032 - acc: 0.9343\n",
      "Epoch 00056: val_loss did not improve from 0.71507\n",
      "36805/36805 [==============================] - 18s 486us/sample - loss: 0.2032 - acc: 0.9343 - val_loss: 0.7266 - val_acc: 0.8225\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1909 - acc: 0.9402\n",
      "Epoch 00057: val_loss did not improve from 0.71507\n",
      "36805/36805 [==============================] - 18s 490us/sample - loss: 0.1909 - acc: 0.9402 - val_loss: 0.7208 - val_acc: 0.8248\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1928 - acc: 0.9381\n",
      "Epoch 00058: val_loss did not improve from 0.71507\n",
      "36805/36805 [==============================] - 18s 491us/sample - loss: 0.1928 - acc: 0.9381 - val_loss: 0.7386 - val_acc: 0.8195\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1826 - acc: 0.9407\n",
      "Epoch 00059: val_loss did not improve from 0.71507\n",
      "36805/36805 [==============================] - 18s 490us/sample - loss: 0.1826 - acc: 0.9407 - val_loss: 0.7167 - val_acc: 0.8283\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1794 - acc: 0.9414\n",
      "Epoch 00060: val_loss did not improve from 0.71507\n",
      "36805/36805 [==============================] - 18s 485us/sample - loss: 0.1795 - acc: 0.9414 - val_loss: 0.7206 - val_acc: 0.8267\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1795 - acc: 0.9429\n",
      "Epoch 00061: val_loss did not improve from 0.71507\n",
      "36805/36805 [==============================] - 18s 489us/sample - loss: 0.1795 - acc: 0.9429 - val_loss: 0.7512 - val_acc: 0.8213\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1720 - acc: 0.9452\n",
      "Epoch 00062: val_loss did not improve from 0.71507\n",
      "36805/36805 [==============================] - 18s 487us/sample - loss: 0.1720 - acc: 0.9452 - val_loss: 0.7477 - val_acc: 0.8197\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1716 - acc: 0.9436\n",
      "Epoch 00063: val_loss did not improve from 0.71507\n",
      "36805/36805 [==============================] - 18s 486us/sample - loss: 0.1716 - acc: 0.9435 - val_loss: 0.7516 - val_acc: 0.8267\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1715 - acc: 0.9440\n",
      "Epoch 00064: val_loss did not improve from 0.71507\n",
      "36805/36805 [==============================] - 18s 482us/sample - loss: 0.1715 - acc: 0.9441 - val_loss: 0.7496 - val_acc: 0.8286\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1640 - acc: 0.9476\n",
      "Epoch 00065: val_loss did not improve from 0.71507\n",
      "36805/36805 [==============================] - 18s 484us/sample - loss: 0.1640 - acc: 0.9476 - val_loss: 0.7367 - val_acc: 0.8300\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1614 - acc: 0.9483\n",
      "Epoch 00066: val_loss did not improve from 0.71507\n",
      "36805/36805 [==============================] - 18s 490us/sample - loss: 0.1614 - acc: 0.9483 - val_loss: 0.7397 - val_acc: 0.8318\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1608 - acc: 0.9468\n",
      "Epoch 00067: val_loss did not improve from 0.71507\n",
      "36805/36805 [==============================] - 18s 487us/sample - loss: 0.1608 - acc: 0.9467 - val_loss: 0.7555 - val_acc: 0.8255\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1547 - acc: 0.9514\n",
      "Epoch 00068: val_loss did not improve from 0.71507\n",
      "36805/36805 [==============================] - 18s 491us/sample - loss: 0.1547 - acc: 0.9514 - val_loss: 0.7399 - val_acc: 0.8272\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1518 - acc: 0.9512\n",
      "Epoch 00069: val_loss did not improve from 0.71507\n",
      "36805/36805 [==============================] - 18s 485us/sample - loss: 0.1518 - acc: 0.9512 - val_loss: 0.7371 - val_acc: 0.8348\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1485 - acc: 0.9513\n",
      "Epoch 00070: val_loss did not improve from 0.71507\n",
      "36805/36805 [==============================] - 18s 483us/sample - loss: 0.1485 - acc: 0.9513 - val_loss: 0.7560 - val_acc: 0.8272\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1465 - acc: 0.9532\n",
      "Epoch 00071: val_loss did not improve from 0.71507\n",
      "36805/36805 [==============================] - 18s 487us/sample - loss: 0.1465 - acc: 0.9532 - val_loss: 0.7342 - val_acc: 0.8302\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1471 - acc: 0.9521\n",
      "Epoch 00072: val_loss did not improve from 0.71507\n",
      "36805/36805 [==============================] - 18s 489us/sample - loss: 0.1471 - acc: 0.9521 - val_loss: 0.7390 - val_acc: 0.8344\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1400 - acc: 0.9544\n",
      "Epoch 00073: val_loss did not improve from 0.71507\n",
      "36805/36805 [==============================] - 18s 480us/sample - loss: 0.1400 - acc: 0.9544 - val_loss: 0.7358 - val_acc: 0.8341\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1374 - acc: 0.9558\n",
      "Epoch 00074: val_loss did not improve from 0.71507\n",
      "36805/36805 [==============================] - 18s 486us/sample - loss: 0.1373 - acc: 0.9558 - val_loss: 0.7560 - val_acc: 0.8297\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1369 - acc: 0.9559\n",
      "Epoch 00075: val_loss did not improve from 0.71507\n",
      "36805/36805 [==============================] - 18s 485us/sample - loss: 0.1369 - acc: 0.9559 - val_loss: 0.7480 - val_acc: 0.8348\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1333 - acc: 0.9570\n",
      "Epoch 00076: val_loss did not improve from 0.71507\n",
      "36805/36805 [==============================] - 18s 485us/sample - loss: 0.1333 - acc: 0.9570 - val_loss: 0.7365 - val_acc: 0.8348\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1315 - acc: 0.9571\n",
      "Epoch 00077: val_loss did not improve from 0.71507\n",
      "36805/36805 [==============================] - 18s 489us/sample - loss: 0.1315 - acc: 0.9571 - val_loss: 0.7540 - val_acc: 0.8323\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1288 - acc: 0.9595\n",
      "Epoch 00078: val_loss did not improve from 0.71507\n",
      "36805/36805 [==============================] - 18s 485us/sample - loss: 0.1288 - acc: 0.9595 - val_loss: 0.7222 - val_acc: 0.8395\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1263 - acc: 0.9585\n",
      "Epoch 00079: val_loss did not improve from 0.71507\n",
      "36805/36805 [==============================] - 18s 484us/sample - loss: 0.1263 - acc: 0.9585 - val_loss: 0.7319 - val_acc: 0.8341\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1258 - acc: 0.9597\n",
      "Epoch 00080: val_loss did not improve from 0.71507\n",
      "36805/36805 [==============================] - 18s 484us/sample - loss: 0.1258 - acc: 0.9597 - val_loss: 0.7323 - val_acc: 0.8376\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1218 - acc: 0.9607\n",
      "Epoch 00081: val_loss did not improve from 0.71507\n",
      "36805/36805 [==============================] - 18s 488us/sample - loss: 0.1218 - acc: 0.9607 - val_loss: 0.7536 - val_acc: 0.8381\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1211 - acc: 0.9602\n",
      "Epoch 00082: val_loss did not improve from 0.71507\n",
      "36805/36805 [==============================] - 18s 482us/sample - loss: 0.1211 - acc: 0.9602 - val_loss: 0.7294 - val_acc: 0.8418\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1173 - acc: 0.9629\n",
      "Epoch 00083: val_loss did not improve from 0.71507\n",
      "36805/36805 [==============================] - 18s 491us/sample - loss: 0.1174 - acc: 0.9629 - val_loss: 0.7336 - val_acc: 0.8418\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1175 - acc: 0.9632\n",
      "Epoch 00084: val_loss did not improve from 0.71507\n",
      "36805/36805 [==============================] - 18s 485us/sample - loss: 0.1175 - acc: 0.9632 - val_loss: 0.7664 - val_acc: 0.8369\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1175 - acc: 0.9630\n",
      "Epoch 00085: val_loss did not improve from 0.71507\n",
      "36805/36805 [==============================] - 18s 486us/sample - loss: 0.1175 - acc: 0.9630 - val_loss: 0.7884 - val_acc: 0.8330\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1149 - acc: 0.9635\n",
      "Epoch 00086: val_loss did not improve from 0.71507\n",
      "36805/36805 [==============================] - 18s 489us/sample - loss: 0.1149 - acc: 0.9635 - val_loss: 0.7569 - val_acc: 0.8395\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1123 - acc: 0.9640\n",
      "Epoch 00087: val_loss did not improve from 0.71507\n",
      "36805/36805 [==============================] - 18s 485us/sample - loss: 0.1123 - acc: 0.9640 - val_loss: 0.7532 - val_acc: 0.8386\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1094 - acc: 0.9646\n",
      "Epoch 00088: val_loss did not improve from 0.71507\n",
      "36805/36805 [==============================] - 18s 489us/sample - loss: 0.1095 - acc: 0.9645 - val_loss: 0.7665 - val_acc: 0.8314\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1128 - acc: 0.9648\n",
      "Epoch 00089: val_loss did not improve from 0.71507\n",
      "36805/36805 [==============================] - 18s 488us/sample - loss: 0.1128 - acc: 0.9648 - val_loss: 0.7436 - val_acc: 0.8446\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1064 - acc: 0.9661\n",
      "Epoch 00090: val_loss did not improve from 0.71507\n",
      "36805/36805 [==============================] - 18s 489us/sample - loss: 0.1064 - acc: 0.9661 - val_loss: 0.7470 - val_acc: 0.8358\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1089 - acc: 0.9652\n",
      "Epoch 00091: val_loss did not improve from 0.71507\n",
      "36805/36805 [==============================] - 18s 488us/sample - loss: 0.1089 - acc: 0.9652 - val_loss: 0.7404 - val_acc: 0.8421\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1055 - acc: 0.9657\n",
      "Epoch 00092: val_loss did not improve from 0.71507\n",
      "36805/36805 [==============================] - 18s 484us/sample - loss: 0.1055 - acc: 0.9657 - val_loss: 0.7551 - val_acc: 0.8435\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1053 - acc: 0.9670\n",
      "Epoch 00093: val_loss did not improve from 0.71507\n",
      "36805/36805 [==============================] - 18s 486us/sample - loss: 0.1054 - acc: 0.9670 - val_loss: 0.7506 - val_acc: 0.8416\n",
      "\n",
      "1D_CNN_custom_multi_3_concat_ch_32_DO_5_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 1s 307us/sample - loss: 0.8005 - acc: 0.7838\n",
      "Loss: 0.8005222551052692 Accuracy: 0.7838006\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.2863 - acc: 0.2633\n",
      "Epoch 00001: val_loss improved from inf to 1.64088, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_6_conv_checkpoint/001-1.6409.hdf5\n",
      "36805/36805 [==============================] - 21s 581us/sample - loss: 2.2862 - acc: 0.2633 - val_loss: 1.6409 - val_acc: 0.4864\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5909 - acc: 0.4913\n",
      "Epoch 00002: val_loss improved from 1.64088 to 1.39654, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_6_conv_checkpoint/002-1.3965.hdf5\n",
      "36805/36805 [==============================] - 18s 498us/sample - loss: 1.5909 - acc: 0.4912 - val_loss: 1.3965 - val_acc: 0.5670\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4141 - acc: 0.5554\n",
      "Epoch 00003: val_loss improved from 1.39654 to 1.27171, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_6_conv_checkpoint/003-1.2717.hdf5\n",
      "36805/36805 [==============================] - 19s 503us/sample - loss: 1.4140 - acc: 0.5555 - val_loss: 1.2717 - val_acc: 0.6108\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2892 - acc: 0.5983\n",
      "Epoch 00004: val_loss improved from 1.27171 to 1.17903, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_6_conv_checkpoint/004-1.1790.hdf5\n",
      "36805/36805 [==============================] - 18s 500us/sample - loss: 1.2892 - acc: 0.5983 - val_loss: 1.1790 - val_acc: 0.6518\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1893 - acc: 0.6332\n",
      "Epoch 00005: val_loss improved from 1.17903 to 1.06458, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_6_conv_checkpoint/005-1.0646.hdf5\n",
      "36805/36805 [==============================] - 18s 502us/sample - loss: 1.1892 - acc: 0.6333 - val_loss: 1.0646 - val_acc: 0.6846\n",
      "Epoch 6/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1077 - acc: 0.6597\n",
      "Epoch 00006: val_loss improved from 1.06458 to 1.00816, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_6_conv_checkpoint/006-1.0082.hdf5\n",
      "36805/36805 [==============================] - 19s 507us/sample - loss: 1.1072 - acc: 0.6599 - val_loss: 1.0082 - val_acc: 0.6932\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0295 - acc: 0.6854\n",
      "Epoch 00007: val_loss improved from 1.00816 to 0.91370, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_6_conv_checkpoint/007-0.9137.hdf5\n",
      "36805/36805 [==============================] - 19s 503us/sample - loss: 1.0295 - acc: 0.6854 - val_loss: 0.9137 - val_acc: 0.7352\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9578 - acc: 0.7110\n",
      "Epoch 00008: val_loss improved from 0.91370 to 0.84694, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_6_conv_checkpoint/008-0.8469.hdf5\n",
      "36805/36805 [==============================] - 18s 493us/sample - loss: 0.9577 - acc: 0.7110 - val_loss: 0.8469 - val_acc: 0.7456\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8962 - acc: 0.7271\n",
      "Epoch 00009: val_loss improved from 0.84694 to 0.79023, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_6_conv_checkpoint/009-0.7902.hdf5\n",
      "36805/36805 [==============================] - 18s 498us/sample - loss: 0.8961 - acc: 0.7272 - val_loss: 0.7902 - val_acc: 0.7738\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8396 - acc: 0.7466\n",
      "Epoch 00010: val_loss improved from 0.79023 to 0.75299, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_6_conv_checkpoint/010-0.7530.hdf5\n",
      "36805/36805 [==============================] - 18s 495us/sample - loss: 0.8396 - acc: 0.7466 - val_loss: 0.7530 - val_acc: 0.7761\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7957 - acc: 0.7617\n",
      "Epoch 00011: val_loss improved from 0.75299 to 0.69959, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_6_conv_checkpoint/011-0.6996.hdf5\n",
      "36805/36805 [==============================] - 18s 500us/sample - loss: 0.7956 - acc: 0.7617 - val_loss: 0.6996 - val_acc: 0.7950\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7500 - acc: 0.7741\n",
      "Epoch 00012: val_loss improved from 0.69959 to 0.65877, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_6_conv_checkpoint/012-0.6588.hdf5\n",
      "36805/36805 [==============================] - 18s 496us/sample - loss: 0.7500 - acc: 0.7741 - val_loss: 0.6588 - val_acc: 0.8125\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7100 - acc: 0.7853\n",
      "Epoch 00013: val_loss improved from 0.65877 to 0.63972, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_6_conv_checkpoint/013-0.6397.hdf5\n",
      "36805/36805 [==============================] - 18s 502us/sample - loss: 0.7100 - acc: 0.7854 - val_loss: 0.6397 - val_acc: 0.8134\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6761 - acc: 0.7976\n",
      "Epoch 00014: val_loss did not improve from 0.63972\n",
      "36805/36805 [==============================] - 18s 496us/sample - loss: 0.6763 - acc: 0.7975 - val_loss: 0.6716 - val_acc: 0.8015\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6441 - acc: 0.8046\n",
      "Epoch 00015: val_loss improved from 0.63972 to 0.57700, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_6_conv_checkpoint/015-0.5770.hdf5\n",
      "36805/36805 [==============================] - 18s 501us/sample - loss: 0.6441 - acc: 0.8046 - val_loss: 0.5770 - val_acc: 0.8283\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6086 - acc: 0.8147\n",
      "Epoch 00016: val_loss improved from 0.57700 to 0.56709, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_6_conv_checkpoint/016-0.5671.hdf5\n",
      "36805/36805 [==============================] - 18s 499us/sample - loss: 0.6085 - acc: 0.8147 - val_loss: 0.5671 - val_acc: 0.8330\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5846 - acc: 0.8230\n",
      "Epoch 00017: val_loss improved from 0.56709 to 0.53661, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_6_conv_checkpoint/017-0.5366.hdf5\n",
      "36805/36805 [==============================] - 19s 504us/sample - loss: 0.5845 - acc: 0.8230 - val_loss: 0.5366 - val_acc: 0.8460\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5599 - acc: 0.8300\n",
      "Epoch 00018: val_loss improved from 0.53661 to 0.51211, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_6_conv_checkpoint/018-0.5121.hdf5\n",
      "36805/36805 [==============================] - 18s 497us/sample - loss: 0.5599 - acc: 0.8300 - val_loss: 0.5121 - val_acc: 0.8563\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5436 - acc: 0.8330\n",
      "Epoch 00019: val_loss improved from 0.51211 to 0.50305, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_6_conv_checkpoint/019-0.5030.hdf5\n",
      "36805/36805 [==============================] - 18s 498us/sample - loss: 0.5436 - acc: 0.8330 - val_loss: 0.5030 - val_acc: 0.8502\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5173 - acc: 0.8432\n",
      "Epoch 00020: val_loss improved from 0.50305 to 0.48851, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_6_conv_checkpoint/020-0.4885.hdf5\n",
      "36805/36805 [==============================] - 18s 498us/sample - loss: 0.5173 - acc: 0.8431 - val_loss: 0.4885 - val_acc: 0.8616\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4917 - acc: 0.8499\n",
      "Epoch 00021: val_loss improved from 0.48851 to 0.46696, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_6_conv_checkpoint/021-0.4670.hdf5\n",
      "36805/36805 [==============================] - 18s 501us/sample - loss: 0.4916 - acc: 0.8500 - val_loss: 0.4670 - val_acc: 0.8677\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4827 - acc: 0.8531\n",
      "Epoch 00022: val_loss did not improve from 0.46696\n",
      "36805/36805 [==============================] - 18s 498us/sample - loss: 0.4827 - acc: 0.8530 - val_loss: 0.4720 - val_acc: 0.8682\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4647 - acc: 0.8582\n",
      "Epoch 00023: val_loss improved from 0.46696 to 0.45590, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_6_conv_checkpoint/023-0.4559.hdf5\n",
      "36805/36805 [==============================] - 18s 496us/sample - loss: 0.4647 - acc: 0.8582 - val_loss: 0.4559 - val_acc: 0.8686\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4467 - acc: 0.8631\n",
      "Epoch 00024: val_loss improved from 0.45590 to 0.45116, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_6_conv_checkpoint/024-0.4512.hdf5\n",
      "36805/36805 [==============================] - 18s 494us/sample - loss: 0.4467 - acc: 0.8631 - val_loss: 0.4512 - val_acc: 0.8670\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4337 - acc: 0.8668\n",
      "Epoch 00025: val_loss improved from 0.45116 to 0.44086, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_6_conv_checkpoint/025-0.4409.hdf5\n",
      "36805/36805 [==============================] - 18s 499us/sample - loss: 0.4339 - acc: 0.8667 - val_loss: 0.4409 - val_acc: 0.8749\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4222 - acc: 0.8696\n",
      "Epoch 00026: val_loss improved from 0.44086 to 0.42067, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_6_conv_checkpoint/026-0.4207.hdf5\n",
      "36805/36805 [==============================] - 18s 499us/sample - loss: 0.4222 - acc: 0.8696 - val_loss: 0.4207 - val_acc: 0.8758\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4016 - acc: 0.8760\n",
      "Epoch 00027: val_loss improved from 0.42067 to 0.41269, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_6_conv_checkpoint/027-0.4127.hdf5\n",
      "36805/36805 [==============================] - 19s 506us/sample - loss: 0.4016 - acc: 0.8760 - val_loss: 0.4127 - val_acc: 0.8835\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3971 - acc: 0.8777\n",
      "Epoch 00028: val_loss improved from 0.41269 to 0.40359, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_6_conv_checkpoint/028-0.4036.hdf5\n",
      "36805/36805 [==============================] - 18s 503us/sample - loss: 0.3971 - acc: 0.8777 - val_loss: 0.4036 - val_acc: 0.8889\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3795 - acc: 0.8831\n",
      "Epoch 00029: val_loss did not improve from 0.40359\n",
      "36805/36805 [==============================] - 18s 497us/sample - loss: 0.3795 - acc: 0.8831 - val_loss: 0.4154 - val_acc: 0.8828\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3748 - acc: 0.8839\n",
      "Epoch 00030: val_loss did not improve from 0.40359\n",
      "36805/36805 [==============================] - 18s 499us/sample - loss: 0.3747 - acc: 0.8839 - val_loss: 0.4130 - val_acc: 0.8859\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3625 - acc: 0.8876\n",
      "Epoch 00031: val_loss improved from 0.40359 to 0.38801, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_6_conv_checkpoint/031-0.3880.hdf5\n",
      "36805/36805 [==============================] - 18s 501us/sample - loss: 0.3625 - acc: 0.8876 - val_loss: 0.3880 - val_acc: 0.8912\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3478 - acc: 0.8910\n",
      "Epoch 00032: val_loss improved from 0.38801 to 0.38029, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_6_conv_checkpoint/032-0.3803.hdf5\n",
      "36805/36805 [==============================] - 18s 497us/sample - loss: 0.3479 - acc: 0.8910 - val_loss: 0.3803 - val_acc: 0.8940\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3397 - acc: 0.8950\n",
      "Epoch 00033: val_loss did not improve from 0.38029\n",
      "36805/36805 [==============================] - 18s 495us/sample - loss: 0.3397 - acc: 0.8950 - val_loss: 0.3876 - val_acc: 0.8915\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3324 - acc: 0.8957\n",
      "Epoch 00034: val_loss improved from 0.38029 to 0.37104, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_6_conv_checkpoint/034-0.3710.hdf5\n",
      "36805/36805 [==============================] - 19s 505us/sample - loss: 0.3324 - acc: 0.8957 - val_loss: 0.3710 - val_acc: 0.8989\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3246 - acc: 0.8982\n",
      "Epoch 00035: val_loss did not improve from 0.37104\n",
      "36805/36805 [==============================] - 18s 495us/sample - loss: 0.3245 - acc: 0.8982 - val_loss: 0.3815 - val_acc: 0.8926\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3129 - acc: 0.9043\n",
      "Epoch 00036: val_loss improved from 0.37104 to 0.36625, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_6_conv_checkpoint/036-0.3662.hdf5\n",
      "36805/36805 [==============================] - 18s 497us/sample - loss: 0.3129 - acc: 0.9043 - val_loss: 0.3662 - val_acc: 0.8984\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3076 - acc: 0.9043\n",
      "Epoch 00037: val_loss did not improve from 0.36625\n",
      "36805/36805 [==============================] - 18s 497us/sample - loss: 0.3076 - acc: 0.9043 - val_loss: 0.3668 - val_acc: 0.8973\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2985 - acc: 0.9067\n",
      "Epoch 00038: val_loss improved from 0.36625 to 0.34929, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_6_conv_checkpoint/038-0.3493.hdf5\n",
      "36805/36805 [==============================] - 18s 498us/sample - loss: 0.2985 - acc: 0.9068 - val_loss: 0.3493 - val_acc: 0.9066\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2928 - acc: 0.9081\n",
      "Epoch 00039: val_loss did not improve from 0.34929\n",
      "36805/36805 [==============================] - 18s 497us/sample - loss: 0.2929 - acc: 0.9081 - val_loss: 0.3685 - val_acc: 0.8926\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2888 - acc: 0.9091\n",
      "Epoch 00040: val_loss did not improve from 0.34929\n",
      "36805/36805 [==============================] - 18s 502us/sample - loss: 0.2888 - acc: 0.9091 - val_loss: 0.3512 - val_acc: 0.9022\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2728 - acc: 0.9141\n",
      "Epoch 00041: val_loss did not improve from 0.34929\n",
      "36805/36805 [==============================] - 18s 496us/sample - loss: 0.2728 - acc: 0.9141 - val_loss: 0.3569 - val_acc: 0.9045\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2703 - acc: 0.9146\n",
      "Epoch 00042: val_loss did not improve from 0.34929\n",
      "36805/36805 [==============================] - 18s 499us/sample - loss: 0.2704 - acc: 0.9145 - val_loss: 0.3494 - val_acc: 0.9022\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2656 - acc: 0.9149\n",
      "Epoch 00043: val_loss did not improve from 0.34929\n",
      "36805/36805 [==============================] - 18s 499us/sample - loss: 0.2656 - acc: 0.9150 - val_loss: 0.3498 - val_acc: 0.9008\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2586 - acc: 0.9199\n",
      "Epoch 00044: val_loss improved from 0.34929 to 0.34671, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_6_conv_checkpoint/044-0.3467.hdf5\n",
      "36805/36805 [==============================] - 18s 500us/sample - loss: 0.2586 - acc: 0.9199 - val_loss: 0.3467 - val_acc: 0.9071\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2557 - acc: 0.9186\n",
      "Epoch 00045: val_loss improved from 0.34671 to 0.34083, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_6_conv_checkpoint/045-0.3408.hdf5\n",
      "36805/36805 [==============================] - 18s 501us/sample - loss: 0.2557 - acc: 0.9186 - val_loss: 0.3408 - val_acc: 0.9066\n",
      "Epoch 46/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2436 - acc: 0.9226\n",
      "Epoch 00046: val_loss did not improve from 0.34083\n",
      "36805/36805 [==============================] - 18s 499us/sample - loss: 0.2435 - acc: 0.9226 - val_loss: 0.3468 - val_acc: 0.9057\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2363 - acc: 0.9241\n",
      "Epoch 00047: val_loss improved from 0.34083 to 0.32737, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_6_conv_checkpoint/047-0.3274.hdf5\n",
      "36805/36805 [==============================] - 18s 497us/sample - loss: 0.2363 - acc: 0.9241 - val_loss: 0.3274 - val_acc: 0.9110\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2357 - acc: 0.9250\n",
      "Epoch 00048: val_loss did not improve from 0.32737\n",
      "36805/36805 [==============================] - 18s 500us/sample - loss: 0.2358 - acc: 0.9249 - val_loss: 0.3393 - val_acc: 0.9085\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2292 - acc: 0.9264\n",
      "Epoch 00049: val_loss improved from 0.32737 to 0.32494, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_6_conv_checkpoint/049-0.3249.hdf5\n",
      "36805/36805 [==============================] - 18s 493us/sample - loss: 0.2292 - acc: 0.9265 - val_loss: 0.3249 - val_acc: 0.9133\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2290 - acc: 0.9263\n",
      "Epoch 00050: val_loss did not improve from 0.32494\n",
      "36805/36805 [==============================] - 18s 493us/sample - loss: 0.2290 - acc: 0.9263 - val_loss: 0.3466 - val_acc: 0.9029\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2238 - acc: 0.9275\n",
      "Epoch 00051: val_loss did not improve from 0.32494\n",
      "36805/36805 [==============================] - 18s 498us/sample - loss: 0.2238 - acc: 0.9275 - val_loss: 0.3277 - val_acc: 0.9126\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2199 - acc: 0.9293\n",
      "Epoch 00052: val_loss did not improve from 0.32494\n",
      "36805/36805 [==============================] - 18s 494us/sample - loss: 0.2199 - acc: 0.9293 - val_loss: 0.3443 - val_acc: 0.9096\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2154 - acc: 0.9299\n",
      "Epoch 00053: val_loss did not improve from 0.32494\n",
      "36805/36805 [==============================] - 18s 494us/sample - loss: 0.2154 - acc: 0.9299 - val_loss: 0.3374 - val_acc: 0.9073\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2087 - acc: 0.9328\n",
      "Epoch 00054: val_loss improved from 0.32494 to 0.32448, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_6_conv_checkpoint/054-0.3245.hdf5\n",
      "36805/36805 [==============================] - 18s 494us/sample - loss: 0.2087 - acc: 0.9328 - val_loss: 0.3245 - val_acc: 0.9129\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2043 - acc: 0.9336\n",
      "Epoch 00055: val_loss did not improve from 0.32448\n",
      "36805/36805 [==============================] - 18s 497us/sample - loss: 0.2044 - acc: 0.9336 - val_loss: 0.3411 - val_acc: 0.9094\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1988 - acc: 0.9359\n",
      "Epoch 00056: val_loss did not improve from 0.32448\n",
      "36805/36805 [==============================] - 18s 500us/sample - loss: 0.1989 - acc: 0.9359 - val_loss: 0.3307 - val_acc: 0.9071\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2001 - acc: 0.9355\n",
      "Epoch 00057: val_loss improved from 0.32448 to 0.32145, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_6_conv_checkpoint/057-0.3214.hdf5\n",
      "36805/36805 [==============================] - 18s 495us/sample - loss: 0.2000 - acc: 0.9355 - val_loss: 0.3214 - val_acc: 0.9126\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1907 - acc: 0.9385\n",
      "Epoch 00058: val_loss did not improve from 0.32145\n",
      "36805/36805 [==============================] - 18s 496us/sample - loss: 0.1907 - acc: 0.9385 - val_loss: 0.3478 - val_acc: 0.9092\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1902 - acc: 0.9361\n",
      "Epoch 00059: val_loss did not improve from 0.32145\n",
      "36805/36805 [==============================] - 18s 498us/sample - loss: 0.1902 - acc: 0.9361 - val_loss: 0.3345 - val_acc: 0.9147\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1837 - acc: 0.9400\n",
      "Epoch 00060: val_loss did not improve from 0.32145\n",
      "36805/36805 [==============================] - 18s 493us/sample - loss: 0.1837 - acc: 0.9400 - val_loss: 0.3438 - val_acc: 0.9064\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1760 - acc: 0.9429\n",
      "Epoch 00061: val_loss did not improve from 0.32145\n",
      "36805/36805 [==============================] - 18s 499us/sample - loss: 0.1760 - acc: 0.9429 - val_loss: 0.3303 - val_acc: 0.9126\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1744 - acc: 0.9433\n",
      "Epoch 00062: val_loss did not improve from 0.32145\n",
      "36805/36805 [==============================] - 18s 498us/sample - loss: 0.1744 - acc: 0.9433 - val_loss: 0.3235 - val_acc: 0.9143\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1750 - acc: 0.9431\n",
      "Epoch 00063: val_loss did not improve from 0.32145\n",
      "36805/36805 [==============================] - 18s 498us/sample - loss: 0.1750 - acc: 0.9431 - val_loss: 0.3236 - val_acc: 0.9145\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1695 - acc: 0.9454\n",
      "Epoch 00064: val_loss did not improve from 0.32145\n",
      "36805/36805 [==============================] - 18s 495us/sample - loss: 0.1695 - acc: 0.9454 - val_loss: 0.3253 - val_acc: 0.9166\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1688 - acc: 0.9447\n",
      "Epoch 00065: val_loss improved from 0.32145 to 0.32002, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_6_conv_checkpoint/065-0.3200.hdf5\n",
      "36805/36805 [==============================] - 18s 499us/sample - loss: 0.1688 - acc: 0.9447 - val_loss: 0.3200 - val_acc: 0.9173\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1680 - acc: 0.9447\n",
      "Epoch 00066: val_loss did not improve from 0.32002\n",
      "36805/36805 [==============================] - 18s 498us/sample - loss: 0.1680 - acc: 0.9447 - val_loss: 0.3283 - val_acc: 0.9150\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1607 - acc: 0.9464\n",
      "Epoch 00067: val_loss did not improve from 0.32002\n",
      "36805/36805 [==============================] - 18s 498us/sample - loss: 0.1607 - acc: 0.9464 - val_loss: 0.3228 - val_acc: 0.9154\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1593 - acc: 0.9483\n",
      "Epoch 00068: val_loss did not improve from 0.32002\n",
      "36805/36805 [==============================] - 18s 489us/sample - loss: 0.1593 - acc: 0.9483 - val_loss: 0.3260 - val_acc: 0.9117\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1572 - acc: 0.9483\n",
      "Epoch 00069: val_loss did not improve from 0.32002\n",
      "36805/36805 [==============================] - 18s 485us/sample - loss: 0.1572 - acc: 0.9483 - val_loss: 0.3351 - val_acc: 0.9133\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1555 - acc: 0.9489\n",
      "Epoch 00070: val_loss did not improve from 0.32002\n",
      "36805/36805 [==============================] - 18s 497us/sample - loss: 0.1554 - acc: 0.9489 - val_loss: 0.3242 - val_acc: 0.9157\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1539 - acc: 0.9488\n",
      "Epoch 00071: val_loss did not improve from 0.32002\n",
      "36805/36805 [==============================] - 18s 491us/sample - loss: 0.1539 - acc: 0.9488 - val_loss: 0.3381 - val_acc: 0.9119\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1471 - acc: 0.9521\n",
      "Epoch 00072: val_loss did not improve from 0.32002\n",
      "36805/36805 [==============================] - 18s 497us/sample - loss: 0.1471 - acc: 0.9521 - val_loss: 0.3280 - val_acc: 0.9178\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1452 - acc: 0.9518\n",
      "Epoch 00073: val_loss did not improve from 0.32002\n",
      "36805/36805 [==============================] - 18s 497us/sample - loss: 0.1452 - acc: 0.9518 - val_loss: 0.3269 - val_acc: 0.9166\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1432 - acc: 0.9521\n",
      "Epoch 00074: val_loss did not improve from 0.32002\n",
      "36805/36805 [==============================] - 18s 495us/sample - loss: 0.1431 - acc: 0.9522 - val_loss: 0.3320 - val_acc: 0.9180\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1398 - acc: 0.9536\n",
      "Epoch 00075: val_loss did not improve from 0.32002\n",
      "36805/36805 [==============================] - 18s 493us/sample - loss: 0.1398 - acc: 0.9536 - val_loss: 0.3439 - val_acc: 0.9092\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1421 - acc: 0.9533\n",
      "Epoch 00076: val_loss did not improve from 0.32002\n",
      "36805/36805 [==============================] - 18s 497us/sample - loss: 0.1421 - acc: 0.9533 - val_loss: 0.3266 - val_acc: 0.9150\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1413 - acc: 0.9530\n",
      "Epoch 00077: val_loss did not improve from 0.32002\n",
      "36805/36805 [==============================] - 18s 494us/sample - loss: 0.1412 - acc: 0.9530 - val_loss: 0.3351 - val_acc: 0.9180\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1392 - acc: 0.9544\n",
      "Epoch 00078: val_loss improved from 0.32002 to 0.31894, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_6_conv_checkpoint/078-0.3189.hdf5\n",
      "36805/36805 [==============================] - 18s 499us/sample - loss: 0.1392 - acc: 0.9544 - val_loss: 0.3189 - val_acc: 0.9187\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1337 - acc: 0.9561\n",
      "Epoch 00079: val_loss did not improve from 0.31894\n",
      "36805/36805 [==============================] - 18s 498us/sample - loss: 0.1337 - acc: 0.9561 - val_loss: 0.3260 - val_acc: 0.9180\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1290 - acc: 0.9578\n",
      "Epoch 00080: val_loss did not improve from 0.31894\n",
      "36805/36805 [==============================] - 18s 495us/sample - loss: 0.1290 - acc: 0.9578 - val_loss: 0.3192 - val_acc: 0.9192\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1252 - acc: 0.9579\n",
      "Epoch 00081: val_loss did not improve from 0.31894\n",
      "36805/36805 [==============================] - 18s 498us/sample - loss: 0.1252 - acc: 0.9579 - val_loss: 0.3310 - val_acc: 0.9171\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1300 - acc: 0.9562\n",
      "Epoch 00082: val_loss did not improve from 0.31894\n",
      "36805/36805 [==============================] - 18s 492us/sample - loss: 0.1300 - acc: 0.9562 - val_loss: 0.3305 - val_acc: 0.9222\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1232 - acc: 0.9595\n",
      "Epoch 00083: val_loss did not improve from 0.31894\n",
      "36805/36805 [==============================] - 18s 497us/sample - loss: 0.1232 - acc: 0.9595 - val_loss: 0.3204 - val_acc: 0.9215\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1261 - acc: 0.9579\n",
      "Epoch 00084: val_loss did not improve from 0.31894\n",
      "36805/36805 [==============================] - 18s 494us/sample - loss: 0.1261 - acc: 0.9579 - val_loss: 0.3275 - val_acc: 0.9185\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1229 - acc: 0.9599\n",
      "Epoch 00085: val_loss did not improve from 0.31894\n",
      "36805/36805 [==============================] - 18s 497us/sample - loss: 0.1229 - acc: 0.9599 - val_loss: 0.3195 - val_acc: 0.9175\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1200 - acc: 0.9596\n",
      "Epoch 00086: val_loss improved from 0.31894 to 0.31423, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_6_conv_checkpoint/086-0.3142.hdf5\n",
      "36805/36805 [==============================] - 18s 497us/sample - loss: 0.1200 - acc: 0.9596 - val_loss: 0.3142 - val_acc: 0.9236\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1166 - acc: 0.9613\n",
      "Epoch 00087: val_loss improved from 0.31423 to 0.31331, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_6_conv_checkpoint/087-0.3133.hdf5\n",
      "36805/36805 [==============================] - 18s 494us/sample - loss: 0.1166 - acc: 0.9613 - val_loss: 0.3133 - val_acc: 0.9215\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1168 - acc: 0.9608\n",
      "Epoch 00088: val_loss did not improve from 0.31331\n",
      "36805/36805 [==============================] - 18s 496us/sample - loss: 0.1168 - acc: 0.9608 - val_loss: 0.3190 - val_acc: 0.9194\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1131 - acc: 0.9627\n",
      "Epoch 00089: val_loss did not improve from 0.31331\n",
      "36805/36805 [==============================] - 18s 494us/sample - loss: 0.1131 - acc: 0.9627 - val_loss: 0.3424 - val_acc: 0.9182\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1161 - acc: 0.9609\n",
      "Epoch 00090: val_loss did not improve from 0.31331\n",
      "36805/36805 [==============================] - 18s 494us/sample - loss: 0.1161 - acc: 0.9608 - val_loss: 0.3230 - val_acc: 0.9236\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1127 - acc: 0.9625\n",
      "Epoch 00091: val_loss did not improve from 0.31331\n",
      "36805/36805 [==============================] - 18s 494us/sample - loss: 0.1127 - acc: 0.9625 - val_loss: 0.3265 - val_acc: 0.9194\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1100 - acc: 0.9633\n",
      "Epoch 00092: val_loss did not improve from 0.31331\n",
      "36805/36805 [==============================] - 18s 498us/sample - loss: 0.1100 - acc: 0.9633 - val_loss: 0.3216 - val_acc: 0.9210\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1043 - acc: 0.9652\n",
      "Epoch 00093: val_loss did not improve from 0.31331\n",
      "36805/36805 [==============================] - 18s 498us/sample - loss: 0.1043 - acc: 0.9652 - val_loss: 0.3389 - val_acc: 0.9180\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1087 - acc: 0.9636\n",
      "Epoch 00094: val_loss did not improve from 0.31331\n",
      "36805/36805 [==============================] - 18s 494us/sample - loss: 0.1087 - acc: 0.9636 - val_loss: 0.3279 - val_acc: 0.9201\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1086 - acc: 0.9636\n",
      "Epoch 00095: val_loss did not improve from 0.31331\n",
      "36805/36805 [==============================] - 18s 496us/sample - loss: 0.1086 - acc: 0.9635 - val_loss: 0.3514 - val_acc: 0.9157\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1081 - acc: 0.9641\n",
      "Epoch 00096: val_loss did not improve from 0.31331\n",
      "36805/36805 [==============================] - 18s 494us/sample - loss: 0.1081 - acc: 0.9641 - val_loss: 0.3347 - val_acc: 0.9208\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1066 - acc: 0.9642\n",
      "Epoch 00097: val_loss did not improve from 0.31331\n",
      "36805/36805 [==============================] - 18s 496us/sample - loss: 0.1067 - acc: 0.9642 - val_loss: 0.3289 - val_acc: 0.9220\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1033 - acc: 0.9657\n",
      "Epoch 00098: val_loss did not improve from 0.31331\n",
      "36805/36805 [==============================] - 18s 495us/sample - loss: 0.1032 - acc: 0.9657 - val_loss: 0.3449 - val_acc: 0.9196\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0998 - acc: 0.9671\n",
      "Epoch 00099: val_loss did not improve from 0.31331\n",
      "36805/36805 [==============================] - 18s 494us/sample - loss: 0.0998 - acc: 0.9671 - val_loss: 0.3434 - val_acc: 0.9173\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0960 - acc: 0.9680\n",
      "Epoch 00100: val_loss did not improve from 0.31331\n",
      "36805/36805 [==============================] - 18s 495us/sample - loss: 0.0960 - acc: 0.9680 - val_loss: 0.3421 - val_acc: 0.9220\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0975 - acc: 0.9677\n",
      "Epoch 00101: val_loss did not improve from 0.31331\n",
      "36805/36805 [==============================] - 18s 499us/sample - loss: 0.0975 - acc: 0.9677 - val_loss: 0.3288 - val_acc: 0.9231\n",
      "Epoch 102/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0978 - acc: 0.9675\n",
      "Epoch 00102: val_loss did not improve from 0.31331\n",
      "36805/36805 [==============================] - 18s 495us/sample - loss: 0.0977 - acc: 0.9675 - val_loss: 0.3396 - val_acc: 0.9201\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0953 - acc: 0.9685\n",
      "Epoch 00103: val_loss did not improve from 0.31331\n",
      "36805/36805 [==============================] - 18s 496us/sample - loss: 0.0953 - acc: 0.9685 - val_loss: 0.3301 - val_acc: 0.9222\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0924 - acc: 0.9679\n",
      "Epoch 00104: val_loss did not improve from 0.31331\n",
      "36805/36805 [==============================] - 18s 496us/sample - loss: 0.0924 - acc: 0.9679 - val_loss: 0.3402 - val_acc: 0.9217\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0910 - acc: 0.9698\n",
      "Epoch 00105: val_loss did not improve from 0.31331\n",
      "36805/36805 [==============================] - 18s 497us/sample - loss: 0.0910 - acc: 0.9698 - val_loss: 0.3392 - val_acc: 0.9238\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0965 - acc: 0.9678\n",
      "Epoch 00106: val_loss did not improve from 0.31331\n",
      "36805/36805 [==============================] - 18s 497us/sample - loss: 0.0964 - acc: 0.9678 - val_loss: 0.3386 - val_acc: 0.9252\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0881 - acc: 0.9717\n",
      "Epoch 00107: val_loss did not improve from 0.31331\n",
      "36805/36805 [==============================] - 18s 497us/sample - loss: 0.0880 - acc: 0.9717 - val_loss: 0.3315 - val_acc: 0.9273\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0874 - acc: 0.9712\n",
      "Epoch 00108: val_loss did not improve from 0.31331\n",
      "36805/36805 [==============================] - 18s 498us/sample - loss: 0.0874 - acc: 0.9713 - val_loss: 0.3263 - val_acc: 0.9243\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0910 - acc: 0.9693\n",
      "Epoch 00109: val_loss did not improve from 0.31331\n",
      "36805/36805 [==============================] - 18s 495us/sample - loss: 0.0910 - acc: 0.9693 - val_loss: 0.3308 - val_acc: 0.9231\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0892 - acc: 0.9703\n",
      "Epoch 00110: val_loss did not improve from 0.31331\n",
      "36805/36805 [==============================] - 18s 496us/sample - loss: 0.0892 - acc: 0.9703 - val_loss: 0.3371 - val_acc: 0.9231\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0887 - acc: 0.9705\n",
      "Epoch 00111: val_loss did not improve from 0.31331\n",
      "36805/36805 [==============================] - 18s 497us/sample - loss: 0.0887 - acc: 0.9705 - val_loss: 0.3321 - val_acc: 0.9248\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0849 - acc: 0.9712\n",
      "Epoch 00112: val_loss did not improve from 0.31331\n",
      "36805/36805 [==============================] - 18s 493us/sample - loss: 0.0849 - acc: 0.9712 - val_loss: 0.3385 - val_acc: 0.9248\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0907 - acc: 0.9698\n",
      "Epoch 00113: val_loss did not improve from 0.31331\n",
      "36805/36805 [==============================] - 18s 497us/sample - loss: 0.0907 - acc: 0.9698 - val_loss: 0.3315 - val_acc: 0.9259\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0830 - acc: 0.9723\n",
      "Epoch 00114: val_loss did not improve from 0.31331\n",
      "36805/36805 [==============================] - 18s 490us/sample - loss: 0.0830 - acc: 0.9723 - val_loss: 0.3350 - val_acc: 0.9252\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0823 - acc: 0.9725\n",
      "Epoch 00115: val_loss did not improve from 0.31331\n",
      "36805/36805 [==============================] - 18s 492us/sample - loss: 0.0823 - acc: 0.9725 - val_loss: 0.3378 - val_acc: 0.9245\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0836 - acc: 0.9718\n",
      "Epoch 00116: val_loss did not improve from 0.31331\n",
      "36805/36805 [==============================] - 18s 495us/sample - loss: 0.0836 - acc: 0.9718 - val_loss: 0.3346 - val_acc: 0.9271\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0833 - acc: 0.9734\n",
      "Epoch 00117: val_loss did not improve from 0.31331\n",
      "36805/36805 [==============================] - 18s 492us/sample - loss: 0.0833 - acc: 0.9734 - val_loss: 0.3409 - val_acc: 0.9248\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0801 - acc: 0.9735\n",
      "Epoch 00118: val_loss did not improve from 0.31331\n",
      "36805/36805 [==============================] - 18s 496us/sample - loss: 0.0801 - acc: 0.9735 - val_loss: 0.3427 - val_acc: 0.9241\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0823 - acc: 0.9728\n",
      "Epoch 00119: val_loss did not improve from 0.31331\n",
      "36805/36805 [==============================] - 18s 498us/sample - loss: 0.0823 - acc: 0.9728 - val_loss: 0.3558 - val_acc: 0.9222\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0807 - acc: 0.9730\n",
      "Epoch 00120: val_loss did not improve from 0.31331\n",
      "36805/36805 [==============================] - 18s 494us/sample - loss: 0.0807 - acc: 0.9730 - val_loss: 0.3474 - val_acc: 0.9262\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0780 - acc: 0.9738\n",
      "Epoch 00121: val_loss did not improve from 0.31331\n",
      "36805/36805 [==============================] - 18s 498us/sample - loss: 0.0780 - acc: 0.9738 - val_loss: 0.3461 - val_acc: 0.9245\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0775 - acc: 0.9747\n",
      "Epoch 00122: val_loss did not improve from 0.31331\n",
      "36805/36805 [==============================] - 18s 494us/sample - loss: 0.0775 - acc: 0.9747 - val_loss: 0.3331 - val_acc: 0.9266\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0804 - acc: 0.9729\n",
      "Epoch 00123: val_loss did not improve from 0.31331\n",
      "36805/36805 [==============================] - 18s 495us/sample - loss: 0.0804 - acc: 0.9729 - val_loss: 0.3547 - val_acc: 0.9229\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0818 - acc: 0.9732\n",
      "Epoch 00124: val_loss did not improve from 0.31331\n",
      "36805/36805 [==============================] - 18s 496us/sample - loss: 0.0818 - acc: 0.9732 - val_loss: 0.3522 - val_acc: 0.9217\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0774 - acc: 0.9751\n",
      "Epoch 00125: val_loss did not improve from 0.31331\n",
      "36805/36805 [==============================] - 18s 501us/sample - loss: 0.0774 - acc: 0.9751 - val_loss: 0.3433 - val_acc: 0.9273\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0742 - acc: 0.9757\n",
      "Epoch 00126: val_loss did not improve from 0.31331\n",
      "36805/36805 [==============================] - 18s 498us/sample - loss: 0.0741 - acc: 0.9757 - val_loss: 0.3469 - val_acc: 0.9250\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0764 - acc: 0.9750\n",
      "Epoch 00127: val_loss did not improve from 0.31331\n",
      "36805/36805 [==============================] - 18s 497us/sample - loss: 0.0765 - acc: 0.9750 - val_loss: 0.3488 - val_acc: 0.9262\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0740 - acc: 0.9756\n",
      "Epoch 00128: val_loss did not improve from 0.31331\n",
      "36805/36805 [==============================] - 18s 499us/sample - loss: 0.0739 - acc: 0.9756 - val_loss: 0.3397 - val_acc: 0.9278\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0747 - acc: 0.9761\n",
      "Epoch 00129: val_loss did not improve from 0.31331\n",
      "36805/36805 [==============================] - 18s 497us/sample - loss: 0.0747 - acc: 0.9761 - val_loss: 0.3402 - val_acc: 0.9259\n",
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0762 - acc: 0.9752\n",
      "Epoch 00130: val_loss did not improve from 0.31331\n",
      "36805/36805 [==============================] - 18s 496us/sample - loss: 0.0762 - acc: 0.9752 - val_loss: 0.3546 - val_acc: 0.9241\n",
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0782 - acc: 0.9735\n",
      "Epoch 00131: val_loss did not improve from 0.31331\n",
      "36805/36805 [==============================] - 18s 500us/sample - loss: 0.0782 - acc: 0.9735 - val_loss: 0.3321 - val_acc: 0.9278\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0744 - acc: 0.9751\n",
      "Epoch 00132: val_loss did not improve from 0.31331\n",
      "36805/36805 [==============================] - 18s 495us/sample - loss: 0.0744 - acc: 0.9751 - val_loss: 0.3403 - val_acc: 0.9252\n",
      "Epoch 133/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0702 - acc: 0.9774\n",
      "Epoch 00133: val_loss did not improve from 0.31331\n",
      "36805/36805 [==============================] - 18s 497us/sample - loss: 0.0702 - acc: 0.9774 - val_loss: 0.3444 - val_acc: 0.9236\n",
      "Epoch 134/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0704 - acc: 0.9768\n",
      "Epoch 00134: val_loss did not improve from 0.31331\n",
      "36805/36805 [==============================] - 18s 493us/sample - loss: 0.0703 - acc: 0.9768 - val_loss: 0.3532 - val_acc: 0.9269\n",
      "Epoch 135/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0709 - acc: 0.9769\n",
      "Epoch 00135: val_loss did not improve from 0.31331\n",
      "36805/36805 [==============================] - 18s 497us/sample - loss: 0.0709 - acc: 0.9769 - val_loss: 0.3422 - val_acc: 0.9269\n",
      "Epoch 136/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0654 - acc: 0.9791\n",
      "Epoch 00136: val_loss did not improve from 0.31331\n",
      "36805/36805 [==============================] - 18s 494us/sample - loss: 0.0654 - acc: 0.9791 - val_loss: 0.3440 - val_acc: 0.9257\n",
      "Epoch 137/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0708 - acc: 0.9769\n",
      "Epoch 00137: val_loss did not improve from 0.31331\n",
      "36805/36805 [==============================] - 18s 499us/sample - loss: 0.0708 - acc: 0.9769 - val_loss: 0.3530 - val_acc: 0.9250\n",
      "\n",
      "1D_CNN_custom_multi_3_concat_ch_32_DO_6_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 1s 293us/sample - loss: 0.4121 - acc: 0.8947\n",
      "Loss: 0.41212150227986394 Accuracy: 0.89470404\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.3553 - acc: 0.2211\n",
      "Epoch 00001: val_loss improved from inf to 1.80643, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_7_conv_checkpoint/001-1.8064.hdf5\n",
      "36805/36805 [==============================] - 23s 618us/sample - loss: 2.3553 - acc: 0.2211 - val_loss: 1.8064 - val_acc: 0.4330\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6498 - acc: 0.4599\n",
      "Epoch 00002: val_loss improved from 1.80643 to 1.32759, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_7_conv_checkpoint/002-1.3276.hdf5\n",
      "36805/36805 [==============================] - 19s 527us/sample - loss: 1.6500 - acc: 0.4599 - val_loss: 1.3276 - val_acc: 0.5973\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3457 - acc: 0.5674\n",
      "Epoch 00003: val_loss improved from 1.32759 to 1.11486, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_7_conv_checkpoint/003-1.1149.hdf5\n",
      "36805/36805 [==============================] - 19s 525us/sample - loss: 1.3457 - acc: 0.5674 - val_loss: 1.1149 - val_acc: 0.6653\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1769 - acc: 0.6236\n",
      "Epoch 00004: val_loss improved from 1.11486 to 0.98145, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_7_conv_checkpoint/004-0.9815.hdf5\n",
      "36805/36805 [==============================] - 19s 526us/sample - loss: 1.1771 - acc: 0.6236 - val_loss: 0.9815 - val_acc: 0.7046\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0616 - acc: 0.6639\n",
      "Epoch 00005: val_loss improved from 0.98145 to 0.86600, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_7_conv_checkpoint/005-0.8660.hdf5\n",
      "36805/36805 [==============================] - 19s 525us/sample - loss: 1.0617 - acc: 0.6638 - val_loss: 0.8660 - val_acc: 0.7468\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9722 - acc: 0.6959\n",
      "Epoch 00006: val_loss improved from 0.86600 to 0.78482, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_7_conv_checkpoint/006-0.7848.hdf5\n",
      "36805/36805 [==============================] - 19s 519us/sample - loss: 0.9721 - acc: 0.6959 - val_loss: 0.7848 - val_acc: 0.7706\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8978 - acc: 0.7192\n",
      "Epoch 00007: val_loss improved from 0.78482 to 0.72443, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_7_conv_checkpoint/007-0.7244.hdf5\n",
      "36805/36805 [==============================] - 19s 526us/sample - loss: 0.8978 - acc: 0.7192 - val_loss: 0.7244 - val_acc: 0.7904\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8382 - acc: 0.7401\n",
      "Epoch 00008: val_loss improved from 0.72443 to 0.71195, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_7_conv_checkpoint/008-0.7119.hdf5\n",
      "36805/36805 [==============================] - 19s 527us/sample - loss: 0.8383 - acc: 0.7401 - val_loss: 0.7119 - val_acc: 0.7829\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7825 - acc: 0.7544\n",
      "Epoch 00009: val_loss improved from 0.71195 to 0.61717, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_7_conv_checkpoint/009-0.6172.hdf5\n",
      "36805/36805 [==============================] - 19s 522us/sample - loss: 0.7825 - acc: 0.7544 - val_loss: 0.6172 - val_acc: 0.8169\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7262 - acc: 0.7746\n",
      "Epoch 00010: val_loss improved from 0.61717 to 0.61306, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_7_conv_checkpoint/010-0.6131.hdf5\n",
      "36805/36805 [==============================] - 19s 527us/sample - loss: 0.7264 - acc: 0.7746 - val_loss: 0.6131 - val_acc: 0.8176\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6904 - acc: 0.7857\n",
      "Epoch 00011: val_loss improved from 0.61306 to 0.60220, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_7_conv_checkpoint/011-0.6022.hdf5\n",
      "36805/36805 [==============================] - 19s 528us/sample - loss: 0.6904 - acc: 0.7857 - val_loss: 0.6022 - val_acc: 0.8143\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6462 - acc: 0.7995\n",
      "Epoch 00012: val_loss improved from 0.60220 to 0.51352, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_7_conv_checkpoint/012-0.5135.hdf5\n",
      "36805/36805 [==============================] - 19s 527us/sample - loss: 0.6461 - acc: 0.7995 - val_loss: 0.5135 - val_acc: 0.8460\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6121 - acc: 0.8093\n",
      "Epoch 00013: val_loss improved from 0.51352 to 0.48451, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_7_conv_checkpoint/013-0.4845.hdf5\n",
      "36805/36805 [==============================] - 19s 528us/sample - loss: 0.6121 - acc: 0.8093 - val_loss: 0.4845 - val_acc: 0.8553\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5787 - acc: 0.8215\n",
      "Epoch 00014: val_loss improved from 0.48451 to 0.45259, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_7_conv_checkpoint/014-0.4526.hdf5\n",
      "36805/36805 [==============================] - 19s 526us/sample - loss: 0.5787 - acc: 0.8215 - val_loss: 0.4526 - val_acc: 0.8607\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5550 - acc: 0.8271\n",
      "Epoch 00015: val_loss improved from 0.45259 to 0.43631, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_7_conv_checkpoint/015-0.4363.hdf5\n",
      "36805/36805 [==============================] - 19s 518us/sample - loss: 0.5550 - acc: 0.8270 - val_loss: 0.4363 - val_acc: 0.8693\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5302 - acc: 0.8347\n",
      "Epoch 00016: val_loss improved from 0.43631 to 0.41249, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_7_conv_checkpoint/016-0.4125.hdf5\n",
      "36805/36805 [==============================] - 19s 521us/sample - loss: 0.5302 - acc: 0.8348 - val_loss: 0.4125 - val_acc: 0.8751\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5077 - acc: 0.8430\n",
      "Epoch 00017: val_loss improved from 0.41249 to 0.40489, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_7_conv_checkpoint/017-0.4049.hdf5\n",
      "36805/36805 [==============================] - 19s 527us/sample - loss: 0.5077 - acc: 0.8430 - val_loss: 0.4049 - val_acc: 0.8772\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4852 - acc: 0.8499\n",
      "Epoch 00018: val_loss improved from 0.40489 to 0.37717, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_7_conv_checkpoint/018-0.3772.hdf5\n",
      "36805/36805 [==============================] - 19s 525us/sample - loss: 0.4852 - acc: 0.8499 - val_loss: 0.3772 - val_acc: 0.8835\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4599 - acc: 0.8568\n",
      "Epoch 00019: val_loss improved from 0.37717 to 0.35719, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_7_conv_checkpoint/019-0.3572.hdf5\n",
      "36805/36805 [==============================] - 19s 524us/sample - loss: 0.4600 - acc: 0.8568 - val_loss: 0.3572 - val_acc: 0.8935\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4418 - acc: 0.8613\n",
      "Epoch 00020: val_loss improved from 0.35719 to 0.35493, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_7_conv_checkpoint/020-0.3549.hdf5\n",
      "36805/36805 [==============================] - 19s 527us/sample - loss: 0.4419 - acc: 0.8613 - val_loss: 0.3549 - val_acc: 0.8977\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4267 - acc: 0.8665\n",
      "Epoch 00021: val_loss improved from 0.35493 to 0.32835, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_7_conv_checkpoint/021-0.3283.hdf5\n",
      "36805/36805 [==============================] - 19s 525us/sample - loss: 0.4268 - acc: 0.8665 - val_loss: 0.3283 - val_acc: 0.9050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4078 - acc: 0.8733\n",
      "Epoch 00022: val_loss improved from 0.32835 to 0.32151, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_7_conv_checkpoint/022-0.3215.hdf5\n",
      "36805/36805 [==============================] - 19s 523us/sample - loss: 0.4078 - acc: 0.8733 - val_loss: 0.3215 - val_acc: 0.9066\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3947 - acc: 0.8770\n",
      "Epoch 00023: val_loss improved from 0.32151 to 0.31073, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_7_conv_checkpoint/023-0.3107.hdf5\n",
      "36805/36805 [==============================] - 20s 530us/sample - loss: 0.3947 - acc: 0.8770 - val_loss: 0.3107 - val_acc: 0.9054\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3855 - acc: 0.8794\n",
      "Epoch 00024: val_loss improved from 0.31073 to 0.30046, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_7_conv_checkpoint/024-0.3005.hdf5\n",
      "36805/36805 [==============================] - 19s 528us/sample - loss: 0.3854 - acc: 0.8794 - val_loss: 0.3005 - val_acc: 0.9140\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3697 - acc: 0.8833\n",
      "Epoch 00025: val_loss improved from 0.30046 to 0.29436, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_7_conv_checkpoint/025-0.2944.hdf5\n",
      "36805/36805 [==============================] - 19s 525us/sample - loss: 0.3696 - acc: 0.8833 - val_loss: 0.2944 - val_acc: 0.9152\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3539 - acc: 0.8890\n",
      "Epoch 00026: val_loss improved from 0.29436 to 0.28208, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_7_conv_checkpoint/026-0.2821.hdf5\n",
      "36805/36805 [==============================] - 19s 523us/sample - loss: 0.3540 - acc: 0.8890 - val_loss: 0.2821 - val_acc: 0.9178\n",
      "Epoch 27/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3479 - acc: 0.8915\n",
      "Epoch 00027: val_loss improved from 0.28208 to 0.27458, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_7_conv_checkpoint/027-0.2746.hdf5\n",
      "36805/36805 [==============================] - 19s 529us/sample - loss: 0.3479 - acc: 0.8915 - val_loss: 0.2746 - val_acc: 0.9166\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3390 - acc: 0.8930\n",
      "Epoch 00028: val_loss improved from 0.27458 to 0.26765, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_7_conv_checkpoint/028-0.2676.hdf5\n",
      "36805/36805 [==============================] - 19s 523us/sample - loss: 0.3390 - acc: 0.8930 - val_loss: 0.2676 - val_acc: 0.9234\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3266 - acc: 0.8984\n",
      "Epoch 00029: val_loss improved from 0.26765 to 0.26083, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_7_conv_checkpoint/029-0.2608.hdf5\n",
      "36805/36805 [==============================] - 19s 524us/sample - loss: 0.3266 - acc: 0.8984 - val_loss: 0.2608 - val_acc: 0.9229\n",
      "Epoch 30/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3198 - acc: 0.8987\n",
      "Epoch 00030: val_loss improved from 0.26083 to 0.25963, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_7_conv_checkpoint/030-0.2596.hdf5\n",
      "36805/36805 [==============================] - 19s 528us/sample - loss: 0.3198 - acc: 0.8987 - val_loss: 0.2596 - val_acc: 0.9241\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3057 - acc: 0.9035\n",
      "Epoch 00031: val_loss improved from 0.25963 to 0.25515, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_7_conv_checkpoint/031-0.2552.hdf5\n",
      "36805/36805 [==============================] - 20s 531us/sample - loss: 0.3056 - acc: 0.9035 - val_loss: 0.2552 - val_acc: 0.9259\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2979 - acc: 0.9080\n",
      "Epoch 00032: val_loss did not improve from 0.25515\n",
      "36805/36805 [==============================] - 19s 522us/sample - loss: 0.2979 - acc: 0.9080 - val_loss: 0.2582 - val_acc: 0.9199\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2911 - acc: 0.9104\n",
      "Epoch 00033: val_loss improved from 0.25515 to 0.23503, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_7_conv_checkpoint/033-0.2350.hdf5\n",
      "36805/36805 [==============================] - 19s 523us/sample - loss: 0.2911 - acc: 0.9104 - val_loss: 0.2350 - val_acc: 0.9322\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2860 - acc: 0.9094\n",
      "Epoch 00034: val_loss improved from 0.23503 to 0.22931, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_7_conv_checkpoint/034-0.2293.hdf5\n",
      "36805/36805 [==============================] - 19s 514us/sample - loss: 0.2860 - acc: 0.9094 - val_loss: 0.2293 - val_acc: 0.9334\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2741 - acc: 0.9140\n",
      "Epoch 00035: val_loss improved from 0.22931 to 0.22647, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_7_conv_checkpoint/035-0.2265.hdf5\n",
      "36805/36805 [==============================] - 19s 522us/sample - loss: 0.2741 - acc: 0.9140 - val_loss: 0.2265 - val_acc: 0.9341\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2709 - acc: 0.9148\n",
      "Epoch 00036: val_loss did not improve from 0.22647\n",
      "36805/36805 [==============================] - 19s 523us/sample - loss: 0.2708 - acc: 0.9148 - val_loss: 0.2275 - val_acc: 0.9336\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2685 - acc: 0.9152\n",
      "Epoch 00037: val_loss did not improve from 0.22647\n",
      "36805/36805 [==============================] - 19s 519us/sample - loss: 0.2686 - acc: 0.9152 - val_loss: 0.2461 - val_acc: 0.9250\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2596 - acc: 0.9190\n",
      "Epoch 00038: val_loss improved from 0.22647 to 0.21307, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_7_conv_checkpoint/038-0.2131.hdf5\n",
      "36805/36805 [==============================] - 20s 531us/sample - loss: 0.2595 - acc: 0.9190 - val_loss: 0.2131 - val_acc: 0.9345\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2548 - acc: 0.9192\n",
      "Epoch 00039: val_loss did not improve from 0.21307\n",
      "36805/36805 [==============================] - 19s 522us/sample - loss: 0.2547 - acc: 0.9193 - val_loss: 0.2206 - val_acc: 0.9378\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2464 - acc: 0.9216\n",
      "Epoch 00040: val_loss did not improve from 0.21307\n",
      "36805/36805 [==============================] - 19s 524us/sample - loss: 0.2464 - acc: 0.9216 - val_loss: 0.2194 - val_acc: 0.9383\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2385 - acc: 0.9245\n",
      "Epoch 00041: val_loss improved from 0.21307 to 0.20899, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_7_conv_checkpoint/041-0.2090.hdf5\n",
      "36805/36805 [==============================] - 19s 522us/sample - loss: 0.2385 - acc: 0.9245 - val_loss: 0.2090 - val_acc: 0.9385\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2376 - acc: 0.9246\n",
      "Epoch 00042: val_loss improved from 0.20899 to 0.20818, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_7_conv_checkpoint/042-0.2082.hdf5\n",
      "36805/36805 [==============================] - 19s 523us/sample - loss: 0.2376 - acc: 0.9247 - val_loss: 0.2082 - val_acc: 0.9392\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2339 - acc: 0.9264\n",
      "Epoch 00043: val_loss improved from 0.20818 to 0.19855, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_7_conv_checkpoint/043-0.1985.hdf5\n",
      "36805/36805 [==============================] - 19s 522us/sample - loss: 0.2340 - acc: 0.9264 - val_loss: 0.1985 - val_acc: 0.9429\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2280 - acc: 0.9275\n",
      "Epoch 00044: val_loss improved from 0.19855 to 0.19572, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_7_conv_checkpoint/044-0.1957.hdf5\n",
      "36805/36805 [==============================] - 19s 526us/sample - loss: 0.2280 - acc: 0.9275 - val_loss: 0.1957 - val_acc: 0.9446\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2166 - acc: 0.9303\n",
      "Epoch 00045: val_loss did not improve from 0.19572\n",
      "36805/36805 [==============================] - 19s 525us/sample - loss: 0.2166 - acc: 0.9303 - val_loss: 0.1966 - val_acc: 0.9441\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2150 - acc: 0.9311\n",
      "Epoch 00046: val_loss did not improve from 0.19572\n",
      "36805/36805 [==============================] - 19s 525us/sample - loss: 0.2150 - acc: 0.9312 - val_loss: 0.2007 - val_acc: 0.9415\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2158 - acc: 0.9304\n",
      "Epoch 00047: val_loss did not improve from 0.19572\n",
      "36805/36805 [==============================] - 19s 521us/sample - loss: 0.2158 - acc: 0.9304 - val_loss: 0.2047 - val_acc: 0.9401\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2104 - acc: 0.9328\n",
      "Epoch 00048: val_loss did not improve from 0.19572\n",
      "36805/36805 [==============================] - 19s 523us/sample - loss: 0.2104 - acc: 0.9328 - val_loss: 0.1972 - val_acc: 0.9422\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2031 - acc: 0.9343\n",
      "Epoch 00049: val_loss improved from 0.19572 to 0.19158, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_7_conv_checkpoint/049-0.1916.hdf5\n",
      "36805/36805 [==============================] - 19s 518us/sample - loss: 0.2031 - acc: 0.9343 - val_loss: 0.1916 - val_acc: 0.9464\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2027 - acc: 0.9352\n",
      "Epoch 00050: val_loss improved from 0.19158 to 0.18852, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_7_conv_checkpoint/050-0.1885.hdf5\n",
      "36805/36805 [==============================] - 20s 530us/sample - loss: 0.2027 - acc: 0.9352 - val_loss: 0.1885 - val_acc: 0.9474\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1966 - acc: 0.9374\n",
      "Epoch 00051: val_loss did not improve from 0.18852\n",
      "36805/36805 [==============================] - 19s 527us/sample - loss: 0.1966 - acc: 0.9374 - val_loss: 0.1955 - val_acc: 0.9422\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1991 - acc: 0.9358\n",
      "Epoch 00052: val_loss improved from 0.18852 to 0.18209, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_7_conv_checkpoint/052-0.1821.hdf5\n",
      "36805/36805 [==============================] - 19s 520us/sample - loss: 0.1990 - acc: 0.9358 - val_loss: 0.1821 - val_acc: 0.9457\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1923 - acc: 0.9375\n",
      "Epoch 00053: val_loss did not improve from 0.18209\n",
      "36805/36805 [==============================] - 19s 518us/sample - loss: 0.1923 - acc: 0.9375 - val_loss: 0.1856 - val_acc: 0.9467\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1879 - acc: 0.9386\n",
      "Epoch 00054: val_loss did not improve from 0.18209\n",
      "36805/36805 [==============================] - 19s 527us/sample - loss: 0.1879 - acc: 0.9386 - val_loss: 0.1882 - val_acc: 0.9446\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1830 - acc: 0.9410\n",
      "Epoch 00055: val_loss did not improve from 0.18209\n",
      "36805/36805 [==============================] - 19s 526us/sample - loss: 0.1830 - acc: 0.9410 - val_loss: 0.1918 - val_acc: 0.9436\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1840 - acc: 0.9405\n",
      "Epoch 00056: val_loss improved from 0.18209 to 0.17613, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_7_conv_checkpoint/056-0.1761.hdf5\n",
      "36805/36805 [==============================] - 19s 526us/sample - loss: 0.1840 - acc: 0.9404 - val_loss: 0.1761 - val_acc: 0.9448\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1768 - acc: 0.9435\n",
      "Epoch 00057: val_loss did not improve from 0.17613\n",
      "36805/36805 [==============================] - 19s 520us/sample - loss: 0.1768 - acc: 0.9435 - val_loss: 0.1869 - val_acc: 0.9469\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1761 - acc: 0.9425\n",
      "Epoch 00058: val_loss did not improve from 0.17613\n",
      "36805/36805 [==============================] - 19s 527us/sample - loss: 0.1761 - acc: 0.9425 - val_loss: 0.1850 - val_acc: 0.9448\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1723 - acc: 0.9439\n",
      "Epoch 00059: val_loss did not improve from 0.17613\n",
      "36805/36805 [==============================] - 19s 518us/sample - loss: 0.1723 - acc: 0.9439 - val_loss: 0.1918 - val_acc: 0.9457\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1690 - acc: 0.9453\n",
      "Epoch 00060: val_loss improved from 0.17613 to 0.17480, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_7_conv_checkpoint/060-0.1748.hdf5\n",
      "36805/36805 [==============================] - 19s 504us/sample - loss: 0.1690 - acc: 0.9453 - val_loss: 0.1748 - val_acc: 0.9485\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1656 - acc: 0.9455\n",
      "Epoch 00061: val_loss did not improve from 0.17480\n",
      "36805/36805 [==============================] - 19s 509us/sample - loss: 0.1656 - acc: 0.9455 - val_loss: 0.1756 - val_acc: 0.9483\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1644 - acc: 0.9468\n",
      "Epoch 00062: val_loss improved from 0.17480 to 0.17297, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_7_conv_checkpoint/062-0.1730.hdf5\n",
      "36805/36805 [==============================] - 19s 518us/sample - loss: 0.1644 - acc: 0.9469 - val_loss: 0.1730 - val_acc: 0.9481\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1604 - acc: 0.9468\n",
      "Epoch 00063: val_loss improved from 0.17297 to 0.17187, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_7_conv_checkpoint/063-0.1719.hdf5\n",
      "36805/36805 [==============================] - 19s 527us/sample - loss: 0.1604 - acc: 0.9468 - val_loss: 0.1719 - val_acc: 0.9499\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1575 - acc: 0.9479\n",
      "Epoch 00064: val_loss did not improve from 0.17187\n",
      "36805/36805 [==============================] - 19s 517us/sample - loss: 0.1574 - acc: 0.9479 - val_loss: 0.1828 - val_acc: 0.9478\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1500 - acc: 0.9508\n",
      "Epoch 00065: val_loss did not improve from 0.17187\n",
      "36805/36805 [==============================] - 19s 522us/sample - loss: 0.1500 - acc: 0.9508 - val_loss: 0.1789 - val_acc: 0.9469\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1545 - acc: 0.9493\n",
      "Epoch 00066: val_loss did not improve from 0.17187\n",
      "36805/36805 [==============================] - 19s 513us/sample - loss: 0.1544 - acc: 0.9493 - val_loss: 0.1762 - val_acc: 0.9490\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1474 - acc: 0.9523\n",
      "Epoch 00067: val_loss did not improve from 0.17187\n",
      "36805/36805 [==============================] - 19s 510us/sample - loss: 0.1475 - acc: 0.9523 - val_loss: 0.1850 - val_acc: 0.9476\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1508 - acc: 0.9491\n",
      "Epoch 00068: val_loss did not improve from 0.17187\n",
      "36805/36805 [==============================] - 19s 508us/sample - loss: 0.1508 - acc: 0.9491 - val_loss: 0.1728 - val_acc: 0.9483\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1421 - acc: 0.9531\n",
      "Epoch 00069: val_loss did not improve from 0.17187\n",
      "36805/36805 [==============================] - 19s 523us/sample - loss: 0.1421 - acc: 0.9531 - val_loss: 0.1762 - val_acc: 0.9481\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1446 - acc: 0.9526\n",
      "Epoch 00070: val_loss did not improve from 0.17187\n",
      "36805/36805 [==============================] - 19s 527us/sample - loss: 0.1446 - acc: 0.9526 - val_loss: 0.1819 - val_acc: 0.9485\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1420 - acc: 0.9539\n",
      "Epoch 00071: val_loss did not improve from 0.17187\n",
      "36805/36805 [==============================] - 19s 518us/sample - loss: 0.1420 - acc: 0.9539 - val_loss: 0.1763 - val_acc: 0.9499\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1354 - acc: 0.9554\n",
      "Epoch 00072: val_loss improved from 0.17187 to 0.17075, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_7_conv_checkpoint/072-0.1707.hdf5\n",
      "36805/36805 [==============================] - 19s 517us/sample - loss: 0.1354 - acc: 0.9554 - val_loss: 0.1707 - val_acc: 0.9518\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1370 - acc: 0.9552\n",
      "Epoch 00073: val_loss did not improve from 0.17075\n",
      "36805/36805 [==============================] - 19s 516us/sample - loss: 0.1370 - acc: 0.9553 - val_loss: 0.1777 - val_acc: 0.9495\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1355 - acc: 0.9558- ETA: 0s - loss: 0.1355 - acc: 0.9\n",
      "Epoch 00074: val_loss did not improve from 0.17075\n",
      "36805/36805 [==============================] - 19s 521us/sample - loss: 0.1355 - acc: 0.9557 - val_loss: 0.1726 - val_acc: 0.9497\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1339 - acc: 0.9560\n",
      "Epoch 00075: val_loss did not improve from 0.17075\n",
      "36805/36805 [==============================] - 19s 526us/sample - loss: 0.1338 - acc: 0.9560 - val_loss: 0.2012 - val_acc: 0.9429\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1299 - acc: 0.9564\n",
      "Epoch 00076: val_loss improved from 0.17075 to 0.16982, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_7_conv_checkpoint/076-0.1698.hdf5\n",
      "36805/36805 [==============================] - 19s 527us/sample - loss: 0.1299 - acc: 0.9564 - val_loss: 0.1698 - val_acc: 0.9495\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1266 - acc: 0.9572\n",
      "Epoch 00077: val_loss did not improve from 0.16982\n",
      "36805/36805 [==============================] - 19s 527us/sample - loss: 0.1266 - acc: 0.9572 - val_loss: 0.1724 - val_acc: 0.9497\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1281 - acc: 0.9577\n",
      "Epoch 00078: val_loss did not improve from 0.16982\n",
      "36805/36805 [==============================] - 19s 526us/sample - loss: 0.1281 - acc: 0.9577 - val_loss: 0.1834 - val_acc: 0.9481\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1260 - acc: 0.9570\n",
      "Epoch 00079: val_loss improved from 0.16982 to 0.16878, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_7_conv_checkpoint/079-0.1688.hdf5\n",
      "36805/36805 [==============================] - 19s 526us/sample - loss: 0.1260 - acc: 0.9570 - val_loss: 0.1688 - val_acc: 0.9529\n",
      "Epoch 80/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1269 - acc: 0.9578\n",
      "Epoch 00080: val_loss did not improve from 0.16878\n",
      "36805/36805 [==============================] - 20s 532us/sample - loss: 0.1271 - acc: 0.9577 - val_loss: 0.1699 - val_acc: 0.9469\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1244 - acc: 0.9586\n",
      "Epoch 00081: val_loss did not improve from 0.16878\n",
      "36805/36805 [==============================] - 19s 516us/sample - loss: 0.1244 - acc: 0.9586 - val_loss: 0.1738 - val_acc: 0.9481\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1188 - acc: 0.9595\n",
      "Epoch 00082: val_loss did not improve from 0.16878\n",
      "36805/36805 [==============================] - 19s 521us/sample - loss: 0.1188 - acc: 0.9595 - val_loss: 0.1762 - val_acc: 0.9499\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1189 - acc: 0.9611\n",
      "Epoch 00083: val_loss did not improve from 0.16878\n",
      "36805/36805 [==============================] - 19s 527us/sample - loss: 0.1189 - acc: 0.9611 - val_loss: 0.1801 - val_acc: 0.9497\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1196 - acc: 0.9592\n",
      "Epoch 00084: val_loss did not improve from 0.16878\n",
      "36805/36805 [==============================] - 19s 527us/sample - loss: 0.1197 - acc: 0.9592 - val_loss: 0.1704 - val_acc: 0.9471\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1141 - acc: 0.9619\n",
      "Epoch 00085: val_loss improved from 0.16878 to 0.16793, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_7_conv_checkpoint/085-0.1679.hdf5\n",
      "36805/36805 [==============================] - 19s 519us/sample - loss: 0.1141 - acc: 0.9619 - val_loss: 0.1679 - val_acc: 0.9522\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1148 - acc: 0.9609\n",
      "Epoch 00086: val_loss did not improve from 0.16793\n",
      "36805/36805 [==============================] - 19s 522us/sample - loss: 0.1148 - acc: 0.9609 - val_loss: 0.1702 - val_acc: 0.9509\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1122 - acc: 0.9622\n",
      "Epoch 00087: val_loss improved from 0.16793 to 0.16776, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_7_conv_checkpoint/087-0.1678.hdf5\n",
      "36805/36805 [==============================] - 19s 514us/sample - loss: 0.1122 - acc: 0.9622 - val_loss: 0.1678 - val_acc: 0.9515\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1105 - acc: 0.9629\n",
      "Epoch 00088: val_loss did not improve from 0.16776\n",
      "36805/36805 [==============================] - 19s 519us/sample - loss: 0.1105 - acc: 0.9629 - val_loss: 0.1955 - val_acc: 0.9483\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1083 - acc: 0.9640\n",
      "Epoch 00089: val_loss did not improve from 0.16776\n",
      "36805/36805 [==============================] - 19s 519us/sample - loss: 0.1083 - acc: 0.9640 - val_loss: 0.1748 - val_acc: 0.9495\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1064 - acc: 0.9633\n",
      "Epoch 00090: val_loss did not improve from 0.16776\n",
      "36805/36805 [==============================] - 19s 523us/sample - loss: 0.1063 - acc: 0.9633 - val_loss: 0.1738 - val_acc: 0.9525\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1055 - acc: 0.9628\n",
      "Epoch 00091: val_loss did not improve from 0.16776\n",
      "36805/36805 [==============================] - 19s 518us/sample - loss: 0.1055 - acc: 0.9628 - val_loss: 0.1780 - val_acc: 0.9481\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1074 - acc: 0.9646\n",
      "Epoch 00092: val_loss did not improve from 0.16776\n",
      "36805/36805 [==============================] - 19s 512us/sample - loss: 0.1074 - acc: 0.9647 - val_loss: 0.1686 - val_acc: 0.9499\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1028 - acc: 0.9647\n",
      "Epoch 00093: val_loss did not improve from 0.16776\n",
      "36805/36805 [==============================] - 19s 518us/sample - loss: 0.1028 - acc: 0.9647 - val_loss: 0.1764 - val_acc: 0.9511\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1032 - acc: 0.9643\n",
      "Epoch 00094: val_loss did not improve from 0.16776\n",
      "36805/36805 [==============================] - 19s 528us/sample - loss: 0.1032 - acc: 0.9643 - val_loss: 0.1766 - val_acc: 0.9497\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1003 - acc: 0.9665\n",
      "Epoch 00095: val_loss did not improve from 0.16776\n",
      "36805/36805 [==============================] - 19s 529us/sample - loss: 0.1003 - acc: 0.9666 - val_loss: 0.1833 - val_acc: 0.9495\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1014 - acc: 0.9663\n",
      "Epoch 00096: val_loss improved from 0.16776 to 0.16674, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_7_conv_checkpoint/096-0.1667.hdf5\n",
      "36805/36805 [==============================] - 19s 526us/sample - loss: 0.1014 - acc: 0.9663 - val_loss: 0.1667 - val_acc: 0.9513\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0989 - acc: 0.9670\n",
      "Epoch 00097: val_loss did not improve from 0.16674\n",
      "36805/36805 [==============================] - 19s 526us/sample - loss: 0.0989 - acc: 0.9669 - val_loss: 0.1860 - val_acc: 0.9481\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0996 - acc: 0.9669\n",
      "Epoch 00098: val_loss did not improve from 0.16674\n",
      "36805/36805 [==============================] - 19s 517us/sample - loss: 0.0996 - acc: 0.9669 - val_loss: 0.1728 - val_acc: 0.9513\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0977 - acc: 0.9659\n",
      "Epoch 00099: val_loss did not improve from 0.16674\n",
      "36805/36805 [==============================] - 19s 528us/sample - loss: 0.0977 - acc: 0.9659 - val_loss: 0.1840 - val_acc: 0.9453\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0951 - acc: 0.9670\n",
      "Epoch 00100: val_loss did not improve from 0.16674\n",
      "36805/36805 [==============================] - 20s 532us/sample - loss: 0.0951 - acc: 0.9670 - val_loss: 0.1740 - val_acc: 0.9511\n",
      "Epoch 101/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0942 - acc: 0.9689\n",
      "Epoch 00101: val_loss did not improve from 0.16674\n",
      "36805/36805 [==============================] - 19s 519us/sample - loss: 0.0942 - acc: 0.9689 - val_loss: 0.1722 - val_acc: 0.9513\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0904 - acc: 0.9688\n",
      "Epoch 00102: val_loss did not improve from 0.16674\n",
      "36805/36805 [==============================] - 19s 523us/sample - loss: 0.0904 - acc: 0.9688 - val_loss: 0.1720 - val_acc: 0.9502\n",
      "Epoch 103/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0914 - acc: 0.9689\n",
      "Epoch 00103: val_loss did not improve from 0.16674\n",
      "36805/36805 [==============================] - 19s 522us/sample - loss: 0.0915 - acc: 0.9689 - val_loss: 0.1695 - val_acc: 0.9515\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0916 - acc: 0.9683\n",
      "Epoch 00104: val_loss did not improve from 0.16674\n",
      "36805/36805 [==============================] - 19s 516us/sample - loss: 0.0917 - acc: 0.9683 - val_loss: 0.1785 - val_acc: 0.9497\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0879 - acc: 0.9703\n",
      "Epoch 00105: val_loss did not improve from 0.16674\n",
      "36805/36805 [==============================] - 19s 529us/sample - loss: 0.0879 - acc: 0.9703 - val_loss: 0.1785 - val_acc: 0.9474\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0871 - acc: 0.9700\n",
      "Epoch 00106: val_loss did not improve from 0.16674\n",
      "36805/36805 [==============================] - 20s 532us/sample - loss: 0.0871 - acc: 0.9700 - val_loss: 0.1700 - val_acc: 0.9520\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0864 - acc: 0.9709\n",
      "Epoch 00107: val_loss did not improve from 0.16674\n",
      "36805/36805 [==============================] - 19s 523us/sample - loss: 0.0863 - acc: 0.9709 - val_loss: 0.1746 - val_acc: 0.9532\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0867 - acc: 0.9704\n",
      "Epoch 00108: val_loss did not improve from 0.16674\n",
      "36805/36805 [==============================] - 19s 517us/sample - loss: 0.0867 - acc: 0.9704 - val_loss: 0.1930 - val_acc: 0.9474\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0839 - acc: 0.9711\n",
      "Epoch 00109: val_loss did not improve from 0.16674\n",
      "36805/36805 [==============================] - 19s 514us/sample - loss: 0.0839 - acc: 0.9711 - val_loss: 0.1784 - val_acc: 0.9520\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0819 - acc: 0.9709\n",
      "Epoch 00110: val_loss did not improve from 0.16674\n",
      "36805/36805 [==============================] - 19s 517us/sample - loss: 0.0819 - acc: 0.9709 - val_loss: 0.1789 - val_acc: 0.9511\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0871 - acc: 0.9700\n",
      "Epoch 00111: val_loss did not improve from 0.16674\n",
      "36805/36805 [==============================] - 19s 520us/sample - loss: 0.0871 - acc: 0.9700 - val_loss: 0.1789 - val_acc: 0.9492\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0842 - acc: 0.9712\n",
      "Epoch 00112: val_loss did not improve from 0.16674\n",
      "36805/36805 [==============================] - 19s 516us/sample - loss: 0.0842 - acc: 0.9712 - val_loss: 0.1737 - val_acc: 0.9518\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0797 - acc: 0.9724\n",
      "Epoch 00113: val_loss did not improve from 0.16674\n",
      "36805/36805 [==============================] - 19s 529us/sample - loss: 0.0797 - acc: 0.9724 - val_loss: 0.2010 - val_acc: 0.9446\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0813 - acc: 0.9720\n",
      "Epoch 00114: val_loss did not improve from 0.16674\n",
      "36805/36805 [==============================] - 19s 523us/sample - loss: 0.0813 - acc: 0.9720 - val_loss: 0.1788 - val_acc: 0.9513\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0767 - acc: 0.9739\n",
      "Epoch 00115: val_loss did not improve from 0.16674\n",
      "36805/36805 [==============================] - 19s 520us/sample - loss: 0.0767 - acc: 0.9739 - val_loss: 0.1852 - val_acc: 0.9502\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0765 - acc: 0.9747\n",
      "Epoch 00116: val_loss did not improve from 0.16674\n",
      "36805/36805 [==============================] - 19s 525us/sample - loss: 0.0765 - acc: 0.9747 - val_loss: 0.1797 - val_acc: 0.9513\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0797 - acc: 0.9730\n",
      "Epoch 00117: val_loss did not improve from 0.16674\n",
      "36805/36805 [==============================] - 19s 520us/sample - loss: 0.0797 - acc: 0.9730 - val_loss: 0.1912 - val_acc: 0.9511\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0766 - acc: 0.9734\n",
      "Epoch 00118: val_loss did not improve from 0.16674\n",
      "36805/36805 [==============================] - 19s 518us/sample - loss: 0.0766 - acc: 0.9734 - val_loss: 0.1816 - val_acc: 0.9529\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0761 - acc: 0.9736\n",
      "Epoch 00119: val_loss did not improve from 0.16674\n",
      "36805/36805 [==============================] - 19s 519us/sample - loss: 0.0761 - acc: 0.9736 - val_loss: 0.1961 - val_acc: 0.9483\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0744 - acc: 0.9749\n",
      "Epoch 00120: val_loss did not improve from 0.16674\n",
      "36805/36805 [==============================] - 19s 516us/sample - loss: 0.0744 - acc: 0.9749 - val_loss: 0.1883 - val_acc: 0.9515\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0726 - acc: 0.9752\n",
      "Epoch 00121: val_loss did not improve from 0.16674\n",
      "36805/36805 [==============================] - 19s 510us/sample - loss: 0.0726 - acc: 0.9752 - val_loss: 0.1796 - val_acc: 0.9520\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0739 - acc: 0.9746\n",
      "Epoch 00122: val_loss did not improve from 0.16674\n",
      "36805/36805 [==============================] - 19s 524us/sample - loss: 0.0739 - acc: 0.9746 - val_loss: 0.1855 - val_acc: 0.9529\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0735 - acc: 0.9752\n",
      "Epoch 00123: val_loss did not improve from 0.16674\n",
      "36805/36805 [==============================] - 20s 531us/sample - loss: 0.0735 - acc: 0.9752 - val_loss: 0.1860 - val_acc: 0.9485\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0749 - acc: 0.9747\n",
      "Epoch 00124: val_loss did not improve from 0.16674\n",
      "36805/36805 [==============================] - 19s 507us/sample - loss: 0.0749 - acc: 0.9747 - val_loss: 0.1740 - val_acc: 0.9534\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0723 - acc: 0.9747\n",
      "Epoch 00125: val_loss did not improve from 0.16674\n",
      "36805/36805 [==============================] - 19s 509us/sample - loss: 0.0723 - acc: 0.9747 - val_loss: 0.1863 - val_acc: 0.9485\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0678 - acc: 0.9761\n",
      "Epoch 00126: val_loss did not improve from 0.16674\n",
      "36805/36805 [==============================] - 19s 522us/sample - loss: 0.0678 - acc: 0.9761 - val_loss: 0.1752 - val_acc: 0.9513\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0693 - acc: 0.9763\n",
      "Epoch 00127: val_loss did not improve from 0.16674\n",
      "36805/36805 [==============================] - 19s 523us/sample - loss: 0.0693 - acc: 0.9763 - val_loss: 0.1861 - val_acc: 0.9481\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0723 - acc: 0.9748\n",
      "Epoch 00128: val_loss did not improve from 0.16674\n",
      "36805/36805 [==============================] - 19s 529us/sample - loss: 0.0723 - acc: 0.9748 - val_loss: 0.1805 - val_acc: 0.9536\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0670 - acc: 0.9773\n",
      "Epoch 00129: val_loss did not improve from 0.16674\n",
      "36805/36805 [==============================] - 20s 531us/sample - loss: 0.0670 - acc: 0.9773 - val_loss: 0.1708 - val_acc: 0.9555\n",
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0667 - acc: 0.9772\n",
      "Epoch 00130: val_loss did not improve from 0.16674\n",
      "36805/36805 [==============================] - 19s 524us/sample - loss: 0.0666 - acc: 0.9772 - val_loss: 0.1868 - val_acc: 0.9515\n",
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0668 - acc: 0.9771- ETA: \n",
      "Epoch 00131: val_loss did not improve from 0.16674\n",
      "36805/36805 [==============================] - 19s 527us/sample - loss: 0.0668 - acc: 0.9771 - val_loss: 0.2022 - val_acc: 0.9515\n",
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0652 - acc: 0.9778\n",
      "Epoch 00132: val_loss did not improve from 0.16674\n",
      "36805/36805 [==============================] - 19s 528us/sample - loss: 0.0652 - acc: 0.9778 - val_loss: 0.1797 - val_acc: 0.9529\n",
      "Epoch 133/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0669 - acc: 0.9774\n",
      "Epoch 00133: val_loss did not improve from 0.16674\n",
      "36805/36805 [==============================] - 19s 518us/sample - loss: 0.0669 - acc: 0.9774 - val_loss: 0.1764 - val_acc: 0.9534\n",
      "Epoch 134/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0661 - acc: 0.9764\n",
      "Epoch 00134: val_loss did not improve from 0.16674\n",
      "36805/36805 [==============================] - 19s 529us/sample - loss: 0.0661 - acc: 0.9764 - val_loss: 0.1820 - val_acc: 0.9515\n",
      "Epoch 135/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0650 - acc: 0.9784\n",
      "Epoch 00135: val_loss did not improve from 0.16674\n",
      "36805/36805 [==============================] - 19s 509us/sample - loss: 0.0650 - acc: 0.9784 - val_loss: 0.1943 - val_acc: 0.9485\n",
      "Epoch 136/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0602 - acc: 0.9791\n",
      "Epoch 00136: val_loss did not improve from 0.16674\n",
      "36805/36805 [==============================] - 19s 514us/sample - loss: 0.0602 - acc: 0.9791 - val_loss: 0.1916 - val_acc: 0.9515\n",
      "Epoch 137/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0630 - acc: 0.9777\n",
      "Epoch 00137: val_loss did not improve from 0.16674\n",
      "36805/36805 [==============================] - 19s 523us/sample - loss: 0.0630 - acc: 0.9777 - val_loss: 0.1906 - val_acc: 0.9509\n",
      "Epoch 138/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0663 - acc: 0.9770\n",
      "Epoch 00138: val_loss did not improve from 0.16674\n",
      "36805/36805 [==============================] - 19s 512us/sample - loss: 0.0663 - acc: 0.9770 - val_loss: 0.1833 - val_acc: 0.9511\n",
      "Epoch 139/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0606 - acc: 0.9791\n",
      "Epoch 00139: val_loss did not improve from 0.16674\n",
      "36805/36805 [==============================] - 19s 521us/sample - loss: 0.0606 - acc: 0.9791 - val_loss: 0.1849 - val_acc: 0.9520\n",
      "Epoch 140/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0602 - acc: 0.9800\n",
      "Epoch 00140: val_loss did not improve from 0.16674\n",
      "36805/36805 [==============================] - 19s 521us/sample - loss: 0.0602 - acc: 0.9800 - val_loss: 0.1993 - val_acc: 0.9502\n",
      "Epoch 141/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0597 - acc: 0.9795\n",
      "Epoch 00141: val_loss did not improve from 0.16674\n",
      "36805/36805 [==============================] - 19s 512us/sample - loss: 0.0597 - acc: 0.9795 - val_loss: 0.1938 - val_acc: 0.9527\n",
      "Epoch 142/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0595 - acc: 0.9804\n",
      "Epoch 00142: val_loss did not improve from 0.16674\n",
      "36805/36805 [==============================] - 19s 522us/sample - loss: 0.0595 - acc: 0.9804 - val_loss: 0.1956 - val_acc: 0.9490\n",
      "Epoch 143/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0581 - acc: 0.9799\n",
      "Epoch 00143: val_loss did not improve from 0.16674\n",
      "36805/36805 [==============================] - 19s 514us/sample - loss: 0.0581 - acc: 0.9799 - val_loss: 0.1989 - val_acc: 0.9527\n",
      "Epoch 144/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0580 - acc: 0.9799\n",
      "Epoch 00144: val_loss did not improve from 0.16674\n",
      "36805/36805 [==============================] - 19s 517us/sample - loss: 0.0581 - acc: 0.9799 - val_loss: 0.1891 - val_acc: 0.9553\n",
      "Epoch 145/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0597 - acc: 0.9799\n",
      "Epoch 00145: val_loss did not improve from 0.16674\n",
      "36805/36805 [==============================] - 19s 523us/sample - loss: 0.0597 - acc: 0.9799 - val_loss: 0.1829 - val_acc: 0.9511\n",
      "Epoch 146/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0573 - acc: 0.9809\n",
      "Epoch 00146: val_loss did not improve from 0.16674\n",
      "36805/36805 [==============================] - 19s 524us/sample - loss: 0.0573 - acc: 0.9809 - val_loss: 0.1814 - val_acc: 0.9539\n",
      "\n",
      "1D_CNN_custom_multi_3_concat_ch_32_DO_7_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 354us/sample - loss: 0.2453 - acc: 0.9354\n",
      "Loss: 0.24532797088999367 Accuracy: 0.9354102\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.4545 - acc: 0.1988\n",
      "Epoch 00001: val_loss improved from inf to 1.73326, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_8_conv_checkpoint/001-1.7333.hdf5\n",
      "36805/36805 [==============================] - 25s 681us/sample - loss: 2.4545 - acc: 0.1988 - val_loss: 1.7333 - val_acc: 0.4689\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6270 - acc: 0.4626\n",
      "Epoch 00002: val_loss improved from 1.73326 to 1.27598, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_8_conv_checkpoint/002-1.2760.hdf5\n",
      "36805/36805 [==============================] - 20s 536us/sample - loss: 1.6270 - acc: 0.4626 - val_loss: 1.2760 - val_acc: 0.5980\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3055 - acc: 0.5678\n",
      "Epoch 00003: val_loss improved from 1.27598 to 1.04323, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_8_conv_checkpoint/003-1.0432.hdf5\n",
      "36805/36805 [==============================] - 20s 536us/sample - loss: 1.3054 - acc: 0.5678 - val_loss: 1.0432 - val_acc: 0.6639\n",
      "Epoch 4/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1230 - acc: 0.6306\n",
      "Epoch 00004: val_loss improved from 1.04323 to 0.90652, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_8_conv_checkpoint/004-0.9065.hdf5\n",
      "36805/36805 [==============================] - 20s 538us/sample - loss: 1.1229 - acc: 0.6305 - val_loss: 0.9065 - val_acc: 0.7028\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9885 - acc: 0.6769\n",
      "Epoch 00005: val_loss improved from 0.90652 to 0.77003, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_8_conv_checkpoint/005-0.7700.hdf5\n",
      "36805/36805 [==============================] - 20s 536us/sample - loss: 0.9885 - acc: 0.6769 - val_loss: 0.7700 - val_acc: 0.7545\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8906 - acc: 0.7111\n",
      "Epoch 00006: val_loss improved from 0.77003 to 0.69437, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_8_conv_checkpoint/006-0.6944.hdf5\n",
      "36805/36805 [==============================] - 20s 530us/sample - loss: 0.8906 - acc: 0.7112 - val_loss: 0.6944 - val_acc: 0.7780\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8109 - acc: 0.7359\n",
      "Epoch 00007: val_loss improved from 0.69437 to 0.61213, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_8_conv_checkpoint/007-0.6121.hdf5\n",
      "36805/36805 [==============================] - 19s 518us/sample - loss: 0.8108 - acc: 0.7359 - val_loss: 0.6121 - val_acc: 0.8074\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7425 - acc: 0.7618\n",
      "Epoch 00008: val_loss improved from 0.61213 to 0.57540, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_8_conv_checkpoint/008-0.5754.hdf5\n",
      "36805/36805 [==============================] - 20s 533us/sample - loss: 0.7425 - acc: 0.7617 - val_loss: 0.5754 - val_acc: 0.8171\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6871 - acc: 0.7802\n",
      "Epoch 00009: val_loss improved from 0.57540 to 0.55025, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_8_conv_checkpoint/009-0.5502.hdf5\n",
      "36805/36805 [==============================] - 20s 534us/sample - loss: 0.6870 - acc: 0.7802 - val_loss: 0.5502 - val_acc: 0.8269\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6402 - acc: 0.7920\n",
      "Epoch 00010: val_loss improved from 0.55025 to 0.47937, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_8_conv_checkpoint/010-0.4794.hdf5\n",
      "36805/36805 [==============================] - 20s 538us/sample - loss: 0.6402 - acc: 0.7920 - val_loss: 0.4794 - val_acc: 0.8523\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6001 - acc: 0.8060- ETA: 1s - loss: 0.59\n",
      "Epoch 00011: val_loss improved from 0.47937 to 0.44701, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_8_conv_checkpoint/011-0.4470.hdf5\n",
      "36805/36805 [==============================] - 19s 525us/sample - loss: 0.6001 - acc: 0.8060 - val_loss: 0.4470 - val_acc: 0.8595\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5666 - acc: 0.8177\n",
      "Epoch 00012: val_loss did not improve from 0.44701\n",
      "36805/36805 [==============================] - 19s 526us/sample - loss: 0.5667 - acc: 0.8177 - val_loss: 0.4638 - val_acc: 0.8502\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5333 - acc: 0.8289\n",
      "Epoch 00013: val_loss improved from 0.44701 to 0.40530, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_8_conv_checkpoint/013-0.4053.hdf5\n",
      "36805/36805 [==============================] - 20s 533us/sample - loss: 0.5334 - acc: 0.8288 - val_loss: 0.4053 - val_acc: 0.8717\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5074 - acc: 0.8371\n",
      "Epoch 00014: val_loss improved from 0.40530 to 0.37958, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_8_conv_checkpoint/014-0.3796.hdf5\n",
      "36805/36805 [==============================] - 19s 528us/sample - loss: 0.5075 - acc: 0.8371 - val_loss: 0.3796 - val_acc: 0.8800\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4810 - acc: 0.8471\n",
      "Epoch 00015: val_loss improved from 0.37958 to 0.36396, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_8_conv_checkpoint/015-0.3640.hdf5\n",
      "36805/36805 [==============================] - 20s 537us/sample - loss: 0.4810 - acc: 0.8471 - val_loss: 0.3640 - val_acc: 0.8812\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4575 - acc: 0.8526\n",
      "Epoch 00016: val_loss improved from 0.36396 to 0.33330, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_8_conv_checkpoint/016-0.3333.hdf5\n",
      "36805/36805 [==============================] - 20s 535us/sample - loss: 0.4575 - acc: 0.8527 - val_loss: 0.3333 - val_acc: 0.8949\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4366 - acc: 0.8602\n",
      "Epoch 00017: val_loss did not improve from 0.33330\n",
      "36805/36805 [==============================] - 19s 526us/sample - loss: 0.4365 - acc: 0.8602 - val_loss: 0.3571 - val_acc: 0.8861\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4119 - acc: 0.8691\n",
      "Epoch 00018: val_loss improved from 0.33330 to 0.31025, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_8_conv_checkpoint/018-0.3102.hdf5\n",
      "36805/36805 [==============================] - 20s 536us/sample - loss: 0.4118 - acc: 0.8691 - val_loss: 0.3102 - val_acc: 0.9057\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3988 - acc: 0.8725\n",
      "Epoch 00019: val_loss improved from 0.31025 to 0.30390, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_8_conv_checkpoint/019-0.3039.hdf5\n",
      "36805/36805 [==============================] - 20s 536us/sample - loss: 0.3988 - acc: 0.8725 - val_loss: 0.3039 - val_acc: 0.9089\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3772 - acc: 0.8785\n",
      "Epoch 00020: val_loss improved from 0.30390 to 0.29005, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_8_conv_checkpoint/020-0.2901.hdf5\n",
      "36805/36805 [==============================] - 19s 525us/sample - loss: 0.3772 - acc: 0.8784 - val_loss: 0.2901 - val_acc: 0.9075\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3639 - acc: 0.8846\n",
      "Epoch 00021: val_loss improved from 0.29005 to 0.28348, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_8_conv_checkpoint/021-0.2835.hdf5\n",
      "36805/36805 [==============================] - 20s 530us/sample - loss: 0.3639 - acc: 0.8846 - val_loss: 0.2835 - val_acc: 0.9152\n",
      "Epoch 22/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3492 - acc: 0.8890\n",
      "Epoch 00022: val_loss improved from 0.28348 to 0.26432, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_8_conv_checkpoint/022-0.2643.hdf5\n",
      "36805/36805 [==============================] - 20s 535us/sample - loss: 0.3489 - acc: 0.8891 - val_loss: 0.2643 - val_acc: 0.9187\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3357 - acc: 0.8937\n",
      "Epoch 00023: val_loss did not improve from 0.26432\n",
      "36805/36805 [==============================] - 20s 531us/sample - loss: 0.3357 - acc: 0.8937 - val_loss: 0.2652 - val_acc: 0.9159\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3223 - acc: 0.8970\n",
      "Epoch 00024: val_loss improved from 0.26432 to 0.24506, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_8_conv_checkpoint/024-0.2451.hdf5\n",
      "36805/36805 [==============================] - 20s 535us/sample - loss: 0.3223 - acc: 0.8970 - val_loss: 0.2451 - val_acc: 0.9276\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3125 - acc: 0.9004\n",
      "Epoch 00025: val_loss did not improve from 0.24506\n",
      "36805/36805 [==============================] - 19s 528us/sample - loss: 0.3127 - acc: 0.9004 - val_loss: 0.2477 - val_acc: 0.9220\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3029 - acc: 0.9030\n",
      "Epoch 00026: val_loss improved from 0.24506 to 0.23446, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_8_conv_checkpoint/026-0.2345.hdf5\n",
      "36805/36805 [==============================] - 19s 525us/sample - loss: 0.3029 - acc: 0.9030 - val_loss: 0.2345 - val_acc: 0.9292\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2970 - acc: 0.9052\n",
      "Epoch 00027: val_loss improved from 0.23446 to 0.22253, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_8_conv_checkpoint/027-0.2225.hdf5\n",
      "36805/36805 [==============================] - 20s 541us/sample - loss: 0.2969 - acc: 0.9053 - val_loss: 0.2225 - val_acc: 0.9322\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2813 - acc: 0.9109\n",
      "Epoch 00028: val_loss improved from 0.22253 to 0.21675, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_8_conv_checkpoint/028-0.2168.hdf5\n",
      "36805/36805 [==============================] - 20s 539us/sample - loss: 0.2813 - acc: 0.9109 - val_loss: 0.2168 - val_acc: 0.9331\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2700 - acc: 0.9140\n",
      "Epoch 00029: val_loss improved from 0.21675 to 0.21578, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_8_conv_checkpoint/029-0.2158.hdf5\n",
      "36805/36805 [==============================] - 19s 516us/sample - loss: 0.2700 - acc: 0.9140 - val_loss: 0.2158 - val_acc: 0.9313\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2679 - acc: 0.9135\n",
      "Epoch 00030: val_loss did not improve from 0.21578\n",
      "36805/36805 [==============================] - 19s 523us/sample - loss: 0.2679 - acc: 0.9135 - val_loss: 0.2240 - val_acc: 0.9331\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2617 - acc: 0.9176\n",
      "Epoch 00031: val_loss improved from 0.21578 to 0.20513, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_8_conv_checkpoint/031-0.2051.hdf5\n",
      "36805/36805 [==============================] - 20s 539us/sample - loss: 0.2617 - acc: 0.9176 - val_loss: 0.2051 - val_acc: 0.9373\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2466 - acc: 0.9215\n",
      "Epoch 00032: val_loss did not improve from 0.20513\n",
      "36805/36805 [==============================] - 19s 528us/sample - loss: 0.2466 - acc: 0.9216 - val_loss: 0.2078 - val_acc: 0.9350\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2422 - acc: 0.9219\n",
      "Epoch 00033: val_loss did not improve from 0.20513\n",
      "36805/36805 [==============================] - 20s 536us/sample - loss: 0.2424 - acc: 0.9219 - val_loss: 0.2343 - val_acc: 0.9234\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2436 - acc: 0.9236\n",
      "Epoch 00034: val_loss did not improve from 0.20513\n",
      "36805/36805 [==============================] - 20s 533us/sample - loss: 0.2436 - acc: 0.9236 - val_loss: 0.2130 - val_acc: 0.9355\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2354 - acc: 0.9241\n",
      "Epoch 00035: val_loss improved from 0.20513 to 0.19886, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_8_conv_checkpoint/035-0.1989.hdf5\n",
      "36805/36805 [==============================] - 19s 519us/sample - loss: 0.2354 - acc: 0.9241 - val_loss: 0.1989 - val_acc: 0.9373\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2303 - acc: 0.9267\n",
      "Epoch 00036: val_loss improved from 0.19886 to 0.19738, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_8_conv_checkpoint/036-0.1974.hdf5\n",
      "36805/36805 [==============================] - 19s 521us/sample - loss: 0.2304 - acc: 0.9266 - val_loss: 0.1974 - val_acc: 0.9406\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2194 - acc: 0.9297\n",
      "Epoch 00037: val_loss did not improve from 0.19738\n",
      "36805/36805 [==============================] - 20s 535us/sample - loss: 0.2193 - acc: 0.9297 - val_loss: 0.1978 - val_acc: 0.9371\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2156 - acc: 0.9315\n",
      "Epoch 00038: val_loss improved from 0.19738 to 0.18785, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_8_conv_checkpoint/038-0.1879.hdf5\n",
      "36805/36805 [==============================] - 20s 534us/sample - loss: 0.2155 - acc: 0.9315 - val_loss: 0.1879 - val_acc: 0.9418\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2112 - acc: 0.9325\n",
      "Epoch 00039: val_loss improved from 0.18785 to 0.18194, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_8_conv_checkpoint/039-0.1819.hdf5\n",
      "36805/36805 [==============================] - 20s 532us/sample - loss: 0.2112 - acc: 0.9325 - val_loss: 0.1819 - val_acc: 0.9434\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2044 - acc: 0.9340\n",
      "Epoch 00040: val_loss improved from 0.18194 to 0.17940, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_8_conv_checkpoint/040-0.1794.hdf5\n",
      "36805/36805 [==============================] - 19s 520us/sample - loss: 0.2044 - acc: 0.9340 - val_loss: 0.1794 - val_acc: 0.9436\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2006 - acc: 0.9365\n",
      "Epoch 00041: val_loss did not improve from 0.17940\n",
      "36805/36805 [==============================] - 19s 530us/sample - loss: 0.2007 - acc: 0.9364 - val_loss: 0.1983 - val_acc: 0.9369\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1984 - acc: 0.9352\n",
      "Epoch 00042: val_loss did not improve from 0.17940\n",
      "36805/36805 [==============================] - 20s 539us/sample - loss: 0.1984 - acc: 0.9352 - val_loss: 0.1830 - val_acc: 0.9432\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1878 - acc: 0.9380\n",
      "Epoch 00043: val_loss did not improve from 0.17940\n",
      "36805/36805 [==============================] - 19s 522us/sample - loss: 0.1878 - acc: 0.9380 - val_loss: 0.1894 - val_acc: 0.9385\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1830 - acc: 0.9410\n",
      "Epoch 00044: val_loss did not improve from 0.17940\n",
      "36805/36805 [==============================] - 19s 517us/sample - loss: 0.1830 - acc: 0.9410 - val_loss: 0.2030 - val_acc: 0.9362\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1854 - acc: 0.9399\n",
      "Epoch 00045: val_loss improved from 0.17940 to 0.17775, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_8_conv_checkpoint/045-0.1778.hdf5\n",
      "36805/36805 [==============================] - 19s 523us/sample - loss: 0.1854 - acc: 0.9399 - val_loss: 0.1778 - val_acc: 0.9450\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1768 - acc: 0.9426\n",
      "Epoch 00046: val_loss did not improve from 0.17775\n",
      "36805/36805 [==============================] - 20s 533us/sample - loss: 0.1768 - acc: 0.9426 - val_loss: 0.1809 - val_acc: 0.9467\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1764 - acc: 0.9427\n",
      "Epoch 00047: val_loss did not improve from 0.17775\n",
      "36805/36805 [==============================] - 20s 530us/sample - loss: 0.1765 - acc: 0.9427 - val_loss: 0.1863 - val_acc: 0.9422\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1723 - acc: 0.9436\n",
      "Epoch 00048: val_loss improved from 0.17775 to 0.17184, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_8_conv_checkpoint/048-0.1718.hdf5\n",
      "36805/36805 [==============================] - 20s 540us/sample - loss: 0.1723 - acc: 0.9436 - val_loss: 0.1718 - val_acc: 0.9474\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1700 - acc: 0.9443\n",
      "Epoch 00049: val_loss did not improve from 0.17184\n",
      "36805/36805 [==============================] - 20s 537us/sample - loss: 0.1699 - acc: 0.9443 - val_loss: 0.1815 - val_acc: 0.9436\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1662 - acc: 0.9448\n",
      "Epoch 00050: val_loss did not improve from 0.17184\n",
      "36805/36805 [==============================] - 19s 529us/sample - loss: 0.1661 - acc: 0.9448 - val_loss: 0.1753 - val_acc: 0.9474\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1601 - acc: 0.9483\n",
      "Epoch 00051: val_loss did not improve from 0.17184\n",
      "36805/36805 [==============================] - 20s 531us/sample - loss: 0.1601 - acc: 0.9483 - val_loss: 0.1778 - val_acc: 0.9467\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1598 - acc: 0.9473\n",
      "Epoch 00052: val_loss improved from 0.17184 to 0.17149, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_8_conv_checkpoint/052-0.1715.hdf5\n",
      "36805/36805 [==============================] - 19s 515us/sample - loss: 0.1598 - acc: 0.9473 - val_loss: 0.1715 - val_acc: 0.9462\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1549 - acc: 0.9487\n",
      "Epoch 00053: val_loss did not improve from 0.17149\n",
      "36805/36805 [==============================] - 19s 524us/sample - loss: 0.1549 - acc: 0.9487 - val_loss: 0.1732 - val_acc: 0.9495\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1476 - acc: 0.9508\n",
      "Epoch 00054: val_loss did not improve from 0.17149\n",
      "36805/36805 [==============================] - 19s 513us/sample - loss: 0.1476 - acc: 0.9508 - val_loss: 0.1744 - val_acc: 0.9488\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1500 - acc: 0.9496\n",
      "Epoch 00055: val_loss did not improve from 0.17149\n",
      "36805/36805 [==============================] - 19s 528us/sample - loss: 0.1499 - acc: 0.9496 - val_loss: 0.1780 - val_acc: 0.9436\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1460 - acc: 0.9521\n",
      "Epoch 00056: val_loss did not improve from 0.17149\n",
      "36805/36805 [==============================] - 19s 510us/sample - loss: 0.1461 - acc: 0.9521 - val_loss: 0.1735 - val_acc: 0.9485\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1420 - acc: 0.9534\n",
      "Epoch 00057: val_loss did not improve from 0.17149\n",
      "36805/36805 [==============================] - 19s 507us/sample - loss: 0.1421 - acc: 0.9533 - val_loss: 0.1897 - val_acc: 0.9415\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1389 - acc: 0.9544\n",
      "Epoch 00058: val_loss did not improve from 0.17149\n",
      "36805/36805 [==============================] - 18s 498us/sample - loss: 0.1390 - acc: 0.9544 - val_loss: 0.1797 - val_acc: 0.9443\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1409 - acc: 0.9536\n",
      "Epoch 00059: val_loss did not improve from 0.17149\n",
      "36805/36805 [==============================] - 19s 512us/sample - loss: 0.1409 - acc: 0.9536 - val_loss: 0.1748 - val_acc: 0.9502\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1394 - acc: 0.9533\n",
      "Epoch 00060: val_loss improved from 0.17149 to 0.16935, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_8_conv_checkpoint/060-0.1694.hdf5\n",
      "36805/36805 [==============================] - 19s 516us/sample - loss: 0.1394 - acc: 0.9533 - val_loss: 0.1694 - val_acc: 0.9506\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1340 - acc: 0.9551\n",
      "Epoch 00061: val_loss did not improve from 0.16935\n",
      "36805/36805 [==============================] - 20s 531us/sample - loss: 0.1340 - acc: 0.9551 - val_loss: 0.1700 - val_acc: 0.9509\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1307 - acc: 0.9560\n",
      "Epoch 00062: val_loss improved from 0.16935 to 0.16889, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_8_conv_checkpoint/062-0.1689.hdf5\n",
      "36805/36805 [==============================] - 19s 528us/sample - loss: 0.1307 - acc: 0.9560 - val_loss: 0.1689 - val_acc: 0.9513\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1293 - acc: 0.9575\n",
      "Epoch 00063: val_loss did not improve from 0.16889\n",
      "36805/36805 [==============================] - 19s 516us/sample - loss: 0.1293 - acc: 0.9575 - val_loss: 0.1792 - val_acc: 0.9460\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1234 - acc: 0.9596\n",
      "Epoch 00064: val_loss did not improve from 0.16889\n",
      "36805/36805 [==============================] - 19s 515us/sample - loss: 0.1234 - acc: 0.9596 - val_loss: 0.1749 - val_acc: 0.9492\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1264 - acc: 0.9585\n",
      "Epoch 00065: val_loss did not improve from 0.16889\n",
      "36805/36805 [==============================] - 19s 514us/sample - loss: 0.1265 - acc: 0.9585 - val_loss: 0.1700 - val_acc: 0.9511\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1233 - acc: 0.9585\n",
      "Epoch 00066: val_loss improved from 0.16889 to 0.16693, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_8_conv_checkpoint/066-0.1669.hdf5\n",
      "36805/36805 [==============================] - 19s 512us/sample - loss: 0.1233 - acc: 0.9585 - val_loss: 0.1669 - val_acc: 0.9513\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1170 - acc: 0.9595\n",
      "Epoch 00067: val_loss did not improve from 0.16693\n",
      "36805/36805 [==============================] - 19s 528us/sample - loss: 0.1170 - acc: 0.9595 - val_loss: 0.1727 - val_acc: 0.9506\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1178 - acc: 0.9595\n",
      "Epoch 00068: val_loss did not improve from 0.16693\n",
      "36805/36805 [==============================] - 19s 514us/sample - loss: 0.1178 - acc: 0.9595 - val_loss: 0.1779 - val_acc: 0.9474\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1152 - acc: 0.9607\n",
      "Epoch 00069: val_loss did not improve from 0.16693\n",
      "36805/36805 [==============================] - 20s 530us/sample - loss: 0.1152 - acc: 0.9607 - val_loss: 0.1765 - val_acc: 0.9499\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1162 - acc: 0.9607\n",
      "Epoch 00070: val_loss did not improve from 0.16693\n",
      "36805/36805 [==============================] - 20s 532us/sample - loss: 0.1162 - acc: 0.9607 - val_loss: 0.1729 - val_acc: 0.9532\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1105 - acc: 0.9627\n",
      "Epoch 00071: val_loss did not improve from 0.16693\n",
      "36805/36805 [==============================] - 20s 534us/sample - loss: 0.1105 - acc: 0.9627 - val_loss: 0.1786 - val_acc: 0.9518\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1062 - acc: 0.9636\n",
      "Epoch 00072: val_loss did not improve from 0.16693\n",
      "36805/36805 [==============================] - 19s 520us/sample - loss: 0.1063 - acc: 0.9635 - val_loss: 0.1708 - val_acc: 0.9534\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1093 - acc: 0.9631\n",
      "Epoch 00073: val_loss improved from 0.16693 to 0.16693, saving model to model/checkpoint/1D_CNN_custom_multi_3_concat_ch_32_DO_8_conv_checkpoint/073-0.1669.hdf5\n",
      "36805/36805 [==============================] - 20s 533us/sample - loss: 0.1093 - acc: 0.9631 - val_loss: 0.1669 - val_acc: 0.9529\n",
      "Epoch 74/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1076 - acc: 0.9630\n",
      "Epoch 00074: val_loss did not improve from 0.16693\n",
      "36805/36805 [==============================] - 19s 526us/sample - loss: 0.1076 - acc: 0.9630 - val_loss: 0.1715 - val_acc: 0.9525\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1034 - acc: 0.9643\n",
      "Epoch 00075: val_loss did not improve from 0.16693\n",
      "36805/36805 [==============================] - 19s 512us/sample - loss: 0.1034 - acc: 0.9643 - val_loss: 0.1817 - val_acc: 0.9499\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1038 - acc: 0.9640\n",
      "Epoch 00076: val_loss did not improve from 0.16693\n",
      "36805/36805 [==============================] - 19s 508us/sample - loss: 0.1038 - acc: 0.9640 - val_loss: 0.1778 - val_acc: 0.9504\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1008 - acc: 0.9655\n",
      "Epoch 00077: val_loss did not improve from 0.16693\n",
      "36805/36805 [==============================] - 19s 522us/sample - loss: 0.1008 - acc: 0.9655 - val_loss: 0.1678 - val_acc: 0.9532\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0987 - acc: 0.9663\n",
      "Epoch 00078: val_loss did not improve from 0.16693\n",
      "36805/36805 [==============================] - 19s 528us/sample - loss: 0.0987 - acc: 0.9663 - val_loss: 0.1713 - val_acc: 0.9543\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0991 - acc: 0.9661\n",
      "Epoch 00079: val_loss did not improve from 0.16693\n",
      "36805/36805 [==============================] - 19s 510us/sample - loss: 0.0992 - acc: 0.9660 - val_loss: 0.1714 - val_acc: 0.9529\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0947 - acc: 0.9676\n",
      "Epoch 00080: val_loss did not improve from 0.16693\n",
      "36805/36805 [==============================] - 19s 516us/sample - loss: 0.0947 - acc: 0.9676 - val_loss: 0.1756 - val_acc: 0.9522\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0972 - acc: 0.9673\n",
      "Epoch 00081: val_loss did not improve from 0.16693\n",
      "36805/36805 [==============================] - 19s 513us/sample - loss: 0.0972 - acc: 0.9673 - val_loss: 0.1673 - val_acc: 0.9557\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0930 - acc: 0.9686\n",
      "Epoch 00082: val_loss did not improve from 0.16693\n",
      "36805/36805 [==============================] - 19s 526us/sample - loss: 0.0930 - acc: 0.9686 - val_loss: 0.1928 - val_acc: 0.9481\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0922 - acc: 0.9679\n",
      "Epoch 00083: val_loss did not improve from 0.16693\n",
      "36805/36805 [==============================] - 19s 526us/sample - loss: 0.0922 - acc: 0.9679 - val_loss: 0.1813 - val_acc: 0.9532\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0893 - acc: 0.9693\n",
      "Epoch 00084: val_loss did not improve from 0.16693\n",
      "36805/36805 [==============================] - 19s 510us/sample - loss: 0.0892 - acc: 0.9694 - val_loss: 0.1733 - val_acc: 0.9536\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0892 - acc: 0.9688\n",
      "Epoch 00085: val_loss did not improve from 0.16693\n",
      "36805/36805 [==============================] - 19s 511us/sample - loss: 0.0892 - acc: 0.9688 - val_loss: 0.1697 - val_acc: 0.9557\n",
      "Epoch 86/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0873 - acc: 0.9696\n",
      "Epoch 00086: val_loss did not improve from 0.16693\n",
      "36805/36805 [==============================] - 19s 528us/sample - loss: 0.0874 - acc: 0.9697 - val_loss: 0.1882 - val_acc: 0.9502\n",
      "Epoch 87/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0856 - acc: 0.9702\n",
      "Epoch 00087: val_loss did not improve from 0.16693\n",
      "36805/36805 [==============================] - 19s 527us/sample - loss: 0.0856 - acc: 0.9702 - val_loss: 0.1763 - val_acc: 0.9541\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0838 - acc: 0.9714\n",
      "Epoch 00088: val_loss did not improve from 0.16693\n",
      "36805/36805 [==============================] - 19s 517us/sample - loss: 0.0838 - acc: 0.9714 - val_loss: 0.1744 - val_acc: 0.9555\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0830 - acc: 0.9714\n",
      "Epoch 00089: val_loss did not improve from 0.16693\n",
      "36805/36805 [==============================] - 19s 514us/sample - loss: 0.0830 - acc: 0.9714 - val_loss: 0.1918 - val_acc: 0.9541\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0824 - acc: 0.9712\n",
      "Epoch 00090: val_loss did not improve from 0.16693\n",
      "36805/36805 [==============================] - 19s 508us/sample - loss: 0.0824 - acc: 0.9712 - val_loss: 0.1773 - val_acc: 0.9525\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0812 - acc: 0.9720\n",
      "Epoch 00091: val_loss did not improve from 0.16693\n",
      "36805/36805 [==============================] - 19s 525us/sample - loss: 0.0812 - acc: 0.9720 - val_loss: 0.1753 - val_acc: 0.9562\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0825 - acc: 0.9717\n",
      "Epoch 00092: val_loss did not improve from 0.16693\n",
      "36805/36805 [==============================] - 19s 519us/sample - loss: 0.0825 - acc: 0.9717 - val_loss: 0.1796 - val_acc: 0.9550\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0771 - acc: 0.9726\n",
      "Epoch 00093: val_loss did not improve from 0.16693\n",
      "36805/36805 [==============================] - 19s 514us/sample - loss: 0.0771 - acc: 0.9726 - val_loss: 0.1762 - val_acc: 0.9548\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0758 - acc: 0.9733\n",
      "Epoch 00094: val_loss did not improve from 0.16693\n",
      "36805/36805 [==============================] - 19s 525us/sample - loss: 0.0758 - acc: 0.9732 - val_loss: 0.1851 - val_acc: 0.9550\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0746 - acc: 0.9742\n",
      "Epoch 00095: val_loss did not improve from 0.16693\n",
      "36805/36805 [==============================] - 19s 509us/sample - loss: 0.0747 - acc: 0.9741 - val_loss: 0.1996 - val_acc: 0.9502\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0790 - acc: 0.9740\n",
      "Epoch 00096: val_loss did not improve from 0.16693\n",
      "36805/36805 [==============================] - 19s 510us/sample - loss: 0.0791 - acc: 0.9740 - val_loss: 0.1873 - val_acc: 0.9543\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0774 - acc: 0.9729\n",
      "Epoch 00097: val_loss did not improve from 0.16693\n",
      "36805/36805 [==============================] - 19s 523us/sample - loss: 0.0774 - acc: 0.9729 - val_loss: 0.1810 - val_acc: 0.9553\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0708 - acc: 0.9755\n",
      "Epoch 00098: val_loss did not improve from 0.16693\n",
      "36805/36805 [==============================] - 20s 535us/sample - loss: 0.0708 - acc: 0.9755 - val_loss: 0.1847 - val_acc: 0.9527\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0721 - acc: 0.9745\n",
      "Epoch 00099: val_loss did not improve from 0.16693\n",
      "36805/36805 [==============================] - 20s 534us/sample - loss: 0.0721 - acc: 0.9745 - val_loss: 0.1799 - val_acc: 0.9557\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0697 - acc: 0.9761\n",
      "Epoch 00100: val_loss did not improve from 0.16693\n",
      "36805/36805 [==============================] - 19s 528us/sample - loss: 0.0697 - acc: 0.9761 - val_loss: 0.1848 - val_acc: 0.9541\n",
      "Epoch 101/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0705 - acc: 0.9751\n",
      "Epoch 00101: val_loss did not improve from 0.16693\n",
      "36805/36805 [==============================] - 19s 525us/sample - loss: 0.0708 - acc: 0.9751 - val_loss: 0.1876 - val_acc: 0.9541\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0697 - acc: 0.9757\n",
      "Epoch 00102: val_loss did not improve from 0.16693\n",
      "36805/36805 [==============================] - 20s 534us/sample - loss: 0.0697 - acc: 0.9757 - val_loss: 0.1844 - val_acc: 0.9553\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0666 - acc: 0.9762\n",
      "Epoch 00103: val_loss did not improve from 0.16693\n",
      "36805/36805 [==============================] - 20s 536us/sample - loss: 0.0666 - acc: 0.9762 - val_loss: 0.1934 - val_acc: 0.9546\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0670 - acc: 0.9766\n",
      "Epoch 00104: val_loss did not improve from 0.16693\n",
      "36805/36805 [==============================] - 19s 516us/sample - loss: 0.0670 - acc: 0.9766 - val_loss: 0.1828 - val_acc: 0.9574\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0642 - acc: 0.9776\n",
      "Epoch 00105: val_loss did not improve from 0.16693\n",
      "36805/36805 [==============================] - 19s 510us/sample - loss: 0.0642 - acc: 0.9776 - val_loss: 0.1903 - val_acc: 0.9527\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0696 - acc: 0.9760\n",
      "Epoch 00106: val_loss did not improve from 0.16693\n",
      "36805/36805 [==============================] - 20s 531us/sample - loss: 0.0696 - acc: 0.9760 - val_loss: 0.1855 - val_acc: 0.9532\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0643 - acc: 0.9777\n",
      "Epoch 00107: val_loss did not improve from 0.16693\n",
      "36805/36805 [==============================] - 19s 529us/sample - loss: 0.0643 - acc: 0.9777 - val_loss: 0.1918 - val_acc: 0.9567\n",
      "Epoch 108/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0637 - acc: 0.9778\n",
      "Epoch 00108: val_loss did not improve from 0.16693\n",
      "36805/36805 [==============================] - 19s 529us/sample - loss: 0.0637 - acc: 0.9778 - val_loss: 0.2068 - val_acc: 0.9506\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0639 - acc: 0.9777\n",
      "Epoch 00109: val_loss did not improve from 0.16693\n",
      "36805/36805 [==============================] - 20s 530us/sample - loss: 0.0639 - acc: 0.9777 - val_loss: 0.1788 - val_acc: 0.9560\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0632 - acc: 0.9779\n",
      "Epoch 00110: val_loss did not improve from 0.16693\n",
      "36805/36805 [==============================] - 20s 535us/sample - loss: 0.0632 - acc: 0.9779 - val_loss: 0.1830 - val_acc: 0.9581\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0617 - acc: 0.9785\n",
      "Epoch 00111: val_loss did not improve from 0.16693\n",
      "36805/36805 [==============================] - 19s 512us/sample - loss: 0.0617 - acc: 0.9785 - val_loss: 0.1786 - val_acc: 0.9613\n",
      "Epoch 112/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0593 - acc: 0.9790\n",
      "Epoch 00112: val_loss did not improve from 0.16693\n",
      "36805/36805 [==============================] - 19s 528us/sample - loss: 0.0593 - acc: 0.9790 - val_loss: 0.1967 - val_acc: 0.9536\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0599 - acc: 0.9790\n",
      "Epoch 00113: val_loss did not improve from 0.16693\n",
      "36805/36805 [==============================] - 19s 521us/sample - loss: 0.0599 - acc: 0.9791 - val_loss: 0.1883 - val_acc: 0.9548\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0595 - acc: 0.9790\n",
      "Epoch 00114: val_loss did not improve from 0.16693\n",
      "36805/36805 [==============================] - 19s 529us/sample - loss: 0.0595 - acc: 0.9790 - val_loss: 0.1995 - val_acc: 0.9522\n",
      "Epoch 115/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0604 - acc: 0.9790\n",
      "Epoch 00115: val_loss did not improve from 0.16693\n",
      "36805/36805 [==============================] - 20s 532us/sample - loss: 0.0603 - acc: 0.9790 - val_loss: 0.2406 - val_acc: 0.9490\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0598 - acc: 0.9791\n",
      "Epoch 00116: val_loss did not improve from 0.16693\n",
      "36805/36805 [==============================] - 19s 523us/sample - loss: 0.0599 - acc: 0.9791 - val_loss: 0.1948 - val_acc: 0.9548\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0564 - acc: 0.9811\n",
      "Epoch 00117: val_loss did not improve from 0.16693\n",
      "36805/36805 [==============================] - 19s 515us/sample - loss: 0.0564 - acc: 0.9811 - val_loss: 0.1947 - val_acc: 0.9583\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0552 - acc: 0.9806\n",
      "Epoch 00118: val_loss did not improve from 0.16693\n",
      "36805/36805 [==============================] - 19s 526us/sample - loss: 0.0552 - acc: 0.9806 - val_loss: 0.1899 - val_acc: 0.9550\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0572 - acc: 0.9799\n",
      "Epoch 00119: val_loss did not improve from 0.16693\n",
      "36805/36805 [==============================] - 19s 514us/sample - loss: 0.0572 - acc: 0.9799 - val_loss: 0.2034 - val_acc: 0.9553\n",
      "Epoch 120/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0576 - acc: 0.9804\n",
      "Epoch 00120: val_loss did not improve from 0.16693\n",
      "36805/36805 [==============================] - 19s 511us/sample - loss: 0.0576 - acc: 0.9804 - val_loss: 0.1886 - val_acc: 0.9581\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0536 - acc: 0.9811\n",
      "Epoch 00121: val_loss did not improve from 0.16693\n",
      "36805/36805 [==============================] - 19s 508us/sample - loss: 0.0537 - acc: 0.9811 - val_loss: 0.1988 - val_acc: 0.9571\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0549 - acc: 0.9809\n",
      "Epoch 00122: val_loss did not improve from 0.16693\n",
      "36805/36805 [==============================] - 19s 519us/sample - loss: 0.0549 - acc: 0.9809 - val_loss: 0.1867 - val_acc: 0.9576\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0556 - acc: 0.9801\n",
      "Epoch 00123: val_loss did not improve from 0.16693\n",
      "36805/36805 [==============================] - 20s 532us/sample - loss: 0.0556 - acc: 0.9801 - val_loss: 0.1933 - val_acc: 0.9583\n",
      "\n",
      "1D_CNN_custom_multi_3_concat_ch_32_DO_8_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 348us/sample - loss: 0.2225 - acc: 0.9360\n",
      "Loss: 0.22245212425076455 Accuracy: 0.93603325\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base = '1D_CNN_custom_multi_3_concat_ch_32_DO'\n",
    "\n",
    "for i in range(3, 9):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    model = build_cnn(conv_num=i)\n",
    "#         model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4),\n",
    "          metrics=['accuracy'])\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model_filename = model_path+'{epoch:03d}-{val_loss:.4f}.hdf5'\n",
    "    checkpointer = ModelCheckpoint(filepath = model_filename, monitor = \"val_loss\", \n",
    "                                   verbose=1, save_best_only=True)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=50)\n",
    "    hist = model.fit(x_train_abs, y_train_onehot, batch_size=64, epochs=500, \n",
    "                     validation_data=[x_val_abs, y_val_onehot], shuffle=True, \n",
    "                     callbacks = [checkpointer, early_stopping])\n",
    "\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "    ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "    ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "    ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    png_path = 'visualization/learning_curve/'\n",
    "    filename = model_name+'.png'\n",
    "    os.makedirs(png_path, exist_ok=True)\n",
    "    fig.savefig(png_path+filename, transparent=True)\n",
    "\n",
    "    model.save(model_path+'000_last.hdf5')\n",
    "    del(model)\n",
    "\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "    model = load_model(model_filename)\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "    print()\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_multi_3_concat_ch_32_DO_3_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_33 (Conv1D)              (None, 16000, 32)    192         input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 16000, 32)    0           conv1d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling1D) (None, 5333, 32)     0           activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_34 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_33[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 5333, 32)     0           conv1d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling1D) (None, 1777, 32)     0           activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_35 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 1777, 32)     0           conv1d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling1D) (None, 592, 32)      0           activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_18 (Flatten)            (None, 170656)       0           max_pooling1d_33[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_19 (Flatten)            (None, 56864)        0           max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_20 (Flatten)            (None, 18944)        0           max_pooling1d_35[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 246464)       0           flatten_18[0][0]                 \n",
      "                                                                 flatten_19[0][0]                 \n",
      "                                                                 flatten_20[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 246464)       0           concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 16)           3943440     dropout_6[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 3,953,936\n",
      "Trainable params: 3,953,936\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 2s 330us/sample - loss: 1.5206 - acc: 0.5225\n",
      "Loss: 1.5206209133223458 Accuracy: 0.5225338\n",
      "\n",
      "1D_CNN_custom_multi_3_concat_ch_32_DO_4_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_8 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_36 (Conv1D)              (None, 16000, 32)    192         input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 16000, 32)    0           conv1d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling1D) (None, 5333, 32)     0           activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_37 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_36[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 5333, 32)     0           conv1d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling1D) (None, 1777, 32)     0           activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_38 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_37[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 1777, 32)     0           conv1d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling1D) (None, 592, 32)      0           activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_39 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 592, 32)      0           conv1d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling1D) (None, 197, 32)      0           activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_21 (Flatten)            (None, 56864)        0           max_pooling1d_37[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_22 (Flatten)            (None, 18944)        0           max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_23 (Flatten)            (None, 6304)         0           max_pooling1d_39[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 82112)        0           flatten_21[0][0]                 \n",
      "                                                                 flatten_22[0][0]                 \n",
      "                                                                 flatten_23[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 82112)        0           concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 16)           1313808     dropout_7[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,329,456\n",
      "Trainable params: 1,329,456\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 316us/sample - loss: 1.2618 - acc: 0.6098\n",
      "Loss: 1.2618204119297078 Accuracy: 0.6097612\n",
      "\n",
      "1D_CNN_custom_multi_3_concat_ch_32_DO_5_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_40 (Conv1D)              (None, 16000, 32)    192         input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 16000, 32)    0           conv1d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling1D) (None, 5333, 32)     0           activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_41 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_40[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 5333, 32)     0           conv1d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling1D) (None, 1777, 32)     0           activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_42 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_41[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 1777, 32)     0           conv1d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling1D) (None, 592, 32)      0           activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_43 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_42[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 592, 32)      0           conv1d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling1D) (None, 197, 32)      0           activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_44 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 197, 64)      0           conv1d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling1D) (None, 65, 64)       0           activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_24 (Flatten)            (None, 18944)        0           max_pooling1d_42[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_25 (Flatten)            (None, 6304)         0           max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_26 (Flatten)            (None, 4160)         0           max_pooling1d_44[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 29408)        0           flatten_24[0][0]                 \n",
      "                                                                 flatten_25[0][0]                 \n",
      "                                                                 flatten_26[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 29408)        0           concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 16)           470544      dropout_8[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 496,496\n",
      "Trainable params: 496,496\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 2s 356us/sample - loss: 0.8005 - acc: 0.7838\n",
      "Loss: 0.8005222551052692 Accuracy: 0.7838006\n",
      "\n",
      "1D_CNN_custom_multi_3_concat_ch_32_DO_6_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_10 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_45 (Conv1D)              (None, 16000, 32)    192         input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 16000, 32)    0           conv1d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling1D) (None, 5333, 32)     0           activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_46 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_45[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 5333, 32)     0           conv1d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling1D) (None, 1777, 32)     0           activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_47 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_46[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 1777, 32)     0           conv1d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling1D) (None, 592, 32)      0           activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_48 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_47[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 592, 32)      0           conv1d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling1D) (None, 197, 32)      0           activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_49 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_48[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 197, 64)      0           conv1d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling1D) (None, 65, 64)       0           activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_50 (Conv1D)              (None, 65, 64)       20544       max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 65, 64)       0           conv1d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling1D) (None, 21, 64)       0           activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_27 (Flatten)            (None, 6304)         0           max_pooling1d_48[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_28 (Flatten)            (None, 4160)         0           max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_29 (Flatten)            (None, 1344)         0           max_pooling1d_50[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 11808)        0           flatten_27[0][0]                 \n",
      "                                                                 flatten_28[0][0]                 \n",
      "                                                                 flatten_29[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 11808)        0           concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 16)           188944      dropout_9[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 235,440\n",
      "Trainable params: 235,440\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 329us/sample - loss: 0.4121 - acc: 0.8947\n",
      "Loss: 0.41212150227986394 Accuracy: 0.89470404\n",
      "\n",
      "1D_CNN_custom_multi_3_concat_ch_32_DO_7_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_51 (Conv1D)              (None, 16000, 32)    192         input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 16000, 32)    0           conv1d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling1D) (None, 5333, 32)     0           activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_52 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_51[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 5333, 32)     0           conv1d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling1D) (None, 1777, 32)     0           activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_53 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_52[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 1777, 32)     0           conv1d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling1D) (None, 592, 32)      0           activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_54 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_53[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 592, 32)      0           conv1d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling1D) (None, 197, 32)      0           activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_55 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_54[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 197, 64)      0           conv1d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling1D) (None, 65, 64)       0           activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_56 (Conv1D)              (None, 65, 64)       20544       max_pooling1d_55[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 65, 64)       0           conv1d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling1D) (None, 21, 64)       0           activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_57 (Conv1D)              (None, 21, 64)       20544       max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 21, 64)       0           conv1d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling1D) (None, 7, 64)        0           activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_30 (Flatten)            (None, 4160)         0           max_pooling1d_55[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_31 (Flatten)            (None, 1344)         0           max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_32 (Flatten)            (None, 448)          0           max_pooling1d_57[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 5952)         0           flatten_30[0][0]                 \n",
      "                                                                 flatten_31[0][0]                 \n",
      "                                                                 flatten_32[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 5952)         0           concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 16)           95248       dropout_10[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 162,288\n",
      "Trainable params: 162,288\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 2s 373us/sample - loss: 0.2453 - acc: 0.9354\n",
      "Loss: 0.24532797088999367 Accuracy: 0.9354102\n",
      "\n",
      "1D_CNN_custom_multi_3_concat_ch_32_DO_8_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_12 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_58 (Conv1D)              (None, 16000, 32)    192         input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 16000, 32)    0           conv1d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling1D) (None, 5333, 32)     0           activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_59 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_58[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 5333, 32)     0           conv1d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling1D) (None, 1777, 32)     0           activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_60 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_59[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 1777, 32)     0           conv1d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling1D) (None, 592, 32)      0           activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_61 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_60[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 592, 32)      0           conv1d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling1D) (None, 197, 32)      0           activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_62 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_61[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 197, 64)      0           conv1d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling1D) (None, 65, 64)       0           activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_63 (Conv1D)              (None, 65, 64)       20544       max_pooling1d_62[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 65, 64)       0           conv1d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling1D) (None, 21, 64)       0           activation_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_64 (Conv1D)              (None, 21, 64)       20544       max_pooling1d_63[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 21, 64)       0           conv1d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling1D) (None, 7, 64)        0           activation_64[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_65 (Conv1D)              (None, 7, 64)        20544       max_pooling1d_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 7, 64)        0           conv1d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling1D) (None, 2, 64)        0           activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_33 (Flatten)            (None, 1344)         0           max_pooling1d_63[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_34 (Flatten)            (None, 448)          0           max_pooling1d_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_35 (Flatten)            (None, 128)          0           max_pooling1d_65[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 1920)         0           flatten_33[0][0]                 \n",
      "                                                                 flatten_34[0][0]                 \n",
      "                                                                 flatten_35[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 1920)         0           concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 16)           30736       dropout_11[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 118,320\n",
      "Trainable params: 118,320\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 388us/sample - loss: 0.2225 - acc: 0.9360\n",
      "Loss: 0.22245212425076455 Accuracy: 0.93603325\n"
     ]
    }
   ],
   "source": [
    "log_dir = 'log'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "base = '1D_CNN_custom_multi_3_concat_ch_32_DO'\n",
    "\n",
    "with open(path.join(log_dir, base), 'w') as log_file:\n",
    "    for i in range(3, 9):\n",
    "        model_name = base+'_{}_conv'.format(i)\n",
    "        print()\n",
    "        print(model_name, 'Model')\n",
    "        model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "        model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "\n",
    "        model = load_model(model_filename)\n",
    "        model.summary()\n",
    "\n",
    "        [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "        print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "        del(model)\n",
    "\n",
    "        log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_multi_3_concat_ch_32_DO_3_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_33 (Conv1D)              (None, 16000, 32)    192         input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 16000, 32)    0           conv1d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling1D) (None, 5333, 32)     0           activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_34 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_33[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 5333, 32)     0           conv1d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling1D) (None, 1777, 32)     0           activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_35 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 1777, 32)     0           conv1d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling1D) (None, 592, 32)      0           activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_18 (Flatten)            (None, 170656)       0           max_pooling1d_33[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_19 (Flatten)            (None, 56864)        0           max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_20 (Flatten)            (None, 18944)        0           max_pooling1d_35[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 246464)       0           flatten_18[0][0]                 \n",
      "                                                                 flatten_19[0][0]                 \n",
      "                                                                 flatten_20[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 246464)       0           concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 16)           3943440     dropout_6[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 3,953,936\n",
      "Trainable params: 3,953,936\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 2s 382us/sample - loss: 2.3379 - acc: 0.5826\n",
      "Loss: 2.337937172229169 Accuracy: 0.5825545\n",
      "\n",
      "1D_CNN_custom_multi_3_concat_ch_32_DO_4_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_8 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_36 (Conv1D)              (None, 16000, 32)    192         input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 16000, 32)    0           conv1d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling1D) (None, 5333, 32)     0           activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_37 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_36[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 5333, 32)     0           conv1d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling1D) (None, 1777, 32)     0           activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_38 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_37[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 1777, 32)     0           conv1d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling1D) (None, 592, 32)      0           activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_39 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 592, 32)      0           conv1d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling1D) (None, 197, 32)      0           activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_21 (Flatten)            (None, 56864)        0           max_pooling1d_37[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_22 (Flatten)            (None, 18944)        0           max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_23 (Flatten)            (None, 6304)         0           max_pooling1d_39[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 82112)        0           flatten_21[0][0]                 \n",
      "                                                                 flatten_22[0][0]                 \n",
      "                                                                 flatten_23[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 82112)        0           concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 16)           1313808     dropout_7[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,329,456\n",
      "Trainable params: 1,329,456\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 380us/sample - loss: 1.6199 - acc: 0.6783\n",
      "Loss: 1.6198623078882013 Accuracy: 0.678297\n",
      "\n",
      "1D_CNN_custom_multi_3_concat_ch_32_DO_5_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_40 (Conv1D)              (None, 16000, 32)    192         input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 16000, 32)    0           conv1d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling1D) (None, 5333, 32)     0           activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_41 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_40[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 5333, 32)     0           conv1d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling1D) (None, 1777, 32)     0           activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_42 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_41[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 1777, 32)     0           conv1d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling1D) (None, 592, 32)      0           activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_43 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_42[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 592, 32)      0           conv1d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling1D) (None, 197, 32)      0           activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_44 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 197, 64)      0           conv1d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling1D) (None, 65, 64)       0           activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_24 (Flatten)            (None, 18944)        0           max_pooling1d_42[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_25 (Flatten)            (None, 6304)         0           max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_26 (Flatten)            (None, 4160)         0           max_pooling1d_44[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 29408)        0           flatten_24[0][0]                 \n",
      "                                                                 flatten_25[0][0]                 \n",
      "                                                                 flatten_26[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 29408)        0           concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 16)           470544      dropout_8[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 496,496\n",
      "Trainable params: 496,496\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 2s 400us/sample - loss: 0.8548 - acc: 0.8123\n",
      "Loss: 0.8547643970725197 Accuracy: 0.81225336\n",
      "\n",
      "1D_CNN_custom_multi_3_concat_ch_32_DO_6_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_10 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_45 (Conv1D)              (None, 16000, 32)    192         input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 16000, 32)    0           conv1d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling1D) (None, 5333, 32)     0           activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_46 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_45[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 5333, 32)     0           conv1d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling1D) (None, 1777, 32)     0           activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_47 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_46[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 1777, 32)     0           conv1d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling1D) (None, 592, 32)      0           activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_48 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_47[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 592, 32)      0           conv1d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling1D) (None, 197, 32)      0           activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_49 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_48[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 197, 64)      0           conv1d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling1D) (None, 65, 64)       0           activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_50 (Conv1D)              (None, 65, 64)       20544       max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 65, 64)       0           conv1d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling1D) (None, 21, 64)       0           activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_27 (Flatten)            (None, 6304)         0           max_pooling1d_48[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_28 (Flatten)            (None, 4160)         0           max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_29 (Flatten)            (None, 1344)         0           max_pooling1d_50[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 11808)        0           flatten_27[0][0]                 \n",
      "                                                                 flatten_28[0][0]                 \n",
      "                                                                 flatten_29[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 11808)        0           concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 16)           188944      dropout_9[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 235,440\n",
      "Trainable params: 235,440\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 385us/sample - loss: 0.4516 - acc: 0.9065\n",
      "Loss: 0.45163340688308823 Accuracy: 0.90654206\n",
      "\n",
      "1D_CNN_custom_multi_3_concat_ch_32_DO_7_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_51 (Conv1D)              (None, 16000, 32)    192         input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 16000, 32)    0           conv1d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling1D) (None, 5333, 32)     0           activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_52 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_51[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 5333, 32)     0           conv1d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling1D) (None, 1777, 32)     0           activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_53 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_52[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 1777, 32)     0           conv1d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling1D) (None, 592, 32)      0           activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_54 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_53[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 592, 32)      0           conv1d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling1D) (None, 197, 32)      0           activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_55 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_54[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 197, 64)      0           conv1d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling1D) (None, 65, 64)       0           activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_56 (Conv1D)              (None, 65, 64)       20544       max_pooling1d_55[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 65, 64)       0           conv1d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling1D) (None, 21, 64)       0           activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_57 (Conv1D)              (None, 21, 64)       20544       max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 21, 64)       0           conv1d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling1D) (None, 7, 64)        0           activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_30 (Flatten)            (None, 4160)         0           max_pooling1d_55[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_31 (Flatten)            (None, 1344)         0           max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_32 (Flatten)            (None, 448)          0           max_pooling1d_57[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 5952)         0           flatten_30[0][0]                 \n",
      "                                                                 flatten_31[0][0]                 \n",
      "                                                                 flatten_32[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 5952)         0           concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 16)           95248       dropout_10[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 162,288\n",
      "Trainable params: 162,288\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 2s 382us/sample - loss: 0.2503 - acc: 0.9404\n",
      "Loss: 0.2502820456247711 Accuracy: 0.9403946\n",
      "\n",
      "1D_CNN_custom_multi_3_concat_ch_32_DO_8_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_12 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_58 (Conv1D)              (None, 16000, 32)    192         input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 16000, 32)    0           conv1d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling1D) (None, 5333, 32)     0           activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_59 (Conv1D)              (None, 5333, 32)     5152        max_pooling1d_58[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 5333, 32)     0           conv1d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling1D) (None, 1777, 32)     0           activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_60 (Conv1D)              (None, 1777, 32)     5152        max_pooling1d_59[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 1777, 32)     0           conv1d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling1D) (None, 592, 32)      0           activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_61 (Conv1D)              (None, 592, 32)      5152        max_pooling1d_60[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 592, 32)      0           conv1d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling1D) (None, 197, 32)      0           activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_62 (Conv1D)              (None, 197, 64)      10304       max_pooling1d_61[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 197, 64)      0           conv1d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling1D) (None, 65, 64)       0           activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_63 (Conv1D)              (None, 65, 64)       20544       max_pooling1d_62[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 65, 64)       0           conv1d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling1D) (None, 21, 64)       0           activation_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_64 (Conv1D)              (None, 21, 64)       20544       max_pooling1d_63[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 21, 64)       0           conv1d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling1D) (None, 7, 64)        0           activation_64[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_65 (Conv1D)              (None, 7, 64)        20544       max_pooling1d_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 7, 64)        0           conv1d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling1D) (None, 2, 64)        0           activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_33 (Flatten)            (None, 1344)         0           max_pooling1d_63[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_34 (Flatten)            (None, 448)          0           max_pooling1d_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_35 (Flatten)            (None, 128)          0           max_pooling1d_65[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 1920)         0           flatten_33[0][0]                 \n",
      "                                                                 flatten_34[0][0]                 \n",
      "                                                                 flatten_35[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 1920)         0           concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 16)           30736       dropout_11[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 118,320\n",
      "Trainable params: 118,320\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 411us/sample - loss: 0.2591 - acc: 0.9398\n",
      "Loss: 0.2591033273775939 Accuracy: 0.93977153\n"
     ]
    }
   ],
   "source": [
    "for i in range(3, 9):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + '000_last.hdf5'\n",
    "\n",
    "    model = load_model(model_filename)\n",
    "    model.summary()\n",
    "\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
