{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path as path\n",
    "import itertools\n",
    "from sklearn.preprocessing import maxabs_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Input,InputLayer, Dense, Activation, BatchNormalization, Flatten, Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D, Dropout\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler, \\\n",
    "                                        EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '6'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = path.join('..', 'data')\n",
    "data_dir = path.join(base_dir, 'data_speech_commands_v0.02')\n",
    " \n",
    "train_txt = path.join(data_dir, 'wav_train_16words.txt')\n",
    "val_txt = path.join(data_dir, 'wav_validation_16words.txt')\n",
    "test_txt = path.join(data_dir, 'wav_test_16words.txt')\n",
    "\n",
    "train_data = np.load(path.join(data_dir, 'wav_train_data.npz'))\n",
    "val_data = np.load(path.join(data_dir, 'wav_validation_data.npz'))\n",
    "test_data = np.load(path.join(data_dir, 'wav_test_data.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36805, 16000, 1),\n",
       " (36805,),\n",
       " (4293, 16000, 1),\n",
       " (4293,),\n",
       " (4815, 16000, 1),\n",
       " (4815,),\n",
       " (16, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = train_data['x_train']\n",
    "y_train = train_data['y_train']\n",
    "x_val = val_data['x_val']\n",
    "y_val = val_data['y_val']\n",
    "x_test = test_data['x_test']\n",
    "y_test = test_data['y_test']\n",
    "y_table = test_data['table']\n",
    "\n",
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape, y_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_test[0].shape\n",
    "output_size = y_table.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_abs = np.asarray([maxabs_scale(wav) for wav in x_train])\n",
    "y_train_onehot = np.asarray([to_categorical(label, output_size) for label in y_train])\n",
    "del x_train, y_train\n",
    "\n",
    "x_val_abs = np.asarray([maxabs_scale(wav) for wav in x_val])\n",
    "y_val_onehot = np.asarray([to_categorical(label, output_size) for label in y_val])\n",
    "del x_val, y_val\n",
    "\n",
    "x_test_abs = np.asarray([maxabs_scale(wav) for wav in x_test])\n",
    "y_test_onehot = np.asarray([to_categorical(label, output_size) for label in y_test])\n",
    "del x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_1d_cnn_custom_DO_075_DO_BN(conv_num=1):\n",
    "    model=Sequential()\n",
    "    model.add(Conv1D (kernel_size=5, filters=64, strides=1, padding='same', input_shape=input_shape)) \n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "#     model.add(MaxPooling1D(pool_size=3, strides=3, padding='same'))\n",
    "    \n",
    "    for i in range(conv_num-1):\n",
    "        model.add(Conv1D (kernel_size=5, filters=64*(2**int((i+1)/4)), \n",
    "                          strides=1, padding='same'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(MaxPooling1D(pool_size=3, strides=3))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dropout(0.75))\n",
    "    model.add(Dense(output_size, activation='softmax' ))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1 (Batc (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1024000)           0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1024000)           0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                16384016  \n",
      "=================================================================\n",
      "Total params: 16,384,656\n",
      "Trainable params: 16,384,528\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_1 (Ba (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_2 (Ba (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 341312)            0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 341312)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                5461008   \n",
      "=================================================================\n",
      "Total params: 5,482,448\n",
      "Trainable params: 5,482,192\n",
      "Non-trainable params: 256\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_3 (Conv1D)            (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_3 (Ba (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_4 (Ba (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_5 (Ba (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                1819664   \n",
      "=================================================================\n",
      "Total params: 1,861,904\n",
      "Trainable params: 1,861,520\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_6 (Conv1D)            (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_6 (Ba (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_7 (Ba (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_8 (Ba (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_9 (Ba (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                606224    \n",
      "=================================================================\n",
      "Total params: 669,264\n",
      "Trainable params: 668,752\n",
      "Non-trainable params: 512\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_10 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_10 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_11 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_12 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_12 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_13 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 592, 128)          41088     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_14 (B (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1 (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                403472    \n",
      "=================================================================\n",
      "Total params: 508,112\n",
      "Trainable params: 507,344\n",
      "Non-trainable params: 768\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_15 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_15 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_16 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_16 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_17 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_17 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_18 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_18 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_19 (Conv1D)           (None, 592, 128)          41088     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_19 (B (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_20 (Conv1D)           (None, 197, 128)          82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_20 (B (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 16)                133136    \n",
      "=================================================================\n",
      "Total params: 320,336\n",
      "Trainable params: 319,312\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_21 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_21 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_22 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_22 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_23 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_23 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_24 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_24 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_25 (Conv1D)           (None, 592, 128)          41088     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_25 (B (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_26 (Conv1D)           (None, 197, 128)          82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_26 (B (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_27 (Conv1D)           (None, 65, 128)           82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_27 (B (None, 65, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_27 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 16)                43024     \n",
      "=================================================================\n",
      "Total params: 312,784\n",
      "Trainable params: 311,504\n",
      "Non-trainable params: 1,280\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_28 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_28 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_28 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_29 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_29 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_29 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_30 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_30 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_30 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_31 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_31 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_31 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_32 (Conv1D)           (None, 592, 128)          41088     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_32 (B (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_32 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_33 (Conv1D)           (None, 197, 128)          82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_33 (B (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_33 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_34 (Conv1D)           (None, 65, 128)           82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_34 (B (None, 65, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_34 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_26 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_35 (Conv1D)           (None, 21, 128)           82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_35 (B (None, 21, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_35 (Activation)   (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_27 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 16)                14352     \n",
      "=================================================================\n",
      "Total params: 366,672\n",
      "Trainable params: 365,136\n",
      "Non-trainable params: 1,536\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_36 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_36 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_36 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_37 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_37 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_37 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_28 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_38 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_38 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_38 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_29 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_39 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_39 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_39 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_30 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_40 (Conv1D)           (None, 592, 128)          41088     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_40 (B (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_40 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_31 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_41 (Conv1D)           (None, 197, 128)          82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_41 (B (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_41 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_32 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_42 (Conv1D)           (None, 65, 128)           82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_42 (B (None, 65, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_42 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_43 (Conv1D)           (None, 21, 128)           82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_43 (B (None, 21, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_43 (Activation)   (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_44 (Conv1D)           (None, 7, 256)            164096    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_44 (B (None, 7, 256)            1024      \n",
      "_________________________________________________________________\n",
      "activation_44 (Activation)   (None, 7, 256)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling (None, 2, 256)            0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 16)                8208      \n",
      "=================================================================\n",
      "Total params: 525,648\n",
      "Trainable params: 523,600\n",
      "Non-trainable params: 2,048\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 10):\n",
    "    model = build_1d_cnn_custom_DO_075_DO_BN(conv_num=i)\n",
    "    model.summary()\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 3.1889 - acc: 0.2245\n",
      "Epoch 00001: val_loss improved from inf to 2.23981, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_BN_1_conv_checkpoint/001-2.2398.hdf5\n",
      "36805/36805 [==============================] - 44s 1ms/sample - loss: 3.1886 - acc: 0.2245 - val_loss: 2.2398 - val_acc: 0.3189\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6612 - acc: 0.4896\n",
      "Epoch 00002: val_loss did not improve from 2.23981\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 1.6612 - acc: 0.4896 - val_loss: 2.4571 - val_acc: 0.3119\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2752 - acc: 0.6044\n",
      "Epoch 00003: val_loss did not improve from 2.23981\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 1.2751 - acc: 0.6044 - val_loss: 2.5985 - val_acc: 0.3021\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0421 - acc: 0.6778\n",
      "Epoch 00004: val_loss did not improve from 2.23981\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 1.0422 - acc: 0.6778 - val_loss: 2.7706 - val_acc: 0.2965\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8708 - acc: 0.7340\n",
      "Epoch 00005: val_loss did not improve from 2.23981\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.8707 - acc: 0.7340 - val_loss: 2.9329 - val_acc: 0.2867\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7547 - acc: 0.7680\n",
      "Epoch 00006: val_loss did not improve from 2.23981\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.7547 - acc: 0.7680 - val_loss: 4.0460 - val_acc: 0.2490\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6580 - acc: 0.7976\n",
      "Epoch 00007: val_loss did not improve from 2.23981\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.6579 - acc: 0.7976 - val_loss: 3.4175 - val_acc: 0.2798\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5767 - acc: 0.8243\n",
      "Epoch 00008: val_loss did not improve from 2.23981\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.5767 - acc: 0.8243 - val_loss: 3.5477 - val_acc: 0.2921\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5229 - acc: 0.8393\n",
      "Epoch 00009: val_loss did not improve from 2.23981\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.5229 - acc: 0.8393 - val_loss: 3.4261 - val_acc: 0.3093\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4647 - acc: 0.8587\n",
      "Epoch 00010: val_loss did not improve from 2.23981\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.4647 - acc: 0.8587 - val_loss: 3.6534 - val_acc: 0.3058\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4259 - acc: 0.8708\n",
      "Epoch 00011: val_loss did not improve from 2.23981\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.4259 - acc: 0.8708 - val_loss: 4.6599 - val_acc: 0.2474\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3775 - acc: 0.8882\n",
      "Epoch 00012: val_loss did not improve from 2.23981\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3775 - acc: 0.8882 - val_loss: 3.8645 - val_acc: 0.2996\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3539 - acc: 0.8925\n",
      "Epoch 00013: val_loss did not improve from 2.23981\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3539 - acc: 0.8925 - val_loss: 4.3199 - val_acc: 0.3010\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3287 - acc: 0.9002\n",
      "Epoch 00014: val_loss did not improve from 2.23981\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.3287 - acc: 0.9003 - val_loss: 4.4899 - val_acc: 0.2616\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2980 - acc: 0.9123\n",
      "Epoch 00015: val_loss did not improve from 2.23981\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.2982 - acc: 0.9123 - val_loss: 3.9466 - val_acc: 0.3184\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2875 - acc: 0.9164\n",
      "Epoch 00016: val_loss did not improve from 2.23981\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.2874 - acc: 0.9164 - val_loss: 4.9160 - val_acc: 0.2891\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2734 - acc: 0.9193\n",
      "Epoch 00017: val_loss did not improve from 2.23981\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2734 - acc: 0.9193 - val_loss: 4.8447 - val_acc: 0.2807\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2501 - acc: 0.9264\n",
      "Epoch 00018: val_loss did not improve from 2.23981\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.2502 - acc: 0.9263 - val_loss: 4.7772 - val_acc: 0.3044\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2366 - acc: 0.9309\n",
      "Epoch 00019: val_loss did not improve from 2.23981\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2366 - acc: 0.9309 - val_loss: 4.4898 - val_acc: 0.3126\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2301 - acc: 0.9337\n",
      "Epoch 00020: val_loss did not improve from 2.23981\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.2301 - acc: 0.9337 - val_loss: 4.9940 - val_acc: 0.2921\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2276 - acc: 0.9362\n",
      "Epoch 00021: val_loss did not improve from 2.23981\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.2276 - acc: 0.9363 - val_loss: 4.6077 - val_acc: 0.3017\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2084 - acc: 0.9413\n",
      "Epoch 00022: val_loss did not improve from 2.23981\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.2084 - acc: 0.9413 - val_loss: 4.6687 - val_acc: 0.3051\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2064 - acc: 0.9416\n",
      "Epoch 00023: val_loss did not improve from 2.23981\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.2064 - acc: 0.9416 - val_loss: 4.5937 - val_acc: 0.3203\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1927 - acc: 0.9468\n",
      "Epoch 00024: val_loss did not improve from 2.23981\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.1927 - acc: 0.9468 - val_loss: 4.5960 - val_acc: 0.3107\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1961 - acc: 0.9448\n",
      "Epoch 00025: val_loss did not improve from 2.23981\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.1961 - acc: 0.9448 - val_loss: 5.5692 - val_acc: 0.2807\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1831 - acc: 0.9498\n",
      "Epoch 00026: val_loss did not improve from 2.23981\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.1831 - acc: 0.9498 - val_loss: 4.8685 - val_acc: 0.3100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1765 - acc: 0.9501\n",
      "Epoch 00027: val_loss did not improve from 2.23981\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.1765 - acc: 0.9501 - val_loss: 5.7983 - val_acc: 0.2807\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1688 - acc: 0.9522\n",
      "Epoch 00028: val_loss did not improve from 2.23981\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.1688 - acc: 0.9522 - val_loss: 4.9282 - val_acc: 0.3017\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1665 - acc: 0.9541\n",
      "Epoch 00029: val_loss did not improve from 2.23981\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.1665 - acc: 0.9541 - val_loss: 5.5324 - val_acc: 0.2690\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1614 - acc: 0.9564\n",
      "Epoch 00030: val_loss did not improve from 2.23981\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.1614 - acc: 0.9564 - val_loss: 5.0112 - val_acc: 0.3035\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1547 - acc: 0.9558\n",
      "Epoch 00031: val_loss did not improve from 2.23981\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.1547 - acc: 0.9558 - val_loss: 5.3396 - val_acc: 0.2888\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1484 - acc: 0.9577\n",
      "Epoch 00032: val_loss did not improve from 2.23981\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.1484 - acc: 0.9578 - val_loss: 5.0045 - val_acc: 0.3252\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1445 - acc: 0.9621\n",
      "Epoch 00033: val_loss did not improve from 2.23981\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.1445 - acc: 0.9621 - val_loss: 6.1283 - val_acc: 0.2744\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1456 - acc: 0.9599\n",
      "Epoch 00034: val_loss did not improve from 2.23981\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.1457 - acc: 0.9598 - val_loss: 5.4261 - val_acc: 0.2986\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1502 - acc: 0.9591\n",
      "Epoch 00035: val_loss did not improve from 2.23981\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.1502 - acc: 0.9591 - val_loss: 5.1809 - val_acc: 0.3212\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1417 - acc: 0.9624\n",
      "Epoch 00036: val_loss did not improve from 2.23981\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.1418 - acc: 0.9624 - val_loss: 6.2415 - val_acc: 0.2872\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1363 - acc: 0.9640\n",
      "Epoch 00037: val_loss did not improve from 2.23981\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.1363 - acc: 0.9640 - val_loss: 5.6320 - val_acc: 0.3063\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1377 - acc: 0.9650\n",
      "Epoch 00038: val_loss did not improve from 2.23981\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.1377 - acc: 0.9650 - val_loss: 5.4425 - val_acc: 0.3133\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1280 - acc: 0.9672\n",
      "Epoch 00039: val_loss did not improve from 2.23981\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.1279 - acc: 0.9672 - val_loss: 6.3749 - val_acc: 0.2551\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1290 - acc: 0.9664\n",
      "Epoch 00040: val_loss did not improve from 2.23981\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.1290 - acc: 0.9664 - val_loss: 5.5107 - val_acc: 0.3126\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1309 - acc: 0.9640\n",
      "Epoch 00041: val_loss did not improve from 2.23981\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.1309 - acc: 0.9640 - val_loss: 5.6534 - val_acc: 0.2993\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1253 - acc: 0.9674\n",
      "Epoch 00042: val_loss did not improve from 2.23981\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.1253 - acc: 0.9674 - val_loss: 5.9600 - val_acc: 0.2984\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1183 - acc: 0.9687\n",
      "Epoch 00043: val_loss did not improve from 2.23981\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.1183 - acc: 0.9687 - val_loss: 5.8542 - val_acc: 0.3061\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1241 - acc: 0.9678\n",
      "Epoch 00044: val_loss did not improve from 2.23981\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.1241 - acc: 0.9678 - val_loss: 6.2096 - val_acc: 0.2793\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1180 - acc: 0.9686\n",
      "Epoch 00045: val_loss did not improve from 2.23981\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.1179 - acc: 0.9686 - val_loss: 5.6206 - val_acc: 0.3222\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1218 - acc: 0.9679\n",
      "Epoch 00046: val_loss did not improve from 2.23981\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.1218 - acc: 0.9679 - val_loss: 5.6726 - val_acc: 0.3224\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1181 - acc: 0.9696\n",
      "Epoch 00047: val_loss did not improve from 2.23981\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.1181 - acc: 0.9696 - val_loss: 6.4067 - val_acc: 0.2737\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1168 - acc: 0.9712\n",
      "Epoch 00048: val_loss did not improve from 2.23981\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.1168 - acc: 0.9712 - val_loss: 6.1449 - val_acc: 0.3031\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1125 - acc: 0.9714\n",
      "Epoch 00049: val_loss did not improve from 2.23981\n",
      "36805/36805 [==============================] - 43s 1ms/sample - loss: 0.1127 - acc: 0.9714 - val_loss: 6.2912 - val_acc: 0.3035\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1137 - acc: 0.9708\n",
      "Epoch 00050: val_loss did not improve from 2.23981\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.1138 - acc: 0.9708 - val_loss: 6.5597 - val_acc: 0.2986\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1174 - acc: 0.9716\n",
      "Epoch 00051: val_loss did not improve from 2.23981\n",
      "36805/36805 [==============================] - 42s 1ms/sample - loss: 0.1174 - acc: 0.9716 - val_loss: 5.7224 - val_acc: 0.3273\n",
      "\n",
      "1D_CNN_custom_DO_075_DO_BN_1_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 404us/sample - loss: 2.2795 - acc: 0.2860\n",
      "Loss: 2.2795086906458852 Accuracy: 0.2859813\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 3.3628 - acc: 0.2351\n",
      "Epoch 00001: val_loss improved from inf to 3.07166, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_BN_2_conv_checkpoint/001-3.0717.hdf5\n",
      "36805/36805 [==============================] - 73s 2ms/sample - loss: 3.3630 - acc: 0.2351 - val_loss: 3.0717 - val_acc: 0.2576\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.6683 - acc: 0.3631\n",
      "Epoch 00002: val_loss improved from 3.07166 to 2.30448, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_BN_2_conv_checkpoint/002-2.3045.hdf5\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 2.6684 - acc: 0.3630 - val_loss: 2.3045 - val_acc: 0.3892\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.1443 - acc: 0.4376\n",
      "Epoch 00003: val_loss improved from 2.30448 to 2.30291, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_BN_2_conv_checkpoint/003-2.3029.hdf5\n",
      "36805/36805 [==============================] - 71s 2ms/sample - loss: 2.1441 - acc: 0.4376 - val_loss: 2.3029 - val_acc: 0.3930\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.8748 - acc: 0.4959\n",
      "Epoch 00004: val_loss improved from 2.30291 to 2.29805, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_BN_2_conv_checkpoint/004-2.2981.hdf5\n",
      "36805/36805 [==============================] - 71s 2ms/sample - loss: 1.8748 - acc: 0.4959 - val_loss: 2.2981 - val_acc: 0.4000\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6530 - acc: 0.5429\n",
      "Epoch 00005: val_loss did not improve from 2.29805\n",
      "36805/36805 [==============================] - 71s 2ms/sample - loss: 1.6530 - acc: 0.5429 - val_loss: 2.6949 - val_acc: 0.3669\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4971 - acc: 0.5831\n",
      "Epoch 00006: val_loss did not improve from 2.29805\n",
      "36805/36805 [==============================] - 71s 2ms/sample - loss: 1.4973 - acc: 0.5830 - val_loss: 3.0459 - val_acc: 0.3611\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3595 - acc: 0.6143\n",
      "Epoch 00007: val_loss did not improve from 2.29805\n",
      "36805/36805 [==============================] - 71s 2ms/sample - loss: 1.3596 - acc: 0.6143 - val_loss: 2.8612 - val_acc: 0.4011\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2199 - acc: 0.6512\n",
      "Epoch 00008: val_loss did not improve from 2.29805\n",
      "36805/36805 [==============================] - 71s 2ms/sample - loss: 1.2200 - acc: 0.6512 - val_loss: 3.4224 - val_acc: 0.3270\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1207 - acc: 0.6767\n",
      "Epoch 00009: val_loss did not improve from 2.29805\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 1.1206 - acc: 0.6768 - val_loss: 2.6097 - val_acc: 0.4125\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0032 - acc: 0.7061\n",
      "Epoch 00010: val_loss did not improve from 2.29805\n",
      "36805/36805 [==============================] - 71s 2ms/sample - loss: 1.0032 - acc: 0.7061 - val_loss: 2.4956 - val_acc: 0.4396\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9031 - acc: 0.7318\n",
      "Epoch 00011: val_loss did not improve from 2.29805\n",
      "36805/36805 [==============================] - 71s 2ms/sample - loss: 0.9031 - acc: 0.7319 - val_loss: 2.4528 - val_acc: 0.4621\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8363 - acc: 0.7504\n",
      "Epoch 00012: val_loss did not improve from 2.29805\n",
      "36805/36805 [==============================] - 71s 2ms/sample - loss: 0.8365 - acc: 0.7504 - val_loss: 3.0263 - val_acc: 0.4193\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7995 - acc: 0.7609\n",
      "Epoch 00013: val_loss did not improve from 2.29805\n",
      "36805/36805 [==============================] - 71s 2ms/sample - loss: 0.7995 - acc: 0.7609 - val_loss: 2.8295 - val_acc: 0.4454\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7313 - acc: 0.7799\n",
      "Epoch 00014: val_loss did not improve from 2.29805\n",
      "36805/36805 [==============================] - 71s 2ms/sample - loss: 0.7315 - acc: 0.7798 - val_loss: 2.6615 - val_acc: 0.4444\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6706 - acc: 0.7971\n",
      "Epoch 00015: val_loss did not improve from 2.29805\n",
      "36805/36805 [==============================] - 71s 2ms/sample - loss: 0.6706 - acc: 0.7971 - val_loss: 2.8154 - val_acc: 0.4521\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6275 - acc: 0.8113\n",
      "Epoch 00016: val_loss did not improve from 2.29805\n",
      "36805/36805 [==============================] - 71s 2ms/sample - loss: 0.6276 - acc: 0.8113 - val_loss: 3.0303 - val_acc: 0.4086\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5821 - acc: 0.8211\n",
      "Epoch 00017: val_loss did not improve from 2.29805\n",
      "36805/36805 [==============================] - 71s 2ms/sample - loss: 0.5823 - acc: 0.8210 - val_loss: 3.2826 - val_acc: 0.4365\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5679 - acc: 0.8275\n",
      "Epoch 00018: val_loss did not improve from 2.29805\n",
      "36805/36805 [==============================] - 71s 2ms/sample - loss: 0.5678 - acc: 0.8276 - val_loss: 3.1081 - val_acc: 0.4384\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5273 - acc: 0.8405\n",
      "Epoch 00019: val_loss did not improve from 2.29805\n",
      "36805/36805 [==============================] - 71s 2ms/sample - loss: 0.5273 - acc: 0.8405 - val_loss: 3.9654 - val_acc: 0.3729\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4904 - acc: 0.8516\n",
      "Epoch 00020: val_loss did not improve from 2.29805\n",
      "36805/36805 [==============================] - 71s 2ms/sample - loss: 0.4905 - acc: 0.8515 - val_loss: 3.9493 - val_acc: 0.3895\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4583 - acc: 0.8592\n",
      "Epoch 00021: val_loss did not improve from 2.29805\n",
      "36805/36805 [==============================] - 71s 2ms/sample - loss: 0.4584 - acc: 0.8592 - val_loss: 3.2367 - val_acc: 0.4389\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4376 - acc: 0.8678\n",
      "Epoch 00022: val_loss did not improve from 2.29805\n",
      "36805/36805 [==============================] - 71s 2ms/sample - loss: 0.4377 - acc: 0.8678 - val_loss: 3.0010 - val_acc: 0.4507\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3983 - acc: 0.8768\n",
      "Epoch 00023: val_loss did not improve from 2.29805\n",
      "36805/36805 [==============================] - 71s 2ms/sample - loss: 0.3983 - acc: 0.8768 - val_loss: 3.0942 - val_acc: 0.4475\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3847 - acc: 0.8812\n",
      "Epoch 00024: val_loss did not improve from 2.29805\n",
      "36805/36805 [==============================] - 71s 2ms/sample - loss: 0.3848 - acc: 0.8812 - val_loss: 3.0434 - val_acc: 0.4729\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3643 - acc: 0.8882\n",
      "Epoch 00025: val_loss did not improve from 2.29805\n",
      "36805/36805 [==============================] - 71s 2ms/sample - loss: 0.3643 - acc: 0.8882 - val_loss: 3.8864 - val_acc: 0.4151\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3467 - acc: 0.8921\n",
      "Epoch 00026: val_loss did not improve from 2.29805\n",
      "36805/36805 [==============================] - 71s 2ms/sample - loss: 0.3466 - acc: 0.8922 - val_loss: 3.5624 - val_acc: 0.4260\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3387 - acc: 0.8971\n",
      "Epoch 00027: val_loss did not improve from 2.29805\n",
      "36805/36805 [==============================] - 71s 2ms/sample - loss: 0.3387 - acc: 0.8972 - val_loss: 3.0301 - val_acc: 0.4687\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3086 - acc: 0.9063\n",
      "Epoch 00028: val_loss did not improve from 2.29805\n",
      "36805/36805 [==============================] - 71s 2ms/sample - loss: 0.3088 - acc: 0.9062 - val_loss: 3.0849 - val_acc: 0.4587\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3070 - acc: 0.9049\n",
      "Epoch 00029: val_loss did not improve from 2.29805\n",
      "36805/36805 [==============================] - 71s 2ms/sample - loss: 0.3069 - acc: 0.9050 - val_loss: 3.2470 - val_acc: 0.4556\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2957 - acc: 0.9095\n",
      "Epoch 00030: val_loss did not improve from 2.29805\n",
      "36805/36805 [==============================] - 71s 2ms/sample - loss: 0.2957 - acc: 0.9095 - val_loss: 3.1384 - val_acc: 0.4640\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2744 - acc: 0.9168\n",
      "Epoch 00031: val_loss did not improve from 2.29805\n",
      "36805/36805 [==============================] - 71s 2ms/sample - loss: 0.2744 - acc: 0.9168 - val_loss: 3.3027 - val_acc: 0.4580\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2609 - acc: 0.9191\n",
      "Epoch 00032: val_loss did not improve from 2.29805\n",
      "36805/36805 [==============================] - 71s 2ms/sample - loss: 0.2609 - acc: 0.9191 - val_loss: 3.1769 - val_acc: 0.4563\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2550 - acc: 0.9226\n",
      "Epoch 00033: val_loss did not improve from 2.29805\n",
      "36805/36805 [==============================] - 71s 2ms/sample - loss: 0.2551 - acc: 0.9225 - val_loss: 3.9462 - val_acc: 0.4337\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2533 - acc: 0.9248\n",
      "Epoch 00034: val_loss did not improve from 2.29805\n",
      "36805/36805 [==============================] - 71s 2ms/sample - loss: 0.2533 - acc: 0.9248 - val_loss: 3.0426 - val_acc: 0.4787\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2407 - acc: 0.9267\n",
      "Epoch 00035: val_loss did not improve from 2.29805\n",
      "36805/36805 [==============================] - 71s 2ms/sample - loss: 0.2407 - acc: 0.9267 - val_loss: 3.8851 - val_acc: 0.4181\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2195 - acc: 0.9323\n",
      "Epoch 00036: val_loss did not improve from 2.29805\n",
      "36805/36805 [==============================] - 71s 2ms/sample - loss: 0.2195 - acc: 0.9323 - val_loss: 3.2582 - val_acc: 0.4596\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2248 - acc: 0.9320\n",
      "Epoch 00037: val_loss did not improve from 2.29805\n",
      "36805/36805 [==============================] - 71s 2ms/sample - loss: 0.2248 - acc: 0.9320 - val_loss: 3.4096 - val_acc: 0.4736\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2111 - acc: 0.9352\n",
      "Epoch 00038: val_loss did not improve from 2.29805\n",
      "36805/36805 [==============================] - 71s 2ms/sample - loss: 0.2111 - acc: 0.9352 - val_loss: 3.4942 - val_acc: 0.4563\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2004 - acc: 0.9395\n",
      "Epoch 00039: val_loss did not improve from 2.29805\n",
      "36805/36805 [==============================] - 71s 2ms/sample - loss: 0.2004 - acc: 0.9395 - val_loss: 3.1570 - val_acc: 0.4868\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2021 - acc: 0.9389\n",
      "Epoch 00040: val_loss did not improve from 2.29805\n",
      "36805/36805 [==============================] - 71s 2ms/sample - loss: 0.2021 - acc: 0.9388 - val_loss: 3.4923 - val_acc: 0.4524\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1946 - acc: 0.9412\n",
      "Epoch 00041: val_loss did not improve from 2.29805\n",
      "36805/36805 [==============================] - 71s 2ms/sample - loss: 0.1946 - acc: 0.9412 - val_loss: 3.3461 - val_acc: 0.4635\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1808 - acc: 0.9470\n",
      "Epoch 00042: val_loss did not improve from 2.29805\n",
      "36805/36805 [==============================] - 71s 2ms/sample - loss: 0.1807 - acc: 0.9470 - val_loss: 3.1870 - val_acc: 0.4875\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1783 - acc: 0.9467\n",
      "Epoch 00043: val_loss did not improve from 2.29805\n",
      "36805/36805 [==============================] - 71s 2ms/sample - loss: 0.1784 - acc: 0.9466 - val_loss: 3.1797 - val_acc: 0.4656\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1721 - acc: 0.9481\n",
      "Epoch 00044: val_loss did not improve from 2.29805\n",
      "36805/36805 [==============================] - 71s 2ms/sample - loss: 0.1721 - acc: 0.9481 - val_loss: 3.1797 - val_acc: 0.4647\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1682 - acc: 0.9500\n",
      "Epoch 00045: val_loss did not improve from 2.29805\n",
      "36805/36805 [==============================] - 71s 2ms/sample - loss: 0.1682 - acc: 0.9500 - val_loss: 3.1953 - val_acc: 0.4875\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1634 - acc: 0.9514\n",
      "Epoch 00046: val_loss did not improve from 2.29805\n",
      "36805/36805 [==============================] - 71s 2ms/sample - loss: 0.1634 - acc: 0.9513 - val_loss: 3.1742 - val_acc: 0.4922\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1613 - acc: 0.9529\n",
      "Epoch 00047: val_loss did not improve from 2.29805\n",
      "36805/36805 [==============================] - 71s 2ms/sample - loss: 0.1613 - acc: 0.9529 - val_loss: 3.2408 - val_acc: 0.4736\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1515 - acc: 0.9550\n",
      "Epoch 00048: val_loss did not improve from 2.29805\n",
      "36805/36805 [==============================] - 71s 2ms/sample - loss: 0.1515 - acc: 0.9550 - val_loss: 3.5057 - val_acc: 0.4610\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1467 - acc: 0.9558\n",
      "Epoch 00049: val_loss did not improve from 2.29805\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 0.1467 - acc: 0.9557 - val_loss: 3.1829 - val_acc: 0.4666\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1492 - acc: 0.9557\n",
      "Epoch 00050: val_loss did not improve from 2.29805\n",
      "36805/36805 [==============================] - 71s 2ms/sample - loss: 0.1493 - acc: 0.9557 - val_loss: 3.4584 - val_acc: 0.4482\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1384 - acc: 0.9595\n",
      "Epoch 00051: val_loss did not improve from 2.29805\n",
      "36805/36805 [==============================] - 71s 2ms/sample - loss: 0.1384 - acc: 0.9595 - val_loss: 3.1751 - val_acc: 0.4726\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1347 - acc: 0.9609\n",
      "Epoch 00052: val_loss did not improve from 2.29805\n",
      "36805/36805 [==============================] - 71s 2ms/sample - loss: 0.1347 - acc: 0.9609 - val_loss: 3.5300 - val_acc: 0.4722\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1333 - acc: 0.9613\n",
      "Epoch 00053: val_loss did not improve from 2.29805\n",
      "36805/36805 [==============================] - 71s 2ms/sample - loss: 0.1333 - acc: 0.9613 - val_loss: 3.2716 - val_acc: 0.4785\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1282 - acc: 0.9616\n",
      "Epoch 00054: val_loss did not improve from 2.29805\n",
      "36805/36805 [==============================] - 71s 2ms/sample - loss: 0.1282 - acc: 0.9616 - val_loss: 3.1772 - val_acc: 0.5078\n",
      "\n",
      "1D_CNN_custom_DO_075_DO_BN_2_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 716us/sample - loss: 2.4671 - acc: 0.3776\n",
      "Loss: 2.4670915194142027 Accuracy: 0.3775701\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7201 - acc: 0.2797\n",
      "Epoch 00001: val_loss improved from inf to 2.01939, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_BN_3_conv_checkpoint/001-2.0194.hdf5\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 2.7200 - acc: 0.2797 - val_loss: 2.0194 - val_acc: 0.3382\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.8943 - acc: 0.4526\n",
      "Epoch 00002: val_loss improved from 2.01939 to 1.78835, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_BN_3_conv_checkpoint/002-1.7884.hdf5\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 1.8947 - acc: 0.4525 - val_loss: 1.7884 - val_acc: 0.5094\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6007 - acc: 0.5249\n",
      "Epoch 00003: val_loss improved from 1.78835 to 1.42605, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_BN_3_conv_checkpoint/003-1.4261.hdf5\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 1.6008 - acc: 0.5249 - val_loss: 1.4261 - val_acc: 0.5698\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4223 - acc: 0.5720\n",
      "Epoch 00004: val_loss did not improve from 1.42605\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 1.4228 - acc: 0.5719 - val_loss: 1.4291 - val_acc: 0.5677\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2763 - acc: 0.6113\n",
      "Epoch 00005: val_loss improved from 1.42605 to 1.28767, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_BN_3_conv_checkpoint/005-1.2877.hdf5\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 1.2763 - acc: 0.6112 - val_loss: 1.2877 - val_acc: 0.6287\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1800 - acc: 0.6392\n",
      "Epoch 00006: val_loss did not improve from 1.28767\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 1.1799 - acc: 0.6393 - val_loss: 1.4385 - val_acc: 0.5989\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0705 - acc: 0.6702\n",
      "Epoch 00007: val_loss did not improve from 1.28767\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 1.0706 - acc: 0.6702 - val_loss: 1.3935 - val_acc: 0.5947\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9910 - acc: 0.6938\n",
      "Epoch 00008: val_loss did not improve from 1.28767\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.9911 - acc: 0.6937 - val_loss: 1.4593 - val_acc: 0.5928\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9274 - acc: 0.7092\n",
      "Epoch 00009: val_loss improved from 1.28767 to 1.23846, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_BN_3_conv_checkpoint/009-1.2385.hdf5\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.9275 - acc: 0.7092 - val_loss: 1.2385 - val_acc: 0.6373\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8547 - acc: 0.7299\n",
      "Epoch 00010: val_loss did not improve from 1.23846\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.8547 - acc: 0.7299 - val_loss: 1.3368 - val_acc: 0.6294\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8210 - acc: 0.7410\n",
      "Epoch 00011: val_loss did not improve from 1.23846\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.8209 - acc: 0.7409 - val_loss: 1.3344 - val_acc: 0.6208\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7629 - acc: 0.7577\n",
      "Epoch 00012: val_loss did not improve from 1.23846\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.7629 - acc: 0.7577 - val_loss: 1.3444 - val_acc: 0.6345\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7168 - acc: 0.7706\n",
      "Epoch 00013: val_loss did not improve from 1.23846\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.7169 - acc: 0.7705 - val_loss: 1.3602 - val_acc: 0.6424\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6788 - acc: 0.7811\n",
      "Epoch 00014: val_loss did not improve from 1.23846\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.6788 - acc: 0.7811 - val_loss: 1.2515 - val_acc: 0.6636\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6500 - acc: 0.7912\n",
      "Epoch 00015: val_loss did not improve from 1.23846\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.6500 - acc: 0.7912 - val_loss: 1.3014 - val_acc: 0.6539\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6075 - acc: 0.8037\n",
      "Epoch 00016: val_loss did not improve from 1.23846\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.6076 - acc: 0.8036 - val_loss: 1.3305 - val_acc: 0.6513\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5927 - acc: 0.8079\n",
      "Epoch 00017: val_loss did not improve from 1.23846\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.5926 - acc: 0.8079 - val_loss: 1.3415 - val_acc: 0.6441\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5466 - acc: 0.8212\n",
      "Epoch 00018: val_loss did not improve from 1.23846\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.5466 - acc: 0.8212 - val_loss: 1.6721 - val_acc: 0.5830\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5268 - acc: 0.8259\n",
      "Epoch 00019: val_loss did not improve from 1.23846\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.5268 - acc: 0.8259 - val_loss: 1.5212 - val_acc: 0.6177\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5117 - acc: 0.8325\n",
      "Epoch 00020: val_loss did not improve from 1.23846\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.5117 - acc: 0.8325 - val_loss: 1.4517 - val_acc: 0.6375\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4867 - acc: 0.8392\n",
      "Epoch 00021: val_loss did not improve from 1.23846\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.4868 - acc: 0.8392 - val_loss: 1.3862 - val_acc: 0.6518\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4709 - acc: 0.8450\n",
      "Epoch 00022: val_loss did not improve from 1.23846\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.4709 - acc: 0.8450 - val_loss: 1.4621 - val_acc: 0.6532\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4490 - acc: 0.8538\n",
      "Epoch 00023: val_loss did not improve from 1.23846\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.4491 - acc: 0.8538 - val_loss: 1.4762 - val_acc: 0.6476\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4407 - acc: 0.8585\n",
      "Epoch 00024: val_loss did not improve from 1.23846\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.4407 - acc: 0.8585 - val_loss: 1.5319 - val_acc: 0.6257\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4204 - acc: 0.8620\n",
      "Epoch 00025: val_loss did not improve from 1.23846\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.4204 - acc: 0.8620 - val_loss: 1.5571 - val_acc: 0.6231\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4012 - acc: 0.8675\n",
      "Epoch 00026: val_loss did not improve from 1.23846\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.4012 - acc: 0.8675 - val_loss: 1.3925 - val_acc: 0.6557\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3876 - acc: 0.8716\n",
      "Epoch 00027: val_loss did not improve from 1.23846\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.3876 - acc: 0.8716 - val_loss: 1.2386 - val_acc: 0.6888\n",
      "Epoch 28/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3651 - acc: 0.8794\n",
      "Epoch 00028: val_loss did not improve from 1.23846\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.3651 - acc: 0.8794 - val_loss: 1.3233 - val_acc: 0.6967\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3592 - acc: 0.8813\n",
      "Epoch 00029: val_loss did not improve from 1.23846\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.3591 - acc: 0.8813 - val_loss: 1.3052 - val_acc: 0.6928\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3564 - acc: 0.8834\n",
      "Epoch 00030: val_loss did not improve from 1.23846\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.3565 - acc: 0.8834 - val_loss: 1.6071 - val_acc: 0.6369\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3405 - acc: 0.8881\n",
      "Epoch 00031: val_loss did not improve from 1.23846\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.3405 - acc: 0.8881 - val_loss: 1.5286 - val_acc: 0.6483\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3300 - acc: 0.8926\n",
      "Epoch 00032: val_loss did not improve from 1.23846\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.3300 - acc: 0.8926 - val_loss: 1.4472 - val_acc: 0.6646\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3155 - acc: 0.8955\n",
      "Epoch 00033: val_loss did not improve from 1.23846\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.3156 - acc: 0.8955 - val_loss: 1.6496 - val_acc: 0.6313\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3066 - acc: 0.8995\n",
      "Epoch 00034: val_loss did not improve from 1.23846\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.3066 - acc: 0.8995 - val_loss: 1.3450 - val_acc: 0.6923\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3015 - acc: 0.9011\n",
      "Epoch 00035: val_loss did not improve from 1.23846\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.3015 - acc: 0.9012 - val_loss: 1.3006 - val_acc: 0.6972\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2855 - acc: 0.9068\n",
      "Epoch 00036: val_loss did not improve from 1.23846\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.2856 - acc: 0.9068 - val_loss: 1.2999 - val_acc: 0.7035\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2871 - acc: 0.9061\n",
      "Epoch 00037: val_loss did not improve from 1.23846\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.2871 - acc: 0.9060 - val_loss: 1.4522 - val_acc: 0.6685\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2757 - acc: 0.9095\n",
      "Epoch 00038: val_loss did not improve from 1.23846\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.2757 - acc: 0.9095 - val_loss: 2.0607 - val_acc: 0.5844\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2685 - acc: 0.9118\n",
      "Epoch 00039: val_loss did not improve from 1.23846\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.2686 - acc: 0.9118 - val_loss: 1.2606 - val_acc: 0.7053\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2703 - acc: 0.9099\n",
      "Epoch 00040: val_loss did not improve from 1.23846\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.2704 - acc: 0.9098 - val_loss: 1.3617 - val_acc: 0.6869\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2562 - acc: 0.9158\n",
      "Epoch 00041: val_loss did not improve from 1.23846\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.2562 - acc: 0.9159 - val_loss: 1.3354 - val_acc: 0.6862\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2503 - acc: 0.9189\n",
      "Epoch 00042: val_loss did not improve from 1.23846\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.2502 - acc: 0.9189 - val_loss: 1.6830 - val_acc: 0.6455\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2528 - acc: 0.9174\n",
      "Epoch 00043: val_loss did not improve from 1.23846\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.2528 - acc: 0.9174 - val_loss: 1.2587 - val_acc: 0.7123\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2383 - acc: 0.9222\n",
      "Epoch 00044: val_loss did not improve from 1.23846\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.2384 - acc: 0.9222 - val_loss: 1.9494 - val_acc: 0.5970\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2352 - acc: 0.9233\n",
      "Epoch 00045: val_loss did not improve from 1.23846\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.2352 - acc: 0.9233 - val_loss: 1.3444 - val_acc: 0.6951\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2265 - acc: 0.9261\n",
      "Epoch 00046: val_loss did not improve from 1.23846\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.2267 - acc: 0.9260 - val_loss: 1.5057 - val_acc: 0.6676\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2277 - acc: 0.9265\n",
      "Epoch 00047: val_loss did not improve from 1.23846\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.2278 - acc: 0.9264 - val_loss: 1.4307 - val_acc: 0.6853\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2211 - acc: 0.9268\n",
      "Epoch 00048: val_loss did not improve from 1.23846\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.2211 - acc: 0.9267 - val_loss: 1.3325 - val_acc: 0.7121\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2199 - acc: 0.9292\n",
      "Epoch 00049: val_loss did not improve from 1.23846\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.2199 - acc: 0.9292 - val_loss: 1.2611 - val_acc: 0.7261\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1997 - acc: 0.9345\n",
      "Epoch 00050: val_loss did not improve from 1.23846\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.1997 - acc: 0.9345 - val_loss: 1.3812 - val_acc: 0.6983\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2051 - acc: 0.9341\n",
      "Epoch 00051: val_loss did not improve from 1.23846\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.2051 - acc: 0.9341 - val_loss: 1.3329 - val_acc: 0.7037\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2055 - acc: 0.9336\n",
      "Epoch 00052: val_loss did not improve from 1.23846\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.2055 - acc: 0.9336 - val_loss: 1.3314 - val_acc: 0.7060\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1862 - acc: 0.9393\n",
      "Epoch 00053: val_loss did not improve from 1.23846\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.1862 - acc: 0.9393 - val_loss: 1.3809 - val_acc: 0.7023\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1904 - acc: 0.9379\n",
      "Epoch 00054: val_loss did not improve from 1.23846\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.1905 - acc: 0.9379 - val_loss: 1.4079 - val_acc: 0.6962\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1904 - acc: 0.9389\n",
      "Epoch 00055: val_loss did not improve from 1.23846\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.1906 - acc: 0.9389 - val_loss: 1.3354 - val_acc: 0.6988\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1927 - acc: 0.9389\n",
      "Epoch 00056: val_loss did not improve from 1.23846\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.1927 - acc: 0.9389 - val_loss: 1.3530 - val_acc: 0.7037\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1784 - acc: 0.9433\n",
      "Epoch 00057: val_loss did not improve from 1.23846\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.1784 - acc: 0.9433 - val_loss: 2.3371 - val_acc: 0.5553\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1780 - acc: 0.9431\n",
      "Epoch 00058: val_loss did not improve from 1.23846\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.1782 - acc: 0.9431 - val_loss: 1.6897 - val_acc: 0.6478\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1851 - acc: 0.9417\n",
      "Epoch 00059: val_loss did not improve from 1.23846\n",
      "36805/36805 [==============================] - 81s 2ms/sample - loss: 0.1851 - acc: 0.9417 - val_loss: 1.4714 - val_acc: 0.6727\n",
      "\n",
      "1D_CNN_custom_DO_075_DO_BN_3_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 812us/sample - loss: 1.3749 - acc: 0.5931\n",
      "Loss: 1.3749287855092858 Accuracy: 0.59314644\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.8300 - acc: 0.2625\n",
      "Epoch 00001: val_loss improved from inf to 1.68567, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_BN_4_conv_checkpoint/001-1.6857.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 2.8302 - acc: 0.2625 - val_loss: 1.6857 - val_acc: 0.4456\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.8659 - acc: 0.4444\n",
      "Epoch 00002: val_loss improved from 1.68567 to 1.24365, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_BN_4_conv_checkpoint/002-1.2436.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 1.8657 - acc: 0.4445 - val_loss: 1.2436 - val_acc: 0.6287\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5386 - acc: 0.5320\n",
      "Epoch 00003: val_loss improved from 1.24365 to 1.15055, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_BN_4_conv_checkpoint/003-1.1505.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 1.5387 - acc: 0.5319 - val_loss: 1.1505 - val_acc: 0.6543\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3413 - acc: 0.5896\n",
      "Epoch 00004: val_loss improved from 1.15055 to 1.06061, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_BN_4_conv_checkpoint/004-1.0606.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 1.3413 - acc: 0.5896 - val_loss: 1.0606 - val_acc: 0.6676\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1950 - acc: 0.6330\n",
      "Epoch 00005: val_loss improved from 1.06061 to 1.05294, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_BN_4_conv_checkpoint/005-1.0529.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 1.1952 - acc: 0.6330 - val_loss: 1.0529 - val_acc: 0.6811\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1167 - acc: 0.6577\n",
      "Epoch 00006: val_loss improved from 1.05294 to 0.91001, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_BN_4_conv_checkpoint/006-0.9100.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 1.1167 - acc: 0.6577 - val_loss: 0.9100 - val_acc: 0.7340\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0334 - acc: 0.6820\n",
      "Epoch 00007: val_loss improved from 0.91001 to 0.89330, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_BN_4_conv_checkpoint/007-0.8933.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 1.0335 - acc: 0.6819 - val_loss: 0.8933 - val_acc: 0.7417\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9811 - acc: 0.6973\n",
      "Epoch 00008: val_loss did not improve from 0.89330\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.9812 - acc: 0.6973 - val_loss: 0.9894 - val_acc: 0.7098\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9209 - acc: 0.7151\n",
      "Epoch 00009: val_loss improved from 0.89330 to 0.86320, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_BN_4_conv_checkpoint/009-0.8632.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.9208 - acc: 0.7151 - val_loss: 0.8632 - val_acc: 0.7447\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8788 - acc: 0.7281\n",
      "Epoch 00010: val_loss improved from 0.86320 to 0.86103, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_BN_4_conv_checkpoint/010-0.8610.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.8793 - acc: 0.7280 - val_loss: 0.8610 - val_acc: 0.7466\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8476 - acc: 0.7392\n",
      "Epoch 00011: val_loss improved from 0.86103 to 0.82698, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_BN_4_conv_checkpoint/011-0.8270.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.8476 - acc: 0.7392 - val_loss: 0.8270 - val_acc: 0.7543\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7976 - acc: 0.7532\n",
      "Epoch 00012: val_loss did not improve from 0.82698\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.7975 - acc: 0.7532 - val_loss: 0.8417 - val_acc: 0.7494\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7778 - acc: 0.7592\n",
      "Epoch 00013: val_loss did not improve from 0.82698\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.7779 - acc: 0.7592 - val_loss: 0.8963 - val_acc: 0.7240\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7485 - acc: 0.7665\n",
      "Epoch 00014: val_loss improved from 0.82698 to 0.81080, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_BN_4_conv_checkpoint/014-0.8108.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.7486 - acc: 0.7665 - val_loss: 0.8108 - val_acc: 0.7619\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7244 - acc: 0.7776\n",
      "Epoch 00015: val_loss did not improve from 0.81080\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.7244 - acc: 0.7776 - val_loss: 0.9161 - val_acc: 0.7358\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6974 - acc: 0.7835\n",
      "Epoch 00016: val_loss did not improve from 0.81080\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.6974 - acc: 0.7835 - val_loss: 0.8534 - val_acc: 0.7491\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6720 - acc: 0.7917\n",
      "Epoch 00017: val_loss did not improve from 0.81080\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.6719 - acc: 0.7916 - val_loss: 0.8283 - val_acc: 0.7605\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6476 - acc: 0.7964\n",
      "Epoch 00018: val_loss did not improve from 0.81080\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.6478 - acc: 0.7964 - val_loss: 0.8181 - val_acc: 0.7568\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6257 - acc: 0.8046\n",
      "Epoch 00019: val_loss improved from 0.81080 to 0.77560, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_BN_4_conv_checkpoint/019-0.7756.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.6257 - acc: 0.8046 - val_loss: 0.7756 - val_acc: 0.7780\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6175 - acc: 0.8049\n",
      "Epoch 00020: val_loss did not improve from 0.77560\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.6175 - acc: 0.8049 - val_loss: 0.8026 - val_acc: 0.7724\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5962 - acc: 0.8104\n",
      "Epoch 00021: val_loss improved from 0.77560 to 0.73444, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_BN_4_conv_checkpoint/021-0.7344.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.5962 - acc: 0.8104 - val_loss: 0.7344 - val_acc: 0.7962\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5842 - acc: 0.8165\n",
      "Epoch 00022: val_loss did not improve from 0.73444\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.5841 - acc: 0.8165 - val_loss: 0.7353 - val_acc: 0.7955\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5618 - acc: 0.8211\n",
      "Epoch 00023: val_loss did not improve from 0.73444\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.5618 - acc: 0.8211 - val_loss: 0.8199 - val_acc: 0.7689\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5440 - acc: 0.8299\n",
      "Epoch 00024: val_loss did not improve from 0.73444\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.5439 - acc: 0.8299 - val_loss: 0.9323 - val_acc: 0.7489\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5347 - acc: 0.8305\n",
      "Epoch 00025: val_loss did not improve from 0.73444\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.5347 - acc: 0.8304 - val_loss: 0.7629 - val_acc: 0.7915\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5240 - acc: 0.8340\n",
      "Epoch 00026: val_loss did not improve from 0.73444\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.5241 - acc: 0.8340 - val_loss: 0.7520 - val_acc: 0.7887\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5111 - acc: 0.8376\n",
      "Epoch 00027: val_loss did not improve from 0.73444\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.5116 - acc: 0.8375 - val_loss: 0.7459 - val_acc: 0.7864\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5006 - acc: 0.8405\n",
      "Epoch 00028: val_loss did not improve from 0.73444\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.5006 - acc: 0.8405 - val_loss: 0.8061 - val_acc: 0.7761\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4860 - acc: 0.8466\n",
      "Epoch 00029: val_loss did not improve from 0.73444\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.4861 - acc: 0.8465 - val_loss: 0.7679 - val_acc: 0.7878\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4747 - acc: 0.8483\n",
      "Epoch 00030: val_loss did not improve from 0.73444\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.4746 - acc: 0.8483 - val_loss: 0.8995 - val_acc: 0.7566\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4625 - acc: 0.8520\n",
      "Epoch 00031: val_loss did not improve from 0.73444\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.4625 - acc: 0.8520 - val_loss: 0.7772 - val_acc: 0.7836\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4515 - acc: 0.8566\n",
      "Epoch 00032: val_loss improved from 0.73444 to 0.69356, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_BN_4_conv_checkpoint/032-0.6936.hdf5\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.4514 - acc: 0.8566 - val_loss: 0.6936 - val_acc: 0.8111\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4461 - acc: 0.8568\n",
      "Epoch 00033: val_loss did not improve from 0.69356\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.4461 - acc: 0.8568 - val_loss: 0.7517 - val_acc: 0.7918\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4344 - acc: 0.8617\n",
      "Epoch 00034: val_loss did not improve from 0.69356\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.4347 - acc: 0.8617 - val_loss: 0.7563 - val_acc: 0.7964\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4260 - acc: 0.8635\n",
      "Epoch 00035: val_loss did not improve from 0.69356\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.4260 - acc: 0.8636 - val_loss: 0.7437 - val_acc: 0.8006\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4159 - acc: 0.8651\n",
      "Epoch 00036: val_loss did not improve from 0.69356\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.4160 - acc: 0.8651 - val_loss: 0.7688 - val_acc: 0.7906\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4062 - acc: 0.8702\n",
      "Epoch 00037: val_loss did not improve from 0.69356\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.4061 - acc: 0.8703 - val_loss: 0.7521 - val_acc: 0.8020\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3937 - acc: 0.8717\n",
      "Epoch 00038: val_loss did not improve from 0.69356\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.3939 - acc: 0.8716 - val_loss: 0.6968 - val_acc: 0.8106\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3917 - acc: 0.8730\n",
      "Epoch 00039: val_loss did not improve from 0.69356\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.3919 - acc: 0.8730 - val_loss: 0.7712 - val_acc: 0.7890\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3913 - acc: 0.8699\n",
      "Epoch 00040: val_loss did not improve from 0.69356\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.3915 - acc: 0.8699 - val_loss: 0.7158 - val_acc: 0.8055\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3803 - acc: 0.8759\n",
      "Epoch 00041: val_loss did not improve from 0.69356\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.3803 - acc: 0.8759 - val_loss: 0.7942 - val_acc: 0.7878\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3719 - acc: 0.8801\n",
      "Epoch 00042: val_loss did not improve from 0.69356\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.3719 - acc: 0.8801 - val_loss: 0.7065 - val_acc: 0.8118\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3637 - acc: 0.8821\n",
      "Epoch 00043: val_loss did not improve from 0.69356\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.3637 - acc: 0.8821 - val_loss: 0.8187 - val_acc: 0.7747\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3595 - acc: 0.8822\n",
      "Epoch 00044: val_loss did not improve from 0.69356\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.3596 - acc: 0.8822 - val_loss: 0.7094 - val_acc: 0.8123\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3520 - acc: 0.8854\n",
      "Epoch 00045: val_loss did not improve from 0.69356\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.3520 - acc: 0.8853 - val_loss: 0.7878 - val_acc: 0.7878\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3472 - acc: 0.8876\n",
      "Epoch 00046: val_loss did not improve from 0.69356\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.3471 - acc: 0.8876 - val_loss: 0.7115 - val_acc: 0.8106\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3390 - acc: 0.8881\n",
      "Epoch 00047: val_loss did not improve from 0.69356\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.3390 - acc: 0.8881 - val_loss: 0.7321 - val_acc: 0.8057\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3335 - acc: 0.8920\n",
      "Epoch 00048: val_loss did not improve from 0.69356\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.3335 - acc: 0.8920 - val_loss: 1.0087 - val_acc: 0.7345\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3289 - acc: 0.8938\n",
      "Epoch 00049: val_loss did not improve from 0.69356\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.3290 - acc: 0.8938 - val_loss: 0.7734 - val_acc: 0.7966\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3179 - acc: 0.8937\n",
      "Epoch 00050: val_loss did not improve from 0.69356\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.3179 - acc: 0.8937 - val_loss: 0.7194 - val_acc: 0.8104\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3181 - acc: 0.8960\n",
      "Epoch 00051: val_loss did not improve from 0.69356\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.3181 - acc: 0.8960 - val_loss: 0.6976 - val_acc: 0.8183\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3135 - acc: 0.8983\n",
      "Epoch 00052: val_loss did not improve from 0.69356\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.3135 - acc: 0.8983 - val_loss: 0.7547 - val_acc: 0.8071\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3062 - acc: 0.8985\n",
      "Epoch 00053: val_loss did not improve from 0.69356\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.3062 - acc: 0.8985 - val_loss: 0.7108 - val_acc: 0.8123\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3023 - acc: 0.9014\n",
      "Epoch 00054: val_loss did not improve from 0.69356\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.3023 - acc: 0.9014 - val_loss: 0.7775 - val_acc: 0.8076\n",
      "Epoch 55/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2942 - acc: 0.9030\n",
      "Epoch 00055: val_loss did not improve from 0.69356\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2943 - acc: 0.9030 - val_loss: 0.7042 - val_acc: 0.8141\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2952 - acc: 0.9041\n",
      "Epoch 00056: val_loss did not improve from 0.69356\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2951 - acc: 0.9041 - val_loss: 0.7273 - val_acc: 0.8074\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2886 - acc: 0.9053\n",
      "Epoch 00057: val_loss did not improve from 0.69356\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2886 - acc: 0.9053 - val_loss: 0.7534 - val_acc: 0.8029\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2916 - acc: 0.9035\n",
      "Epoch 00058: val_loss did not improve from 0.69356\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2916 - acc: 0.9035 - val_loss: 0.7111 - val_acc: 0.8202\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2855 - acc: 0.9054\n",
      "Epoch 00059: val_loss did not improve from 0.69356\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2854 - acc: 0.9054 - val_loss: 0.7100 - val_acc: 0.8123\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2772 - acc: 0.9090\n",
      "Epoch 00060: val_loss improved from 0.69356 to 0.68783, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_BN_4_conv_checkpoint/060-0.6878.hdf5\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2772 - acc: 0.9090 - val_loss: 0.6878 - val_acc: 0.8239\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2751 - acc: 0.9096\n",
      "Epoch 00061: val_loss did not improve from 0.68783\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2751 - acc: 0.9096 - val_loss: 0.7349 - val_acc: 0.8130\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2762 - acc: 0.9101\n",
      "Epoch 00062: val_loss did not improve from 0.68783\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2763 - acc: 0.9101 - val_loss: 0.7312 - val_acc: 0.8104\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2632 - acc: 0.9133\n",
      "Epoch 00063: val_loss did not improve from 0.68783\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2631 - acc: 0.9133 - val_loss: 0.7297 - val_acc: 0.8120\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2614 - acc: 0.9127\n",
      "Epoch 00064: val_loss did not improve from 0.68783\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2614 - acc: 0.9127 - val_loss: 0.7150 - val_acc: 0.8216\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2592 - acc: 0.9171\n",
      "Epoch 00065: val_loss did not improve from 0.68783\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2592 - acc: 0.9171 - val_loss: 0.7436 - val_acc: 0.8076\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2596 - acc: 0.9158\n",
      "Epoch 00066: val_loss did not improve from 0.68783\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2596 - acc: 0.9158 - val_loss: 0.9487 - val_acc: 0.7573\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2559 - acc: 0.9162\n",
      "Epoch 00067: val_loss did not improve from 0.68783\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2560 - acc: 0.9162 - val_loss: 0.8496 - val_acc: 0.7883\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2514 - acc: 0.9196\n",
      "Epoch 00068: val_loss did not improve from 0.68783\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2514 - acc: 0.9195 - val_loss: 0.7189 - val_acc: 0.8148\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2524 - acc: 0.9183\n",
      "Epoch 00069: val_loss did not improve from 0.68783\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2523 - acc: 0.9183 - val_loss: 0.7828 - val_acc: 0.8025\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2450 - acc: 0.9196\n",
      "Epoch 00070: val_loss did not improve from 0.68783\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2450 - acc: 0.9196 - val_loss: 0.8036 - val_acc: 0.8015\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2429 - acc: 0.9210\n",
      "Epoch 00071: val_loss did not improve from 0.68783\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2429 - acc: 0.9209 - val_loss: 0.7169 - val_acc: 0.8209\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2402 - acc: 0.9218\n",
      "Epoch 00072: val_loss did not improve from 0.68783\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2402 - acc: 0.9217 - val_loss: 0.7531 - val_acc: 0.8064\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2405 - acc: 0.9212\n",
      "Epoch 00073: val_loss did not improve from 0.68783\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2407 - acc: 0.9211 - val_loss: 0.7055 - val_acc: 0.8164\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2334 - acc: 0.9231\n",
      "Epoch 00074: val_loss did not improve from 0.68783\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2333 - acc: 0.9231 - val_loss: 0.7836 - val_acc: 0.8062\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2321 - acc: 0.9236\n",
      "Epoch 00075: val_loss did not improve from 0.68783\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2324 - acc: 0.9236 - val_loss: 0.7795 - val_acc: 0.8046\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2333 - acc: 0.9250\n",
      "Epoch 00076: val_loss did not improve from 0.68783\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2333 - acc: 0.9250 - val_loss: 0.9043 - val_acc: 0.7803\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2213 - acc: 0.9287\n",
      "Epoch 00077: val_loss did not improve from 0.68783\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2213 - acc: 0.9288 - val_loss: 0.7699 - val_acc: 0.8022\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2250 - acc: 0.9258\n",
      "Epoch 00078: val_loss did not improve from 0.68783\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2250 - acc: 0.9258 - val_loss: 1.0937 - val_acc: 0.7391\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2244 - acc: 0.9253\n",
      "Epoch 00079: val_loss did not improve from 0.68783\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2244 - acc: 0.9253 - val_loss: 0.7540 - val_acc: 0.8111\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2160 - acc: 0.9297\n",
      "Epoch 00080: val_loss did not improve from 0.68783\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2160 - acc: 0.9297 - val_loss: 0.7881 - val_acc: 0.8095\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2215 - acc: 0.9278\n",
      "Epoch 00081: val_loss did not improve from 0.68783\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2215 - acc: 0.9278 - val_loss: 0.7862 - val_acc: 0.8041\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2123 - acc: 0.9321\n",
      "Epoch 00082: val_loss did not improve from 0.68783\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2123 - acc: 0.9321 - val_loss: 0.7851 - val_acc: 0.8095\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2135 - acc: 0.9298\n",
      "Epoch 00083: val_loss did not improve from 0.68783\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2134 - acc: 0.9298 - val_loss: 0.7110 - val_acc: 0.8206\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2083 - acc: 0.9315\n",
      "Epoch 00084: val_loss did not improve from 0.68783\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2084 - acc: 0.9315 - val_loss: 0.7520 - val_acc: 0.8113\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2066 - acc: 0.9323\n",
      "Epoch 00085: val_loss did not improve from 0.68783\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.2067 - acc: 0.9322 - val_loss: 0.7949 - val_acc: 0.8097\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2060 - acc: 0.9337\n",
      "Epoch 00086: val_loss did not improve from 0.68783\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2061 - acc: 0.9337 - val_loss: 0.6945 - val_acc: 0.8265\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2075 - acc: 0.9310\n",
      "Epoch 00087: val_loss did not improve from 0.68783\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2075 - acc: 0.9310 - val_loss: 0.7476 - val_acc: 0.8213\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2033 - acc: 0.9354\n",
      "Epoch 00088: val_loss did not improve from 0.68783\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2033 - acc: 0.9354 - val_loss: 0.7142 - val_acc: 0.8234\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2009 - acc: 0.9332\n",
      "Epoch 00089: val_loss did not improve from 0.68783\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2009 - acc: 0.9332 - val_loss: 0.7466 - val_acc: 0.8183\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2026 - acc: 0.9345\n",
      "Epoch 00090: val_loss did not improve from 0.68783\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.2026 - acc: 0.9345 - val_loss: 0.7163 - val_acc: 0.8269\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1954 - acc: 0.9373\n",
      "Epoch 00091: val_loss did not improve from 0.68783\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.1954 - acc: 0.9373 - val_loss: 0.7622 - val_acc: 0.8223\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1962 - acc: 0.9357\n",
      "Epoch 00092: val_loss did not improve from 0.68783\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.1962 - acc: 0.9357 - val_loss: 0.7619 - val_acc: 0.8202\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1938 - acc: 0.9374\n",
      "Epoch 00093: val_loss did not improve from 0.68783\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1937 - acc: 0.9375 - val_loss: 0.7840 - val_acc: 0.8099\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1892 - acc: 0.9383\n",
      "Epoch 00094: val_loss did not improve from 0.68783\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.1892 - acc: 0.9384 - val_loss: 0.7728 - val_acc: 0.8164\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1901 - acc: 0.9377\n",
      "Epoch 00095: val_loss did not improve from 0.68783\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.1903 - acc: 0.9376 - val_loss: 0.7318 - val_acc: 0.8216\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1977 - acc: 0.9362\n",
      "Epoch 00096: val_loss did not improve from 0.68783\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.1976 - acc: 0.9362 - val_loss: 0.7436 - val_acc: 0.8234\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1823 - acc: 0.9415\n",
      "Epoch 00097: val_loss did not improve from 0.68783\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.1823 - acc: 0.9414 - val_loss: 0.8160 - val_acc: 0.8060\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1861 - acc: 0.9398\n",
      "Epoch 00098: val_loss did not improve from 0.68783\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.1861 - acc: 0.9398 - val_loss: 0.7534 - val_acc: 0.8230\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1853 - acc: 0.9386\n",
      "Epoch 00099: val_loss did not improve from 0.68783\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.1854 - acc: 0.9386 - val_loss: 0.7037 - val_acc: 0.8295\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1832 - acc: 0.9405\n",
      "Epoch 00100: val_loss did not improve from 0.68783\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.1832 - acc: 0.9405 - val_loss: 0.7959 - val_acc: 0.8190\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1785 - acc: 0.9423\n",
      "Epoch 00101: val_loss did not improve from 0.68783\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.1786 - acc: 0.9422 - val_loss: 0.7952 - val_acc: 0.8060\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1849 - acc: 0.9401\n",
      "Epoch 00102: val_loss did not improve from 0.68783\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1848 - acc: 0.9401 - val_loss: 0.7462 - val_acc: 0.8143\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1766 - acc: 0.9426\n",
      "Epoch 00103: val_loss did not improve from 0.68783\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.1766 - acc: 0.9425 - val_loss: 0.8148 - val_acc: 0.8097\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1810 - acc: 0.9402\n",
      "Epoch 00104: val_loss did not improve from 0.68783\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.1811 - acc: 0.9402 - val_loss: 0.7423 - val_acc: 0.8155\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1768 - acc: 0.9411\n",
      "Epoch 00105: val_loss did not improve from 0.68783\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.1768 - acc: 0.9410 - val_loss: 0.7592 - val_acc: 0.8204\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1773 - acc: 0.9419\n",
      "Epoch 00106: val_loss did not improve from 0.68783\n",
      "36805/36805 [==============================] - 84s 2ms/sample - loss: 0.1773 - acc: 0.9419 - val_loss: 0.7302 - val_acc: 0.8332\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1751 - acc: 0.9430\n",
      "Epoch 00107: val_loss did not improve from 0.68783\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.1751 - acc: 0.9430 - val_loss: 0.7301 - val_acc: 0.8348\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1715 - acc: 0.9443\n",
      "Epoch 00108: val_loss did not improve from 0.68783\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.1715 - acc: 0.9443 - val_loss: 0.8500 - val_acc: 0.8029\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1740 - acc: 0.9438\n",
      "Epoch 00109: val_loss did not improve from 0.68783\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.1739 - acc: 0.9438 - val_loss: 0.7783 - val_acc: 0.8223\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1678 - acc: 0.9465\n",
      "Epoch 00110: val_loss did not improve from 0.68783\n",
      "36805/36805 [==============================] - 85s 2ms/sample - loss: 0.1678 - acc: 0.9465 - val_loss: 0.7382 - val_acc: 0.8262\n",
      "\n",
      "1D_CNN_custom_DO_075_DO_BN_4_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 875us/sample - loss: 0.8049 - acc: 0.7769\n",
      "Loss: 0.804949192441265 Accuracy: 0.776947\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.8651 - acc: 0.2566\n",
      "Epoch 00001: val_loss improved from inf to 1.74870, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_BN_5_conv_checkpoint/001-1.7487.hdf5\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 2.8652 - acc: 0.2566 - val_loss: 1.7487 - val_acc: 0.4065\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.7746 - acc: 0.4686\n",
      "Epoch 00002: val_loss improved from 1.74870 to 1.17821, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_BN_5_conv_checkpoint/002-1.1782.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 1.7746 - acc: 0.4686 - val_loss: 1.1782 - val_acc: 0.6296\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4338 - acc: 0.5638\n",
      "Epoch 00003: val_loss improved from 1.17821 to 1.06763, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_BN_5_conv_checkpoint/003-1.0676.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 1.4338 - acc: 0.5638 - val_loss: 1.0676 - val_acc: 0.6734\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2329 - acc: 0.6242\n",
      "Epoch 00004: val_loss improved from 1.06763 to 0.97361, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_BN_5_conv_checkpoint/004-0.9736.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 1.2328 - acc: 0.6242 - val_loss: 0.9736 - val_acc: 0.7098\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1117 - acc: 0.6611\n",
      "Epoch 00005: val_loss improved from 0.97361 to 0.93947, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_BN_5_conv_checkpoint/005-0.9395.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 1.1119 - acc: 0.6611 - val_loss: 0.9395 - val_acc: 0.7170\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0174 - acc: 0.6910\n",
      "Epoch 00006: val_loss improved from 0.93947 to 0.83145, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_BN_5_conv_checkpoint/006-0.8315.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 1.0174 - acc: 0.6910 - val_loss: 0.8315 - val_acc: 0.7771\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9463 - acc: 0.7126\n",
      "Epoch 00007: val_loss improved from 0.83145 to 0.79169, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_BN_5_conv_checkpoint/007-0.7917.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.9465 - acc: 0.7126 - val_loss: 0.7917 - val_acc: 0.7666\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8871 - acc: 0.7290\n",
      "Epoch 00008: val_loss improved from 0.79169 to 0.74778, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_BN_5_conv_checkpoint/008-0.7478.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.8871 - acc: 0.7289 - val_loss: 0.7478 - val_acc: 0.7831\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8409 - acc: 0.7444\n",
      "Epoch 00009: val_loss did not improve from 0.74778\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.8409 - acc: 0.7444 - val_loss: 0.7758 - val_acc: 0.7673\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7986 - acc: 0.7556\n",
      "Epoch 00010: val_loss did not improve from 0.74778\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.7986 - acc: 0.7556 - val_loss: 0.8225 - val_acc: 0.7587\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7656 - acc: 0.7660\n",
      "Epoch 00011: val_loss did not improve from 0.74778\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.7655 - acc: 0.7660 - val_loss: 0.8724 - val_acc: 0.7449\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7353 - acc: 0.7748\n",
      "Epoch 00012: val_loss improved from 0.74778 to 0.66300, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_BN_5_conv_checkpoint/012-0.6630.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.7354 - acc: 0.7748 - val_loss: 0.6630 - val_acc: 0.8167\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7091 - acc: 0.7830\n",
      "Epoch 00013: val_loss improved from 0.66300 to 0.64354, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_BN_5_conv_checkpoint/013-0.6435.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.7091 - acc: 0.7830 - val_loss: 0.6435 - val_acc: 0.8174\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6736 - acc: 0.7950\n",
      "Epoch 00014: val_loss did not improve from 0.64354\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.6735 - acc: 0.7950 - val_loss: 0.7067 - val_acc: 0.7876\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6533 - acc: 0.8005\n",
      "Epoch 00015: val_loss improved from 0.64354 to 0.63133, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_BN_5_conv_checkpoint/015-0.6313.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.6533 - acc: 0.8005 - val_loss: 0.6313 - val_acc: 0.8244\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6308 - acc: 0.8058\n",
      "Epoch 00016: val_loss did not improve from 0.63133\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.6309 - acc: 0.8058 - val_loss: 0.7886 - val_acc: 0.7645\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6254 - acc: 0.8093\n",
      "Epoch 00017: val_loss improved from 0.63133 to 0.61256, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_BN_5_conv_checkpoint/017-0.6126.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.6253 - acc: 0.8093 - val_loss: 0.6126 - val_acc: 0.8197\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5980 - acc: 0.8164\n",
      "Epoch 00018: val_loss did not improve from 0.61256\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.5981 - acc: 0.8163 - val_loss: 0.6174 - val_acc: 0.8230\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5783 - acc: 0.8231\n",
      "Epoch 00019: val_loss improved from 0.61256 to 0.61028, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_BN_5_conv_checkpoint/019-0.6103.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.5783 - acc: 0.8231 - val_loss: 0.6103 - val_acc: 0.8237\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5636 - acc: 0.8276\n",
      "Epoch 00020: val_loss did not improve from 0.61028\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.5638 - acc: 0.8276 - val_loss: 0.6225 - val_acc: 0.8190\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5505 - acc: 0.8299\n",
      "Epoch 00021: val_loss improved from 0.61028 to 0.58950, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_BN_5_conv_checkpoint/021-0.5895.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.5505 - acc: 0.8299 - val_loss: 0.5895 - val_acc: 0.8379\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5355 - acc: 0.8336\n",
      "Epoch 00022: val_loss improved from 0.58950 to 0.58081, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_BN_5_conv_checkpoint/022-0.5808.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.5356 - acc: 0.8336 - val_loss: 0.5808 - val_acc: 0.8348\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5237 - acc: 0.8380\n",
      "Epoch 00023: val_loss did not improve from 0.58081\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.5238 - acc: 0.8380 - val_loss: 0.5823 - val_acc: 0.8325\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5118 - acc: 0.8423\n",
      "Epoch 00024: val_loss did not improve from 0.58081\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.5122 - acc: 0.8423 - val_loss: 0.7013 - val_acc: 0.7969\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4943 - acc: 0.8485\n",
      "Epoch 00025: val_loss improved from 0.58081 to 0.54793, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_BN_5_conv_checkpoint/025-0.5479.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.4943 - acc: 0.8485 - val_loss: 0.5479 - val_acc: 0.8400\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4786 - acc: 0.8533\n",
      "Epoch 00026: val_loss did not improve from 0.54793\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.4788 - acc: 0.8533 - val_loss: 0.5971 - val_acc: 0.8286\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4788 - acc: 0.8498\n",
      "Epoch 00027: val_loss did not improve from 0.54793\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.4789 - acc: 0.8498 - val_loss: 0.6187 - val_acc: 0.8202\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4616 - acc: 0.8574\n",
      "Epoch 00028: val_loss did not improve from 0.54793\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.4616 - acc: 0.8573 - val_loss: 0.6071 - val_acc: 0.8307\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4543 - acc: 0.8602\n",
      "Epoch 00029: val_loss did not improve from 0.54793\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.4543 - acc: 0.8602 - val_loss: 0.6363 - val_acc: 0.8209\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4393 - acc: 0.8609\n",
      "Epoch 00030: val_loss improved from 0.54793 to 0.54098, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_BN_5_conv_checkpoint/030-0.5410.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.4393 - acc: 0.8609 - val_loss: 0.5410 - val_acc: 0.8498\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4265 - acc: 0.8668\n",
      "Epoch 00031: val_loss did not improve from 0.54098\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.4265 - acc: 0.8668 - val_loss: 0.5736 - val_acc: 0.8430\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4209 - acc: 0.8679\n",
      "Epoch 00032: val_loss did not improve from 0.54098\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.4210 - acc: 0.8678 - val_loss: 0.5953 - val_acc: 0.8421\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4174 - acc: 0.8699\n",
      "Epoch 00033: val_loss did not improve from 0.54098\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.4174 - acc: 0.8699 - val_loss: 0.5658 - val_acc: 0.8465\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4096 - acc: 0.8720\n",
      "Epoch 00034: val_loss did not improve from 0.54098\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.4096 - acc: 0.8720 - val_loss: 0.6158 - val_acc: 0.8286\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3928 - acc: 0.8765\n",
      "Epoch 00035: val_loss did not improve from 0.54098\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.3929 - acc: 0.8765 - val_loss: 0.5543 - val_acc: 0.8470\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3894 - acc: 0.8793\n",
      "Epoch 00036: val_loss did not improve from 0.54098\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.3894 - acc: 0.8793 - val_loss: 0.5798 - val_acc: 0.8446\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3845 - acc: 0.8802\n",
      "Epoch 00037: val_loss improved from 0.54098 to 0.52705, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_BN_5_conv_checkpoint/037-0.5271.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.3847 - acc: 0.8802 - val_loss: 0.5271 - val_acc: 0.8523\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3781 - acc: 0.8793\n",
      "Epoch 00038: val_loss did not improve from 0.52705\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.3782 - acc: 0.8793 - val_loss: 0.5687 - val_acc: 0.8397\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3693 - acc: 0.8840\n",
      "Epoch 00039: val_loss improved from 0.52705 to 0.52640, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_BN_5_conv_checkpoint/039-0.5264.hdf5\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.3693 - acc: 0.8840 - val_loss: 0.5264 - val_acc: 0.8579\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3544 - acc: 0.8893\n",
      "Epoch 00040: val_loss did not improve from 0.52640\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.3544 - acc: 0.8893 - val_loss: 0.5382 - val_acc: 0.8505\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3504 - acc: 0.8887\n",
      "Epoch 00041: val_loss improved from 0.52640 to 0.51960, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_BN_5_conv_checkpoint/041-0.5196.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.3503 - acc: 0.8887 - val_loss: 0.5196 - val_acc: 0.8626\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3423 - acc: 0.8908\n",
      "Epoch 00042: val_loss did not improve from 0.51960\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.3422 - acc: 0.8907 - val_loss: 0.6273 - val_acc: 0.8295\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3446 - acc: 0.8900\n",
      "Epoch 00043: val_loss did not improve from 0.51960\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.3445 - acc: 0.8900 - val_loss: 0.5504 - val_acc: 0.8556\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3329 - acc: 0.8933\n",
      "Epoch 00044: val_loss improved from 0.51960 to 0.50333, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_BN_5_conv_checkpoint/044-0.5033.hdf5\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.3329 - acc: 0.8933 - val_loss: 0.5033 - val_acc: 0.8628\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3226 - acc: 0.8980\n",
      "Epoch 00045: val_loss did not improve from 0.50333\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.3231 - acc: 0.8979 - val_loss: 0.5166 - val_acc: 0.8563\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3209 - acc: 0.8980\n",
      "Epoch 00046: val_loss did not improve from 0.50333\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.3212 - acc: 0.8979 - val_loss: 0.5314 - val_acc: 0.8544\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3288 - acc: 0.8945\n",
      "Epoch 00047: val_loss did not improve from 0.50333\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.3288 - acc: 0.8945 - val_loss: 0.5486 - val_acc: 0.8549\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3112 - acc: 0.9018\n",
      "Epoch 00048: val_loss did not improve from 0.50333\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.3112 - acc: 0.9018 - val_loss: 0.5098 - val_acc: 0.8658\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3062 - acc: 0.9036\n",
      "Epoch 00049: val_loss did not improve from 0.50333\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.3062 - acc: 0.9036 - val_loss: 0.5038 - val_acc: 0.8661\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2963 - acc: 0.9041\n",
      "Epoch 00050: val_loss did not improve from 0.50333\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2964 - acc: 0.9041 - val_loss: 0.5542 - val_acc: 0.8519\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2966 - acc: 0.9062\n",
      "Epoch 00051: val_loss did not improve from 0.50333\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2967 - acc: 0.9061 - val_loss: 0.6012 - val_acc: 0.8421\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2880 - acc: 0.9058\n",
      "Epoch 00052: val_loss did not improve from 0.50333\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2880 - acc: 0.9057 - val_loss: 0.5168 - val_acc: 0.8581\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2886 - acc: 0.9062\n",
      "Epoch 00053: val_loss did not improve from 0.50333\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2885 - acc: 0.9063 - val_loss: 0.5699 - val_acc: 0.8523\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2816 - acc: 0.9083\n",
      "Epoch 00054: val_loss did not improve from 0.50333\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2816 - acc: 0.9084 - val_loss: 0.5382 - val_acc: 0.8556\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2754 - acc: 0.9119\n",
      "Epoch 00055: val_loss did not improve from 0.50333\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2754 - acc: 0.9119 - val_loss: 0.5212 - val_acc: 0.8556\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2735 - acc: 0.9117\n",
      "Epoch 00056: val_loss did not improve from 0.50333\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2737 - acc: 0.9117 - val_loss: 0.5391 - val_acc: 0.8616\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2723 - acc: 0.9123\n",
      "Epoch 00057: val_loss did not improve from 0.50333\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2723 - acc: 0.9123 - val_loss: 0.6045 - val_acc: 0.8479\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2659 - acc: 0.9142\n",
      "Epoch 00058: val_loss did not improve from 0.50333\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2658 - acc: 0.9142 - val_loss: 0.5379 - val_acc: 0.8563\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2602 - acc: 0.9167\n",
      "Epoch 00059: val_loss did not improve from 0.50333\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2602 - acc: 0.9167 - val_loss: 0.5094 - val_acc: 0.8691\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2559 - acc: 0.9195\n",
      "Epoch 00060: val_loss did not improve from 0.50333\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2560 - acc: 0.9194 - val_loss: 0.6208 - val_acc: 0.8381\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2580 - acc: 0.9183\n",
      "Epoch 00061: val_loss did not improve from 0.50333\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2582 - acc: 0.9183 - val_loss: 0.5191 - val_acc: 0.8621\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2491 - acc: 0.9204\n",
      "Epoch 00062: val_loss did not improve from 0.50333\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2491 - acc: 0.9204 - val_loss: 0.5497 - val_acc: 0.8614\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2460 - acc: 0.9214\n",
      "Epoch 00063: val_loss did not improve from 0.50333\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2461 - acc: 0.9214 - val_loss: 0.5326 - val_acc: 0.8560\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2419 - acc: 0.9229\n",
      "Epoch 00064: val_loss did not improve from 0.50333\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2419 - acc: 0.9229 - val_loss: 0.5220 - val_acc: 0.8640\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2439 - acc: 0.9228\n",
      "Epoch 00065: val_loss improved from 0.50333 to 0.48975, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_BN_5_conv_checkpoint/065-0.4897.hdf5\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2440 - acc: 0.9228 - val_loss: 0.4897 - val_acc: 0.8726\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2346 - acc: 0.9248\n",
      "Epoch 00066: val_loss did not improve from 0.48975\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2345 - acc: 0.9248 - val_loss: 0.5428 - val_acc: 0.8630\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2357 - acc: 0.9242\n",
      "Epoch 00067: val_loss did not improve from 0.48975\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2357 - acc: 0.9242 - val_loss: 0.5440 - val_acc: 0.8591\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2311 - acc: 0.9239\n",
      "Epoch 00068: val_loss did not improve from 0.48975\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2311 - acc: 0.9240 - val_loss: 0.5486 - val_acc: 0.8665\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2267 - acc: 0.9263\n",
      "Epoch 00069: val_loss did not improve from 0.48975\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2266 - acc: 0.9263 - val_loss: 0.5159 - val_acc: 0.8737\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2215 - acc: 0.9290\n",
      "Epoch 00070: val_loss did not improve from 0.48975\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2215 - acc: 0.9290 - val_loss: 0.5100 - val_acc: 0.8654\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2149 - acc: 0.9310\n",
      "Epoch 00071: val_loss did not improve from 0.48975\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2149 - acc: 0.9310 - val_loss: 0.5243 - val_acc: 0.8668\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2194 - acc: 0.9289\n",
      "Epoch 00072: val_loss did not improve from 0.48975\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2195 - acc: 0.9288 - val_loss: 0.5408 - val_acc: 0.8605\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2203 - acc: 0.9288\n",
      "Epoch 00073: val_loss did not improve from 0.48975\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2203 - acc: 0.9288 - val_loss: 0.5141 - val_acc: 0.8682\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2094 - acc: 0.9325\n",
      "Epoch 00074: val_loss did not improve from 0.48975\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2095 - acc: 0.9325 - val_loss: 0.4996 - val_acc: 0.8761\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2062 - acc: 0.9347\n",
      "Epoch 00075: val_loss did not improve from 0.48975\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2063 - acc: 0.9346 - val_loss: 0.5304 - val_acc: 0.8707\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2055 - acc: 0.9337\n",
      "Epoch 00076: val_loss did not improve from 0.48975\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.2056 - acc: 0.9337 - val_loss: 0.6264 - val_acc: 0.8444\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1988 - acc: 0.9374\n",
      "Epoch 00077: val_loss did not improve from 0.48975\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1989 - acc: 0.9374 - val_loss: 0.5071 - val_acc: 0.8742\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2012 - acc: 0.9355\n",
      "Epoch 00078: val_loss did not improve from 0.48975\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.2013 - acc: 0.9355 - val_loss: 0.5062 - val_acc: 0.8726\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1950 - acc: 0.9366\n",
      "Epoch 00079: val_loss did not improve from 0.48975\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1950 - acc: 0.9366 - val_loss: 0.5553 - val_acc: 0.8616\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1931 - acc: 0.9375\n",
      "Epoch 00080: val_loss did not improve from 0.48975\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1932 - acc: 0.9375 - val_loss: 0.5337 - val_acc: 0.8707\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1964 - acc: 0.9370\n",
      "Epoch 00081: val_loss did not improve from 0.48975\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1964 - acc: 0.9370 - val_loss: 0.5171 - val_acc: 0.8744\n",
      "Epoch 82/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1963 - acc: 0.9362\n",
      "Epoch 00082: val_loss did not improve from 0.48975\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1963 - acc: 0.9363 - val_loss: 0.5798 - val_acc: 0.8521\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1953 - acc: 0.9377\n",
      "Epoch 00083: val_loss did not improve from 0.48975\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1953 - acc: 0.9378 - val_loss: 0.5381 - val_acc: 0.8698\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1808 - acc: 0.9420\n",
      "Epoch 00084: val_loss did not improve from 0.48975\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1811 - acc: 0.9420 - val_loss: 0.5286 - val_acc: 0.8663\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1911 - acc: 0.9392\n",
      "Epoch 00085: val_loss did not improve from 0.48975\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1912 - acc: 0.9392 - val_loss: 0.5229 - val_acc: 0.8705\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1806 - acc: 0.9413\n",
      "Epoch 00086: val_loss did not improve from 0.48975\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1807 - acc: 0.9412 - val_loss: 0.5397 - val_acc: 0.8640\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1811 - acc: 0.9423\n",
      "Epoch 00087: val_loss did not improve from 0.48975\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1811 - acc: 0.9423 - val_loss: 0.5696 - val_acc: 0.8574\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1768 - acc: 0.9417\n",
      "Epoch 00088: val_loss did not improve from 0.48975\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1770 - acc: 0.9417 - val_loss: 0.5107 - val_acc: 0.8700\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1838 - acc: 0.9409\n",
      "Epoch 00089: val_loss did not improve from 0.48975\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1839 - acc: 0.9409 - val_loss: 0.5300 - val_acc: 0.8707\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1751 - acc: 0.9427\n",
      "Epoch 00090: val_loss did not improve from 0.48975\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1751 - acc: 0.9427 - val_loss: 0.5220 - val_acc: 0.8749\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1706 - acc: 0.9446\n",
      "Epoch 00091: val_loss did not improve from 0.48975\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1706 - acc: 0.9446 - val_loss: 0.6402 - val_acc: 0.8507\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1715 - acc: 0.9448\n",
      "Epoch 00092: val_loss did not improve from 0.48975\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1715 - acc: 0.9448 - val_loss: 0.5301 - val_acc: 0.8726\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1684 - acc: 0.9468\n",
      "Epoch 00093: val_loss did not improve from 0.48975\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1684 - acc: 0.9468 - val_loss: 0.5143 - val_acc: 0.8761\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1679 - acc: 0.9467\n",
      "Epoch 00094: val_loss did not improve from 0.48975\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1680 - acc: 0.9467 - val_loss: 0.5303 - val_acc: 0.8772\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1693 - acc: 0.9466\n",
      "Epoch 00095: val_loss did not improve from 0.48975\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1693 - acc: 0.9466 - val_loss: 0.5049 - val_acc: 0.8784\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1629 - acc: 0.9472\n",
      "Epoch 00096: val_loss did not improve from 0.48975\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1629 - acc: 0.9472 - val_loss: 0.5177 - val_acc: 0.8749\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1607 - acc: 0.9486\n",
      "Epoch 00097: val_loss did not improve from 0.48975\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1608 - acc: 0.9486 - val_loss: 0.5055 - val_acc: 0.8824\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1617 - acc: 0.9484\n",
      "Epoch 00098: val_loss did not improve from 0.48975\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1617 - acc: 0.9484 - val_loss: 0.6158 - val_acc: 0.8616\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1558 - acc: 0.9506\n",
      "Epoch 00099: val_loss did not improve from 0.48975\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1558 - acc: 0.9506 - val_loss: 0.5333 - val_acc: 0.8682\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1556 - acc: 0.9492\n",
      "Epoch 00100: val_loss did not improve from 0.48975\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1557 - acc: 0.9491 - val_loss: 0.5285 - val_acc: 0.8719\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1616 - acc: 0.9495\n",
      "Epoch 00101: val_loss did not improve from 0.48975\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1617 - acc: 0.9495 - val_loss: 0.5471 - val_acc: 0.8693\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1510 - acc: 0.9510\n",
      "Epoch 00102: val_loss did not improve from 0.48975\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1510 - acc: 0.9510 - val_loss: 0.5710 - val_acc: 0.8633\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1533 - acc: 0.9512\n",
      "Epoch 00103: val_loss did not improve from 0.48975\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1533 - acc: 0.9512 - val_loss: 0.5378 - val_acc: 0.8744\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1537 - acc: 0.9499\n",
      "Epoch 00104: val_loss did not improve from 0.48975\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1539 - acc: 0.9498 - val_loss: 0.5469 - val_acc: 0.8721\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1527 - acc: 0.9495\n",
      "Epoch 00105: val_loss did not improve from 0.48975\n",
      "36805/36805 [==============================] - 87s 2ms/sample - loss: 0.1528 - acc: 0.9494 - val_loss: 0.5724 - val_acc: 0.8640\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1514 - acc: 0.9520\n",
      "Epoch 00106: val_loss did not improve from 0.48975\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1514 - acc: 0.9520 - val_loss: 0.5087 - val_acc: 0.8779\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1473 - acc: 0.9529\n",
      "Epoch 00107: val_loss did not improve from 0.48975\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1473 - acc: 0.9529 - val_loss: 0.5126 - val_acc: 0.8800\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1406 - acc: 0.9550\n",
      "Epoch 00108: val_loss did not improve from 0.48975\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1406 - acc: 0.9550 - val_loss: 0.5419 - val_acc: 0.8749\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1458 - acc: 0.9527\n",
      "Epoch 00109: val_loss did not improve from 0.48975\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1458 - acc: 0.9528 - val_loss: 0.5274 - val_acc: 0.8784\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1423 - acc: 0.9527\n",
      "Epoch 00110: val_loss did not improve from 0.48975\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1423 - acc: 0.9527 - val_loss: 0.5683 - val_acc: 0.8642\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1391 - acc: 0.9561\n",
      "Epoch 00111: val_loss did not improve from 0.48975\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1391 - acc: 0.9561 - val_loss: 0.5487 - val_acc: 0.8756\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1408 - acc: 0.9534\n",
      "Epoch 00112: val_loss did not improve from 0.48975\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1408 - acc: 0.9534 - val_loss: 0.5612 - val_acc: 0.8691\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1401 - acc: 0.9546\n",
      "Epoch 00113: val_loss did not improve from 0.48975\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1400 - acc: 0.9546 - val_loss: 0.5819 - val_acc: 0.8586\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1388 - acc: 0.9557\n",
      "Epoch 00114: val_loss did not improve from 0.48975\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1388 - acc: 0.9557 - val_loss: 0.5271 - val_acc: 0.8789\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1403 - acc: 0.9557\n",
      "Epoch 00115: val_loss did not improve from 0.48975\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1403 - acc: 0.9556 - val_loss: 0.5908 - val_acc: 0.8644\n",
      "\n",
      "1D_CNN_custom_DO_075_DO_BN_5_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 914us/sample - loss: 0.5975 - acc: 0.8397\n",
      "Loss: 0.597500038320278 Accuracy: 0.8396677\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.9785 - acc: 0.2126\n",
      "Epoch 00001: val_loss improved from inf to 1.91298, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_BN_6_conv_checkpoint/001-1.9130.hdf5\n",
      "36805/36805 [==============================] - 98s 3ms/sample - loss: 2.9785 - acc: 0.2127 - val_loss: 1.9130 - val_acc: 0.3457\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.8788 - acc: 0.4240\n",
      "Epoch 00002: val_loss improved from 1.91298 to 1.22668, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_BN_6_conv_checkpoint/002-1.2267.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 1.8787 - acc: 0.4240 - val_loss: 1.2267 - val_acc: 0.6280\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4273 - acc: 0.5532\n",
      "Epoch 00003: val_loss improved from 1.22668 to 0.98819, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_BN_6_conv_checkpoint/003-0.9882.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 1.4275 - acc: 0.5531 - val_loss: 0.9882 - val_acc: 0.6909\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1992 - acc: 0.6246\n",
      "Epoch 00004: val_loss improved from 0.98819 to 0.86630, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_BN_6_conv_checkpoint/004-0.8663.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 1.1992 - acc: 0.6245 - val_loss: 0.8663 - val_acc: 0.7335\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0537 - acc: 0.6745\n",
      "Epoch 00005: val_loss improved from 0.86630 to 0.76369, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_BN_6_conv_checkpoint/005-0.7637.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 1.0539 - acc: 0.6744 - val_loss: 0.7637 - val_acc: 0.7785\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9468 - acc: 0.7093\n",
      "Epoch 00006: val_loss improved from 0.76369 to 0.70380, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_BN_6_conv_checkpoint/006-0.7038.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.9469 - acc: 0.7093 - val_loss: 0.7038 - val_acc: 0.8013\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8769 - acc: 0.7322\n",
      "Epoch 00007: val_loss did not improve from 0.70380\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.8768 - acc: 0.7322 - val_loss: 0.7461 - val_acc: 0.7824\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8121 - acc: 0.7524\n",
      "Epoch 00008: val_loss improved from 0.70380 to 0.64967, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_BN_6_conv_checkpoint/008-0.6497.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.8122 - acc: 0.7523 - val_loss: 0.6497 - val_acc: 0.8204\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7631 - acc: 0.7696\n",
      "Epoch 00009: val_loss improved from 0.64967 to 0.62521, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_BN_6_conv_checkpoint/009-0.6252.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.7631 - acc: 0.7696 - val_loss: 0.6252 - val_acc: 0.8234\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7240 - acc: 0.7809\n",
      "Epoch 00010: val_loss improved from 0.62521 to 0.62410, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_BN_6_conv_checkpoint/010-0.6241.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.7242 - acc: 0.7808 - val_loss: 0.6241 - val_acc: 0.8337\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6801 - acc: 0.7967\n",
      "Epoch 00011: val_loss did not improve from 0.62410\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.6804 - acc: 0.7967 - val_loss: 0.7060 - val_acc: 0.7976\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6392 - acc: 0.8079\n",
      "Epoch 00012: val_loss did not improve from 0.62410\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.6393 - acc: 0.8079 - val_loss: 0.6334 - val_acc: 0.8272\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6120 - acc: 0.8179\n",
      "Epoch 00013: val_loss improved from 0.62410 to 0.53556, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_BN_6_conv_checkpoint/013-0.5356.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.6120 - acc: 0.8179 - val_loss: 0.5356 - val_acc: 0.8560\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5826 - acc: 0.8263\n",
      "Epoch 00014: val_loss did not improve from 0.53556\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.5826 - acc: 0.8263 - val_loss: 0.5515 - val_acc: 0.8423\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5664 - acc: 0.8291\n",
      "Epoch 00015: val_loss improved from 0.53556 to 0.48973, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_BN_6_conv_checkpoint/015-0.4897.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.5665 - acc: 0.8291 - val_loss: 0.4897 - val_acc: 0.8647\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5436 - acc: 0.8368\n",
      "Epoch 00016: val_loss did not improve from 0.48973\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.5437 - acc: 0.8368 - val_loss: 0.6425 - val_acc: 0.8220\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5206 - acc: 0.8446\n",
      "Epoch 00017: val_loss did not improve from 0.48973\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.5206 - acc: 0.8446 - val_loss: 0.6959 - val_acc: 0.7941\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5028 - acc: 0.8502\n",
      "Epoch 00018: val_loss improved from 0.48973 to 0.43179, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_BN_6_conv_checkpoint/018-0.4318.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.5028 - acc: 0.8502 - val_loss: 0.4318 - val_acc: 0.8840\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4868 - acc: 0.8547\n",
      "Epoch 00019: val_loss did not improve from 0.43179\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.4867 - acc: 0.8547 - val_loss: 0.4868 - val_acc: 0.8668\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4738 - acc: 0.8580\n",
      "Epoch 00020: val_loss did not improve from 0.43179\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.4739 - acc: 0.8579 - val_loss: 0.4523 - val_acc: 0.8710\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4527 - acc: 0.8635\n",
      "Epoch 00021: val_loss improved from 0.43179 to 0.42247, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_BN_6_conv_checkpoint/021-0.4225.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.4527 - acc: 0.8635 - val_loss: 0.4225 - val_acc: 0.8859\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4465 - acc: 0.8638\n",
      "Epoch 00022: val_loss did not improve from 0.42247\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.4466 - acc: 0.8637 - val_loss: 0.4344 - val_acc: 0.8819\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4326 - acc: 0.8687\n",
      "Epoch 00023: val_loss did not improve from 0.42247\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.4327 - acc: 0.8686 - val_loss: 0.4627 - val_acc: 0.8756\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4251 - acc: 0.8720\n",
      "Epoch 00024: val_loss improved from 0.42247 to 0.41230, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_BN_6_conv_checkpoint/024-0.4123.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.4252 - acc: 0.8720 - val_loss: 0.4123 - val_acc: 0.8796\n",
      "Epoch 25/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4101 - acc: 0.8761\n",
      "Epoch 00025: val_loss improved from 0.41230 to 0.40969, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_BN_6_conv_checkpoint/025-0.4097.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.4103 - acc: 0.8760 - val_loss: 0.4097 - val_acc: 0.8910\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3964 - acc: 0.8786\n",
      "Epoch 00026: val_loss improved from 0.40969 to 0.40722, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_BN_6_conv_checkpoint/026-0.4072.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.3964 - acc: 0.8786 - val_loss: 0.4072 - val_acc: 0.8921\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3883 - acc: 0.8796\n",
      "Epoch 00027: val_loss did not improve from 0.40722\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.3882 - acc: 0.8797 - val_loss: 0.4165 - val_acc: 0.8863\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3781 - acc: 0.8843\n",
      "Epoch 00028: val_loss did not improve from 0.40722\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.3781 - acc: 0.8843 - val_loss: 0.4088 - val_acc: 0.8861\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3731 - acc: 0.8860\n",
      "Epoch 00029: val_loss did not improve from 0.40722\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.3732 - acc: 0.8860 - val_loss: 0.4303 - val_acc: 0.8775\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3626 - acc: 0.8888\n",
      "Epoch 00030: val_loss improved from 0.40722 to 0.40397, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_BN_6_conv_checkpoint/030-0.4040.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.3626 - acc: 0.8888 - val_loss: 0.4040 - val_acc: 0.8928\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3520 - acc: 0.8925\n",
      "Epoch 00031: val_loss improved from 0.40397 to 0.36870, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_BN_6_conv_checkpoint/031-0.3687.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.3520 - acc: 0.8925 - val_loss: 0.3687 - val_acc: 0.8970\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3388 - acc: 0.8940\n",
      "Epoch 00032: val_loss did not improve from 0.36870\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.3387 - acc: 0.8940 - val_loss: 0.3701 - val_acc: 0.8954\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3389 - acc: 0.8965\n",
      "Epoch 00033: val_loss did not improve from 0.36870\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.3389 - acc: 0.8965 - val_loss: 0.3769 - val_acc: 0.8968\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3311 - acc: 0.8972\n",
      "Epoch 00034: val_loss did not improve from 0.36870\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.3312 - acc: 0.8972 - val_loss: 0.3770 - val_acc: 0.9022\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3222 - acc: 0.9002\n",
      "Epoch 00035: val_loss did not improve from 0.36870\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.3222 - acc: 0.9002 - val_loss: 0.3918 - val_acc: 0.8935\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3125 - acc: 0.9025\n",
      "Epoch 00036: val_loss did not improve from 0.36870\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.3126 - acc: 0.9025 - val_loss: 0.4166 - val_acc: 0.8894\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3136 - acc: 0.9027\n",
      "Epoch 00037: val_loss did not improve from 0.36870\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.3137 - acc: 0.9026 - val_loss: 0.4067 - val_acc: 0.8875\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3039 - acc: 0.9053\n",
      "Epoch 00038: val_loss did not improve from 0.36870\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.3039 - acc: 0.9053 - val_loss: 0.3862 - val_acc: 0.8898\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2907 - acc: 0.9101\n",
      "Epoch 00039: val_loss improved from 0.36870 to 0.34709, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_BN_6_conv_checkpoint/039-0.3471.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2907 - acc: 0.9100 - val_loss: 0.3471 - val_acc: 0.9103\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2960 - acc: 0.9085\n",
      "Epoch 00040: val_loss did not improve from 0.34709\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2962 - acc: 0.9085 - val_loss: 0.3939 - val_acc: 0.8905\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2870 - acc: 0.9106\n",
      "Epoch 00041: val_loss did not improve from 0.34709\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2871 - acc: 0.9105 - val_loss: 0.4409 - val_acc: 0.8803\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2824 - acc: 0.9121\n",
      "Epoch 00042: val_loss did not improve from 0.34709\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2827 - acc: 0.9121 - val_loss: 0.3808 - val_acc: 0.9015\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2810 - acc: 0.9109\n",
      "Epoch 00043: val_loss did not improve from 0.34709\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2811 - acc: 0.9109 - val_loss: 0.3788 - val_acc: 0.8940\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2676 - acc: 0.9130\n",
      "Epoch 00044: val_loss did not improve from 0.34709\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2676 - acc: 0.9130 - val_loss: 0.3587 - val_acc: 0.9057\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2687 - acc: 0.9151\n",
      "Epoch 00045: val_loss did not improve from 0.34709\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2686 - acc: 0.9151 - val_loss: 0.3657 - val_acc: 0.9026\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2615 - acc: 0.9167\n",
      "Epoch 00046: val_loss did not improve from 0.34709\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2615 - acc: 0.9167 - val_loss: 0.4025 - val_acc: 0.8973\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2621 - acc: 0.9175\n",
      "Epoch 00047: val_loss did not improve from 0.34709\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2621 - acc: 0.9175 - val_loss: 0.3718 - val_acc: 0.9094\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2512 - acc: 0.9211\n",
      "Epoch 00048: val_loss did not improve from 0.34709\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2512 - acc: 0.9211 - val_loss: 0.4290 - val_acc: 0.8924\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2551 - acc: 0.9183\n",
      "Epoch 00049: val_loss did not improve from 0.34709\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2552 - acc: 0.9183 - val_loss: 0.3549 - val_acc: 0.9047\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2452 - acc: 0.9227\n",
      "Epoch 00050: val_loss did not improve from 0.34709\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2451 - acc: 0.9227 - val_loss: 0.3565 - val_acc: 0.9117\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2365 - acc: 0.9247\n",
      "Epoch 00051: val_loss improved from 0.34709 to 0.32730, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_BN_6_conv_checkpoint/051-0.3273.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2366 - acc: 0.9247 - val_loss: 0.3273 - val_acc: 0.9119\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2296 - acc: 0.9269\n",
      "Epoch 00052: val_loss did not improve from 0.32730\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2296 - acc: 0.9269 - val_loss: 0.3595 - val_acc: 0.9012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2396 - acc: 0.9246\n",
      "Epoch 00053: val_loss did not improve from 0.32730\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2396 - acc: 0.9246 - val_loss: 0.3447 - val_acc: 0.9092\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2275 - acc: 0.9267\n",
      "Epoch 00054: val_loss did not improve from 0.32730\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2278 - acc: 0.9266 - val_loss: 0.3375 - val_acc: 0.9073\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2320 - acc: 0.9259\n",
      "Epoch 00055: val_loss did not improve from 0.32730\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2320 - acc: 0.9259 - val_loss: 0.3571 - val_acc: 0.9096\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2178 - acc: 0.9297\n",
      "Epoch 00056: val_loss improved from 0.32730 to 0.32379, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_BN_6_conv_checkpoint/056-0.3238.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2178 - acc: 0.9297 - val_loss: 0.3238 - val_acc: 0.9168\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2173 - acc: 0.9303\n",
      "Epoch 00057: val_loss did not improve from 0.32379\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2174 - acc: 0.9303 - val_loss: 0.3822 - val_acc: 0.9057\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2176 - acc: 0.9317\n",
      "Epoch 00058: val_loss did not improve from 0.32379\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2176 - acc: 0.9317 - val_loss: 0.3890 - val_acc: 0.9059\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2103 - acc: 0.9328\n",
      "Epoch 00059: val_loss did not improve from 0.32379\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2104 - acc: 0.9328 - val_loss: 0.3487 - val_acc: 0.9099\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2146 - acc: 0.9319\n",
      "Epoch 00060: val_loss did not improve from 0.32379\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2146 - acc: 0.9319 - val_loss: 0.3723 - val_acc: 0.9001\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2057 - acc: 0.9327\n",
      "Epoch 00061: val_loss did not improve from 0.32379\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2059 - acc: 0.9326 - val_loss: 0.3336 - val_acc: 0.9171\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2105 - acc: 0.9306\n",
      "Epoch 00062: val_loss did not improve from 0.32379\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2107 - acc: 0.9305 - val_loss: 0.3661 - val_acc: 0.9094\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2076 - acc: 0.9330\n",
      "Epoch 00063: val_loss did not improve from 0.32379\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.2076 - acc: 0.9330 - val_loss: 0.3332 - val_acc: 0.9073\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1943 - acc: 0.9379\n",
      "Epoch 00064: val_loss did not improve from 0.32379\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1943 - acc: 0.9379 - val_loss: 0.4556 - val_acc: 0.8763\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1957 - acc: 0.9369\n",
      "Epoch 00065: val_loss did not improve from 0.32379\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1957 - acc: 0.9369 - val_loss: 0.3403 - val_acc: 0.9194\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1907 - acc: 0.9379\n",
      "Epoch 00066: val_loss did not improve from 0.32379\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1908 - acc: 0.9379 - val_loss: 0.3896 - val_acc: 0.9026\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1917 - acc: 0.9385\n",
      "Epoch 00067: val_loss did not improve from 0.32379\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1917 - acc: 0.9385 - val_loss: 0.3304 - val_acc: 0.9173\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1832 - acc: 0.9410\n",
      "Epoch 00068: val_loss did not improve from 0.32379\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1832 - acc: 0.9410 - val_loss: 0.3436 - val_acc: 0.9133\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1858 - acc: 0.9393\n",
      "Epoch 00069: val_loss did not improve from 0.32379\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1858 - acc: 0.9393 - val_loss: 0.3388 - val_acc: 0.9194\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1829 - acc: 0.9408\n",
      "Epoch 00070: val_loss did not improve from 0.32379\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1830 - acc: 0.9408 - val_loss: 0.3641 - val_acc: 0.9061\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1803 - acc: 0.9394\n",
      "Epoch 00071: val_loss did not improve from 0.32379\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1802 - acc: 0.9394 - val_loss: 0.3426 - val_acc: 0.9157\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1775 - acc: 0.9431\n",
      "Epoch 00072: val_loss did not improve from 0.32379\n",
      "36805/36805 [==============================] - 88s 2ms/sample - loss: 0.1776 - acc: 0.9431 - val_loss: 0.3554 - val_acc: 0.9106\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1761 - acc: 0.9422\n",
      "Epoch 00073: val_loss did not improve from 0.32379\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1761 - acc: 0.9422 - val_loss: 0.3365 - val_acc: 0.9203\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1758 - acc: 0.9436\n",
      "Epoch 00074: val_loss did not improve from 0.32379\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1759 - acc: 0.9436 - val_loss: 0.3388 - val_acc: 0.9115\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1795 - acc: 0.9409\n",
      "Epoch 00075: val_loss did not improve from 0.32379\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1795 - acc: 0.9409 - val_loss: 0.3747 - val_acc: 0.9073\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1650 - acc: 0.9469\n",
      "Epoch 00076: val_loss did not improve from 0.32379\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1651 - acc: 0.9469 - val_loss: 0.3272 - val_acc: 0.9147\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1665 - acc: 0.9449\n",
      "Epoch 00077: val_loss improved from 0.32379 to 0.30787, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_BN_6_conv_checkpoint/077-0.3079.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1665 - acc: 0.9449 - val_loss: 0.3079 - val_acc: 0.9252\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1639 - acc: 0.9458\n",
      "Epoch 00078: val_loss did not improve from 0.30787\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1639 - acc: 0.9458 - val_loss: 0.3917 - val_acc: 0.9085\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1639 - acc: 0.9457\n",
      "Epoch 00079: val_loss did not improve from 0.30787\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1639 - acc: 0.9457 - val_loss: 0.4078 - val_acc: 0.9043\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1644 - acc: 0.9469\n",
      "Epoch 00080: val_loss did not improve from 0.30787\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1644 - acc: 0.9469 - val_loss: 0.3654 - val_acc: 0.9185\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1577 - acc: 0.9487\n",
      "Epoch 00081: val_loss did not improve from 0.30787\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1577 - acc: 0.9487 - val_loss: 0.3427 - val_acc: 0.9192\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1555 - acc: 0.9487\n",
      "Epoch 00082: val_loss did not improve from 0.30787\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1556 - acc: 0.9486 - val_loss: 0.4273 - val_acc: 0.8989\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1575 - acc: 0.9483\n",
      "Epoch 00083: val_loss did not improve from 0.30787\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1575 - acc: 0.9483 - val_loss: 0.3456 - val_acc: 0.9122\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1522 - acc: 0.9497\n",
      "Epoch 00084: val_loss did not improve from 0.30787\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1521 - acc: 0.9497 - val_loss: 0.3244 - val_acc: 0.9208\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1488 - acc: 0.9513\n",
      "Epoch 00085: val_loss did not improve from 0.30787\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1488 - acc: 0.9512 - val_loss: 0.3192 - val_acc: 0.9262\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1525 - acc: 0.9503\n",
      "Epoch 00086: val_loss did not improve from 0.30787\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1526 - acc: 0.9503 - val_loss: 0.3179 - val_acc: 0.9243\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1487 - acc: 0.9518\n",
      "Epoch 00087: val_loss did not improve from 0.30787\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1488 - acc: 0.9517 - val_loss: 0.3280 - val_acc: 0.9243\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1476 - acc: 0.9525\n",
      "Epoch 00088: val_loss did not improve from 0.30787\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1477 - acc: 0.9525 - val_loss: 0.4148 - val_acc: 0.8956\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1592 - acc: 0.9486\n",
      "Epoch 00089: val_loss did not improve from 0.30787\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1591 - acc: 0.9486 - val_loss: 0.3716 - val_acc: 0.9089\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1393 - acc: 0.9554\n",
      "Epoch 00090: val_loss did not improve from 0.30787\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1392 - acc: 0.9554 - val_loss: 0.3158 - val_acc: 0.9217\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1379 - acc: 0.9548\n",
      "Epoch 00091: val_loss did not improve from 0.30787\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1379 - acc: 0.9548 - val_loss: 0.3152 - val_acc: 0.9269\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1405 - acc: 0.9542\n",
      "Epoch 00092: val_loss did not improve from 0.30787\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1405 - acc: 0.9542 - val_loss: 0.3217 - val_acc: 0.9168\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1368 - acc: 0.9545\n",
      "Epoch 00093: val_loss did not improve from 0.30787\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1368 - acc: 0.9545 - val_loss: 0.3490 - val_acc: 0.9201\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1382 - acc: 0.9560\n",
      "Epoch 00094: val_loss improved from 0.30787 to 0.30445, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_BN_6_conv_checkpoint/094-0.3045.hdf5\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1381 - acc: 0.9560 - val_loss: 0.3045 - val_acc: 0.9222\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1373 - acc: 0.9550\n",
      "Epoch 00095: val_loss did not improve from 0.30445\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1374 - acc: 0.9550 - val_loss: 0.3754 - val_acc: 0.9124\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1341 - acc: 0.9567\n",
      "Epoch 00096: val_loss did not improve from 0.30445\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1343 - acc: 0.9566 - val_loss: 0.4051 - val_acc: 0.9071\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1397 - acc: 0.9550\n",
      "Epoch 00097: val_loss did not improve from 0.30445\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1397 - acc: 0.9550 - val_loss: 0.3227 - val_acc: 0.9206\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1249 - acc: 0.9589\n",
      "Epoch 00098: val_loss did not improve from 0.30445\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1249 - acc: 0.9589 - val_loss: 0.3575 - val_acc: 0.9178\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1362 - acc: 0.9555\n",
      "Epoch 00099: val_loss did not improve from 0.30445\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1362 - acc: 0.9555 - val_loss: 0.3596 - val_acc: 0.9066\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1248 - acc: 0.9582\n",
      "Epoch 00100: val_loss did not improve from 0.30445\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1248 - acc: 0.9582 - val_loss: 0.3506 - val_acc: 0.9185\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1248 - acc: 0.9594\n",
      "Epoch 00101: val_loss did not improve from 0.30445\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1251 - acc: 0.9594 - val_loss: 0.4402 - val_acc: 0.8933\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1329 - acc: 0.9563\n",
      "Epoch 00102: val_loss did not improve from 0.30445\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1329 - acc: 0.9563 - val_loss: 0.3184 - val_acc: 0.9236\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1237 - acc: 0.9600\n",
      "Epoch 00103: val_loss did not improve from 0.30445\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1238 - acc: 0.9600 - val_loss: 0.3947 - val_acc: 0.9106\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1261 - acc: 0.9593\n",
      "Epoch 00104: val_loss did not improve from 0.30445\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1261 - acc: 0.9594 - val_loss: 0.3353 - val_acc: 0.9259\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1180 - acc: 0.9612\n",
      "Epoch 00105: val_loss did not improve from 0.30445\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1181 - acc: 0.9612 - val_loss: 0.3439 - val_acc: 0.9178\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1243 - acc: 0.9593\n",
      "Epoch 00106: val_loss did not improve from 0.30445\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1243 - acc: 0.9593 - val_loss: 0.4359 - val_acc: 0.8973\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1229 - acc: 0.9601\n",
      "Epoch 00107: val_loss did not improve from 0.30445\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1229 - acc: 0.9601 - val_loss: 0.3662 - val_acc: 0.9187\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1188 - acc: 0.9608\n",
      "Epoch 00108: val_loss did not improve from 0.30445\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1188 - acc: 0.9608 - val_loss: 0.3330 - val_acc: 0.9229\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1166 - acc: 0.9607\n",
      "Epoch 00109: val_loss did not improve from 0.30445\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1166 - acc: 0.9607 - val_loss: 0.3848 - val_acc: 0.9131\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1181 - acc: 0.9598\n",
      "Epoch 00110: val_loss did not improve from 0.30445\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1182 - acc: 0.9598 - val_loss: 0.3504 - val_acc: 0.9224\n",
      "Epoch 111/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1163 - acc: 0.9619\n",
      "Epoch 00111: val_loss did not improve from 0.30445\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1163 - acc: 0.9619 - val_loss: 0.3820 - val_acc: 0.9133\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1085 - acc: 0.9637\n",
      "Epoch 00112: val_loss did not improve from 0.30445\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1085 - acc: 0.9637 - val_loss: 0.3541 - val_acc: 0.9175\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1136 - acc: 0.9632\n",
      "Epoch 00113: val_loss did not improve from 0.30445\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1137 - acc: 0.9632 - val_loss: 0.3237 - val_acc: 0.9287\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1111 - acc: 0.9633\n",
      "Epoch 00114: val_loss did not improve from 0.30445\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1111 - acc: 0.9633 - val_loss: 0.3731 - val_acc: 0.9159\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1117 - acc: 0.9620\n",
      "Epoch 00115: val_loss did not improve from 0.30445\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1118 - acc: 0.9620 - val_loss: 0.3861 - val_acc: 0.9113\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1182 - acc: 0.9604\n",
      "Epoch 00116: val_loss did not improve from 0.30445\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1185 - acc: 0.9604 - val_loss: 0.3523 - val_acc: 0.9220\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1144 - acc: 0.9622\n",
      "Epoch 00117: val_loss did not improve from 0.30445\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1144 - acc: 0.9622 - val_loss: 0.3433 - val_acc: 0.9262\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1072 - acc: 0.9655\n",
      "Epoch 00118: val_loss did not improve from 0.30445\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1072 - acc: 0.9655 - val_loss: 0.3251 - val_acc: 0.9259\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1043 - acc: 0.9660\n",
      "Epoch 00119: val_loss did not improve from 0.30445\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1043 - acc: 0.9660 - val_loss: 0.3184 - val_acc: 0.9259\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1109 - acc: 0.9637\n",
      "Epoch 00120: val_loss did not improve from 0.30445\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1109 - acc: 0.9637 - val_loss: 0.3229 - val_acc: 0.9257\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1009 - acc: 0.9670\n",
      "Epoch 00121: val_loss did not improve from 0.30445\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1009 - acc: 0.9669 - val_loss: 0.3745 - val_acc: 0.9178\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1047 - acc: 0.9657\n",
      "Epoch 00122: val_loss did not improve from 0.30445\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1047 - acc: 0.9657 - val_loss: 0.3860 - val_acc: 0.9196\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1045 - acc: 0.9656\n",
      "Epoch 00123: val_loss did not improve from 0.30445\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1046 - acc: 0.9656 - val_loss: 0.3119 - val_acc: 0.9297\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0989 - acc: 0.9673\n",
      "Epoch 00124: val_loss did not improve from 0.30445\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0989 - acc: 0.9673 - val_loss: 0.3509 - val_acc: 0.9236\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0995 - acc: 0.9682\n",
      "Epoch 00125: val_loss did not improve from 0.30445\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0995 - acc: 0.9682 - val_loss: 0.3353 - val_acc: 0.9234\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1017 - acc: 0.9666\n",
      "Epoch 00126: val_loss did not improve from 0.30445\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1017 - acc: 0.9666 - val_loss: 0.3412 - val_acc: 0.9243\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1002 - acc: 0.9669\n",
      "Epoch 00127: val_loss did not improve from 0.30445\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.1002 - acc: 0.9669 - val_loss: 0.3609 - val_acc: 0.9175\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0961 - acc: 0.9697\n",
      "Epoch 00128: val_loss did not improve from 0.30445\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0961 - acc: 0.9697 - val_loss: 0.3148 - val_acc: 0.9245\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0974 - acc: 0.9671\n",
      "Epoch 00129: val_loss did not improve from 0.30445\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0974 - acc: 0.9671 - val_loss: 0.3136 - val_acc: 0.9259\n",
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0951 - acc: 0.9690\n",
      "Epoch 00130: val_loss did not improve from 0.30445\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0951 - acc: 0.9690 - val_loss: 0.3217 - val_acc: 0.9224\n",
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0958 - acc: 0.9683\n",
      "Epoch 00131: val_loss did not improve from 0.30445\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0958 - acc: 0.9683 - val_loss: 0.3371 - val_acc: 0.9250\n",
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0967 - acc: 0.9682\n",
      "Epoch 00132: val_loss did not improve from 0.30445\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0966 - acc: 0.9682 - val_loss: 0.3397 - val_acc: 0.9227\n",
      "Epoch 133/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0919 - acc: 0.9699\n",
      "Epoch 00133: val_loss did not improve from 0.30445\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0919 - acc: 0.9698 - val_loss: 0.3244 - val_acc: 0.9299\n",
      "Epoch 134/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0927 - acc: 0.9692\n",
      "Epoch 00134: val_loss did not improve from 0.30445\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0927 - acc: 0.9692 - val_loss: 0.3756 - val_acc: 0.9140\n",
      "Epoch 135/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0907 - acc: 0.9707\n",
      "Epoch 00135: val_loss did not improve from 0.30445\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0906 - acc: 0.9707 - val_loss: 0.3707 - val_acc: 0.9203\n",
      "Epoch 136/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0968 - acc: 0.9681\n",
      "Epoch 00136: val_loss did not improve from 0.30445\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0968 - acc: 0.9681 - val_loss: 0.3665 - val_acc: 0.9206\n",
      "Epoch 137/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0926 - acc: 0.9705\n",
      "Epoch 00137: val_loss did not improve from 0.30445\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0925 - acc: 0.9705 - val_loss: 0.3189 - val_acc: 0.9297\n",
      "Epoch 138/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0892 - acc: 0.9711\n",
      "Epoch 00138: val_loss did not improve from 0.30445\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0893 - acc: 0.9710 - val_loss: 0.3217 - val_acc: 0.9266\n",
      "Epoch 139/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0956 - acc: 0.9688\n",
      "Epoch 00139: val_loss did not improve from 0.30445\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0956 - acc: 0.9688 - val_loss: 0.3362 - val_acc: 0.9259\n",
      "Epoch 140/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0895 - acc: 0.9713\n",
      "Epoch 00140: val_loss did not improve from 0.30445\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0895 - acc: 0.9713 - val_loss: 0.3233 - val_acc: 0.9257\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 141/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0867 - acc: 0.9711\n",
      "Epoch 00141: val_loss did not improve from 0.30445\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0867 - acc: 0.9711 - val_loss: 0.3500 - val_acc: 0.9231\n",
      "Epoch 142/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0850 - acc: 0.9723\n",
      "Epoch 00142: val_loss did not improve from 0.30445\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0852 - acc: 0.9723 - val_loss: 0.3685 - val_acc: 0.9222\n",
      "Epoch 143/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0939 - acc: 0.9689\n",
      "Epoch 00143: val_loss did not improve from 0.30445\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0939 - acc: 0.9689 - val_loss: 0.3456 - val_acc: 0.9257\n",
      "Epoch 144/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0825 - acc: 0.9729\n",
      "Epoch 00144: val_loss did not improve from 0.30445\n",
      "36805/36805 [==============================] - 89s 2ms/sample - loss: 0.0825 - acc: 0.9729 - val_loss: 0.4134 - val_acc: 0.9152\n",
      "\n",
      "1D_CNN_custom_DO_075_DO_BN_6_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzs3XecVNX5+PHPmbI72xtLB5cuLGXp2CgSCYJiRcSWqLHHqEmM/GJijNFoEo1KRA352mNERY2iRCMRRFSUBelVet3G9jrl+f1xtrO7LLDDAvO8X6957cydc889d3bmPPecc++5RkRQSimlABytXQCllFInDg0KSimlqmlQUEopVU2DglJKqWoaFJRSSlXToKCUUqqaBgWllFLVNCgopZSqpkFBKaVUNVdrF+BItWnTRlJSUlq7GEopdVJZvnx5togkHy7dSRcUUlJSSE9Pb+1iKKXUScUYs7M56bT7SCmlVDUNCkoppappUFBKKVUtaGMKxhgPsBgIr9zOXBH5Xb004cCrwFAgB5gmIjuOdFter5c9e/ZQVlZ2zOUOVR6Ph86dO+N2u1u7KEqpVhTMgeZy4FwRKTLGuIElxpj/iMjSWmluBHJFpKcx5krgT8C0I93Qnj17iImJISUlBWNMy5Q+hIgIOTk57Nmzh27durV2cZRSrSho3UdiFVW+dFc+6t/R5yLglcrnc4Hx5ihq9bKyMpKSkjQgHCVjDElJSdrSUkoFd0zBGOM0xqwEMoFPReSbekk6AbsBRMQH5ANJR7mtYylqyNPPTykFQQ4KIuIXkTSgMzDCGNP/aPIxxtxsjEk3xqRnZWUdVVn8/lLKy/cSCHiPan2llAoFx+XsIxHJAxYCE+u9tRfoAmCMcQFx2AHn+uvPFpFhIjIsOfmwF+Q1KBAopaJiPyItHxTy8vJ49tlnj2rdSZMmkZeX1+z0Dz74II8//vhRbUsppQ4naEHBGJNsjImvfB4BnAdsrJfsA+BHlc8vBz4TkfrjDi2kaldbPvumgoLP52ty3fnz5xMfH9/iZVJKqaMRzJZCB2ChMWY1sAw7pvChMeYhY8yUyjQvAEnGmO+BnwMzglUYY+yuigRaPO8ZM2awdetW0tLSuPfee1m0aBHnnHMOU6ZMoV+/fgBcfPHFDB06lNTUVGbPnl29bkpKCtnZ2ezYsYO+ffty0003kZqayoQJEygtLW1yuytXrmTUqFEMHDiQSy65hNzcXABmzpxJv379GDhwIFdeeSUAn3/+OWlpaaSlpTF48GAKCwtb/HNQSp38gnZKqoisBgY3sPyBWs/LgKktud0tW+6mqGhlA+XxEwiU4HBEYozziPKMjk6jV6+nGn3/scceY+3ataxcabe7aNEiVqxYwdq1a6tP8XzxxRdJTEyktLSU4cOHc9lll5GUVHdMfcuWLbzxxhv84x//4IorruCdd97hmmuuaXS71113HX/7298YM2YMDzzwAL///e956qmneOyxx9i+fTvh4eHVXVOPP/44s2bN4qyzzqKoqAiPx3NEn4FSKjSEzBXNNSfXBKl3qp4RI0bUOed/5syZDBo0iFGjRrF79262bNlyyDrdunUjLS0NgKFDh7Jjx45G88/PzycvL48xY8YA8KMf/YjFixcDMHDgQK6++mr++c9/4nLZuH/WWWfx85//nJkzZ5KXl1e9XCmlajvlaobGjuj9/hJKStbj8fTA7U4IejmioqKqny9atIgFCxbw9ddfExkZydixYxu8JiA8PLz6udPpPGz3UWM++ugjFi9ezLx583jkkUdYs2YNM2bMYPLkycyfP5+zzjqLTz75hNNPP/2o8ldKnbpCpqUAVU2Flh9TiImJabKPPj8/n4SEBCIjI9m4cSNLly5tNG1zxcXFkZCQwBdffAHAa6+9xpgxYwgEAuzevZtx48bxpz/9ifz8fIqKiti6dSsDBgzgvvvuY/jw4WzcWH/MXymlTsGWQmNqBppbvvsoKSmJs846i/79+3P++eczefLkOu9PnDiR559/nr59+9KnTx9GjRrVItt95ZVXuPXWWykpKaF79+689NJL+P1+rrnmGvLz8xERfvaznxEfH89vf/tbFi5ciMPhIDU1lfPPP79FyqCUOrWYoJ0BGiTDhg2T+jfZ2bBhA3379m1yvUCgguLi1YSHn0ZY2NFd63Cqa87nqJQ6ORljlovIsMOlC6Huo6pdbfnuI6WUOlWETFComtvnZGsZKaXU8RQyQUFbCkopdXghExRqZgHVloJSSjUmZIKC5QjKNBdKKXWqCLGgYNCWglJKNS6kgoK9VuHEaClER0cf0XKllDoeQioogNGzj5RSqgkhFhSC01KYMWMGs2bNqn5ddSOcoqIixo8fz5AhQxgwYADvv/9+s/MUEe6991769+/PgAEDePPNNwHYv38/o0ePJi0tjf79+/PFF1/g9/v58Y9/XJ32ySefbPF9VEqFhlNvmou774aVh06dDRDhLwbjAEfEkeWZlgZPNT519rRp07j77ru54447AHjrrbf45JNP8Hg8vPfee8TGxpKdnc2oUaOYMmVKs+6H/O6777Jy5UpWrVpFdnY2w4cPZ/To0fzrX//ihz/8Iffffz9+v5+SkhJWrlzJ3r17Wbt2LcAR3clNKaVqO/WCQpOCc3P6wYMHk5mZyb59+8jKyiIhIYEuXbrg9Xr59a9/zeLFi3E4HOzdu5eMjAzat29/2DyXLFnC9OnTcTqdtGvXjjFjxrBs2TKGDx/ODTfcgNfr5eKLLyYtLY3u3buzbds27rzzTiZPnsyECROCsp9KqVPfqRcUmjiiLy/ZCBgiI/u0+GanTp3K3LlzOXDgANOmTQPg9ddfJysri+XLl+N2u0lJSWlwyuwjMXr0aBYvXsxHH33Ej3/8Y37+859z3XXXsWrVKj755BOef/553nrrLV588cWW2C2lVIgJuTGFYF2nMG3aNObMmcPcuXOZOtXeTC4/P5+2bdvidrtZuHAhO3fubHZ+55xzDm+++SZ+v5+srCwWL17MiBEj2LlzJ+3ateOmm27iJz/5CStWrCA7O5tAIMBll13Gww8/zIoVK4Kyj0qpU9+p11JoUvCuU0hNTaWwsJBOnTrRoUMHAK6++mouvPBCBgwYwLBhw47opjaXXHIJX3/9NYMGDcIYw5///Gfat2/PK6+8wl/+8hfcbjfR0dG8+uqr7N27l+uvv55AwAa8Rx99NCj7qJQ69YXM1NkApaXfEwiUExWVGqzindR06mylTl06dXaDdJoLpZRqSsgFBZ3mQimlGhdSQcFeH6AtBaWUakxIBQWd5kIppZoWYkHhxJkQTymlTkRBCwrGmC7GmIXGmPXGmHXGmLsaSDPWGJNvjFlZ+XggWOWp3B4g2lpQSqlGBLOl4AN+ISL9gFHAHcaYfg2k+0JE0iofDwWxPNTsbssGhby8PJ599tmjWnfSpEk6V5FS6oQRtKAgIvtFZEXl80JgA9ApWNtrjmDdkrOpoODz+Zpcd/78+cTHx7doeZRS6mgdlzEFY0wKMBj4poG3zzDGrDLG/McYE+SryuzutvS1CjNmzGDr1q2kpaVx7733smjRIs455xymTJlCv362cXTxxRczdOhQUlNTmT17dvW6KSkpZGdns2PHDvr27ctNN91EamoqEyZMoLS09JBtzZs3j5EjRzJ48GB+8IMfkJGRAUBRURHXX389AwYMYODAgbzzzjsAfPzxxwwZMoRBgwYxfvz4Ft1vpdSpJ+hXNBtjooHPgUdE5N1678UCAREpMsZMAp4WkV4N5HEzcDNA165dh9afQ6j2lbhNzJyNSAWBQDlOZxRHEg8PM3M2O3bs4IILLqieunrRokVMnjyZtWvX0q1bNwAOHjxIYmIipaWlDB8+nM8//5ykpCRSUlJIT0+nqKiInj17kp6eTlpaGldccQVTpkzhmmuuqbOt3Nxc4uPjMcbwf//3f2zYsIEnnniC++67j/Lycp6qLGhubi4+n48hQ4awePFiunXrVl2GxugVzUqdupp7RXNQ5z4yxriBd4DX6wcEABEpqPV8vjHmWWNMGxHJrpduNjAb7DQXx1CiyvygGbc0OCYjRoyoDggAM2fO5L333gNg9+7dbNmyhaSkpDrrdOvWjbS0NACGDh3Kjh07Dsl3z549TJs2jf3791NRUVG9jQULFjBnzpzqdAkJCcybN4/Ro0dXp2kqICilFAQxKBjbgf8CsEFE/tpImvZAhoiIMWYE9vA951i229QRvddbSFnZNiIjU3E6j/BGO0coKiqq+vmiRYtYsGABX3/9NZGRkYwdO7bBKbTDw8Ornzudzga7j+68805+/vOfM2XKFBYtWsSDDz4YlPIrpUJTMMcUzgKuBc6tdcrpJGPMrcaYWyvTXA6sNcasAmYCV0pQ+7OqdrdlxxRiYmIoLCxs9P38/HwSEhKIjIxk48aNLF269Ki3lZ+fT6dOdrz+lVdeqV5+3nnn1bklaG5uLqNGjWLx4sVs374dsF1YSinVlGCefbRERIyIDKx1yul8EXleRJ6vTPOMiKSKyCARGSUiXwWrPFBz9lFLx52kpCTOOuss+vfvz7333nvI+xMnTsTn89G3b19mzJjBqFGjjnpbDz74IFOnTmXo0KG0adOmevlvfvMbcnNz6d+/P4MGDWLhwoUkJycze/ZsLr30UgYNGlR98x+llGpMSE2d7fMVUlq6iYiI3rhcscEq4klLB5qVOnXp1NkNCs51CkopdaoIqaBQ032k8x8ppVRDQiooBGuaC6WUOlWEVFComeZCWwpKKdWQkAoKNdNcaEtBKaUaEmJBQVsKSinVlJAKCsacOC2F6Ojo1i6CUkodIqSCgp6SqpRSTQvRoNDyU2fXnmLiwQcf5PHHH6eoqIjx48czZMgQBgwYwPvvv3/YvBqbYruhKbAbmy5bKaWOVlBnSW0Nd398NysPNDJ3NuD3F2JMGA5HeKNp6ktrn8ZTExufaW/atGncfffd3HHHHQC89dZbfPLJJ3g8Ht577z1iY2PJzs5m1KhRTJkypdZZUId68cUX60yxfdlllxEIBLjpppvqTIEN8Ic//IG4uDjWrFkD2PmOlFLqWJxyQeHwWn7O7MGDB5OZmcm+ffvIysoiISGBLl264PV6+fWvf83ixYtxOBzs3buXjIwM2rdv32heDU2xnZWV1eAU2A1Nl62UUsfilAsKTR3RAxQVrcTlSsDjOa1Ftzt16lTmzp3LgQMHqieee/3118nKymL58uW43W5SUlIanDK7SnOn2FZKqWAJsTEFAEdQprmYNm0ac+bMYe7cuUydOhWw01y3bdsWt9vNwoULqX/HuPoam2K7sSmwG5ouWymljkUIBgVDMM4+Sk1NpbCwkE6dOtGhQwcArr76atLT0xkwYACvvvoqp59+epN5NDbFdmNTYDc0XbZSSh2LkJo6G6C4eB0ORzgRET2DUbyTmk6drdSpS6fObpQ5IS5eU0qpE1EIBgUHOs2FUko17JQJCs09+jdGWwoN0c9EKQWnSFDweDzk5OQ0s2LTlkJ9IkJOTg4ej6e1i6KUamWnxHUKnTt3Zs+ePWRlZR02bUVFJiI+wsNb/iK2k5nH46Fz586tXQylVCs7JYKC2+2uvtr3cNat+z1FRd+RlrYpyKVSSqmTzynRfXQkHI5wAgG9SlgppRoSgkHBQyBQ3trFUEqpE1IIBoVwRDQoKKVUQ4IWFIwxXYwxC40x640x64wxdzWQxhhjZhpjvjfGrDbGDAlWearY7iMNCkop1ZBgDjT7gF+IyApjTAyw3BjzqYisr5XmfKBX5WMk8Fzl36Axxo4piEiT9zVQSqlQFLSWgojsF5EVlc8LgQ1Ap3rJLgJeFWspEG+M6RCsMoEdUwBBxBfMzSil1EnpuIwpGGNSgMHAN/Xe6gTsrvV6D4cGjhZVdcc17UJSSqlDBT0oGGOigXeAu0Wk4CjzuNkYk26MSW/OBWpNqQoKOtislFKHCmpQMMa4sQHhdRF5t4Eke4EutV53rlxWh4jMFpFhIjIsOTn5mMpU01LQaxWUUqq+YJ59ZIAXgA0i8tdGkn0AXFd5FtIoIF9E9gerTFA1pqDdR0op1ZBgnn10FnAtsMYYs7Jy2a+BrgAi8jwwH5gEfA+UANcHsTyAPfsINCgopVRDghYURGQJ9t6XTaUR4I5glaEhOtCslFKNC50rmv/3Pxg5EteePEDHFJRSqiGhExSKiuDbb3EWVAB69pFSSjUkdIJCVBQAjhI/oN1HSinVkNAJCtHRADhK7V3X/P7i1iyNUkqdkEIuKDjL7C77fHmtWRqllDohhU5QqOw+cpXZE6J8voOtWRqllDohhU5QqOo+KvEDTrxeDQpKKVVf6ASFypaCKS7G7U7A58tt5QIppdSJJ3SCQkQEGAPFxbhcidpSUEqpBoROUDDGdiEVFeFyJeiYglJKNSB0ggLYLqSiItzuRO0+UkqpBoRWUIiO1u4jpZRqQmgFheqWgnYfKaVUQ0IrKNRqKfh8eYj4W7tESil1Qgm9oFA5pgDg8+W3coGUUurEElpBobL7yOVKANBxBaWUqie0gkKt7iPQqS6UUqq+0AsKdbqP9LRUpZSqLbSCQnX3kQ0K2n2klFJ1hVZQiI6GsjLcjlhAu4+UUqq+ZgUFY8xdxphYY71gjFlhjJkQ7MK1uKrps8vDAPB6tftIKaVqa25L4QYRKQAmAAnAtcBjQStVsFTffc2L0xmtLQWllKqnuUHBVP6dBLwmIutqLTt5VAaFqtNSdUxBKaXqam5QWG6M+S82KHxijIkBAsErVpBUdh9VDTbr2UdKKVWXq5npbgTSgG0iUmKMSQSuD16xgqSqpVBcjDs6UbuPlFKqnua2FM4ANolInjHmGuA3wMk3R0SdloJ2HymlVH3NDQrPASXGmEHAL4CtwKtNrWCMedEYk2mMWdvI+2ONMfnGmJWVjweOqORHo9aYgt5TQSmlDtXcoOATEQEuAp4RkVlAzGHWeRmYeJg0X4hIWuXjoWaW5ejV6j6quqeC3S2llFLQ/KBQaIz5f9hTUT8yxjgAd1MriMhi4MTqn6nVfeR2JyJSTiBQ2rplUkqpE0hzg8I0oBx7vcIBoDPwlxbY/hnGmFXGmP8YY1IbS2SMudkYk26MSc/Kyjr6rdVpKdiZUrULSSmlajQrKFQGgteBOGPMBUCZiDQ5ptAMK4DTRGQQ8Dfg301sf7aIDBORYcnJyUe/xYgIMEbnP1JKqUY0d5qLK4BvganAFcA3xpjLj2XDIlIgIkWVz+cDbmNMm2PJ87CMqXVLTp0+Wyml6mvudQr3A8NFJBPAGJMMLADmHu2GjTHtgQwREWPMCGyAyjna/Jqt+p4KeqMdpZSqr7lBwVEVECrlcJhWhjHmDWAs0MYYswf4HZWD0yLyPHA5cJsxxgeUAlfK8TgV6JCWgo4pKKVUleYGhY+NMZ8Ab1S+ngbMb2oFEZl+mPefAZ5p5vZbTvWNdtoCUF6+77gXQSmlTlTNCgoicq8x5jLgrMpFs0XkveAVK4gqu4+czgjCwjpSVra1tUuklFInjOa2FBCRd4B3gliW4yMqCvLtDB0RET0oLdWgoJRSVZoMCsaYQqChfn4DiIjEBqVUwRQdDftsl1FERA8OHvxvKxdIKaVOHE0GBRE53FQWJ5/KgWYAj6cHFRX78PtLcTojWrlgSinV+kLrHs1QPdAMEBHRHYCysm2tWSKllDphhGZQKC4GbPcRoOMKSilVKfSCQlQUlJaC34/Ho0FBKaVqC72gUPvua+4knM5YDQpKKVUp9IJC1fTZxcUYY4iI6KHXKiilVKXQCwq17r4Geq2CUkrVFrpBoXKw2ePpQVnZDkT8rVgopZQ6MYReUKh19zWwLQURL2Vlu1uxUEopdWIIvaBQ1VIoLARqTkvVcQWllArFoNChg/27dy+g1yoopVRtoRcUOncGpxO2bwcgPLwzxoRRWrqllQumlFKtL/SCgssFp50G2+zUFsY4iYoaQGHh8lYumFJKtb7QCwoA3bpVtxQAYmNHUli4TM9AUkqFvNAMCt27V7cUwAYFv7+I4uINrVgopZRqfaEZFLp1g6ys6tNSY2NHAlBY+E1rlkoppVpdaAaF7nbK7KoupIiIXrhc8RQUaFBQSoW20AwK3brZv5VBwRgHMTEjNCgopUJeaAaFqpZCvXGF4uK1+HxFrVQopZRqfaEZFJKS7JXN9c5AggBFRXpqqlIqdIVmUDDmkDOQYmJGAGgXklIqpIVmUIBDrlUIC0vG4+lOfv5XrVgopZRqXUELCsaYF40xmcaYtY28b4wxM40x3xtjVhtjhgSrLA2qaimIVC9KSPgBeXn/IxAoP65FUUqpE0UwWwovAxObeP98oFfl42bguSCW5VDdutl7NWdkVC9KSpqM319Efv6S41oUpZQ6UQQtKIjIYuBgE0kuAl4VaykQb4zpEKzyHKLeaakACQnjMSaMnJyPjlsxlFLqRNKaYwqdgNp3ttlTuewQxpibjTHpxpj0rKysltl6A6elOp1RxMePJSdnfstsQymlTjKu1i5Ac4jIbGA2wLBhw+QwyZunRw8ID4fly+Hqq6sXJyVN5vvv76K0dGv1vRaUUie+igrIzrZ/AwGIi4P4eMjPh337wOsFtxvCwuxfEXtX3uJiKCmx60VH19yc0euFnBw7I05EBCQn23zz821ap9M+XC67vLDQ5uN22+UlJfYRGwtt2kBZmS1faSn4fDZdUpJdPysL8vLscp8P/H5bvrZtoX17u9727TB+PFx6aXA/x9YMCnuBLrVed65cdnyEh8OZZ8LChXUWJyZOAu4iJ2c+nTvfedyKo9Th+P01lVhFhf0Ku1y2sikttY+KCnsfqfbtYccO+OYbW4k5HLaicjhqnlflV15u83K77euiIvB4bAWZnW3zCQuztyIJD7eVV9WjosJuKy7O3rdq/357xrfLZfMqKLDrxsfbZVWVXu3KLxCo2ZeCgpq88/Pte1Xlrv+39nOv11as0jKHjK3G5ap5QPX0bID9DDs12JfSwmUI/iYa9QHwU2PMHGAkkC8i+49rCcaNgwcesIcDSUkAREb2JCKiNzk58zQohICcHFvpORy2YvL7bSVZUWEfVc/rLxOByEj749271x6JOhy2AiwvtxViVJQ9uszKgpUrbUUXF2ePOqvyauxRtT2oqfRKS5u/X8a0XAXZvr2twLOz7Wun01ZQ8fH2+X//ayvz9u1rKi2fr2b/Kyrs+Rx+f91Kz+m0wccYu7/l5ZCQYIf74uPtZ+Vw2MBQFTyq/tZf5nRCx462DOHhNs/8fMjNtfl07GiXe722PF6vTRMVZR+RkTVBsbjYvud02mohOdl+9llZdllcnP0/+/01gc0YiImx+VQti4y0/+uCAvvZRUTY/KKi7P6Xl9vvn89ntxEXZ/OpragIDhywLY34+Jb5fx5O0IKCMeYNYCzQxhizB/gd4AYQkeeB+cAk4HugBLg+WGVp1Lnn2qDw+ed12mRt217Jzp1/oKRkM5GRvY97sUKBiG1uR0fbH35Zma2cs7LsDzk3Fw4etO8lJtofbG6ubY5X/SB377YVcnm5/ZH7fPZv1aP+a5fL/rDCw+2Pcd8++7clREfbfao6go+MrKlgPB4YONBWmPt86zjgLSCxbBThYYbISLs/DT3cbrv/fr99HREVoCJyG3lh6yg2B3D7Y3EGogkPcxDviWVQwtm43Yb9++1nc9ppkDqkgBzXajYdXE+MO4HRHScS4YghEAAhgNd1EIfbS6K7AxUVNZVkeTnkF/hJTHAQEWFrqqpuj+joupWXP+BnXeYGBrRLxRhDma+M+Vvm0yW2C0M6DMHpcNb5rPwBP/sK92GMoXNs5+pl+4v20ymmE6ZW5msy1vDuhndJiU/hitQriHBHVL/n9XtZn7Webbnb2F9kjycFQ7kxGAyRxkGMw0n/tv0Z0mEILocLEWFVxire3fAu2SXZpLVPIyGhBz6gXPz4A36iHC4GthtIu+h25JXlkb4vnWyTTVF8ETFhMbjjulBunGSXZON1e0mMSKTcV86Hu79if9F+bh56M0M6DKner4hoH/HtylmyawnPLf+YMl8Z7aPbkxiRSKQ7kkh3JBH7IojMiiTCHUGkOxKPy4PTONlftJ+MkgyG+4cTz/Gpi4ycZO2tYcOGSXp6estkVlFhD02uvx6eeabW4gy+/rorHTrcSO/ez7bMtk5y+fmwfF0e3+xYw/C2ZxMRYSgstEdB+fm2gne57NFQeTnk5gp5+ZCfZ6q7AwoLoTRQQJ7sImNtKhXlBpcL4hL85DjXQsdlEFYIRiBjIOwYCwEnJGyHmL1QHgd+N0TmQNwunClfEdZxE3HZPyDpwJVE+NvjcvsIRO/HF7WTGP9pxAd64g/PYkv7R8lzbyAq50wiDo4kOaoNXeK6MKhnMj16wP7yrfxh2yT6R43lqk6/I0e28n7GkxT5c+kclYKXUjbmf4cxcE3fmzir07n8b/unrMleTp923eiW1JH/bf8fC7YtoFdSL87veT5DOwylQ0Q3DpZn8NXeL5i3eR7rs9YDMDZlLL8b8zsGtx+ML+Dj2WXP8sqqV8gvzycgAa4deC0Pn/sw4c5w3tv4Hv/e+G8+3fYp2SXZjf6PfjL4Jzx3wXO4HC7S96Uza9ks5qydQ5mvrDpNuDOcnok9yS7JJqski4AEAEhNTmVKnykM6ziMTjGd+Neaf/HSypco95fTNqotIzuN5JqB1+AL+Hh55cs4jIPnL3ie5Mhkrn73at5e/zantzmdC3pdwOtrXq+upOM98dwx/A5+O/q3HCw9yO3zb2f+lvlU+G0zaFC7QfRN7suCbQvILsmma1xXxqWMI788nw1ZG9iUs6m67IkRifywxw/pk9SHfYX7eGfDO+SUNi+qx4TFEBseS15ZHsXeYhzGQXRYNAXlBY2u0y6qHZnFmQjNqyMNBo/LQ6mvlHEp4zhYepC1mWvx17p5V6Q7kpiwmCPKt0pa+zR+ecYvuXrg1YdP3FD5jFkuIsMOmy6kgwLAxIn2sGrdujqLN268kczMNzjjjN243Uktt70W4gv4KK4optxfTpvINjhMwyeSVfgrWJ2xmvUHtrI9IxuHLxqpiCCvuISckjxC0P11AAAgAElEQVR2FG1gf9n3xFb0JSlvAuwfSuG+DpSWOBCxzdfMTCjs+AFccCvE7Ict58NHz0Jeit1IwlYY8wcoi4fdZ0CHFTDwnxCZg7O0PeGlKcSWpWI8RWQmzcXvLKGrfxxjI37GmtJPWGfeoMKRf0jZkyPaEu7ysKdwV4P7Fh0WzWlxp7Eua12D7wP0S+7H7vzdFHuL6Z3Um43ZG6vfczlcvHbJa1za91LOfvFs1metp8JfQUAC+MVPcmQyvZJ6sSNvB26Hm8EdBpNdks2SXTXXsaTEp7CvcB8V/gq6xnVlYo+JbD64mSW7luAL+KrTuR1uzuxyJlekXoE/4OfhLx4mszgTAIdxEJAAE3pMoGdCT3JKc3hz3ZucFncaFf4K9hftp11UOyb0mMDo00YzsN1AOsZ0pLC8kKIK2+n87oZ3eezLx5jQYwJ5ZXl8u/dbotxRXDvwWi7sc2H15/DuhnfZkb+DtpFtaRvVluSoZLx+Lx9t+YjFOxdXV2Buh5srUq+gc2xn9hXu45Otn1SXt1NMJ/LL84kOi2ZA2wF8uu1Tbh92O+n70/l277eMTRnLr878Ffnl+by74V3eXv82fZL6kFWSRYm3hFuG3kK/5H4Ulhfy/qb32XJwC+O7jWdIhyEs2bWEJbuWkByVTPeE7kzsMZGpqVNZm7mW2ctns3TPUnbl7yLSHclFp1/EBb0uoE+bPnSM6Vj9OYoIgiAilPvLWbZ3GYt3LqbMV0acJ45+yf2Y0mcKyZHJbM/bzs68nTgdTpzGicM4KPOVsWL/CtZkrqFnYk9GdR5F59jORLojKSgvYHf+7urvh9vp5mCpPfN+eMfhADz9zdP8c/U/6Z7QncHtBxMbHovDOEhrn8aYlDF4XB58AR+F5YWUeEso9ZVS4i2xz732ebm/HK/fS/vo9iREJPDZ9s94a91bTO8/nTtHHl23tgaF5vrTn2DGDNtx165d9eKiorWkpw+gW7dHOO20X7fc9ir5A36eS3+OcSnjSG2bWr08qziLWctmsadgDynxKfRO6s2wjsPoHNuZ3fm7Sd+XzmurX+Pj7z+u/gFHuWLoHjUQlzeR4iIH4UW9iM2awAHfJnZ2eQxfZBPj96UJcLAHJK+HsBIAHH4P4b52uP1xGIfgC8uh2LGPFM9AJnS9jNe2/hl/wM+ZyZM5PSmVV7c8gTF2n8r8ZTiNk/N7TqZvch8OFB1ga+5W1mauJSABpvefTs/Envzlq7+QXZKNx+Xh8n6XM7HHREZ1HkWbyDYEJMDCHQt5a91b+MXPuJRx9ErsRUF5ARX+CpIik2gf3Z5+yf1wOVxsPbiVeZvnUeotxWEctI9uT5e4LqzNXMsHmz4gMSKR34/9PX2T+1YfveWW5vLXpX9lya4ljO82nk+3fcrbU99mSIch/O2bv9EjsQc3DL6BSHfkIR/ZygMrWXVgFed2O5cucV3wBXxkFGXQMaZjdddHcUUx3x/8nm2520iISGBEpxF18iosL2TBtgV8f/B78sryuGrAVXW+B1/s/IJ7P72XhIgEfjr8p5zf6/xGA3+Vmd/M5K6P76JPUh/uGH4H1w26jjhP3OG/jJWKK4rZkL2BrQe3Mvq00XSIqblsyBfwsXD7QowxjEsZx4bsDVw852K25m5l1qRZ3D78dgByS3NJiEiok+/H33/MLR/eQvvo9rx68av0adOn2WVqSNX/OdwVfkz5nIxEpE732pHQoNBcy5bBiBHwxhtw5ZV13lq16ocUF69m5MjtOJ2eo8q+zFdGbmlunR8YwIebP+TCNy7EYRzckHYD3RK6sTF7I3PXz6XMV0ZieDI55ZkN5ukq7gzrr8B3sBMEXJC0GdqthrAicPigzSZw2uZ5UtE5DPb+lL7JfenRvi3uqGJcnhKSYiNpFx9Ll6QkYmIMEdHlfLPvazZkbWBb7jYySzLJK8vDYEiKSGJgu4HcNvw2wpxh7MrfxZ+//DPvbHiHA0UHmNBjAi9MeYG2UW1ZnbGarnFdaRvVtk6ZRQS/+HE57DBWQXkBi3Ys4pyu5xxSiRwvxRXFTP7XZD7f+Tk3DbmJ2RfObpVytKSMogzaRrU96orjSOSX5bM1d2t1/3lT/AE/DuM4LuVSDdOg0Fw+nx3av/RSePHFOm/l5n7GqlXj6dXrWTp1uu2wWYkIf1/+dwrLC7lxyI3syNvB9Hems6dgD8tuWka/5H7Vaae9dTUff/8fRkVey4K8ZwkYH2EV7XBtP5+ST++D7NPBXQJtNkKH5RCzjzjTlS4RfTg9eiSdOznp1MmeVRETY/vyu3SxZ254KWbxzsXEeeI4s8uZLfdZ1ROQADvzdpISn3LS/tiLK4qZu37uIYOYSp1qNCgcienT4bPP7EnWjpomuojw3XdnUl6+n5Ejt+BwuKvfK6ooIjosuk7aez+9lye+fgKwA0pev7e6zzYhrB13R33LV59HsOSbEnZMbQtrroJ5s3FE5tGhnZvObaPo3dueqdKhgy1KTIy9+DolxZ7RopRSR6O5QeGkuKI56KZMgTlz4NtvYdSo6sXGGLp2vZ/Vay7kk7UPU+oexMbsjby74V2W71/Okz98krtH3Y0/4OeWD2/hhe9e4I5hP+UMz0946stnyM110OnLP7IuL53N50/k9k9/Qdtlz9Ltgg/ZEVbM7y6bztRHoVeveMLCWnH/lVKqkrYUwJ4An5wMv/oV/PGPdd4KBAJc/FIy8/bUzO03stNIwl3hfLHzC+ZcPoe3181l7oa3GVb8G3a9+hCZGbYrpVs3ezFNSgpkDb6XBSWP86sz72NTzka+3fstu+/Zfcg53EopFQzaUjgSCQkwejR88AF5D/yKn87/KVklWTwx4QkWbFvAvD0HubwT3DjsHob1/jVtIttQ4i3h7H+MY9rcaTaP//6FtSt+yQUXwCWXwA9+YOctqeIPPMad/ynmz1/9CYC7R96tAUEpdcLRoFBlyhS+efwepj3Tn72lGcSExZD2fBqCcMnpl/Cbvj7y8/5OlLmT9evbMHt2JOtfmweTryXNeSX33nE9F15oxwAa4nQ4mTVpFh1jOvLYksf4cdqPj+vuKaVUc2j3UaWdqxYzaM4YEiITmXP9fHom9uT+z+5nR94O5l4xF6c/jyeeuI9//etBNmzohdMJ110Hv/lNzSzczeUL+KpPzVRKqeNBzz46Av6An3GvjGPl1i9Z+b9edP9yfc1MXKWlfLc5ijvvhC+/hK5dN3D99RncdtvY2te6KaXUCa25QaE1b7JzwnhsyWN8sesLZnW6me5LN8Hbb4MIpVfdyIwOLzN8uLBlCzz/fID33/8NY8f+gLCwhYfPWCmlTjIh31L4du+3nPnCmUxNncq/LnoNk5YGXi/rr3yIKx5KZR39uWHiPh7/V0cSEsDnK2TFilFUVGQwbNgKPJ6uLVYWpZQKFm0pNENRRRFXvXMVnWI78dzk5zAuF/zhD8zZPJhhD11IprsT/wm/mBe6PUxC5UwMLlcM/fv/GxEv69dPJ/DZ/+z0n0opdQoI6aBw13/uYnvedv55yT+J99g7WHzkvphr+CfD3KtYtczLxAtc8N57dnyhUmRkL3r3/ju+NV/hGP8D+H//r/kb3bsXbrlFA4lS6oQUskFh6Z6lvLjyRe476z7OOe0cwN66cOoVhrTBho8296bDoLZw2WV2BtWvv66zfrt2V9Jj8QAA5JUX7U0FmuOee2D2bDtqrZRSJ5iQDQq///z3tIlsw/3n3A/Ym8Vcdpmdc+ij/ziJSam8h8Lkyfa2V++8UzcDr5fEjzIoTQnDFJXi+7+/HX6jixbZQWyAXQ3fI0AppVpTSAaFb/d+y8fff8wvz/glUWFRgL3eYN8+O4N2nVNNY2PhvPNsUCiruYMVH3+MyciER/9Cfn+D/+lHEb+PRvl88LOfQdeu9kavGhSUUiegkAwKv//89yRFJHHHiDsAe0uFZ56BO+6wt1Y4xE9+Yivxc86pqcxfegnatiXistvw334D4btLOPDydBo9m+vtt2HNGnjiCejcGXbuDM7OKaXUMQi5oLA5ZzPzt8znnlH3EB0WjQjceqvtNnr44UZWuvhi+Pe/YfNm6NcPTj8dPvjAXtLsdpNw47P4kiNxvTKXTZtuIBCoODSPxYttq+PSS21rQVsKSqkTUMgFhU3Z9kbg5/U4D4AFC2DFCnjkEYhr6s6FF11kmxRXXWVveHDJJXCnvVeqCQvDefmPSFrmInPHy6xZcyF+f3Hd9dPTYdgwe6W0BgWl1Akq5ILCnoI9AHSJ7QLAU0/ZMYTp05uxcu/e9syht96y3UFday5cM5ddhqPMR/99d5Cbu4BVq36I15tn3ywvh1WrbFAAu96ePeD3t+SuKaXUMQu5oLC7YDcuh4u2UW3ZvBnmz4fbb4fwY70H+OjRkJhI4qIC+vV7k8LCb1mxYgSZmW8ha1aD11sTFE47zQ48HzhwzPujlFItKeSCwp6CPXSM6YjT4WTmTHu26S23tEDGbjdceCHMm0fbhIsYOPBjjHGzfv00dr071aYZPtz+rWph6GCzUuoEE5JBoXNsZ4qL4eWX7RBBi812eumlkJcHL7xAwj2vMvy1czn99FcJX30AbxwcCP/cpqsKCjquoJQ6wQQ1KBhjJhpjNhljvjfGzGjg/R8bY7KMMSsrHz8JZnnABoUusV346isoLoYrr2zBzM87D6Ki4Lbb4NVXMc88Q/ut3Wm7uwel/RLYuOnHbNjwI3wdKidS0qCglDrBBC0oGGOcwCzgfKAfMN0Y06+BpG+KSFrl4/+CVR4AEWF3wW46x3Zm8WJ7DdmZZ7bgBiIi4NFH4d574fvv7f04Z8zAsW4TMeNuJSXlQTIy/snyLWMJxEUj2n2klDrBBLOlMAL4XkS2iUgFMAe4KIjbO6yDpQcp85VVB4XBgxu/feZRu/NO+POf7e3Y7rsPliwBvx8zYiQpKb8jLe0zRPyUJBVRsPYtCguXt3ABlFLq6AUzKHQCdtd6vadyWX2XGWNWG2PmGmO6NJSRMeZmY0y6MSY9KyvrqAtUdTpqu4jOfPONPWEoqG69tWbAovLMo/j4MYwYsQFn9/649uayfPkwdr4wjuLtnwe5MEopdXitPdA8D0gRkYHAp8ArDSUSkdkiMkxEhiUnJx/1xnYX2BhVsKcL5eXHIShERsJf/2pn2uvYsXqxwxFGRJ8xROZE0yfjWk77ySIYN5bVi87g4MEFQS6UUko1LphBYS9Q+8i/c+WyaiKSIyLllS//DxgaxPJUtxS2rewMwNlnB3Nrla66CubOBWPqLu/aFZOXT4c/rEDaJhO530m3n33H2m/PY/Pm2/H5at1v4fXX4cYb7bUOqmWJwHff2b9KqaAGhWVAL2NMN2NMGHAl8EHtBMaYDrVeTgE2BLE87CnYg9M4WflFe1JTISkpmFs7jKrTUtetw/z1Sczrc4heW8GwR1I4sO05lixJYNm3A8n+5ZlwzTXw4ovw9NOtWOBGbNx4cleon30GQ4bAJ5+0dklOPhUVegbdKShoQUFEfMBPgU+wlf1bIrLOGPOQMWZKZbKfGWPWGWNWAT8Dfhys8kDNhWtffekMftfR4VQFhREj7Bwbl1+OefFFIr/axZkPD6bv5kvp86sM2jzxNRnnQsE5bQg88GuK1n/U+EysVURgzhw7HUcwvfQS9O0Lzz575OuWlMALL9gru1vTf/9r/86f37rlaMiKFXAMY2hB99BD9v9fUHD4tJdfDg88EPwyqWMnIifVY+jQoXK0zn3lXBn49BkCIv/611Fn0zKKikTOP19k+fK6y+fMEXG5REAkOVm8v/uVbPv+t7LivRTxeZCc4cim5/pKzqInJbBvr0hBgchHH4ncfbfIH/8o8r//iUyZYtcHkV/8QsTna365fD6Rp58WeeghkTVrRAKBhtPt3i0SF2e30b69SHHxke3/fffZdV999cjWa2nDhtlynH5665Whoc84L08kIkJk6tTm5+PziTz7rEhOTsuVrTF+v0iXLvaz++ijptN+/71N5/GIZGUFv2ynqlmz7G/yKAHp0ow6ttUr+SN9HEtQ6P233jLy8akCx/TZBt/ixSLvvSdSXl5nsfcvv6+p7Os/wsPrPn/iCZE777SvL7lExOs9/HZzckR++MO6+Y4cKbJ6dd10gYDIpEm20nrlFZvuT39qOu8FC0R+8xuRigqRXbtsBQEiI0Yc4YfTgvLyRBwOkXbtbFl27z6+29+0SWT6dBtcv/yy7nuzZ9syud3Nr0jff9+uM3ly48H8cNavF5kwQeS775pO98UXNd+RX/6yZnl+/qFpH320Ju0f/nB05Qp1W7bY7+qMGUedhQaFegKBgEQ+EilnPHiPGCNSUnJU2bS+zZvF/78FkvePu2XHr7rI1huRlX9C0r8cLDuW/UJK5jwtgc2ba9I/+aT9N//kJ41XFCUlIs88I9K5s62E/v53kf377bLkZLusdqX/0ks2z6eftq/PP18kMdFG2szMQ7ezfLlIZGRNgLr6apGwMFuZgMg33xz757Jkicjvfidy4YUiH3/cvHU++MBu/29/s39ffNEGrRtvFPnvf4+9TE3597/tjzwy0galrl1FDh6sef/ss+1nX/tzPpypU0WcziNbp7bvvhNp08auP31602lvv90G9qFDRYYMscuWLLHb//DDumnT0kRGjbLfk3btREpLa97LzBR5552a70wgYA9C/P6Gt1tcLPLCCyJvvXXk+5eRYb/Tn39+5Ou2tttus7+ZffuOOgsNCvUcLDkoPIgMueOv0rXrUWVxwgkEApKX96Vs3/6gLF9+lixcaGThQuSLLxJl+fKzZMuWX0h+/jcSuP9++6++7TaRmTNF7r/fHg22aycSH19z1H7GGSJff113I1lZIpddVlNp7tljj2zPOafmh7t8uYgxNUeDo0fbo2ARkZ07RTp2tJXe72u1dH75S3tUGR0tct11x/ZBLFhg83Q4bNni4myXRSBgK5C3365J6/PVHHnfc49tVZWU2C6w6dNrjmpPO02krKzxbTanS87vt91k//jHoe+NHi3So4etqL791nYZXnKJLXNVd8ujj9pKd9Cgw28rL8/uyx13iFxwga1AVq06/HpVtm6134UuXWz3o8cjkpvbcFqv1wasqVNFHnzQ/u8PHhS54gpb7v79a74bmzbZZU8+WfN/euEF+14gYL+HYL8bgYDIvffa15dfXjd4iIj89a+2jFXfoVdead6+BQL2oKiqW7ZLl+a1nI/Wxo22BTx5su3WnTXL7vuRbLOsTOQ//7F/MzPt/+PGG4+pWBoU6ll1YJXwINLjwrfkBz84qixOeOXlB2Tv3r/Lxo23yIoV58iiRWGycCHy9Vcpkn/5gJofk8NhK5obbrBdTPfcI7JoUeMtCa9XZPx4W+mMGGG7jbZsqZtm5UqRf/5T5JFH7A83PFykXz9bYURH11RQzz9vg09Vv/ftt9u0Dz8s8sADIlWtnEDAVhQ//7ntbqptxQpbYZaV2XTDhtmgk5cnsn27SEKC3b+qYOZwiHz6qU0/aZLd3oIF9gh23Dib57XX2mDi8Yikptr1Zs489LMoKbFp4+NFXn7Zbn/XLltB1e/mqRo3cbns51Nl61a7/JFHapY9/rhddtVVdp+Nsd1Zs2bZ5StWNPp/FxFb0YLI0qW2EklOtoG7qW6k2u/dfrsNJNu2iSxbZvN6/nn73nPP2YA5caLd91//2r7/7ru2q7MqrctlAwLY74KIHZuq6poLBOz/pWNH+3+aO1eqx3NA5Lzz7N9zzqk5uKgKTFXdVeedJ/LZZ/b76HTaFsPhuspee02qW8tPPFFTdhEb3BsKfh9/LHLuufbzPBKBgP1OxcTYfa1qIYPIj37UvDw+/likd2+7ztChNhiAyIYNR1aWejQo1PPR5o+EB5Go07+S228/qixOOhUVubJv30uyatVEWbTQKV++jaxcMFz2bntWMjLelszM96SsbG/zMsvKskfPIPLUU02n3bfP/gAmTbJ9yOvWNZ52wwZbGVX9cNq0EUlPt4EKbOXodtsjx5kzbYXpcNj3Lr1U5I037POXX67Jc/58u57TaQffU1Nt99b559u0nTuLREXZNFV93K++at+LjRXZu1dk7FiRtm1t8HvoIZFbb7VHu0OH2vWqAkf//jXlSUy0FejixTZfsK2gdu1EBg+2XVMitpvLmLrBLhCw26nKq+rI5eBBG8Quuqjpwfxx40R69qypIJ97rm7lV1t5uT0gqGqp5OTYyuv662vKkppqx5OqglWXLjb4JiXZ13Fx9ki+rMweJFSddLBhg60Mu3cXef11+505++yaba9ebYN2jx72/zBwoM1n8mS7/i232FbGG2/Y//uQIfb71Levzauw0OZTUCAyfLhdp0MH+/9paEwoN9d+/iNG2Hy9XnsAce65dj+vuMJu5+abbTB85x2RadNqvo89etiTQhqSn28Ppt56S2TePBtgXn/drvfcczWf5Z49tsXQ0KB8eXnNPvl8InfdZdP16mW7bBMS7OspUxr/3zeTBoV6Pt/xuZz74kQhet9h67RTUUVFtuza9aQsXdpTFi6kzuObb/rJ5s0/k6ysD8TrLWw8kw0bRP7858b7e49WSYmtGDZvtj9Yt9t+Ne+8U2THDpGf/cxWIFU/1FtusUfZVUfhqamHdud88IHtlhGx3TFV3Q5PPWXHS3r2tK+XLLFpMjJsQKjq2vjqq5rtQc36MTE2b59P5C9/sZXajBn2rK8xY+quM2mSrYTeece+njHDrtetmz3SbciSJbYCqz2mUbWvvXrZvHbsqKn8i4tF3nzTBpkHH6xZx+u1FWnPnnVPWMjKstuu+uymTBF57DH7unZ3U1UwANtNVPX5+ny2i7F2y6fqCP/cc+3rDz+sWTcpyXaD1Pb11zYo1/78S0vtPtf+bs2fbwNibGzDFWphoT0YuOIKmy4y0rYuV660+5+ZaSt7h6PuWX5VXYRVR+Dnnlv3wCQ83AbuTz6xr3/607rbzcqy71cFwqpH7972QGL48EO/j2VltuXcubPIgQO2pfToo7bb0u22LbFJk2w+d91V03W5c6f9vh9jK0FEg0KDlixp+LsVSgIBvxQVrZPCwtWSn79Udu78i6xc+UP5/PMIWbgQWbQoXFavvkC2bLlbNm68RbZvf1AKCpZL4GjPZjlSu3fbSvGuuw7tFti5s6Z7ScT+MMEepR3O8uV1xxZ277Zn+NSuhOr/kB94wLZMtmyxZcnMtF1UjQkE7JHjp5/a7p7a+V17rS1rv37272uvHb7MtX32WU1LrapCr+ruAtsls3Nn3XWqKufBg23F3adPzbqvvFLTlRIeXlOhVzlwwOY9Zsyhffv1/fGPNp+qzzcQsAPpy5Y1fgDxzTc1XUxN+fRT2xK56qqm023bZsdkqj6fqgH3hir1zMyas/WuvLKmC/DVV+2BRO2zUKqO3C+91Lb6hgypGT+75BIbuFavtvs+aJCt4Jcta7iMS5fWtASrHhMm2PLFxdn3Zs06/GdylJobFIxNe/IYNmyYpKenH9W6L70EN9wAW7ZAz54tXLCTXCBQTn7+l+TkzCM7+wO83iwcjgi83mwggMfTg06dbqNNm4spLd1GRcU+4uLOJiKiR+sWPCsLjmE+rONGxN7V6d577QV7e/fae28cidJSSE+H9evtXfuKi8HlgkmTYMwY+7z+Nn/7W/jiC3uf8LZtYeRImDwZ0tIgELD3APnsM5g3Dy64oO76O3dC+/aHv1dtTo7dt7vuOrQMLSE7G+Ljm5f31q2wdCmsWWMnozz9dJgwwc6TX9uvfgXffgsffdT0/6GkBK67Dtats89TUmD8eLjkEhgwoG7aQAByc5ueKmH+fFi50v4vhg+HQYPs8uJiyMiwsysHiTFmuYgMO2y6UAoK/+//weOP299WML67p6KKiixycuZx4MDL5Od/ccj7ERG9iIoaSERED9zuNjidMcTHjyEqqm8rlPYkkJsL+fm2cjkRZGXZq7qnTwdHa8+PqYJJg0IDLr8c1q610/WoI1dYuJKCgqVERvbG7W5LXt5CcnMXUFKyibKybYjUTNiXkDCBuLizCQTKcLniiYs7k8jIfoj4cTjCcbla+kYWSqmmNDcohNTx8ubN0Lt3a5fi5BUTk0ZMTFr16+jo/nTufCcAIgH8/hK83mwyM19n795nyc39L8a4sNNg1eYgIWE8SUkX4PPlU1Gxn5iY4SQlTSYsrO1x3COlVH0h01IIBGzX4e23wxNPBKFgqg6RACIBHA4XFRVZFBR8RWnpNoxxUVGxn8zMNykr2waA0xmD318IGGJiRpCUdAEJCeOJiRmC15tLTs77BAJlJCdPIzy8fXX+xmh3h1LNpd1H9ezaBaedBs8/D7fcEoSCqSMiIpSX78btTsbh8FBUtIqcnA/JyZlHYeG3ABgTVtklVfUddRIV1Zfy8r0EAmW0b/8jOnX6KWFhHQBTGSQcOJ3RmPr3r1AqxGn3UT2bN9u/2n10YjDG4PF0rX5d1TWVkvIbKioyyM//koKCpTidMSQnX4oxLg4ceJmiotXExY0mEChh//6X2Lfv+UPyDgvrQGzsmRjjoqxsGy5XHAkJE/B4ulFauhmvN5uwsHZ4PN1JSpqM0xl5PHddqRNayASFQAAGD9agcDIIC2tHcvKlJCdfWmd59+6P1nv9GNnZ/yYQKLMX3RBAxEtR0WoKCr7GGCceTzcqKvazbduvqtdzOCIJBEoAcLkSadfuGlyuWPz+UjyeLkRGno7D4SEQKMPjSSEiore2PFTICJnuIxXaysv3UlGRQUREL1yuGHy+IgoL09m3bxZZWe8BgsMRRiBQdsi64eFd8XhS8HqzCAQqcLvbNPFIwhg3AF5vNuXle4iM7EN8/FgNLKpVafeRUrWEh3ciPLxT9WuXK5qEhLEkJIwlEPBhjL24yevNoqRkEyI+jHFTXLyW3NxP8XqziYzsh8Phxus9SFgCfdwAAAxpSURBVEXFfoqL1+D1Zle3OpoSHT2EpKQLCARKAcHtboPLFQ/YQXMIYMdMUomJGYLTeYQXttUjIhqE1FHRloJSx8ieipuD15uN15tTPTjudicRFtaRgwc/Yffuxykt3YTD4amcTqC8iRwduN2JuFwJiPjw+fJwueKJiupPRERvwsM743B4KC/fhc+XS1hYJzyeLjid0QQCXrKz3yUn50Pi48+ld+9n64zdqNClZx8pdQKxgcCPw+FCRPD7i/H58qrPmDLGQSBQQXHxKgoL06moOIDXm4vD4cbpjMXrzaG4eC1lZVuru7iMceF0xuLzHayzLbc7mcTEH5KV9S5gSEw8D4cjCpEKvN4sRASPpytudxJ+fwmBQDkORxgOhwe3uy1udxJlZTspLd1EbOwZdOx4K+AgJ+cDAJKSpuByRVdvr6Iii9LSrcTEDMbhOMyUGKrVaFBQ6hQkIvh8ufj9JYSHd8AYJ35/KRUV+/D7ixHxERU1AIfDTWnpDrZtu4+SkvX4/cUYE1Z5caBQVrYLn+8gDkcUDkc4Il4CgVJ8vjzAng4cHt6FsrKtOJ1xle/bbjKHI4rY2JEAVFQcoKRkffXyuLizgQA+Xx5RUQOIjx+Dz1dAUdEK/P5inM4o3O5kPJ5ueDxdcLmScDoj8fuLCATKcDpjcLuT8HhSmrwORSRASckmKioy8HhSKltP2hveFA0KSqkjFgiU4/Xm4Ha3xeFwUVCQzt69T+NwRNGu3TUAZGS8RnHxasCJ6/+3d+/BcZVlHMe/v91NNvekpRe1lLZAB7kM90sFEQRHARnKODhWEVGZ4R8cQZlRKl4G/mN0RJ1BLgNIwQ4wIGiHGRUoTB2QWym00FJKuZS2trTSNmmSppvdPP7xvlk2adLG0uScJc9nZid7Ltk8eZKzz573nPOcXCutrWdSX38YO3Y8TXv7M2QyDWSzjXR2vkKxuB0Iey+53ERKpc64t1LYaxy53EG0tp5OsbiD7u7VQIba2slIecwK9PS8T6nUXl4/k6mjufkUWlrmkM8fQi7XSlfXSrq6ltPQcCSTJn2NXK6Fnp51lEqdgKipmUhj47HU1k4tH3/56Cy2EuHkg0/Ono8XBedcosxKdHWtIpebQD4/reKNt4/du/9DobCR3t7t9PV1kc02kcnUUSzupFDYTEfHs7S3P0dNzaTYXFEUClswK5LJ1FJbO5Xm5tPI56fR07OO7u6VtLf/m87OZeW2KlINDQ1H0N29Zq9FKBzn6S8EpQHLcrmJ5b2WUqkr9u06KJ6pVsCsQF9fgUymhvr62dTVzcCshFmRXK6VTKaR7u7VdHUtJ5OpI5+fQV1deJRKnezc+TKlUgdNTSfQ2Hh0PAFhIjU1E5Fy7NixhPb2Z2luPolJky7+WNfUeFFwzo07ZqV40H8bdXUzyWbrKBY72LbtcUIL+Blks62AUShspqtrBT0965FySNkBX8HYvXsDPT3vASKbbaSvr4fe3g9jIciTydQi5enr28WuXWsoFDbHSLL0F5hstpmmpuPo6yvQ07OO3t4PyvGGzsIt5ZYvQwuvlc02MXPmDUyf/uP9yo2fkuqcG3ekLLW1UwY0VszlWpgy5ZI91m1sPJIJE754QH9+/+nNkiiVdlEqdVBTM3nA8ZFSaRe7d68nk8mTzx+CJIrFdrq711Asbqe3dxvF4jZKpW5aWk6jpeU0OjqeY/Pm+8jnpx/QeIfiRcE55w6QyoPd2Ww92Wz9Hutks/U0NAxsrZDLtdLScsqwr9vWdhZtbWcduED3YlTbTEo6T9KbktZKum6I5XlJD8blL0iaOZrxOOec27tRKwoKl4jeApwPHAV8U9JRg1a7AthuZocDNwM3jVY8zjnn9m009xROBdaa2TsWDv0/AMwdtM5cYEF8/jBwrvzafOecS8xoFoVpwPqK6Q1x3pDrWDiPrB3Yy12vnXPOjaaquHWVpCslLZW0dOvWrUmH45xzn1ijWRQ2ApXnTx0c5w25jsKJwa3Ah4NfyMzuMLOTzezkyZMnj1K4zjnnRrMovATMljRLUi0wD1g0aJ1FwOXx+SXAU1ZtV9M559wnyKhdp2BmRUk/AP5JuCTvbjNbKelGYKmZLQLuAu6TtBbYRigczjnnElJ1bS4kbQXW7ee3TwL+ewDDGU0e64FXLXFC9cRaLXFC9cQ6WnHOMLN9jr9XXVH4OCQtHUnvjzTwWA+8aokTqifWaokTqifWpOOsirOPnHPOjQ0vCs4558rGW1G4I+kA/g8e64FXLXFC9cRaLXFC9cSaaJzj6piCc865vRtvewrOOef2YtwUhX218U6KpOmSnpa0StJKSVfH+RMlPSHprfh1QtKx9pOUlfSKpMfi9KzY+nxtbIVem3SMAJLaJD0sabWkNyR9Lo15lfSj+Ld/XdL9kurSklNJd0vaIun1inlD5lDBH2LMKySdmHCcv45/+xWSHpXUVrFsfozzTUlfGas4h4u1Ytm1kkzSpDg95jkdF0VhhG28k1IErjWzo4A5wFUxtuuAxWY2G1gcp9PiauCNiumbgJtjC/TthJboafB74B9m9lngOELMqcqrpGnAD4GTzewYwoWe80hPTu8Bzhs0b7gcng/Mjo8rgVvHKEYYOs4ngGPM7FhgDTAfIG5f84Cj4/f8Mb5HjJV72DNWJE0Hvgy8XzF7zHM6LooCI2vjnQgz22Rmy+LznYQ3rmkMbCu+ALg4mQgHknQw8FXgzjgt4BxC63NISaySWoEvEK6ax8wKZraDdOY1B9TH/l8NwCZSklMz+xeh20Cl4XI4F7jXgueBNkmfTipOM3s8dl8GeJ7Qf60/zgfMbLeZvQusJbxHjIlhcgrhnjI/ASoP9I55TsdLURhJG+/ExTvPnQC8AEw1s01x0WZgakJhDfY7wj9uX5w+CNhRsfGlJbezgK3An+JQ152SGklZXs1sI/AbwqfDTYT28S+Tzpz2Gy6Had7Ovg/8PT5PXZyS5gIbzWz5oEVjHut4KQqpJ6kJ+AtwjZl1VC6LTQITP01M0oXAFjN7OelYRiAHnAjcamYnAF0MGipKQ17jePxcQhH7DNDIEEMLaZWGHO6LpOsJw7QLk45lKJIagJ8Bv0w6Fhg/RWEkbbwTI6mGUBAWmtkjcfYH/buJ8euWpOKrcAZwkaT3CENw5xDG7dvi0AekJ7cbgA1m9kKcfphQJNKW1y8B75rZVjPrBR4h5DmNOe03XA5Tt51J+i5wIXBpRQfmtMV5GOFDwfK4bR0MLJP0KRKIdbwUhZG08U5EHJO/C3jDzH5bsaiyrfjlwN/GOrbBzGy+mR1sZjMJOXzKzC4Fnia0Pof0xLoZWC/piDjrXGAV6cvr+8AcSQ3xf6E/ztTltMJwOVwEfCeeMTMHaK8YZhpzks4jDHVeZGbdFYsWAfMk5SXNIhzEfTGJGAHM7DUzm2JmM+O2tQE4Mf4Pj31OzWxcPIALCGcgvA1cn3Q8FXF9nrD7vQJ4NT4uIIzVLwbeAp4EJiYd66C4zwYei88PJWxUa4GHgHzS8cW4jgeWxtz+FZiQxrwCNwCrgdeB+4B8WnIK3E841tFLeLO6YrgcAiKc5fc28BrhjKok41xLGI/v365uq1j/+hjnm8D5Sed00PL3gElJ5dSvaHbOOVc2XoaPnHPOjYAXBeecc2VeFJxzzpV5UXDOOVfmRcE551yZFwXnxpCksxW7yzqXRl4UnHPOlXlRcG4Ikr4t6UVJr0q6XeEeEp2Sbo73PlgsaXJc93hJz1f07e+/v8Dhkp6UtFzSMkmHxZdv0kf3eVgYr2R2LhW8KDg3iKQjgW8AZ5jZ8UAJuJTQrG6pmR0NLAF+Fb/lXuCnFvr2v1YxfyFwi5kdB5xOuIoVQifcawj39jiU0OvIuVTI7XsV58adc4GTgJfih/h6QtO3PuDBuM6fgUfifRvazGxJnL8AeEhSMzDNzB4FMLMegPh6L5rZhjj9KjATeGb0fy3n9s2LgnN7ErDAzOYPmCn9YtB6+9sjZnfF8xK+HboU8eEj5/a0GLhE0hQo35N4BmF76e9c+i3gGTNrB7ZLOjPOvwxYYuEuehskXRxfIx/75juXav4JxblBzGyVpJ8Dj0vKELpZXkW4Uc+pcdkWwnEHCO2jb4tv+u8A34vzLwNul3RjfI2vj+Gv4dx+8S6pzo2QpE4za0o6DudGkw8fOeecK/M9Beecc2W+p+Ccc67Mi4JzzrkyLwrOOefKvCg455wr86LgnHOuzIuCc865sv8BRkc2tW/DFbIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 956us/sample - loss: 0.3791 - acc: 0.9024\n",
      "Loss: 0.37910410796494254 Accuracy: 0.9023884\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 3.0999 - acc: 0.1834\n",
      "Epoch 00001: val_loss improved from inf to 1.92708, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_BN_7_conv_checkpoint/001-1.9271.hdf5\n",
      "36805/36805 [==============================] - 103s 3ms/sample - loss: 3.0998 - acc: 0.1834 - val_loss: 1.9271 - val_acc: 0.3972\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9570 - acc: 0.3914\n",
      "Epoch 00002: val_loss improved from 1.92708 to 1.21084, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_BN_7_conv_checkpoint/002-1.2108.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 1.9569 - acc: 0.3914 - val_loss: 1.2108 - val_acc: 0.6378\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5052 - acc: 0.5215\n",
      "Epoch 00003: val_loss improved from 1.21084 to 0.95652, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_BN_7_conv_checkpoint/003-0.9565.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 1.5053 - acc: 0.5215 - val_loss: 0.9565 - val_acc: 0.7233\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2378 - acc: 0.6112\n",
      "Epoch 00004: val_loss improved from 0.95652 to 0.77223, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_BN_7_conv_checkpoint/004-0.7722.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 1.2381 - acc: 0.6112 - val_loss: 0.7722 - val_acc: 0.7706\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0504 - acc: 0.6721\n",
      "Epoch 00005: val_loss improved from 0.77223 to 0.70798, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_BN_7_conv_checkpoint/005-0.7080.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 1.0503 - acc: 0.6721 - val_loss: 0.7080 - val_acc: 0.7932\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9199 - acc: 0.7153\n",
      "Epoch 00006: val_loss improved from 0.70798 to 0.60752, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_BN_7_conv_checkpoint/006-0.6075.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.9200 - acc: 0.7153 - val_loss: 0.6075 - val_acc: 0.8355\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8105 - acc: 0.7512\n",
      "Epoch 00007: val_loss improved from 0.60752 to 0.58403, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_BN_7_conv_checkpoint/007-0.5840.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.8105 - acc: 0.7512 - val_loss: 0.5840 - val_acc: 0.8307\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7273 - acc: 0.7782\n",
      "Epoch 00008: val_loss improved from 0.58403 to 0.49639, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_BN_7_conv_checkpoint/008-0.4964.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.7273 - acc: 0.7782 - val_loss: 0.4964 - val_acc: 0.8654\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6599 - acc: 0.8003\n",
      "Epoch 00009: val_loss did not improve from 0.49639\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.6600 - acc: 0.8002 - val_loss: 0.6187 - val_acc: 0.8116\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6060 - acc: 0.8168\n",
      "Epoch 00010: val_loss did not improve from 0.49639\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.6059 - acc: 0.8168 - val_loss: 0.5562 - val_acc: 0.8418\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5649 - acc: 0.8293\n",
      "Epoch 00011: val_loss improved from 0.49639 to 0.41416, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_BN_7_conv_checkpoint/011-0.4142.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.5650 - acc: 0.8293 - val_loss: 0.4142 - val_acc: 0.8840\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5170 - acc: 0.8455\n",
      "Epoch 00012: val_loss did not improve from 0.41416\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.5171 - acc: 0.8455 - val_loss: 0.4242 - val_acc: 0.8870\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4913 - acc: 0.8526\n",
      "Epoch 00013: val_loss did not improve from 0.41416\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.4914 - acc: 0.8525 - val_loss: 0.4932 - val_acc: 0.8581\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4626 - acc: 0.8606\n",
      "Epoch 00014: val_loss did not improve from 0.41416\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.4626 - acc: 0.8606 - val_loss: 0.7024 - val_acc: 0.7873\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4441 - acc: 0.8667\n",
      "Epoch 00015: val_loss improved from 0.41416 to 0.33798, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_BN_7_conv_checkpoint/015-0.3380.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.4446 - acc: 0.8667 - val_loss: 0.3380 - val_acc: 0.9005\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4286 - acc: 0.8694\n",
      "Epoch 00016: val_loss improved from 0.33798 to 0.31125, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_BN_7_conv_checkpoint/016-0.3112.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.4285 - acc: 0.8694 - val_loss: 0.3112 - val_acc: 0.9143\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3863 - acc: 0.8821\n",
      "Epoch 00017: val_loss did not improve from 0.31125\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.3863 - acc: 0.8821 - val_loss: 0.3183 - val_acc: 0.9138\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3732 - acc: 0.8849\n",
      "Epoch 00018: val_loss did not improve from 0.31125\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.3731 - acc: 0.8849 - val_loss: 0.3222 - val_acc: 0.9073\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3628 - acc: 0.8889\n",
      "Epoch 00019: val_loss did not improve from 0.31125\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.3628 - acc: 0.8889 - val_loss: 0.4198 - val_acc: 0.8833\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3468 - acc: 0.8931\n",
      "Epoch 00020: val_loss improved from 0.31125 to 0.30656, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_BN_7_conv_checkpoint/020-0.3066.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.3469 - acc: 0.8930 - val_loss: 0.3066 - val_acc: 0.9166\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3321 - acc: 0.8978\n",
      "Epoch 00021: val_loss did not improve from 0.30656\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.3323 - acc: 0.8978 - val_loss: 0.3557 - val_acc: 0.8994\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3362 - acc: 0.8963\n",
      "Epoch 00022: val_loss improved from 0.30656 to 0.26719, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_BN_7_conv_checkpoint/022-0.2672.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.3361 - acc: 0.8963 - val_loss: 0.2672 - val_acc: 0.9271\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3055 - acc: 0.9081\n",
      "Epoch 00023: val_loss did not improve from 0.26719\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.3055 - acc: 0.9081 - val_loss: 0.2718 - val_acc: 0.9276\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2919 - acc: 0.9095\n",
      "Epoch 00024: val_loss improved from 0.26719 to 0.24775, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_BN_7_conv_checkpoint/024-0.2477.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.2918 - acc: 0.9096 - val_loss: 0.2477 - val_acc: 0.9334\n",
      "Epoch 25/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2872 - acc: 0.9103\n",
      "Epoch 00025: val_loss improved from 0.24775 to 0.23752, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_BN_7_conv_checkpoint/025-0.2375.hdf5\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.2872 - acc: 0.9103 - val_loss: 0.2375 - val_acc: 0.9359\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2781 - acc: 0.9121\n",
      "Epoch 00026: val_loss did not improve from 0.23752\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.2781 - acc: 0.9121 - val_loss: 0.3300 - val_acc: 0.9203\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2719 - acc: 0.9157\n",
      "Epoch 00027: val_loss improved from 0.23752 to 0.23405, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_BN_7_conv_checkpoint/027-0.2340.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.2720 - acc: 0.9157 - val_loss: 0.2340 - val_acc: 0.9355\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2647 - acc: 0.9184\n",
      "Epoch 00028: val_loss did not improve from 0.23405\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.2648 - acc: 0.9183 - val_loss: 0.3041 - val_acc: 0.9199\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2594 - acc: 0.9189\n",
      "Epoch 00029: val_loss did not improve from 0.23405\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.2594 - acc: 0.9190 - val_loss: 0.3204 - val_acc: 0.9152\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2490 - acc: 0.9224\n",
      "Epoch 00030: val_loss improved from 0.23405 to 0.23164, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_BN_7_conv_checkpoint/030-0.2316.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.2490 - acc: 0.9224 - val_loss: 0.2316 - val_acc: 0.9366\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2348 - acc: 0.9262\n",
      "Epoch 00031: val_loss did not improve from 0.23164\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.2348 - acc: 0.9262 - val_loss: 0.2664 - val_acc: 0.9341\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2371 - acc: 0.9261\n",
      "Epoch 00032: val_loss did not improve from 0.23164\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.2371 - acc: 0.9261 - val_loss: 0.2805 - val_acc: 0.9271\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2257 - acc: 0.9296\n",
      "Epoch 00033: val_loss improved from 0.23164 to 0.22200, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_BN_7_conv_checkpoint/033-0.2220.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.2257 - acc: 0.9296 - val_loss: 0.2220 - val_acc: 0.9385\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2202 - acc: 0.9318\n",
      "Epoch 00034: val_loss did not improve from 0.22200\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.2202 - acc: 0.9318 - val_loss: 0.2499 - val_acc: 0.9343\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2129 - acc: 0.9330\n",
      "Epoch 00035: val_loss did not improve from 0.22200\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.2130 - acc: 0.9329 - val_loss: 0.2248 - val_acc: 0.9399\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2120 - acc: 0.9340\n",
      "Epoch 00036: val_loss did not improve from 0.22200\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.2119 - acc: 0.9341 - val_loss: 0.2632 - val_acc: 0.9280\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2058 - acc: 0.9359\n",
      "Epoch 00037: val_loss did not improve from 0.22200\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.2058 - acc: 0.9358 - val_loss: 0.2410 - val_acc: 0.9392\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2013 - acc: 0.9359\n",
      "Epoch 00038: val_loss did not improve from 0.22200\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.2015 - acc: 0.9359 - val_loss: 0.2380 - val_acc: 0.9392\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2012 - acc: 0.9370\n",
      "Epoch 00039: val_loss did not improve from 0.22200\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.2013 - acc: 0.9370 - val_loss: 0.2292 - val_acc: 0.9385\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1909 - acc: 0.9402\n",
      "Epoch 00040: val_loss did not improve from 0.22200\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1909 - acc: 0.9401 - val_loss: 0.2300 - val_acc: 0.9366\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1877 - acc: 0.9410\n",
      "Epoch 00041: val_loss did not improve from 0.22200\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1878 - acc: 0.9410 - val_loss: 0.2266 - val_acc: 0.9399\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1805 - acc: 0.9418\n",
      "Epoch 00042: val_loss did not improve from 0.22200\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1805 - acc: 0.9418 - val_loss: 0.2272 - val_acc: 0.9371\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1743 - acc: 0.9454\n",
      "Epoch 00043: val_loss did not improve from 0.22200\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1743 - acc: 0.9454 - val_loss: 0.2556 - val_acc: 0.9334\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1741 - acc: 0.9444\n",
      "Epoch 00044: val_loss did not improve from 0.22200\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1741 - acc: 0.9444 - val_loss: 0.2756 - val_acc: 0.9294\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1741 - acc: 0.9445\n",
      "Epoch 00045: val_loss did not improve from 0.22200\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1741 - acc: 0.9445 - val_loss: 0.3340 - val_acc: 0.9133\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1697 - acc: 0.9461\n",
      "Epoch 00046: val_loss improved from 0.22200 to 0.20546, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_BN_7_conv_checkpoint/046-0.2055.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1697 - acc: 0.9461 - val_loss: 0.2055 - val_acc: 0.9467\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1636 - acc: 0.9474\n",
      "Epoch 00047: val_loss did not improve from 0.20546\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1636 - acc: 0.9474 - val_loss: 0.2374 - val_acc: 0.9401\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1588 - acc: 0.9493\n",
      "Epoch 00048: val_loss did not improve from 0.20546\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1588 - acc: 0.9493 - val_loss: 0.2281 - val_acc: 0.9387\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1560 - acc: 0.9507\n",
      "Epoch 00049: val_loss did not improve from 0.20546\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1560 - acc: 0.9507 - val_loss: 0.2269 - val_acc: 0.9415\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1585 - acc: 0.9493\n",
      "Epoch 00050: val_loss did not improve from 0.20546\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1585 - acc: 0.9493 - val_loss: 0.2399 - val_acc: 0.9359\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1490 - acc: 0.9524\n",
      "Epoch 00051: val_loss did not improve from 0.20546\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1491 - acc: 0.9523 - val_loss: 0.2191 - val_acc: 0.9434\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1533 - acc: 0.9511\n",
      "Epoch 00052: val_loss improved from 0.20546 to 0.20396, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_BN_7_conv_checkpoint/052-0.2040.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1533 - acc: 0.9511 - val_loss: 0.2040 - val_acc: 0.9460\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1414 - acc: 0.9557\n",
      "Epoch 00053: val_loss did not improve from 0.20396\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1414 - acc: 0.9556 - val_loss: 0.2444 - val_acc: 0.9401\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1436 - acc: 0.9540\n",
      "Epoch 00054: val_loss did not improve from 0.20396\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1436 - acc: 0.9541 - val_loss: 0.2623 - val_acc: 0.9345\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1383 - acc: 0.9556\n",
      "Epoch 00055: val_loss did not improve from 0.20396\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1382 - acc: 0.9556 - val_loss: 0.2210 - val_acc: 0.9434\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1325 - acc: 0.9573\n",
      "Epoch 00056: val_loss did not improve from 0.20396\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1325 - acc: 0.9573 - val_loss: 0.2439 - val_acc: 0.9334\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1347 - acc: 0.9569\n",
      "Epoch 00057: val_loss did not improve from 0.20396\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1347 - acc: 0.9569 - val_loss: 0.3377 - val_acc: 0.9092\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1301 - acc: 0.9575\n",
      "Epoch 00058: val_loss did not improve from 0.20396\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1300 - acc: 0.9575 - val_loss: 0.2294 - val_acc: 0.9481\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1236 - acc: 0.9595\n",
      "Epoch 00059: val_loss did not improve from 0.20396\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1236 - acc: 0.9595 - val_loss: 0.2394 - val_acc: 0.9392\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1249 - acc: 0.9594\n",
      "Epoch 00060: val_loss did not improve from 0.20396\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1249 - acc: 0.9594 - val_loss: 0.2490 - val_acc: 0.9434\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1262 - acc: 0.9606\n",
      "Epoch 00061: val_loss did not improve from 0.20396\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1262 - acc: 0.9606 - val_loss: 0.2098 - val_acc: 0.9446\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1197 - acc: 0.9605\n",
      "Epoch 00062: val_loss did not improve from 0.20396\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1198 - acc: 0.9605 - val_loss: 0.2432 - val_acc: 0.9392\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1237 - acc: 0.9600\n",
      "Epoch 00063: val_loss did not improve from 0.20396\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1237 - acc: 0.9600 - val_loss: 0.2527 - val_acc: 0.9362\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1160 - acc: 0.9614\n",
      "Epoch 00064: val_loss did not improve from 0.20396\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1164 - acc: 0.9613 - val_loss: 0.2808 - val_acc: 0.9320\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1341 - acc: 0.9556\n",
      "Epoch 00065: val_loss did not improve from 0.20396\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1342 - acc: 0.9556 - val_loss: 0.2055 - val_acc: 0.9481\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1178 - acc: 0.9626\n",
      "Epoch 00066: val_loss did not improve from 0.20396\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1179 - acc: 0.9625 - val_loss: 0.2505 - val_acc: 0.9373\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1084 - acc: 0.9654\n",
      "Epoch 00067: val_loss did not improve from 0.20396\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1084 - acc: 0.9654 - val_loss: 0.2721 - val_acc: 0.9357\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1087 - acc: 0.9640\n",
      "Epoch 00068: val_loss did not improve from 0.20396\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1087 - acc: 0.9640 - val_loss: 0.2194 - val_acc: 0.9427\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1093 - acc: 0.9650\n",
      "Epoch 00069: val_loss did not improve from 0.20396\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1094 - acc: 0.9650 - val_loss: 0.2447 - val_acc: 0.9383\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1045 - acc: 0.9652\n",
      "Epoch 00070: val_loss did not improve from 0.20396\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1046 - acc: 0.9652 - val_loss: 0.2604 - val_acc: 0.9376\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1120 - acc: 0.9649\n",
      "Epoch 00071: val_loss did not improve from 0.20396\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1120 - acc: 0.9649 - val_loss: 0.2127 - val_acc: 0.9474\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1042 - acc: 0.9652\n",
      "Epoch 00072: val_loss did not improve from 0.20396\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1042 - acc: 0.9652 - val_loss: 0.2064 - val_acc: 0.9481\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0997 - acc: 0.9672\n",
      "Epoch 00073: val_loss did not improve from 0.20396\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0998 - acc: 0.9672 - val_loss: 0.3022 - val_acc: 0.9317\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1042 - acc: 0.9662\n",
      "Epoch 00074: val_loss improved from 0.20396 to 0.19930, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_BN_7_conv_checkpoint/074-0.1993.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1043 - acc: 0.9662 - val_loss: 0.1993 - val_acc: 0.9546\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0984 - acc: 0.9693\n",
      "Epoch 00075: val_loss did not improve from 0.19930\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0984 - acc: 0.9693 - val_loss: 0.2132 - val_acc: 0.9443\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1015 - acc: 0.9669\n",
      "Epoch 00076: val_loss did not improve from 0.19930\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.1015 - acc: 0.9669 - val_loss: 0.2158 - val_acc: 0.9488\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0899 - acc: 0.9707\n",
      "Epoch 00077: val_loss did not improve from 0.19930\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0899 - acc: 0.9707 - val_loss: 0.2553 - val_acc: 0.9411\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0964 - acc: 0.9680\n",
      "Epoch 00078: val_loss did not improve from 0.19930\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0964 - acc: 0.9680 - val_loss: 0.2112 - val_acc: 0.9513\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0924 - acc: 0.9699\n",
      "Epoch 00079: val_loss did not improve from 0.19930\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0924 - acc: 0.9699 - val_loss: 0.2577 - val_acc: 0.9345\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0887 - acc: 0.9716\n",
      "Epoch 00080: val_loss did not improve from 0.19930\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0887 - acc: 0.9716 - val_loss: 0.2332 - val_acc: 0.9476\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0876 - acc: 0.9712\n",
      "Epoch 00081: val_loss did not improve from 0.19930\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0876 - acc: 0.9712 - val_loss: 0.3036 - val_acc: 0.9266\n",
      "Epoch 82/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0848 - acc: 0.9721\n",
      "Epoch 00082: val_loss did not improve from 0.19930\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0848 - acc: 0.9721 - val_loss: 0.2428 - val_acc: 0.9387\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0854 - acc: 0.9732\n",
      "Epoch 00083: val_loss did not improve from 0.19930\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0854 - acc: 0.9732 - val_loss: 0.2262 - val_acc: 0.9488\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0859 - acc: 0.9712\n",
      "Epoch 00084: val_loss did not improve from 0.19930\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0859 - acc: 0.9712 - val_loss: 0.2793 - val_acc: 0.9348\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0830 - acc: 0.9730\n",
      "Epoch 00085: val_loss did not improve from 0.19930\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0830 - acc: 0.9730 - val_loss: 0.2203 - val_acc: 0.9464\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0817 - acc: 0.9728\n",
      "Epoch 00086: val_loss did not improve from 0.19930\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0817 - acc: 0.9728 - val_loss: 0.2209 - val_acc: 0.9476\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0816 - acc: 0.9737\n",
      "Epoch 00087: val_loss did not improve from 0.19930\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0816 - acc: 0.9738 - val_loss: 0.2373 - val_acc: 0.9471\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0791 - acc: 0.9742\n",
      "Epoch 00088: val_loss did not improve from 0.19930\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0791 - acc: 0.9742 - val_loss: 0.2889 - val_acc: 0.9315\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0828 - acc: 0.9727\n",
      "Epoch 00089: val_loss did not improve from 0.19930\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0828 - acc: 0.9727 - val_loss: 0.2270 - val_acc: 0.9471\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0760 - acc: 0.9754\n",
      "Epoch 00090: val_loss did not improve from 0.19930\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0761 - acc: 0.9754 - val_loss: 0.2999 - val_acc: 0.9345\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0925 - acc: 0.9694\n",
      "Epoch 00091: val_loss did not improve from 0.19930\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0925 - acc: 0.9694 - val_loss: 0.2917 - val_acc: 0.9311\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0717 - acc: 0.9769\n",
      "Epoch 00092: val_loss did not improve from 0.19930\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0719 - acc: 0.9769 - val_loss: 0.2427 - val_acc: 0.9453\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0763 - acc: 0.9755\n",
      "Epoch 00093: val_loss did not improve from 0.19930\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0763 - acc: 0.9755 - val_loss: 0.2433 - val_acc: 0.9411\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0767 - acc: 0.9757\n",
      "Epoch 00094: val_loss did not improve from 0.19930\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0767 - acc: 0.9757 - val_loss: 0.2485 - val_acc: 0.9406\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0718 - acc: 0.9765\n",
      "Epoch 00095: val_loss did not improve from 0.19930\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0720 - acc: 0.9765 - val_loss: 0.2720 - val_acc: 0.9399\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0775 - acc: 0.9740\n",
      "Epoch 00096: val_loss did not improve from 0.19930\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0775 - acc: 0.9739 - val_loss: 0.2369 - val_acc: 0.9453\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0747 - acc: 0.9755\n",
      "Epoch 00097: val_loss did not improve from 0.19930\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0748 - acc: 0.9755 - val_loss: 0.2658 - val_acc: 0.9464\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0709 - acc: 0.9764\n",
      "Epoch 00098: val_loss did not improve from 0.19930\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0709 - acc: 0.9764 - val_loss: 0.2639 - val_acc: 0.9425\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0663 - acc: 0.9780\n",
      "Epoch 00099: val_loss did not improve from 0.19930\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0663 - acc: 0.9780 - val_loss: 0.2660 - val_acc: 0.9399\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0680 - acc: 0.9787\n",
      "Epoch 00100: val_loss did not improve from 0.19930\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0681 - acc: 0.9786 - val_loss: 0.2779 - val_acc: 0.9380\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0705 - acc: 0.9775\n",
      "Epoch 00101: val_loss did not improve from 0.19930\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0706 - acc: 0.9775 - val_loss: 0.3249 - val_acc: 0.9250\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0667 - acc: 0.9783\n",
      "Epoch 00102: val_loss did not improve from 0.19930\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0670 - acc: 0.9783 - val_loss: 0.2617 - val_acc: 0.9427\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0744 - acc: 0.9762\n",
      "Epoch 00103: val_loss did not improve from 0.19930\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0744 - acc: 0.9763 - val_loss: 0.2450 - val_acc: 0.9467\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0612 - acc: 0.9802\n",
      "Epoch 00104: val_loss did not improve from 0.19930\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0612 - acc: 0.9802 - val_loss: 0.2382 - val_acc: 0.9450\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0583 - acc: 0.9819\n",
      "Epoch 00105: val_loss did not improve from 0.19930\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0584 - acc: 0.9819 - val_loss: 0.2523 - val_acc: 0.9432\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0665 - acc: 0.9783\n",
      "Epoch 00106: val_loss did not improve from 0.19930\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0665 - acc: 0.9783 - val_loss: 0.2291 - val_acc: 0.9485\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0626 - acc: 0.9795\n",
      "Epoch 00107: val_loss did not improve from 0.19930\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0626 - acc: 0.9795 - val_loss: 0.2614 - val_acc: 0.9436\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0653 - acc: 0.9783\n",
      "Epoch 00108: val_loss did not improve from 0.19930\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0654 - acc: 0.9783 - val_loss: 0.2601 - val_acc: 0.9432\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0666 - acc: 0.9784\n",
      "Epoch 00109: val_loss did not improve from 0.19930\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0666 - acc: 0.9784 - val_loss: 0.2975 - val_acc: 0.9280\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0607 - acc: 0.9804\n",
      "Epoch 00110: val_loss did not improve from 0.19930\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0607 - acc: 0.9804 - val_loss: 0.2134 - val_acc: 0.9495\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0651 - acc: 0.9783\n",
      "Epoch 00111: val_loss did not improve from 0.19930\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0652 - acc: 0.9782 - val_loss: 0.2713 - val_acc: 0.9446\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0654 - acc: 0.9787\n",
      "Epoch 00112: val_loss did not improve from 0.19930\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0654 - acc: 0.9787 - val_loss: 0.2379 - val_acc: 0.9464\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0554 - acc: 0.9818\n",
      "Epoch 00113: val_loss did not improve from 0.19930\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0554 - acc: 0.9818 - val_loss: 0.2702 - val_acc: 0.9432\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0568 - acc: 0.9813\n",
      "Epoch 00114: val_loss did not improve from 0.19930\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0568 - acc: 0.9813 - val_loss: 0.2991 - val_acc: 0.9413\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0533 - acc: 0.9829\n",
      "Epoch 00115: val_loss did not improve from 0.19930\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0535 - acc: 0.9829 - val_loss: 0.3298 - val_acc: 0.9313\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0671 - acc: 0.9787\n",
      "Epoch 00116: val_loss did not improve from 0.19930\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0671 - acc: 0.9787 - val_loss: 0.2521 - val_acc: 0.9492\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0569 - acc: 0.9814\n",
      "Epoch 00117: val_loss did not improve from 0.19930\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0568 - acc: 0.9814 - val_loss: 0.2205 - val_acc: 0.9497\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0519 - acc: 0.9839\n",
      "Epoch 00118: val_loss did not improve from 0.19930\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0519 - acc: 0.9839 - val_loss: 0.2853 - val_acc: 0.9383\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0516 - acc: 0.9833\n",
      "Epoch 00119: val_loss did not improve from 0.19930\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0516 - acc: 0.9833 - val_loss: 0.2147 - val_acc: 0.9532\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0523 - acc: 0.9829\n",
      "Epoch 00120: val_loss did not improve from 0.19930\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0523 - acc: 0.9829 - val_loss: 0.2491 - val_acc: 0.9476\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0548 - acc: 0.9822\n",
      "Epoch 00121: val_loss did not improve from 0.19930\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0548 - acc: 0.9822 - val_loss: 0.3146 - val_acc: 0.9322\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0535 - acc: 0.9822\n",
      "Epoch 00122: val_loss did not improve from 0.19930\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0536 - acc: 0.9822 - val_loss: 0.2455 - val_acc: 0.9450\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0606 - acc: 0.9800\n",
      "Epoch 00123: val_loss did not improve from 0.19930\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0606 - acc: 0.9800 - val_loss: 0.2999 - val_acc: 0.9362\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0498 - acc: 0.9839\n",
      "Epoch 00124: val_loss did not improve from 0.19930\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0498 - acc: 0.9839 - val_loss: 0.2833 - val_acc: 0.9443\n",
      "\n",
      "1D_CNN_custom_DO_075_DO_BN_7_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzs3Xd8ldX9wPHPuTt7hxFGAJlhBBmiVEQRq1gRB6KVqrRKh9uWX6m2Vmtr3VrU1jorbgRREUSxgqACZQjIlCGYkEESsnNvcsf5/XGySSBALoHc7/v1uq/kPuOc8zz33vN9znme5zxKa40QQggBYGnrAgghhDh5SFAQQghRS4KCEEKIWhIUhBBC1JKgIIQQopYEBSGEELUkKAghhKglQUEIIUQtCQpCCCFq2dq6AEcrMTFRp6amtnUxhBDilLJu3bp8rXXSkZY75YJCamoqa9eubetiCCHEKUUpta8ly0n3kRBCiFoSFIQQQtSSoCCEEKLWKXdOoSler5fMzEw8Hk9bF+WU5XK56NKlC3a7va2LIoRoQ+0iKGRmZhIVFUVqaipKqbYuzilHa01BQQGZmZn06NGjrYsjhGhD7aL7yOPxkJCQIAHhGCmlSEhIkJaWEKJ9BAVAAsJxkv0nhIB2FBSOxO93U1m5n0DA29ZFEUKIk1bIBIVAwENVVTZat35QKCoq4p///OcxrTthwgSKiopavPx9993HY489dkx5CSHEkYRMUKjrHtGtnvbhgoLP5zvsuosWLSI2NrbVyySEEMciZIJCzaZqHWj1lGfOnMnu3btJT09nxowZLFu2jLPPPpuJEycyYMAAACZNmsSwYcNIS0vj+eefr103NTWV/Px89u7dS//+/bnppptIS0vjggsuwO12HzbfDRs2MGrUKAYPHsxll11GYWEhALNmzWLAgAEMHjyYq6++GoAvvviC9PR00tPTGTp0KKWlpa2+H4QQp76gXZKqlHIBywFndT5ztdZ/brSME5gNDAMKgCla673Hk+/OnXdQVrbhkOla+wkEKrBYwlDq6DY7MjKd3r2fanb+Qw89xObNm9mwweS7bNky1q9fz+bNm2sv8Xz55ZeJj4/H7XYzYsQIrrjiChISEhqVfSdvvfUWL7zwAldddRXz5s1j6tSpzeZ73XXX8fTTT3POOedw7733cv/99/PUU0/x0EMP8f333+N0Omu7ph577DGeffZZRo8eTVlZGS6X66j2gRAiNASzpVAJnKe1HgKkAxcqpUY1WuYXQKHW+jTgSeDhYBXmRF9dM3LkyAbX/M+aNYshQ4YwatQoMjIy2Llz5yHr9OjRg/T0dACGDRvG3r17m02/uLiYoqIizjnnHACuv/56li9fDsDgwYO59tpref3117HZTAAcPXo0d911F7NmzaKoqKh2uhBC1Be0mkFrrYGy6rf26lfjDv1Lgfuq/58LPKOUUtXrHpPmjuj9fjcVFVtwuXpit8cfa/ItFhERUfv/smXL+Oyzz1i5ciXh4eGMHTu2yXsCnE5n7f9Wq/WI3UfNWbhwIcuXL2fBggX87W9/49tvv2XmzJlcfPHFLFq0iNGjR/PJJ5/Qr1+/Y0pfCNF+BfWcglLKqpTaABwAlmitVzdaJAXIANBa+4BiIIEgUCp45xSioqIO20dfXFxMXFwc4eHhbN++nVWrVh13njExMcTFxbFixQoAXnvtNc455xwCgQAZGRmce+65PPzwwxQXF1NWVsbu3bsZNGgQv//97xkxYgTbt28/7jIIIdqfoPYhaK39QLpSKhaYr5QaqLXefLTpKKWmA9MBunXrdoylqYl/rR8UEhISGD16NAMHDuSiiy7i4osvbjD/wgsv5LnnnqN///707duXUaMa96Idm1dffZVf/epXVFRU0LNnT1555RX8fj9Tp06luLgYrTW33XYbsbGx/OlPf2Lp0qVYLBbS0tK46KKLWqUMQoj2RR1HT83RZaTUvUCF1vqxetM+Ae7TWq9U5uxvDpB0uO6j4cOH68YP2dm2bRv9+/c/bP5a+ygr24DT2QWHo+PxbEq71ZL9KIQ4NSml1mmthx9puaB1HymlkqpbCCilwoDxQOM+iw+B66v/vxL4/HjOJxxeTffRiQmCQghxKgpm91En4FWllBVTI8/RWn+klPoLsFZr/SHwEvCaUmoXcBC4OnjFqbn6qPW7j4QQor0I5tVHm4ChTUy/t97/HmBysMpQn7kk1RKUE81CCNFehNAdzWA2V7qPhBCiOSEVFJRS0lIQQojDCKmgYDZXgoIQQjQnpIKCuYHt5AgKkZGRRzVdCCFOhJAKCqDkklQhhDiMkAoKwWopzJw5k2effbb2fc2DcMrKyhg3bhynn346gwYN4oMPPmhxmlprZsyYwcCBAxk0aBDvvPMOANnZ2YwZM4b09HQGDhzIihUr8Pv93HDDDbXLPvnkk62+jUKI0ND+hsq84w7YcOjQ2QDOgBu0Bmv40aWZng5PNT909pQpU7jjjju4+eabAZgzZw6ffPIJLpeL+fPnEx0dTX5+PqNGjWLixIktGrH1vffeY8OGDWzcuJH8/HxGjBjBmDFjePPNN/nxj3/MPffcg9/vp6Kigg0bNrB//342bzYjiBzNk9yEEKK+9hcUjqj1u4+GDh3KgQMHyMrKIi8vj7i4OLp27YrX6+Xuu+9m+fLlWCwW9u/fT25uLh07HnmYjS+//JJrrrkGq9VKhw4dOOecc1izZg0jRozg5z//OV6vl0mTJpGenk7Pnj3Zs2cPt956KxdffDEXXHBBq2+jECI0tL+gcJgj+ir3bvx+N5GRA1s928mTJzN37lxycnKYMmUKAG+88QZ5eXmsW7cOu91Oampqk0NmH40xY8awfPlyFi5cyA033MBdd93Fddddx8aNG/nkk0947rnnmDNnDi+//HJrbJYQIsSE1DmFYF6SOmXKFN5++23mzp3L5MnmJu3i4mKSk5Ox2+0sXbqUffv2tTi9s88+m3feeQe/309eXh7Lly9n5MiR7Nu3jw4dOnDTTTdx4403sn79evLz8wkEAlxxxRX89a9/Zf369UHZRiFE+9f+WgqHEcxLUtPS0igtLSUlJYVOnToBcO2113LJJZcwaNAghg8fflQPtbnssstYuXIlQ4YMQSnFI488QseOHXn11Vd59NFHsdvtREZGMnv2bPbv38+0adMIBMy2/f3vfw/KNgoh2r8TNnR2aznWobMBPJ4MvN48oqJOD1bxTmkydLYQ7VebD519MjJX/ZxaQVAIIU6kkAoKNQPinWqtIyGEOFFCMCjAyTLUhRBCnGxCKiiYE83ISKlCCNGMkAoKdU9fk+4jIYRoSkgFBWkpCCHE4YVUUAjWOYWioiL++c9/HtO6EyZMkLGKhBAnjZAKCjUthdbuPjpcUPD5fIddd9GiRcTGxrZqeYQQ4liFVFCoOafQ2t1HM2fOZPfu3aSnpzNjxgyWLVvG2WefzcSJExkwYAAAkyZNYtiwYaSlpfH888/Xrpuamkp+fj579+6lf//+3HTTTaSlpXHBBRfgdrsPyWvBggWcccYZDB06lPPPP5/c3FwAysrKmDZtGoMGDWLw4MHMmzcPgMWLF3P66aczZMgQxo0b16rbLYRof9rdMBeHGTkbrSMIBPpisYTRgtGrax1h5GweeughNm/ezIbqjJctW8b69evZvHkzPXr0AODll18mPj4et9vNiBEjuOKKK0hISGiQzs6dO3nrrbd44YUXuOqqq5g3bx5Tp05tsMyPfvQjVq1ahVKKF198kUceeYTHH3+cBx54gJiYGL799lsACgsLycvL46abbmL58uX06NGDgwcPtnyjhRAhqd0FhZYJ/tVHI0eOrA0IALNmzWL+/PkAZGRksHPnzkOCQo8ePUhPTwdg2LBh7N2795B0MzMzmTJlCtnZ2VRVVdXm8dlnn/H222/XLhcXF8eCBQsYM2ZM7TLx8fGtuo1CiPYnaEFBKdUVmA10wNTCz2ut/9FombHAB8D31ZPe01r/5XjyPdwRvd9fRUXFDlyuHtjtCc0v2AoiIiJq/1+2bBmfffYZK1euJDw8nLFjxzY5hLbT6az932q1Ntl9dOutt3LXXXcxceJEli1bxn333ReU8gshQlMwzyn4gN9qrQcAo4CblVIDmlhuhdY6vfp1XAHhSOouSW3dlkJUVBSlpaXNzi8uLiYuLo7w8HC2b9/OqlWrjjmv4uJiUlJSAHj11Vdrp48fP77BI0ELCwsZNWoUy5cv5/vvTcyV7iMhxJEELShorbO11uur/y8FtgEpwcqvZYJzSWpCQgKjR49m4MCBzJgx45D5F154IT6fj/79+zNz5kxGjRp1zHndd999TJ48mWHDhpGYmFg7/Y9//COFhYUMHDiQIUOGsHTpUpKSknj++ee5/PLLGTJkSO3Df4QQojknZOhspVQqsBwYqLUuqTd9LDAPyASygN9prbccLq3jGTpbaz9lZd/gdHbB4TjyIzFDjQydLUT71dKhs4N+olkpFYmp+O+oHxCqrQe6a63LlFITgPeB3k2kMR2YDtCtW7fjKQ3Q+t1HQgjRXgT1PgWllB0TEN7QWr/XeL7WukRrXVb9/yLArpRKbGK557XWw7XWw5OSko6nRNV/ZZgLIYRoStCCgjJPtHkJ2Ka1fqKZZTpWL4dSamR1eQqCWCbAImMfCSFEM4LZfTQa+BnwrVKq5nayu4FuAFrr54ArgV8rpXyAG7haB71vxzxoRwghxKGCFhS01l9S11/T3DLPAM8EqwxNUUpJS0EIIZoRYmMfgdlkCQpCCNGUkAsK5ga2tg8KkZGRbV0EIYQ4RMgFBTnRLIQQzQu5oGCuQGrdE80zZ85sMMTEfffdx2OPPUZZWRnjxo3j9NNPZ9CgQXzwwQdHTKu5IbabGgK7ueGyhRDiWLW7UVLvWHwHG3KaGTsbCATcaK2xWsNbnGZ6x3SeurD5kfamTJnCHXfcwc033wzAnDlz+OSTT3C5XMyfP5/o6Gjy8/MZNWoUEydOrA5MTWtqiO1AINDkENhNDZcthBDHo90FhZZp3ZbC0KFDOXDgAFlZWeTl5REXF0fXrl3xer3cfffdLF++HIvFwv79+8nNzaVjx+aH2GhqiO28vLwmh8BuarhsIYQ4Hu0uKBzuiB7A7d6N3+8mMnJgq+Y7efJk5s6dS05OTu3Ac2+88QZ5eXmsW7cOu91Oampqk0Nm12jpENtCCBEsIXdOIViXpE6ZMoW3336buXPnMnnyZMAMc52cnIzdbmfp0qXs27fvsGk0N8R2c0NgNzVcthBCHI+QCwrBuiQ1LS2N0tJSUlJS6NSpEwDXXnsta9euZdCgQcyePZt+/fodNo3mhthubgjspobLFkKI43FChs5uTcczdDaAx5OB15tHVNTpwSjeKU2Gzhai/Wrp0Nkh2FJo/UtShRCivQi5oFAzIN6p1kISQogTod0EhZZX8sF5JOepToKkEALaSVBwuVwUFBS0qGIzJ5qRoS7q0VpTUFCAy+Vq66IIIdpYu7hPoUuXLmRmZpKXl3fEZf3+MrzeApzO7SjVLja/VbhcLrp06dLWxRBCtLF2USva7fbau32PJDf3DbZtm8rIkTsID+8T5JIJIcSppV10Hx0NiyUMgEBA7hQWQojGQjAomH7zQMDdxiURQoiTTwgGBWkpCCFEc0IwKJiWgt8vLQUhhGgsBIOCtBSEEKI5IRgU5JyCEEI0J+SCgtUqLQUhhGhOyAWFuu4jaSkIIURjQQsKSqmuSqmlSqmtSqktSqnbm1hGKaVmKaV2KaU2KaWCPp51XfeRtBSEEKKxYN7R7AN+q7Ver5SKAtYppZZorbfWW+YioHf16wzgX9V/g0ZaCkII0bygtRS01tla6/XV/5cC24CURotdCszWxiogVinVKVhlAqrHO7LIJalCCNGEE3JOQSmVCgwFVjealQJk1HufyaGBo7XLgsUSJt1HQgjRhKAHBaVUJDAPuENrXXKMaUxXSq1VSq1tyUioR2KxuKT7SAghmhDUoKCUsmMCwhta6/eaWGQ/0LXe+y7V0xrQWj+vtR6utR6elJR03OWyWqWlIIQQTQnm1UcKeAnYprV+opnFPgSuq74KaRRQrLXODlaZalgsEfj9pcHORgghTjnBvPpoNPAz4Ful1IbqaXcD3QC01s8Bi4AJwC6gApgWtNJs3gxz5sBtt+FwJOH15gctKyGEOFUFLShorb8E1BGW0cDNwSpDAzt2wAMPwBVXYLcnU1Gx44RkK4QQp5LQuaM5Otr8LS3Fbk/C6z3QtuURQoiTUOgFhZISHI5kvN4CtPa3bZmEEOIkEzpBISrK/C0pwW5PBgJ4vQfbtEhCCHGyCZ2gUK/7yOEwl7VKF5IQQjQUekGhtqUAVVXHfyOcEEK0J6ETFCIjzd+SEux2aSkIIURTQicoWCwmMJSW4nDUtBQkKAghRH2hExTAdCGVlGC3JwAKr1e6j4QQor7QCgpRUVBSglJW7PYE6T4SQohGQisoREdDqRnzyG5Plu4jIYRoJPSCQokZvdvcwCbdR0IIUV9oBYXq7iMAuz1JWgpCCNFIaAWFRt1Hck5BCCEaCr2gUK/7yOcrJBDwtnGhhBDi5BFaQSEqyrQUtK53A5s8V0EIIWq0KCgopW5XSkVXPyHtJaXUeqXUBcEuXKuLjgafDzye2hvYpAtJCCHqtLSl8HOtdQlwARCHeaLaQ0ErVbA0GP/ItBRk/CMhhKjT0qBQ8wS1CcBrWustHOGpaielmuGzS0trB8WTloIQQtRpaVBYp5T6FBMUPlFKRQGB4BUrSBo9aAeQexWEEKKelj6j+RdAOrBHa12hlIoHpgWvWEFSLyjYbLGAVe5VEEKIelraUjgT2KG1LlJKTQX+CBQHr1hBUq/7SCkLDoc8q1kIIepraVD4F1ChlBoC/BbYDcwOWqmCpV5LAWrGP5LuIyGEqNHSoODTWmvgUuAZrfWzQFTwihUk9Z7TDGaoC2kpCCFEnZYGhVKl1B8wl6IuVEpZAPvhVlBKvayUOqCU2tzM/LFKqWKl1Ibq171HV/RjUO85zWDuapZzCkIIUaelQWEKUIm5XyEH6AI8eoR1/gNceIRlVmit06tff2lhWY5deLh5Alu97iO5+kgIIeq0KChUB4I3gBil1E8Aj9b6sOcUtNbLgYPHX8RWpFSDkVIdjiT8/hL8fk8bF0wIIU4OLR3m4irgf8Bk4CpgtVLqylbI/0yl1Eal1MdKqbRWSO/IGoyU2gEArzf3hGQthBAnu5bep3APMEJrfQBAKZUEfAbMPY681wPdtdZlSqkJwPtA76YWVEpNB6YDdOvW7TiypMFIqWFhPQBwu3fjcnU/vnSFEKIdaOk5BUtNQKhWcBTrNklrXaK1Lqv+fxFgV0olNrPs81rr4Vrr4UlJSceTbd1IqUBY2GkAuN27ji9NIYRoJ1raUlislPoEeKv6/RRg0fFkrJTqCORqrbVSaiQmyBQcT5otEh0Nxea+O6ezK0o5cbt3Bj1bIYQ4FbQoKGitZyilrgBGV096Xms9/3DrKKXeAsYCiUqpTODPVF/GqrV+DrgS+LVSyge4gaur74UIruhoyMioLqOFsLCe0lIQQohqLW0poLWeB8w7iuWvOcL8Z4BnWppeq6nXfQQQFtabigppKQghBBwhKCilSoGmjt4VoLXW0UEpVTDVO9EMJigUFn6K1gHMPXlCCBG6DhsUtNan3lAWR1JzSarWoBRhYacRCHiorNyPy9W1rUsnhBBtKvQOjaOiTEAoLwcgPNxcBSsnm4UQIhSDQqORUuWyVCGEqBPyQUEuSxVCiDqhFxTqPWgH5LJUIYSoL/SCQqOWAshlqUIIUUOCAiYoeDy70TrQRoUSQoiTQ+gFhUbdR0CDy1KFECKUhV5QaKKlIJelCiGEEbpBoVFLAeSyVCGECL2g4HSC3d6gpVBzWWpFxXdtWDAhhGh7oRcU4JDxj5SyEBExgPLyTW1YKCGEaHuhGxSKihpMiooaTmnpWk7E6N1CCHGyCs2gkJoKe/Y0mBQVNRyfrxCP5/u2KZMQQpwEQjMo9OkD3zU8fxAVNRyA0tK1bVEiIYQ4KYRmUOjbFw4ehIK6p39GRAxEKYcEBSFESAvNoNCnj/m7Y0ftJIvFQWTkEAkKQoiQFppBoW9f87eJLqTS0nUy3IUQImSFZlBITQWbrUFLAUxQ8PtL5CY2IUTICs2gYLNBr15yslkIIRoJzaAApgupUVAIDx+AxeKSoCCECFmhGxT69IGdO8Hvr51ksdiIjBwqQUEIEbJCNyj07QuVlZCR0WCyOdm8Hq39zawohBDtV9CCglLqZaXUAaXU5mbmK6XULKXULqXUJqXU6cEqS5OauCwVICZmNIFAOSUl/zuhxRFCiJNBMFsK/wEuPMz8i4De1a/pwL+CWJZDNXNZalzcBYCVgoKFJ7Q4QghxMghaUNBaLwcOHmaRS4HZ2lgFxCqlOgWrPIdITjYD4zVqKdjtccTEnMXBgxIUhBChx9aGeacA9Tv0M6unZTdeUCk1HdOaoFu3bq2Tu1JNjoEEkJBwMXv2zKSycj9OZ0rr5CeEaDVag88HgQA4HObnXDNda7BY6t4XF8OBA1BeDhUV5om8HTtCTAxUVZlTi1YruFzmUSs165WXmxH2/X5z/BgVZfLx+828vDwzWo7WZn2/36RfWQkdOkCXLia9gwfNq6gICgtNuW02s47Vaspqs9W9arYlPNzka7dDTg5kZUHv3jB8eHD3bVsGhRbTWj8PPA8wfPjw1hvbum9f+PLLQybHx5ugUFCwiM6db2q17ITw+UwFUVhofvxhYaYiqKoyL7/fVDJgKguLxbwPBExlU1JiKiSr1TwvqrLSVE5FRRARAbGx4PVCdrYZ2isyEuLjTTqlpVBWZl7l5SYvq9WUw+s1adW8/H5TCUZFmTKXlYHbbZa1WMzy9ctcUzlHRJiyHzxoKuOUFOjXz2zjN9/A5uozjOHhprILBMz21aRnt0NcnKkMKytNJevx1JWvvNy8vN6G+9XhMH+rqszfiAhT9pISk0Z78dvftu+gsB/oWu99l+ppJ06fPvDmm+bbHhZWOzkiIg2nsxsFBQslKJxktDY//JoKrqLCvPx+8xG6XKYiyM83H2vN0V9ODvzwg1kvPNwsW1PplJfXpaWUqcBqjkQrqzTlZYrS0rqKSGtTUXk8Jt+airLmyM/rNfOUMvdI9jotQF6eYtNGxfdHPTK7BqsXLF7wuUBb62Z1Xw6x38Pec6G4ugVtLzfL+Fx16yfugMpoKO0MmLJGRpqyBgJ1FbrdbvaXw6lRdjcVxeGUlpr9ERVl5mkNAfzYrOB0WnDYVe3Rbnk57N9vlomPh06dYN8+WLLE7Ke0NDjvPJOe2232k8Vi9lNN/lVVJmCWlJiKPSnJfFYOh3mFh5vpLlfdUXVVVd3+rgkOZWUmjagoE5g6dDDbHBZmpmfn+MkvdhMTFonDYcpXWWnSqjlSj4iA8CgvFeSjyjtQXmap3X/h4aZsCQl1rQeLxaxjt0NuLuzJcHOwKouoGD/xsXb6d0wlLk7hcEBWaRarc5aT5OxC1/A+RFuT8PkUPl/dd6yiwgTWqiqzLzt3Nq2PYGvLoPAhcItS6m3gDKBYa31I11FQpaWZvb95M4wYUTtZKUVCwsXk5MwmEKjEYnGe0GIdK3/AT3ZZNgfdBynyFJGWlEZCeELt/F0Hd9ExsiORjshD1j1QfoB1WeuIccUQHxZPj9geOG1mu3cf3M2yvcuIdETSNaYrXp9mW+53/FC0n8EJoxgadw4et4W1WWtYk/sl3oA5XNMBC9pvI+B14K2IpLI0korKSkoDB3AHirF6Y7FWxeMtSqY8pzMVuZ1x+hNxOiwoZT4avx+K7Fs5cPZPCVjd6Nw0OHgaVEWYis9RDmEFoAJmelEqxO6DTushbjeEFYKzBEpSIL8flsI+BPL6QGkKdNiE6vYVVrsmIvdcYg6ej620R+0PvLzvC+QNvwOXO5W4stE4dQweRyaVjmy01Y22enAG4on0nkZ4VQ8c3mRs3kSidCfibF2oCBSxRv+LZUmzscbG0q37hVwa8yOSY2KIj3JxwPs9eyo2UuLLI9HRlSRHV8KsEViVjUJ/NlvKP2enZyVe7QEg2prIhE43cX7KJF7b8whf5M2r/fxSIrtRVlVKcVUhVmWlX/xAeif2ZFXmKnLKs7EqK5f0upLJaVeyYv/nvL99Pt1iuvHw+Q8zNnUsBRUFfPTdR3y651M+//5zDpQfYHzP8Vw76FoK3AV8uOND1mStwe114693uXaHiA7ckH4D1w+5njVZa3hu7XPsK97H6N4/4coBV5KWnEa8M4ndB/fy1tZXWfDdAsLt4XSJ7kKCKx6bxYbVYsVusWOz2CjyFJFduIusor1UeCvw+DyE2cJIiU6hc1Rn4sPisbnicDljCHNGkRSexIWnXUhSRBIen4fXN73O3K1zySjJIKs0C4AIFUFkUSSRFZFEOCLILctlT9EevAEvcb44UiNTiXREYrVY6RzVmeuHXM+4HuOYt20eMz+byfdF3+OwOuga3RW/9lNaWYqlykJyUTJJ3iSindFEOiLpEduD81LOIzWhDwsrnueZsmco9BRCCZAB3X7oxsW9LyazJJOFOxcSqDfG2oCkAUw/fTrj+oxj8a7FfLDjA8Lt4fyo649IjU1lYc43rF65mqvTrubWM25ttTqjKSpYTxpTSr0FjAUSgVzgz4AdQGv9nFJKAc9grlCqAKZprY9419jw4cP12rWtdHPZvn1mHKRnn4Xf/KbBrIKChXz77U8YPPgT4uMvaJ38joPWGlVzCAMs37ecWatnUeGtwBfwsb90P7sO7qLKX1W7jNPqZHLaZIZ3Gs5rm15jXfY6ohxRXNlnKhelTKVrRB9sgXD+vfFJXt/3EJ5AWe26StuI8fbDH/BR6tp++MK5YyFgg4j8495OpW24vJ1IKh1Hz4JbCNhLWNntMuyE0TlwFgdtmylWe/FjtlOhiLTGowlQ5i+sTSfJmUKPqH5EWuNxqkiKdSYZ7h1klPzQIL/OUZ3RWpNdZo5HpqVP47ELHuP97e/ziw9/wY+6/YhoZzRf/fAVlf5KukR3oVNkJyIcETitTvJlDoSAAAAgAElEQVQr8tl1cBe55blNbo/D6uDKAVfi8Xn4bM9nlFSWNJgf44yhY2RHMksyKfeW1+0HFEM6DuGc7ueQHJGMzWJjZeZKPtzxIQEdIMwWxj1n38NP+vyEpXuXsnr/auJd8XSN6UppZSnrstexu3A3wzsPZ1yPcWzP384L61+gpLKEMFsYE3pP4H/7/0dGSQZpSWlsz9+OX/vpENGB83qcR+eozry79V1+KDb7a2DyQM5NPZdoZzROqzlY8Gs/G3M3smDHgtpA0TehLwOTB7J41+IG2wNgURbGpo7FoixkFGdQ6CnEH/Dj1358AR++gI8IewS9E3rTI7YHUY4onDYn5VXlZJVlkV1qDngOug82SNuqrJyTeg5b87aSU5ZDv8R+9E/sT6fITlgtVsqryin3llNWVUZZVRlJEUmcFncaMa4Yfij+gb1Fe3H73PgDfrbmbaXAXUC0M5qSyhIGdxjMDUNuILssm4ySDOwWO5GOSPwBP3kVeeRV5FFaWUppVSn7ivbV7geF4rL+l3FJn0uwW+yUVpWyeNdiluxZQrQzmmnp07i8/+XkleexLX8bb29+mzVZa2q3aWjHofgCPjYf2IxG47K5OL3T6fxy2C+5bsh1R/oZNUkptU5rfcTOp6AFhWBp1aCgtWlXXnwxvPJKg1l+v5uvvkqgY8dp9OnzbOvkV8+m3E28vul1Fu1cRJW/CqvFSmJ4IgOTBpLeMZ2fDfkZ4fZwtNY8uepJ7v/ifq4acBW3nXEbc7fO5a8r/kpyRDIdwrpQWmzFUdWRGF9fIip7YvclorwR7LF9xJ6o2fispTiLBqE2XY8nehMMfAdslaYgAQtYArDtMlh9KxZHJdEd89GJW6mK24jFGqBj2UX0tlxAVIwPHZWBKww6OXqT4Epmr1rKFt+HYPUyKuEiRnccT6Q9GgCLNYDN6cXmqMIVXY41vJRwh5PkiGQiHZGUVpZS4C4gtyyX7LJsskqzyCrNYm/RXj7c8SHl3nIUiv5J/Vn000V0j+1e9/kE/OYo0h6GRZlmfUFFAXuL9tIlugsdIjs0ud8rvBXsPribjJIM+if2JzU2FYDt+dt5ZcMrPLHyCWJdsRx0H2R8r/F8cPUHuGyu2se01g/M9bm9bvIr8smryCOrNIvMkkx8AR9XpV1FckQyAF6/lz2FeyirKqPCW0G3mG50i+mGUgqtNcWVxbi9brwBL1GOKOLC4g7JZ2/RXj7e+TETek9osD9aorSylLVZaxmRMoJIRyRur5un//c0C75bwJhuY7is/2UM6zSsdhsDOsCa/WtIikiiZ1zPZtPNLMnkvW3vMSh5EGNTx6KUwu11s2zvMn4o/oGcshyinFFcPfBqOkd1PqoyN8cf8FPuLWdP4R7e3fIu7+94ny7RXZhx1gzG9RjX7Od0JJW+SuZvn88HOz5gXI9xTEufhtViPfKKQEllCcv3Lefb3G+Z1G8S/ZP6H7KM1+/FoixNpvlN9jes3r+a8T3H0yu+FwBFniIySzLpm9AXu9V+TNtUQ4JCS118sWkxbD70HrstW66iqOgLzjxzPxZLy3vafAEf5VXlxLhimpz/f0v+j0e/fhSbxcZ5Pc4jPiy+tutn84HNFHmK6Badyu8HPsvi3R+zIPcZOulhHNBb8FtMV0KnnBvovPFp1q+KrD0xWdsn7DD9rREREBZThis5kx7RfenYQZGYCM7YArLsKyjUeykOZDO288WM7zOGDh0gMbHuyo22VOwpZvbG2WzJ28JD5z9ErCv2hOS7MWcjNy+6mbiwOOZcOYcwe9iRVxLiFCBBoaXuuw8eeMCc0Yls2NeelzefLVsub3EX0gNfPMBTq5/ioNvcnvGzwT/jXxf/iwhHRO0yHp+H5EeTObfHubx4yYtEWpL4+GOYOxe2bIH8Ak1exBd4L/iVOUEI8NXv4LOH6dSrAOfIV4n2nUZk5iSsVjj/fLj0UnN6xHZKXEsmhGgLLQ0KUo2MGGEuv1i/HsaMaTArPv4irNYYcnPfJD7+Asqqyrh36b1clXYVo7qMarDskyuf5N5l9zKh9wRGdh5JoaeQWatnsTZrLe9cMRdLwQDWroX3ty+m1FXK5pduYdj/JZGba64uSEyEM8+EYcMUiYljSe60kY2uWXRLTGL6DTfQqRM4HEnA707gzhFChBoJCjVXHa1Zc0hQsFpdJCVdTl7ePHJKHmDiO1ewJmsNb29+m02/3kRieCIAb377Jnd9ehdX9L+Cd658h4pyK/PmwSVFl/Bx5U8Z8vg49KydUBWJZfI7WE9LoH/YuSSNMzdWjx8PY8c2PtJ3AjNOxB4QQohaEhSSk6FbNxMU6vH4PNy5+E4yCr8jvKqE9etHsb/sIA+f/zB//PyPTF8wnXlXzWPW6ln89tPfMrb7WGb2eZ277rTyn//UXCM9jv5nf8CmkWdy+aNPcs+5v2XMggVcO+ha/v2g7HohxMlHaiYwrYV6QcEf8DP1vanM2zaPvgl92VsIEfYCPp26hLO7n41FWZixZAajXhrF//b/jz6BS8l4dDYjtrqw22HyZLjlFjjjDLBYRnHFnMtZsvtRzvckUu4tZ8rAKW24sUII0byT4DqTk8Dw4bBnDxQUoLXm9sW3M2/bPJ644Am237KdTVNu4d1RmjM6mUvM7jrzLkYmjuN/mWtQS//Kzr++R4/O0Tz3HGRmwhtvmPMDNVfx/O28v1HuLef2xbeTHJHMmO5jDlMYIYRoOxIUoPa8QuZXH3P1vKt5ds2z/O7M33HnmXcCkJLyayxUkZX1LAcOwK9+aeF/d31A1Owt/G7kPXy/x8KSJfDLX5reqMb6JfbjF0N/gTfg5Yr+V2A7istbhRDiRJLaCWDYMGadAXd/8wv8Ngt/GfsX7hlzT+3siIgBJCT8hNmzM3jySU1ZmeLOWyO4997+xLbw8vn7xt7H9vzt/Gr4r4K0EUIIcfwkKACz933I7RfBhH2KZ+5fTY+ugxvMLy2FBx98kbff7sDQoTm8+WZH+vU7ujw6R3Vm+bTlrVhqIYRofSHffbRm/xqmL5jOuQnDeH92JT3+MbvB/Px8c7nonDnJ3HTTi8yaNZa+feX5zUKI9imkg8KB8gNc9s5ldIzsyJyfL8Y+7Ub4xz9g61bADAM8Zox5++GHir/9LQ6fbwd59UanFEKI9iSkg8LfV/yd3PJc3r/6fXMj2oMPmqEubr2VnGzNmDHmaqLFi80QSYmJkwgPH8D3399DIFB15Awaa09P+xBCtEshGxSKPEW8+M2LXD3watI7ppuJSUnw5z9T9vlqfnJeBTk55gEh55xjZitlpVevx3G7d7F//zNHl+G+febJI8uWtep2CCFEawrZoPDi+hcpqyrjrlF3NZjunTqNyZb32LDDxbvvmhvQ6ktIuJD4+AvZu/cvVFUdxfMDVq0yj3bauLEVSi+EEMERkkHB6/cya/Uszk09l6GdhjaYd/9TMSwOXMBzrjuYcK67yfV79Xocv7+MvXv/3PJMa4LB/hP7xFEhhDgaIRkUah7Xd9eZDVsJW7bAww/DdeOzudH9DLz/fpPrR0QMICXl12RlPUdR0Zcty7QmKGRlHU/RhRAiqEIyKDyz5hn6JvRlQu8JtdMCAZg+HWJi4PHXO5jHdL78crNp9OjxIC5XD7Ztm4rPV3zkTDdtMn+lpSCEOImFXFDwB/ysy1rHJX0uqX2MI8CLL8LXX8Pjj0NisgVuuAH++19zgrgJtpUbSJ9zFpWeDHbuvOXwmR48aC5jAgkKQoiTWsgFhb1Fe6n0VzZ4fmpFBdx9t7lJ7bqaZ2Jff715hvMvf1lXodf34IO4nnyNtPUTyc19nezsl5rPtKbrKC3NBIVT7Gl3QojQEXJBYVv+NgD6J9YFhTffhIICuP9+qH3ed2qquZFt+XLo1w+efbYukZIS04pQisSHvyLRMpYdO6aTl9f0OYjaoDBhgolAxS3obhJCiDYQekEhzwSFfolm8CKt4emnYfBgOPvsRgvfdps5+3zWWeYBCd9+a6YvXgxeLzz9NKqggAFv9CI6eiRbt06hsPC/h2a6aZMZPvX008176UISQpykQi4obM/fToeIDsSFxQGwYoWps2+5pV4rob4ePeDttyEszLQcwFyVlJQEv/oV3HYblhdeZnDVXwkP78PmzZdRXr69YRobN5qok5Ji3ssVSEKIk1RQg4JS6kKl1A6l1C6l1Mwm5t+glMpTSm2oft0YzPKA6T6qaSWAaSXExcG11x5mpfh4c7Lh9ddNhb5wIUycCFar6XMKC8P2xvsMGrQIi8XF5s2T6q5I8vlMa2PIkLqgIC0FIcRJKmhBQSllBZ4FLgIGANcopQY0seg7Wuv06teLwSoPgNaabfnbas8nZGTA/Pnwi19AePgRVr7tNnNH8rXXmnMKkyaZ6dHRZtS8JUtwubqSlvYuHs9utm27Dq0DsGOHWW/IEOjUyawjQUEIcZIKZkthJLBLa71Ha10FvA1cGsT8juhA+QGKPEW1Vx699Rb4/fDrX7dg5QED4IILzNhFEREwblzdvPHjTeWfkUFs7Dn06vUEBQUfsnnzpXjXfWGWGTzYdEHFx0tQEEKctIIZFFKAjHrvM6unNXaFUmqTUmquUqprEMtTe+VRTffRwoWQng49e7YwgdtvN38vvNBU8DXGjzd/lywBICXlFk477SkKCz8j59Pfou1W6F99tVNKigQFIcRJq61PNC8AUrXWg4ElwKtNLaSUmq6UWquUWpuXl3fMmdVcedQ/sT+FhfDVV2ZI7Ba78EK44w6YMaPh9IEDoUMH+OyzmvLSpcvtDB+wkg5L/BT397Pj+1vx+90SFIQQJ7VgBoX9QP0j/y7V02pprQu01pXVb18EhjWVkNb6ea31cK318KSkpGMu0Pb87UTYI+gS3YVPPjFdR0cVFCwWePLJQ4dOVQrOP98EhUCgdnL4vxfiOOCl4g8/JTv7edavP5PKJCtagoIQ4iQVzKCwBuitlOqhlHIAVwMf1l9AKdWp3tuJwLYglqf2yiOlFAsXQmIijBzZSomPHw95eXVjHOXmwkMPwaRJdL76DQYNWkhVVTbZLITcHPJz3kNreaynEOLkErSgoLX2AbcAn2Aq+zla6y1Kqb8opSZWL3abUmqLUmojcBtwQ7DKA6al0D+pP34/fPyx6Q2yWlsp8fPPN3+rzytw//3g8ZhhV4GEhAmMGvU9MQOmoDTsXHEFq1b14ocfHj22p7gJIUQQ2IKZuNZ6EbCo0bR76/3/B+APwSxDjbKqMjJKMuiX0I/Vq82wFkfVdXQkKSnmCqVnn4VXXzX3Jtx8M/TpU7uI1RpO3MCpwDv0i3qQfWGfsmfP/1FaupYBA95CqUYx+r77zNVKt93WigUVQojmtfWJ5hNme765y7h/Un8WLjQthB//uJUzmTIFDhyAzp3hkUfMq7HqG9jiKvqSnr6Unj0fIS9vDrt23YGuP1BeTg787W/wl7+YITWEEOIECGpL4WRSExT6Jfbjb4vNcEZxca2cyb33wh//aE5IN6fRXc1du/6OqqpsMjOfJBCoIiXlZiIjB8Hs2eZu6IIC+PzzIEQwIYQ4VMgEhckDJjOkwxC6RfRm06ZDryptNYcLCGDObtvttUFBKUWvXo8RCLjJynqB7Ox/Ex7Wn/R/5WA5vS/WXdmoOXMkKAghToiQ6T5y2pwM6jCIXd/Z8fnMqBNtwmIxw13UGxRPKQt9+vyLs87KonfvZ4nb7MKxt5CdF+wg70wP/nlv4i6uHmTv3/+uvR9CCCFaW8gEhRo1jzZos6AApgvp++8PediOw5FMSspv6P3FQHR0NMm/mYf7kuFYiz3s/Fd/fvjbQPjVrwjccC35OfPJz//AjK/UXvl8sHv3ic935UpYvfrE5yvESSAkg0JYGPTu3YaFOOss+PJLmDzZnDPw+2HbNjPuxvPPw7vvoq69loSul9P9xs/RMdH0ea87KQ9sxZMMlv0HyH3ucjZvnsTmzZce/hnRs2dD375QXn7ovKoq82yI0tLgbevx+M1vzBVdubknLk+vFy6/HK6+Wp6Qd6rQGp544uRoQb/0khluf/36ti7JsdNan1KvYcOG6eNx3nlajxhxXEkcP59P64ce0tpu1zomRuuwMK3NV9u8XC6tN22qW/6GG7QGHejUSRdtnad9qR2198x0nZExSy9bZtOrVvXRhYUrdCAQaJhPIKB1v34mzZdeqpteXKz1n/+sdceOZt4tt5yQzT4qS5bU7Y9XXjlx+b77bl2+X3994vI9GllZ5rNtDYGA1rt2tU5aTaV9Ijz5pPm84uO1zs8/MXk2Vlmp9W9+U/fdufjio09j3jzze/3hh4bTW2k/Amt1C+rYNq/kj/Z1PEEhENA6IUHrG2885iRa1/r1Wl9zjdZ33qn1f/6j9cqV5gtRWdlwua+/1rprV62//NK8f+IJ89GtW6cLC7/QX36ZrJcuRa9c2VPv2jVDZ2W9qAsKluiqT94zy9ntWp9xRl16V11lpl90kYmSkZFaFxWduO0+ktJSrVNTte7dW+vOnbW+4ooTl/e4cVqnpJjAfDIGy3XrtLZYtH7nndZJ7y9/Md+FRYuOL52NG7X2eOre792rdWys1t26aT15stZz5x5f+s1ZssTsjzFjtLZatb7ppuNPc8cOk97SpS1bPjNT6x/9yOzH3/1O6/vvN/+vXXvosl5v05V8SUndQdp119VNf/xxrcPDtZ4+XevNm49pc2pIUGhCZqbZ4qefPuYk2k79L1JRkdYREVpff73WBw9q77qvdc6u5/SGDeP10qVWvXQpeulSdO45aG+0RefeNUxr0IXLntae5R+anfCnP5m01q417596qmFeK1aYH9htt524Iz6tta6qMvkqZcrwy1+aoNU4UAbDjh1mX/z1r6YiS0oyP+KjFQho/eKLJvhWVDS9zM6dZluP1k03mTKef/7Rr9tYTo75HoHW3btrXVbWcL7fr/Xdd2v93/8ePp0Pq79TP/tZ3bRp07R2Os1+TEkxn+fChYdPJy9P6wMHWlb2Awe0fvllrePitB440FSqd95p8lm1qmVpNGXr1rrKedSoht/9AwcO/S0sXqx1YqLZj2++aaYVF5uAeOmlDZetqjLdFFdddWi+v/+9yXPCBLMN33xjDgBsNtN6cLnM/LvvPuZNk6DQhIULzRYvX37MSZw8br5ZN+hy6tRJ63ff1X5fpa6o+F4XbpmjA1aLPnBdD71qUYL229GZl6ILh6Ar45Te+OXZevv26Xrfvke094xBOtCrl6kEtm7Vun9/XdvCANOKOZwXXjAVaeNK5Wj4/Vq/8YbWvXqZPO+800yvqXCWLDn2tBtbv978CDMyGk6/6y7zI8zO1vr9902+H398dGkXF2s9ZUrd51I/2Gqt9fff17XUpkw5tJLx+7X+9FOtP/ig6bQjIsxLqUO7GRorKWn43uczFabfb97/5jfm6Pqll0x5ZsxouPwjj5jpiYmmwm7KDz+YbpuaLtDPP9d62zZz9F7zGZaXaz1kiKnA9+w5NA2vV+t//EPrqCgTiGuOsIuKTHAZMsQciY8fr/WZZ5rvp8Vi8jvttLrur+Ji8zsYMsQE3ZaoqjIt7z/8wXSpJiWZoHD77Sb9mtb50qUmzylTtHa7zef2yCPmcxg40GxzfTWthW++qZtW080FWi9bVjd9506tHQ5zkFdYaPbn2LFmOzt31rqgwOz/Bx80+/cYSVBowoMPmi0+mXpKjll2tqnYHn9c61df1To93WzcBReYI6g77jDvq38c/muu1AGbVWvQOfeepdetG6W//DJRL12K3vwn80XN/XV/7Y+L0oEOHUwgKC42P8a4OJOfx6P1//2fad7m5JhyPPpo3Re9SxetZ8/Wevt207dbU/kcSW6u+cGD+UF/9FFdZVlebo6Sbr+9ZWkdLt9vvtH6xz+uK2+/fnWVXUmJ+TFOnmzeezzmaK/m6LekxBxVzJhhtj8zsy7dkhKtZ83S+vLLTRpWq/myjR2rdYcOZhu0NvvG6TQVaE05Xn7ZzKusNJVGTVAErefPb1j+554z09980/x98MHm98Pf/24qrBdfrJv261+b9c4+2+xjm81M09r0qVqtdUfZa9aY+Wefbf5OnXpoHl6v1qNHm5bcxo1a9+ypdd++Wk+aZKbVP+rftcvsz/R0rV9/3Xxnn3jCdNENGlT33e3e3QSH554zFb7VqvWFF2p97rnmyP3887W+7DKt773XBPfGQXX+fHMwo5TWl1xi9u+aNeaz/93vTJpTp5rv88GDpvu0/gFQaqr5/paXm77mSZPMwU7PnuZ9zf6bNs38f9VVdZ9vfYWFWkdHa33WWSafnBzz/vzzze9k5EhTdo/HTIuMNOeKtDYHEjXfgU8/bf4zPkoSFJowZYr5zNslr9dU0B061H2hfvzjuvlffGGm9e7doNuiqipfH9j/tq5KDtcadEVn9P/eidZff91VL18erde/1UH7HVZdOXaI9g8ZUPcDio/X+uc/r/thLFum9dChdXmDCSYXX2yOwB54wHRZPfSQ+aF+8IH5Ab/8sjm6c7m0/te/mq7QJ0wwP8qqKnN0m5Rkzoc8+KCpXN55x6Rbk//QoYe2LNavNyf1k5JMhfnRRybP4cO1fu0180Ot6bKqceONpj/3zDNN5VSz7S6X1n36mIolM9MEspoK5YYbzLkhrU2TFEzgXrrUVK5jx5oWis9n/o+IMCe3a9IYPdpUmiNGmIpi69a68px+utaDB5vK5OyzTQXcVNdezTmnmgD1ySda//OfZtrEiaZyBpN3TXA/eNB8DjWV6WmnmW6fggJTAdc/7+Dzaf3ee3X96G+8YaYvWlT32dd0T9b30Ud1+7HmFR1tPoM5c8y2ZGZqnZZm5qWk1B2pH42sLJN/UlLDvGw2EwQcDpNvz57m83z1VbOe19vw+/enP5n9MWmS+bt8udZvv23WB63/+MfDH/i8+aZJv3dvs0/tdhNwXn7ZrP/iiybYgdbPP1+3XmWl2bd//vPRb/thSFBoQr9+h3bztTuBgLly6ZlntP7uu4bTb7/dBIemvPqqDlx0kS7Y+pretm2a3rbtBv3dd7frb7+9Qu/5pfkRVEWjv33QrjfN6avL080PruInw3R2xiu6oGCxdpft1oH/fmYqtaee0voXv6i7+ulwr759zZFmc2oqtNNPN38vuUTrAQMOTWfkSK3vucccbYL5wb37rkk7MdGc9Ny3ry7dBQvqKqn0dK2/+qphvqtXm4rzjDNMX+5nn5mjwhUrzPR+/UwwiYxs/kTt+eebvOPiTJnrN1MzM03FDabL4v336+ZlZGidnGwqlCVLTDcWmM9Va1OhgDmyz8zU+t//Nl0WNUewV15pKvRBg0z5bDYToH0+E8xuvLGulVIjJ8cEgKQk01VSc6LV4zFdGWFh5qAjKsrk0a2b1s8+2zCNa64x29Jcczw725y72bXLtNKaCmoFBeYAp6XnF5rj85m85s4121qT3nffmQOm5OTD9yXn5JiWHWh9661109eubXm34ooVdcGppnvO56sLfDabOSg5AVoaFJRZ9tQxfPhwvXbt2qNez+2GyEgzNNH99wehYO1YoKoCz7/vp+SseMpi8qio2EJZ8TeEbcilZADoesOPWyxhOJ3dcLm6YrPFAhasPgfRcWcRl3ghYTrZDBp48KC5u9tmMyPJOp3NF+CHH6B7d3A4zHXgU6ea6YWF5uXxQHQ0dOlipns88M9/wlNPQUb1E2E7dIAVKw69QeXjjyE7G66//ujGUV+2DCZMgNhYWLTIPNe1KV9/DaNHm+FNVq8+9NmvX3wBH30Ef/iDGRG3vhUrzHM6KqufQxUWZu6Ej42FkhLo2NH8n5NjwiKY/TBxIrz8shlOJTMTRo2CmBhzU1509JG3zeMx+dQv6/bt8I9/mHzsdhg7Fi691Hx+9QUC5p6YqKgj59PWAoEjD0szc6b5jnz1lalAjsW+feZ+oTvvrEvjv/+F6dPN9/QEDWGjlFqntR5+xOVCJSisWWMeqDNvnrk3SRw/r/cgfn85gYCHqqpsKiq2UVGxA4/nByorM/D7SwGN11uI12tuQLNao7Hbk3A6OxERMYjIyKFERg4mPLwfNltM85n95z+QlgYjRrS8gH4/fPopzJ9vhh8fOPC4tvcQu3ebyjYx8fDLvfIKDBsGgwcffR75+ebBTVu2mJuifvKTunm//a0JKNdcY149e5oKu7HiYjM9PPzo8w91Ne3QIwWPU4AEhUbefht++lPYuRN69QpCwUSztNZUVGynsPAz3O5deL15VFZmUla2Eb+/pHY5my0Bi8VB/RvtLRYHTmcXnM6uhIX1Jjy8H+Hh/QgL63H4ICKEaECCQhPKy00LvB0E/XZB6wAez/eUl2+hvHwrlZX70NqPeUypAiAQ8FBZmUll5T48nn1A3ffVZovFbu+A3Z6AxeLE5yvC7y8lPLwfMTHnEBbWk8rK/Xi9B4iKGkFc3His1rC22Vgh2lhLg0LIDJ0NEBHR1iUQ9SllISysF2FhvUhMnHjE5f1+N273d1RUfIfHsw+PZy9ebx5ebz6BQCUOR2es1gjKyr6hoOCjQ9a3WMIJD++L31+K31+B05lSnX9vwsL6EBbWg0CgEp+vBIvFicuVisvVHav12LtdtNaUl2/C4eiIw9HhmNMR4kQJqaAgTm1WaxiRkUOIjDzyELeVlVlUVeXidHbBZouhqGg5+fnv4/HswWbri8USRmVlJiUlqzlwYA7Q/GizTmd3IiLSsNniCAQ8BALu6r8e7PYEIiLScDq74fXmUVWVjd2eSGTkULT2k5HxOKWlq1HKTnLyFDp3/hVRUWdgschPT5ycQqr7SIimBAKVuN178Hj2YrGEYbPFEAhU4PHsw+3eQ0XFVsrLt+D3l2OxuLBYnFgsYVgsLqqqcnC7d6C1DzDnRXy+IsAPgMvVky5dbsft3k1Oziv4/aVYrZFER4/Gbk8EdIAz5O0AAAu0SURBVHWXmRet/Tgcybhc3QELZWXfUFGxDaezO1FRw4mMHIzT2RWHoxNebz6VlT8A1LZy/P5yvN58bLZ4HI4jnPwWIUfOKQhxggQCXrzeA9jtiVgsTvx+N+Xl3+LzFRMbe25tq8DnK+HgwY8pKvqC4uKv8PvLUMoCWFDKjlIWqqpy8HrzAHC5ehAePgCPZy8VFds4XGumIUV09FnExo6lqmo/5eVb0NqHw9EZpzMFh6MTTmcnrNYYLBYnVms4NlscdnsCStnR2lf98gMBLJYI7PYErNZIlFJHtW/8/nJyc9/E7y+lU6ebsNlOgUtV2ykJCkKcovz+CrT2Nri6yucrw+3eSWVlRm0XldPZHQhUXwa8F6s1Crs9Ebd7FwUFCygrW4/D0ZHw8DQsFidVVVnVJ97zjqlcStmqW0hhKGWtnmbHZovFZovG5yuhqioHCBAePgCXqxsFBR9Vt5zAbk+iW7c/YLPFUVn5Az5fITUPiXI6O9fbnu+orMyo3p547Pbk6vkpOJ3dsNsTa4OT3+/B5yvE5yvEYgnD4eiIUlbKy7dSXr4Rl6snMTGjq4NvaJOgIESI8/s9WK2uQ6YHAl6qqnLx+0sIBCoJBCrwegvx+QrQ2o9StupK34pSFvz+MrzeAny+QgIBN36/m5pWizkxX4zfX4zVGo3D0QkIUF6+Bbd7J7Gx59Kly20oZWfPnt9TVLSsthxWaxRKWdE60ODSZFA4HB3x+8sbTTdMYHLi95fVdts1WoL6rSqHI4WEhJ/gdKZgs8Xidu+mtHQ1Hk8GDkdHnM5OBAKV/9/evcfIVZZxHP/+ZqY7eyudLZZGtrUXIdZyv4SAKOFmKEiAPyBUEFFJ+AcjqInSgDf+UaIRMUEu4VawoQQs2hDkVkgNiUAL1AKFSqG0FCm7lW7dtszu7M7jH++7w7Ddy7Rldm7PJ9l0zpmz0+c578w8e95zzvvGHHeQz2cx6yed7qS1dR7J5BT6+jbT1/c+Zv2Y5Wlqmk4mcxqZzClITeTzHyNNoqnpICDJtm3L6OpaitkAHR1nkMmcRmvrPJqbZ+9x4YKZkc/vjlfP7UJqIpFIk81uYteuteRy28hkTmXy5BP261xUVRQFSQuAm4EkcKeZ/WbY82ngPuA44L/AxWb27liv6UXBudoUrsR6nWSyhXR6BonEJ3exDwz0ks1uQkrQ3Dy3UMxC11w3fX3/iZcmbyab3YRZjmSynWSynVSqg1QqE2+i3Mrg4C7a24+gre0Idu5cQ1fXUnp6VhYKTCLRyuTJx9HcPJdc7kP6+7eSSDSTSk0llZoSj4RS9PVtZvfu9QwO9pJOf4F0upNEogUQ2exGentXM1aX3gEHnEwqdQA9PSvJ53cX1icSzQzdixO66XIUX2o9mlSqg1mzrmfmzB/t9b6HKrgkVeFPjVuArwNbgFWSlpvZuqLNrgC2m9khkhYCNwIXlysm51zlSKK9feS7ylOpySM+l0hMil1HBwPjfp/toa1tPtOnXwJAPt/PwMD2eJPk/n/15XI99PauinG2YNZPLtfNwEAvHR1n0tIyu/D/7tz5Ch9//DbZ7EYGBnqBwTDOkFIkEpMKxS2ZbCOfz5HPZ+Nd/0eSSk1h+/an+eijx0mnO/c77vGU7UhB0knAL83srLi8CMDMfl20zRNxm39KSgFbgWk2RlB+pOCcc3uv1COFcp596QTeK1reEteNuI2FzsEdwIFljMk559wYauKUvKQrJa2WtLq7e9+unHDOOTe+chaF94GZRcsz4roRt4ndR1MIJ5w/xczuMLPjzez4adOmlSlc55xz5SwKq4BDJc2R1AQsBJYP22Y5cHl8fCHwzFjnE5xzzpVX2a4+MrMBSd8HniBcknq3mb0u6QbCDEDLgbuA+yVtAD4iFA7nnHMVUtZRuczsMeCxYet+XvQ4C1xUzhicc86VriZONDvnnJsYXhScc84V1NzYR5K6gU37+OufA7Z9huFUSj3kUQ85QH3kUQ85QH3kUc4cZpnZuJdv1lxR2B+SVpdyR1+1q4c86iEHqI886iEHqI88qiEH7z5yzjlX4EXBOedcQaMVhTsqHcBnpB7yqIccoD7yqIccoD7yqHgODXVOwTnn3Nga7UjBOefcGBqmKEhaIGm9pA2Srq10PKWQNFPSs5LWSXpd0tVx/VRJT0l6K/7bUelYSyEpKekVSY/G5TmSXoht8mAcI6tqScpIeljSm5LekHRSLbaFpB/G99Nrkh6Q1FwLbSHpbkldkl4rWjfi/lfwx5jPWknHVi7yT4ySw2/je2qtpEckZYqeWxRzWC/prImIsSGKQtEscGcD84FvSppf2ahKMgD82MzmAycCV8W4rwVWmNmhwIq4XAuuBt4oWr4RuMnMDgG2E2biq2Y3A4+b2TzgKEIuNdUWkjqBHwDHm9nhhHHJhmY9rPa2uBdYMGzdaPv/bODQ+HMlcOsExTiee9kzh6eAw83sSODfwCKA+FlfCBwWf+dP8busrBqiKAAnABvM7B0z6weWAudXOKZxmdkHZvZyfNxL+BLqJMS+OG62GLigMhGWTtIM4BvAnXFZwOnAw3GTqs5D0hTgFMIgjphZv5n1UINtQRjzrCUOV98KfEANtIWZ/YMwcGax0fb/+cB9FjwPZCR9fmIiHd1IOZjZk3GSMYDnCdMMQMhhqZn1mdlGYAPhu6ysGqUolDILXFWTNBs4BngBmG5mH8SntgLTKxTW3vgD8BM+men8QKCn6MNQ7W0yB+gG7oldYHdKaqPG2sLM3gd+B2wmFIMdwEvUVlsUG23/1+pn/nvA3+PjiuTQKEWhpklqB/4CXGNm/yt+Ls4/UdWXkEk6F+gys5cqHct+SAHHArea2THALoZ1FdVIW3QQ/gKdAxwMtLFnd0ZNqoX9PxZJ1xG6jJdUMo5GKQqlzAJXlSRNIhSEJWa2LK7+cOhQOP7bVan4SnQycJ6kdwldd6cT+uczsQsDqr9NtgBbzOyFuPwwoUjUWlucCWw0s24zywHLCO1TS21RbLT9X1OfeUnfAc4FLi2aaKwiOTRKUShlFriqE/vd7wLeMLPfFz1VPGPd5cDfJjq2vWFmi8xshpnNJuz7Z8zsUuBZwox7UOV5mNlW4D1JX4qrzgDWUWNtQeg2OlFSa3x/DeVRM20xzGj7fznw7XgV0onAjqJupqoiaQGha/U8M9td9NRyYKGktKQ5hJPmL5Y9IDNriB/gHMKZ/beB6yodT4kxf5VwOLwWWBN/ziH0x68A3gKeBqZWOta9yOlU4NH4eG58k28AHgLSlY5vnNiPBlbH9vgr0FGLbQH8CngTeA24H0jXQlsADxDOg+QIR25XjLb/ARGuOHwbeJVwtVW15rCBcO5g6DN+W9H218Uc1gNnT0SMfkezc865gkbpPnLOOVcCLwrOOecKvCg455wr8KLgnHOuwIuCc865Ai8Kzk0gSacOjRLrXDXyouCcc67Ai4JzI5D0LUkvSloj6fY4F8ROSTfFuQhWSJoWtz1a0vNF4+EPjel/iKSnJf1L0suSvhhfvr1oXoYl8c5i56qCFwXnhpH0ZeBi4GQzOxoYBC4lDB632swOA1YCv4i/ch/wUwvj4b9atH4JcIuZHQV8hXAnK4TRbq8hzO0xlzD2kHNVITX+Js41nDOA44BV8Y/4FsJAa3ngwbjNn4FlcZ6FjJmtjOsXAw9Jmgx0mtkjAGaWBYiv96KZbYnLa4DZwHPlT8u58XlRcG5PAhab2aJPrZR+Nmy7fR0jpq/o8SD+OXRVxLuPnNvTCuBCSQdBYR7gWYTPy9BIopcAz5nZDmC7pK/F9ZcBKy3MlLdF0gXxNdKSWic0C+f2gf+F4twwZrZO0vXAk5IShBEtryJMrHNCfK6LcN4BwpDNt8Uv/XeA78b1lwG3S7ohvsZFE5iGc/vER0l1rkSSdppZe6XjcK6cvPvIOedcgR8pOOecK/AjBeeccwVeFJxzzhV4UXDOOVfgRcE551yBFwXnnHMFXhScc84V/B8p61BfOuh2igAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 973us/sample - loss: 0.2264 - acc: 0.9375\n",
      "Loss: 0.2263956613107541 Accuracy: 0.937487\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 3.1223 - acc: 0.1817\n",
      "Epoch 00001: val_loss improved from inf to 2.00249, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_BN_8_conv_checkpoint/001-2.0025.hdf5\n",
      "36805/36805 [==============================] - 105s 3ms/sample - loss: 3.1226 - acc: 0.1817 - val_loss: 2.0025 - val_acc: 0.4067\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.0484 - acc: 0.3659\n",
      "Epoch 00002: val_loss improved from 2.00249 to 1.20355, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_BN_8_conv_checkpoint/002-1.2036.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 2.0484 - acc: 0.3659 - val_loss: 1.2036 - val_acc: 0.6534\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5205 - acc: 0.5180\n",
      "Epoch 00003: val_loss improved from 1.20355 to 0.92815, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_BN_8_conv_checkpoint/003-0.9282.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 1.5204 - acc: 0.5180 - val_loss: 0.9282 - val_acc: 0.7065\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1770 - acc: 0.6265\n",
      "Epoch 00004: val_loss improved from 0.92815 to 0.65050, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_BN_8_conv_checkpoint/004-0.6505.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 1.1772 - acc: 0.6265 - val_loss: 0.6505 - val_acc: 0.8171\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9515 - acc: 0.7014\n",
      "Epoch 00005: val_loss improved from 0.65050 to 0.51835, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_BN_8_conv_checkpoint/005-0.5183.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.9516 - acc: 0.7014 - val_loss: 0.5183 - val_acc: 0.8619\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7968 - acc: 0.7544\n",
      "Epoch 00006: val_loss improved from 0.51835 to 0.42003, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_BN_8_conv_checkpoint/006-0.4200.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.7968 - acc: 0.7544 - val_loss: 0.4200 - val_acc: 0.8896\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6756 - acc: 0.7942\n",
      "Epoch 00007: val_loss improved from 0.42003 to 0.36371, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_BN_8_conv_checkpoint/007-0.3637.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.6755 - acc: 0.7942 - val_loss: 0.3637 - val_acc: 0.8959\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5885 - acc: 0.8194\n",
      "Epoch 00008: val_loss improved from 0.36371 to 0.35705, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_BN_8_conv_checkpoint/008-0.3570.hdf5\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.5885 - acc: 0.8194 - val_loss: 0.3570 - val_acc: 0.8984\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5150 - acc: 0.8438\n",
      "Epoch 00009: val_loss improved from 0.35705 to 0.30787, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_BN_8_conv_checkpoint/009-0.3079.hdf5\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.5150 - acc: 0.8438 - val_loss: 0.3079 - val_acc: 0.9119\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4587 - acc: 0.8571\n",
      "Epoch 00010: val_loss improved from 0.30787 to 0.28881, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_BN_8_conv_checkpoint/010-0.2888.hdf5\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.4587 - acc: 0.8571 - val_loss: 0.2888 - val_acc: 0.9180\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4192 - acc: 0.8723\n",
      "Epoch 00011: val_loss improved from 0.28881 to 0.28041, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_BN_8_conv_checkpoint/011-0.2804.hdf5\n",
      "36805/36805 [==============================] - 94s 3ms/sample - loss: 0.4194 - acc: 0.8722 - val_loss: 0.2804 - val_acc: 0.9222\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3918 - acc: 0.8798\n",
      "Epoch 00012: val_loss did not improve from 0.28041\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.3918 - acc: 0.8798 - val_loss: 0.3238 - val_acc: 0.9008\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3628 - acc: 0.8888\n",
      "Epoch 00013: val_loss improved from 0.28041 to 0.24976, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_BN_8_conv_checkpoint/013-0.2498.hdf5\n",
      "36805/36805 [==============================] - 94s 3ms/sample - loss: 0.3628 - acc: 0.8888 - val_loss: 0.2498 - val_acc: 0.9327\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3357 - acc: 0.8960\n",
      "Epoch 00014: val_loss improved from 0.24976 to 0.24816, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_BN_8_conv_checkpoint/014-0.2482.hdf5\n",
      "36805/36805 [==============================] - 94s 3ms/sample - loss: 0.3358 - acc: 0.8960 - val_loss: 0.2482 - val_acc: 0.9306\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3154 - acc: 0.9026\n",
      "Epoch 00015: val_loss improved from 0.24816 to 0.20185, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_BN_8_conv_checkpoint/015-0.2018.hdf5\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.3154 - acc: 0.9025 - val_loss: 0.2018 - val_acc: 0.9418\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2877 - acc: 0.9105\n",
      "Epoch 00016: val_loss did not improve from 0.20185\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.2877 - acc: 0.9105 - val_loss: 0.2531 - val_acc: 0.9278\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2800 - acc: 0.9142\n",
      "Epoch 00017: val_loss improved from 0.20185 to 0.17646, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_BN_8_conv_checkpoint/017-0.1765.hdf5\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.2800 - acc: 0.9141 - val_loss: 0.1765 - val_acc: 0.9518\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2625 - acc: 0.9191\n",
      "Epoch 00018: val_loss did not improve from 0.17646\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.2626 - acc: 0.9191 - val_loss: 0.1962 - val_acc: 0.9436\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2488 - acc: 0.9226\n",
      "Epoch 00019: val_loss did not improve from 0.17646\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.2488 - acc: 0.9226 - val_loss: 0.2007 - val_acc: 0.9390\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2407 - acc: 0.9258\n",
      "Epoch 00020: val_loss did not improve from 0.17646\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.2407 - acc: 0.9259 - val_loss: 0.2016 - val_acc: 0.9464\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2272 - acc: 0.9287\n",
      "Epoch 00021: val_loss improved from 0.17646 to 0.17598, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_BN_8_conv_checkpoint/021-0.1760.hdf5\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.2274 - acc: 0.9287 - val_loss: 0.1760 - val_acc: 0.9492\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2259 - acc: 0.9293\n",
      "Epoch 00022: val_loss did not improve from 0.17598\n",
      "36805/36805 [==============================] - 94s 3ms/sample - loss: 0.2263 - acc: 0.9292 - val_loss: 0.1890 - val_acc: 0.9520\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2172 - acc: 0.9316\n",
      "Epoch 00023: val_loss did not improve from 0.17598\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.2174 - acc: 0.9315 - val_loss: 0.2393 - val_acc: 0.9304\n",
      "Epoch 24/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2085 - acc: 0.9331\n",
      "Epoch 00024: val_loss improved from 0.17598 to 0.16472, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_BN_8_conv_checkpoint/024-0.1647.hdf5\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.2087 - acc: 0.9331 - val_loss: 0.1647 - val_acc: 0.9513\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2025 - acc: 0.9368\n",
      "Epoch 00025: val_loss did not improve from 0.16472\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.2026 - acc: 0.9368 - val_loss: 0.2072 - val_acc: 0.9406\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1839 - acc: 0.9431\n",
      "Epoch 00026: val_loss did not improve from 0.16472\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.1842 - acc: 0.9430 - val_loss: 0.1964 - val_acc: 0.9462\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1822 - acc: 0.9427\n",
      "Epoch 00027: val_loss improved from 0.16472 to 0.15812, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_BN_8_conv_checkpoint/027-0.1581.hdf5\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.1822 - acc: 0.9427 - val_loss: 0.1581 - val_acc: 0.9527\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1705 - acc: 0.9471\n",
      "Epoch 00028: val_loss did not improve from 0.15812\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.1706 - acc: 0.9471 - val_loss: 0.1849 - val_acc: 0.9490\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1698 - acc: 0.9469\n",
      "Epoch 00029: val_loss did not improve from 0.15812\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.1698 - acc: 0.9469 - val_loss: 0.1897 - val_acc: 0.9453\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1607 - acc: 0.9506\n",
      "Epoch 00030: val_loss improved from 0.15812 to 0.15529, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_BN_8_conv_checkpoint/030-0.1553.hdf5\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.1607 - acc: 0.9506 - val_loss: 0.1553 - val_acc: 0.9555\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1568 - acc: 0.9504\n",
      "Epoch 00031: val_loss improved from 0.15529 to 0.14677, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_BN_8_conv_checkpoint/031-0.1468.hdf5\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.1567 - acc: 0.9504 - val_loss: 0.1468 - val_acc: 0.9567\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1478 - acc: 0.9525\n",
      "Epoch 00032: val_loss did not improve from 0.14677\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.1480 - acc: 0.9525 - val_loss: 0.1566 - val_acc: 0.9574\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1479 - acc: 0.9526\n",
      "Epoch 00033: val_loss did not improve from 0.14677\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.1479 - acc: 0.9526 - val_loss: 0.1594 - val_acc: 0.9585\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1394 - acc: 0.9568\n",
      "Epoch 00034: val_loss did not improve from 0.14677\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.1396 - acc: 0.9567 - val_loss: 0.1902 - val_acc: 0.9471\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1488 - acc: 0.9532\n",
      "Epoch 00035: val_loss improved from 0.14677 to 0.14428, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_BN_8_conv_checkpoint/035-0.1443.hdf5\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.1488 - acc: 0.9532 - val_loss: 0.1443 - val_acc: 0.9588\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1278 - acc: 0.9604\n",
      "Epoch 00036: val_loss did not improve from 0.14428\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.1280 - acc: 0.9604 - val_loss: 0.1560 - val_acc: 0.9541\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1278 - acc: 0.9589\n",
      "Epoch 00037: val_loss did not improve from 0.14428\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.1278 - acc: 0.9589 - val_loss: 0.1539 - val_acc: 0.9539\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1223 - acc: 0.9618\n",
      "Epoch 00038: val_loss did not improve from 0.14428\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.1222 - acc: 0.9618 - val_loss: 0.1526 - val_acc: 0.9567\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1206 - acc: 0.9615\n",
      "Epoch 00039: val_loss improved from 0.14428 to 0.14110, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_BN_8_conv_checkpoint/039-0.1411.hdf5\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.1206 - acc: 0.9615 - val_loss: 0.1411 - val_acc: 0.9597\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1170 - acc: 0.9631\n",
      "Epoch 00040: val_loss did not improve from 0.14110\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.1170 - acc: 0.9631 - val_loss: 0.1590 - val_acc: 0.9590\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1166 - acc: 0.9619\n",
      "Epoch 00041: val_loss did not improve from 0.14110\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.1166 - acc: 0.9619 - val_loss: 0.1524 - val_acc: 0.9588\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1047 - acc: 0.9667\n",
      "Epoch 00042: val_loss improved from 0.14110 to 0.13818, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_BN_8_conv_checkpoint/042-0.1382.hdf5\n",
      "36805/36805 [==============================] - 94s 3ms/sample - loss: 0.1047 - acc: 0.9667 - val_loss: 0.1382 - val_acc: 0.9597\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1042 - acc: 0.9665\n",
      "Epoch 00043: val_loss did not improve from 0.13818\n",
      "36805/36805 [==============================] - 94s 3ms/sample - loss: 0.1042 - acc: 0.9665 - val_loss: 0.1538 - val_acc: 0.9581\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1025 - acc: 0.9665\n",
      "Epoch 00044: val_loss did not improve from 0.13818\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.1025 - acc: 0.9665 - val_loss: 0.1839 - val_acc: 0.9476\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1036 - acc: 0.9658\n",
      "Epoch 00045: val_loss did not improve from 0.13818\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.1038 - acc: 0.9658 - val_loss: 0.1987 - val_acc: 0.9443\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1088 - acc: 0.9649\n",
      "Epoch 00046: val_loss did not improve from 0.13818\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.1088 - acc: 0.9649 - val_loss: 0.1801 - val_acc: 0.9504\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0899 - acc: 0.9707\n",
      "Epoch 00047: val_loss improved from 0.13818 to 0.13159, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_BN_8_conv_checkpoint/047-0.1316.hdf5\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0899 - acc: 0.9707 - val_loss: 0.1316 - val_acc: 0.9653\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0867 - acc: 0.9725\n",
      "Epoch 00048: val_loss did not improve from 0.13159\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0867 - acc: 0.9724 - val_loss: 0.1683 - val_acc: 0.9548\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0954 - acc: 0.9700\n",
      "Epoch 00049: val_loss did not improve from 0.13159\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0955 - acc: 0.9700 - val_loss: 0.1631 - val_acc: 0.9574\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0887 - acc: 0.9715\n",
      "Epoch 00050: val_loss did not improve from 0.13159\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.0887 - acc: 0.9715 - val_loss: 0.1375 - val_acc: 0.9630\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0834 - acc: 0.9734\n",
      "Epoch 00051: val_loss did not improve from 0.13159\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0834 - acc: 0.9734 - val_loss: 0.1521 - val_acc: 0.9571\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0792 - acc: 0.9743\n",
      "Epoch 00052: val_loss did not improve from 0.13159\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0792 - acc: 0.9743 - val_loss: 0.1508 - val_acc: 0.9595\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0810 - acc: 0.9743\n",
      "Epoch 00053: val_loss did not improve from 0.13159\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0810 - acc: 0.9743 - val_loss: 0.1525 - val_acc: 0.9562\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0796 - acc: 0.9750\n",
      "Epoch 00054: val_loss improved from 0.13159 to 0.12584, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_BN_8_conv_checkpoint/054-0.1258.hdf5\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0796 - acc: 0.9750 - val_loss: 0.1258 - val_acc: 0.9637\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0775 - acc: 0.9744\n",
      "Epoch 00055: val_loss did not improve from 0.12584\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0775 - acc: 0.9744 - val_loss: 0.1607 - val_acc: 0.9578\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0687 - acc: 0.9791\n",
      "Epoch 00056: val_loss did not improve from 0.12584\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.0688 - acc: 0.9791 - val_loss: 0.1699 - val_acc: 0.9546\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0781 - acc: 0.9743\n",
      "Epoch 00057: val_loss did not improve from 0.12584\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0785 - acc: 0.9743 - val_loss: 0.1796 - val_acc: 0.9546\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0942 - acc: 0.9698\n",
      "Epoch 00058: val_loss did not improve from 0.12584\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0943 - acc: 0.9698 - val_loss: 0.1482 - val_acc: 0.9637\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0747 - acc: 0.9764\n",
      "Epoch 00059: val_loss did not improve from 0.12584\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0747 - acc: 0.9764 - val_loss: 0.1383 - val_acc: 0.9644\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0633 - acc: 0.9798\n",
      "Epoch 00060: val_loss did not improve from 0.12584\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0633 - acc: 0.9798 - val_loss: 0.1493 - val_acc: 0.9604\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0606 - acc: 0.9804\n",
      "Epoch 00061: val_loss did not improve from 0.12584\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0606 - acc: 0.9804 - val_loss: 0.1408 - val_acc: 0.9634\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0629 - acc: 0.9802\n",
      "Epoch 00062: val_loss did not improve from 0.12584\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0629 - acc: 0.9802 - val_loss: 0.1707 - val_acc: 0.9590\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0623 - acc: 0.9801\n",
      "Epoch 00063: val_loss did not improve from 0.12584\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0623 - acc: 0.9801 - val_loss: 0.2061 - val_acc: 0.9534\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0592 - acc: 0.9819\n",
      "Epoch 00064: val_loss did not improve from 0.12584\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0593 - acc: 0.9819 - val_loss: 0.1344 - val_acc: 0.9637\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0652 - acc: 0.9795\n",
      "Epoch 00065: val_loss did not improve from 0.12584\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0652 - acc: 0.9795 - val_loss: 0.1983 - val_acc: 0.9543\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0617 - acc: 0.9804\n",
      "Epoch 00066: val_loss did not improve from 0.12584\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0617 - acc: 0.9804 - val_loss: 0.1785 - val_acc: 0.9581\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0562 - acc: 0.9817\n",
      "Epoch 00067: val_loss did not improve from 0.12584\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.0562 - acc: 0.9817 - val_loss: 0.1621 - val_acc: 0.9613\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0543 - acc: 0.9826\n",
      "Epoch 00068: val_loss did not improve from 0.12584\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0544 - acc: 0.9826 - val_loss: 0.1924 - val_acc: 0.9522\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0604 - acc: 0.9812\n",
      "Epoch 00069: val_loss did not improve from 0.12584\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0604 - acc: 0.9812 - val_loss: 0.1791 - val_acc: 0.9574\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0520 - acc: 0.9836\n",
      "Epoch 00070: val_loss did not improve from 0.12584\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.0520 - acc: 0.9836 - val_loss: 0.1969 - val_acc: 0.9571\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0564 - acc: 0.9823\n",
      "Epoch 00071: val_loss did not improve from 0.12584\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0564 - acc: 0.9823 - val_loss: 0.1637 - val_acc: 0.9571\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0530 - acc: 0.9830\n",
      "Epoch 00072: val_loss did not improve from 0.12584\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.0530 - acc: 0.9830 - val_loss: 0.1838 - val_acc: 0.9543\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0510 - acc: 0.9840\n",
      "Epoch 00073: val_loss did not improve from 0.12584\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0510 - acc: 0.9840 - val_loss: 0.1887 - val_acc: 0.9557\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0536 - acc: 0.9829\n",
      "Epoch 00074: val_loss did not improve from 0.12584\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0536 - acc: 0.9829 - val_loss: 0.1726 - val_acc: 0.9557\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0517 - acc: 0.9837\n",
      "Epoch 00075: val_loss did not improve from 0.12584\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0517 - acc: 0.9837 - val_loss: 0.1974 - val_acc: 0.9550\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0466 - acc: 0.9851\n",
      "Epoch 00076: val_loss did not improve from 0.12584\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0466 - acc: 0.9851 - val_loss: 0.1816 - val_acc: 0.9574\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0465 - acc: 0.9851\n",
      "Epoch 00077: val_loss did not improve from 0.12584\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0465 - acc: 0.9851 - val_loss: 0.1835 - val_acc: 0.9578\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0451 - acc: 0.9855\n",
      "Epoch 00078: val_loss did not improve from 0.12584\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0451 - acc: 0.9855 - val_loss: 0.1603 - val_acc: 0.9611\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0474 - acc: 0.9852\n",
      "Epoch 00079: val_loss did not improve from 0.12584\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0474 - acc: 0.9852 - val_loss: 0.1831 - val_acc: 0.9571\n",
      "Epoch 80/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0455 - acc: 0.9856\n",
      "Epoch 00080: val_loss did not improve from 0.12584\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0456 - acc: 0.9856 - val_loss: 0.2294 - val_acc: 0.9499\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0571 - acc: 0.9817\n",
      "Epoch 00081: val_loss did not improve from 0.12584\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0571 - acc: 0.9817 - val_loss: 0.1567 - val_acc: 0.9632\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0386 - acc: 0.9876\n",
      "Epoch 00082: val_loss did not improve from 0.12584\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0386 - acc: 0.9876 - val_loss: 0.1788 - val_acc: 0.9604\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0394 - acc: 0.9878\n",
      "Epoch 00083: val_loss did not improve from 0.12584\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0394 - acc: 0.9878 - val_loss: 0.1623 - val_acc: 0.9609\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0420 - acc: 0.9865\n",
      "Epoch 00084: val_loss did not improve from 0.12584\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0420 - acc: 0.9865 - val_loss: 0.1590 - val_acc: 0.9637\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0403 - acc: 0.9877\n",
      "Epoch 00085: val_loss did not improve from 0.12584\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0404 - acc: 0.9877 - val_loss: 0.1656 - val_acc: 0.9623\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0493 - acc: 0.9835\n",
      "Epoch 00086: val_loss did not improve from 0.12584\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0493 - acc: 0.9835 - val_loss: 0.1811 - val_acc: 0.9550\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0381 - acc: 0.9884\n",
      "Epoch 00087: val_loss did not improve from 0.12584\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0381 - acc: 0.9884 - val_loss: 0.1765 - val_acc: 0.9604\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0359 - acc: 0.9886\n",
      "Epoch 00088: val_loss did not improve from 0.12584\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0359 - acc: 0.9886 - val_loss: 0.1866 - val_acc: 0.9602\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0358 - acc: 0.9887\n",
      "Epoch 00089: val_loss did not improve from 0.12584\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0358 - acc: 0.9888 - val_loss: 0.1455 - val_acc: 0.9658\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0375 - acc: 0.9885\n",
      "Epoch 00090: val_loss did not improve from 0.12584\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0375 - acc: 0.9885 - val_loss: 0.2441 - val_acc: 0.9478\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0346 - acc: 0.9892\n",
      "Epoch 00091: val_loss did not improve from 0.12584\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0346 - acc: 0.9892 - val_loss: 0.1374 - val_acc: 0.9681\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0351 - acc: 0.9895\n",
      "Epoch 00092: val_loss did not improve from 0.12584\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0352 - acc: 0.9895 - val_loss: 0.1713 - val_acc: 0.9627\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0403 - acc: 0.9873\n",
      "Epoch 00093: val_loss did not improve from 0.12584\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0403 - acc: 0.9873 - val_loss: 0.1843 - val_acc: 0.9574\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0341 - acc: 0.9893\n",
      "Epoch 00094: val_loss did not improve from 0.12584\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0341 - acc: 0.9893 - val_loss: 0.2059 - val_acc: 0.9534\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0353 - acc: 0.9889\n",
      "Epoch 00095: val_loss did not improve from 0.12584\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0353 - acc: 0.9889 - val_loss: 0.1832 - val_acc: 0.9613\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0377 - acc: 0.9876\n",
      "Epoch 00096: val_loss did not improve from 0.12584\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0377 - acc: 0.9876 - val_loss: 0.2020 - val_acc: 0.9541\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0312 - acc: 0.9901\n",
      "Epoch 00097: val_loss did not improve from 0.12584\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0312 - acc: 0.9901 - val_loss: 0.1772 - val_acc: 0.9604\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0306 - acc: 0.9907\n",
      "Epoch 00098: val_loss did not improve from 0.12584\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0306 - acc: 0.9907 - val_loss: 0.1722 - val_acc: 0.9651\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0312 - acc: 0.9901\n",
      "Epoch 00099: val_loss did not improve from 0.12584\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0312 - acc: 0.9901 - val_loss: 0.1986 - val_acc: 0.9590\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0300 - acc: 0.9911\n",
      "Epoch 00100: val_loss did not improve from 0.12584\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0300 - acc: 0.9911 - val_loss: 0.2023 - val_acc: 0.9590\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0312 - acc: 0.9896\n",
      "Epoch 00101: val_loss did not improve from 0.12584\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0311 - acc: 0.9896 - val_loss: 0.1897 - val_acc: 0.9611\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0328 - acc: 0.9900\n",
      "Epoch 00102: val_loss did not improve from 0.12584\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0328 - acc: 0.9900 - val_loss: 0.1859 - val_acc: 0.9609\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0300 - acc: 0.9905\n",
      "Epoch 00103: val_loss did not improve from 0.12584\n",
      "36805/36805 [==============================] - 90s 2ms/sample - loss: 0.0300 - acc: 0.9905 - val_loss: 0.2161 - val_acc: 0.9567\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0284 - acc: 0.9909\n",
      "Epoch 00104: val_loss did not improve from 0.12584\n",
      "36805/36805 [==============================] - 91s 2ms/sample - loss: 0.0284 - acc: 0.9909 - val_loss: 0.1744 - val_acc: 0.9620\n",
      "\n",
      "1D_CNN_custom_DO_075_DO_BN_8_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 0.1616 - acc: 0.9529\n",
      "Loss: 0.16164906880318314 Accuracy: 0.95285565\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.7998 - acc: 0.2876\n",
      "Epoch 00001: val_loss improved from inf to 1.56511, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_BN_9_conv_checkpoint/001-1.5651.hdf5\n",
      "36805/36805 [==============================] - 114s 3ms/sample - loss: 2.7998 - acc: 0.2876 - val_loss: 1.5651 - val_acc: 0.5090\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4840 - acc: 0.5570\n",
      "Epoch 00002: val_loss improved from 1.56511 to 0.65389, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_BN_9_conv_checkpoint/002-0.6539.hdf5\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 1.4840 - acc: 0.5570 - val_loss: 0.6539 - val_acc: 0.8116\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9697 - acc: 0.7076\n",
      "Epoch 00003: val_loss improved from 0.65389 to 0.47572, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_BN_9_conv_checkpoint/003-0.4757.hdf5\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.9696 - acc: 0.7076 - val_loss: 0.4757 - val_acc: 0.8567\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7106 - acc: 0.7876\n",
      "Epoch 00004: val_loss improved from 0.47572 to 0.37117, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_BN_9_conv_checkpoint/004-0.3712.hdf5\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.7106 - acc: 0.7875 - val_loss: 0.3712 - val_acc: 0.8887\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5557 - acc: 0.8308\n",
      "Epoch 00005: val_loss improved from 0.37117 to 0.29893, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_BN_9_conv_checkpoint/005-0.2989.hdf5\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.5559 - acc: 0.8307 - val_loss: 0.2989 - val_acc: 0.9133\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4595 - acc: 0.8587\n",
      "Epoch 00006: val_loss improved from 0.29893 to 0.23824, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_BN_9_conv_checkpoint/006-0.2382.hdf5\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.4595 - acc: 0.8587 - val_loss: 0.2382 - val_acc: 0.9315\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3900 - acc: 0.8815\n",
      "Epoch 00007: val_loss did not improve from 0.23824\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.3900 - acc: 0.8815 - val_loss: 0.3195 - val_acc: 0.9082\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3443 - acc: 0.8944\n",
      "Epoch 00008: val_loss did not improve from 0.23824\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.3443 - acc: 0.8944 - val_loss: 0.2682 - val_acc: 0.9236\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3062 - acc: 0.9058\n",
      "Epoch 00009: val_loss improved from 0.23824 to 0.22038, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_BN_9_conv_checkpoint/009-0.2204.hdf5\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.3062 - acc: 0.9058 - val_loss: 0.2204 - val_acc: 0.9320\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2746 - acc: 0.9165\n",
      "Epoch 00010: val_loss improved from 0.22038 to 0.18440, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_BN_9_conv_checkpoint/010-0.1844.hdf5\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.2750 - acc: 0.9165 - val_loss: 0.1844 - val_acc: 0.9446\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2595 - acc: 0.9197\n",
      "Epoch 00011: val_loss improved from 0.18440 to 0.17705, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_BN_9_conv_checkpoint/011-0.1770.hdf5\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.2594 - acc: 0.9197 - val_loss: 0.1770 - val_acc: 0.9509\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2233 - acc: 0.9310\n",
      "Epoch 00012: val_loss improved from 0.17705 to 0.16548, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_BN_9_conv_checkpoint/012-0.1655.hdf5\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.2234 - acc: 0.9310 - val_loss: 0.1655 - val_acc: 0.9492\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2185 - acc: 0.9324\n",
      "Epoch 00013: val_loss improved from 0.16548 to 0.15530, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_BN_9_conv_checkpoint/013-0.1553.hdf5\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.2185 - acc: 0.9324 - val_loss: 0.1553 - val_acc: 0.9564\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1869 - acc: 0.9429\n",
      "Epoch 00014: val_loss improved from 0.15530 to 0.14841, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_BN_9_conv_checkpoint/014-0.1484.hdf5\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.1869 - acc: 0.9429 - val_loss: 0.1484 - val_acc: 0.9567\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1782 - acc: 0.9444\n",
      "Epoch 00015: val_loss did not improve from 0.14841\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.1782 - acc: 0.9444 - val_loss: 0.1701 - val_acc: 0.9529\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1662 - acc: 0.9486\n",
      "Epoch 00016: val_loss did not improve from 0.14841\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.1661 - acc: 0.9486 - val_loss: 0.1523 - val_acc: 0.9574\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1540 - acc: 0.9521\n",
      "Epoch 00017: val_loss did not improve from 0.14841\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.1542 - acc: 0.9521 - val_loss: 0.1517 - val_acc: 0.9581\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1553 - acc: 0.9519\n",
      "Epoch 00018: val_loss did not improve from 0.14841\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.1554 - acc: 0.9519 - val_loss: 0.1608 - val_acc: 0.9490\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1402 - acc: 0.9559\n",
      "Epoch 00019: val_loss improved from 0.14841 to 0.13031, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_BN_9_conv_checkpoint/019-0.1303.hdf5\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.1404 - acc: 0.9558 - val_loss: 0.1303 - val_acc: 0.9613\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1317 - acc: 0.9581\n",
      "Epoch 00020: val_loss improved from 0.13031 to 0.12409, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_BN_9_conv_checkpoint/020-0.1241.hdf5\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.1318 - acc: 0.9581 - val_loss: 0.1241 - val_acc: 0.9637\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1275 - acc: 0.9600\n",
      "Epoch 00021: val_loss did not improve from 0.12409\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.1275 - acc: 0.9600 - val_loss: 0.1454 - val_acc: 0.9574\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1140 - acc: 0.9643\n",
      "Epoch 00022: val_loss improved from 0.12409 to 0.12172, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_BN_9_conv_checkpoint/022-0.1217.hdf5\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.1140 - acc: 0.9644 - val_loss: 0.1217 - val_acc: 0.9618\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1102 - acc: 0.9661\n",
      "Epoch 00023: val_loss did not improve from 0.12172\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.1108 - acc: 0.9660 - val_loss: 0.1684 - val_acc: 0.9509\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1205 - acc: 0.9623\n",
      "Epoch 00024: val_loss did not improve from 0.12172\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.1205 - acc: 0.9623 - val_loss: 0.1395 - val_acc: 0.9569\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0943 - acc: 0.9705\n",
      "Epoch 00025: val_loss improved from 0.12172 to 0.11093, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_BN_9_conv_checkpoint/025-0.1109.hdf5\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0943 - acc: 0.9705 - val_loss: 0.1109 - val_acc: 0.9665\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0921 - acc: 0.9705\n",
      "Epoch 00026: val_loss did not improve from 0.11093\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0921 - acc: 0.9705 - val_loss: 0.1135 - val_acc: 0.9674\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0887 - acc: 0.9724\n",
      "Epoch 00027: val_loss did not improve from 0.11093\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0887 - acc: 0.9724 - val_loss: 0.1349 - val_acc: 0.9627\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0854 - acc: 0.9733\n",
      "Epoch 00028: val_loss did not improve from 0.11093\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0854 - acc: 0.9733 - val_loss: 0.1863 - val_acc: 0.9506\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0796 - acc: 0.9746\n",
      "Epoch 00029: val_loss did not improve from 0.11093\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0796 - acc: 0.9746 - val_loss: 0.1167 - val_acc: 0.9648\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0735 - acc: 0.9768\n",
      "Epoch 00030: val_loss did not improve from 0.11093\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0735 - acc: 0.9768 - val_loss: 0.1360 - val_acc: 0.9611\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0705 - acc: 0.9785\n",
      "Epoch 00031: val_loss did not improve from 0.11093\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0706 - acc: 0.9784 - val_loss: 0.1114 - val_acc: 0.9690\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0747 - acc: 0.9767\n",
      "Epoch 00032: val_loss improved from 0.11093 to 0.10473, saving model to model/checkpoint/1D_CNN_custom_DO_075_DO_BN_9_conv_checkpoint/032-0.1047.hdf5\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0747 - acc: 0.9767 - val_loss: 0.1047 - val_acc: 0.9686\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0650 - acc: 0.9800\n",
      "Epoch 00033: val_loss did not improve from 0.10473\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0650 - acc: 0.9800 - val_loss: 0.1251 - val_acc: 0.9620\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0641 - acc: 0.9803\n",
      "Epoch 00034: val_loss did not improve from 0.10473\n",
      "36805/36805 [==============================] - 92s 3ms/sample - loss: 0.0641 - acc: 0.9803 - val_loss: 0.1258 - val_acc: 0.9644\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0602 - acc: 0.9806\n",
      "Epoch 00035: val_loss did not improve from 0.10473\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0603 - acc: 0.9805 - val_loss: 0.1546 - val_acc: 0.9562\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0703 - acc: 0.9779\n",
      "Epoch 00036: val_loss did not improve from 0.10473\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0703 - acc: 0.9779 - val_loss: 0.2047 - val_acc: 0.9497\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0635 - acc: 0.9798\n",
      "Epoch 00037: val_loss did not improve from 0.10473\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0635 - acc: 0.9797 - val_loss: 0.1330 - val_acc: 0.9623\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0547 - acc: 0.9829\n",
      "Epoch 00038: val_loss did not improve from 0.10473\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0547 - acc: 0.9829 - val_loss: 0.1122 - val_acc: 0.9658\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0494 - acc: 0.9843\n",
      "Epoch 00039: val_loss did not improve from 0.10473\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0494 - acc: 0.9843 - val_loss: 0.1404 - val_acc: 0.9604\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0445 - acc: 0.9864\n",
      "Epoch 00040: val_loss did not improve from 0.10473\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0445 - acc: 0.9864 - val_loss: 0.1363 - val_acc: 0.9623\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0492 - acc: 0.9846\n",
      "Epoch 00041: val_loss did not improve from 0.10473\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0492 - acc: 0.9846 - val_loss: 0.1167 - val_acc: 0.9669\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0469 - acc: 0.9854\n",
      "Epoch 00042: val_loss did not improve from 0.10473\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0471 - acc: 0.9854 - val_loss: 0.1457 - val_acc: 0.9623\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0583 - acc: 0.9819\n",
      "Epoch 00043: val_loss did not improve from 0.10473\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0584 - acc: 0.9819 - val_loss: 0.1634 - val_acc: 0.9560\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0543 - acc: 0.9832\n",
      "Epoch 00044: val_loss did not improve from 0.10473\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0543 - acc: 0.9832 - val_loss: 0.1161 - val_acc: 0.9693\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0355 - acc: 0.9883\n",
      "Epoch 00045: val_loss did not improve from 0.10473\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0355 - acc: 0.9883 - val_loss: 0.1197 - val_acc: 0.9693\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0391 - acc: 0.9882\n",
      "Epoch 00046: val_loss did not improve from 0.10473\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0391 - acc: 0.9882 - val_loss: 0.1199 - val_acc: 0.9683\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0405 - acc: 0.9877\n",
      "Epoch 00047: val_loss did not improve from 0.10473\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0406 - acc: 0.9877 - val_loss: 0.1359 - val_acc: 0.9613\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0392 - acc: 0.9877\n",
      "Epoch 00048: val_loss did not improve from 0.10473\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0391 - acc: 0.9877 - val_loss: 0.1211 - val_acc: 0.9669\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0302 - acc: 0.9912\n",
      "Epoch 00049: val_loss did not improve from 0.10473\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0303 - acc: 0.9912 - val_loss: 0.1481 - val_acc: 0.9644\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0413 - acc: 0.9869\n",
      "Epoch 00050: val_loss did not improve from 0.10473\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0413 - acc: 0.9869 - val_loss: 0.1369 - val_acc: 0.9651\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0343 - acc: 0.9895\n",
      "Epoch 00051: val_loss did not improve from 0.10473\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0343 - acc: 0.9895 - val_loss: 0.1307 - val_acc: 0.9651\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0329 - acc: 0.9897\n",
      "Epoch 00052: val_loss did not improve from 0.10473\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0329 - acc: 0.9897 - val_loss: 0.1177 - val_acc: 0.9681\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0359 - acc: 0.9883\n",
      "Epoch 00053: val_loss did not improve from 0.10473\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0361 - acc: 0.9883 - val_loss: 0.1774 - val_acc: 0.9567\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0409 - acc: 0.9870\n",
      "Epoch 00054: val_loss did not improve from 0.10473\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0409 - acc: 0.9870 - val_loss: 0.1099 - val_acc: 0.9720\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0268 - acc: 0.9918\n",
      "Epoch 00055: val_loss did not improve from 0.10473\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.0268 - acc: 0.9918 - val_loss: 0.1264 - val_acc: 0.9676\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0282 - acc: 0.9911\n",
      "Epoch 00056: val_loss did not improve from 0.10473\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0282 - acc: 0.9911 - val_loss: 0.1246 - val_acc: 0.9665\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0296 - acc: 0.9909\n",
      "Epoch 00057: val_loss did not improve from 0.10473\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0299 - acc: 0.9909 - val_loss: 0.1591 - val_acc: 0.9616\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0407 - acc: 0.9880\n",
      "Epoch 00058: val_loss did not improve from 0.10473\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0407 - acc: 0.9880 - val_loss: 0.1198 - val_acc: 0.9718\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0229 - acc: 0.9930\n",
      "Epoch 00059: val_loss did not improve from 0.10473\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0230 - acc: 0.9929 - val_loss: 0.1218 - val_acc: 0.9674\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0327 - acc: 0.9900\n",
      "Epoch 00060: val_loss did not improve from 0.10473\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0328 - acc: 0.9899 - val_loss: 0.1270 - val_acc: 0.9667\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0375 - acc: 0.9883\n",
      "Epoch 00061: val_loss did not improve from 0.10473\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0375 - acc: 0.9883 - val_loss: 0.1304 - val_acc: 0.9669\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0229 - acc: 0.9929\n",
      "Epoch 00062: val_loss did not improve from 0.10473\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0229 - acc: 0.9929 - val_loss: 0.1587 - val_acc: 0.9641\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0210 - acc: 0.9939\n",
      "Epoch 00063: val_loss did not improve from 0.10473\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0209 - acc: 0.9939 - val_loss: 0.1384 - val_acc: 0.9634\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0240 - acc: 0.9928\n",
      "Epoch 00064: val_loss did not improve from 0.10473\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0240 - acc: 0.9928 - val_loss: 0.1343 - val_acc: 0.9672\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0224 - acc: 0.9933\n",
      "Epoch 00065: val_loss did not improve from 0.10473\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0224 - acc: 0.9933 - val_loss: 0.1709 - val_acc: 0.9557\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0256 - acc: 0.9920\n",
      "Epoch 00066: val_loss did not improve from 0.10473\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0256 - acc: 0.9920 - val_loss: 0.1281 - val_acc: 0.9674\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0227 - acc: 0.9927\n",
      "Epoch 00067: val_loss did not improve from 0.10473\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0227 - acc: 0.9927 - val_loss: 0.1317 - val_acc: 0.9679\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0223 - acc: 0.9932\n",
      "Epoch 00068: val_loss did not improve from 0.10473\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0223 - acc: 0.9932 - val_loss: 0.1326 - val_acc: 0.9679\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0301 - acc: 0.9914\n",
      "Epoch 00069: val_loss did not improve from 0.10473\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0301 - acc: 0.9914 - val_loss: 0.1141 - val_acc: 0.9700\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0220 - acc: 0.9933\n",
      "Epoch 00070: val_loss did not improve from 0.10473\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.0220 - acc: 0.9933 - val_loss: 0.1432 - val_acc: 0.9632\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0176 - acc: 0.9948\n",
      "Epoch 00071: val_loss did not improve from 0.10473\n",
      "36805/36805 [==============================] - 93s 3ms/sample - loss: 0.0176 - acc: 0.9948 - val_loss: 0.2227 - val_acc: 0.9495\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0175 - acc: 0.9947\n",
      "Epoch 00072: val_loss did not improve from 0.10473\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0175 - acc: 0.9946 - val_loss: 0.1441 - val_acc: 0.9697\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0285 - acc: 0.9912\n",
      "Epoch 00073: val_loss did not improve from 0.10473\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0285 - acc: 0.9913 - val_loss: 0.1706 - val_acc: 0.9665\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0168 - acc: 0.9950\n",
      "Epoch 00074: val_loss did not improve from 0.10473\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0168 - acc: 0.9950 - val_loss: 0.1855 - val_acc: 0.9585\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0171 - acc: 0.9943\n",
      "Epoch 00075: val_loss did not improve from 0.10473\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0171 - acc: 0.9943 - val_loss: 0.1324 - val_acc: 0.9667\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0168 - acc: 0.9948\n",
      "Epoch 00076: val_loss did not improve from 0.10473\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0168 - acc: 0.9948 - val_loss: 0.1360 - val_acc: 0.9672\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0189 - acc: 0.9942\n",
      "Epoch 00077: val_loss did not improve from 0.10473\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0189 - acc: 0.9942 - val_loss: 0.1740 - val_acc: 0.9637\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0275 - acc: 0.9914\n",
      "Epoch 00078: val_loss did not improve from 0.10473\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0275 - acc: 0.9914 - val_loss: 0.1449 - val_acc: 0.9688\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0150 - acc: 0.9954\n",
      "Epoch 00079: val_loss did not improve from 0.10473\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0150 - acc: 0.9954 - val_loss: 0.1412 - val_acc: 0.9672\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0144 - acc: 0.9958\n",
      "Epoch 00080: val_loss did not improve from 0.10473\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0144 - acc: 0.9958 - val_loss: 0.1554 - val_acc: 0.9655\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0182 - acc: 0.9946\n",
      "Epoch 00081: val_loss did not improve from 0.10473\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0183 - acc: 0.9946 - val_loss: 0.1322 - val_acc: 0.9688\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0287 - acc: 0.9916\n",
      "Epoch 00082: val_loss did not improve from 0.10473\n",
      "36805/36805 [==============================] - 92s 2ms/sample - loss: 0.0287 - acc: 0.9916 - val_loss: 0.1318 - val_acc: 0.9690\n",
      "\n",
      "1D_CNN_custom_DO_075_DO_BN_9_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.1518 - acc: 0.9553\n",
      "Loss: 0.15181389430176123 Accuracy: 0.9553479\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 10):\n",
    "    base = '1D_CNN_custom_DO_075_DO_BN'\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    model = build_1d_cnn_custom_DO_075_DO_BN(conv_num=i)\n",
    "#         model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4),\n",
    "          metrics=['accuracy'])\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model_filename = model_path+'{epoch:03d}-{val_loss:.4f}.hdf5'\n",
    "    checkpointer = ModelCheckpoint(filepath = model_filename, monitor = \"val_loss\", \n",
    "                                   verbose=1, save_best_only=True)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=50)\n",
    "    hist = model.fit(x_train_abs, y_train_onehot, batch_size=64, epochs=500, \n",
    "                     validation_data=[x_val_abs, y_val_onehot], shuffle=True, \n",
    "                     callbacks = [checkpointer, early_stopping])\n",
    "\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "    ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "    ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "    ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    png_path = 'visualization/learning_curve/'\n",
    "    filename = model_name+'.png'\n",
    "    os.makedirs(png_path, exist_ok=True)\n",
    "    fig.savefig(png_path+filename, transparent=True)\n",
    "\n",
    "    model.save(model_path+'000_last.hdf5')\n",
    "    del(model)\n",
    "\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "    model = load_model(model_filename)\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "    print()\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_DO_075_DO_BN_1_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1 (Batc (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1024000)           0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1024000)           0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                16384016  \n",
      "=================================================================\n",
      "Total params: 16,384,656\n",
      "Trainable params: 16,384,528\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 620us/sample - loss: 2.2795 - acc: 0.2860\n",
      "Loss: 2.2795086906458852 Accuracy: 0.2859813\n",
      "\n",
      "1D_CNN_custom_DO_075_DO_BN_2_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_1 (Ba (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_2 (Ba (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 341312)            0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 341312)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                5461008   \n",
      "=================================================================\n",
      "Total params: 5,482,448\n",
      "Trainable params: 5,482,192\n",
      "Non-trainable params: 256\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 926us/sample - loss: 2.4671 - acc: 0.3776\n",
      "Loss: 2.4670915194142027 Accuracy: 0.3775701\n",
      "\n",
      "1D_CNN_custom_DO_075_DO_BN_3_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_3 (Conv1D)            (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_3 (Ba (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_4 (Ba (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_5 (Ba (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                1819664   \n",
      "=================================================================\n",
      "Total params: 1,861,904\n",
      "Trainable params: 1,861,520\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 1.3749 - acc: 0.5931\n",
      "Loss: 1.3749287855092858 Accuracy: 0.59314644\n",
      "\n",
      "1D_CNN_custom_DO_075_DO_BN_4_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_6 (Conv1D)            (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_6 (Ba (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_7 (Ba (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_8 (Ba (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_9 (Ba (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                606224    \n",
      "=================================================================\n",
      "Total params: 669,264\n",
      "Trainable params: 668,752\n",
      "Non-trainable params: 512\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 0.8049 - acc: 0.7769\n",
      "Loss: 0.804949192441265 Accuracy: 0.776947\n",
      "\n",
      "1D_CNN_custom_DO_075_DO_BN_5_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_10 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_10 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_11 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_12 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_12 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_13 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 592, 128)          41088     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_14 (B (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1 (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                403472    \n",
      "=================================================================\n",
      "Total params: 508,112\n",
      "Trainable params: 507,344\n",
      "Non-trainable params: 768\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 0.5975 - acc: 0.8397\n",
      "Loss: 0.597500038320278 Accuracy: 0.8396677\n",
      "\n",
      "1D_CNN_custom_DO_075_DO_BN_6_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_15 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_15 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_16 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_16 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_17 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_17 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_18 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_18 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_19 (Conv1D)           (None, 592, 128)          41088     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_19 (B (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_20 (Conv1D)           (None, 197, 128)          82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_20 (B (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 16)                133136    \n",
      "=================================================================\n",
      "Total params: 320,336\n",
      "Trainable params: 319,312\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.3791 - acc: 0.9024\n",
      "Loss: 0.37910410796494254 Accuracy: 0.9023884\n",
      "\n",
      "1D_CNN_custom_DO_075_DO_BN_7_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_21 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_21 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_22 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_22 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_23 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_23 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_24 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_24 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_25 (Conv1D)           (None, 592, 128)          41088     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_25 (B (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_26 (Conv1D)           (None, 197, 128)          82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_26 (B (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_27 (Conv1D)           (None, 65, 128)           82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_27 (B (None, 65, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_27 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 16)                43024     \n",
      "=================================================================\n",
      "Total params: 312,784\n",
      "Trainable params: 311,504\n",
      "Non-trainable params: 1,280\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.2264 - acc: 0.9375\n",
      "Loss: 0.2263956613107541 Accuracy: 0.937487\n",
      "\n",
      "1D_CNN_custom_DO_075_DO_BN_8_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_28 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_28 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_28 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_29 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_29 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_29 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_30 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_30 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_30 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_31 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_31 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_31 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_32 (Conv1D)           (None, 592, 128)          41088     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_32 (B (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_32 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_33 (Conv1D)           (None, 197, 128)          82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_33 (B (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_33 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_34 (Conv1D)           (None, 65, 128)           82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_34 (B (None, 65, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_34 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_26 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_35 (Conv1D)           (None, 21, 128)           82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_35 (B (None, 21, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_35 (Activation)   (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_27 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 16)                14352     \n",
      "=================================================================\n",
      "Total params: 366,672\n",
      "Trainable params: 365,136\n",
      "Non-trainable params: 1,536\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.1616 - acc: 0.9529\n",
      "Loss: 0.16164906880318314 Accuracy: 0.95285565\n",
      "\n",
      "1D_CNN_custom_DO_075_DO_BN_9_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_36 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_36 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_36 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_37 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_37 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_37 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_28 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_38 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_38 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_38 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_29 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_39 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_39 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_39 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_30 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_40 (Conv1D)           (None, 592, 128)          41088     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_40 (B (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_40 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_31 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_41 (Conv1D)           (None, 197, 128)          82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_41 (B (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_41 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_32 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_42 (Conv1D)           (None, 65, 128)           82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_42 (B (None, 65, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_42 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_43 (Conv1D)           (None, 21, 128)           82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_43 (B (None, 21, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_43 (Activation)   (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_44 (Conv1D)           (None, 7, 256)            164096    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_44 (B (None, 7, 256)            1024      \n",
      "_________________________________________________________________\n",
      "activation_44 (Activation)   (None, 7, 256)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling (None, 2, 256)            0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 16)                8208      \n",
      "=================================================================\n",
      "Total params: 525,648\n",
      "Trainable params: 523,600\n",
      "Non-trainable params: 2,048\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.1518 - acc: 0.9553\n",
      "Loss: 0.15181389430176123 Accuracy: 0.9553479\n"
     ]
    }
   ],
   "source": [
    "log_dir = 'log'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "base = '1D_CNN_custom_DO_075_DO_BN'\n",
    "\n",
    "with open(path.join(log_dir, base), 'w') as log_file:\n",
    "    for i in range(1, 10):\n",
    "        model_name = base+'_{}_conv'.format(i)\n",
    "        print()\n",
    "        print(model_name, 'Model')\n",
    "        model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "        model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "\n",
    "        model = load_model(model_filename)\n",
    "        model.summary()\n",
    "\n",
    "        [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "        print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "        del(model)\n",
    "\n",
    "        log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_DO_075_DO_BN_1_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1 (Batc (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1024000)           0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1024000)           0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                16384016  \n",
      "=================================================================\n",
      "Total params: 16,384,656\n",
      "Trainable params: 16,384,528\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 745us/sample - loss: 5.9978 - acc: 0.3049\n",
      "Loss: 5.9977877699078554 Accuracy: 0.3048806\n",
      "\n",
      "1D_CNN_custom_DO_075_DO_BN_2_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_1 (Ba (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_2 (Ba (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 341312)            0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 341312)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                5461008   \n",
      "=================================================================\n",
      "Total params: 5,482,448\n",
      "Trainable params: 5,482,192\n",
      "Non-trainable params: 256\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 5s 1ms/sample - loss: 3.3951 - acc: 0.4617\n",
      "Loss: 3.395131604470939 Accuracy: 0.46168223\n",
      "\n",
      "1D_CNN_custom_DO_075_DO_BN_3_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_3 (Conv1D)            (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_3 (Ba (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_4 (Ba (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_5 (Ba (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 113728)            0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                1819664   \n",
      "=================================================================\n",
      "Total params: 1,861,904\n",
      "Trainable params: 1,861,520\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 1.6130 - acc: 0.6426\n",
      "Loss: 1.613013028058679 Accuracy: 0.64257526\n",
      "\n",
      "1D_CNN_custom_DO_075_DO_BN_4_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_6 (Conv1D)            (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_6 (Ba (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_7 (Ba (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_8 (Ba (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_9 (Ba (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 37888)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                606224    \n",
      "=================================================================\n",
      "Total params: 669,264\n",
      "Trainable params: 668,752\n",
      "Non-trainable params: 512\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.8305 - acc: 0.7934\n",
      "Loss: 0.8305449603997782 Accuracy: 0.7933541\n",
      "\n",
      "1D_CNN_custom_DO_075_DO_BN_5_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_10 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_10 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_11 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_12 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_12 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_13 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 592, 128)          41088     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_14 (B (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1 (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 25216)             0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                403472    \n",
      "=================================================================\n",
      "Total params: 508,112\n",
      "Trainable params: 507,344\n",
      "Non-trainable params: 768\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.7014 - acc: 0.8320\n",
      "Loss: 0.7013837373021483 Accuracy: 0.8319834\n",
      "\n",
      "1D_CNN_custom_DO_075_DO_BN_6_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_15 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_15 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_16 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_16 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_17 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_17 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_18 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_18 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_19 (Conv1D)           (None, 592, 128)          41088     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_19 (B (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_20 (Conv1D)           (None, 197, 128)          82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_20 (B (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 8320)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 16)                133136    \n",
      "=================================================================\n",
      "Total params: 320,336\n",
      "Trainable params: 319,312\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 7s 1ms/sample - loss: 0.5094 - acc: 0.8812\n",
      "Loss: 0.5093580839294758 Accuracy: 0.88120455\n",
      "\n",
      "1D_CNN_custom_DO_075_DO_BN_7_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_21 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_21 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_22 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_22 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_23 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_23 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_24 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_24 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_25 (Conv1D)           (None, 592, 128)          41088     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_25 (B (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_26 (Conv1D)           (None, 197, 128)          82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_26 (B (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_27 (Conv1D)           (None, 65, 128)           82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_27 (B (None, 65, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_27 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 16)                43024     \n",
      "=================================================================\n",
      "Total params: 312,784\n",
      "Trainable params: 311,504\n",
      "Non-trainable params: 1,280\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.3077 - acc: 0.9321\n",
      "Loss: 0.30774628279202637 Accuracy: 0.93208724\n",
      "\n",
      "1D_CNN_custom_DO_075_DO_BN_8_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_28 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_28 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_28 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_29 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_29 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_29 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_30 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_30 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_30 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_31 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_31 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_31 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_32 (Conv1D)           (None, 592, 128)          41088     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_32 (B (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_32 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_33 (Conv1D)           (None, 197, 128)          82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_33 (B (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_33 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_34 (Conv1D)           (None, 65, 128)           82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_34 (B (None, 65, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_34 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_26 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_35 (Conv1D)           (None, 21, 128)           82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_35 (B (None, 21, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_35 (Activation)   (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_27 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 16)                14352     \n",
      "=================================================================\n",
      "Total params: 366,672\n",
      "Trainable params: 365,136\n",
      "Non-trainable params: 1,536\n",
      "_________________________________________________________________\n",
      "4815/4815 [==============================] - 6s 1ms/sample - loss: 0.2159 - acc: 0.9497\n",
      "Loss: 0.21587861883219503 Accuracy: 0.9497404\n",
      "\n",
      "1D_CNN_custom_DO_075_DO_BN_9_conv Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_36 (Conv1D)           (None, 16000, 64)         384       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_36 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_36 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_37 (Conv1D)           (None, 16000, 64)         20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_37 (B (None, 16000, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_37 (Activation)   (None, 16000, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_28 (MaxPooling (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_38 (Conv1D)           (None, 5333, 64)          20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_38 (B (None, 5333, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_38 (Activation)   (None, 5333, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_29 (MaxPooling (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_39 (Conv1D)           (None, 1777, 64)          20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_39 (B (None, 1777, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_39 (Activation)   (None, 1777, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_30 (MaxPooling (None, 592, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_40 (Conv1D)           (None, 592, 128)          41088     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_40 (B (None, 592, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_40 (Activation)   (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_31 (MaxPooling (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_41 (Conv1D)           (None, 197, 128)          82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_41 (B (None, 197, 128)          512       \n",
      "_________________________________________________________________\n",
      "activation_41 (Activation)   (None, 197, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_32 (MaxPooling (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_42 (Conv1D)           (None, 65, 128)           82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_42 (B (None, 65, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_42 (Activation)   (None, 65, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_43 (Conv1D)           (None, 21, 128)           82048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_43 (B (None, 21, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_43 (Activation)   (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_44 (Conv1D)           (None, 7, 256)            164096    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_44 (B (None, 7, 256)            1024      \n",
      "_________________________________________________________________\n",
      "activation_44 (Activation)   (None, 7, 256)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling (None, 2, 256)            0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 16)                8208      \n",
      "=================================================================\n",
      "Total params: 525,648\n",
      "Trainable params: 523,600\n",
      "Non-trainable params: 2,048\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 7s 1ms/sample - loss: 0.1780 - acc: 0.9585\n",
      "Loss: 0.17802275132005357 Accuracy: 0.95846313\n"
     ]
    }
   ],
   "source": [
    "# log_dir = 'log'\n",
    "# os.makedirs(log_dir, exist_ok=True)\n",
    "# base = '1D_CNN_custom_DO_BN'\n",
    "\n",
    "# with open(path.join(log_dir, base), 'w') as log_file:\n",
    "for i in range(1, 10):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + '000_last.hdf5'\n",
    "\n",
    "    model = load_model(model_filename)\n",
    "    model.summary()\n",
    "\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "    del(model)\n",
    "\n",
    "#         log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
