{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path as path\n",
    "import itertools\n",
    "from sklearn.preprocessing import maxabs_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Input, Dense, Activation, BatchNormalization, \\\n",
    "                                    Flatten, Conv1D, MaxPooling1D, Dropout, \\\n",
    "                                    Concatenate, GlobalMaxPool1D, GlobalAvgPool1D\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler, \\\n",
    "                                        EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = path.join('..', 'data')\n",
    "data_dir = path.join(base_dir, 'data_speech_commands_v0.02')\n",
    " \n",
    "train_txt = path.join(data_dir, 'wav_train_16words.txt')\n",
    "val_txt = path.join(data_dir, 'wav_validation_16words.txt')\n",
    "test_txt = path.join(data_dir, 'wav_test_16words.txt')\n",
    "\n",
    "train_data = np.load(path.join(data_dir, 'wav_train_data.npz'))\n",
    "val_data = np.load(path.join(data_dir, 'wav_validation_data.npz'))\n",
    "test_data = np.load(path.join(data_dir, 'wav_test_data.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36805, 16000, 1),\n",
       " (36805,),\n",
       " (4293, 16000, 1),\n",
       " (4293,),\n",
       " (4815, 16000, 1),\n",
       " (4815,),\n",
       " (16, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = train_data['x_train']\n",
    "y_train = train_data['y_train']\n",
    "x_val = val_data['x_val']\n",
    "y_val = val_data['y_val']\n",
    "x_test = test_data['x_test']\n",
    "y_test = test_data['y_test']\n",
    "y_table = test_data['table']\n",
    "\n",
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape, y_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_test[0].shape\n",
    "output_size = y_table.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_abs = np.asarray([maxabs_scale(wav) for wav in x_train])\n",
    "y_train_onehot = np.asarray([to_categorical(label, output_size) for label in y_train])\n",
    "del x_train, y_train\n",
    "\n",
    "x_val_abs = np.asarray([maxabs_scale(wav) for wav in x_val])\n",
    "y_val_onehot = np.asarray([to_categorical(label, output_size) for label in y_val])\n",
    "del x_val, y_val\n",
    "\n",
    "x_test_abs = np.asarray([maxabs_scale(wav) for wav in x_test])\n",
    "y_test_onehot = np.asarray([to_categorical(label, output_size) for label in y_test])\n",
    "del x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn(conv_num=1):\n",
    "    filter_size = 64\n",
    "\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    x = input_layer\n",
    "\n",
    "    layer_outputs = []\n",
    "    for i in range(conv_num):\n",
    "        x = Conv1D (kernel_size=5, filters=filter_size*(2**(i//4)), \n",
    "                          strides=1, padding='same')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('relu')(x)\n",
    "        x = MaxPooling1D(pool_size=3, strides=3)(x)\n",
    "        layer_outputs.append(x)    \n",
    "    \n",
    "    x = Concatenate()([Flatten()(output) for output in layer_outputs[-2:]])\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(output_size, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 16000, 64)    384         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1 (BatchNo (None, 16000, 64)    256         conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 16000, 64)    0           batch_normalization_v1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D)    (None, 5333, 64)     0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 5333, 64)     20544       max_pooling1d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_1 (Batch (None, 5333, 64)     256         conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 5333, 64)     0           batch_normalization_v1_1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 1777, 64)     0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 1777, 64)     20544       max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_2 (Batch (None, 1777, 64)     256         conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 1777, 64)     0           batch_normalization_v1_2[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 592, 64)      0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 113728)       0           max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 37888)        0           max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 151616)       0           flatten[0][0]                    \n",
      "                                                                 flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 151616)       0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 16)           2425872     dropout[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 2,468,112\n",
      "Trainable params: 2,467,728\n",
      "Non-trainable params: 384\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 16000, 64)    384         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_3 (Batch (None, 16000, 64)    256         conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 16000, 64)    0           batch_normalization_v1_3[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 5333, 64)     0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 5333, 64)     20544       max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_4 (Batch (None, 5333, 64)     256         conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 5333, 64)     0           batch_normalization_v1_4[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 1777, 64)     0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 1777, 64)     20544       max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_5 (Batch (None, 1777, 64)     256         conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 1777, 64)     0           batch_normalization_v1_5[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 592, 64)      0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 592, 64)      20544       max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_6 (Batch (None, 592, 64)      256         conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 592, 64)      0           batch_normalization_v1_6[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1D)  (None, 197, 64)      0           activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 37888)        0           max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 12608)        0           max_pooling1d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 50496)        0           flatten_2[0][0]                  \n",
      "                                                                 flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 50496)        0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 16)           807952      dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 870,992\n",
      "Trainable params: 870,480\n",
      "Non-trainable params: 512\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 16000, 64)    384         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_7 (Batch (None, 16000, 64)    256         conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 16000, 64)    0           batch_normalization_v1_7[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1D)  (None, 5333, 64)     0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 5333, 64)     20544       max_pooling1d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_8 (Batch (None, 5333, 64)     256         conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 5333, 64)     0           batch_normalization_v1_8[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1D)  (None, 1777, 64)     0           activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 1777, 64)     20544       max_pooling1d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_9 (Batch (None, 1777, 64)     256         conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 1777, 64)     0           batch_normalization_v1_9[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1D)  (None, 592, 64)      0           activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_10 (Batc (None, 592, 64)      256         conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 592, 64)      0           batch_normalization_v1_10[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling1D) (None, 197, 64)      0           activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_11 (Batc (None, 197, 128)     512         conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 197, 128)     0           batch_normalization_v1_11[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling1D) (None, 65, 128)      0           activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 12608)        0           max_pooling1d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, 8320)         0           max_pooling1d_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 20928)        0           flatten_4[0][0]                  \n",
      "                                                                 flatten_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 20928)        0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 16)           334864      dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 439,504\n",
      "Trainable params: 438,736\n",
      "Non-trainable params: 768\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 16000, 64)    384         input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_12 (Batc (None, 16000, 64)    256         conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_12[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling1D) (None, 5333, 64)     0           activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_13 (Batc (None, 5333, 64)     256         conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_13[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling1D) (None, 1777, 64)     0           activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_13[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_14 (Batc (None, 1777, 64)     256         conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_14[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling1D) (None, 592, 64)      0           activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_14[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_15 (Batc (None, 592, 64)      256         conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 592, 64)      0           batch_normalization_v1_15[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling1D) (None, 197, 64)      0           activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_15[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_16 (Batc (None, 197, 128)     512         conv1d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 197, 128)     0           batch_normalization_v1_16[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling1D) (None, 65, 128)      0           activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_17 (Conv1D)              (None, 65, 128)      82048       max_pooling1d_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_17 (Batc (None, 65, 128)      512         conv1d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 65, 128)      0           batch_normalization_v1_17[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling1D) (None, 21, 128)      0           activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_6 (Flatten)             (None, 8320)         0           max_pooling1d_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_7 (Flatten)             (None, 2688)         0           max_pooling1d_17[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 11008)        0           flatten_6[0][0]                  \n",
      "                                                                 flatten_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 11008)        0           concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 16)           176144      dropout_3[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 363,344\n",
      "Trainable params: 362,320\n",
      "Non-trainable params: 1,024\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_18 (Conv1D)              (None, 16000, 64)    384         input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_18 (Batc (None, 16000, 64)    256         conv1d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_18[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling1D) (None, 5333, 64)     0           activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_19 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_18[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_19 (Batc (None, 5333, 64)     256         conv1d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_19[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling1D) (None, 1777, 64)     0           activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_20 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_19[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_20 (Batc (None, 1777, 64)     256         conv1d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_20[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling1D) (None, 592, 64)      0           activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_21 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_20[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_21 (Batc (None, 592, 64)      256         conv1d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 592, 64)      0           batch_normalization_v1_21[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling1D) (None, 197, 64)      0           activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_22 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_21[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_22 (Batc (None, 197, 128)     512         conv1d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 197, 128)     0           batch_normalization_v1_22[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling1D) (None, 65, 128)      0           activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_23 (Conv1D)              (None, 65, 128)      82048       max_pooling1d_22[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_23 (Batc (None, 65, 128)      512         conv1d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 65, 128)      0           batch_normalization_v1_23[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling1D) (None, 21, 128)      0           activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_24 (Conv1D)              (None, 21, 128)      82048       max_pooling1d_23[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_24 (Batc (None, 21, 128)      512         conv1d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 21, 128)      0           batch_normalization_v1_24[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling1D) (None, 7, 128)       0           activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_8 (Flatten)             (None, 2688)         0           max_pooling1d_23[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_9 (Flatten)             (None, 896)          0           max_pooling1d_24[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 3584)         0           flatten_8[0][0]                  \n",
      "                                                                 flatten_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 3584)         0           concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 16)           57360       dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 327,120\n",
      "Trainable params: 325,840\n",
      "Non-trainable params: 1,280\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_25 (Conv1D)              (None, 16000, 64)    384         input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_25 (Batc (None, 16000, 64)    256         conv1d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_25[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling1D) (None, 5333, 64)     0           activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_26 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_25[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_26 (Batc (None, 5333, 64)     256         conv1d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_26[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_26 (MaxPooling1D) (None, 1777, 64)     0           activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_27 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_26[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_27 (Batc (None, 1777, 64)     256         conv1d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_27[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_27 (MaxPooling1D) (None, 592, 64)      0           activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_28 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_27[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_28 (Batc (None, 592, 64)      256         conv1d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 592, 64)      0           batch_normalization_v1_28[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_28 (MaxPooling1D) (None, 197, 64)      0           activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_29 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_28[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_29 (Batc (None, 197, 128)     512         conv1d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 197, 128)     0           batch_normalization_v1_29[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_29 (MaxPooling1D) (None, 65, 128)      0           activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_30 (Conv1D)              (None, 65, 128)      82048       max_pooling1d_29[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_30 (Batc (None, 65, 128)      512         conv1d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 65, 128)      0           batch_normalization_v1_30[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_30 (MaxPooling1D) (None, 21, 128)      0           activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_31 (Conv1D)              (None, 21, 128)      82048       max_pooling1d_30[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_31 (Batc (None, 21, 128)      512         conv1d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 21, 128)      0           batch_normalization_v1_31[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_31 (MaxPooling1D) (None, 7, 128)       0           activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_32 (Conv1D)              (None, 7, 128)       82048       max_pooling1d_31[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_32 (Batc (None, 7, 128)       512         conv1d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 7, 128)       0           batch_normalization_v1_32[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_32 (MaxPooling1D) (None, 2, 128)       0           activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_10 (Flatten)            (None, 896)          0           max_pooling1d_31[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_11 (Flatten)            (None, 256)          0           max_pooling1d_32[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 1152)         0           flatten_10[0][0]                 \n",
      "                                                                 flatten_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 1152)         0           concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 16)           18448       dropout_5[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 370,768\n",
      "Trainable params: 369,232\n",
      "Non-trainable params: 1,536\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in range(3, 9):\n",
    "    model = build_cnn(conv_num=i)\n",
    "    model.summary()\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36805 samples, validate on 4293 samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.4412 - acc: 0.3589\n",
      "Epoch 00001: val_loss improved from inf to 1.73466, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_BN_3_conv_checkpoint/001-1.7347.hdf5\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 2.4410 - acc: 0.3589 - val_loss: 1.7347 - val_acc: 0.4379\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5130 - acc: 0.5568\n",
      "Epoch 00002: val_loss improved from 1.73466 to 1.54924, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_BN_3_conv_checkpoint/002-1.5492.hdf5\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 1.5129 - acc: 0.5569 - val_loss: 1.5492 - val_acc: 0.5630\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2082 - acc: 0.6430\n",
      "Epoch 00003: val_loss improved from 1.54924 to 1.30276, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_BN_3_conv_checkpoint/003-1.3028.hdf5\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 1.2081 - acc: 0.6431 - val_loss: 1.3028 - val_acc: 0.6140\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0004 - acc: 0.6979\n",
      "Epoch 00004: val_loss did not improve from 1.30276\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 1.0004 - acc: 0.6979 - val_loss: 1.8741 - val_acc: 0.5227\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8349 - acc: 0.7409\n",
      "Epoch 00005: val_loss did not improve from 1.30276\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.8350 - acc: 0.7409 - val_loss: 1.4453 - val_acc: 0.6208\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7423 - acc: 0.7690\n",
      "Epoch 00006: val_loss did not improve from 1.30276\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.7423 - acc: 0.7690 - val_loss: 1.5368 - val_acc: 0.5986\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6287 - acc: 0.8004\n",
      "Epoch 00007: val_loss did not improve from 1.30276\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.6287 - acc: 0.8004 - val_loss: 1.7050 - val_acc: 0.5917\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5502 - acc: 0.8254\n",
      "Epoch 00008: val_loss did not improve from 1.30276\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.5501 - acc: 0.8254 - val_loss: 1.6578 - val_acc: 0.6150\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4674 - acc: 0.8481\n",
      "Epoch 00009: val_loss did not improve from 1.30276\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.4673 - acc: 0.8481 - val_loss: 1.8594 - val_acc: 0.5611\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4389 - acc: 0.8570\n",
      "Epoch 00010: val_loss did not improve from 1.30276\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.4388 - acc: 0.8570 - val_loss: 1.8206 - val_acc: 0.6075\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3984 - acc: 0.8727\n",
      "Epoch 00011: val_loss did not improve from 1.30276\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.3983 - acc: 0.8728 - val_loss: 1.6134 - val_acc: 0.6338\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3348 - acc: 0.8903\n",
      "Epoch 00012: val_loss did not improve from 1.30276\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.3348 - acc: 0.8903 - val_loss: 1.9512 - val_acc: 0.5954\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3254 - acc: 0.8929\n",
      "Epoch 00013: val_loss did not improve from 1.30276\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.3254 - acc: 0.8929 - val_loss: 1.7731 - val_acc: 0.6413\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2964 - acc: 0.9034\n",
      "Epoch 00014: val_loss did not improve from 1.30276\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.2963 - acc: 0.9034 - val_loss: 1.9883 - val_acc: 0.6105\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2827 - acc: 0.9097\n",
      "Epoch 00015: val_loss did not improve from 1.30276\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.2827 - acc: 0.9097 - val_loss: 1.8811 - val_acc: 0.6129\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2590 - acc: 0.9156\n",
      "Epoch 00016: val_loss did not improve from 1.30276\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2590 - acc: 0.9156 - val_loss: 1.8639 - val_acc: 0.6191\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2318 - acc: 0.9273\n",
      "Epoch 00017: val_loss did not improve from 1.30276\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.2318 - acc: 0.9273 - val_loss: 2.0245 - val_acc: 0.6094\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2288 - acc: 0.9261\n",
      "Epoch 00018: val_loss did not improve from 1.30276\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.2287 - acc: 0.9261 - val_loss: 1.7271 - val_acc: 0.6478\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1983 - acc: 0.9373\n",
      "Epoch 00019: val_loss did not improve from 1.30276\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1983 - acc: 0.9373 - val_loss: 2.0999 - val_acc: 0.6308\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1939 - acc: 0.9388\n",
      "Epoch 00020: val_loss did not improve from 1.30276\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1940 - acc: 0.9388 - val_loss: 2.0956 - val_acc: 0.6219\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2095 - acc: 0.9346\n",
      "Epoch 00021: val_loss did not improve from 1.30276\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.2095 - acc: 0.9345 - val_loss: 2.2313 - val_acc: 0.6112\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1887 - acc: 0.9407\n",
      "Epoch 00022: val_loss did not improve from 1.30276\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1886 - acc: 0.9407 - val_loss: 1.8202 - val_acc: 0.6515\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1670 - acc: 0.9480\n",
      "Epoch 00023: val_loss did not improve from 1.30276\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1671 - acc: 0.9480 - val_loss: 2.4794 - val_acc: 0.5812\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1581 - acc: 0.9502\n",
      "Epoch 00024: val_loss did not improve from 1.30276\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1581 - acc: 0.9502 - val_loss: 2.6053 - val_acc: 0.5891\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1796 - acc: 0.9459\n",
      "Epoch 00025: val_loss did not improve from 1.30276\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1798 - acc: 0.9459 - val_loss: 2.5118 - val_acc: 0.6268\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1546 - acc: 0.9511\n",
      "Epoch 00026: val_loss did not improve from 1.30276\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1546 - acc: 0.9511 - val_loss: 1.8675 - val_acc: 0.6562\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1546 - acc: 0.9523\n",
      "Epoch 00027: val_loss did not improve from 1.30276\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1546 - acc: 0.9523 - val_loss: 2.1275 - val_acc: 0.6359\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1396 - acc: 0.9571\n",
      "Epoch 00028: val_loss did not improve from 1.30276\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1398 - acc: 0.9571 - val_loss: 2.2655 - val_acc: 0.6210\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1386 - acc: 0.9589\n",
      "Epoch 00029: val_loss did not improve from 1.30276\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1386 - acc: 0.9589 - val_loss: 2.0559 - val_acc: 0.6597\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1425 - acc: 0.9548\n",
      "Epoch 00030: val_loss did not improve from 1.30276\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1425 - acc: 0.9548 - val_loss: 1.9260 - val_acc: 0.6734\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1253 - acc: 0.9625\n",
      "Epoch 00031: val_loss did not improve from 1.30276\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1254 - acc: 0.9625 - val_loss: 2.1904 - val_acc: 0.6406\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1275 - acc: 0.9608\n",
      "Epoch 00032: val_loss did not improve from 1.30276\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1275 - acc: 0.9608 - val_loss: 1.9764 - val_acc: 0.6702\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1110 - acc: 0.9661\n",
      "Epoch 00033: val_loss did not improve from 1.30276\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1110 - acc: 0.9661 - val_loss: 2.0766 - val_acc: 0.6508\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1178 - acc: 0.9643\n",
      "Epoch 00034: val_loss did not improve from 1.30276\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1177 - acc: 0.9643 - val_loss: 1.9306 - val_acc: 0.6774\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1222 - acc: 0.9640\n",
      "Epoch 00035: val_loss did not improve from 1.30276\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1223 - acc: 0.9640 - val_loss: 2.5211 - val_acc: 0.6182\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1259 - acc: 0.9620\n",
      "Epoch 00036: val_loss did not improve from 1.30276\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1259 - acc: 0.9620 - val_loss: 2.5995 - val_acc: 0.6045\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1092 - acc: 0.9681\n",
      "Epoch 00037: val_loss did not improve from 1.30276\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1095 - acc: 0.9680 - val_loss: 2.2523 - val_acc: 0.6492\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1457 - acc: 0.9598\n",
      "Epoch 00038: val_loss did not improve from 1.30276\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1460 - acc: 0.9598 - val_loss: 2.2195 - val_acc: 0.6378\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1073 - acc: 0.9682\n",
      "Epoch 00039: val_loss did not improve from 1.30276\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1073 - acc: 0.9682 - val_loss: 2.0688 - val_acc: 0.6555\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0923 - acc: 0.9723\n",
      "Epoch 00040: val_loss did not improve from 1.30276\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0926 - acc: 0.9722 - val_loss: 2.0992 - val_acc: 0.6618\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1037 - acc: 0.9703\n",
      "Epoch 00041: val_loss did not improve from 1.30276\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.1037 - acc: 0.9703 - val_loss: 2.2289 - val_acc: 0.6469\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0843 - acc: 0.9753\n",
      "Epoch 00042: val_loss did not improve from 1.30276\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0843 - acc: 0.9753 - val_loss: 2.4039 - val_acc: 0.6429\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0855 - acc: 0.9747\n",
      "Epoch 00043: val_loss did not improve from 1.30276\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0855 - acc: 0.9747 - val_loss: 2.3681 - val_acc: 0.6341\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0982 - acc: 0.9705\n",
      "Epoch 00044: val_loss did not improve from 1.30276\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0982 - acc: 0.9705 - val_loss: 2.6829 - val_acc: 0.6133\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0939 - acc: 0.9727\n",
      "Epoch 00045: val_loss did not improve from 1.30276\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0939 - acc: 0.9727 - val_loss: 2.4404 - val_acc: 0.6343\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0896 - acc: 0.9740\n",
      "Epoch 00046: val_loss did not improve from 1.30276\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0896 - acc: 0.9741 - val_loss: 2.0197 - val_acc: 0.6813\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0868 - acc: 0.9742\n",
      "Epoch 00047: val_loss did not improve from 1.30276\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0868 - acc: 0.9742 - val_loss: 2.6954 - val_acc: 0.6012\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0817 - acc: 0.9768\n",
      "Epoch 00048: val_loss did not improve from 1.30276\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0818 - acc: 0.9768 - val_loss: 2.2671 - val_acc: 0.6520\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0870 - acc: 0.9757\n",
      "Epoch 00049: val_loss did not improve from 1.30276\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0870 - acc: 0.9756 - val_loss: 1.9974 - val_acc: 0.6909\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0790 - acc: 0.9768\n",
      "Epoch 00050: val_loss did not improve from 1.30276\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0791 - acc: 0.9768 - val_loss: 2.1854 - val_acc: 0.6720\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0838 - acc: 0.9755\n",
      "Epoch 00051: val_loss did not improve from 1.30276\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0839 - acc: 0.9755 - val_loss: 2.1223 - val_acc: 0.6806\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0813 - acc: 0.9769\n",
      "Epoch 00052: val_loss did not improve from 1.30276\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0814 - acc: 0.9769 - val_loss: 2.3716 - val_acc: 0.6369\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0848 - acc: 0.9755\n",
      "Epoch 00053: val_loss did not improve from 1.30276\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0848 - acc: 0.9755 - val_loss: 2.4680 - val_acc: 0.6431\n",
      "\n",
      "1D_CNN_custom_multi_2_concat_DO_BN_3_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 603us/sample - loss: 1.4145 - acc: 0.5699\n",
      "Loss: 1.414538485634983 Accuracy: 0.5698858\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.2267 - acc: 0.3609\n",
      "Epoch 00001: val_loss improved from inf to 1.56417, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_BN_4_conv_checkpoint/001-1.5642.hdf5\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 2.2266 - acc: 0.3609 - val_loss: 1.5642 - val_acc: 0.5013\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4004 - acc: 0.5736\n",
      "Epoch 00002: val_loss improved from 1.56417 to 1.09965, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_BN_4_conv_checkpoint/002-1.0997.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 1.4004 - acc: 0.5736 - val_loss: 1.0997 - val_acc: 0.6681\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1485 - acc: 0.6487\n",
      "Epoch 00003: val_loss did not improve from 1.09965\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 1.1485 - acc: 0.6487 - val_loss: 1.1769 - val_acc: 0.6431\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0089 - acc: 0.6892\n",
      "Epoch 00004: val_loss improved from 1.09965 to 1.07009, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_BN_4_conv_checkpoint/004-1.0701.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 1.0090 - acc: 0.6892 - val_loss: 1.0701 - val_acc: 0.6706\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8956 - acc: 0.7237\n",
      "Epoch 00005: val_loss improved from 1.07009 to 1.00501, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_BN_4_conv_checkpoint/005-1.0050.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.8956 - acc: 0.7237 - val_loss: 1.0050 - val_acc: 0.6997\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7996 - acc: 0.7514\n",
      "Epoch 00006: val_loss improved from 1.00501 to 0.92121, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_BN_4_conv_checkpoint/006-0.9212.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.7998 - acc: 0.7514 - val_loss: 0.9212 - val_acc: 0.7354\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7273 - acc: 0.7749\n",
      "Epoch 00007: val_loss did not improve from 0.92121\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.7275 - acc: 0.7749 - val_loss: 1.0122 - val_acc: 0.7112\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6671 - acc: 0.7911\n",
      "Epoch 00008: val_loss did not improve from 0.92121\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.6672 - acc: 0.7911 - val_loss: 1.0736 - val_acc: 0.6846\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6098 - acc: 0.8075\n",
      "Epoch 00009: val_loss did not improve from 0.92121\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.6099 - acc: 0.8074 - val_loss: 0.9685 - val_acc: 0.7207\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5540 - acc: 0.8267\n",
      "Epoch 00010: val_loss did not improve from 0.92121\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.5540 - acc: 0.8267 - val_loss: 0.9662 - val_acc: 0.7338\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5093 - acc: 0.8393\n",
      "Epoch 00011: val_loss did not improve from 0.92121\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.5093 - acc: 0.8393 - val_loss: 0.9543 - val_acc: 0.7412\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4721 - acc: 0.8492\n",
      "Epoch 00012: val_loss improved from 0.92121 to 0.89147, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_BN_4_conv_checkpoint/012-0.8915.hdf5\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.4723 - acc: 0.8491 - val_loss: 0.8915 - val_acc: 0.7626\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4276 - acc: 0.8630\n",
      "Epoch 00013: val_loss did not improve from 0.89147\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.4276 - acc: 0.8630 - val_loss: 1.0243 - val_acc: 0.7202\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3967 - acc: 0.8723\n",
      "Epoch 00014: val_loss did not improve from 0.89147\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3966 - acc: 0.8723 - val_loss: 0.9630 - val_acc: 0.7442\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3703 - acc: 0.8801\n",
      "Epoch 00015: val_loss did not improve from 0.89147\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3703 - acc: 0.8801 - val_loss: 0.9735 - val_acc: 0.7494\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3351 - acc: 0.8932\n",
      "Epoch 00016: val_loss did not improve from 0.89147\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3351 - acc: 0.8931 - val_loss: 1.0102 - val_acc: 0.7324\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3313 - acc: 0.8928\n",
      "Epoch 00017: val_loss did not improve from 0.89147\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3315 - acc: 0.8928 - val_loss: 0.9102 - val_acc: 0.7570\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3156 - acc: 0.8972\n",
      "Epoch 00018: val_loss did not improve from 0.89147\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.3156 - acc: 0.8972 - val_loss: 1.0978 - val_acc: 0.7268\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2939 - acc: 0.9054\n",
      "Epoch 00019: val_loss did not improve from 0.89147\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2940 - acc: 0.9054 - val_loss: 1.0008 - val_acc: 0.7538\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2711 - acc: 0.9099\n",
      "Epoch 00020: val_loss did not improve from 0.89147\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2711 - acc: 0.9099 - val_loss: 0.9419 - val_acc: 0.7673\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2617 - acc: 0.9147\n",
      "Epoch 00021: val_loss did not improve from 0.89147\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2617 - acc: 0.9147 - val_loss: 0.9061 - val_acc: 0.7666\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2447 - acc: 0.9204\n",
      "Epoch 00022: val_loss did not improve from 0.89147\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2448 - acc: 0.9203 - val_loss: 1.0434 - val_acc: 0.7400\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2361 - acc: 0.9235\n",
      "Epoch 00023: val_loss did not improve from 0.89147\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2361 - acc: 0.9234 - val_loss: 1.0955 - val_acc: 0.7333\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2168 - acc: 0.9294\n",
      "Epoch 00024: val_loss did not improve from 0.89147\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2170 - acc: 0.9293 - val_loss: 1.1456 - val_acc: 0.7338\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2134 - acc: 0.9294\n",
      "Epoch 00025: val_loss did not improve from 0.89147\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2134 - acc: 0.9294 - val_loss: 0.8930 - val_acc: 0.7747\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2038 - acc: 0.9333\n",
      "Epoch 00026: val_loss did not improve from 0.89147\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.2038 - acc: 0.9333 - val_loss: 0.9231 - val_acc: 0.7785\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1894 - acc: 0.9368\n",
      "Epoch 00027: val_loss did not improve from 0.89147\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1894 - acc: 0.9368 - val_loss: 1.0450 - val_acc: 0.7545\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1856 - acc: 0.9394\n",
      "Epoch 00028: val_loss did not improve from 0.89147\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1856 - acc: 0.9394 - val_loss: 1.0321 - val_acc: 0.7580\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1727 - acc: 0.9446\n",
      "Epoch 00029: val_loss did not improve from 0.89147\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1727 - acc: 0.9446 - val_loss: 1.0683 - val_acc: 0.7533\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1779 - acc: 0.9417\n",
      "Epoch 00030: val_loss did not improve from 0.89147\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1779 - acc: 0.9417 - val_loss: 1.0342 - val_acc: 0.7652\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1598 - acc: 0.9484\n",
      "Epoch 00031: val_loss did not improve from 0.89147\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1598 - acc: 0.9483 - val_loss: 0.9570 - val_acc: 0.7785\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1620 - acc: 0.9476\n",
      "Epoch 00032: val_loss did not improve from 0.89147\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1620 - acc: 0.9476 - val_loss: 1.0347 - val_acc: 0.7645\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1591 - acc: 0.9488\n",
      "Epoch 00033: val_loss did not improve from 0.89147\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1592 - acc: 0.9487 - val_loss: 1.0361 - val_acc: 0.7601\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1555 - acc: 0.9493\n",
      "Epoch 00034: val_loss did not improve from 0.89147\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1555 - acc: 0.9493 - val_loss: 0.9475 - val_acc: 0.7880\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1436 - acc: 0.9546\n",
      "Epoch 00035: val_loss did not improve from 0.89147\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1438 - acc: 0.9545 - val_loss: 1.0438 - val_acc: 0.7715\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1438 - acc: 0.9558\n",
      "Epoch 00036: val_loss did not improve from 0.89147\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1438 - acc: 0.9558 - val_loss: 0.9187 - val_acc: 0.8006\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1302 - acc: 0.9579\n",
      "Epoch 00037: val_loss did not improve from 0.89147\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1304 - acc: 0.9578 - val_loss: 1.1655 - val_acc: 0.7435\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1339 - acc: 0.9573\n",
      "Epoch 00038: val_loss did not improve from 0.89147\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1342 - acc: 0.9573 - val_loss: 1.2454 - val_acc: 0.7340\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1346 - acc: 0.9561\n",
      "Epoch 00039: val_loss did not improve from 0.89147\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1347 - acc: 0.9561 - val_loss: 1.3965 - val_acc: 0.7049\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1230 - acc: 0.9613\n",
      "Epoch 00040: val_loss did not improve from 0.89147\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1230 - acc: 0.9613 - val_loss: 0.9824 - val_acc: 0.7906\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1224 - acc: 0.9609\n",
      "Epoch 00041: val_loss did not improve from 0.89147\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1225 - acc: 0.9609 - val_loss: 1.2422 - val_acc: 0.7454\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1188 - acc: 0.9620\n",
      "Epoch 00042: val_loss did not improve from 0.89147\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1188 - acc: 0.9620 - val_loss: 1.0186 - val_acc: 0.7782\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1151 - acc: 0.9633\n",
      "Epoch 00043: val_loss did not improve from 0.89147\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1151 - acc: 0.9633 - val_loss: 0.9875 - val_acc: 0.7941\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1153 - acc: 0.9635\n",
      "Epoch 00044: val_loss did not improve from 0.89147\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1153 - acc: 0.9635 - val_loss: 1.1712 - val_acc: 0.7529\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1066 - acc: 0.9664\n",
      "Epoch 00045: val_loss did not improve from 0.89147\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1067 - acc: 0.9664 - val_loss: 1.0025 - val_acc: 0.7908\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1160 - acc: 0.9637\n",
      "Epoch 00046: val_loss did not improve from 0.89147\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1160 - acc: 0.9637 - val_loss: 1.0594 - val_acc: 0.7822\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1029 - acc: 0.9679\n",
      "Epoch 00047: val_loss did not improve from 0.89147\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1029 - acc: 0.9679 - val_loss: 1.2263 - val_acc: 0.7452\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1043 - acc: 0.9675\n",
      "Epoch 00048: val_loss did not improve from 0.89147\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1043 - acc: 0.9675 - val_loss: 1.0218 - val_acc: 0.7932\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0983 - acc: 0.9685\n",
      "Epoch 00049: val_loss did not improve from 0.89147\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0984 - acc: 0.9685 - val_loss: 1.1709 - val_acc: 0.7640\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1045 - acc: 0.9678\n",
      "Epoch 00050: val_loss did not improve from 0.89147\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1044 - acc: 0.9678 - val_loss: 1.0741 - val_acc: 0.7866\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1006 - acc: 0.9682\n",
      "Epoch 00051: val_loss did not improve from 0.89147\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.1005 - acc: 0.9682 - val_loss: 1.1336 - val_acc: 0.7652\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0884 - acc: 0.9727\n",
      "Epoch 00052: val_loss did not improve from 0.89147\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0885 - acc: 0.9727 - val_loss: 1.1288 - val_acc: 0.7817\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0947 - acc: 0.9710\n",
      "Epoch 00053: val_loss did not improve from 0.89147\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0947 - acc: 0.9710 - val_loss: 1.0239 - val_acc: 0.7948\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0861 - acc: 0.9733\n",
      "Epoch 00054: val_loss did not improve from 0.89147\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0861 - acc: 0.9733 - val_loss: 1.0130 - val_acc: 0.7983\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0831 - acc: 0.9750\n",
      "Epoch 00055: val_loss did not improve from 0.89147\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0831 - acc: 0.9750 - val_loss: 1.0120 - val_acc: 0.8029\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0920 - acc: 0.9714\n",
      "Epoch 00056: val_loss did not improve from 0.89147\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0920 - acc: 0.9714 - val_loss: 1.1272 - val_acc: 0.7736\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0830 - acc: 0.9741\n",
      "Epoch 00057: val_loss did not improve from 0.89147\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0831 - acc: 0.9741 - val_loss: 1.2008 - val_acc: 0.7741\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0845 - acc: 0.9749\n",
      "Epoch 00058: val_loss did not improve from 0.89147\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0844 - acc: 0.9749 - val_loss: 1.0844 - val_acc: 0.7850\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0825 - acc: 0.9740\n",
      "Epoch 00059: val_loss did not improve from 0.89147\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0826 - acc: 0.9740 - val_loss: 1.0668 - val_acc: 0.7824\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0843 - acc: 0.9739\n",
      "Epoch 00060: val_loss did not improve from 0.89147\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0843 - acc: 0.9739 - val_loss: 1.0945 - val_acc: 0.7838\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0793 - acc: 0.9764\n",
      "Epoch 00061: val_loss did not improve from 0.89147\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0793 - acc: 0.9764 - val_loss: 1.0722 - val_acc: 0.7959\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0811 - acc: 0.9746\n",
      "Epoch 00062: val_loss did not improve from 0.89147\n",
      "36805/36805 [==============================] - 53s 1ms/sample - loss: 0.0812 - acc: 0.9746 - val_loss: 1.0510 - val_acc: 0.7929\n",
      "\n",
      "1D_CNN_custom_multi_2_concat_DO_BN_4_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 618us/sample - loss: 0.9842 - acc: 0.7161\n",
      "Loss: 0.9842279992742331 Accuracy: 0.7160955\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.1379 - acc: 0.3668\n",
      "Epoch 00001: val_loss improved from inf to 1.46362, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_BN_5_conv_checkpoint/001-1.4636.hdf5\n",
      "36805/36805 [==============================] - 61s 2ms/sample - loss: 2.1379 - acc: 0.3668 - val_loss: 1.4636 - val_acc: 0.5234\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3280 - acc: 0.5888\n",
      "Epoch 00002: val_loss improved from 1.46362 to 1.01669, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_BN_5_conv_checkpoint/002-1.0167.hdf5\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 1.3282 - acc: 0.5887 - val_loss: 1.0167 - val_acc: 0.7016\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0657 - acc: 0.6721\n",
      "Epoch 00003: val_loss improved from 1.01669 to 0.94440, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_BN_5_conv_checkpoint/003-0.9444.hdf5\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 1.0657 - acc: 0.6722 - val_loss: 0.9444 - val_acc: 0.7212\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9274 - acc: 0.7183\n",
      "Epoch 00004: val_loss improved from 0.94440 to 0.89148, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_BN_5_conv_checkpoint/004-0.8915.hdf5\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.9273 - acc: 0.7184 - val_loss: 0.8915 - val_acc: 0.7417\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8260 - acc: 0.7473\n",
      "Epoch 00005: val_loss improved from 0.89148 to 0.77713, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_BN_5_conv_checkpoint/005-0.7771.hdf5\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.8261 - acc: 0.7472 - val_loss: 0.7771 - val_acc: 0.7750\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7558 - acc: 0.7723\n",
      "Epoch 00006: val_loss did not improve from 0.77713\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.7558 - acc: 0.7723 - val_loss: 0.9114 - val_acc: 0.7296\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6795 - acc: 0.7949\n",
      "Epoch 00007: val_loss did not improve from 0.77713\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.6795 - acc: 0.7949 - val_loss: 0.8029 - val_acc: 0.7629\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6355 - acc: 0.8089\n",
      "Epoch 00008: val_loss improved from 0.77713 to 0.64851, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_BN_5_conv_checkpoint/008-0.6485.hdf5\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.6356 - acc: 0.8089 - val_loss: 0.6485 - val_acc: 0.8162\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5936 - acc: 0.8193\n",
      "Epoch 00009: val_loss did not improve from 0.64851\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.5936 - acc: 0.8193 - val_loss: 0.6574 - val_acc: 0.8181\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5513 - acc: 0.8341\n",
      "Epoch 00010: val_loss improved from 0.64851 to 0.64050, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_BN_5_conv_checkpoint/010-0.6405.hdf5\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.5515 - acc: 0.8341 - val_loss: 0.6405 - val_acc: 0.8204\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5189 - acc: 0.8425\n",
      "Epoch 00011: val_loss did not improve from 0.64050\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.5189 - acc: 0.8425 - val_loss: 0.6726 - val_acc: 0.8099\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4952 - acc: 0.8497\n",
      "Epoch 00012: val_loss did not improve from 0.64050\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.4952 - acc: 0.8497 - val_loss: 0.6504 - val_acc: 0.8185\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4544 - acc: 0.8617\n",
      "Epoch 00013: val_loss did not improve from 0.64050\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.4545 - acc: 0.8616 - val_loss: 0.6581 - val_acc: 0.8169\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4401 - acc: 0.8642\n",
      "Epoch 00014: val_loss did not improve from 0.64050\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.4401 - acc: 0.8642 - val_loss: 0.6647 - val_acc: 0.8141\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4099 - acc: 0.8748\n",
      "Epoch 00015: val_loss did not improve from 0.64050\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.4100 - acc: 0.8747 - val_loss: 0.6833 - val_acc: 0.8157\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3947 - acc: 0.8804\n",
      "Epoch 00016: val_loss did not improve from 0.64050\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.3951 - acc: 0.8804 - val_loss: 0.6562 - val_acc: 0.8265\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3746 - acc: 0.8872\n",
      "Epoch 00017: val_loss improved from 0.64050 to 0.49575, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_BN_5_conv_checkpoint/017-0.4957.hdf5\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.3746 - acc: 0.8872 - val_loss: 0.4957 - val_acc: 0.8558\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3598 - acc: 0.8888\n",
      "Epoch 00018: val_loss did not improve from 0.49575\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.3598 - acc: 0.8888 - val_loss: 0.5954 - val_acc: 0.8328\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3383 - acc: 0.8942\n",
      "Epoch 00019: val_loss did not improve from 0.49575\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.3384 - acc: 0.8941 - val_loss: 0.5434 - val_acc: 0.8509\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3350 - acc: 0.8957\n",
      "Epoch 00020: val_loss did not improve from 0.49575\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.3350 - acc: 0.8956 - val_loss: 0.5257 - val_acc: 0.8570\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3179 - acc: 0.9000\n",
      "Epoch 00021: val_loss did not improve from 0.49575\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.3181 - acc: 0.9000 - val_loss: 0.4958 - val_acc: 0.8614\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3020 - acc: 0.9070\n",
      "Epoch 00022: val_loss did not improve from 0.49575\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.3020 - acc: 0.9069 - val_loss: 0.5745 - val_acc: 0.8411\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2942 - acc: 0.9077\n",
      "Epoch 00023: val_loss did not improve from 0.49575\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.2942 - acc: 0.9077 - val_loss: 0.5324 - val_acc: 0.8532\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2738 - acc: 0.9154\n",
      "Epoch 00024: val_loss did not improve from 0.49575\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.2738 - acc: 0.9154 - val_loss: 0.5463 - val_acc: 0.8595\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2735 - acc: 0.9143\n",
      "Epoch 00025: val_loss improved from 0.49575 to 0.49183, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_BN_5_conv_checkpoint/025-0.4918.hdf5\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.2735 - acc: 0.9143 - val_loss: 0.4918 - val_acc: 0.8700\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2505 - acc: 0.9208\n",
      "Epoch 00026: val_loss did not improve from 0.49183\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.2508 - acc: 0.9208 - val_loss: 0.5292 - val_acc: 0.8612\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2505 - acc: 0.9207\n",
      "Epoch 00027: val_loss did not improve from 0.49183\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.2505 - acc: 0.9207 - val_loss: 0.5449 - val_acc: 0.8588\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2359 - acc: 0.9252\n",
      "Epoch 00028: val_loss did not improve from 0.49183\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.2359 - acc: 0.9251 - val_loss: 0.6989 - val_acc: 0.8218\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2327 - acc: 0.9283\n",
      "Epoch 00029: val_loss improved from 0.49183 to 0.47455, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_BN_5_conv_checkpoint/029-0.4746.hdf5\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.2327 - acc: 0.9283 - val_loss: 0.4746 - val_acc: 0.8742\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2237 - acc: 0.9295\n",
      "Epoch 00030: val_loss did not improve from 0.47455\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.2237 - acc: 0.9295 - val_loss: 0.4849 - val_acc: 0.8768\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2158 - acc: 0.9309\n",
      "Epoch 00031: val_loss did not improve from 0.47455\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.2158 - acc: 0.9309 - val_loss: 0.5025 - val_acc: 0.8663\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2069 - acc: 0.9342\n",
      "Epoch 00032: val_loss did not improve from 0.47455\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.2070 - acc: 0.9341 - val_loss: 0.4797 - val_acc: 0.8696\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2008 - acc: 0.9349\n",
      "Epoch 00033: val_loss did not improve from 0.47455\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.2008 - acc: 0.9349 - val_loss: 0.6856 - val_acc: 0.8283\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1989 - acc: 0.9345\n",
      "Epoch 00034: val_loss did not improve from 0.47455\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1989 - acc: 0.9344 - val_loss: 0.5008 - val_acc: 0.8763\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1950 - acc: 0.9384\n",
      "Epoch 00035: val_loss did not improve from 0.47455\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1952 - acc: 0.9384 - val_loss: 0.5189 - val_acc: 0.8598\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1868 - acc: 0.9401\n",
      "Epoch 00036: val_loss did not improve from 0.47455\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1868 - acc: 0.9401 - val_loss: 0.4746 - val_acc: 0.8742\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1768 - acc: 0.9440\n",
      "Epoch 00037: val_loss improved from 0.47455 to 0.47309, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_BN_5_conv_checkpoint/037-0.4731.hdf5\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1768 - acc: 0.9440 - val_loss: 0.4731 - val_acc: 0.8710\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1728 - acc: 0.9444\n",
      "Epoch 00038: val_loss did not improve from 0.47309\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1728 - acc: 0.9444 - val_loss: 0.6061 - val_acc: 0.8458\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1632 - acc: 0.9472\n",
      "Epoch 00039: val_loss did not improve from 0.47309\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1633 - acc: 0.9472 - val_loss: 0.4887 - val_acc: 0.8726\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1623 - acc: 0.9465\n",
      "Epoch 00040: val_loss did not improve from 0.47309\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1623 - acc: 0.9466 - val_loss: 0.5466 - val_acc: 0.8563\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1571 - acc: 0.9499\n",
      "Epoch 00041: val_loss did not improve from 0.47309\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1573 - acc: 0.9498 - val_loss: 0.5172 - val_acc: 0.8712\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1648 - acc: 0.9471\n",
      "Epoch 00042: val_loss did not improve from 0.47309\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1648 - acc: 0.9470 - val_loss: 0.4894 - val_acc: 0.8824\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1530 - acc: 0.9518\n",
      "Epoch 00043: val_loss improved from 0.47309 to 0.44772, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_BN_5_conv_checkpoint/043-0.4477.hdf5\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1530 - acc: 0.9518 - val_loss: 0.4477 - val_acc: 0.8887\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1450 - acc: 0.9528\n",
      "Epoch 00044: val_loss did not improve from 0.44772\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1450 - acc: 0.9528 - val_loss: 0.5035 - val_acc: 0.8854\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1393 - acc: 0.9534\n",
      "Epoch 00045: val_loss did not improve from 0.44772\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1394 - acc: 0.9534 - val_loss: 0.5149 - val_acc: 0.8777\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1390 - acc: 0.9558\n",
      "Epoch 00046: val_loss did not improve from 0.44772\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1390 - acc: 0.9558 - val_loss: 0.5598 - val_acc: 0.8616\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1335 - acc: 0.9562\n",
      "Epoch 00047: val_loss did not improve from 0.44772\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1336 - acc: 0.9561 - val_loss: 0.4893 - val_acc: 0.8826\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1377 - acc: 0.9545\n",
      "Epoch 00048: val_loss did not improve from 0.44772\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1377 - acc: 0.9545 - val_loss: 0.5102 - val_acc: 0.8705\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1298 - acc: 0.9579\n",
      "Epoch 00049: val_loss did not improve from 0.44772\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1299 - acc: 0.9579 - val_loss: 1.0109 - val_acc: 0.7836\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1286 - acc: 0.9587\n",
      "Epoch 00050: val_loss did not improve from 0.44772\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1287 - acc: 0.9586 - val_loss: 0.4690 - val_acc: 0.8840\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1334 - acc: 0.9558\n",
      "Epoch 00051: val_loss did not improve from 0.44772\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1335 - acc: 0.9557 - val_loss: 0.5116 - val_acc: 0.8772\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1173 - acc: 0.9629\n",
      "Epoch 00052: val_loss did not improve from 0.44772\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1173 - acc: 0.9629 - val_loss: 0.6802 - val_acc: 0.8351\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1185 - acc: 0.9613\n",
      "Epoch 00053: val_loss did not improve from 0.44772\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1185 - acc: 0.9613 - val_loss: 0.5164 - val_acc: 0.8740\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1115 - acc: 0.9639\n",
      "Epoch 00054: val_loss did not improve from 0.44772\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1115 - acc: 0.9639 - val_loss: 0.4918 - val_acc: 0.8786\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1156 - acc: 0.9625\n",
      "Epoch 00055: val_loss did not improve from 0.44772\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1155 - acc: 0.9625 - val_loss: 0.5369 - val_acc: 0.8779\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1075 - acc: 0.9655\n",
      "Epoch 00056: val_loss did not improve from 0.44772\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1075 - acc: 0.9655 - val_loss: 0.4911 - val_acc: 0.8845\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1123 - acc: 0.9634\n",
      "Epoch 00057: val_loss did not improve from 0.44772\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1124 - acc: 0.9634 - val_loss: 0.6285 - val_acc: 0.8546\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1034 - acc: 0.9658\n",
      "Epoch 00058: val_loss did not improve from 0.44772\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1037 - acc: 0.9657 - val_loss: 0.6211 - val_acc: 0.8481\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1094 - acc: 0.9635\n",
      "Epoch 00059: val_loss did not improve from 0.44772\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1094 - acc: 0.9635 - val_loss: 0.5675 - val_acc: 0.8682\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0983 - acc: 0.9685\n",
      "Epoch 00060: val_loss did not improve from 0.44772\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0984 - acc: 0.9685 - val_loss: 0.4634 - val_acc: 0.8947\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1159 - acc: 0.9625\n",
      "Epoch 00061: val_loss did not improve from 0.44772\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.1159 - acc: 0.9625 - val_loss: 0.4819 - val_acc: 0.8845\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0970 - acc: 0.9693\n",
      "Epoch 00062: val_loss did not improve from 0.44772\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0970 - acc: 0.9693 - val_loss: 0.5198 - val_acc: 0.8800\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0952 - acc: 0.9700\n",
      "Epoch 00063: val_loss did not improve from 0.44772\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0952 - acc: 0.9700 - val_loss: 0.5078 - val_acc: 0.8796\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0925 - acc: 0.9698\n",
      "Epoch 00064: val_loss did not improve from 0.44772\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0925 - acc: 0.9698 - val_loss: 0.6170 - val_acc: 0.8537\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0916 - acc: 0.9705\n",
      "Epoch 00065: val_loss did not improve from 0.44772\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0916 - acc: 0.9705 - val_loss: 0.5270 - val_acc: 0.8800\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0921 - acc: 0.9712\n",
      "Epoch 00066: val_loss did not improve from 0.44772\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0921 - acc: 0.9712 - val_loss: 0.5522 - val_acc: 0.8679\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0869 - acc: 0.9727\n",
      "Epoch 00067: val_loss did not improve from 0.44772\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0870 - acc: 0.9727 - val_loss: 0.5255 - val_acc: 0.8735\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0899 - acc: 0.9706\n",
      "Epoch 00068: val_loss did not improve from 0.44772\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0899 - acc: 0.9706 - val_loss: 0.7827 - val_acc: 0.8183\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0879 - acc: 0.9718\n",
      "Epoch 00069: val_loss did not improve from 0.44772\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0879 - acc: 0.9718 - val_loss: 0.4750 - val_acc: 0.8949\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0833 - acc: 0.9724\n",
      "Epoch 00070: val_loss did not improve from 0.44772\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0833 - acc: 0.9724 - val_loss: 0.5089 - val_acc: 0.8870\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0811 - acc: 0.9745\n",
      "Epoch 00071: val_loss did not improve from 0.44772\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0811 - acc: 0.9745 - val_loss: 0.5156 - val_acc: 0.8826\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0843 - acc: 0.9722\n",
      "Epoch 00072: val_loss did not improve from 0.44772\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0843 - acc: 0.9722 - val_loss: 0.6765 - val_acc: 0.8563\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0790 - acc: 0.9746\n",
      "Epoch 00073: val_loss did not improve from 0.44772\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0790 - acc: 0.9747 - val_loss: 0.6185 - val_acc: 0.8682\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0784 - acc: 0.9752\n",
      "Epoch 00074: val_loss did not improve from 0.44772\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0786 - acc: 0.9752 - val_loss: 0.5302 - val_acc: 0.8842\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0765 - acc: 0.9754\n",
      "Epoch 00075: val_loss did not improve from 0.44772\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0765 - acc: 0.9754 - val_loss: 0.4751 - val_acc: 0.8877\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0773 - acc: 0.9743\n",
      "Epoch 00076: val_loss did not improve from 0.44772\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0773 - acc: 0.9743 - val_loss: 0.5817 - val_acc: 0.8800\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0809 - acc: 0.9745\n",
      "Epoch 00077: val_loss did not improve from 0.44772\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0810 - acc: 0.9745 - val_loss: 0.4820 - val_acc: 0.8863\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0779 - acc: 0.9763\n",
      "Epoch 00078: val_loss did not improve from 0.44772\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0779 - acc: 0.9763 - val_loss: 0.5398 - val_acc: 0.8854\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0765 - acc: 0.9761\n",
      "Epoch 00079: val_loss did not improve from 0.44772\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0765 - acc: 0.9761 - val_loss: 0.8788 - val_acc: 0.8272\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0701 - acc: 0.9775\n",
      "Epoch 00080: val_loss did not improve from 0.44772\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0702 - acc: 0.9774 - val_loss: 0.4741 - val_acc: 0.8938\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0740 - acc: 0.9758\n",
      "Epoch 00081: val_loss did not improve from 0.44772\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0741 - acc: 0.9758 - val_loss: 0.4852 - val_acc: 0.8970\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0773 - acc: 0.9756\n",
      "Epoch 00082: val_loss did not improve from 0.44772\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0774 - acc: 0.9756 - val_loss: 0.5010 - val_acc: 0.8924\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0769 - acc: 0.9760\n",
      "Epoch 00083: val_loss did not improve from 0.44772\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0769 - acc: 0.9760 - val_loss: 0.5135 - val_acc: 0.8903\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0681 - acc: 0.9779\n",
      "Epoch 00084: val_loss did not improve from 0.44772\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0681 - acc: 0.9779 - val_loss: 0.5103 - val_acc: 0.8901\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0687 - acc: 0.9789\n",
      "Epoch 00085: val_loss did not improve from 0.44772\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0687 - acc: 0.9789 - val_loss: 0.5133 - val_acc: 0.8875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0648 - acc: 0.9787\n",
      "Epoch 00086: val_loss did not improve from 0.44772\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0649 - acc: 0.9786 - val_loss: 0.5379 - val_acc: 0.8859\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0728 - acc: 0.9768\n",
      "Epoch 00087: val_loss did not improve from 0.44772\n",
      "36805/36805 [==============================] - 54s 1ms/sample - loss: 0.0729 - acc: 0.9767 - val_loss: 0.6664 - val_acc: 0.8607\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0693 - acc: 0.9783\n",
      "Epoch 00088: val_loss did not improve from 0.44772\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0693 - acc: 0.9783 - val_loss: 0.5573 - val_acc: 0.8751\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0661 - acc: 0.9798\n",
      "Epoch 00089: val_loss did not improve from 0.44772\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0661 - acc: 0.9798 - val_loss: 0.5063 - val_acc: 0.8994\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0590 - acc: 0.9818\n",
      "Epoch 00090: val_loss did not improve from 0.44772\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0590 - acc: 0.9818 - val_loss: 0.5515 - val_acc: 0.8898\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0633 - acc: 0.9796\n",
      "Epoch 00091: val_loss did not improve from 0.44772\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0634 - acc: 0.9796 - val_loss: 0.5280 - val_acc: 0.8826\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0635 - acc: 0.9796\n",
      "Epoch 00092: val_loss did not improve from 0.44772\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0635 - acc: 0.9796 - val_loss: 0.5577 - val_acc: 0.8798\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0571 - acc: 0.9824\n",
      "Epoch 00093: val_loss did not improve from 0.44772\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0571 - acc: 0.9824 - val_loss: 0.4990 - val_acc: 0.8924\n",
      "\n",
      "1D_CNN_custom_multi_2_concat_DO_BN_5_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 669us/sample - loss: 0.5304 - acc: 0.8577\n",
      "Loss: 0.5303774013202386 Accuracy: 0.85773623\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.1006 - acc: 0.3654\n",
      "Epoch 00001: val_loss improved from inf to 1.67003, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_BN_6_conv_checkpoint/001-1.6700.hdf5\n",
      "36805/36805 [==============================] - 65s 2ms/sample - loss: 2.1007 - acc: 0.3654 - val_loss: 1.6700 - val_acc: 0.4510\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2343 - acc: 0.6127\n",
      "Epoch 00002: val_loss improved from 1.67003 to 0.85433, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_BN_6_conv_checkpoint/002-0.8543.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 1.2344 - acc: 0.6126 - val_loss: 0.8543 - val_acc: 0.7358\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9310 - acc: 0.7145\n",
      "Epoch 00003: val_loss improved from 0.85433 to 0.69167, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_BN_6_conv_checkpoint/003-0.6917.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.9309 - acc: 0.7145 - val_loss: 0.6917 - val_acc: 0.7957\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7665 - acc: 0.7674\n",
      "Epoch 00004: val_loss improved from 0.69167 to 0.66914, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_BN_6_conv_checkpoint/004-0.6691.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.7667 - acc: 0.7673 - val_loss: 0.6691 - val_acc: 0.8090\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6482 - acc: 0.8051\n",
      "Epoch 00005: val_loss improved from 0.66914 to 0.51672, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_BN_6_conv_checkpoint/005-0.5167.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.6482 - acc: 0.8051 - val_loss: 0.5167 - val_acc: 0.8593\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5698 - acc: 0.8315\n",
      "Epoch 00006: val_loss did not improve from 0.51672\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.5698 - acc: 0.8314 - val_loss: 0.5243 - val_acc: 0.8470\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4999 - acc: 0.8502\n",
      "Epoch 00007: val_loss improved from 0.51672 to 0.51379, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_BN_6_conv_checkpoint/007-0.5138.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.4999 - acc: 0.8502 - val_loss: 0.5138 - val_acc: 0.8530\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4509 - acc: 0.8659\n",
      "Epoch 00008: val_loss improved from 0.51379 to 0.41106, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_BN_6_conv_checkpoint/008-0.4111.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.4509 - acc: 0.8659 - val_loss: 0.4111 - val_acc: 0.8817\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4078 - acc: 0.8782\n",
      "Epoch 00009: val_loss did not improve from 0.41106\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.4077 - acc: 0.8782 - val_loss: 0.4289 - val_acc: 0.8763\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3805 - acc: 0.8853\n",
      "Epoch 00010: val_loss improved from 0.41106 to 0.40092, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_BN_6_conv_checkpoint/010-0.4009.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.3804 - acc: 0.8853 - val_loss: 0.4009 - val_acc: 0.8933\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3524 - acc: 0.8926\n",
      "Epoch 00011: val_loss improved from 0.40092 to 0.35234, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_BN_6_conv_checkpoint/011-0.3523.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.3524 - acc: 0.8926 - val_loss: 0.3523 - val_acc: 0.9031\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3313 - acc: 0.9004\n",
      "Epoch 00012: val_loss improved from 0.35234 to 0.34812, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_BN_6_conv_checkpoint/012-0.3481.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.3312 - acc: 0.9004 - val_loss: 0.3481 - val_acc: 0.9061\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3089 - acc: 0.9077\n",
      "Epoch 00013: val_loss improved from 0.34812 to 0.33087, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_BN_6_conv_checkpoint/013-0.3309.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.3089 - acc: 0.9077 - val_loss: 0.3309 - val_acc: 0.9075\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2871 - acc: 0.9116\n",
      "Epoch 00014: val_loss did not improve from 0.33087\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.2871 - acc: 0.9116 - val_loss: 0.3362 - val_acc: 0.9068\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2747 - acc: 0.9160\n",
      "Epoch 00015: val_loss improved from 0.33087 to 0.32506, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_BN_6_conv_checkpoint/015-0.3251.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.2747 - acc: 0.9160 - val_loss: 0.3251 - val_acc: 0.9068\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2515 - acc: 0.9210\n",
      "Epoch 00016: val_loss improved from 0.32506 to 0.31243, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_BN_6_conv_checkpoint/016-0.3124.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.2515 - acc: 0.9210 - val_loss: 0.3124 - val_acc: 0.9143\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2424 - acc: 0.9260\n",
      "Epoch 00017: val_loss did not improve from 0.31243\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.2425 - acc: 0.9260 - val_loss: 0.3234 - val_acc: 0.9094\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2321 - acc: 0.9273\n",
      "Epoch 00018: val_loss did not improve from 0.31243\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.2321 - acc: 0.9273 - val_loss: 0.3557 - val_acc: 0.9031\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2197 - acc: 0.9318\n",
      "Epoch 00019: val_loss did not improve from 0.31243\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.2197 - acc: 0.9318 - val_loss: 0.3216 - val_acc: 0.9161\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2055 - acc: 0.9348\n",
      "Epoch 00020: val_loss did not improve from 0.31243\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.2055 - acc: 0.9348 - val_loss: 0.3817 - val_acc: 0.9043\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1971 - acc: 0.9393\n",
      "Epoch 00021: val_loss improved from 0.31243 to 0.29656, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_BN_6_conv_checkpoint/021-0.2966.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1970 - acc: 0.9393 - val_loss: 0.2966 - val_acc: 0.9264\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1853 - acc: 0.9410\n",
      "Epoch 00022: val_loss did not improve from 0.29656\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1854 - acc: 0.9410 - val_loss: 0.3359 - val_acc: 0.9140\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1779 - acc: 0.9447\n",
      "Epoch 00023: val_loss did not improve from 0.29656\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1778 - acc: 0.9447 - val_loss: 0.3081 - val_acc: 0.9175\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1670 - acc: 0.9473\n",
      "Epoch 00024: val_loss improved from 0.29656 to 0.27939, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_BN_6_conv_checkpoint/024-0.2794.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1669 - acc: 0.9473 - val_loss: 0.2794 - val_acc: 0.9297\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1613 - acc: 0.9493\n",
      "Epoch 00025: val_loss did not improve from 0.27939\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1614 - acc: 0.9492 - val_loss: 0.3734 - val_acc: 0.9015\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1547 - acc: 0.9502\n",
      "Epoch 00026: val_loss did not improve from 0.27939\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1547 - acc: 0.9502 - val_loss: 0.2818 - val_acc: 0.9308\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1533 - acc: 0.9492\n",
      "Epoch 00027: val_loss did not improve from 0.27939\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1534 - acc: 0.9491 - val_loss: 0.2980 - val_acc: 0.9224\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1490 - acc: 0.9530\n",
      "Epoch 00028: val_loss did not improve from 0.27939\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1493 - acc: 0.9530 - val_loss: 0.3245 - val_acc: 0.9182\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1451 - acc: 0.9541\n",
      "Epoch 00029: val_loss did not improve from 0.27939\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1452 - acc: 0.9541 - val_loss: 0.3265 - val_acc: 0.9208\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1359 - acc: 0.9571\n",
      "Epoch 00030: val_loss did not improve from 0.27939\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1359 - acc: 0.9571 - val_loss: 0.2843 - val_acc: 0.9311\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1286 - acc: 0.9585\n",
      "Epoch 00031: val_loss improved from 0.27939 to 0.27026, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_BN_6_conv_checkpoint/031-0.2703.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1285 - acc: 0.9585 - val_loss: 0.2703 - val_acc: 0.9357\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1205 - acc: 0.9614\n",
      "Epoch 00032: val_loss did not improve from 0.27026\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1205 - acc: 0.9614 - val_loss: 0.3151 - val_acc: 0.9266\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1241 - acc: 0.9615\n",
      "Epoch 00033: val_loss did not improve from 0.27026\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1241 - acc: 0.9615 - val_loss: 0.3367 - val_acc: 0.9185\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1164 - acc: 0.9623\n",
      "Epoch 00034: val_loss did not improve from 0.27026\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1164 - acc: 0.9623 - val_loss: 0.3155 - val_acc: 0.9273\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1149 - acc: 0.9630\n",
      "Epoch 00035: val_loss did not improve from 0.27026\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1150 - acc: 0.9630 - val_loss: 0.3148 - val_acc: 0.9271\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1117 - acc: 0.9648\n",
      "Epoch 00036: val_loss did not improve from 0.27026\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1117 - acc: 0.9648 - val_loss: 0.2952 - val_acc: 0.9327\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1058 - acc: 0.9662\n",
      "Epoch 00037: val_loss improved from 0.27026 to 0.26611, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_BN_6_conv_checkpoint/037-0.2661.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1058 - acc: 0.9662 - val_loss: 0.2661 - val_acc: 0.9313\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1008 - acc: 0.9678\n",
      "Epoch 00038: val_loss did not improve from 0.26611\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1009 - acc: 0.9678 - val_loss: 0.3389 - val_acc: 0.9201\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0984 - acc: 0.9683\n",
      "Epoch 00039: val_loss did not improve from 0.26611\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0986 - acc: 0.9682 - val_loss: 0.3221 - val_acc: 0.9241\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0990 - acc: 0.9683\n",
      "Epoch 00040: val_loss did not improve from 0.26611\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0990 - acc: 0.9683 - val_loss: 0.2857 - val_acc: 0.9336\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0882 - acc: 0.9707\n",
      "Epoch 00041: val_loss did not improve from 0.26611\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0882 - acc: 0.9707 - val_loss: 0.3043 - val_acc: 0.9250\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0898 - acc: 0.9709\n",
      "Epoch 00042: val_loss did not improve from 0.26611\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0900 - acc: 0.9708 - val_loss: 0.3325 - val_acc: 0.9201\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0967 - acc: 0.9685\n",
      "Epoch 00043: val_loss did not improve from 0.26611\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0967 - acc: 0.9685 - val_loss: 0.2774 - val_acc: 0.9378\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0827 - acc: 0.9733\n",
      "Epoch 00044: val_loss did not improve from 0.26611\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.0828 - acc: 0.9733 - val_loss: 0.2872 - val_acc: 0.9308\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0798 - acc: 0.9743\n",
      "Epoch 00045: val_loss did not improve from 0.26611\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0797 - acc: 0.9744 - val_loss: 0.2923 - val_acc: 0.9313\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0800 - acc: 0.9734\n",
      "Epoch 00046: val_loss did not improve from 0.26611\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0800 - acc: 0.9734 - val_loss: 0.2698 - val_acc: 0.9327\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0752 - acc: 0.9762\n",
      "Epoch 00047: val_loss did not improve from 0.26611\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0752 - acc: 0.9761 - val_loss: 0.2913 - val_acc: 0.9350\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0777 - acc: 0.9746\n",
      "Epoch 00048: val_loss did not improve from 0.26611\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0777 - acc: 0.9746 - val_loss: 0.2986 - val_acc: 0.9276\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0717 - acc: 0.9768\n",
      "Epoch 00049: val_loss did not improve from 0.26611\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0718 - acc: 0.9767 - val_loss: 0.3409 - val_acc: 0.9182\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0761 - acc: 0.9754\n",
      "Epoch 00050: val_loss did not improve from 0.26611\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0762 - acc: 0.9753 - val_loss: 0.3045 - val_acc: 0.9359\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0726 - acc: 0.9765\n",
      "Epoch 00051: val_loss did not improve from 0.26611\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0726 - acc: 0.9765 - val_loss: 0.4122 - val_acc: 0.9113\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0656 - acc: 0.9786\n",
      "Epoch 00052: val_loss did not improve from 0.26611\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0657 - acc: 0.9786 - val_loss: 0.2879 - val_acc: 0.9315\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0716 - acc: 0.9761\n",
      "Epoch 00053: val_loss did not improve from 0.26611\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0716 - acc: 0.9761 - val_loss: 0.2718 - val_acc: 0.9378\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0651 - acc: 0.9790\n",
      "Epoch 00054: val_loss did not improve from 0.26611\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0651 - acc: 0.9790 - val_loss: 0.2844 - val_acc: 0.9371\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0651 - acc: 0.9795\n",
      "Epoch 00055: val_loss did not improve from 0.26611\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0651 - acc: 0.9795 - val_loss: 0.2961 - val_acc: 0.9366\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0682 - acc: 0.9772\n",
      "Epoch 00056: val_loss did not improve from 0.26611\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0682 - acc: 0.9772 - val_loss: 0.2901 - val_acc: 0.9352\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0596 - acc: 0.9803\n",
      "Epoch 00057: val_loss did not improve from 0.26611\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0596 - acc: 0.9803 - val_loss: 0.2910 - val_acc: 0.9322\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0604 - acc: 0.9807\n",
      "Epoch 00058: val_loss improved from 0.26611 to 0.26411, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_BN_6_conv_checkpoint/058-0.2641.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0604 - acc: 0.9807 - val_loss: 0.2641 - val_acc: 0.9376\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0540 - acc: 0.9826\n",
      "Epoch 00059: val_loss did not improve from 0.26411\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0540 - acc: 0.9826 - val_loss: 0.2709 - val_acc: 0.9373\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0574 - acc: 0.9810\n",
      "Epoch 00060: val_loss did not improve from 0.26411\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0574 - acc: 0.9810 - val_loss: 0.2954 - val_acc: 0.9355\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0561 - acc: 0.9821\n",
      "Epoch 00061: val_loss did not improve from 0.26411\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0561 - acc: 0.9821 - val_loss: 0.3744 - val_acc: 0.9192\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0554 - acc: 0.9824\n",
      "Epoch 00062: val_loss did not improve from 0.26411\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0554 - acc: 0.9824 - val_loss: 0.3073 - val_acc: 0.9338\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0518 - acc: 0.9835\n",
      "Epoch 00063: val_loss did not improve from 0.26411\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0518 - acc: 0.9835 - val_loss: 0.2812 - val_acc: 0.9378\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0519 - acc: 0.9829\n",
      "Epoch 00064: val_loss did not improve from 0.26411\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0519 - acc: 0.9828 - val_loss: 0.3050 - val_acc: 0.9357\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0523 - acc: 0.9827\n",
      "Epoch 00065: val_loss did not improve from 0.26411\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0523 - acc: 0.9827 - val_loss: 0.3102 - val_acc: 0.9271\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0462 - acc: 0.9857\n",
      "Epoch 00066: val_loss did not improve from 0.26411\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0462 - acc: 0.9857 - val_loss: 0.2693 - val_acc: 0.9383\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0449 - acc: 0.9849\n",
      "Epoch 00067: val_loss did not improve from 0.26411\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0449 - acc: 0.9849 - val_loss: 0.2931 - val_acc: 0.9371\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0462 - acc: 0.9857\n",
      "Epoch 00068: val_loss did not improve from 0.26411\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0462 - acc: 0.9857 - val_loss: 0.3039 - val_acc: 0.9329\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0500 - acc: 0.9839\n",
      "Epoch 00069: val_loss did not improve from 0.26411\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0501 - acc: 0.9839 - val_loss: 0.3224 - val_acc: 0.9308\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0527 - acc: 0.9833\n",
      "Epoch 00070: val_loss did not improve from 0.26411\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0527 - acc: 0.9833 - val_loss: 0.2840 - val_acc: 0.9359\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0438 - acc: 0.9862\n",
      "Epoch 00071: val_loss did not improve from 0.26411\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0438 - acc: 0.9863 - val_loss: 0.3014 - val_acc: 0.9373\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0436 - acc: 0.9861\n",
      "Epoch 00072: val_loss did not improve from 0.26411\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0436 - acc: 0.9861 - val_loss: 0.2972 - val_acc: 0.9420\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0405 - acc: 0.9870\n",
      "Epoch 00073: val_loss did not improve from 0.26411\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0404 - acc: 0.9870 - val_loss: 0.3054 - val_acc: 0.9338\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0410 - acc: 0.9872\n",
      "Epoch 00074: val_loss did not improve from 0.26411\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0410 - acc: 0.9872 - val_loss: 0.2942 - val_acc: 0.9355\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0394 - acc: 0.9880\n",
      "Epoch 00075: val_loss did not improve from 0.26411\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0394 - acc: 0.9879 - val_loss: 0.3859 - val_acc: 0.9185\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0472 - acc: 0.9855\n",
      "Epoch 00076: val_loss did not improve from 0.26411\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0474 - acc: 0.9855 - val_loss: 0.3030 - val_acc: 0.9320\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0492 - acc: 0.9839\n",
      "Epoch 00077: val_loss did not improve from 0.26411\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0492 - acc: 0.9839 - val_loss: 0.2807 - val_acc: 0.9406\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0367 - acc: 0.9884\n",
      "Epoch 00078: val_loss did not improve from 0.26411\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0367 - acc: 0.9884 - val_loss: 0.2919 - val_acc: 0.9439\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0371 - acc: 0.9881\n",
      "Epoch 00079: val_loss did not improve from 0.26411\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0371 - acc: 0.9881 - val_loss: 0.3989 - val_acc: 0.9180\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0364 - acc: 0.9879\n",
      "Epoch 00080: val_loss did not improve from 0.26411\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0364 - acc: 0.9879 - val_loss: 0.3265 - val_acc: 0.9366\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0395 - acc: 0.9879\n",
      "Epoch 00081: val_loss did not improve from 0.26411\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0395 - acc: 0.9879 - val_loss: 0.3093 - val_acc: 0.9380\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0384 - acc: 0.9877\n",
      "Epoch 00082: val_loss did not improve from 0.26411\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0386 - acc: 0.9877 - val_loss: 0.3065 - val_acc: 0.9364\n",
      "Epoch 83/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0439 - acc: 0.9860\n",
      "Epoch 00083: val_loss did not improve from 0.26411\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0439 - acc: 0.9860 - val_loss: 0.2958 - val_acc: 0.9397\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0345 - acc: 0.9896\n",
      "Epoch 00084: val_loss did not improve from 0.26411\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0345 - acc: 0.9896 - val_loss: 0.2910 - val_acc: 0.9408\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0349 - acc: 0.9882\n",
      "Epoch 00085: val_loss did not improve from 0.26411\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0349 - acc: 0.9882 - val_loss: 0.3413 - val_acc: 0.9327\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0396 - acc: 0.9873\n",
      "Epoch 00086: val_loss did not improve from 0.26411\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0396 - acc: 0.9873 - val_loss: 0.3101 - val_acc: 0.9420\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0368 - acc: 0.9881\n",
      "Epoch 00087: val_loss did not improve from 0.26411\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0370 - acc: 0.9881 - val_loss: 0.3015 - val_acc: 0.9371\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0413 - acc: 0.9865\n",
      "Epoch 00088: val_loss did not improve from 0.26411\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0413 - acc: 0.9865 - val_loss: 0.2813 - val_acc: 0.9427\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0325 - acc: 0.9894\n",
      "Epoch 00089: val_loss did not improve from 0.26411\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0326 - acc: 0.9894 - val_loss: 0.3371 - val_acc: 0.9334\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0318 - acc: 0.9901\n",
      "Epoch 00090: val_loss did not improve from 0.26411\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0319 - acc: 0.9901 - val_loss: 0.3569 - val_acc: 0.9334\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0405 - acc: 0.9872\n",
      "Epoch 00091: val_loss did not improve from 0.26411\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0405 - acc: 0.9872 - val_loss: 0.2910 - val_acc: 0.9373\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0299 - acc: 0.9907\n",
      "Epoch 00092: val_loss did not improve from 0.26411\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0299 - acc: 0.9907 - val_loss: 0.3132 - val_acc: 0.9394\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0320 - acc: 0.9898\n",
      "Epoch 00093: val_loss did not improve from 0.26411\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0320 - acc: 0.9898 - val_loss: 0.3012 - val_acc: 0.9380\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0285 - acc: 0.9913\n",
      "Epoch 00094: val_loss did not improve from 0.26411\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0286 - acc: 0.9913 - val_loss: 0.3168 - val_acc: 0.9392\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0413 - acc: 0.9871\n",
      "Epoch 00095: val_loss did not improve from 0.26411\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0413 - acc: 0.9871 - val_loss: 0.3050 - val_acc: 0.9383\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0297 - acc: 0.9908\n",
      "Epoch 00096: val_loss did not improve from 0.26411\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0297 - acc: 0.9908 - val_loss: 0.3219 - val_acc: 0.9373\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0316 - acc: 0.9903\n",
      "Epoch 00097: val_loss did not improve from 0.26411\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0315 - acc: 0.9903 - val_loss: 0.2913 - val_acc: 0.9434\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0285 - acc: 0.9913\n",
      "Epoch 00098: val_loss did not improve from 0.26411\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0286 - acc: 0.9913 - val_loss: 0.2902 - val_acc: 0.9439\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0407 - acc: 0.9877\n",
      "Epoch 00099: val_loss did not improve from 0.26411\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0409 - acc: 0.9876 - val_loss: 0.3721 - val_acc: 0.9250\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0401 - acc: 0.9878\n",
      "Epoch 00100: val_loss did not improve from 0.26411\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0401 - acc: 0.9878 - val_loss: 0.3034 - val_acc: 0.9336\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0256 - acc: 0.9923\n",
      "Epoch 00101: val_loss did not improve from 0.26411\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0256 - acc: 0.9923 - val_loss: 0.3946 - val_acc: 0.9199\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0258 - acc: 0.9924\n",
      "Epoch 00102: val_loss did not improve from 0.26411\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0258 - acc: 0.9924 - val_loss: 0.3188 - val_acc: 0.9343\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0294 - acc: 0.9905\n",
      "Epoch 00103: val_loss did not improve from 0.26411\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.0294 - acc: 0.9905 - val_loss: 0.3694 - val_acc: 0.9285\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0281 - acc: 0.9913\n",
      "Epoch 00104: val_loss did not improve from 0.26411\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0281 - acc: 0.9913 - val_loss: 0.3175 - val_acc: 0.9401\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0282 - acc: 0.9914\n",
      "Epoch 00105: val_loss did not improve from 0.26411\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0282 - acc: 0.9914 - val_loss: 0.3036 - val_acc: 0.9425\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0294 - acc: 0.9908\n",
      "Epoch 00106: val_loss did not improve from 0.26411\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0294 - acc: 0.9908 - val_loss: 0.3325 - val_acc: 0.9397\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0244 - acc: 0.9927\n",
      "Epoch 00107: val_loss did not improve from 0.26411\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0244 - acc: 0.9927 - val_loss: 0.3244 - val_acc: 0.9378\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0288 - acc: 0.9907\n",
      "Epoch 00108: val_loss did not improve from 0.26411\n",
      "36805/36805 [==============================] - 55s 1ms/sample - loss: 0.0289 - acc: 0.9907 - val_loss: 0.6003 - val_acc: 0.8915\n",
      "\n",
      "1D_CNN_custom_multi_2_concat_DO_BN_6_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 697us/sample - loss: 0.2986 - acc: 0.9213\n",
      "Loss: 0.29863198925030193 Accuracy: 0.92128766\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.1552 - acc: 0.3472\n",
      "Epoch 00001: val_loss improved from inf to 1.36953, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_BN_7_conv_checkpoint/001-1.3695.hdf5\n",
      "36805/36805 [==============================] - 69s 2ms/sample - loss: 2.1551 - acc: 0.3473 - val_loss: 1.3695 - val_acc: 0.5632\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1605 - acc: 0.6308\n",
      "Epoch 00002: val_loss improved from 1.36953 to 0.82601, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_BN_7_conv_checkpoint/002-0.8260.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 1.1606 - acc: 0.6308 - val_loss: 0.8260 - val_acc: 0.7419\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8052 - acc: 0.7513\n",
      "Epoch 00003: val_loss improved from 0.82601 to 0.54838, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_BN_7_conv_checkpoint/003-0.5484.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.8051 - acc: 0.7513 - val_loss: 0.5484 - val_acc: 0.8460\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6246 - acc: 0.8072\n",
      "Epoch 00004: val_loss improved from 0.54838 to 0.53029, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_BN_7_conv_checkpoint/004-0.5303.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.6249 - acc: 0.8071 - val_loss: 0.5303 - val_acc: 0.8470\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5092 - acc: 0.8435\n",
      "Epoch 00005: val_loss improved from 0.53029 to 0.33996, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_BN_7_conv_checkpoint/005-0.3400.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.5091 - acc: 0.8435 - val_loss: 0.3400 - val_acc: 0.9017\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4344 - acc: 0.8691\n",
      "Epoch 00006: val_loss improved from 0.33996 to 0.30987, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_BN_7_conv_checkpoint/006-0.3099.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.4345 - acc: 0.8691 - val_loss: 0.3099 - val_acc: 0.9082\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3733 - acc: 0.8868\n",
      "Epoch 00007: val_loss improved from 0.30987 to 0.28676, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_BN_7_conv_checkpoint/007-0.2868.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.3734 - acc: 0.8868 - val_loss: 0.2868 - val_acc: 0.9164\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3314 - acc: 0.8973\n",
      "Epoch 00008: val_loss did not improve from 0.28676\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.3314 - acc: 0.8973 - val_loss: 0.2885 - val_acc: 0.9168\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2967 - acc: 0.9093\n",
      "Epoch 00009: val_loss improved from 0.28676 to 0.23874, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_BN_7_conv_checkpoint/009-0.2387.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.2967 - acc: 0.9093 - val_loss: 0.2387 - val_acc: 0.9324\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2742 - acc: 0.9153\n",
      "Epoch 00010: val_loss did not improve from 0.23874\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.2742 - acc: 0.9153 - val_loss: 0.2461 - val_acc: 0.9287\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2514 - acc: 0.9226\n",
      "Epoch 00011: val_loss improved from 0.23874 to 0.22337, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_BN_7_conv_checkpoint/011-0.2234.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.2515 - acc: 0.9226 - val_loss: 0.2234 - val_acc: 0.9352\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2268 - acc: 0.9297\n",
      "Epoch 00012: val_loss improved from 0.22337 to 0.20666, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_BN_7_conv_checkpoint/012-0.2067.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.2268 - acc: 0.9297 - val_loss: 0.2067 - val_acc: 0.9397\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2126 - acc: 0.9319\n",
      "Epoch 00013: val_loss did not improve from 0.20666\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.2126 - acc: 0.9319 - val_loss: 0.2430 - val_acc: 0.9271\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2000 - acc: 0.9385\n",
      "Epoch 00014: val_loss did not improve from 0.20666\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.2001 - acc: 0.9384 - val_loss: 0.2163 - val_acc: 0.9366\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1851 - acc: 0.9420\n",
      "Epoch 00015: val_loss did not improve from 0.20666\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1853 - acc: 0.9420 - val_loss: 0.2395 - val_acc: 0.9322\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1691 - acc: 0.9462\n",
      "Epoch 00016: val_loss improved from 0.20666 to 0.20568, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_BN_7_conv_checkpoint/016-0.2057.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1692 - acc: 0.9461 - val_loss: 0.2057 - val_acc: 0.9401\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1675 - acc: 0.9467\n",
      "Epoch 00017: val_loss did not improve from 0.20568\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1674 - acc: 0.9467 - val_loss: 0.2214 - val_acc: 0.9352\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1512 - acc: 0.9521\n",
      "Epoch 00018: val_loss improved from 0.20568 to 0.18820, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_BN_7_conv_checkpoint/018-0.1882.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1513 - acc: 0.9520 - val_loss: 0.1882 - val_acc: 0.9457\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1456 - acc: 0.9538\n",
      "Epoch 00019: val_loss improved from 0.18820 to 0.18001, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_BN_7_conv_checkpoint/019-0.1800.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1456 - acc: 0.9538 - val_loss: 0.1800 - val_acc: 0.9525\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1341 - acc: 0.9573\n",
      "Epoch 00020: val_loss improved from 0.18001 to 0.17000, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_BN_7_conv_checkpoint/020-0.1700.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1343 - acc: 0.9573 - val_loss: 0.1700 - val_acc: 0.9506\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1331 - acc: 0.9577\n",
      "Epoch 00021: val_loss did not improve from 0.17000\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1331 - acc: 0.9577 - val_loss: 0.1765 - val_acc: 0.9488\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1236 - acc: 0.9602\n",
      "Epoch 00022: val_loss did not improve from 0.17000\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1236 - acc: 0.9602 - val_loss: 0.2216 - val_acc: 0.9366\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1153 - acc: 0.9627\n",
      "Epoch 00023: val_loss did not improve from 0.17000\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1153 - acc: 0.9627 - val_loss: 0.1869 - val_acc: 0.9483\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1088 - acc: 0.9641\n",
      "Epoch 00024: val_loss did not improve from 0.17000\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1088 - acc: 0.9641 - val_loss: 0.1807 - val_acc: 0.9469\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1029 - acc: 0.9664\n",
      "Epoch 00025: val_loss did not improve from 0.17000\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.1029 - acc: 0.9664 - val_loss: 0.1773 - val_acc: 0.9499\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0978 - acc: 0.9673\n",
      "Epoch 00026: val_loss did not improve from 0.17000\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0978 - acc: 0.9673 - val_loss: 0.1888 - val_acc: 0.9453\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0910 - acc: 0.9709\n",
      "Epoch 00027: val_loss did not improve from 0.17000\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0910 - acc: 0.9709 - val_loss: 0.1967 - val_acc: 0.9502\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0947 - acc: 0.9694\n",
      "Epoch 00028: val_loss improved from 0.17000 to 0.16328, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_BN_7_conv_checkpoint/028-0.1633.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0947 - acc: 0.9694 - val_loss: 0.1633 - val_acc: 0.9562\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0880 - acc: 0.9710\n",
      "Epoch 00029: val_loss did not improve from 0.16328\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0880 - acc: 0.9710 - val_loss: 0.1865 - val_acc: 0.9502\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0782 - acc: 0.9755\n",
      "Epoch 00030: val_loss did not improve from 0.16328\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0782 - acc: 0.9755 - val_loss: 0.2079 - val_acc: 0.9453\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0795 - acc: 0.9747\n",
      "Epoch 00031: val_loss did not improve from 0.16328\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0795 - acc: 0.9747 - val_loss: 0.1714 - val_acc: 0.9548\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0763 - acc: 0.9749\n",
      "Epoch 00032: val_loss did not improve from 0.16328\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0764 - acc: 0.9748 - val_loss: 0.2566 - val_acc: 0.9276\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0757 - acc: 0.9764\n",
      "Epoch 00033: val_loss improved from 0.16328 to 0.15838, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_BN_7_conv_checkpoint/033-0.1584.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0758 - acc: 0.9763 - val_loss: 0.1584 - val_acc: 0.9557\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0778 - acc: 0.9751\n",
      "Epoch 00034: val_loss did not improve from 0.15838\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0778 - acc: 0.9751 - val_loss: 0.1744 - val_acc: 0.9527\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0641 - acc: 0.9799\n",
      "Epoch 00035: val_loss did not improve from 0.15838\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0642 - acc: 0.9798 - val_loss: 0.1682 - val_acc: 0.9541\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0658 - acc: 0.9783\n",
      "Epoch 00036: val_loss improved from 0.15838 to 0.15350, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_BN_7_conv_checkpoint/036-0.1535.hdf5\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0658 - acc: 0.9783 - val_loss: 0.1535 - val_acc: 0.9592\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0602 - acc: 0.9815\n",
      "Epoch 00037: val_loss did not improve from 0.15350\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0602 - acc: 0.9816 - val_loss: 0.1826 - val_acc: 0.9522\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0576 - acc: 0.9827\n",
      "Epoch 00038: val_loss did not improve from 0.15350\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0576 - acc: 0.9827 - val_loss: 0.1957 - val_acc: 0.9436\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0596 - acc: 0.9805\n",
      "Epoch 00039: val_loss did not improve from 0.15350\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0596 - acc: 0.9805 - val_loss: 0.2110 - val_acc: 0.9478\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0528 - acc: 0.9833\n",
      "Epoch 00040: val_loss did not improve from 0.15350\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0528 - acc: 0.9833 - val_loss: 0.1820 - val_acc: 0.9497\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0561 - acc: 0.9815\n",
      "Epoch 00041: val_loss did not improve from 0.15350\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0561 - acc: 0.9816 - val_loss: 0.1809 - val_acc: 0.9564\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0492 - acc: 0.9842\n",
      "Epoch 00042: val_loss did not improve from 0.15350\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.0492 - acc: 0.9842 - val_loss: 0.1794 - val_acc: 0.9541\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0509 - acc: 0.9835\n",
      "Epoch 00043: val_loss did not improve from 0.15350\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0509 - acc: 0.9835 - val_loss: 0.3112 - val_acc: 0.9243\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0496 - acc: 0.9838\n",
      "Epoch 00044: val_loss did not improve from 0.15350\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.0496 - acc: 0.9838 - val_loss: 0.1886 - val_acc: 0.9553\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0449 - acc: 0.9855\n",
      "Epoch 00045: val_loss did not improve from 0.15350\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0449 - acc: 0.9855 - val_loss: 0.1947 - val_acc: 0.9504\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0437 - acc: 0.9867\n",
      "Epoch 00046: val_loss did not improve from 0.15350\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.0437 - acc: 0.9867 - val_loss: 0.2752 - val_acc: 0.9327\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0525 - acc: 0.9831\n",
      "Epoch 00047: val_loss did not improve from 0.15350\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.0525 - acc: 0.9831 - val_loss: 0.1539 - val_acc: 0.9609\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0396 - acc: 0.9879\n",
      "Epoch 00048: val_loss did not improve from 0.15350\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.0396 - acc: 0.9879 - val_loss: 0.2186 - val_acc: 0.9450\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0399 - acc: 0.9877\n",
      "Epoch 00049: val_loss did not improve from 0.15350\n",
      "36805/36805 [==============================] - 55s 2ms/sample - loss: 0.0401 - acc: 0.9876 - val_loss: 0.1961 - val_acc: 0.9522\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0458 - acc: 0.9858\n",
      "Epoch 00050: val_loss did not improve from 0.15350\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0458 - acc: 0.9858 - val_loss: 0.1983 - val_acc: 0.9527\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0344 - acc: 0.9896\n",
      "Epoch 00051: val_loss did not improve from 0.15350\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0344 - acc: 0.9896 - val_loss: 0.1987 - val_acc: 0.9497\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0353 - acc: 0.9895\n",
      "Epoch 00052: val_loss did not improve from 0.15350\n",
      "36805/36805 [==============================] - 57s 2ms/sample - loss: 0.0353 - acc: 0.9895 - val_loss: 0.1909 - val_acc: 0.9557\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0331 - acc: 0.9899\n",
      "Epoch 00053: val_loss did not improve from 0.15350\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0331 - acc: 0.9898 - val_loss: 0.2674 - val_acc: 0.9345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0391 - acc: 0.9870\n",
      "Epoch 00054: val_loss did not improve from 0.15350\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0391 - acc: 0.9870 - val_loss: 0.2039 - val_acc: 0.9504\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0363 - acc: 0.9892\n",
      "Epoch 00055: val_loss did not improve from 0.15350\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0363 - acc: 0.9892 - val_loss: 0.1792 - val_acc: 0.9555\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0304 - acc: 0.9905\n",
      "Epoch 00056: val_loss did not improve from 0.15350\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0304 - acc: 0.9905 - val_loss: 0.1917 - val_acc: 0.9560\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0316 - acc: 0.9902\n",
      "Epoch 00057: val_loss did not improve from 0.15350\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0316 - acc: 0.9902 - val_loss: 0.1618 - val_acc: 0.9592\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0271 - acc: 0.9922\n",
      "Epoch 00058: val_loss did not improve from 0.15350\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0272 - acc: 0.9921 - val_loss: 0.2595 - val_acc: 0.9460\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0372 - acc: 0.9883\n",
      "Epoch 00059: val_loss did not improve from 0.15350\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0371 - acc: 0.9883 - val_loss: 0.2105 - val_acc: 0.9525\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0292 - acc: 0.9911\n",
      "Epoch 00060: val_loss did not improve from 0.15350\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0292 - acc: 0.9911 - val_loss: 0.1917 - val_acc: 0.9536\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0269 - acc: 0.9920\n",
      "Epoch 00061: val_loss did not improve from 0.15350\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0272 - acc: 0.9920 - val_loss: 0.1749 - val_acc: 0.9597\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0359 - acc: 0.9892\n",
      "Epoch 00062: val_loss did not improve from 0.15350\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0359 - acc: 0.9892 - val_loss: 0.1769 - val_acc: 0.9571\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0244 - acc: 0.9929\n",
      "Epoch 00063: val_loss did not improve from 0.15350\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0244 - acc: 0.9929 - val_loss: 0.3250 - val_acc: 0.9224\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0282 - acc: 0.9913\n",
      "Epoch 00064: val_loss did not improve from 0.15350\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0282 - acc: 0.9913 - val_loss: 0.2055 - val_acc: 0.9532\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0248 - acc: 0.9927\n",
      "Epoch 00065: val_loss did not improve from 0.15350\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0248 - acc: 0.9927 - val_loss: 0.2050 - val_acc: 0.9539\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0255 - acc: 0.9920\n",
      "Epoch 00066: val_loss did not improve from 0.15350\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0257 - acc: 0.9920 - val_loss: 0.2159 - val_acc: 0.9495\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0356 - acc: 0.9889\n",
      "Epoch 00067: val_loss did not improve from 0.15350\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0356 - acc: 0.9889 - val_loss: 0.1877 - val_acc: 0.9581\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0215 - acc: 0.9938\n",
      "Epoch 00068: val_loss did not improve from 0.15350\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0215 - acc: 0.9938 - val_loss: 0.1964 - val_acc: 0.9509\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0217 - acc: 0.9930\n",
      "Epoch 00069: val_loss did not improve from 0.15350\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0217 - acc: 0.9930 - val_loss: 0.2167 - val_acc: 0.9515\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0268 - acc: 0.9920\n",
      "Epoch 00070: val_loss did not improve from 0.15350\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0268 - acc: 0.9920 - val_loss: 0.2590 - val_acc: 0.9422\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0244 - acc: 0.9922\n",
      "Epoch 00071: val_loss did not improve from 0.15350\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0245 - acc: 0.9921 - val_loss: 0.2682 - val_acc: 0.9446\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0352 - acc: 0.9890\n",
      "Epoch 00072: val_loss did not improve from 0.15350\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0352 - acc: 0.9891 - val_loss: 0.2357 - val_acc: 0.9446\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0188 - acc: 0.9943\n",
      "Epoch 00073: val_loss did not improve from 0.15350\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0188 - acc: 0.9943 - val_loss: 0.1950 - val_acc: 0.9574\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0192 - acc: 0.9944\n",
      "Epoch 00074: val_loss did not improve from 0.15350\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0192 - acc: 0.9944 - val_loss: 0.1787 - val_acc: 0.9604\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0187 - acc: 0.9944\n",
      "Epoch 00075: val_loss did not improve from 0.15350\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0188 - acc: 0.9944 - val_loss: 0.2634 - val_acc: 0.9383\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0286 - acc: 0.9907\n",
      "Epoch 00076: val_loss did not improve from 0.15350\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0286 - acc: 0.9907 - val_loss: 0.1842 - val_acc: 0.9583\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0267 - acc: 0.9917\n",
      "Epoch 00077: val_loss did not improve from 0.15350\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0268 - acc: 0.9917 - val_loss: 0.2123 - val_acc: 0.9562\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0231 - acc: 0.9930\n",
      "Epoch 00078: val_loss did not improve from 0.15350\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0231 - acc: 0.9930 - val_loss: 0.1873 - val_acc: 0.9578\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0259 - acc: 0.9914\n",
      "Epoch 00079: val_loss did not improve from 0.15350\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0259 - acc: 0.9914 - val_loss: 0.1635 - val_acc: 0.9632\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0164 - acc: 0.9949\n",
      "Epoch 00080: val_loss did not improve from 0.15350\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0165 - acc: 0.9949 - val_loss: 0.1900 - val_acc: 0.9595\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0238 - acc: 0.9926\n",
      "Epoch 00081: val_loss did not improve from 0.15350\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0238 - acc: 0.9926 - val_loss: 0.2053 - val_acc: 0.9585\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0161 - acc: 0.9957\n",
      "Epoch 00082: val_loss did not improve from 0.15350\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0161 - acc: 0.9957 - val_loss: 0.2198 - val_acc: 0.9557\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0191 - acc: 0.9942\n",
      "Epoch 00083: val_loss did not improve from 0.15350\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0191 - acc: 0.9942 - val_loss: 0.2134 - val_acc: 0.9522\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0182 - acc: 0.9942\n",
      "Epoch 00084: val_loss did not improve from 0.15350\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0182 - acc: 0.9942 - val_loss: 0.2310 - val_acc: 0.9532\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0187 - acc: 0.9942\n",
      "Epoch 00085: val_loss did not improve from 0.15350\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0187 - acc: 0.9942 - val_loss: 0.2389 - val_acc: 0.9441\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0202 - acc: 0.9936\n",
      "Epoch 00086: val_loss did not improve from 0.15350\n",
      "36805/36805 [==============================] - 56s 2ms/sample - loss: 0.0203 - acc: 0.9936 - val_loss: 0.2392 - val_acc: 0.9509\n",
      "\n",
      "1D_CNN_custom_multi_2_concat_DO_BN_7_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 706us/sample - loss: 0.1916 - acc: 0.9445\n",
      "Loss: 0.1915753168960103 Accuracy: 0.9445483\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.0626 - acc: 0.3874\n",
      "Epoch 00001: val_loss improved from inf to 1.43547, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_BN_8_conv_checkpoint/001-1.4355.hdf5\n",
      "36805/36805 [==============================] - 72s 2ms/sample - loss: 2.0627 - acc: 0.3874 - val_loss: 1.4355 - val_acc: 0.5304\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0009 - acc: 0.6858\n",
      "Epoch 00002: val_loss improved from 1.43547 to 0.53513, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_BN_8_conv_checkpoint/002-0.5351.hdf5\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 1.0009 - acc: 0.6858 - val_loss: 0.5351 - val_acc: 0.8453\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6646 - acc: 0.7948\n",
      "Epoch 00003: val_loss improved from 0.53513 to 0.44129, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_BN_8_conv_checkpoint/003-0.4413.hdf5\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.6646 - acc: 0.7948 - val_loss: 0.4413 - val_acc: 0.8635\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5062 - acc: 0.8455\n",
      "Epoch 00004: val_loss improved from 0.44129 to 0.35594, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_BN_8_conv_checkpoint/004-0.3559.hdf5\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.5062 - acc: 0.8455 - val_loss: 0.3559 - val_acc: 0.8884\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4139 - acc: 0.8739\n",
      "Epoch 00005: val_loss improved from 0.35594 to 0.26681, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_BN_8_conv_checkpoint/005-0.2668.hdf5\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.4139 - acc: 0.8739 - val_loss: 0.2668 - val_acc: 0.9222\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3434 - acc: 0.8946\n",
      "Epoch 00006: val_loss improved from 0.26681 to 0.25875, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_BN_8_conv_checkpoint/006-0.2587.hdf5\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.3434 - acc: 0.8946 - val_loss: 0.2587 - val_acc: 0.9255\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3054 - acc: 0.9064\n",
      "Epoch 00007: val_loss improved from 0.25875 to 0.20534, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_BN_8_conv_checkpoint/007-0.2053.hdf5\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.3054 - acc: 0.9064 - val_loss: 0.2053 - val_acc: 0.9364\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2687 - acc: 0.9176\n",
      "Epoch 00008: val_loss did not improve from 0.20534\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.2687 - acc: 0.9176 - val_loss: 0.2111 - val_acc: 0.9378\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2422 - acc: 0.9260\n",
      "Epoch 00009: val_loss did not improve from 0.20534\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.2422 - acc: 0.9260 - val_loss: 0.2061 - val_acc: 0.9387\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2155 - acc: 0.9334\n",
      "Epoch 00010: val_loss improved from 0.20534 to 0.18117, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_BN_8_conv_checkpoint/010-0.1812.hdf5\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.2157 - acc: 0.9334 - val_loss: 0.1812 - val_acc: 0.9450\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2067 - acc: 0.9355\n",
      "Epoch 00011: val_loss did not improve from 0.18117\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.2067 - acc: 0.9355 - val_loss: 0.1849 - val_acc: 0.9413\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1782 - acc: 0.9434\n",
      "Epoch 00012: val_loss improved from 0.18117 to 0.17596, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_BN_8_conv_checkpoint/012-0.1760.hdf5\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.1781 - acc: 0.9434 - val_loss: 0.1760 - val_acc: 0.9469\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1650 - acc: 0.9474\n",
      "Epoch 00013: val_loss improved from 0.17596 to 0.16027, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_BN_8_conv_checkpoint/013-0.1603.hdf5\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.1651 - acc: 0.9474 - val_loss: 0.1603 - val_acc: 0.9525\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1559 - acc: 0.9509\n",
      "Epoch 00014: val_loss improved from 0.16027 to 0.14185, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_BN_8_conv_checkpoint/014-0.1418.hdf5\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.1559 - acc: 0.9509 - val_loss: 0.1418 - val_acc: 0.9546\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1423 - acc: 0.9558\n",
      "Epoch 00015: val_loss did not improve from 0.14185\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.1425 - acc: 0.9557 - val_loss: 0.1570 - val_acc: 0.9513\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1414 - acc: 0.9548\n",
      "Epoch 00016: val_loss improved from 0.14185 to 0.12980, saving model to model/checkpoint/1D_CNN_custom_multi_2_concat_DO_BN_8_conv_checkpoint/016-0.1298.hdf5\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.1414 - acc: 0.9548 - val_loss: 0.1298 - val_acc: 0.9597\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1253 - acc: 0.9611\n",
      "Epoch 00017: val_loss did not improve from 0.12980\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.1253 - acc: 0.9611 - val_loss: 0.1688 - val_acc: 0.9502\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1143 - acc: 0.9641\n",
      "Epoch 00018: val_loss did not improve from 0.12980\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.1145 - acc: 0.9641 - val_loss: 0.2217 - val_acc: 0.9329\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1154 - acc: 0.9630\n",
      "Epoch 00019: val_loss did not improve from 0.12980\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.1155 - acc: 0.9630 - val_loss: 0.1390 - val_acc: 0.9567\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1026 - acc: 0.9679\n",
      "Epoch 00020: val_loss did not improve from 0.12980\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.1026 - acc: 0.9679 - val_loss: 0.1703 - val_acc: 0.9488\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0961 - acc: 0.9692\n",
      "Epoch 00021: val_loss did not improve from 0.12980\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0961 - acc: 0.9692 - val_loss: 0.1465 - val_acc: 0.9562\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0900 - acc: 0.9713\n",
      "Epoch 00022: val_loss did not improve from 0.12980\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0900 - acc: 0.9713 - val_loss: 0.1467 - val_acc: 0.9536\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0821 - acc: 0.9749\n",
      "Epoch 00023: val_loss did not improve from 0.12980\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0820 - acc: 0.9749 - val_loss: 0.1402 - val_acc: 0.9536\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0805 - acc: 0.9751\n",
      "Epoch 00024: val_loss did not improve from 0.12980\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0805 - acc: 0.9751 - val_loss: 0.1686 - val_acc: 0.9495\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0763 - acc: 0.9756\n",
      "Epoch 00025: val_loss did not improve from 0.12980\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0763 - acc: 0.9756 - val_loss: 0.1566 - val_acc: 0.9527\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0698 - acc: 0.9783\n",
      "Epoch 00026: val_loss did not improve from 0.12980\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0698 - acc: 0.9782 - val_loss: 0.1987 - val_acc: 0.9455\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0689 - acc: 0.9779\n",
      "Epoch 00027: val_loss did not improve from 0.12980\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0690 - acc: 0.9778 - val_loss: 0.1346 - val_acc: 0.9599\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0696 - acc: 0.9779\n",
      "Epoch 00028: val_loss did not improve from 0.12980\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0696 - acc: 0.9779 - val_loss: 0.1389 - val_acc: 0.9611\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0577 - acc: 0.9825\n",
      "Epoch 00029: val_loss did not improve from 0.12980\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0577 - acc: 0.9825 - val_loss: 0.1473 - val_acc: 0.9616\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0560 - acc: 0.9826\n",
      "Epoch 00030: val_loss did not improve from 0.12980\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0560 - acc: 0.9826 - val_loss: 0.1526 - val_acc: 0.9595\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0591 - acc: 0.9817\n",
      "Epoch 00031: val_loss did not improve from 0.12980\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0591 - acc: 0.9817 - val_loss: 0.1386 - val_acc: 0.9637\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0487 - acc: 0.9851\n",
      "Epoch 00032: val_loss did not improve from 0.12980\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0487 - acc: 0.9851 - val_loss: 0.1614 - val_acc: 0.9555\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0475 - acc: 0.9848\n",
      "Epoch 00033: val_loss did not improve from 0.12980\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0476 - acc: 0.9848 - val_loss: 0.1731 - val_acc: 0.9536\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0518 - acc: 0.9837\n",
      "Epoch 00034: val_loss did not improve from 0.12980\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0518 - acc: 0.9837 - val_loss: 0.1707 - val_acc: 0.9532\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0435 - acc: 0.9868\n",
      "Epoch 00035: val_loss did not improve from 0.12980\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0435 - acc: 0.9868 - val_loss: 0.1460 - val_acc: 0.9595\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0455 - acc: 0.9860\n",
      "Epoch 00036: val_loss did not improve from 0.12980\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0457 - acc: 0.9860 - val_loss: 0.1331 - val_acc: 0.9604\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0457 - acc: 0.9858\n",
      "Epoch 00037: val_loss did not improve from 0.12980\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0457 - acc: 0.9858 - val_loss: 0.1418 - val_acc: 0.9592\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0378 - acc: 0.9892\n",
      "Epoch 00038: val_loss did not improve from 0.12980\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0379 - acc: 0.9892 - val_loss: 0.1613 - val_acc: 0.9557\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0474 - acc: 0.9849\n",
      "Epoch 00039: val_loss did not improve from 0.12980\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0474 - acc: 0.9849 - val_loss: 0.1379 - val_acc: 0.9602\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0315 - acc: 0.9907\n",
      "Epoch 00040: val_loss did not improve from 0.12980\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0316 - acc: 0.9907 - val_loss: 0.1315 - val_acc: 0.9665\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0372 - acc: 0.9885\n",
      "Epoch 00041: val_loss did not improve from 0.12980\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0373 - acc: 0.9885 - val_loss: 0.1713 - val_acc: 0.9525\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0419 - acc: 0.9868\n",
      "Epoch 00042: val_loss did not improve from 0.12980\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0419 - acc: 0.9868 - val_loss: 0.1851 - val_acc: 0.9509\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0382 - acc: 0.9880\n",
      "Epoch 00043: val_loss did not improve from 0.12980\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0382 - acc: 0.9880 - val_loss: 0.1600 - val_acc: 0.9613\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0298 - acc: 0.9908\n",
      "Epoch 00044: val_loss did not improve from 0.12980\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0298 - acc: 0.9908 - val_loss: 0.1410 - val_acc: 0.9620\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0265 - acc: 0.9921\n",
      "Epoch 00045: val_loss did not improve from 0.12980\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0266 - acc: 0.9920 - val_loss: 0.1975 - val_acc: 0.9495\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0464 - acc: 0.9857\n",
      "Epoch 00046: val_loss did not improve from 0.12980\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0464 - acc: 0.9857 - val_loss: 0.1402 - val_acc: 0.9618\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0258 - acc: 0.9918\n",
      "Epoch 00047: val_loss did not improve from 0.12980\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0258 - acc: 0.9918 - val_loss: 0.1640 - val_acc: 0.9548\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0216 - acc: 0.9937\n",
      "Epoch 00048: val_loss did not improve from 0.12980\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0216 - acc: 0.9937 - val_loss: 0.1489 - val_acc: 0.9637\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0246 - acc: 0.9932\n",
      "Epoch 00049: val_loss did not improve from 0.12980\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0246 - acc: 0.9932 - val_loss: 0.1486 - val_acc: 0.9620\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0263 - acc: 0.9918\n",
      "Epoch 00050: val_loss did not improve from 0.12980\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0265 - acc: 0.9918 - val_loss: 0.1895 - val_acc: 0.9560\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0328 - acc: 0.9899\n",
      "Epoch 00051: val_loss did not improve from 0.12980\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0328 - acc: 0.9899 - val_loss: 0.1386 - val_acc: 0.9632\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0217 - acc: 0.9940\n",
      "Epoch 00052: val_loss did not improve from 0.12980\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0217 - acc: 0.9940 - val_loss: 0.1426 - val_acc: 0.9660\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0244 - acc: 0.9927\n",
      "Epoch 00053: val_loss did not improve from 0.12980\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0245 - acc: 0.9927 - val_loss: 0.2062 - val_acc: 0.9536\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0281 - acc: 0.9916\n",
      "Epoch 00054: val_loss did not improve from 0.12980\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0281 - acc: 0.9916 - val_loss: 0.1438 - val_acc: 0.9623\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0198 - acc: 0.9944\n",
      "Epoch 00055: val_loss did not improve from 0.12980\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0198 - acc: 0.9944 - val_loss: 0.1806 - val_acc: 0.9604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0229 - acc: 0.9930\n",
      "Epoch 00056: val_loss did not improve from 0.12980\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0229 - acc: 0.9930 - val_loss: 0.1665 - val_acc: 0.9620\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0216 - acc: 0.9940\n",
      "Epoch 00057: val_loss did not improve from 0.12980\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0216 - acc: 0.9940 - val_loss: 0.1381 - val_acc: 0.9660\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0201 - acc: 0.9941\n",
      "Epoch 00058: val_loss did not improve from 0.12980\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0203 - acc: 0.9941 - val_loss: 0.1829 - val_acc: 0.9574\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0322 - acc: 0.9901\n",
      "Epoch 00059: val_loss did not improve from 0.12980\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0322 - acc: 0.9901 - val_loss: 0.1720 - val_acc: 0.9562\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0158 - acc: 0.9953\n",
      "Epoch 00060: val_loss did not improve from 0.12980\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0159 - acc: 0.9953 - val_loss: 0.1420 - val_acc: 0.9648\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0293 - acc: 0.9910\n",
      "Epoch 00061: val_loss did not improve from 0.12980\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0294 - acc: 0.9910 - val_loss: 0.1406 - val_acc: 0.9660\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0218 - acc: 0.9931\n",
      "Epoch 00062: val_loss did not improve from 0.12980\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0218 - acc: 0.9931 - val_loss: 0.1647 - val_acc: 0.9630\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0150 - acc: 0.9958\n",
      "Epoch 00063: val_loss did not improve from 0.12980\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0150 - acc: 0.9958 - val_loss: 0.1414 - val_acc: 0.9655\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0165 - acc: 0.9950\n",
      "Epoch 00064: val_loss did not improve from 0.12980\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0165 - acc: 0.9950 - val_loss: 0.1407 - val_acc: 0.9660\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0165 - acc: 0.9953\n",
      "Epoch 00065: val_loss did not improve from 0.12980\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0165 - acc: 0.9953 - val_loss: 0.1556 - val_acc: 0.9644\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0149 - acc: 0.9957\n",
      "Epoch 00066: val_loss did not improve from 0.12980\n",
      "36805/36805 [==============================] - 58s 2ms/sample - loss: 0.0149 - acc: 0.9957 - val_loss: 0.1869 - val_acc: 0.9546\n",
      "\n",
      "1D_CNN_custom_multi_2_concat_DO_BN_8_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 744us/sample - loss: 0.1814 - acc: 0.9441\n",
      "Loss: 0.1813951828181434 Accuracy: 0.9441329\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base = '1D_CNN_custom_multi_2_concat_DO_BN'\n",
    "\n",
    "for i in range(3, 9):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    model = build_cnn(conv_num=i)\n",
    "#         model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4),\n",
    "          metrics=['accuracy'])\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model_filename = model_path+'{epoch:03d}-{val_loss:.4f}.hdf5'\n",
    "    checkpointer = ModelCheckpoint(filepath = model_filename, monitor = \"val_loss\", \n",
    "                                   verbose=1, save_best_only=True)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=50)\n",
    "    hist = model.fit(x_train_abs, y_train_onehot, batch_size=64, epochs=500, \n",
    "                     validation_data=[x_val_abs, y_val_onehot], shuffle=True, \n",
    "                     callbacks = [checkpointer, early_stopping])\n",
    "\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "    ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "    ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "    ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    png_path = 'visualization/learning_curve/'\n",
    "    filename = model_name+'.png'\n",
    "    os.makedirs(png_path, exist_ok=True)\n",
    "    fig.savefig(png_path+filename, transparent=True)\n",
    "\n",
    "    model.save(model_path+'000_last.hdf5')\n",
    "    del(model)\n",
    "    \n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "    model = load_model(model_filename)\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "    print()\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_multi_2_concat_DO_BN_3_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_33 (Conv1D)              (None, 16000, 64)    384         input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_33 (Batc (None, 16000, 64)    256         conv1d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_33[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling1D) (None, 5333, 64)     0           activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_34 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_33[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_34 (Batc (None, 5333, 64)     256         conv1d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_34[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling1D) (None, 1777, 64)     0           activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_35 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_35 (Batc (None, 1777, 64)     256         conv1d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_35[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling1D) (None, 592, 64)      0           activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_12 (Flatten)            (None, 113728)       0           max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_13 (Flatten)            (None, 37888)        0           max_pooling1d_35[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 151616)       0           flatten_12[0][0]                 \n",
      "                                                                 flatten_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 151616)       0           concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 16)           2425872     dropout_6[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 2,468,112\n",
      "Trainable params: 2,467,728\n",
      "Non-trainable params: 384\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 688us/sample - loss: 1.4145 - acc: 0.5699\n",
      "Loss: 1.414538485634983 Accuracy: 0.5698858\n",
      "\n",
      "1D_CNN_custom_multi_2_concat_DO_BN_4_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_8 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_36 (Conv1D)              (None, 16000, 64)    384         input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_36 (Batc (None, 16000, 64)    256         conv1d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_36[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling1D) (None, 5333, 64)     0           activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_37 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_36[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_37 (Batc (None, 5333, 64)     256         conv1d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_37[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling1D) (None, 1777, 64)     0           activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_38 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_37[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_38 (Batc (None, 1777, 64)     256         conv1d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_38[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling1D) (None, 592, 64)      0           activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_39 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_39 (Batc (None, 592, 64)      256         conv1d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 592, 64)      0           batch_normalization_v1_39[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling1D) (None, 197, 64)      0           activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_14 (Flatten)            (None, 37888)        0           max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_15 (Flatten)            (None, 12608)        0           max_pooling1d_39[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 50496)        0           flatten_14[0][0]                 \n",
      "                                                                 flatten_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 50496)        0           concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 16)           807952      dropout_7[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 870,992\n",
      "Trainable params: 870,480\n",
      "Non-trainable params: 512\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 707us/sample - loss: 0.9842 - acc: 0.7161\n",
      "Loss: 0.9842279992742331 Accuracy: 0.7160955\n",
      "\n",
      "1D_CNN_custom_multi_2_concat_DO_BN_5_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_40 (Conv1D)              (None, 16000, 64)    384         input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_40 (Batc (None, 16000, 64)    256         conv1d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_40[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling1D) (None, 5333, 64)     0           activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_41 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_40[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_41 (Batc (None, 5333, 64)     256         conv1d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_41[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling1D) (None, 1777, 64)     0           activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_42 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_41[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_42 (Batc (None, 1777, 64)     256         conv1d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_42[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling1D) (None, 592, 64)      0           activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_43 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_42[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_43 (Batc (None, 592, 64)      256         conv1d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 592, 64)      0           batch_normalization_v1_43[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling1D) (None, 197, 64)      0           activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_44 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_44 (Batc (None, 197, 128)     512         conv1d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 197, 128)     0           batch_normalization_v1_44[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling1D) (None, 65, 128)      0           activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_16 (Flatten)            (None, 12608)        0           max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_17 (Flatten)            (None, 8320)         0           max_pooling1d_44[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 20928)        0           flatten_16[0][0]                 \n",
      "                                                                 flatten_17[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 20928)        0           concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 16)           334864      dropout_8[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 439,504\n",
      "Trainable params: 438,736\n",
      "Non-trainable params: 768\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 733us/sample - loss: 0.5304 - acc: 0.8577\n",
      "Loss: 0.5303774013202386 Accuracy: 0.85773623\n",
      "\n",
      "1D_CNN_custom_multi_2_concat_DO_BN_6_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_10 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_45 (Conv1D)              (None, 16000, 64)    384         input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_45 (Batc (None, 16000, 64)    256         conv1d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_45[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling1D) (None, 5333, 64)     0           activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_46 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_45[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_46 (Batc (None, 5333, 64)     256         conv1d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_46[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling1D) (None, 1777, 64)     0           activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_47 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_46[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_47 (Batc (None, 1777, 64)     256         conv1d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_47[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling1D) (None, 592, 64)      0           activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_48 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_47[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_48 (Batc (None, 592, 64)      256         conv1d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 592, 64)      0           batch_normalization_v1_48[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling1D) (None, 197, 64)      0           activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_49 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_48[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_49 (Batc (None, 197, 128)     512         conv1d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 197, 128)     0           batch_normalization_v1_49[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling1D) (None, 65, 128)      0           activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_50 (Conv1D)              (None, 65, 128)      82048       max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_50 (Batc (None, 65, 128)      512         conv1d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 65, 128)      0           batch_normalization_v1_50[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling1D) (None, 21, 128)      0           activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_18 (Flatten)            (None, 8320)         0           max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_19 (Flatten)            (None, 2688)         0           max_pooling1d_50[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 11008)        0           flatten_18[0][0]                 \n",
      "                                                                 flatten_19[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 11008)        0           concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 16)           176144      dropout_9[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 363,344\n",
      "Trainable params: 362,320\n",
      "Non-trainable params: 1,024\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 767us/sample - loss: 0.2986 - acc: 0.9213\n",
      "Loss: 0.29863198925030193 Accuracy: 0.92128766\n",
      "\n",
      "1D_CNN_custom_multi_2_concat_DO_BN_7_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_51 (Conv1D)              (None, 16000, 64)    384         input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_51 (Batc (None, 16000, 64)    256         conv1d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_51[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling1D) (None, 5333, 64)     0           activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_52 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_51[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_52 (Batc (None, 5333, 64)     256         conv1d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_52[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling1D) (None, 1777, 64)     0           activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_53 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_52[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_53 (Batc (None, 1777, 64)     256         conv1d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_53[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling1D) (None, 592, 64)      0           activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_54 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_53[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_54 (Batc (None, 592, 64)      256         conv1d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 592, 64)      0           batch_normalization_v1_54[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling1D) (None, 197, 64)      0           activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_55 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_54[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_55 (Batc (None, 197, 128)     512         conv1d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 197, 128)     0           batch_normalization_v1_55[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling1D) (None, 65, 128)      0           activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_56 (Conv1D)              (None, 65, 128)      82048       max_pooling1d_55[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_56 (Batc (None, 65, 128)      512         conv1d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 65, 128)      0           batch_normalization_v1_56[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling1D) (None, 21, 128)      0           activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_57 (Conv1D)              (None, 21, 128)      82048       max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_57 (Batc (None, 21, 128)      512         conv1d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 21, 128)      0           batch_normalization_v1_57[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling1D) (None, 7, 128)       0           activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_20 (Flatten)            (None, 2688)         0           max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_21 (Flatten)            (None, 896)          0           max_pooling1d_57[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 3584)         0           flatten_20[0][0]                 \n",
      "                                                                 flatten_21[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 3584)         0           concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 16)           57360       dropout_10[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 327,120\n",
      "Trainable params: 325,840\n",
      "Non-trainable params: 1,280\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 785us/sample - loss: 0.1916 - acc: 0.9445\n",
      "Loss: 0.1915753168960103 Accuracy: 0.9445483\n",
      "\n",
      "1D_CNN_custom_multi_2_concat_DO_BN_8_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_12 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_58 (Conv1D)              (None, 16000, 64)    384         input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_58 (Batc (None, 16000, 64)    256         conv1d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_58[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling1D) (None, 5333, 64)     0           activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_59 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_58[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_59 (Batc (None, 5333, 64)     256         conv1d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_59[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling1D) (None, 1777, 64)     0           activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_60 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_59[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_60 (Batc (None, 1777, 64)     256         conv1d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_60[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling1D) (None, 592, 64)      0           activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_61 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_60[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_61 (Batc (None, 592, 64)      256         conv1d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 592, 64)      0           batch_normalization_v1_61[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling1D) (None, 197, 64)      0           activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_62 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_61[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_62 (Batc (None, 197, 128)     512         conv1d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 197, 128)     0           batch_normalization_v1_62[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling1D) (None, 65, 128)      0           activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_63 (Conv1D)              (None, 65, 128)      82048       max_pooling1d_62[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_63 (Batc (None, 65, 128)      512         conv1d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 65, 128)      0           batch_normalization_v1_63[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling1D) (None, 21, 128)      0           activation_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_64 (Conv1D)              (None, 21, 128)      82048       max_pooling1d_63[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_64 (Batc (None, 21, 128)      512         conv1d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 21, 128)      0           batch_normalization_v1_64[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling1D) (None, 7, 128)       0           activation_64[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_65 (Conv1D)              (None, 7, 128)       82048       max_pooling1d_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_65 (Batc (None, 7, 128)       512         conv1d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 7, 128)       0           batch_normalization_v1_65[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling1D) (None, 2, 128)       0           activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_22 (Flatten)            (None, 896)          0           max_pooling1d_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_23 (Flatten)            (None, 256)          0           max_pooling1d_65[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 1152)         0           flatten_22[0][0]                 \n",
      "                                                                 flatten_23[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 1152)         0           concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 16)           18448       dropout_11[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 370,768\n",
      "Trainable params: 369,232\n",
      "Non-trainable params: 1,536\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 804us/sample - loss: 0.1814 - acc: 0.9441\n",
      "Loss: 0.1813951828181434 Accuracy: 0.9441329\n"
     ]
    }
   ],
   "source": [
    "log_dir = 'log'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "base = '1D_CNN_custom_multi_2_concat_DO_BN'\n",
    "\n",
    "with open(path.join(log_dir, base), 'w') as log_file:\n",
    "    for i in range(3, 9):\n",
    "        model_name = base+'_{}_conv'.format(i)\n",
    "        print()\n",
    "        print(model_name, 'Model')\n",
    "        model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "        model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "\n",
    "        model = load_model(model_filename)\n",
    "        model.summary()\n",
    "\n",
    "        [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "        print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "        del(model)\n",
    "\n",
    "        log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_multi_2_concat_DO_BN_3_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_33 (Conv1D)              (None, 16000, 64)    384         input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_33 (Batc (None, 16000, 64)    256         conv1d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_33[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling1D) (None, 5333, 64)     0           activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_34 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_33[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_34 (Batc (None, 5333, 64)     256         conv1d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_34[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling1D) (None, 1777, 64)     0           activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_35 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_35 (Batc (None, 1777, 64)     256         conv1d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_35[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling1D) (None, 592, 64)      0           activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_12 (Flatten)            (None, 113728)       0           max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_13 (Flatten)            (None, 37888)        0           max_pooling1d_35[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 151616)       0           flatten_12[0][0]                 \n",
      "                                                                 flatten_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 151616)       0           concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 16)           2425872     dropout_6[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 2,468,112\n",
      "Trainable params: 2,467,728\n",
      "Non-trainable params: 384\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 785us/sample - loss: 2.6926 - acc: 0.6137\n",
      "Loss: 2.6926016541656304 Accuracy: 0.6137072\n",
      "\n",
      "1D_CNN_custom_multi_2_concat_DO_BN_4_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_8 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_36 (Conv1D)              (None, 16000, 64)    384         input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_36 (Batc (None, 16000, 64)    256         conv1d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_36[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling1D) (None, 5333, 64)     0           activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_37 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_36[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_37 (Batc (None, 5333, 64)     256         conv1d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_37[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling1D) (None, 1777, 64)     0           activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_38 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_37[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_38 (Batc (None, 1777, 64)     256         conv1d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_38[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling1D) (None, 592, 64)      0           activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_39 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_39 (Batc (None, 592, 64)      256         conv1d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 592, 64)      0           batch_normalization_v1_39[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling1D) (None, 197, 64)      0           activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_14 (Flatten)            (None, 37888)        0           max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_15 (Flatten)            (None, 12608)        0           max_pooling1d_39[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 50496)        0           flatten_14[0][0]                 \n",
      "                                                                 flatten_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 50496)        0           concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 16)           807952      dropout_7[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 870,992\n",
      "Trainable params: 870,480\n",
      "Non-trainable params: 512\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 801us/sample - loss: 1.1726 - acc: 0.7572\n",
      "Loss: 1.1725702812738508 Accuracy: 0.75721705\n",
      "\n",
      "1D_CNN_custom_multi_2_concat_DO_BN_5_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_40 (Conv1D)              (None, 16000, 64)    384         input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_40 (Batc (None, 16000, 64)    256         conv1d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_40[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling1D) (None, 5333, 64)     0           activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_41 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_40[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_41 (Batc (None, 5333, 64)     256         conv1d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_41[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling1D) (None, 1777, 64)     0           activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_42 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_41[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_42 (Batc (None, 1777, 64)     256         conv1d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_42[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling1D) (None, 592, 64)      0           activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_43 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_42[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_43 (Batc (None, 592, 64)      256         conv1d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 592, 64)      0           batch_normalization_v1_43[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling1D) (None, 197, 64)      0           activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_44 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_44 (Batc (None, 197, 128)     512         conv1d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 197, 128)     0           batch_normalization_v1_44[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling1D) (None, 65, 128)      0           activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_16 (Flatten)            (None, 12608)        0           max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_17 (Flatten)            (None, 8320)         0           max_pooling1d_44[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 20928)        0           flatten_16[0][0]                 \n",
      "                                                                 flatten_17[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 20928)        0           concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 16)           334864      dropout_8[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 439,504\n",
      "Trainable params: 438,736\n",
      "Non-trainable params: 768\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 808us/sample - loss: 0.6140 - acc: 0.8615\n",
      "Loss: 0.6139808005633013 Accuracy: 0.8614746\n",
      "\n",
      "1D_CNN_custom_multi_2_concat_DO_BN_6_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_10 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_45 (Conv1D)              (None, 16000, 64)    384         input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_45 (Batc (None, 16000, 64)    256         conv1d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_45[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling1D) (None, 5333, 64)     0           activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_46 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_45[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_46 (Batc (None, 5333, 64)     256         conv1d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_46[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling1D) (None, 1777, 64)     0           activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_47 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_46[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_47 (Batc (None, 1777, 64)     256         conv1d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_47[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling1D) (None, 592, 64)      0           activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_48 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_47[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_48 (Batc (None, 592, 64)      256         conv1d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 592, 64)      0           batch_normalization_v1_48[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling1D) (None, 197, 64)      0           activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_49 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_48[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_49 (Batc (None, 197, 128)     512         conv1d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 197, 128)     0           batch_normalization_v1_49[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling1D) (None, 65, 128)      0           activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_50 (Conv1D)              (None, 65, 128)      82048       max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_50 (Batc (None, 65, 128)      512         conv1d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 65, 128)      0           batch_normalization_v1_50[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling1D) (None, 21, 128)      0           activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_18 (Flatten)            (None, 8320)         0           max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_19 (Flatten)            (None, 2688)         0           max_pooling1d_50[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 11008)        0           flatten_18[0][0]                 \n",
      "                                                                 flatten_19[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 11008)        0           concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 16)           176144      dropout_9[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 363,344\n",
      "Trainable params: 362,320\n",
      "Non-trainable params: 1,024\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 851us/sample - loss: 0.7012 - acc: 0.8712\n",
      "Loss: 0.7012080360486376 Accuracy: 0.8712357\n",
      "\n",
      "1D_CNN_custom_multi_2_concat_DO_BN_7_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_51 (Conv1D)              (None, 16000, 64)    384         input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_51 (Batc (None, 16000, 64)    256         conv1d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_51[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling1D) (None, 5333, 64)     0           activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_52 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_51[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_52 (Batc (None, 5333, 64)     256         conv1d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_52[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling1D) (None, 1777, 64)     0           activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_53 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_52[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_53 (Batc (None, 1777, 64)     256         conv1d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_53[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling1D) (None, 592, 64)      0           activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_54 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_53[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_54 (Batc (None, 592, 64)      256         conv1d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 592, 64)      0           batch_normalization_v1_54[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling1D) (None, 197, 64)      0           activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_55 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_54[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_55 (Batc (None, 197, 128)     512         conv1d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 197, 128)     0           batch_normalization_v1_55[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling1D) (None, 65, 128)      0           activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_56 (Conv1D)              (None, 65, 128)      82048       max_pooling1d_55[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_56 (Batc (None, 65, 128)      512         conv1d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 65, 128)      0           batch_normalization_v1_56[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling1D) (None, 21, 128)      0           activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_57 (Conv1D)              (None, 21, 128)      82048       max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_57 (Batc (None, 21, 128)      512         conv1d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 21, 128)      0           batch_normalization_v1_57[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling1D) (None, 7, 128)       0           activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_20 (Flatten)            (None, 2688)         0           max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_21 (Flatten)            (None, 896)          0           max_pooling1d_57[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 3584)         0           flatten_20[0][0]                 \n",
      "                                                                 flatten_21[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 3584)         0           concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 16)           57360       dropout_10[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 327,120\n",
      "Trainable params: 325,840\n",
      "Non-trainable params: 1,280\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 4s 921us/sample - loss: 0.2797 - acc: 0.9396\n",
      "Loss: 0.2796869198765571 Accuracy: 0.9395639\n",
      "\n",
      "1D_CNN_custom_multi_2_concat_DO_BN_8_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_12 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_58 (Conv1D)              (None, 16000, 64)    384         input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_58 (Batc (None, 16000, 64)    256         conv1d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 16000, 64)    0           batch_normalization_v1_58[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling1D) (None, 5333, 64)     0           activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_59 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_58[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_59 (Batc (None, 5333, 64)     256         conv1d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 5333, 64)     0           batch_normalization_v1_59[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling1D) (None, 1777, 64)     0           activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_60 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_59[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_60 (Batc (None, 1777, 64)     256         conv1d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 1777, 64)     0           batch_normalization_v1_60[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling1D) (None, 592, 64)      0           activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_61 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_60[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_61 (Batc (None, 592, 64)      256         conv1d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 592, 64)      0           batch_normalization_v1_61[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling1D) (None, 197, 64)      0           activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_62 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_61[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_62 (Batc (None, 197, 128)     512         conv1d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 197, 128)     0           batch_normalization_v1_62[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling1D) (None, 65, 128)      0           activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_63 (Conv1D)              (None, 65, 128)      82048       max_pooling1d_62[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_63 (Batc (None, 65, 128)      512         conv1d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 65, 128)      0           batch_normalization_v1_63[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling1D) (None, 21, 128)      0           activation_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_64 (Conv1D)              (None, 21, 128)      82048       max_pooling1d_63[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_64 (Batc (None, 21, 128)      512         conv1d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 21, 128)      0           batch_normalization_v1_64[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling1D) (None, 7, 128)       0           activation_64[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_65 (Conv1D)              (None, 7, 128)       82048       max_pooling1d_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_65 (Batc (None, 7, 128)       512         conv1d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 7, 128)       0           batch_normalization_v1_65[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling1D) (None, 2, 128)       0           activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_22 (Flatten)            (None, 896)          0           max_pooling1d_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_23 (Flatten)            (None, 256)          0           max_pooling1d_65[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 1152)         0           flatten_22[0][0]                 \n",
      "                                                                 flatten_23[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 1152)         0           concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 16)           18448       dropout_11[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 370,768\n",
      "Trainable params: 369,232\n",
      "Non-trainable params: 1,536\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 4s 891us/sample - loss: 0.2629 - acc: 0.9421\n",
      "Loss: 0.2629415217754146 Accuracy: 0.94205606\n"
     ]
    }
   ],
   "source": [
    "# log_dir = 'log'\n",
    "# os.makedirs(log_dir, exist_ok=True)\n",
    "# base = '1D_CNN_custom_DO_BN'\n",
    "\n",
    "# with open(path.join(log_dir, base), 'w') as log_file:\n",
    "for i in range(3, 9):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + '000_last.hdf5'\n",
    "\n",
    "    model = load_model(model_filename)\n",
    "    model.summary()\n",
    "\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "    del(model)\n",
    "\n",
    "#         log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
