{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path as path\n",
    "import itertools\n",
    "from sklearn.preprocessing import maxabs_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Input, Dense, Activation, BatchNormalization, \\\n",
    "                                    Flatten, Conv1D, MaxPooling1D, Dropout, \\\n",
    "                                    Concatenate, GlobalMaxPool1D, GlobalAvgPool1D\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler, \\\n",
    "                                        EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = path.join('..', 'data')\n",
    "data_dir = path.join(base_dir, 'data_speech_commands_v0.02')\n",
    " \n",
    "train_txt = path.join(data_dir, 'wav_train_16words.txt')\n",
    "val_txt = path.join(data_dir, 'wav_validation_16words.txt')\n",
    "test_txt = path.join(data_dir, 'wav_test_16words.txt')\n",
    "\n",
    "train_data = np.load(path.join(data_dir, 'wav_train_data.npz'))\n",
    "val_data = np.load(path.join(data_dir, 'wav_validation_data.npz'))\n",
    "test_data = np.load(path.join(data_dir, 'wav_test_data.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36805, 16000, 1),\n",
       " (36805,),\n",
       " (4293, 16000, 1),\n",
       " (4293,),\n",
       " (4815, 16000, 1),\n",
       " (4815,),\n",
       " (16, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = train_data['x_train']\n",
    "y_train = train_data['y_train']\n",
    "x_val = val_data['x_val']\n",
    "y_val = val_data['y_val']\n",
    "x_test = test_data['x_test']\n",
    "y_test = test_data['y_test']\n",
    "y_table = test_data['table']\n",
    "\n",
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape, y_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_test[0].shape\n",
    "output_size = y_table.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_abs = np.asarray([maxabs_scale(wav) for wav in x_train])\n",
    "y_train_onehot = np.asarray([to_categorical(label, output_size) for label in y_train])\n",
    "del x_train, y_train\n",
    "\n",
    "x_val_abs = np.asarray([maxabs_scale(wav) for wav in x_val])\n",
    "y_val_onehot = np.asarray([to_categorical(label, output_size) for label in y_val])\n",
    "del x_val, y_val\n",
    "\n",
    "x_test_abs = np.asarray([maxabs_scale(wav) for wav in x_test])\n",
    "y_test_onehot = np.asarray([to_categorical(label, output_size) for label in y_test])\n",
    "del x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn(conv_num=1):\n",
    "    filter_size = 64\n",
    "\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    x = input_layer\n",
    "\n",
    "    layer_outputs = []\n",
    "    for i in range(conv_num):\n",
    "        x = Conv1D (kernel_size=5, filters=filter_size*(2**(i//4)), \n",
    "                          strides=1, padding='same')(x)\n",
    "        x = Activation('relu')(x)\n",
    "        x = MaxPooling1D(pool_size=3, strides=3)(x)\n",
    "        layer_outputs.append(x)    \n",
    "    \n",
    "    x = Concatenate()([GlobalMaxPool1D()(output) for output in layer_outputs[-2:]])\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(output_size, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 16000, 64)    384         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 16000, 64)    0           conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D)    (None, 5333, 64)     0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 5333, 64)     20544       max_pooling1d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 5333, 64)     0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 1777, 64)     0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 1777, 64)     20544       max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 1777, 64)     0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 592, 64)      0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d (GlobalMax (None, 64)           0           max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 64)           0           max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 128)          0           global_max_pooling1d[0][0]       \n",
      "                                                                 global_max_pooling1d_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 128)          0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 16)           2064        dropout[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 43,536\n",
      "Trainable params: 43,536\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 16000, 64)    384         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 16000, 64)    0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 5333, 64)     0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 5333, 64)     20544       max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 5333, 64)     0           conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 1777, 64)     0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 1777, 64)     20544       max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 1777, 64)     0           conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 592, 64)      0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 592, 64)      20544       max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 592, 64)      0           conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1D)  (None, 197, 64)      0           activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_2 (GlobalM (None, 64)           0           max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_3 (GlobalM (None, 64)           0           max_pooling1d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 128)          0           global_max_pooling1d_2[0][0]     \n",
      "                                                                 global_max_pooling1d_3[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 128)          0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 16)           2064        dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 64,080\n",
      "Trainable params: 64,080\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 16000, 64)    384         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 16000, 64)    0           conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1D)  (None, 5333, 64)     0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 5333, 64)     20544       max_pooling1d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 5333, 64)     0           conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1D)  (None, 1777, 64)     0           activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 1777, 64)     20544       max_pooling1d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 1777, 64)     0           conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1D)  (None, 592, 64)      0           activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 592, 64)      0           conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling1D) (None, 197, 64)      0           activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 197, 128)     0           conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling1D) (None, 65, 128)      0           activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_4 (GlobalM (None, 64)           0           max_pooling1d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_5 (GlobalM (None, 128)          0           max_pooling1d_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 192)          0           global_max_pooling1d_4[0][0]     \n",
      "                                                                 global_max_pooling1d_5[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 192)          0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 16)           3088        dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 106,192\n",
      "Trainable params: 106,192\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 16000, 64)    384         input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 16000, 64)    0           conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling1D) (None, 5333, 64)     0           activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 5333, 64)     0           conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling1D) (None, 1777, 64)     0           activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_13[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 1777, 64)     0           conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling1D) (None, 592, 64)      0           activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_14[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 592, 64)      0           conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling1D) (None, 197, 64)      0           activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_15[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 197, 128)     0           conv1d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling1D) (None, 65, 128)      0           activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_17 (Conv1D)              (None, 65, 128)      82048       max_pooling1d_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 65, 128)      0           conv1d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling1D) (None, 21, 128)      0           activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_6 (GlobalM (None, 128)          0           max_pooling1d_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_7 (GlobalM (None, 128)          0           max_pooling1d_17[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 256)          0           global_max_pooling1d_6[0][0]     \n",
      "                                                                 global_max_pooling1d_7[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 256)          0           concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 16)           4112        dropout_3[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 189,264\n",
      "Trainable params: 189,264\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_18 (Conv1D)              (None, 16000, 64)    384         input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 16000, 64)    0           conv1d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling1D) (None, 5333, 64)     0           activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_19 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_18[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 5333, 64)     0           conv1d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling1D) (None, 1777, 64)     0           activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_20 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_19[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 1777, 64)     0           conv1d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling1D) (None, 592, 64)      0           activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_21 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_20[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 592, 64)      0           conv1d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling1D) (None, 197, 64)      0           activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_22 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_21[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 197, 128)     0           conv1d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling1D) (None, 65, 128)      0           activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_23 (Conv1D)              (None, 65, 128)      82048       max_pooling1d_22[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 65, 128)      0           conv1d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling1D) (None, 21, 128)      0           activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_24 (Conv1D)              (None, 21, 128)      82048       max_pooling1d_23[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 21, 128)      0           conv1d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling1D) (None, 7, 128)       0           activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_8 (GlobalM (None, 128)          0           max_pooling1d_23[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_9 (GlobalM (None, 128)          0           max_pooling1d_24[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 256)          0           global_max_pooling1d_8[0][0]     \n",
      "                                                                 global_max_pooling1d_9[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 256)          0           concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 16)           4112        dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 271,312\n",
      "Trainable params: 271,312\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_25 (Conv1D)              (None, 16000, 64)    384         input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 16000, 64)    0           conv1d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling1D) (None, 5333, 64)     0           activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_26 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_25[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 5333, 64)     0           conv1d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_26 (MaxPooling1D) (None, 1777, 64)     0           activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_27 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_26[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 1777, 64)     0           conv1d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_27 (MaxPooling1D) (None, 592, 64)      0           activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_28 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_27[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 592, 64)      0           conv1d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_28 (MaxPooling1D) (None, 197, 64)      0           activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_29 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_28[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 197, 128)     0           conv1d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_29 (MaxPooling1D) (None, 65, 128)      0           activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_30 (Conv1D)              (None, 65, 128)      82048       max_pooling1d_29[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 65, 128)      0           conv1d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_30 (MaxPooling1D) (None, 21, 128)      0           activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_31 (Conv1D)              (None, 21, 128)      82048       max_pooling1d_30[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 21, 128)      0           conv1d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_31 (MaxPooling1D) (None, 7, 128)       0           activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_32 (Conv1D)              (None, 7, 128)       82048       max_pooling1d_31[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 7, 128)       0           conv1d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_32 (MaxPooling1D) (None, 2, 128)       0           activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_10 (Global (None, 128)          0           max_pooling1d_31[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_11 (Global (None, 128)          0           max_pooling1d_32[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 256)          0           global_max_pooling1d_10[0][0]    \n",
      "                                                                 global_max_pooling1d_11[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 256)          0           concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 16)           4112        dropout_5[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 353,360\n",
      "Trainable params: 353,360\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in range(3, 9):\n",
    "    model = build_cnn(conv_num=i)\n",
    "    model.summary()\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36805 samples, validate on 4293 samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.6280 - acc: 0.1472\n",
      "Epoch 00001: val_loss improved from inf to 2.42892, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_3_conv_checkpoint/001-2.4289.hdf5\n",
      "36805/36805 [==============================] - 30s 824us/sample - loss: 2.6279 - acc: 0.1473 - val_loss: 2.4289 - val_acc: 0.2541\n",
      "Epoch 2/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 2.3702 - acc: 0.2209\n",
      "Epoch 00002: val_loss improved from 2.42892 to 2.15127, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_3_conv_checkpoint/002-2.1513.hdf5\n",
      "36805/36805 [==============================] - 29s 777us/sample - loss: 2.3701 - acc: 0.2209 - val_loss: 2.1513 - val_acc: 0.3170\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.2093 - acc: 0.2635\n",
      "Epoch 00003: val_loss improved from 2.15127 to 1.99141, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_3_conv_checkpoint/003-1.9914.hdf5\n",
      "36805/36805 [==============================] - 29s 776us/sample - loss: 2.2094 - acc: 0.2634 - val_loss: 1.9914 - val_acc: 0.3827\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.0929 - acc: 0.2977\n",
      "Epoch 00004: val_loss improved from 1.99141 to 1.85394, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_3_conv_checkpoint/004-1.8539.hdf5\n",
      "36805/36805 [==============================] - 29s 776us/sample - loss: 2.0929 - acc: 0.2977 - val_loss: 1.8539 - val_acc: 0.4307\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.9954 - acc: 0.3304\n",
      "Epoch 00005: val_loss improved from 1.85394 to 1.75107, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_3_conv_checkpoint/005-1.7511.hdf5\n",
      "36805/36805 [==============================] - 29s 776us/sample - loss: 1.9954 - acc: 0.3304 - val_loss: 1.7511 - val_acc: 0.4596\n",
      "Epoch 6/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.9179 - acc: 0.3552\n",
      "Epoch 00006: val_loss improved from 1.75107 to 1.66123, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_3_conv_checkpoint/006-1.6612.hdf5\n",
      "36805/36805 [==============================] - 29s 780us/sample - loss: 1.9180 - acc: 0.3552 - val_loss: 1.6612 - val_acc: 0.4896\n",
      "Epoch 7/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.8535 - acc: 0.3766\n",
      "Epoch 00007: val_loss improved from 1.66123 to 1.58813, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_3_conv_checkpoint/007-1.5881.hdf5\n",
      "36805/36805 [==============================] - 29s 778us/sample - loss: 1.8532 - acc: 0.3766 - val_loss: 1.5881 - val_acc: 0.5083\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.7859 - acc: 0.4017\n",
      "Epoch 00008: val_loss improved from 1.58813 to 1.51589, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_3_conv_checkpoint/008-1.5159.hdf5\n",
      "36805/36805 [==============================] - 29s 778us/sample - loss: 1.7860 - acc: 0.4016 - val_loss: 1.5159 - val_acc: 0.5274\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.7345 - acc: 0.4165\n",
      "Epoch 00009: val_loss improved from 1.51589 to 1.46323, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_3_conv_checkpoint/009-1.4632.hdf5\n",
      "36805/36805 [==============================] - 28s 774us/sample - loss: 1.7345 - acc: 0.4165 - val_loss: 1.4632 - val_acc: 0.5362\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6932 - acc: 0.4299\n",
      "Epoch 00010: val_loss improved from 1.46323 to 1.40994, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_3_conv_checkpoint/010-1.4099.hdf5\n",
      "36805/36805 [==============================] - 29s 777us/sample - loss: 1.6932 - acc: 0.4299 - val_loss: 1.4099 - val_acc: 0.5656\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6473 - acc: 0.4462\n",
      "Epoch 00011: val_loss improved from 1.40994 to 1.36158, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_3_conv_checkpoint/011-1.3616.hdf5\n",
      "36805/36805 [==============================] - 29s 780us/sample - loss: 1.6473 - acc: 0.4462 - val_loss: 1.3616 - val_acc: 0.5651\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.6060 - acc: 0.4613\n",
      "Epoch 00012: val_loss improved from 1.36158 to 1.31764, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_3_conv_checkpoint/012-1.3176.hdf5\n",
      "36805/36805 [==============================] - 29s 776us/sample - loss: 1.6060 - acc: 0.4613 - val_loss: 1.3176 - val_acc: 0.5856\n",
      "Epoch 13/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.5747 - acc: 0.4753\n",
      "Epoch 00013: val_loss improved from 1.31764 to 1.28397, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_3_conv_checkpoint/013-1.2840.hdf5\n",
      "36805/36805 [==============================] - 29s 779us/sample - loss: 1.5744 - acc: 0.4754 - val_loss: 1.2840 - val_acc: 0.5970\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5389 - acc: 0.4887\n",
      "Epoch 00014: val_loss improved from 1.28397 to 1.24659, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_3_conv_checkpoint/014-1.2466.hdf5\n",
      "36805/36805 [==============================] - 29s 778us/sample - loss: 1.5389 - acc: 0.4887 - val_loss: 1.2466 - val_acc: 0.6087\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5061 - acc: 0.4989\n",
      "Epoch 00015: val_loss improved from 1.24659 to 1.21402, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_3_conv_checkpoint/015-1.2140.hdf5\n",
      "36805/36805 [==============================] - 29s 777us/sample - loss: 1.5060 - acc: 0.4989 - val_loss: 1.2140 - val_acc: 0.6194\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4857 - acc: 0.5040\n",
      "Epoch 00016: val_loss improved from 1.21402 to 1.18757, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_3_conv_checkpoint/016-1.1876.hdf5\n",
      "36805/36805 [==============================] - 29s 778us/sample - loss: 1.4856 - acc: 0.5040 - val_loss: 1.1876 - val_acc: 0.6240\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4590 - acc: 0.5163\n",
      "Epoch 00017: val_loss improved from 1.18757 to 1.17358, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_3_conv_checkpoint/017-1.1736.hdf5\n",
      "36805/36805 [==============================] - 29s 776us/sample - loss: 1.4589 - acc: 0.5163 - val_loss: 1.1736 - val_acc: 0.6278\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4226 - acc: 0.5236\n",
      "Epoch 00018: val_loss improved from 1.17358 to 1.15292, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_3_conv_checkpoint/018-1.1529.hdf5\n",
      "36805/36805 [==============================] - 29s 775us/sample - loss: 1.4227 - acc: 0.5236 - val_loss: 1.1529 - val_acc: 0.6362\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4121 - acc: 0.5329\n",
      "Epoch 00019: val_loss improved from 1.15292 to 1.11977, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_3_conv_checkpoint/019-1.1198.hdf5\n",
      "36805/36805 [==============================] - 29s 777us/sample - loss: 1.4121 - acc: 0.5328 - val_loss: 1.1198 - val_acc: 0.6448\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3835 - acc: 0.5412\n",
      "Epoch 00020: val_loss improved from 1.11977 to 1.09514, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_3_conv_checkpoint/020-1.0951.hdf5\n",
      "36805/36805 [==============================] - 29s 775us/sample - loss: 1.3836 - acc: 0.5411 - val_loss: 1.0951 - val_acc: 0.6527\n",
      "Epoch 21/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3716 - acc: 0.5466\n",
      "Epoch 00021: val_loss improved from 1.09514 to 1.07950, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_3_conv_checkpoint/021-1.0795.hdf5\n",
      "36805/36805 [==============================] - 29s 778us/sample - loss: 1.3716 - acc: 0.5466 - val_loss: 1.0795 - val_acc: 0.6622\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3497 - acc: 0.5534\n",
      "Epoch 00022: val_loss improved from 1.07950 to 1.06688, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_3_conv_checkpoint/022-1.0669.hdf5\n",
      "36805/36805 [==============================] - 29s 776us/sample - loss: 1.3497 - acc: 0.5534 - val_loss: 1.0669 - val_acc: 0.6655\n",
      "Epoch 23/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3367 - acc: 0.5575\n",
      "Epoch 00023: val_loss improved from 1.06688 to 1.04320, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_3_conv_checkpoint/023-1.0432.hdf5\n",
      "36805/36805 [==============================] - 29s 778us/sample - loss: 1.3366 - acc: 0.5575 - val_loss: 1.0432 - val_acc: 0.6755\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3210 - acc: 0.5645\n",
      "Epoch 00024: val_loss improved from 1.04320 to 1.03766, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_3_conv_checkpoint/024-1.0377.hdf5\n",
      "36805/36805 [==============================] - 29s 776us/sample - loss: 1.3210 - acc: 0.5645 - val_loss: 1.0377 - val_acc: 0.6734\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3068 - acc: 0.5709\n",
      "Epoch 00025: val_loss improved from 1.03766 to 1.01504, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_3_conv_checkpoint/025-1.0150.hdf5\n",
      "36805/36805 [==============================] - 29s 778us/sample - loss: 1.3067 - acc: 0.5709 - val_loss: 1.0150 - val_acc: 0.6806\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2897 - acc: 0.5742\n",
      "Epoch 00026: val_loss improved from 1.01504 to 1.00993, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_3_conv_checkpoint/026-1.0099.hdf5\n",
      "36805/36805 [==============================] - 29s 775us/sample - loss: 1.2897 - acc: 0.5742 - val_loss: 1.0099 - val_acc: 0.6751\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2799 - acc: 0.5786\n",
      "Epoch 00027: val_loss improved from 1.00993 to 0.98912, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_3_conv_checkpoint/027-0.9891.hdf5\n",
      "36805/36805 [==============================] - 28s 773us/sample - loss: 1.2800 - acc: 0.5786 - val_loss: 0.9891 - val_acc: 0.6935\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2674 - acc: 0.5851\n",
      "Epoch 00028: val_loss improved from 0.98912 to 0.97359, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_3_conv_checkpoint/028-0.9736.hdf5\n",
      "36805/36805 [==============================] - 29s 780us/sample - loss: 1.2675 - acc: 0.5851 - val_loss: 0.9736 - val_acc: 0.6958\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2536 - acc: 0.5909\n",
      "Epoch 00029: val_loss improved from 0.97359 to 0.96552, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_3_conv_checkpoint/029-0.9655.hdf5\n",
      "36805/36805 [==============================] - 29s 776us/sample - loss: 1.2537 - acc: 0.5908 - val_loss: 0.9655 - val_acc: 0.6990\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2409 - acc: 0.5951\n",
      "Epoch 00030: val_loss improved from 0.96552 to 0.95556, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_3_conv_checkpoint/030-0.9556.hdf5\n",
      "36805/36805 [==============================] - 29s 776us/sample - loss: 1.2408 - acc: 0.5951 - val_loss: 0.9556 - val_acc: 0.7028\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2401 - acc: 0.5943\n",
      "Epoch 00031: val_loss improved from 0.95556 to 0.95301, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_3_conv_checkpoint/031-0.9530.hdf5\n",
      "36805/36805 [==============================] - 29s 778us/sample - loss: 1.2401 - acc: 0.5942 - val_loss: 0.9530 - val_acc: 0.7063\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2211 - acc: 0.6015\n",
      "Epoch 00032: val_loss improved from 0.95301 to 0.93250, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_3_conv_checkpoint/032-0.9325.hdf5\n",
      "36805/36805 [==============================] - 28s 774us/sample - loss: 1.2211 - acc: 0.6015 - val_loss: 0.9325 - val_acc: 0.7053\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2148 - acc: 0.6029\n",
      "Epoch 00033: val_loss improved from 0.93250 to 0.92585, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_3_conv_checkpoint/033-0.9258.hdf5\n",
      "36805/36805 [==============================] - 29s 779us/sample - loss: 1.2147 - acc: 0.6030 - val_loss: 0.9258 - val_acc: 0.7179\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2035 - acc: 0.6061\n",
      "Epoch 00034: val_loss improved from 0.92585 to 0.91081, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_3_conv_checkpoint/034-0.9108.hdf5\n",
      "36805/36805 [==============================] - 29s 777us/sample - loss: 1.2034 - acc: 0.6061 - val_loss: 0.9108 - val_acc: 0.7137\n",
      "Epoch 35/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2004 - acc: 0.6072\n",
      "Epoch 00035: val_loss improved from 0.91081 to 0.90367, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_3_conv_checkpoint/035-0.9037.hdf5\n",
      "36805/36805 [==============================] - 29s 775us/sample - loss: 1.2003 - acc: 0.6073 - val_loss: 0.9037 - val_acc: 0.7228\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1965 - acc: 0.6101\n",
      "Epoch 00036: val_loss did not improve from 0.90367\n",
      "36805/36805 [==============================] - 29s 777us/sample - loss: 1.1964 - acc: 0.6102 - val_loss: 0.9041 - val_acc: 0.7174\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1839 - acc: 0.6162\n",
      "Epoch 00037: val_loss improved from 0.90367 to 0.88551, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_3_conv_checkpoint/037-0.8855.hdf5\n",
      "36805/36805 [==============================] - 29s 777us/sample - loss: 1.1839 - acc: 0.6162 - val_loss: 0.8855 - val_acc: 0.7265\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1729 - acc: 0.6193\n",
      "Epoch 00038: val_loss did not improve from 0.88551\n",
      "36805/36805 [==============================] - 29s 775us/sample - loss: 1.1729 - acc: 0.6193 - val_loss: 0.8916 - val_acc: 0.7230\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1641 - acc: 0.6224\n",
      "Epoch 00039: val_loss improved from 0.88551 to 0.88210, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_3_conv_checkpoint/039-0.8821.hdf5\n",
      "36805/36805 [==============================] - 29s 777us/sample - loss: 1.1642 - acc: 0.6224 - val_loss: 0.8821 - val_acc: 0.7272\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1597 - acc: 0.6232\n",
      "Epoch 00040: val_loss improved from 0.88210 to 0.87438, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_3_conv_checkpoint/040-0.8744.hdf5\n",
      "36805/36805 [==============================] - 29s 777us/sample - loss: 1.1596 - acc: 0.6233 - val_loss: 0.8744 - val_acc: 0.7268\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1546 - acc: 0.6265\n",
      "Epoch 00041: val_loss did not improve from 0.87438\n",
      "36805/36805 [==============================] - 29s 776us/sample - loss: 1.1546 - acc: 0.6265 - val_loss: 0.8819 - val_acc: 0.7298\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1572 - acc: 0.6224\n",
      "Epoch 00042: val_loss improved from 0.87438 to 0.87118, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_3_conv_checkpoint/042-0.8712.hdf5\n",
      "36805/36805 [==============================] - 29s 779us/sample - loss: 1.1572 - acc: 0.6225 - val_loss: 0.8712 - val_acc: 0.7289\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1421 - acc: 0.6295\n",
      "Epoch 00043: val_loss improved from 0.87118 to 0.85629, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_3_conv_checkpoint/043-0.8563.hdf5\n",
      "36805/36805 [==============================] - 28s 774us/sample - loss: 1.1420 - acc: 0.6296 - val_loss: 0.8563 - val_acc: 0.7356\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1398 - acc: 0.6308\n",
      "Epoch 00044: val_loss improved from 0.85629 to 0.84679, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_3_conv_checkpoint/044-0.8468.hdf5\n",
      "36805/36805 [==============================] - 28s 773us/sample - loss: 1.1397 - acc: 0.6308 - val_loss: 0.8468 - val_acc: 0.7412\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1330 - acc: 0.6318\n",
      "Epoch 00045: val_loss did not improve from 0.84679\n",
      "36805/36805 [==============================] - 29s 777us/sample - loss: 1.1331 - acc: 0.6318 - val_loss: 0.8528 - val_acc: 0.7363\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1328 - acc: 0.6331\n",
      "Epoch 00046: val_loss improved from 0.84679 to 0.84434, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_3_conv_checkpoint/046-0.8443.hdf5\n",
      "36805/36805 [==============================] - 29s 777us/sample - loss: 1.1327 - acc: 0.6331 - val_loss: 0.8443 - val_acc: 0.7424\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1196 - acc: 0.6396\n",
      "Epoch 00047: val_loss improved from 0.84434 to 0.83573, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_3_conv_checkpoint/047-0.8357.hdf5\n",
      "36805/36805 [==============================] - 28s 774us/sample - loss: 1.1195 - acc: 0.6397 - val_loss: 0.8357 - val_acc: 0.7424\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1173 - acc: 0.6356\n",
      "Epoch 00048: val_loss did not improve from 0.83573\n",
      "36805/36805 [==============================] - 29s 777us/sample - loss: 1.1174 - acc: 0.6356 - val_loss: 0.8360 - val_acc: 0.7431\n",
      "Epoch 49/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.1067 - acc: 0.6427\n",
      "Epoch 00049: val_loss improved from 0.83573 to 0.83099, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_3_conv_checkpoint/049-0.8310.hdf5\n",
      "36805/36805 [==============================] - 29s 776us/sample - loss: 1.1069 - acc: 0.6427 - val_loss: 0.8310 - val_acc: 0.7445\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1101 - acc: 0.6411\n",
      "Epoch 00050: val_loss improved from 0.83099 to 0.81362, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_3_conv_checkpoint/050-0.8136.hdf5\n",
      "36805/36805 [==============================] - 29s 778us/sample - loss: 1.1103 - acc: 0.6411 - val_loss: 0.8136 - val_acc: 0.7461\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1006 - acc: 0.6423\n",
      "Epoch 00051: val_loss did not improve from 0.81362\n",
      "36805/36805 [==============================] - 28s 773us/sample - loss: 1.1007 - acc: 0.6423 - val_loss: 0.8160 - val_acc: 0.7510\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0948 - acc: 0.6464\n",
      "Epoch 00052: val_loss improved from 0.81362 to 0.80940, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_3_conv_checkpoint/052-0.8094.hdf5\n",
      "36805/36805 [==============================] - 29s 777us/sample - loss: 1.0947 - acc: 0.6465 - val_loss: 0.8094 - val_acc: 0.7496\n",
      "Epoch 53/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0957 - acc: 0.6450\n",
      "Epoch 00053: val_loss improved from 0.80940 to 0.80730, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_3_conv_checkpoint/053-0.8073.hdf5\n",
      "36805/36805 [==============================] - 29s 778us/sample - loss: 1.0951 - acc: 0.6452 - val_loss: 0.8073 - val_acc: 0.7522\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0930 - acc: 0.6487\n",
      "Epoch 00054: val_loss did not improve from 0.80730\n",
      "36805/36805 [==============================] - 29s 777us/sample - loss: 1.0931 - acc: 0.6486 - val_loss: 0.8095 - val_acc: 0.7501\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0905 - acc: 0.6464\n",
      "Epoch 00055: val_loss improved from 0.80730 to 0.80135, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_3_conv_checkpoint/055-0.8013.hdf5\n",
      "36805/36805 [==============================] - 29s 777us/sample - loss: 1.0904 - acc: 0.6464 - val_loss: 0.8013 - val_acc: 0.7531\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0849 - acc: 0.6496\n",
      "Epoch 00056: val_loss improved from 0.80135 to 0.79792, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_3_conv_checkpoint/056-0.7979.hdf5\n",
      "36805/36805 [==============================] - 29s 776us/sample - loss: 1.0849 - acc: 0.6496 - val_loss: 0.7979 - val_acc: 0.7545\n",
      "Epoch 57/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0854 - acc: 0.6487\n",
      "Epoch 00057: val_loss improved from 0.79792 to 0.78683, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_3_conv_checkpoint/057-0.7868.hdf5\n",
      "36805/36805 [==============================] - 29s 775us/sample - loss: 1.0855 - acc: 0.6487 - val_loss: 0.7868 - val_acc: 0.7547\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0716 - acc: 0.6537\n",
      "Epoch 00058: val_loss did not improve from 0.78683\n",
      "36805/36805 [==============================] - 29s 778us/sample - loss: 1.0717 - acc: 0.6536 - val_loss: 0.8045 - val_acc: 0.7536\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0725 - acc: 0.6535\n",
      "Epoch 00059: val_loss improved from 0.78683 to 0.78511, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_3_conv_checkpoint/059-0.7851.hdf5\n",
      "36805/36805 [==============================] - 29s 779us/sample - loss: 1.0727 - acc: 0.6535 - val_loss: 0.7851 - val_acc: 0.7538\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0690 - acc: 0.6575\n",
      "Epoch 00060: val_loss did not improve from 0.78511\n",
      "36805/36805 [==============================] - 28s 773us/sample - loss: 1.0690 - acc: 0.6575 - val_loss: 0.7928 - val_acc: 0.7526\n",
      "Epoch 61/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0641 - acc: 0.6562\n",
      "Epoch 00061: val_loss improved from 0.78511 to 0.78099, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_3_conv_checkpoint/061-0.7810.hdf5\n",
      "36805/36805 [==============================] - 29s 778us/sample - loss: 1.0638 - acc: 0.6564 - val_loss: 0.7810 - val_acc: 0.7573\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0665 - acc: 0.6556\n",
      "Epoch 00062: val_loss improved from 0.78099 to 0.77549, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_3_conv_checkpoint/062-0.7755.hdf5\n",
      "36805/36805 [==============================] - 29s 777us/sample - loss: 1.0665 - acc: 0.6556 - val_loss: 0.7755 - val_acc: 0.7605\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0541 - acc: 0.6607\n",
      "Epoch 00063: val_loss improved from 0.77549 to 0.77210, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_3_conv_checkpoint/063-0.7721.hdf5\n",
      "36805/36805 [==============================] - 29s 776us/sample - loss: 1.0541 - acc: 0.6607 - val_loss: 0.7721 - val_acc: 0.7640\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0522 - acc: 0.6607\n",
      "Epoch 00064: val_loss did not improve from 0.77210\n",
      "36805/36805 [==============================] - 29s 777us/sample - loss: 1.0522 - acc: 0.6608 - val_loss: 0.7754 - val_acc: 0.7608\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0537 - acc: 0.6622\n",
      "Epoch 00065: val_loss did not improve from 0.77210\n",
      "36805/36805 [==============================] - 29s 776us/sample - loss: 1.0537 - acc: 0.6622 - val_loss: 0.7856 - val_acc: 0.7610\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0554 - acc: 0.6628\n",
      "Epoch 00066: val_loss improved from 0.77210 to 0.76501, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_3_conv_checkpoint/066-0.7650.hdf5\n",
      "36805/36805 [==============================] - 29s 779us/sample - loss: 1.0555 - acc: 0.6628 - val_loss: 0.7650 - val_acc: 0.7631\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0554 - acc: 0.6604\n",
      "Epoch 00067: val_loss did not improve from 0.76501\n",
      "36805/36805 [==============================] - 29s 776us/sample - loss: 1.0554 - acc: 0.6604 - val_loss: 0.7734 - val_acc: 0.7666\n",
      "Epoch 68/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0439 - acc: 0.6639\n",
      "Epoch 00068: val_loss improved from 0.76501 to 0.76297, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_3_conv_checkpoint/068-0.7630.hdf5\n",
      "36805/36805 [==============================] - 29s 778us/sample - loss: 1.0440 - acc: 0.6639 - val_loss: 0.7630 - val_acc: 0.7650\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0407 - acc: 0.6673\n",
      "Epoch 00069: val_loss did not improve from 0.76297\n",
      "36805/36805 [==============================] - 28s 772us/sample - loss: 1.0408 - acc: 0.6672 - val_loss: 0.7672 - val_acc: 0.7659\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0415 - acc: 0.6645\n",
      "Epoch 00070: val_loss improved from 0.76297 to 0.76230, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_3_conv_checkpoint/070-0.7623.hdf5\n",
      "36805/36805 [==============================] - 29s 778us/sample - loss: 1.0414 - acc: 0.6646 - val_loss: 0.7623 - val_acc: 0.7661\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0365 - acc: 0.6668\n",
      "Epoch 00071: val_loss improved from 0.76230 to 0.75935, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_3_conv_checkpoint/071-0.7594.hdf5\n",
      "36805/36805 [==============================] - 29s 776us/sample - loss: 1.0364 - acc: 0.6668 - val_loss: 0.7594 - val_acc: 0.7701\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0358 - acc: 0.6680\n",
      "Epoch 00072: val_loss improved from 0.75935 to 0.75200, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_3_conv_checkpoint/072-0.7520.hdf5\n",
      "36805/36805 [==============================] - 29s 776us/sample - loss: 1.0358 - acc: 0.6680 - val_loss: 0.7520 - val_acc: 0.7671\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0328 - acc: 0.6688\n",
      "Epoch 00073: val_loss did not improve from 0.75200\n",
      "36805/36805 [==============================] - 29s 777us/sample - loss: 1.0327 - acc: 0.6688 - val_loss: 0.7566 - val_acc: 0.7717\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0269 - acc: 0.6710\n",
      "Epoch 00074: val_loss improved from 0.75200 to 0.74433, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_3_conv_checkpoint/074-0.7443.hdf5\n",
      "36805/36805 [==============================] - 29s 777us/sample - loss: 1.0269 - acc: 0.6709 - val_loss: 0.7443 - val_acc: 0.7713\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0339 - acc: 0.6673\n",
      "Epoch 00075: val_loss did not improve from 0.74433\n",
      "36805/36805 [==============================] - 29s 775us/sample - loss: 1.0340 - acc: 0.6673 - val_loss: 0.7506 - val_acc: 0.7715\n",
      "Epoch 76/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0280 - acc: 0.6708\n",
      "Epoch 00076: val_loss did not improve from 0.74433\n",
      "36805/36805 [==============================] - 29s 776us/sample - loss: 1.0280 - acc: 0.6709 - val_loss: 0.7447 - val_acc: 0.7722\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0173 - acc: 0.6746\n",
      "Epoch 00077: val_loss improved from 0.74433 to 0.73898, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_3_conv_checkpoint/077-0.7390.hdf5\n",
      "36805/36805 [==============================] - 29s 776us/sample - loss: 1.0174 - acc: 0.6746 - val_loss: 0.7390 - val_acc: 0.7754\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0257 - acc: 0.6698\n",
      "Epoch 00078: val_loss did not improve from 0.73898\n",
      "36805/36805 [==============================] - 28s 773us/sample - loss: 1.0259 - acc: 0.6698 - val_loss: 0.7423 - val_acc: 0.7764\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0203 - acc: 0.6717\n",
      "Epoch 00079: val_loss did not improve from 0.73898\n",
      "36805/36805 [==============================] - 28s 773us/sample - loss: 1.0204 - acc: 0.6716 - val_loss: 0.7434 - val_acc: 0.7727\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0147 - acc: 0.6749\n",
      "Epoch 00080: val_loss did not improve from 0.73898\n",
      "36805/36805 [==============================] - 29s 775us/sample - loss: 1.0147 - acc: 0.6749 - val_loss: 0.7392 - val_acc: 0.7738\n",
      "Epoch 81/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0175 - acc: 0.6722\n",
      "Epoch 00081: val_loss did not improve from 0.73898\n",
      "36805/36805 [==============================] - 29s 775us/sample - loss: 1.0175 - acc: 0.6721 - val_loss: 0.7451 - val_acc: 0.7738\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0171 - acc: 0.6708\n",
      "Epoch 00082: val_loss did not improve from 0.73898\n",
      "36805/36805 [==============================] - 29s 777us/sample - loss: 1.0172 - acc: 0.6708 - val_loss: 0.7419 - val_acc: 0.7734\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0115 - acc: 0.6737\n",
      "Epoch 00083: val_loss improved from 0.73898 to 0.72724, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_3_conv_checkpoint/083-0.7272.hdf5\n",
      "36805/36805 [==============================] - 29s 776us/sample - loss: 1.0114 - acc: 0.6737 - val_loss: 0.7272 - val_acc: 0.7771\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0090 - acc: 0.6731\n",
      "Epoch 00084: val_loss did not improve from 0.72724\n",
      "36805/36805 [==============================] - 29s 777us/sample - loss: 1.0090 - acc: 0.6731 - val_loss: 0.7356 - val_acc: 0.7768\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0083 - acc: 0.6744\n",
      "Epoch 00085: val_loss improved from 0.72724 to 0.72185, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_3_conv_checkpoint/085-0.7219.hdf5\n",
      "36805/36805 [==============================] - 29s 778us/sample - loss: 1.0082 - acc: 0.6744 - val_loss: 0.7219 - val_acc: 0.7813\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0068 - acc: 0.6779\n",
      "Epoch 00086: val_loss did not improve from 0.72185\n",
      "36805/36805 [==============================] - 29s 776us/sample - loss: 1.0069 - acc: 0.6778 - val_loss: 0.7296 - val_acc: 0.7810\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9979 - acc: 0.6783\n",
      "Epoch 00087: val_loss did not improve from 0.72185\n",
      "36805/36805 [==============================] - 29s 776us/sample - loss: 0.9979 - acc: 0.6783 - val_loss: 0.7299 - val_acc: 0.7782\n",
      "Epoch 88/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0035 - acc: 0.6763\n",
      "Epoch 00088: val_loss did not improve from 0.72185\n",
      "36805/36805 [==============================] - 29s 776us/sample - loss: 1.0036 - acc: 0.6762 - val_loss: 0.7243 - val_acc: 0.7778\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9990 - acc: 0.6783\n",
      "Epoch 00089: val_loss did not improve from 0.72185\n",
      "36805/36805 [==============================] - 28s 773us/sample - loss: 0.9990 - acc: 0.6783 - val_loss: 0.7231 - val_acc: 0.7810\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9910 - acc: 0.6800\n",
      "Epoch 00090: val_loss did not improve from 0.72185\n",
      "36805/36805 [==============================] - 29s 776us/sample - loss: 0.9910 - acc: 0.6800 - val_loss: 0.7239 - val_acc: 0.7813\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9995 - acc: 0.6800\n",
      "Epoch 00091: val_loss did not improve from 0.72185\n",
      "36805/36805 [==============================] - 29s 777us/sample - loss: 0.9995 - acc: 0.6800 - val_loss: 0.7225 - val_acc: 0.7792\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9984 - acc: 0.6799\n",
      "Epoch 00092: val_loss did not improve from 0.72185\n",
      "36805/36805 [==============================] - 29s 775us/sample - loss: 0.9986 - acc: 0.6799 - val_loss: 0.7260 - val_acc: 0.7815\n",
      "Epoch 93/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9895 - acc: 0.6825\n",
      "Epoch 00093: val_loss improved from 0.72185 to 0.71380, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_3_conv_checkpoint/093-0.7138.hdf5\n",
      "36805/36805 [==============================] - 29s 781us/sample - loss: 0.9894 - acc: 0.6826 - val_loss: 0.7138 - val_acc: 0.7859\n",
      "Epoch 94/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9919 - acc: 0.6809\n",
      "Epoch 00094: val_loss did not improve from 0.71380\n",
      "36805/36805 [==============================] - 29s 776us/sample - loss: 0.9918 - acc: 0.6809 - val_loss: 0.7146 - val_acc: 0.7857\n",
      "Epoch 95/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9867 - acc: 0.6832\n",
      "Epoch 00095: val_loss improved from 0.71380 to 0.70830, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_3_conv_checkpoint/095-0.7083.hdf5\n",
      "36805/36805 [==============================] - 29s 777us/sample - loss: 0.9868 - acc: 0.6832 - val_loss: 0.7083 - val_acc: 0.7824\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9901 - acc: 0.6824\n",
      "Epoch 00096: val_loss improved from 0.70830 to 0.70179, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_3_conv_checkpoint/096-0.7018.hdf5\n",
      "36805/36805 [==============================] - 29s 778us/sample - loss: 0.9902 - acc: 0.6824 - val_loss: 0.7018 - val_acc: 0.7890\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9875 - acc: 0.6807\n",
      "Epoch 00097: val_loss did not improve from 0.70179\n",
      "36805/36805 [==============================] - 29s 777us/sample - loss: 0.9875 - acc: 0.6807 - val_loss: 0.7092 - val_acc: 0.7836\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9904 - acc: 0.6845\n",
      "Epoch 00098: val_loss did not improve from 0.70179\n",
      "36805/36805 [==============================] - 29s 774us/sample - loss: 0.9904 - acc: 0.6845 - val_loss: 0.7103 - val_acc: 0.7871\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9830 - acc: 0.6845\n",
      "Epoch 00099: val_loss did not improve from 0.70179\n",
      "36805/36805 [==============================] - 29s 776us/sample - loss: 0.9829 - acc: 0.6846 - val_loss: 0.7106 - val_acc: 0.7869\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9873 - acc: 0.6831\n",
      "Epoch 00100: val_loss did not improve from 0.70179\n",
      "36805/36805 [==============================] - 28s 773us/sample - loss: 0.9872 - acc: 0.6831 - val_loss: 0.7078 - val_acc: 0.7841\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9771 - acc: 0.6859\n",
      "Epoch 00101: val_loss did not improve from 0.70179\n",
      "36805/36805 [==============================] - 29s 777us/sample - loss: 0.9772 - acc: 0.6859 - val_loss: 0.7034 - val_acc: 0.7873\n",
      "Epoch 102/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9811 - acc: 0.6817\n",
      "Epoch 00102: val_loss improved from 0.70179 to 0.70022, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_3_conv_checkpoint/102-0.7002.hdf5\n",
      "36805/36805 [==============================] - 29s 777us/sample - loss: 0.9814 - acc: 0.6816 - val_loss: 0.7002 - val_acc: 0.7892\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9768 - acc: 0.6874\n",
      "Epoch 00103: val_loss did not improve from 0.70022\n",
      "36805/36805 [==============================] - 29s 776us/sample - loss: 0.9768 - acc: 0.6874 - val_loss: 0.7115 - val_acc: 0.7813\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9735 - acc: 0.6868\n",
      "Epoch 00104: val_loss improved from 0.70022 to 0.69712, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_3_conv_checkpoint/104-0.6971.hdf5\n",
      "36805/36805 [==============================] - 29s 778us/sample - loss: 0.9736 - acc: 0.6868 - val_loss: 0.6971 - val_acc: 0.7897\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9725 - acc: 0.6864\n",
      "Epoch 00105: val_loss did not improve from 0.69712\n",
      "36805/36805 [==============================] - 29s 777us/sample - loss: 0.9725 - acc: 0.6864 - val_loss: 0.7005 - val_acc: 0.7883\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9683 - acc: 0.6897\n",
      "Epoch 00106: val_loss did not improve from 0.69712\n",
      "36805/36805 [==============================] - 29s 777us/sample - loss: 0.9684 - acc: 0.6896 - val_loss: 0.7019 - val_acc: 0.7913\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9669 - acc: 0.6883\n",
      "Epoch 00107: val_loss improved from 0.69712 to 0.69642, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_3_conv_checkpoint/107-0.6964.hdf5\n",
      "36805/36805 [==============================] - 29s 778us/sample - loss: 0.9668 - acc: 0.6883 - val_loss: 0.6964 - val_acc: 0.7871\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9675 - acc: 0.6891\n",
      "Epoch 00108: val_loss did not improve from 0.69642\n",
      "36805/36805 [==============================] - 29s 777us/sample - loss: 0.9674 - acc: 0.6891 - val_loss: 0.6981 - val_acc: 0.7857\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9709 - acc: 0.6894\n",
      "Epoch 00109: val_loss improved from 0.69642 to 0.69068, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_3_conv_checkpoint/109-0.6907.hdf5\n",
      "36805/36805 [==============================] - 29s 779us/sample - loss: 0.9708 - acc: 0.6894 - val_loss: 0.6907 - val_acc: 0.7899\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9675 - acc: 0.6892\n",
      "Epoch 00110: val_loss did not improve from 0.69068\n",
      "36805/36805 [==============================] - 29s 776us/sample - loss: 0.9675 - acc: 0.6892 - val_loss: 0.6917 - val_acc: 0.7925\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9613 - acc: 0.6882\n",
      "Epoch 00111: val_loss did not improve from 0.69068\n",
      "36805/36805 [==============================] - 29s 776us/sample - loss: 0.9614 - acc: 0.6882 - val_loss: 0.7022 - val_acc: 0.7843\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9644 - acc: 0.6907\n",
      "Epoch 00112: val_loss did not improve from 0.69068\n",
      "36805/36805 [==============================] - 29s 778us/sample - loss: 0.9644 - acc: 0.6907 - val_loss: 0.6931 - val_acc: 0.7957\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9567 - acc: 0.6917\n",
      "Epoch 00113: val_loss did not improve from 0.69068\n",
      "36805/36805 [==============================] - 29s 779us/sample - loss: 0.9569 - acc: 0.6917 - val_loss: 0.6930 - val_acc: 0.7911\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9621 - acc: 0.6939\n",
      "Epoch 00114: val_loss improved from 0.69068 to 0.68375, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_3_conv_checkpoint/114-0.6837.hdf5\n",
      "36805/36805 [==============================] - 29s 780us/sample - loss: 0.9621 - acc: 0.6939 - val_loss: 0.6837 - val_acc: 0.7999\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9550 - acc: 0.6952\n",
      "Epoch 00115: val_loss did not improve from 0.68375\n",
      "36805/36805 [==============================] - 29s 779us/sample - loss: 0.9551 - acc: 0.6951 - val_loss: 0.6890 - val_acc: 0.7983\n",
      "Epoch 116/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9548 - acc: 0.6930\n",
      "Epoch 00116: val_loss did not improve from 0.68375\n",
      "36805/36805 [==============================] - 29s 778us/sample - loss: 0.9548 - acc: 0.6930 - val_loss: 0.6950 - val_acc: 0.7948\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9598 - acc: 0.6928\n",
      "Epoch 00117: val_loss did not improve from 0.68375\n",
      "36805/36805 [==============================] - 29s 776us/sample - loss: 0.9599 - acc: 0.6928 - val_loss: 0.6894 - val_acc: 0.7948\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9559 - acc: 0.6945\n",
      "Epoch 00118: val_loss improved from 0.68375 to 0.67873, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_3_conv_checkpoint/118-0.6787.hdf5\n",
      "36805/36805 [==============================] - 29s 777us/sample - loss: 0.9560 - acc: 0.6944 - val_loss: 0.6787 - val_acc: 0.7929\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9554 - acc: 0.6938\n",
      "Epoch 00119: val_loss did not improve from 0.67873\n",
      "36805/36805 [==============================] - 29s 775us/sample - loss: 0.9554 - acc: 0.6938 - val_loss: 0.6902 - val_acc: 0.7920\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9522 - acc: 0.6931\n",
      "Epoch 00120: val_loss did not improve from 0.67873\n",
      "36805/36805 [==============================] - 29s 777us/sample - loss: 0.9523 - acc: 0.6930 - val_loss: 0.6906 - val_acc: 0.7945\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9467 - acc: 0.6939\n",
      "Epoch 00121: val_loss did not improve from 0.67873\n",
      "36805/36805 [==============================] - 29s 777us/sample - loss: 0.9466 - acc: 0.6940 - val_loss: 0.6801 - val_acc: 0.7957\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9514 - acc: 0.6969\n",
      "Epoch 00122: val_loss did not improve from 0.67873\n",
      "36805/36805 [==============================] - 29s 777us/sample - loss: 0.9514 - acc: 0.6969 - val_loss: 0.6857 - val_acc: 0.7932\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9478 - acc: 0.6978\n",
      "Epoch 00123: val_loss did not improve from 0.67873\n",
      "36805/36805 [==============================] - 29s 776us/sample - loss: 0.9478 - acc: 0.6978 - val_loss: 0.6789 - val_acc: 0.7959\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9475 - acc: 0.6938\n",
      "Epoch 00124: val_loss improved from 0.67873 to 0.67510, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_3_conv_checkpoint/124-0.6751.hdf5\n",
      "36805/36805 [==============================] - 29s 778us/sample - loss: 0.9477 - acc: 0.6937 - val_loss: 0.6751 - val_acc: 0.8004\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9505 - acc: 0.6959\n",
      "Epoch 00125: val_loss did not improve from 0.67510\n",
      "36805/36805 [==============================] - 29s 775us/sample - loss: 0.9505 - acc: 0.6959 - val_loss: 0.6826 - val_acc: 0.7957\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9487 - acc: 0.6972\n",
      "Epoch 00126: val_loss did not improve from 0.67510\n",
      "36805/36805 [==============================] - 29s 776us/sample - loss: 0.9488 - acc: 0.6972 - val_loss: 0.6782 - val_acc: 0.7985\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9443 - acc: 0.6995\n",
      "Epoch 00127: val_loss improved from 0.67510 to 0.67312, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_3_conv_checkpoint/127-0.6731.hdf5\n",
      "36805/36805 [==============================] - 29s 787us/sample - loss: 0.9442 - acc: 0.6995 - val_loss: 0.6731 - val_acc: 0.8011\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9420 - acc: 0.6973\n",
      "Epoch 00128: val_loss improved from 0.67312 to 0.67086, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_3_conv_checkpoint/128-0.6709.hdf5\n",
      "36805/36805 [==============================] - 29s 778us/sample - loss: 0.9419 - acc: 0.6974 - val_loss: 0.6709 - val_acc: 0.8022\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9402 - acc: 0.6995\n",
      "Epoch 00129: val_loss did not improve from 0.67086\n",
      "36805/36805 [==============================] - 29s 777us/sample - loss: 0.9401 - acc: 0.6996 - val_loss: 0.6830 - val_acc: 0.7911\n",
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9364 - acc: 0.6980\n",
      "Epoch 00130: val_loss did not improve from 0.67086\n",
      "36805/36805 [==============================] - 29s 775us/sample - loss: 0.9363 - acc: 0.6980 - val_loss: 0.6773 - val_acc: 0.7959\n",
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9333 - acc: 0.7021\n",
      "Epoch 00131: val_loss improved from 0.67086 to 0.66506, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_3_conv_checkpoint/131-0.6651.hdf5\n",
      "36805/36805 [==============================] - 29s 777us/sample - loss: 0.9333 - acc: 0.7021 - val_loss: 0.6651 - val_acc: 0.7973\n",
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9340 - acc: 0.6992\n",
      "Epoch 00132: val_loss did not improve from 0.66506\n",
      "36805/36805 [==============================] - 29s 775us/sample - loss: 0.9340 - acc: 0.6992 - val_loss: 0.6823 - val_acc: 0.7985\n",
      "Epoch 133/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9314 - acc: 0.6998\n",
      "Epoch 00133: val_loss did not improve from 0.66506\n",
      "36805/36805 [==============================] - 29s 775us/sample - loss: 0.9314 - acc: 0.6998 - val_loss: 0.6763 - val_acc: 0.7980\n",
      "Epoch 134/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9406 - acc: 0.6958\n",
      "Epoch 00134: val_loss did not improve from 0.66506\n",
      "36805/36805 [==============================] - 29s 777us/sample - loss: 0.9405 - acc: 0.6958 - val_loss: 0.6758 - val_acc: 0.8001\n",
      "Epoch 135/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9325 - acc: 0.7024\n",
      "Epoch 00135: val_loss did not improve from 0.66506\n",
      "36805/36805 [==============================] - 29s 775us/sample - loss: 0.9324 - acc: 0.7024 - val_loss: 0.6691 - val_acc: 0.8027\n",
      "Epoch 136/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9338 - acc: 0.7017\n",
      "Epoch 00136: val_loss improved from 0.66506 to 0.66146, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_3_conv_checkpoint/136-0.6615.hdf5\n",
      "36805/36805 [==============================] - 29s 775us/sample - loss: 0.9338 - acc: 0.7018 - val_loss: 0.6615 - val_acc: 0.8062\n",
      "Epoch 137/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9294 - acc: 0.7008\n",
      "Epoch 00137: val_loss did not improve from 0.66146\n",
      "36805/36805 [==============================] - 29s 777us/sample - loss: 0.9295 - acc: 0.7008 - val_loss: 0.6645 - val_acc: 0.8001\n",
      "Epoch 138/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9272 - acc: 0.7001\n",
      "Epoch 00138: val_loss did not improve from 0.66146\n",
      "36805/36805 [==============================] - 29s 775us/sample - loss: 0.9272 - acc: 0.7001 - val_loss: 0.6641 - val_acc: 0.8039\n",
      "Epoch 139/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9274 - acc: 0.7012\n",
      "Epoch 00139: val_loss improved from 0.66146 to 0.66077, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_3_conv_checkpoint/139-0.6608.hdf5\n",
      "36805/36805 [==============================] - 29s 776us/sample - loss: 0.9274 - acc: 0.7013 - val_loss: 0.6608 - val_acc: 0.8001\n",
      "Epoch 140/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9288 - acc: 0.7032\n",
      "Epoch 00140: val_loss did not improve from 0.66077\n",
      "36805/36805 [==============================] - 29s 777us/sample - loss: 0.9287 - acc: 0.7032 - val_loss: 0.6654 - val_acc: 0.8032\n",
      "Epoch 141/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9299 - acc: 0.7005\n",
      "Epoch 00141: val_loss did not improve from 0.66077\n",
      "36805/36805 [==============================] - 29s 778us/sample - loss: 0.9299 - acc: 0.7005 - val_loss: 0.6654 - val_acc: 0.8041\n",
      "Epoch 142/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9254 - acc: 0.7056\n",
      "Epoch 00142: val_loss did not improve from 0.66077\n",
      "36805/36805 [==============================] - 29s 776us/sample - loss: 0.9254 - acc: 0.7056 - val_loss: 0.6662 - val_acc: 0.7976\n",
      "Epoch 143/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9230 - acc: 0.7047\n",
      "Epoch 00143: val_loss improved from 0.66077 to 0.65901, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_3_conv_checkpoint/143-0.6590.hdf5\n",
      "36805/36805 [==============================] - 29s 779us/sample - loss: 0.9230 - acc: 0.7047 - val_loss: 0.6590 - val_acc: 0.8001\n",
      "Epoch 144/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9187 - acc: 0.7034\n",
      "Epoch 00144: val_loss did not improve from 0.65901\n",
      "36805/36805 [==============================] - 29s 776us/sample - loss: 0.9186 - acc: 0.7034 - val_loss: 0.6612 - val_acc: 0.8015\n",
      "Epoch 145/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9240 - acc: 0.7051\n",
      "Epoch 00145: val_loss improved from 0.65901 to 0.65380, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_3_conv_checkpoint/145-0.6538.hdf5\n",
      "36805/36805 [==============================] - 29s 776us/sample - loss: 0.9239 - acc: 0.7051 - val_loss: 0.6538 - val_acc: 0.8027\n",
      "Epoch 146/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9240 - acc: 0.7022\n",
      "Epoch 00146: val_loss did not improve from 0.65380\n",
      "36805/36805 [==============================] - 29s 776us/sample - loss: 0.9239 - acc: 0.7022 - val_loss: 0.6638 - val_acc: 0.7997\n",
      "Epoch 147/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9168 - acc: 0.7040\n",
      "Epoch 00147: val_loss did not improve from 0.65380\n",
      "36805/36805 [==============================] - 29s 775us/sample - loss: 0.9168 - acc: 0.7040 - val_loss: 0.6593 - val_acc: 0.8020\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 148/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9204 - acc: 0.7038\n",
      "Epoch 00148: val_loss did not improve from 0.65380\n",
      "36805/36805 [==============================] - 29s 776us/sample - loss: 0.9205 - acc: 0.7037 - val_loss: 0.6567 - val_acc: 0.7997\n",
      "Epoch 149/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9165 - acc: 0.7057\n",
      "Epoch 00149: val_loss did not improve from 0.65380\n",
      "36805/36805 [==============================] - 29s 776us/sample - loss: 0.9166 - acc: 0.7057 - val_loss: 0.6606 - val_acc: 0.8022\n",
      "Epoch 150/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9143 - acc: 0.7079\n",
      "Epoch 00150: val_loss did not improve from 0.65380\n",
      "36805/36805 [==============================] - 29s 776us/sample - loss: 0.9144 - acc: 0.7079 - val_loss: 0.6609 - val_acc: 0.8015\n",
      "Epoch 151/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9098 - acc: 0.7095\n",
      "Epoch 00151: val_loss did not improve from 0.65380\n",
      "36805/36805 [==============================] - 29s 776us/sample - loss: 0.9098 - acc: 0.7095 - val_loss: 0.6592 - val_acc: 0.8041\n",
      "Epoch 152/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9162 - acc: 0.7056- ETA: 0s - loss: 0.9161 - acc: 0\n",
      "Epoch 00152: val_loss did not improve from 0.65380\n",
      "36805/36805 [==============================] - 29s 779us/sample - loss: 0.9163 - acc: 0.7056 - val_loss: 0.6587 - val_acc: 0.8043\n",
      "Epoch 153/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9127 - acc: 0.7089\n",
      "Epoch 00153: val_loss improved from 0.65380 to 0.65145, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_3_conv_checkpoint/153-0.6515.hdf5\n",
      "36805/36805 [==============================] - 29s 778us/sample - loss: 0.9127 - acc: 0.7089 - val_loss: 0.6515 - val_acc: 0.8041\n",
      "Epoch 154/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9094 - acc: 0.7083\n",
      "Epoch 00154: val_loss improved from 0.65145 to 0.64461, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_3_conv_checkpoint/154-0.6446.hdf5\n",
      "36805/36805 [==============================] - 29s 776us/sample - loss: 0.9093 - acc: 0.7083 - val_loss: 0.6446 - val_acc: 0.8090\n",
      "Epoch 155/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9100 - acc: 0.7097\n",
      "Epoch 00155: val_loss did not improve from 0.64461\n",
      "36805/36805 [==============================] - 29s 775us/sample - loss: 0.9099 - acc: 0.7097 - val_loss: 0.6624 - val_acc: 0.8022\n",
      "Epoch 156/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9123 - acc: 0.7080\n",
      "Epoch 00156: val_loss did not improve from 0.64461\n",
      "36805/36805 [==============================] - 29s 775us/sample - loss: 0.9123 - acc: 0.7080 - val_loss: 0.6521 - val_acc: 0.8053\n",
      "Epoch 157/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9070 - acc: 0.7096\n",
      "Epoch 00157: val_loss did not improve from 0.64461\n",
      "36805/36805 [==============================] - 29s 777us/sample - loss: 0.9071 - acc: 0.7096 - val_loss: 0.6556 - val_acc: 0.8053\n",
      "Epoch 158/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9142 - acc: 0.7060\n",
      "Epoch 00158: val_loss did not improve from 0.64461\n",
      "36805/36805 [==============================] - 29s 776us/sample - loss: 0.9141 - acc: 0.7060 - val_loss: 0.6777 - val_acc: 0.7948\n",
      "Epoch 159/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9058 - acc: 0.7103\n",
      "Epoch 00159: val_loss did not improve from 0.64461\n",
      "36805/36805 [==============================] - 29s 777us/sample - loss: 0.9062 - acc: 0.7101 - val_loss: 0.6518 - val_acc: 0.8025\n",
      "Epoch 160/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9034 - acc: 0.7085\n",
      "Epoch 00160: val_loss did not improve from 0.64461\n",
      "36805/36805 [==============================] - 29s 775us/sample - loss: 0.9034 - acc: 0.7085 - val_loss: 0.6479 - val_acc: 0.8064\n",
      "Epoch 161/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9008 - acc: 0.7092\n",
      "Epoch 00161: val_loss did not improve from 0.64461\n",
      "36805/36805 [==============================] - 29s 784us/sample - loss: 0.9006 - acc: 0.7091 - val_loss: 0.6531 - val_acc: 0.8046\n",
      "Epoch 162/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9047 - acc: 0.7080\n",
      "Epoch 00162: val_loss did not improve from 0.64461\n",
      "36805/36805 [==============================] - 29s 776us/sample - loss: 0.9051 - acc: 0.7080 - val_loss: 0.6539 - val_acc: 0.8116\n",
      "Epoch 163/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9004 - acc: 0.7107\n",
      "Epoch 00163: val_loss did not improve from 0.64461\n",
      "36805/36805 [==============================] - 29s 776us/sample - loss: 0.9003 - acc: 0.7107 - val_loss: 0.6491 - val_acc: 0.8025\n",
      "Epoch 164/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9026 - acc: 0.7098\n",
      "Epoch 00164: val_loss did not improve from 0.64461\n",
      "36805/36805 [==============================] - 29s 776us/sample - loss: 0.9028 - acc: 0.7098 - val_loss: 0.6659 - val_acc: 0.8055\n",
      "Epoch 165/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9050 - acc: 0.7128\n",
      "Epoch 00165: val_loss did not improve from 0.64461\n",
      "36805/36805 [==============================] - 29s 775us/sample - loss: 0.9050 - acc: 0.7128 - val_loss: 0.6478 - val_acc: 0.8095\n",
      "Epoch 166/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8943 - acc: 0.7114\n",
      "Epoch 00166: val_loss did not improve from 0.64461\n",
      "36805/36805 [==============================] - 29s 776us/sample - loss: 0.8943 - acc: 0.7114 - val_loss: 0.6451 - val_acc: 0.8095\n",
      "Epoch 167/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9005 - acc: 0.7109\n",
      "Epoch 00167: val_loss did not improve from 0.64461\n",
      "36805/36805 [==============================] - 29s 775us/sample - loss: 0.9004 - acc: 0.7108 - val_loss: 0.6449 - val_acc: 0.8078\n",
      "Epoch 168/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8971 - acc: 0.7124\n",
      "Epoch 00168: val_loss improved from 0.64461 to 0.64315, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_3_conv_checkpoint/168-0.6431.hdf5\n",
      "36805/36805 [==============================] - 29s 775us/sample - loss: 0.8972 - acc: 0.7124 - val_loss: 0.6431 - val_acc: 0.8076\n",
      "Epoch 169/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9038 - acc: 0.7104\n",
      "Epoch 00169: val_loss improved from 0.64315 to 0.63988, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_3_conv_checkpoint/169-0.6399.hdf5\n",
      "36805/36805 [==============================] - 29s 776us/sample - loss: 0.9038 - acc: 0.7104 - val_loss: 0.6399 - val_acc: 0.8071\n",
      "Epoch 170/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8992 - acc: 0.7122\n",
      "Epoch 00170: val_loss did not improve from 0.63988\n",
      "36805/36805 [==============================] - 29s 776us/sample - loss: 0.8991 - acc: 0.7123 - val_loss: 0.6412 - val_acc: 0.8041\n",
      "Epoch 171/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8942 - acc: 0.7147\n",
      "Epoch 00171: val_loss did not improve from 0.63988\n",
      "36805/36805 [==============================] - 29s 776us/sample - loss: 0.8942 - acc: 0.7147 - val_loss: 0.6452 - val_acc: 0.8127\n",
      "Epoch 172/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8906 - acc: 0.7144\n",
      "Epoch 00172: val_loss did not improve from 0.63988\n",
      "36805/36805 [==============================] - 29s 777us/sample - loss: 0.8906 - acc: 0.7144 - val_loss: 0.6504 - val_acc: 0.8060\n",
      "Epoch 173/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8963 - acc: 0.7124\n",
      "Epoch 00173: val_loss improved from 0.63988 to 0.63359, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_3_conv_checkpoint/173-0.6336.hdf5\n",
      "36805/36805 [==============================] - 29s 777us/sample - loss: 0.8963 - acc: 0.7125 - val_loss: 0.6336 - val_acc: 0.8118\n",
      "Epoch 174/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8944 - acc: 0.7129\n",
      "Epoch 00174: val_loss did not improve from 0.63359\n",
      "36805/36805 [==============================] - 29s 775us/sample - loss: 0.8944 - acc: 0.7129 - val_loss: 0.6361 - val_acc: 0.8106\n",
      "Epoch 175/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8898 - acc: 0.7160\n",
      "Epoch 00175: val_loss did not improve from 0.63359\n",
      "36805/36805 [==============================] - 28s 773us/sample - loss: 0.8898 - acc: 0.7160 - val_loss: 0.6495 - val_acc: 0.8074\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 176/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8940 - acc: 0.7134\n",
      "Epoch 00176: val_loss did not improve from 0.63359\n",
      "36805/36805 [==============================] - 29s 774us/sample - loss: 0.8939 - acc: 0.7134 - val_loss: 0.6498 - val_acc: 0.8043\n",
      "Epoch 177/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8906 - acc: 0.7129\n",
      "Epoch 00177: val_loss did not improve from 0.63359\n",
      "36805/36805 [==============================] - 29s 777us/sample - loss: 0.8906 - acc: 0.7128 - val_loss: 0.6450 - val_acc: 0.8036\n",
      "Epoch 178/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8835 - acc: 0.7166\n",
      "Epoch 00178: val_loss did not improve from 0.63359\n",
      "36805/36805 [==============================] - 28s 773us/sample - loss: 0.8836 - acc: 0.7166 - val_loss: 0.6376 - val_acc: 0.8104\n",
      "Epoch 179/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8921 - acc: 0.7156\n",
      "Epoch 00179: val_loss did not improve from 0.63359\n",
      "36805/36805 [==============================] - 29s 781us/sample - loss: 0.8918 - acc: 0.7157 - val_loss: 0.6346 - val_acc: 0.8127\n",
      "Epoch 180/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8874 - acc: 0.7161\n",
      "Epoch 00180: val_loss did not improve from 0.63359\n",
      "36805/36805 [==============================] - 29s 784us/sample - loss: 0.8873 - acc: 0.7162 - val_loss: 0.6389 - val_acc: 0.8078\n",
      "Epoch 181/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8809 - acc: 0.7192\n",
      "Epoch 00181: val_loss did not improve from 0.63359\n",
      "36805/36805 [==============================] - 29s 783us/sample - loss: 0.8809 - acc: 0.7192 - val_loss: 0.6358 - val_acc: 0.8104\n",
      "Epoch 182/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8914 - acc: 0.7143\n",
      "Epoch 00182: val_loss did not improve from 0.63359\n",
      "36805/36805 [==============================] - 29s 783us/sample - loss: 0.8914 - acc: 0.7144 - val_loss: 0.6406 - val_acc: 0.8130\n",
      "Epoch 183/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8845 - acc: 0.7145\n",
      "Epoch 00183: val_loss did not improve from 0.63359\n",
      "36805/36805 [==============================] - 29s 784us/sample - loss: 0.8845 - acc: 0.7145 - val_loss: 0.6340 - val_acc: 0.8143\n",
      "Epoch 184/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8799 - acc: 0.7174\n",
      "Epoch 00184: val_loss did not improve from 0.63359\n",
      "36805/36805 [==============================] - 29s 784us/sample - loss: 0.8801 - acc: 0.7172 - val_loss: 0.6419 - val_acc: 0.8076\n",
      "Epoch 185/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8806 - acc: 0.7171\n",
      "Epoch 00185: val_loss did not improve from 0.63359\n",
      "36805/36805 [==============================] - 29s 785us/sample - loss: 0.8809 - acc: 0.7171 - val_loss: 0.6376 - val_acc: 0.8067\n",
      "Epoch 186/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8868 - acc: 0.7166\n",
      "Epoch 00186: val_loss improved from 0.63359 to 0.63238, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_3_conv_checkpoint/186-0.6324.hdf5\n",
      "36805/36805 [==============================] - 29s 786us/sample - loss: 0.8867 - acc: 0.7166 - val_loss: 0.6324 - val_acc: 0.8134\n",
      "Epoch 187/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8835 - acc: 0.7178\n",
      "Epoch 00187: val_loss improved from 0.63238 to 0.62901, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_3_conv_checkpoint/187-0.6290.hdf5\n",
      "36805/36805 [==============================] - 29s 784us/sample - loss: 0.8834 - acc: 0.7178 - val_loss: 0.6290 - val_acc: 0.8153\n",
      "Epoch 188/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8852 - acc: 0.7155\n",
      "Epoch 00188: val_loss did not improve from 0.62901\n",
      "36805/36805 [==============================] - 29s 787us/sample - loss: 0.8848 - acc: 0.7156 - val_loss: 0.6294 - val_acc: 0.8139\n",
      "Epoch 189/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8772 - acc: 0.7201\n",
      "Epoch 00189: val_loss did not improve from 0.62901\n",
      "36805/36805 [==============================] - 29s 785us/sample - loss: 0.8771 - acc: 0.7201 - val_loss: 0.6356 - val_acc: 0.8043\n",
      "Epoch 190/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8800 - acc: 0.7164\n",
      "Epoch 00190: val_loss did not improve from 0.62901\n",
      "36805/36805 [==============================] - 29s 784us/sample - loss: 0.8800 - acc: 0.7165 - val_loss: 0.6457 - val_acc: 0.8053\n",
      "Epoch 191/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8804 - acc: 0.7182\n",
      "Epoch 00191: val_loss did not improve from 0.62901\n",
      "36805/36805 [==============================] - 29s 785us/sample - loss: 0.8805 - acc: 0.7182 - val_loss: 0.6393 - val_acc: 0.8048\n",
      "Epoch 192/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8755 - acc: 0.7190\n",
      "Epoch 00192: val_loss improved from 0.62901 to 0.62434, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_3_conv_checkpoint/192-0.6243.hdf5\n",
      "36805/36805 [==============================] - 29s 785us/sample - loss: 0.8755 - acc: 0.7190 - val_loss: 0.6243 - val_acc: 0.8157\n",
      "Epoch 193/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8799 - acc: 0.7155\n",
      "Epoch 00193: val_loss did not improve from 0.62434\n",
      "36805/36805 [==============================] - 29s 783us/sample - loss: 0.8796 - acc: 0.7156 - val_loss: 0.6384 - val_acc: 0.8141\n",
      "Epoch 194/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8735 - acc: 0.7184\n",
      "Epoch 00194: val_loss did not improve from 0.62434\n",
      "36805/36805 [==============================] - 29s 785us/sample - loss: 0.8732 - acc: 0.7186 - val_loss: 0.6252 - val_acc: 0.8125\n",
      "Epoch 195/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8734 - acc: 0.7189\n",
      "Epoch 00195: val_loss did not improve from 0.62434\n",
      "36805/36805 [==============================] - 29s 783us/sample - loss: 0.8734 - acc: 0.7189 - val_loss: 0.6394 - val_acc: 0.8153\n",
      "Epoch 196/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8759 - acc: 0.7187\n",
      "Epoch 00196: val_loss did not improve from 0.62434\n",
      "36805/36805 [==============================] - 29s 782us/sample - loss: 0.8757 - acc: 0.7188 - val_loss: 0.6277 - val_acc: 0.8102\n",
      "Epoch 197/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8732 - acc: 0.7203\n",
      "Epoch 00197: val_loss did not improve from 0.62434\n",
      "36805/36805 [==============================] - 29s 783us/sample - loss: 0.8731 - acc: 0.7204 - val_loss: 0.6282 - val_acc: 0.8106\n",
      "Epoch 198/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8762 - acc: 0.7186\n",
      "Epoch 00198: val_loss did not improve from 0.62434\n",
      "36805/36805 [==============================] - 29s 784us/sample - loss: 0.8761 - acc: 0.7187 - val_loss: 0.6409 - val_acc: 0.8104\n",
      "Epoch 199/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8725 - acc: 0.7191\n",
      "Epoch 00199: val_loss did not improve from 0.62434\n",
      "36805/36805 [==============================] - 29s 775us/sample - loss: 0.8726 - acc: 0.7191 - val_loss: 0.6368 - val_acc: 0.8064\n",
      "Epoch 200/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8781 - acc: 0.7183\n",
      "Epoch 00200: val_loss did not improve from 0.62434\n",
      "36805/36805 [==============================] - 29s 777us/sample - loss: 0.8780 - acc: 0.7183 - val_loss: 0.6333 - val_acc: 0.8137\n",
      "Epoch 201/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8739 - acc: 0.7194\n",
      "Epoch 00201: val_loss did not improve from 0.62434\n",
      "36805/36805 [==============================] - 29s 776us/sample - loss: 0.8738 - acc: 0.7194 - val_loss: 0.6289 - val_acc: 0.8125\n",
      "Epoch 202/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8697 - acc: 0.7205\n",
      "Epoch 00202: val_loss improved from 0.62434 to 0.62316, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_3_conv_checkpoint/202-0.6232.hdf5\n",
      "36805/36805 [==============================] - 29s 778us/sample - loss: 0.8698 - acc: 0.7206 - val_loss: 0.6232 - val_acc: 0.8167\n",
      "Epoch 203/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8679 - acc: 0.7228\n",
      "Epoch 00203: val_loss did not improve from 0.62316\n",
      "36805/36805 [==============================] - 29s 776us/sample - loss: 0.8678 - acc: 0.7228 - val_loss: 0.6302 - val_acc: 0.8123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 204/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8632 - acc: 0.7241\n",
      "Epoch 00204: val_loss did not improve from 0.62316\n",
      "36805/36805 [==============================] - 29s 778us/sample - loss: 0.8632 - acc: 0.7241 - val_loss: 0.6283 - val_acc: 0.8125\n",
      "Epoch 205/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8641 - acc: 0.7254\n",
      "Epoch 00205: val_loss did not improve from 0.62316\n",
      "36805/36805 [==============================] - 29s 778us/sample - loss: 0.8641 - acc: 0.7254 - val_loss: 0.6242 - val_acc: 0.8134\n",
      "Epoch 206/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8665 - acc: 0.7216\n",
      "Epoch 00206: val_loss did not improve from 0.62316\n",
      "36805/36805 [==============================] - 29s 776us/sample - loss: 0.8664 - acc: 0.7216 - val_loss: 0.6314 - val_acc: 0.8111\n",
      "Epoch 207/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8646 - acc: 0.7222\n",
      "Epoch 00207: val_loss did not improve from 0.62316\n",
      "36805/36805 [==============================] - 29s 775us/sample - loss: 0.8645 - acc: 0.7222 - val_loss: 0.6317 - val_acc: 0.8132\n",
      "Epoch 208/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8640 - acc: 0.7228\n",
      "Epoch 00208: val_loss did not improve from 0.62316\n",
      "36805/36805 [==============================] - 28s 769us/sample - loss: 0.8641 - acc: 0.7228 - val_loss: 0.6234 - val_acc: 0.8141\n",
      "Epoch 209/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8678 - acc: 0.7231\n",
      "Epoch 00209: val_loss did not improve from 0.62316\n",
      "36805/36805 [==============================] - 28s 773us/sample - loss: 0.8679 - acc: 0.7231 - val_loss: 0.6276 - val_acc: 0.8137\n",
      "Epoch 210/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8616 - acc: 0.7204\n",
      "Epoch 00210: val_loss improved from 0.62316 to 0.62180, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_3_conv_checkpoint/210-0.6218.hdf5\n",
      "36805/36805 [==============================] - 29s 777us/sample - loss: 0.8617 - acc: 0.7203 - val_loss: 0.6218 - val_acc: 0.8146\n",
      "Epoch 211/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8696 - acc: 0.7239\n",
      "Epoch 00211: val_loss did not improve from 0.62180\n",
      "36805/36805 [==============================] - 29s 777us/sample - loss: 0.8696 - acc: 0.7240 - val_loss: 0.6223 - val_acc: 0.8118\n",
      "Epoch 212/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8632 - acc: 0.7209\n",
      "Epoch 00212: val_loss did not improve from 0.62180\n",
      "36805/36805 [==============================] - 29s 775us/sample - loss: 0.8632 - acc: 0.7209 - val_loss: 0.6267 - val_acc: 0.8157\n",
      "Epoch 213/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8640 - acc: 0.7236\n",
      "Epoch 00213: val_loss improved from 0.62180 to 0.61609, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_3_conv_checkpoint/213-0.6161.hdf5\n",
      "36805/36805 [==============================] - 29s 776us/sample - loss: 0.8639 - acc: 0.7236 - val_loss: 0.6161 - val_acc: 0.8111\n",
      "Epoch 214/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8601 - acc: 0.7230\n",
      "Epoch 00214: val_loss did not improve from 0.61609\n",
      "36805/36805 [==============================] - 28s 772us/sample - loss: 0.8601 - acc: 0.7230 - val_loss: 0.6312 - val_acc: 0.8057\n",
      "Epoch 215/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8613 - acc: 0.7229\n",
      "Epoch 00215: val_loss did not improve from 0.61609\n",
      "36805/36805 [==============================] - 28s 772us/sample - loss: 0.8615 - acc: 0.7230 - val_loss: 0.6246 - val_acc: 0.8111\n",
      "Epoch 216/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8600 - acc: 0.7248\n",
      "Epoch 00216: val_loss improved from 0.61609 to 0.61093, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_3_conv_checkpoint/216-0.6109.hdf5\n",
      "36805/36805 [==============================] - 29s 783us/sample - loss: 0.8596 - acc: 0.7249 - val_loss: 0.6109 - val_acc: 0.8185\n",
      "Epoch 217/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8584 - acc: 0.7264\n",
      "Epoch 00217: val_loss did not improve from 0.61093\n",
      "36805/36805 [==============================] - 28s 774us/sample - loss: 0.8583 - acc: 0.7264 - val_loss: 0.6209 - val_acc: 0.8167\n",
      "Epoch 218/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8598 - acc: 0.7203\n",
      "Epoch 00218: val_loss did not improve from 0.61093\n",
      "36805/36805 [==============================] - 28s 771us/sample - loss: 0.8597 - acc: 0.7203 - val_loss: 0.6261 - val_acc: 0.8137\n",
      "Epoch 219/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8626 - acc: 0.7235\n",
      "Epoch 00219: val_loss did not improve from 0.61093\n",
      "36805/36805 [==============================] - 29s 775us/sample - loss: 0.8626 - acc: 0.7235 - val_loss: 0.6200 - val_acc: 0.8116\n",
      "Epoch 220/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8658 - acc: 0.7216\n",
      "Epoch 00220: val_loss did not improve from 0.61093\n",
      "36805/36805 [==============================] - 29s 775us/sample - loss: 0.8657 - acc: 0.7217 - val_loss: 0.6263 - val_acc: 0.8120\n",
      "Epoch 221/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8572 - acc: 0.7245\n",
      "Epoch 00221: val_loss did not improve from 0.61093\n",
      "36805/36805 [==============================] - 28s 773us/sample - loss: 0.8574 - acc: 0.7245 - val_loss: 0.6200 - val_acc: 0.8162\n",
      "Epoch 222/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8627 - acc: 0.7222\n",
      "Epoch 00222: val_loss did not improve from 0.61093\n",
      "36805/36805 [==============================] - 28s 773us/sample - loss: 0.8627 - acc: 0.7222 - val_loss: 0.6112 - val_acc: 0.8195\n",
      "Epoch 223/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8565 - acc: 0.7259\n",
      "Epoch 00223: val_loss did not improve from 0.61093\n",
      "36805/36805 [==============================] - 28s 771us/sample - loss: 0.8565 - acc: 0.7259 - val_loss: 0.6115 - val_acc: 0.8192\n",
      "Epoch 224/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8565 - acc: 0.7262\n",
      "Epoch 00224: val_loss did not improve from 0.61093\n",
      "36805/36805 [==============================] - 28s 772us/sample - loss: 0.8568 - acc: 0.7262 - val_loss: 0.6128 - val_acc: 0.8218\n",
      "Epoch 225/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8545 - acc: 0.7261\n",
      "Epoch 00225: val_loss improved from 0.61093 to 0.60931, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_3_conv_checkpoint/225-0.6093.hdf5\n",
      "36805/36805 [==============================] - 29s 776us/sample - loss: 0.8545 - acc: 0.7262 - val_loss: 0.6093 - val_acc: 0.8139\n",
      "Epoch 226/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8535 - acc: 0.7232\n",
      "Epoch 00226: val_loss did not improve from 0.60931\n",
      "36805/36805 [==============================] - 29s 775us/sample - loss: 0.8534 - acc: 0.7232 - val_loss: 0.6197 - val_acc: 0.8146\n",
      "Epoch 227/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8459 - acc: 0.7282\n",
      "Epoch 00227: val_loss did not improve from 0.60931\n",
      "36805/36805 [==============================] - 29s 775us/sample - loss: 0.8459 - acc: 0.7282 - val_loss: 0.6145 - val_acc: 0.8209\n",
      "Epoch 228/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8552 - acc: 0.7251\n",
      "Epoch 00228: val_loss did not improve from 0.60931\n",
      "36805/36805 [==============================] - 28s 774us/sample - loss: 0.8553 - acc: 0.7251 - val_loss: 0.6109 - val_acc: 0.8139\n",
      "Epoch 229/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8500 - acc: 0.7254\n",
      "Epoch 00229: val_loss improved from 0.60931 to 0.60918, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_3_conv_checkpoint/229-0.6092.hdf5\n",
      "36805/36805 [==============================] - 28s 774us/sample - loss: 0.8500 - acc: 0.7254 - val_loss: 0.6092 - val_acc: 0.8183\n",
      "Epoch 230/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8509 - acc: 0.7251\n",
      "Epoch 00230: val_loss did not improve from 0.60918\n",
      "36805/36805 [==============================] - 28s 773us/sample - loss: 0.8509 - acc: 0.7250 - val_loss: 0.6093 - val_acc: 0.8241\n",
      "Epoch 231/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8472 - acc: 0.7288\n",
      "Epoch 00231: val_loss did not improve from 0.60918\n",
      "36805/36805 [==============================] - 28s 774us/sample - loss: 0.8472 - acc: 0.7288 - val_loss: 0.6155 - val_acc: 0.8146\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 232/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8532 - acc: 0.7248\n",
      "Epoch 00232: val_loss did not improve from 0.60918\n",
      "36805/36805 [==============================] - 29s 776us/sample - loss: 0.8532 - acc: 0.7248 - val_loss: 0.6160 - val_acc: 0.8160\n",
      "Epoch 233/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8523 - acc: 0.7250\n",
      "Epoch 00233: val_loss did not improve from 0.60918\n",
      "36805/36805 [==============================] - 29s 775us/sample - loss: 0.8523 - acc: 0.7250 - val_loss: 0.6187 - val_acc: 0.8132\n",
      "Epoch 234/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8515 - acc: 0.7255\n",
      "Epoch 00234: val_loss did not improve from 0.60918\n",
      "36805/36805 [==============================] - 29s 783us/sample - loss: 0.8515 - acc: 0.7256 - val_loss: 0.6210 - val_acc: 0.8157\n",
      "Epoch 235/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8480 - acc: 0.7280\n",
      "Epoch 00235: val_loss did not improve from 0.60918\n",
      "36805/36805 [==============================] - 29s 782us/sample - loss: 0.8485 - acc: 0.7278 - val_loss: 0.6132 - val_acc: 0.8178\n",
      "Epoch 236/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8532 - acc: 0.7243\n",
      "Epoch 00236: val_loss did not improve from 0.60918\n",
      "36805/36805 [==============================] - 29s 779us/sample - loss: 0.8533 - acc: 0.7243 - val_loss: 0.6210 - val_acc: 0.8141\n",
      "Epoch 237/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8473 - acc: 0.7281\n",
      "Epoch 00237: val_loss did not improve from 0.60918\n",
      "36805/36805 [==============================] - 29s 778us/sample - loss: 0.8473 - acc: 0.7281 - val_loss: 0.6119 - val_acc: 0.8143\n",
      "Epoch 238/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8458 - acc: 0.7291\n",
      "Epoch 00238: val_loss did not improve from 0.60918\n",
      "36805/36805 [==============================] - 29s 777us/sample - loss: 0.8461 - acc: 0.7291 - val_loss: 0.6170 - val_acc: 0.8178\n",
      "Epoch 239/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8460 - acc: 0.7297\n",
      "Epoch 00239: val_loss did not improve from 0.60918\n",
      "36805/36805 [==============================] - 29s 781us/sample - loss: 0.8461 - acc: 0.7297 - val_loss: 0.6136 - val_acc: 0.8150\n",
      "Epoch 240/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8433 - acc: 0.7298\n",
      "Epoch 00240: val_loss did not improve from 0.60918\n",
      "36805/36805 [==============================] - 29s 778us/sample - loss: 0.8439 - acc: 0.7296 - val_loss: 0.6208 - val_acc: 0.8162\n",
      "Epoch 241/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8444 - acc: 0.7271\n",
      "Epoch 00241: val_loss did not improve from 0.60918\n",
      "36805/36805 [==============================] - 29s 782us/sample - loss: 0.8443 - acc: 0.7271 - val_loss: 0.6244 - val_acc: 0.8113\n",
      "Epoch 242/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8477 - acc: 0.7269\n",
      "Epoch 00242: val_loss did not improve from 0.60918\n",
      "36805/36805 [==============================] - 29s 782us/sample - loss: 0.8473 - acc: 0.7270 - val_loss: 0.6105 - val_acc: 0.8174\n",
      "Epoch 243/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8464 - acc: 0.7276\n",
      "Epoch 00243: val_loss did not improve from 0.60918\n",
      "36805/36805 [==============================] - 29s 782us/sample - loss: 0.8466 - acc: 0.7275 - val_loss: 0.6113 - val_acc: 0.8181\n",
      "Epoch 244/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8480 - acc: 0.7272\n",
      "Epoch 00244: val_loss improved from 0.60918 to 0.60786, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_3_conv_checkpoint/244-0.6079.hdf5\n",
      "36805/36805 [==============================] - 29s 776us/sample - loss: 0.8479 - acc: 0.7272 - val_loss: 0.6079 - val_acc: 0.8178\n",
      "Epoch 245/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8451 - acc: 0.7276\n",
      "Epoch 00245: val_loss improved from 0.60786 to 0.60375, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_3_conv_checkpoint/245-0.6038.hdf5\n",
      "36805/36805 [==============================] - 29s 779us/sample - loss: 0.8451 - acc: 0.7277 - val_loss: 0.6038 - val_acc: 0.8220\n",
      "Epoch 246/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8404 - acc: 0.7295\n",
      "Epoch 00246: val_loss did not improve from 0.60375\n",
      "36805/36805 [==============================] - 29s 781us/sample - loss: 0.8402 - acc: 0.7296 - val_loss: 0.6102 - val_acc: 0.8192\n",
      "Epoch 247/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8421 - acc: 0.7313\n",
      "Epoch 00247: val_loss did not improve from 0.60375\n",
      "36805/36805 [==============================] - 29s 782us/sample - loss: 0.8422 - acc: 0.7312 - val_loss: 0.6136 - val_acc: 0.8139\n",
      "Epoch 248/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8400 - acc: 0.7317\n",
      "Epoch 00248: val_loss did not improve from 0.60375\n",
      "36805/36805 [==============================] - 29s 783us/sample - loss: 0.8399 - acc: 0.7317 - val_loss: 0.6159 - val_acc: 0.8155\n",
      "Epoch 249/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8332 - acc: 0.7316\n",
      "Epoch 00249: val_loss did not improve from 0.60375\n",
      "36805/36805 [==============================] - 29s 782us/sample - loss: 0.8332 - acc: 0.7316 - val_loss: 0.6093 - val_acc: 0.8202\n",
      "Epoch 250/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8385 - acc: 0.7284\n",
      "Epoch 00250: val_loss did not improve from 0.60375\n",
      "36805/36805 [==============================] - 29s 778us/sample - loss: 0.8381 - acc: 0.7285 - val_loss: 0.6095 - val_acc: 0.8120\n",
      "Epoch 251/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8419 - acc: 0.7297\n",
      "Epoch 00251: val_loss did not improve from 0.60375\n",
      "36805/36805 [==============================] - 29s 779us/sample - loss: 0.8418 - acc: 0.7298 - val_loss: 0.6160 - val_acc: 0.8125\n",
      "Epoch 252/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8366 - acc: 0.7302\n",
      "Epoch 00252: val_loss did not improve from 0.60375\n",
      "36805/36805 [==============================] - 29s 778us/sample - loss: 0.8366 - acc: 0.7301 - val_loss: 0.6097 - val_acc: 0.8185\n",
      "Epoch 253/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8358 - acc: 0.7311\n",
      "Epoch 00253: val_loss did not improve from 0.60375\n",
      "36805/36805 [==============================] - 29s 782us/sample - loss: 0.8358 - acc: 0.7311 - val_loss: 0.6127 - val_acc: 0.8178\n",
      "Epoch 254/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8421 - acc: 0.7287\n",
      "Epoch 00254: val_loss did not improve from 0.60375\n",
      "36805/36805 [==============================] - 29s 780us/sample - loss: 0.8420 - acc: 0.7288 - val_loss: 0.6070 - val_acc: 0.8185\n",
      "Epoch 255/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8362 - acc: 0.7313\n",
      "Epoch 00255: val_loss improved from 0.60375 to 0.60237, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_3_conv_checkpoint/255-0.6024.hdf5\n",
      "36805/36805 [==============================] - 29s 776us/sample - loss: 0.8361 - acc: 0.7313 - val_loss: 0.6024 - val_acc: 0.8213\n",
      "Epoch 256/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8373 - acc: 0.7315\n",
      "Epoch 00256: val_loss did not improve from 0.60237\n",
      "36805/36805 [==============================] - 29s 776us/sample - loss: 0.8372 - acc: 0.7316 - val_loss: 0.6090 - val_acc: 0.8232\n",
      "Epoch 257/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8361 - acc: 0.7313\n",
      "Epoch 00257: val_loss did not improve from 0.60237\n",
      "36805/36805 [==============================] - 29s 775us/sample - loss: 0.8361 - acc: 0.7313 - val_loss: 0.6060 - val_acc: 0.8185\n",
      "Epoch 258/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8356 - acc: 0.7314\n",
      "Epoch 00258: val_loss did not improve from 0.60237\n",
      "36805/36805 [==============================] - 29s 775us/sample - loss: 0.8356 - acc: 0.7314 - val_loss: 0.6092 - val_acc: 0.8195\n",
      "Epoch 259/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8351 - acc: 0.7309\n",
      "Epoch 00259: val_loss did not improve from 0.60237\n",
      "36805/36805 [==============================] - 29s 774us/sample - loss: 0.8351 - acc: 0.7309 - val_loss: 0.6070 - val_acc: 0.8127\n",
      "Epoch 260/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8286 - acc: 0.7313\n",
      "Epoch 00260: val_loss did not improve from 0.60237\n",
      "36805/36805 [==============================] - 28s 772us/sample - loss: 0.8284 - acc: 0.7314 - val_loss: 0.6103 - val_acc: 0.8088\n",
      "Epoch 261/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8334 - acc: 0.7321\n",
      "Epoch 00261: val_loss improved from 0.60237 to 0.59957, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_3_conv_checkpoint/261-0.5996.hdf5\n",
      "36805/36805 [==============================] - 29s 776us/sample - loss: 0.8333 - acc: 0.7321 - val_loss: 0.5996 - val_acc: 0.8197\n",
      "Epoch 262/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8325 - acc: 0.7306\n",
      "Epoch 00262: val_loss did not improve from 0.59957\n",
      "36805/36805 [==============================] - 28s 773us/sample - loss: 0.8326 - acc: 0.7305 - val_loss: 0.6048 - val_acc: 0.8190\n",
      "Epoch 263/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8341 - acc: 0.7321\n",
      "Epoch 00263: val_loss did not improve from 0.59957\n",
      "36805/36805 [==============================] - 28s 774us/sample - loss: 0.8342 - acc: 0.7321 - val_loss: 0.6049 - val_acc: 0.8146\n",
      "Epoch 264/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8294 - acc: 0.7307\n",
      "Epoch 00264: val_loss did not improve from 0.59957\n",
      "36805/36805 [==============================] - 29s 776us/sample - loss: 0.8293 - acc: 0.7308 - val_loss: 0.6025 - val_acc: 0.8211\n",
      "Epoch 265/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8294 - acc: 0.7321\n",
      "Epoch 00265: val_loss did not improve from 0.59957\n",
      "36805/36805 [==============================] - 29s 775us/sample - loss: 0.8294 - acc: 0.7321 - val_loss: 0.6002 - val_acc: 0.8157\n",
      "Epoch 266/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8265 - acc: 0.7366\n",
      "Epoch 00266: val_loss did not improve from 0.59957\n",
      "36805/36805 [==============================] - 28s 773us/sample - loss: 0.8265 - acc: 0.7366 - val_loss: 0.6047 - val_acc: 0.8183\n",
      "Epoch 267/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8291 - acc: 0.7330\n",
      "Epoch 00267: val_loss did not improve from 0.59957\n",
      "36805/36805 [==============================] - 28s 772us/sample - loss: 0.8290 - acc: 0.7331 - val_loss: 0.6060 - val_acc: 0.8183\n",
      "Epoch 268/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8293 - acc: 0.7335\n",
      "Epoch 00268: val_loss did not improve from 0.59957\n",
      "36805/36805 [==============================] - 28s 774us/sample - loss: 0.8293 - acc: 0.7335 - val_loss: 0.6009 - val_acc: 0.8178\n",
      "Epoch 269/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8232 - acc: 0.7355\n",
      "Epoch 00269: val_loss did not improve from 0.59957\n",
      "36805/36805 [==============================] - 28s 773us/sample - loss: 0.8232 - acc: 0.7355 - val_loss: 0.6070 - val_acc: 0.8174\n",
      "Epoch 270/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8241 - acc: 0.7320\n",
      "Epoch 00270: val_loss did not improve from 0.59957\n",
      "36805/36805 [==============================] - 28s 771us/sample - loss: 0.8241 - acc: 0.7321 - val_loss: 0.6059 - val_acc: 0.8174\n",
      "Epoch 271/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8277 - acc: 0.7309\n",
      "Epoch 00271: val_loss did not improve from 0.59957\n",
      "36805/36805 [==============================] - 29s 775us/sample - loss: 0.8278 - acc: 0.7309 - val_loss: 0.5997 - val_acc: 0.8164\n",
      "Epoch 272/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8281 - acc: 0.7339\n",
      "Epoch 00272: val_loss did not improve from 0.59957\n",
      "36805/36805 [==============================] - 29s 782us/sample - loss: 0.8280 - acc: 0.7340 - val_loss: 0.6061 - val_acc: 0.8148\n",
      "Epoch 273/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8273 - acc: 0.7326\n",
      "Epoch 00273: val_loss did not improve from 0.59957\n",
      "36805/36805 [==============================] - 29s 780us/sample - loss: 0.8275 - acc: 0.7325 - val_loss: 0.6199 - val_acc: 0.8097\n",
      "Epoch 274/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8270 - acc: 0.7325\n",
      "Epoch 00274: val_loss improved from 0.59957 to 0.59680, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_3_conv_checkpoint/274-0.5968.hdf5\n",
      "36805/36805 [==============================] - 28s 774us/sample - loss: 0.8270 - acc: 0.7324 - val_loss: 0.5968 - val_acc: 0.8206\n",
      "Epoch 275/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8245 - acc: 0.7369\n",
      "Epoch 00275: val_loss did not improve from 0.59680\n",
      "36805/36805 [==============================] - 28s 773us/sample - loss: 0.8245 - acc: 0.7369 - val_loss: 0.6041 - val_acc: 0.8199\n",
      "Epoch 276/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8269 - acc: 0.7340\n",
      "Epoch 00276: val_loss did not improve from 0.59680\n",
      "36805/36805 [==============================] - 28s 772us/sample - loss: 0.8266 - acc: 0.7342 - val_loss: 0.6008 - val_acc: 0.8225\n",
      "Epoch 277/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8227 - acc: 0.7342\n",
      "Epoch 00277: val_loss did not improve from 0.59680\n",
      "36805/36805 [==============================] - 29s 775us/sample - loss: 0.8226 - acc: 0.7341 - val_loss: 0.5995 - val_acc: 0.8155\n",
      "Epoch 278/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8268 - acc: 0.7308\n",
      "Epoch 00278: val_loss did not improve from 0.59680\n",
      "36805/36805 [==============================] - 28s 774us/sample - loss: 0.8267 - acc: 0.7308 - val_loss: 0.6037 - val_acc: 0.8141\n",
      "Epoch 279/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8211 - acc: 0.7355\n",
      "Epoch 00279: val_loss did not improve from 0.59680\n",
      "36805/36805 [==============================] - 28s 774us/sample - loss: 0.8212 - acc: 0.7354 - val_loss: 0.5982 - val_acc: 0.8199\n",
      "Epoch 280/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8249 - acc: 0.7351\n",
      "Epoch 00280: val_loss did not improve from 0.59680\n",
      "36805/36805 [==============================] - 28s 774us/sample - loss: 0.8249 - acc: 0.7351 - val_loss: 0.6060 - val_acc: 0.8204\n",
      "Epoch 281/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8276 - acc: 0.7327\n",
      "Epoch 00281: val_loss did not improve from 0.59680\n",
      "36805/36805 [==============================] - 28s 772us/sample - loss: 0.8276 - acc: 0.7327 - val_loss: 0.5980 - val_acc: 0.8150\n",
      "Epoch 282/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8214 - acc: 0.7352\n",
      "Epoch 00282: val_loss did not improve from 0.59680\n",
      "36805/36805 [==============================] - 28s 773us/sample - loss: 0.8213 - acc: 0.7352 - val_loss: 0.6003 - val_acc: 0.8253\n",
      "Epoch 283/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8177 - acc: 0.7369\n",
      "Epoch 00283: val_loss did not improve from 0.59680\n",
      "36805/36805 [==============================] - 28s 771us/sample - loss: 0.8178 - acc: 0.7369 - val_loss: 0.6147 - val_acc: 0.8150\n",
      "Epoch 284/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8189 - acc: 0.7361\n",
      "Epoch 00284: val_loss did not improve from 0.59680\n",
      "36805/36805 [==============================] - 28s 769us/sample - loss: 0.8189 - acc: 0.7360 - val_loss: 0.6145 - val_acc: 0.8148\n",
      "Epoch 285/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8182 - acc: 0.7353\n",
      "Epoch 00285: val_loss did not improve from 0.59680\n",
      "36805/36805 [==============================] - 28s 771us/sample - loss: 0.8182 - acc: 0.7353 - val_loss: 0.5991 - val_acc: 0.8213\n",
      "Epoch 286/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8190 - acc: 0.7335\n",
      "Epoch 00286: val_loss did not improve from 0.59680\n",
      "36805/36805 [==============================] - 28s 771us/sample - loss: 0.8190 - acc: 0.7335 - val_loss: 0.6007 - val_acc: 0.8197\n",
      "Epoch 287/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8189 - acc: 0.7359\n",
      "Epoch 00287: val_loss did not improve from 0.59680\n",
      "36805/36805 [==============================] - 28s 772us/sample - loss: 0.8189 - acc: 0.7359 - val_loss: 0.6013 - val_acc: 0.8199\n",
      "Epoch 288/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8190 - acc: 0.7351\n",
      "Epoch 00288: val_loss improved from 0.59680 to 0.59583, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_3_conv_checkpoint/288-0.5958.hdf5\n",
      "36805/36805 [==============================] - 28s 774us/sample - loss: 0.8189 - acc: 0.7351 - val_loss: 0.5958 - val_acc: 0.8225\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 289/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8127 - acc: 0.7382\n",
      "Epoch 00289: val_loss did not improve from 0.59583\n",
      "36805/36805 [==============================] - 28s 772us/sample - loss: 0.8129 - acc: 0.7381 - val_loss: 0.6089 - val_acc: 0.8176\n",
      "Epoch 290/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8199 - acc: 0.7368\n",
      "Epoch 00290: val_loss did not improve from 0.59583\n",
      "36805/36805 [==============================] - 28s 772us/sample - loss: 0.8199 - acc: 0.7368 - val_loss: 0.6041 - val_acc: 0.8183\n",
      "Epoch 291/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8146 - acc: 0.7358\n",
      "Epoch 00291: val_loss did not improve from 0.59583\n",
      "36805/36805 [==============================] - 28s 772us/sample - loss: 0.8146 - acc: 0.7358 - val_loss: 0.6007 - val_acc: 0.8195\n",
      "Epoch 292/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8163 - acc: 0.7371\n",
      "Epoch 00292: val_loss did not improve from 0.59583\n",
      "36805/36805 [==============================] - 29s 778us/sample - loss: 0.8160 - acc: 0.7371 - val_loss: 0.5994 - val_acc: 0.8195\n",
      "Epoch 293/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8103 - acc: 0.7417\n",
      "Epoch 00293: val_loss improved from 0.59583 to 0.59308, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_3_conv_checkpoint/293-0.5931.hdf5\n",
      "36805/36805 [==============================] - 29s 782us/sample - loss: 0.8102 - acc: 0.7417 - val_loss: 0.5931 - val_acc: 0.8209\n",
      "Epoch 294/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8199 - acc: 0.7344\n",
      "Epoch 00294: val_loss did not improve from 0.59308\n",
      "36805/36805 [==============================] - 29s 782us/sample - loss: 0.8199 - acc: 0.7344 - val_loss: 0.6001 - val_acc: 0.8232\n",
      "Epoch 295/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8112 - acc: 0.7380\n",
      "Epoch 00295: val_loss did not improve from 0.59308\n",
      "36805/36805 [==============================] - 28s 771us/sample - loss: 0.8112 - acc: 0.7381 - val_loss: 0.6017 - val_acc: 0.8185\n",
      "Epoch 296/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8114 - acc: 0.7389\n",
      "Epoch 00296: val_loss did not improve from 0.59308\n",
      "36805/36805 [==============================] - 28s 770us/sample - loss: 0.8114 - acc: 0.7389 - val_loss: 0.5965 - val_acc: 0.8202\n",
      "Epoch 297/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8198 - acc: 0.7333\n",
      "Epoch 00297: val_loss did not improve from 0.59308\n",
      "36805/36805 [==============================] - 28s 770us/sample - loss: 0.8200 - acc: 0.7332 - val_loss: 0.5952 - val_acc: 0.8234\n",
      "Epoch 298/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8159 - acc: 0.7348\n",
      "Epoch 00298: val_loss did not improve from 0.59308\n",
      "36805/36805 [==============================] - 28s 772us/sample - loss: 0.8159 - acc: 0.7348 - val_loss: 0.6027 - val_acc: 0.8199\n",
      "Epoch 299/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8102 - acc: 0.7386\n",
      "Epoch 00299: val_loss did not improve from 0.59308\n",
      "36805/36805 [==============================] - 28s 773us/sample - loss: 0.8102 - acc: 0.7386 - val_loss: 0.6019 - val_acc: 0.8218\n",
      "Epoch 300/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8157 - acc: 0.7372\n",
      "Epoch 00300: val_loss did not improve from 0.59308\n",
      "36805/36805 [==============================] - 28s 771us/sample - loss: 0.8159 - acc: 0.7371 - val_loss: 0.5965 - val_acc: 0.8211\n",
      "Epoch 301/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8117 - acc: 0.7368\n",
      "Epoch 00301: val_loss did not improve from 0.59308\n",
      "36805/36805 [==============================] - 28s 774us/sample - loss: 0.8117 - acc: 0.7368 - val_loss: 0.6008 - val_acc: 0.8220\n",
      "Epoch 302/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8170 - acc: 0.7384\n",
      "Epoch 00302: val_loss did not improve from 0.59308\n",
      "36805/36805 [==============================] - 28s 772us/sample - loss: 0.8170 - acc: 0.7384 - val_loss: 0.6070 - val_acc: 0.8160\n",
      "Epoch 303/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8085 - acc: 0.7387\n",
      "Epoch 00303: val_loss did not improve from 0.59308\n",
      "36805/36805 [==============================] - 28s 771us/sample - loss: 0.8084 - acc: 0.7387 - val_loss: 0.5964 - val_acc: 0.8232\n",
      "Epoch 304/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8034 - acc: 0.7404\n",
      "Epoch 00304: val_loss did not improve from 0.59308\n",
      "36805/36805 [==============================] - 28s 772us/sample - loss: 0.8034 - acc: 0.7403 - val_loss: 0.5954 - val_acc: 0.8220\n",
      "Epoch 305/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8119 - acc: 0.7387\n",
      "Epoch 00305: val_loss did not improve from 0.59308\n",
      "36805/36805 [==============================] - 28s 769us/sample - loss: 0.8119 - acc: 0.7388 - val_loss: 0.6048 - val_acc: 0.8188\n",
      "Epoch 306/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8056 - acc: 0.7417\n",
      "Epoch 00306: val_loss did not improve from 0.59308\n",
      "36805/36805 [==============================] - 28s 771us/sample - loss: 0.8053 - acc: 0.7417 - val_loss: 0.6041 - val_acc: 0.8199\n",
      "Epoch 307/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8071 - acc: 0.7408\n",
      "Epoch 00307: val_loss did not improve from 0.59308\n",
      "36805/36805 [==============================] - 28s 774us/sample - loss: 0.8071 - acc: 0.7408 - val_loss: 0.6022 - val_acc: 0.8174\n",
      "Epoch 308/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8142 - acc: 0.7382\n",
      "Epoch 00308: val_loss did not improve from 0.59308\n",
      "36805/36805 [==============================] - 28s 771us/sample - loss: 0.8143 - acc: 0.7381 - val_loss: 0.5932 - val_acc: 0.8225\n",
      "Epoch 309/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8092 - acc: 0.7416\n",
      "Epoch 00309: val_loss improved from 0.59308 to 0.59284, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_3_conv_checkpoint/309-0.5928.hdf5\n",
      "36805/36805 [==============================] - 29s 774us/sample - loss: 0.8092 - acc: 0.7416 - val_loss: 0.5928 - val_acc: 0.8225\n",
      "Epoch 310/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8115 - acc: 0.7390\n",
      "Epoch 00310: val_loss did not improve from 0.59284\n",
      "36805/36805 [==============================] - 28s 770us/sample - loss: 0.8117 - acc: 0.7390 - val_loss: 0.6013 - val_acc: 0.8183\n",
      "Epoch 311/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8114 - acc: 0.7374\n",
      "Epoch 00311: val_loss did not improve from 0.59284\n",
      "36805/36805 [==============================] - 28s 772us/sample - loss: 0.8114 - acc: 0.7374 - val_loss: 0.5982 - val_acc: 0.8199\n",
      "Epoch 312/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8068 - acc: 0.7412\n",
      "Epoch 00312: val_loss did not improve from 0.59284\n",
      "36805/36805 [==============================] - 28s 769us/sample - loss: 0.8069 - acc: 0.7412 - val_loss: 0.5989 - val_acc: 0.8234\n",
      "Epoch 313/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8070 - acc: 0.7368\n",
      "Epoch 00313: val_loss did not improve from 0.59284\n",
      "36805/36805 [==============================] - 29s 775us/sample - loss: 0.8072 - acc: 0.7368 - val_loss: 0.6074 - val_acc: 0.8153\n",
      "Epoch 314/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8045 - acc: 0.7389\n",
      "Epoch 00314: val_loss improved from 0.59284 to 0.58854, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_3_conv_checkpoint/314-0.5885.hdf5\n",
      "36805/36805 [==============================] - 28s 774us/sample - loss: 0.8045 - acc: 0.7389 - val_loss: 0.5885 - val_acc: 0.8227\n",
      "Epoch 315/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8068 - acc: 0.7416\n",
      "Epoch 00315: val_loss improved from 0.58854 to 0.58685, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_3_conv_checkpoint/315-0.5868.hdf5\n",
      "36805/36805 [==============================] - 28s 774us/sample - loss: 0.8068 - acc: 0.7416 - val_loss: 0.5868 - val_acc: 0.8239\n",
      "Epoch 316/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8016 - acc: 0.7393\n",
      "Epoch 00316: val_loss did not improve from 0.58685\n",
      "36805/36805 [==============================] - 28s 772us/sample - loss: 0.8015 - acc: 0.7394 - val_loss: 0.5952 - val_acc: 0.8197\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 317/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8026 - acc: 0.7395\n",
      "Epoch 00317: val_loss did not improve from 0.58685\n",
      "36805/36805 [==============================] - 28s 773us/sample - loss: 0.8026 - acc: 0.7395 - val_loss: 0.5892 - val_acc: 0.8244\n",
      "Epoch 318/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8012 - acc: 0.7386\n",
      "Epoch 00318: val_loss did not improve from 0.58685\n",
      "36805/36805 [==============================] - 28s 774us/sample - loss: 0.8012 - acc: 0.7386 - val_loss: 0.5937 - val_acc: 0.8223\n",
      "Epoch 319/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8058 - acc: 0.7400\n",
      "Epoch 00319: val_loss did not improve from 0.58685\n",
      "36805/36805 [==============================] - 28s 773us/sample - loss: 0.8056 - acc: 0.7400 - val_loss: 0.5908 - val_acc: 0.8260\n",
      "Epoch 320/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8039 - acc: 0.7439\n",
      "Epoch 00320: val_loss did not improve from 0.58685\n",
      "36805/36805 [==============================] - 28s 772us/sample - loss: 0.8038 - acc: 0.7439 - val_loss: 0.5894 - val_acc: 0.8227\n",
      "Epoch 321/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8129 - acc: 0.7394\n",
      "Epoch 00321: val_loss did not improve from 0.58685\n",
      "36805/36805 [==============================] - 28s 772us/sample - loss: 0.8129 - acc: 0.7394 - val_loss: 0.6008 - val_acc: 0.8246\n",
      "Epoch 322/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8058 - acc: 0.7394\n",
      "Epoch 00322: val_loss did not improve from 0.58685\n",
      "36805/36805 [==============================] - 28s 773us/sample - loss: 0.8058 - acc: 0.7394 - val_loss: 0.5951 - val_acc: 0.8218\n",
      "Epoch 323/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7956 - acc: 0.7417\n",
      "Epoch 00323: val_loss did not improve from 0.58685\n",
      "36805/36805 [==============================] - 28s 774us/sample - loss: 0.7956 - acc: 0.7416 - val_loss: 0.5966 - val_acc: 0.8227\n",
      "Epoch 324/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8081 - acc: 0.7386\n",
      "Epoch 00324: val_loss did not improve from 0.58685\n",
      "36805/36805 [==============================] - 28s 772us/sample - loss: 0.8081 - acc: 0.7386 - val_loss: 0.5941 - val_acc: 0.8209\n",
      "Epoch 325/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7973 - acc: 0.7404\n",
      "Epoch 00325: val_loss did not improve from 0.58685\n",
      "36805/36805 [==============================] - 28s 773us/sample - loss: 0.7973 - acc: 0.7403 - val_loss: 0.5966 - val_acc: 0.8244\n",
      "Epoch 326/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8016 - acc: 0.7426\n",
      "Epoch 00326: val_loss did not improve from 0.58685\n",
      "36805/36805 [==============================] - 28s 772us/sample - loss: 0.8016 - acc: 0.7426 - val_loss: 0.5943 - val_acc: 0.8202\n",
      "Epoch 327/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7997 - acc: 0.7408\n",
      "Epoch 00327: val_loss did not improve from 0.58685\n",
      "36805/36805 [==============================] - 28s 773us/sample - loss: 0.7997 - acc: 0.7408 - val_loss: 0.5922 - val_acc: 0.8248\n",
      "Epoch 328/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7930 - acc: 0.7425\n",
      "Epoch 00328: val_loss did not improve from 0.58685\n",
      "36805/36805 [==============================] - 28s 772us/sample - loss: 0.7930 - acc: 0.7425 - val_loss: 0.5895 - val_acc: 0.8225\n",
      "Epoch 329/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8031 - acc: 0.7398\n",
      "Epoch 00329: val_loss did not improve from 0.58685\n",
      "36805/36805 [==============================] - 28s 773us/sample - loss: 0.8031 - acc: 0.7398 - val_loss: 0.6045 - val_acc: 0.8192\n",
      "Epoch 330/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8065 - acc: 0.7379\n",
      "Epoch 00330: val_loss did not improve from 0.58685\n",
      "36805/36805 [==============================] - 28s 769us/sample - loss: 0.8065 - acc: 0.7379 - val_loss: 0.5966 - val_acc: 0.8206\n",
      "Epoch 331/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7968 - acc: 0.7466\n",
      "Epoch 00331: val_loss did not improve from 0.58685\n",
      "36805/36805 [==============================] - 28s 773us/sample - loss: 0.7967 - acc: 0.7466 - val_loss: 0.5945 - val_acc: 0.8239\n",
      "Epoch 332/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7989 - acc: 0.7420\n",
      "Epoch 00332: val_loss did not improve from 0.58685\n",
      "36805/36805 [==============================] - 28s 773us/sample - loss: 0.7990 - acc: 0.7419 - val_loss: 0.5885 - val_acc: 0.8286\n",
      "Epoch 333/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7956 - acc: 0.7442\n",
      "Epoch 00333: val_loss did not improve from 0.58685\n",
      "36805/36805 [==============================] - 28s 772us/sample - loss: 0.7955 - acc: 0.7442 - val_loss: 0.5959 - val_acc: 0.8248\n",
      "Epoch 334/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7990 - acc: 0.7438\n",
      "Epoch 00334: val_loss did not improve from 0.58685\n",
      "36805/36805 [==============================] - 28s 773us/sample - loss: 0.7989 - acc: 0.7438 - val_loss: 0.5983 - val_acc: 0.8230\n",
      "Epoch 335/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7953 - acc: 0.7438\n",
      "Epoch 00335: val_loss improved from 0.58685 to 0.58500, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_3_conv_checkpoint/335-0.5850.hdf5\n",
      "36805/36805 [==============================] - 29s 776us/sample - loss: 0.7953 - acc: 0.7438 - val_loss: 0.5850 - val_acc: 0.8237\n",
      "Epoch 336/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7955 - acc: 0.7433\n",
      "Epoch 00336: val_loss did not improve from 0.58500\n",
      "36805/36805 [==============================] - 29s 775us/sample - loss: 0.7955 - acc: 0.7433 - val_loss: 0.5959 - val_acc: 0.8192\n",
      "Epoch 337/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7960 - acc: 0.7420\n",
      "Epoch 00337: val_loss improved from 0.58500 to 0.58486, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_3_conv_checkpoint/337-0.5849.hdf5\n",
      "36805/36805 [==============================] - 28s 773us/sample - loss: 0.7961 - acc: 0.7420 - val_loss: 0.5849 - val_acc: 0.8239\n",
      "Epoch 338/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7978 - acc: 0.7436\n",
      "Epoch 00338: val_loss did not improve from 0.58486\n",
      "36805/36805 [==============================] - 28s 771us/sample - loss: 0.7978 - acc: 0.7435 - val_loss: 0.5874 - val_acc: 0.8255\n",
      "Epoch 339/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7946 - acc: 0.7450\n",
      "Epoch 00339: val_loss did not improve from 0.58486\n",
      "36805/36805 [==============================] - 28s 771us/sample - loss: 0.7946 - acc: 0.7450 - val_loss: 0.5849 - val_acc: 0.8272\n",
      "Epoch 340/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7882 - acc: 0.7461\n",
      "Epoch 00340: val_loss did not improve from 0.58486\n",
      "36805/36805 [==============================] - 28s 774us/sample - loss: 0.7881 - acc: 0.7461 - val_loss: 0.5933 - val_acc: 0.8230\n",
      "Epoch 341/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7969 - acc: 0.7437\n",
      "Epoch 00341: val_loss did not improve from 0.58486\n",
      "36805/36805 [==============================] - 28s 772us/sample - loss: 0.7969 - acc: 0.7437 - val_loss: 0.5885 - val_acc: 0.8220\n",
      "Epoch 342/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7929 - acc: 0.7434\n",
      "Epoch 00342: val_loss improved from 0.58486 to 0.58148, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_3_conv_checkpoint/342-0.5815.hdf5\n",
      "36805/36805 [==============================] - 28s 773us/sample - loss: 0.7929 - acc: 0.7434 - val_loss: 0.5815 - val_acc: 0.8253\n",
      "Epoch 343/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7974 - acc: 0.7414\n",
      "Epoch 00343: val_loss did not improve from 0.58148\n",
      "36805/36805 [==============================] - 28s 772us/sample - loss: 0.7973 - acc: 0.7414 - val_loss: 0.5880 - val_acc: 0.8234\n",
      "Epoch 344/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7912 - acc: 0.7445\n",
      "Epoch 00344: val_loss did not improve from 0.58148\n",
      "36805/36805 [==============================] - 28s 772us/sample - loss: 0.7912 - acc: 0.7445 - val_loss: 0.5872 - val_acc: 0.8230\n",
      "Epoch 345/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7888 - acc: 0.7450\n",
      "Epoch 00345: val_loss did not improve from 0.58148\n",
      "36805/36805 [==============================] - 28s 772us/sample - loss: 0.7887 - acc: 0.7450 - val_loss: 0.5980 - val_acc: 0.8174\n",
      "Epoch 346/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7943 - acc: 0.7437\n",
      "Epoch 00346: val_loss did not improve from 0.58148\n",
      "36805/36805 [==============================] - 28s 770us/sample - loss: 0.7940 - acc: 0.7438 - val_loss: 0.5996 - val_acc: 0.8211\n",
      "Epoch 347/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7868 - acc: 0.7436\n",
      "Epoch 00347: val_loss did not improve from 0.58148\n",
      "36805/36805 [==============================] - 28s 772us/sample - loss: 0.7870 - acc: 0.7435 - val_loss: 0.5889 - val_acc: 0.8248\n",
      "Epoch 348/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7973 - acc: 0.7424\n",
      "Epoch 00348: val_loss did not improve from 0.58148\n",
      "36805/36805 [==============================] - 28s 772us/sample - loss: 0.7967 - acc: 0.7425 - val_loss: 0.5929 - val_acc: 0.8225\n",
      "Epoch 349/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7892 - acc: 0.7443\n",
      "Epoch 00349: val_loss did not improve from 0.58148\n",
      "36805/36805 [==============================] - 28s 768us/sample - loss: 0.7891 - acc: 0.7444 - val_loss: 0.6006 - val_acc: 0.8209\n",
      "Epoch 350/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7924 - acc: 0.7444\n",
      "Epoch 00350: val_loss did not improve from 0.58148\n",
      "36805/36805 [==============================] - 28s 773us/sample - loss: 0.7924 - acc: 0.7444 - val_loss: 0.5928 - val_acc: 0.8241\n",
      "Epoch 351/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7929 - acc: 0.7451\n",
      "Epoch 00351: val_loss did not improve from 0.58148\n",
      "36805/36805 [==============================] - 28s 773us/sample - loss: 0.7929 - acc: 0.7451 - val_loss: 0.5855 - val_acc: 0.8269\n",
      "Epoch 352/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7906 - acc: 0.7463\n",
      "Epoch 00352: val_loss did not improve from 0.58148\n",
      "36805/36805 [==============================] - 28s 773us/sample - loss: 0.7905 - acc: 0.7463 - val_loss: 0.5905 - val_acc: 0.8227\n",
      "Epoch 353/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7927 - acc: 0.7470\n",
      "Epoch 00353: val_loss did not improve from 0.58148\n",
      "36805/36805 [==============================] - 28s 772us/sample - loss: 0.7926 - acc: 0.7470 - val_loss: 0.5952 - val_acc: 0.8234\n",
      "Epoch 354/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7893 - acc: 0.7442\n",
      "Epoch 00354: val_loss did not improve from 0.58148\n",
      "36805/36805 [==============================] - 28s 772us/sample - loss: 0.7893 - acc: 0.7441 - val_loss: 0.5882 - val_acc: 0.8199\n",
      "Epoch 355/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7855 - acc: 0.7448\n",
      "Epoch 00355: val_loss did not improve from 0.58148\n",
      "36805/36805 [==============================] - 29s 780us/sample - loss: 0.7852 - acc: 0.7448 - val_loss: 0.5857 - val_acc: 0.8260\n",
      "Epoch 356/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7850 - acc: 0.7468\n",
      "Epoch 00356: val_loss did not improve from 0.58148\n",
      "36805/36805 [==============================] - 28s 771us/sample - loss: 0.7850 - acc: 0.7468 - val_loss: 0.5883 - val_acc: 0.8225\n",
      "Epoch 357/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7883 - acc: 0.7442\n",
      "Epoch 00357: val_loss did not improve from 0.58148\n",
      "36805/36805 [==============================] - 28s 772us/sample - loss: 0.7883 - acc: 0.7442 - val_loss: 0.5845 - val_acc: 0.8255\n",
      "Epoch 358/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7938 - acc: 0.7426\n",
      "Epoch 00358: val_loss did not improve from 0.58148\n",
      "36805/36805 [==============================] - 28s 773us/sample - loss: 0.7940 - acc: 0.7426 - val_loss: 0.6033 - val_acc: 0.8188\n",
      "Epoch 359/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7870 - acc: 0.7467\n",
      "Epoch 00359: val_loss did not improve from 0.58148\n",
      "36805/36805 [==============================] - 28s 772us/sample - loss: 0.7870 - acc: 0.7467 - val_loss: 0.5882 - val_acc: 0.8265\n",
      "Epoch 360/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7874 - acc: 0.7429\n",
      "Epoch 00360: val_loss did not improve from 0.58148\n",
      "36805/36805 [==============================] - 28s 773us/sample - loss: 0.7874 - acc: 0.7429 - val_loss: 0.5869 - val_acc: 0.8267\n",
      "Epoch 361/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7906 - acc: 0.7449\n",
      "Epoch 00361: val_loss did not improve from 0.58148\n",
      "36805/36805 [==============================] - 28s 773us/sample - loss: 0.7906 - acc: 0.7449 - val_loss: 0.5831 - val_acc: 0.8251\n",
      "Epoch 362/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7877 - acc: 0.7441\n",
      "Epoch 00362: val_loss did not improve from 0.58148\n",
      "36805/36805 [==============================] - 28s 774us/sample - loss: 0.7878 - acc: 0.7441 - val_loss: 0.5937 - val_acc: 0.8204\n",
      "Epoch 363/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7842 - acc: 0.7447\n",
      "Epoch 00363: val_loss did not improve from 0.58148\n",
      "36805/36805 [==============================] - 28s 773us/sample - loss: 0.7842 - acc: 0.7446 - val_loss: 0.5869 - val_acc: 0.8251\n",
      "Epoch 364/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7858 - acc: 0.7464\n",
      "Epoch 00364: val_loss did not improve from 0.58148\n",
      "36805/36805 [==============================] - 28s 772us/sample - loss: 0.7859 - acc: 0.7464 - val_loss: 0.5823 - val_acc: 0.8225\n",
      "Epoch 365/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7859 - acc: 0.7460\n",
      "Epoch 00365: val_loss did not improve from 0.58148\n",
      "36805/36805 [==============================] - 28s 770us/sample - loss: 0.7861 - acc: 0.7459 - val_loss: 0.5871 - val_acc: 0.8253\n",
      "Epoch 366/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7838 - acc: 0.7479\n",
      "Epoch 00366: val_loss did not improve from 0.58148\n",
      "36805/36805 [==============================] - 28s 773us/sample - loss: 0.7834 - acc: 0.7480 - val_loss: 0.5856 - val_acc: 0.8258\n",
      "Epoch 367/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7802 - acc: 0.7478\n",
      "Epoch 00367: val_loss did not improve from 0.58148\n",
      "36805/36805 [==============================] - 28s 771us/sample - loss: 0.7802 - acc: 0.7478 - val_loss: 0.5828 - val_acc: 0.8307\n",
      "Epoch 368/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7843 - acc: 0.7471\n",
      "Epoch 00368: val_loss did not improve from 0.58148\n",
      "36805/36805 [==============================] - 28s 772us/sample - loss: 0.7843 - acc: 0.7471 - val_loss: 0.5883 - val_acc: 0.8239\n",
      "Epoch 369/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7883 - acc: 0.7436\n",
      "Epoch 00369: val_loss did not improve from 0.58148\n",
      "36805/36805 [==============================] - 28s 774us/sample - loss: 0.7883 - acc: 0.7436 - val_loss: 0.5914 - val_acc: 0.8251\n",
      "Epoch 370/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7812 - acc: 0.7477\n",
      "Epoch 00370: val_loss did not improve from 0.58148\n",
      "36805/36805 [==============================] - 28s 772us/sample - loss: 0.7812 - acc: 0.7477 - val_loss: 0.5835 - val_acc: 0.8321\n",
      "Epoch 371/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7796 - acc: 0.7458\n",
      "Epoch 00371: val_loss did not improve from 0.58148\n",
      "36805/36805 [==============================] - 28s 772us/sample - loss: 0.7799 - acc: 0.7458 - val_loss: 0.5889 - val_acc: 0.8267\n",
      "Epoch 372/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7850 - acc: 0.7455\n",
      "Epoch 00372: val_loss did not improve from 0.58148\n",
      "36805/36805 [==============================] - 28s 772us/sample - loss: 0.7852 - acc: 0.7454 - val_loss: 0.5886 - val_acc: 0.8269\n",
      "Epoch 373/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7809 - acc: 0.7479\n",
      "Epoch 00373: val_loss did not improve from 0.58148\n",
      "36805/36805 [==============================] - 29s 782us/sample - loss: 0.7810 - acc: 0.7478 - val_loss: 0.5888 - val_acc: 0.8251\n",
      "Epoch 374/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7830 - acc: 0.7459\n",
      "Epoch 00374: val_loss did not improve from 0.58148\n",
      "36805/36805 [==============================] - 29s 780us/sample - loss: 0.7833 - acc: 0.7459 - val_loss: 0.5951 - val_acc: 0.8204\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 375/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7740 - acc: 0.7483\n",
      "Epoch 00375: val_loss did not improve from 0.58148\n",
      "36805/36805 [==============================] - 29s 780us/sample - loss: 0.7740 - acc: 0.7482 - val_loss: 0.5818 - val_acc: 0.8279\n",
      "Epoch 376/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7806 - acc: 0.7476\n",
      "Epoch 00376: val_loss did not improve from 0.58148\n",
      "36805/36805 [==============================] - 29s 780us/sample - loss: 0.7805 - acc: 0.7476 - val_loss: 0.5866 - val_acc: 0.8267\n",
      "Epoch 377/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7759 - acc: 0.7482\n",
      "Epoch 00377: val_loss did not improve from 0.58148\n",
      "36805/36805 [==============================] - 28s 773us/sample - loss: 0.7759 - acc: 0.7482 - val_loss: 0.5865 - val_acc: 0.8237\n",
      "Epoch 378/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7796 - acc: 0.7471\n",
      "Epoch 00378: val_loss did not improve from 0.58148\n",
      "36805/36805 [==============================] - 28s 770us/sample - loss: 0.7799 - acc: 0.7470 - val_loss: 0.5843 - val_acc: 0.8265\n",
      "Epoch 379/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7742 - acc: 0.7517\n",
      "Epoch 00379: val_loss did not improve from 0.58148\n",
      "36805/36805 [==============================] - 28s 773us/sample - loss: 0.7743 - acc: 0.7517 - val_loss: 0.5954 - val_acc: 0.8230\n",
      "Epoch 380/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7784 - acc: 0.7491\n",
      "Epoch 00380: val_loss did not improve from 0.58148\n",
      "36805/36805 [==============================] - 28s 773us/sample - loss: 0.7784 - acc: 0.7491 - val_loss: 0.5818 - val_acc: 0.8283\n",
      "Epoch 381/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7795 - acc: 0.7496\n",
      "Epoch 00381: val_loss did not improve from 0.58148\n",
      "36805/36805 [==============================] - 29s 775us/sample - loss: 0.7794 - acc: 0.7496 - val_loss: 0.5895 - val_acc: 0.8262\n",
      "Epoch 382/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7773 - acc: 0.7490\n",
      "Epoch 00382: val_loss did not improve from 0.58148\n",
      "36805/36805 [==============================] - 28s 774us/sample - loss: 0.7775 - acc: 0.7490 - val_loss: 0.6029 - val_acc: 0.8178\n",
      "Epoch 383/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7753 - acc: 0.7485\n",
      "Epoch 00383: val_loss did not improve from 0.58148\n",
      "36805/36805 [==============================] - 29s 776us/sample - loss: 0.7753 - acc: 0.7485 - val_loss: 0.5835 - val_acc: 0.8244\n",
      "Epoch 384/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7713 - acc: 0.7487\n",
      "Epoch 00384: val_loss did not improve from 0.58148\n",
      "36805/36805 [==============================] - 28s 772us/sample - loss: 0.7712 - acc: 0.7487 - val_loss: 0.5872 - val_acc: 0.8220\n",
      "Epoch 385/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7766 - acc: 0.7504\n",
      "Epoch 00385: val_loss improved from 0.58148 to 0.58043, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_3_conv_checkpoint/385-0.5804.hdf5\n",
      "36805/36805 [==============================] - 28s 772us/sample - loss: 0.7767 - acc: 0.7504 - val_loss: 0.5804 - val_acc: 0.8255\n",
      "Epoch 386/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7729 - acc: 0.7500\n",
      "Epoch 00386: val_loss did not improve from 0.58043\n",
      "36805/36805 [==============================] - 28s 771us/sample - loss: 0.7729 - acc: 0.7500 - val_loss: 0.5870 - val_acc: 0.8223\n",
      "Epoch 387/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7730 - acc: 0.7477\n",
      "Epoch 00387: val_loss did not improve from 0.58043\n",
      "36805/36805 [==============================] - 28s 767us/sample - loss: 0.7731 - acc: 0.7476 - val_loss: 0.5931 - val_acc: 0.8237\n",
      "Epoch 388/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7788 - acc: 0.7455\n",
      "Epoch 00388: val_loss improved from 0.58043 to 0.57959, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_3_conv_checkpoint/388-0.5796.hdf5\n",
      "36805/36805 [==============================] - 28s 773us/sample - loss: 0.7787 - acc: 0.7456 - val_loss: 0.5796 - val_acc: 0.8279\n",
      "Epoch 389/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7775 - acc: 0.7495\n",
      "Epoch 00389: val_loss did not improve from 0.57959\n",
      "36805/36805 [==============================] - 28s 771us/sample - loss: 0.7775 - acc: 0.7495 - val_loss: 0.5893 - val_acc: 0.8267\n",
      "Epoch 390/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7778 - acc: 0.7490\n",
      "Epoch 00390: val_loss did not improve from 0.57959\n",
      "36805/36805 [==============================] - 28s 770us/sample - loss: 0.7773 - acc: 0.7491 - val_loss: 0.5802 - val_acc: 0.8260\n",
      "Epoch 391/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7786 - acc: 0.7475\n",
      "Epoch 00391: val_loss did not improve from 0.57959\n",
      "36805/36805 [==============================] - 28s 772us/sample - loss: 0.7785 - acc: 0.7476 - val_loss: 0.5852 - val_acc: 0.8260\n",
      "Epoch 392/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7754 - acc: 0.7485\n",
      "Epoch 00392: val_loss improved from 0.57959 to 0.57926, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_3_conv_checkpoint/392-0.5793.hdf5\n",
      "36805/36805 [==============================] - 28s 772us/sample - loss: 0.7753 - acc: 0.7486 - val_loss: 0.5793 - val_acc: 0.8300\n",
      "Epoch 393/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7725 - acc: 0.7494\n",
      "Epoch 00393: val_loss did not improve from 0.57926\n",
      "36805/36805 [==============================] - 28s 772us/sample - loss: 0.7725 - acc: 0.7494 - val_loss: 0.5877 - val_acc: 0.8220\n",
      "Epoch 394/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7777 - acc: 0.7474\n",
      "Epoch 00394: val_loss did not improve from 0.57926\n",
      "36805/36805 [==============================] - 28s 770us/sample - loss: 0.7777 - acc: 0.7474 - val_loss: 0.5849 - val_acc: 0.8274\n",
      "Epoch 395/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7725 - acc: 0.7494\n",
      "Epoch 00395: val_loss did not improve from 0.57926\n",
      "36805/36805 [==============================] - 28s 772us/sample - loss: 0.7725 - acc: 0.7495 - val_loss: 0.5858 - val_acc: 0.8251\n",
      "Epoch 396/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7784 - acc: 0.7462\n",
      "Epoch 00396: val_loss did not improve from 0.57926\n",
      "36805/36805 [==============================] - 28s 772us/sample - loss: 0.7786 - acc: 0.7461 - val_loss: 0.5863 - val_acc: 0.8225\n",
      "Epoch 397/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7821 - acc: 0.7494\n",
      "Epoch 00397: val_loss did not improve from 0.57926\n",
      "36805/36805 [==============================] - 28s 773us/sample - loss: 0.7822 - acc: 0.7494 - val_loss: 0.5882 - val_acc: 0.8265\n",
      "Epoch 398/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7758 - acc: 0.7481\n",
      "Epoch 00398: val_loss did not improve from 0.57926\n",
      "36805/36805 [==============================] - 28s 773us/sample - loss: 0.7760 - acc: 0.7482 - val_loss: 0.5958 - val_acc: 0.8248\n",
      "Epoch 399/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7639 - acc: 0.7510\n",
      "Epoch 00399: val_loss improved from 0.57926 to 0.57848, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_3_conv_checkpoint/399-0.5785.hdf5\n",
      "36805/36805 [==============================] - 28s 773us/sample - loss: 0.7641 - acc: 0.7510 - val_loss: 0.5785 - val_acc: 0.8314\n",
      "Epoch 400/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7730 - acc: 0.7498\n",
      "Epoch 00400: val_loss did not improve from 0.57848\n",
      "36805/36805 [==============================] - 28s 772us/sample - loss: 0.7729 - acc: 0.7499 - val_loss: 0.5788 - val_acc: 0.8302\n",
      "Epoch 401/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7730 - acc: 0.7491\n",
      "Epoch 00401: val_loss did not improve from 0.57848\n",
      "36805/36805 [==============================] - 28s 772us/sample - loss: 0.7729 - acc: 0.7491 - val_loss: 0.5862 - val_acc: 0.8269\n",
      "Epoch 402/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7679 - acc: 0.7491\n",
      "Epoch 00402: val_loss did not improve from 0.57848\n",
      "36805/36805 [==============================] - 28s 770us/sample - loss: 0.7678 - acc: 0.7491 - val_loss: 0.5809 - val_acc: 0.8302\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 403/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7772 - acc: 0.7514\n",
      "Epoch 00403: val_loss did not improve from 0.57848\n",
      "36805/36805 [==============================] - 28s 773us/sample - loss: 0.7772 - acc: 0.7514 - val_loss: 0.5879 - val_acc: 0.8255\n",
      "Epoch 404/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7734 - acc: 0.7488\n",
      "Epoch 00404: val_loss did not improve from 0.57848\n",
      "36805/36805 [==============================] - 28s 770us/sample - loss: 0.7733 - acc: 0.7488 - val_loss: 0.5978 - val_acc: 0.8237\n",
      "Epoch 405/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7736 - acc: 0.7480\n",
      "Epoch 00405: val_loss did not improve from 0.57848\n",
      "36805/36805 [==============================] - 28s 771us/sample - loss: 0.7736 - acc: 0.7480 - val_loss: 0.5790 - val_acc: 0.8255\n",
      "Epoch 406/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7714 - acc: 0.7483\n",
      "Epoch 00406: val_loss did not improve from 0.57848\n",
      "36805/36805 [==============================] - 28s 773us/sample - loss: 0.7714 - acc: 0.7484 - val_loss: 0.5914 - val_acc: 0.8218\n",
      "Epoch 407/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7713 - acc: 0.7486\n",
      "Epoch 00407: val_loss did not improve from 0.57848\n",
      "36805/36805 [==============================] - 28s 773us/sample - loss: 0.7713 - acc: 0.7486 - val_loss: 0.5856 - val_acc: 0.8269\n",
      "Epoch 408/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7705 - acc: 0.7501\n",
      "Epoch 00408: val_loss did not improve from 0.57848\n",
      "36805/36805 [==============================] - 28s 771us/sample - loss: 0.7705 - acc: 0.7501 - val_loss: 0.5869 - val_acc: 0.8262\n",
      "Epoch 409/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7671 - acc: 0.7484\n",
      "Epoch 00409: val_loss did not improve from 0.57848\n",
      "36805/36805 [==============================] - 28s 774us/sample - loss: 0.7671 - acc: 0.7484 - val_loss: 0.5908 - val_acc: 0.8251\n",
      "Epoch 410/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7685 - acc: 0.7537\n",
      "Epoch 00410: val_loss did not improve from 0.57848\n",
      "36805/36805 [==============================] - 28s 773us/sample - loss: 0.7684 - acc: 0.7537 - val_loss: 0.5845 - val_acc: 0.8290\n",
      "Epoch 411/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7692 - acc: 0.7492\n",
      "Epoch 00411: val_loss did not improve from 0.57848\n",
      "36805/36805 [==============================] - 28s 772us/sample - loss: 0.7692 - acc: 0.7493 - val_loss: 0.5789 - val_acc: 0.8246\n",
      "Epoch 412/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7696 - acc: 0.7515\n",
      "Epoch 00412: val_loss did not improve from 0.57848\n",
      "36805/36805 [==============================] - 28s 773us/sample - loss: 0.7698 - acc: 0.7514 - val_loss: 0.5877 - val_acc: 0.8253\n",
      "Epoch 413/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7731 - acc: 0.7509\n",
      "Epoch 00413: val_loss did not improve from 0.57848\n",
      "36805/36805 [==============================] - 28s 772us/sample - loss: 0.7732 - acc: 0.7509 - val_loss: 0.5952 - val_acc: 0.8246\n",
      "Epoch 414/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7580 - acc: 0.7513\n",
      "Epoch 00414: val_loss did not improve from 0.57848\n",
      "36805/36805 [==============================] - 28s 771us/sample - loss: 0.7581 - acc: 0.7512 - val_loss: 0.5833 - val_acc: 0.8230\n",
      "Epoch 415/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7672 - acc: 0.7530\n",
      "Epoch 00415: val_loss did not improve from 0.57848\n",
      "36805/36805 [==============================] - 28s 772us/sample - loss: 0.7672 - acc: 0.7530 - val_loss: 0.5818 - val_acc: 0.8274\n",
      "Epoch 416/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7711 - acc: 0.7500\n",
      "Epoch 00416: val_loss did not improve from 0.57848\n",
      "36805/36805 [==============================] - 28s 771us/sample - loss: 0.7711 - acc: 0.7500 - val_loss: 0.5823 - val_acc: 0.8255\n",
      "Epoch 417/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7703 - acc: 0.7521\n",
      "Epoch 00417: val_loss did not improve from 0.57848\n",
      "36805/36805 [==============================] - 28s 773us/sample - loss: 0.7704 - acc: 0.7522 - val_loss: 0.5898 - val_acc: 0.8269\n",
      "Epoch 418/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7702 - acc: 0.7499\n",
      "Epoch 00418: val_loss did not improve from 0.57848\n",
      "36805/36805 [==============================] - 28s 773us/sample - loss: 0.7703 - acc: 0.7499 - val_loss: 0.5901 - val_acc: 0.8244\n",
      "Epoch 419/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7702 - acc: 0.7504\n",
      "Epoch 00419: val_loss did not improve from 0.57848\n",
      "36805/36805 [==============================] - 28s 769us/sample - loss: 0.7702 - acc: 0.7505 - val_loss: 0.5799 - val_acc: 0.8253\n",
      "Epoch 420/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7669 - acc: 0.7501\n",
      "Epoch 00420: val_loss did not improve from 0.57848\n",
      "36805/36805 [==============================] - 28s 770us/sample - loss: 0.7668 - acc: 0.7501 - val_loss: 0.5869 - val_acc: 0.8246\n",
      "Epoch 421/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7650 - acc: 0.7526\n",
      "Epoch 00421: val_loss did not improve from 0.57848\n",
      "36805/36805 [==============================] - 28s 773us/sample - loss: 0.7649 - acc: 0.7526 - val_loss: 0.5893 - val_acc: 0.8253\n",
      "Epoch 422/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7660 - acc: 0.7513\n",
      "Epoch 00422: val_loss did not improve from 0.57848\n",
      "36805/36805 [==============================] - 28s 771us/sample - loss: 0.7659 - acc: 0.7513 - val_loss: 0.5803 - val_acc: 0.8265\n",
      "Epoch 423/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7668 - acc: 0.7536\n",
      "Epoch 00423: val_loss did not improve from 0.57848\n",
      "36805/36805 [==============================] - 28s 768us/sample - loss: 0.7668 - acc: 0.7536 - val_loss: 0.5861 - val_acc: 0.8246\n",
      "Epoch 424/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7617 - acc: 0.7536\n",
      "Epoch 00424: val_loss improved from 0.57848 to 0.57513, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_3_conv_checkpoint/424-0.5751.hdf5\n",
      "36805/36805 [==============================] - 29s 783us/sample - loss: 0.7619 - acc: 0.7536 - val_loss: 0.5751 - val_acc: 0.8260\n",
      "Epoch 425/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7619 - acc: 0.7530\n",
      "Epoch 00425: val_loss did not improve from 0.57513\n",
      "36805/36805 [==============================] - 28s 770us/sample - loss: 0.7618 - acc: 0.7530 - val_loss: 0.5789 - val_acc: 0.8290\n",
      "Epoch 426/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7727 - acc: 0.7514\n",
      "Epoch 00426: val_loss did not improve from 0.57513\n",
      "36805/36805 [==============================] - 28s 769us/sample - loss: 0.7727 - acc: 0.7514 - val_loss: 0.5978 - val_acc: 0.8195\n",
      "Epoch 427/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7574 - acc: 0.7527\n",
      "Epoch 00427: val_loss did not improve from 0.57513\n",
      "36805/36805 [==============================] - 28s 772us/sample - loss: 0.7573 - acc: 0.7528 - val_loss: 0.5757 - val_acc: 0.8274\n",
      "Epoch 428/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7649 - acc: 0.7529\n",
      "Epoch 00428: val_loss did not improve from 0.57513\n",
      "36805/36805 [==============================] - 28s 774us/sample - loss: 0.7646 - acc: 0.7529 - val_loss: 0.5869 - val_acc: 0.8274\n",
      "Epoch 429/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7635 - acc: 0.7521\n",
      "Epoch 00429: val_loss did not improve from 0.57513\n",
      "36805/36805 [==============================] - 28s 773us/sample - loss: 0.7635 - acc: 0.7521 - val_loss: 0.5803 - val_acc: 0.8279\n",
      "Epoch 430/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7602 - acc: 0.7549\n",
      "Epoch 00430: val_loss improved from 0.57513 to 0.57397, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_3_conv_checkpoint/430-0.5740.hdf5\n",
      "36805/36805 [==============================] - 28s 774us/sample - loss: 0.7603 - acc: 0.7549 - val_loss: 0.5740 - val_acc: 0.8304\n",
      "Epoch 431/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7625 - acc: 0.7524\n",
      "Epoch 00431: val_loss did not improve from 0.57397\n",
      "36805/36805 [==============================] - 28s 772us/sample - loss: 0.7626 - acc: 0.7523 - val_loss: 0.5807 - val_acc: 0.8290\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 432/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7559 - acc: 0.7543\n",
      "Epoch 00432: val_loss did not improve from 0.57397\n",
      "36805/36805 [==============================] - 28s 773us/sample - loss: 0.7558 - acc: 0.7544 - val_loss: 0.5843 - val_acc: 0.8241\n",
      "Epoch 433/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7598 - acc: 0.7531\n",
      "Epoch 00433: val_loss did not improve from 0.57397\n",
      "36805/36805 [==============================] - 28s 772us/sample - loss: 0.7598 - acc: 0.7531 - val_loss: 0.5808 - val_acc: 0.8295\n",
      "Epoch 434/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7659 - acc: 0.7514\n",
      "Epoch 00434: val_loss did not improve from 0.57397\n",
      "36805/36805 [==============================] - 29s 775us/sample - loss: 0.7661 - acc: 0.7514 - val_loss: 0.5994 - val_acc: 0.8213\n",
      "Epoch 435/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7638 - acc: 0.7522\n",
      "Epoch 00435: val_loss did not improve from 0.57397\n",
      "36805/36805 [==============================] - 28s 774us/sample - loss: 0.7638 - acc: 0.7522 - val_loss: 0.5809 - val_acc: 0.8262\n",
      "Epoch 436/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7637 - acc: 0.7517\n",
      "Epoch 00436: val_loss did not improve from 0.57397\n",
      "36805/36805 [==============================] - 28s 773us/sample - loss: 0.7637 - acc: 0.7517 - val_loss: 0.5798 - val_acc: 0.8269\n",
      "Epoch 437/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7602 - acc: 0.7514\n",
      "Epoch 00437: val_loss did not improve from 0.57397\n",
      "36805/36805 [==============================] - 28s 772us/sample - loss: 0.7602 - acc: 0.7514 - val_loss: 0.5893 - val_acc: 0.8239\n",
      "Epoch 438/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7638 - acc: 0.7518\n",
      "Epoch 00438: val_loss did not improve from 0.57397\n",
      "36805/36805 [==============================] - 28s 772us/sample - loss: 0.7637 - acc: 0.7518 - val_loss: 0.5863 - val_acc: 0.8269\n",
      "Epoch 439/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7655 - acc: 0.7519\n",
      "Epoch 00439: val_loss did not improve from 0.57397\n",
      "36805/36805 [==============================] - 28s 771us/sample - loss: 0.7655 - acc: 0.7519 - val_loss: 0.5776 - val_acc: 0.8297\n",
      "Epoch 440/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7631 - acc: 0.7555\n",
      "Epoch 00440: val_loss did not improve from 0.57397\n",
      "36805/36805 [==============================] - 28s 773us/sample - loss: 0.7632 - acc: 0.7555 - val_loss: 0.5852 - val_acc: 0.8267\n",
      "Epoch 441/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7553 - acc: 0.7530\n",
      "Epoch 00441: val_loss did not improve from 0.57397\n",
      "36805/36805 [==============================] - 28s 772us/sample - loss: 0.7552 - acc: 0.7530 - val_loss: 0.5882 - val_acc: 0.8260\n",
      "Epoch 442/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7621 - acc: 0.7532\n",
      "Epoch 00442: val_loss did not improve from 0.57397\n",
      "36805/36805 [==============================] - 29s 778us/sample - loss: 0.7618 - acc: 0.7533 - val_loss: 0.5879 - val_acc: 0.8239\n",
      "Epoch 443/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7575 - acc: 0.7555\n",
      "Epoch 00443: val_loss did not improve from 0.57397\n",
      "36805/36805 [==============================] - 28s 771us/sample - loss: 0.7574 - acc: 0.7555 - val_loss: 0.5773 - val_acc: 0.8302\n",
      "Epoch 444/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7624 - acc: 0.7519\n",
      "Epoch 00444: val_loss improved from 0.57397 to 0.57330, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_3_conv_checkpoint/444-0.5733.hdf5\n",
      "36805/36805 [==============================] - 28s 770us/sample - loss: 0.7624 - acc: 0.7519 - val_loss: 0.5733 - val_acc: 0.8300\n",
      "Epoch 445/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7537 - acc: 0.7566\n",
      "Epoch 00445: val_loss did not improve from 0.57330\n",
      "36805/36805 [==============================] - 28s 772us/sample - loss: 0.7536 - acc: 0.7566 - val_loss: 0.5743 - val_acc: 0.8304\n",
      "Epoch 446/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7595 - acc: 0.7555\n",
      "Epoch 00446: val_loss did not improve from 0.57330\n",
      "36805/36805 [==============================] - 28s 772us/sample - loss: 0.7593 - acc: 0.7555 - val_loss: 0.5752 - val_acc: 0.8283\n",
      "Epoch 447/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7559 - acc: 0.7547\n",
      "Epoch 00447: val_loss did not improve from 0.57330\n",
      "36805/36805 [==============================] - 28s 771us/sample - loss: 0.7559 - acc: 0.7547 - val_loss: 0.5856 - val_acc: 0.8276\n",
      "Epoch 448/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7598 - acc: 0.7554\n",
      "Epoch 00448: val_loss did not improve from 0.57330\n",
      "36805/36805 [==============================] - 28s 770us/sample - loss: 0.7598 - acc: 0.7554 - val_loss: 0.5791 - val_acc: 0.8260\n",
      "Epoch 449/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7524 - acc: 0.7557\n",
      "Epoch 00449: val_loss improved from 0.57330 to 0.57227, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_3_conv_checkpoint/449-0.5723.hdf5\n",
      "36805/36805 [==============================] - 28s 773us/sample - loss: 0.7522 - acc: 0.7557 - val_loss: 0.5723 - val_acc: 0.8318\n",
      "Epoch 450/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7541 - acc: 0.7578\n",
      "Epoch 00450: val_loss did not improve from 0.57227\n",
      "36805/36805 [==============================] - 28s 773us/sample - loss: 0.7540 - acc: 0.7579 - val_loss: 0.5856 - val_acc: 0.8227\n",
      "Epoch 451/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7566 - acc: 0.7546\n",
      "Epoch 00451: val_loss did not improve from 0.57227\n",
      "36805/36805 [==============================] - 28s 771us/sample - loss: 0.7567 - acc: 0.7545 - val_loss: 0.5828 - val_acc: 0.8283\n",
      "Epoch 452/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7537 - acc: 0.7560\n",
      "Epoch 00452: val_loss did not improve from 0.57227\n",
      "36805/36805 [==============================] - 28s 771us/sample - loss: 0.7537 - acc: 0.7559 - val_loss: 0.5835 - val_acc: 0.8255\n",
      "Epoch 453/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7617 - acc: 0.7535\n",
      "Epoch 00453: val_loss did not improve from 0.57227\n",
      "36805/36805 [==============================] - 28s 773us/sample - loss: 0.7617 - acc: 0.7535 - val_loss: 0.5822 - val_acc: 0.8241\n",
      "Epoch 454/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7500 - acc: 0.7562\n",
      "Epoch 00454: val_loss did not improve from 0.57227\n",
      "36805/36805 [==============================] - 28s 770us/sample - loss: 0.7499 - acc: 0.7562 - val_loss: 0.5967 - val_acc: 0.8204\n",
      "Epoch 455/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7510 - acc: 0.7580\n",
      "Epoch 00455: val_loss did not improve from 0.57227\n",
      "36805/36805 [==============================] - 28s 772us/sample - loss: 0.7510 - acc: 0.7580 - val_loss: 0.5882 - val_acc: 0.8232\n",
      "Epoch 456/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7575 - acc: 0.7536\n",
      "Epoch 00456: val_loss did not improve from 0.57227\n",
      "36805/36805 [==============================] - 28s 772us/sample - loss: 0.7575 - acc: 0.7535 - val_loss: 0.5850 - val_acc: 0.8241\n",
      "Epoch 457/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7551 - acc: 0.7560\n",
      "Epoch 00457: val_loss did not improve from 0.57227\n",
      "36805/36805 [==============================] - 28s 773us/sample - loss: 0.7551 - acc: 0.7560 - val_loss: 0.5782 - val_acc: 0.8304\n",
      "Epoch 458/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7498 - acc: 0.7572\n",
      "Epoch 00458: val_loss did not improve from 0.57227\n",
      "36805/36805 [==============================] - 28s 770us/sample - loss: 0.7496 - acc: 0.7573 - val_loss: 0.5835 - val_acc: 0.8255\n",
      "Epoch 459/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7527 - acc: 0.7558\n",
      "Epoch 00459: val_loss did not improve from 0.57227\n",
      "36805/36805 [==============================] - 28s 773us/sample - loss: 0.7527 - acc: 0.7558 - val_loss: 0.5757 - val_acc: 0.8286\n",
      "Epoch 460/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7508 - acc: 0.7555\n",
      "Epoch 00460: val_loss improved from 0.57227 to 0.56922, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_3_conv_checkpoint/460-0.5692.hdf5\n",
      "36805/36805 [==============================] - 29s 775us/sample - loss: 0.7509 - acc: 0.7554 - val_loss: 0.5692 - val_acc: 0.8283\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 461/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7461 - acc: 0.7560\n",
      "Epoch 00461: val_loss did not improve from 0.56922\n",
      "36805/36805 [==============================] - 28s 774us/sample - loss: 0.7460 - acc: 0.7560 - val_loss: 0.5718 - val_acc: 0.8316\n",
      "Epoch 462/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7496 - acc: 0.7563\n",
      "Epoch 00462: val_loss did not improve from 0.56922\n",
      "36805/36805 [==============================] - 28s 769us/sample - loss: 0.7497 - acc: 0.7563 - val_loss: 0.5809 - val_acc: 0.8302\n",
      "Epoch 463/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7575 - acc: 0.7519\n",
      "Epoch 00463: val_loss did not improve from 0.56922\n",
      "36805/36805 [==============================] - 28s 772us/sample - loss: 0.7573 - acc: 0.7521 - val_loss: 0.5794 - val_acc: 0.8297\n",
      "Epoch 464/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7533 - acc: 0.7548\n",
      "Epoch 00464: val_loss did not improve from 0.56922\n",
      "36805/36805 [==============================] - 28s 773us/sample - loss: 0.7533 - acc: 0.7548 - val_loss: 0.5836 - val_acc: 0.8255\n",
      "Epoch 465/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7516 - acc: 0.7545\n",
      "Epoch 00465: val_loss did not improve from 0.56922\n",
      "36805/36805 [==============================] - 28s 771us/sample - loss: 0.7516 - acc: 0.7545 - val_loss: 0.5843 - val_acc: 0.8295\n",
      "Epoch 466/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7442 - acc: 0.7583\n",
      "Epoch 00466: val_loss did not improve from 0.56922\n",
      "36805/36805 [==============================] - 28s 771us/sample - loss: 0.7442 - acc: 0.7583 - val_loss: 0.5782 - val_acc: 0.8314\n",
      "Epoch 467/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7482 - acc: 0.7590\n",
      "Epoch 00467: val_loss did not improve from 0.56922\n",
      "36805/36805 [==============================] - 28s 773us/sample - loss: 0.7482 - acc: 0.7591 - val_loss: 0.5805 - val_acc: 0.8295\n",
      "Epoch 468/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7580 - acc: 0.7553\n",
      "Epoch 00468: val_loss did not improve from 0.56922\n",
      "36805/36805 [==============================] - 28s 771us/sample - loss: 0.7580 - acc: 0.7553 - val_loss: 0.5799 - val_acc: 0.8272\n",
      "Epoch 469/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7460 - acc: 0.7579\n",
      "Epoch 00469: val_loss did not improve from 0.56922\n",
      "36805/36805 [==============================] - 28s 772us/sample - loss: 0.7462 - acc: 0.7579 - val_loss: 0.5776 - val_acc: 0.8281\n",
      "Epoch 470/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7489 - acc: 0.7585\n",
      "Epoch 00470: val_loss did not improve from 0.56922\n",
      "36805/36805 [==============================] - 28s 769us/sample - loss: 0.7490 - acc: 0.7585 - val_loss: 0.5722 - val_acc: 0.8337\n",
      "Epoch 471/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7482 - acc: 0.7575\n",
      "Epoch 00471: val_loss did not improve from 0.56922\n",
      "36805/36805 [==============================] - 28s 772us/sample - loss: 0.7481 - acc: 0.7575 - val_loss: 0.5812 - val_acc: 0.8234\n",
      "Epoch 472/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7432 - acc: 0.7567\n",
      "Epoch 00472: val_loss did not improve from 0.56922\n",
      "36805/36805 [==============================] - 28s 769us/sample - loss: 0.7432 - acc: 0.7567 - val_loss: 0.5829 - val_acc: 0.8276\n",
      "Epoch 473/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7486 - acc: 0.7550\n",
      "Epoch 00473: val_loss did not improve from 0.56922\n",
      "36805/36805 [==============================] - 28s 773us/sample - loss: 0.7485 - acc: 0.7550 - val_loss: 0.5864 - val_acc: 0.8255\n",
      "Epoch 474/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7484 - acc: 0.7576\n",
      "Epoch 00474: val_loss did not improve from 0.56922\n",
      "36805/36805 [==============================] - 28s 771us/sample - loss: 0.7485 - acc: 0.7575 - val_loss: 0.5821 - val_acc: 0.8290\n",
      "Epoch 475/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7491 - acc: 0.7556\n",
      "Epoch 00475: val_loss did not improve from 0.56922\n",
      "36805/36805 [==============================] - 29s 780us/sample - loss: 0.7491 - acc: 0.7556 - val_loss: 0.5737 - val_acc: 0.8328\n",
      "Epoch 476/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7449 - acc: 0.7571\n",
      "Epoch 00476: val_loss did not improve from 0.56922\n",
      "36805/36805 [==============================] - 28s 773us/sample - loss: 0.7449 - acc: 0.7571 - val_loss: 0.5786 - val_acc: 0.8276\n",
      "Epoch 477/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7396 - acc: 0.7568\n",
      "Epoch 00477: val_loss did not improve from 0.56922\n",
      "36805/36805 [==============================] - 28s 773us/sample - loss: 0.7395 - acc: 0.7569 - val_loss: 0.5741 - val_acc: 0.8339\n",
      "Epoch 478/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7491 - acc: 0.7541\n",
      "Epoch 00478: val_loss did not improve from 0.56922\n",
      "36805/36805 [==============================] - 28s 771us/sample - loss: 0.7491 - acc: 0.7541 - val_loss: 0.5856 - val_acc: 0.8272\n",
      "Epoch 479/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7448 - acc: 0.7581\n",
      "Epoch 00479: val_loss did not improve from 0.56922\n",
      "36805/36805 [==============================] - 28s 774us/sample - loss: 0.7449 - acc: 0.7581 - val_loss: 0.5810 - val_acc: 0.8286\n",
      "Epoch 480/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7495 - acc: 0.7572\n",
      "Epoch 00480: val_loss did not improve from 0.56922\n",
      "36805/36805 [==============================] - 28s 774us/sample - loss: 0.7495 - acc: 0.7572 - val_loss: 0.5696 - val_acc: 0.8297\n",
      "Epoch 481/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7456 - acc: 0.7556\n",
      "Epoch 00481: val_loss did not improve from 0.56922\n",
      "36805/36805 [==============================] - 28s 773us/sample - loss: 0.7456 - acc: 0.7555 - val_loss: 0.5710 - val_acc: 0.8328\n",
      "Epoch 482/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7434 - acc: 0.7587\n",
      "Epoch 00482: val_loss did not improve from 0.56922\n",
      "36805/36805 [==============================] - 28s 771us/sample - loss: 0.7433 - acc: 0.7588 - val_loss: 0.5712 - val_acc: 0.8323\n",
      "Epoch 483/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7494 - acc: 0.7561\n",
      "Epoch 00483: val_loss did not improve from 0.56922\n",
      "36805/36805 [==============================] - 29s 775us/sample - loss: 0.7494 - acc: 0.7561 - val_loss: 0.5811 - val_acc: 0.8286\n",
      "Epoch 484/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7420 - acc: 0.7571\n",
      "Epoch 00484: val_loss did not improve from 0.56922\n",
      "36805/36805 [==============================] - 28s 771us/sample - loss: 0.7420 - acc: 0.7572 - val_loss: 0.5720 - val_acc: 0.8288\n",
      "Epoch 485/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7423 - acc: 0.7577\n",
      "Epoch 00485: val_loss did not improve from 0.56922\n",
      "36805/36805 [==============================] - 28s 774us/sample - loss: 0.7422 - acc: 0.7578 - val_loss: 0.5871 - val_acc: 0.8290\n",
      "Epoch 486/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7445 - acc: 0.7608\n",
      "Epoch 00486: val_loss did not improve from 0.56922\n",
      "36805/36805 [==============================] - 28s 773us/sample - loss: 0.7444 - acc: 0.7608 - val_loss: 0.5791 - val_acc: 0.8288\n",
      "Epoch 487/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7430 - acc: 0.7581\n",
      "Epoch 00487: val_loss did not improve from 0.56922\n",
      "36805/36805 [==============================] - 28s 772us/sample - loss: 0.7429 - acc: 0.7581 - val_loss: 0.5783 - val_acc: 0.8316\n",
      "Epoch 488/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7408 - acc: 0.7593\n",
      "Epoch 00488: val_loss did not improve from 0.56922\n",
      "36805/36805 [==============================] - 28s 774us/sample - loss: 0.7407 - acc: 0.7593 - val_loss: 0.5771 - val_acc: 0.8276\n",
      "Epoch 489/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7444 - acc: 0.7581\n",
      "Epoch 00489: val_loss did not improve from 0.56922\n",
      "36805/36805 [==============================] - 28s 772us/sample - loss: 0.7444 - acc: 0.7581 - val_loss: 0.5759 - val_acc: 0.8293\n",
      "Epoch 490/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7451 - acc: 0.7579\n",
      "Epoch 00490: val_loss did not improve from 0.56922\n",
      "36805/36805 [==============================] - 28s 772us/sample - loss: 0.7453 - acc: 0.7578 - val_loss: 0.5726 - val_acc: 0.8272\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 491/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7344 - acc: 0.7597\n",
      "Epoch 00491: val_loss did not improve from 0.56922\n",
      "36805/36805 [==============================] - 28s 773us/sample - loss: 0.7344 - acc: 0.7597 - val_loss: 0.5742 - val_acc: 0.8297\n",
      "Epoch 492/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7369 - acc: 0.7600\n",
      "Epoch 00492: val_loss did not improve from 0.56922\n",
      "36805/36805 [==============================] - 28s 772us/sample - loss: 0.7369 - acc: 0.7600 - val_loss: 0.5714 - val_acc: 0.8307\n",
      "Epoch 493/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7395 - acc: 0.7619\n",
      "Epoch 00493: val_loss did not improve from 0.56922\n",
      "36805/36805 [==============================] - 29s 779us/sample - loss: 0.7395 - acc: 0.7620 - val_loss: 0.5777 - val_acc: 0.8297\n",
      "Epoch 494/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7393 - acc: 0.7591\n",
      "Epoch 00494: val_loss did not improve from 0.56922\n",
      "36805/36805 [==============================] - 29s 778us/sample - loss: 0.7397 - acc: 0.7590 - val_loss: 0.5808 - val_acc: 0.8253\n",
      "Epoch 495/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7430 - acc: 0.7599\n",
      "Epoch 00495: val_loss did not improve from 0.56922\n",
      "36805/36805 [==============================] - 29s 778us/sample - loss: 0.7430 - acc: 0.7599 - val_loss: 0.5812 - val_acc: 0.8269\n",
      "Epoch 496/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7374 - acc: 0.7589\n",
      "Epoch 00496: val_loss did not improve from 0.56922\n",
      "36805/36805 [==============================] - 29s 781us/sample - loss: 0.7374 - acc: 0.7589 - val_loss: 0.5738 - val_acc: 0.8283\n",
      "Epoch 497/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7422 - acc: 0.7587\n",
      "Epoch 00497: val_loss did not improve from 0.56922\n",
      "36805/36805 [==============================] - 29s 776us/sample - loss: 0.7423 - acc: 0.7586 - val_loss: 0.5692 - val_acc: 0.8283\n",
      "Epoch 498/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7419 - acc: 0.7613\n",
      "Epoch 00498: val_loss did not improve from 0.56922\n",
      "36805/36805 [==============================] - 28s 774us/sample - loss: 0.7421 - acc: 0.7613 - val_loss: 0.5822 - val_acc: 0.8281\n",
      "Epoch 499/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7370 - acc: 0.7603\n",
      "Epoch 00499: val_loss did not improve from 0.56922\n",
      "36805/36805 [==============================] - 29s 780us/sample - loss: 0.7370 - acc: 0.7602 - val_loss: 0.5790 - val_acc: 0.8276\n",
      "Epoch 500/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7362 - acc: 0.7597\n",
      "Epoch 00500: val_loss did not improve from 0.56922\n",
      "36805/36805 [==============================] - 29s 775us/sample - loss: 0.7368 - acc: 0.7596 - val_loss: 0.5748 - val_acc: 0.8288\n",
      "\n",
      "1D_CNN_custom_multi_2_GMP_DO_3_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 370us/sample - loss: 0.6528 - acc: 0.8073\n",
      "Loss: 0.6527872024047907 Accuracy: 0.807269\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 2.5651 - acc: 0.1542\n",
      "Epoch 00001: val_loss improved from inf to 2.23552, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_4_conv_checkpoint/001-2.2355.hdf5\n",
      "36805/36805 [==============================] - 30s 820us/sample - loss: 2.5645 - acc: 0.1544 - val_loss: 2.2355 - val_acc: 0.3222\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.2166 - acc: 0.2514\n",
      "Epoch 00002: val_loss improved from 2.23552 to 1.95257, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_4_conv_checkpoint/002-1.9526.hdf5\n",
      "36805/36805 [==============================] - 29s 800us/sample - loss: 2.2165 - acc: 0.2515 - val_loss: 1.9526 - val_acc: 0.3997\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.0398 - acc: 0.3137\n",
      "Epoch 00003: val_loss improved from 1.95257 to 1.77503, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_4_conv_checkpoint/003-1.7750.hdf5\n",
      "36805/36805 [==============================] - 29s 799us/sample - loss: 2.0399 - acc: 0.3137 - val_loss: 1.7750 - val_acc: 0.4510\n",
      "Epoch 4/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.8973 - acc: 0.3633\n",
      "Epoch 00004: val_loss improved from 1.77503 to 1.63058, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_4_conv_checkpoint/004-1.6306.hdf5\n",
      "36805/36805 [==============================] - 29s 800us/sample - loss: 1.8972 - acc: 0.3634 - val_loss: 1.6306 - val_acc: 0.4908\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.7769 - acc: 0.4052\n",
      "Epoch 00005: val_loss improved from 1.63058 to 1.48405, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_4_conv_checkpoint/005-1.4841.hdf5\n",
      "36805/36805 [==============================] - 30s 803us/sample - loss: 1.7769 - acc: 0.4052 - val_loss: 1.4841 - val_acc: 0.5542\n",
      "Epoch 6/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.6812 - acc: 0.4365\n",
      "Epoch 00006: val_loss improved from 1.48405 to 1.37934, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_4_conv_checkpoint/006-1.3793.hdf5\n",
      "36805/36805 [==============================] - 29s 801us/sample - loss: 1.6812 - acc: 0.4365 - val_loss: 1.3793 - val_acc: 0.5912\n",
      "Epoch 7/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.5825 - acc: 0.4734\n",
      "Epoch 00007: val_loss improved from 1.37934 to 1.28822, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_4_conv_checkpoint/007-1.2882.hdf5\n",
      "36805/36805 [==============================] - 29s 801us/sample - loss: 1.5821 - acc: 0.4735 - val_loss: 1.2882 - val_acc: 0.6082\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.5134 - acc: 0.4993\n",
      "Epoch 00008: val_loss improved from 1.28822 to 1.20268, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_4_conv_checkpoint/008-1.2027.hdf5\n",
      "36805/36805 [==============================] - 29s 798us/sample - loss: 1.5134 - acc: 0.4993 - val_loss: 1.2027 - val_acc: 0.6310\n",
      "Epoch 9/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.4426 - acc: 0.5216\n",
      "Epoch 00009: val_loss improved from 1.20268 to 1.13077, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_4_conv_checkpoint/009-1.1308.hdf5\n",
      "36805/36805 [==============================] - 30s 805us/sample - loss: 1.4424 - acc: 0.5217 - val_loss: 1.1308 - val_acc: 0.6497\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.3836 - acc: 0.5440\n",
      "Epoch 00010: val_loss improved from 1.13077 to 1.07089, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_4_conv_checkpoint/010-1.0709.hdf5\n",
      "36805/36805 [==============================] - 30s 802us/sample - loss: 1.3835 - acc: 0.5441 - val_loss: 1.0709 - val_acc: 0.6713\n",
      "Epoch 11/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3344 - acc: 0.5582\n",
      "Epoch 00011: val_loss improved from 1.07089 to 1.02314, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_4_conv_checkpoint/011-1.0231.hdf5\n",
      "36805/36805 [==============================] - 29s 799us/sample - loss: 1.3344 - acc: 0.5582 - val_loss: 1.0231 - val_acc: 0.6939\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2841 - acc: 0.5774\n",
      "Epoch 00012: val_loss improved from 1.02314 to 0.98307, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_4_conv_checkpoint/012-0.9831.hdf5\n",
      "36805/36805 [==============================] - 29s 801us/sample - loss: 1.2840 - acc: 0.5774 - val_loss: 0.9831 - val_acc: 0.6988\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2437 - acc: 0.5937\n",
      "Epoch 00013: val_loss improved from 0.98307 to 0.93425, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_4_conv_checkpoint/013-0.9342.hdf5\n",
      "36805/36805 [==============================] - 29s 799us/sample - loss: 1.2438 - acc: 0.5937 - val_loss: 0.9342 - val_acc: 0.7156\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.2021 - acc: 0.6076\n",
      "Epoch 00014: val_loss improved from 0.93425 to 0.90644, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_4_conv_checkpoint/014-0.9064.hdf5\n",
      "36805/36805 [==============================] - 29s 801us/sample - loss: 1.2022 - acc: 0.6076 - val_loss: 0.9064 - val_acc: 0.7319\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1733 - acc: 0.6173\n",
      "Epoch 00015: val_loss improved from 0.90644 to 0.87116, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_4_conv_checkpoint/015-0.8712.hdf5\n",
      "36805/36805 [==============================] - 30s 803us/sample - loss: 1.1731 - acc: 0.6173 - val_loss: 0.8712 - val_acc: 0.7372\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1439 - acc: 0.6293\n",
      "Epoch 00016: val_loss improved from 0.87116 to 0.84560, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_4_conv_checkpoint/016-0.8456.hdf5\n",
      "36805/36805 [==============================] - 29s 801us/sample - loss: 1.1440 - acc: 0.6293 - val_loss: 0.8456 - val_acc: 0.7524\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1133 - acc: 0.6383\n",
      "Epoch 00017: val_loss improved from 0.84560 to 0.82926, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_4_conv_checkpoint/017-0.8293.hdf5\n",
      "36805/36805 [==============================] - 30s 802us/sample - loss: 1.1135 - acc: 0.6383 - val_loss: 0.8293 - val_acc: 0.7538\n",
      "Epoch 18/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0862 - acc: 0.6502\n",
      "Epoch 00018: val_loss improved from 0.82926 to 0.80248, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_4_conv_checkpoint/018-0.8025.hdf5\n",
      "36805/36805 [==============================] - 29s 800us/sample - loss: 1.0858 - acc: 0.6503 - val_loss: 0.8025 - val_acc: 0.7584\n",
      "Epoch 19/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0609 - acc: 0.6609\n",
      "Epoch 00019: val_loss improved from 0.80248 to 0.77277, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_4_conv_checkpoint/019-0.7728.hdf5\n",
      "36805/36805 [==============================] - 29s 800us/sample - loss: 1.0609 - acc: 0.6609 - val_loss: 0.7728 - val_acc: 0.7701\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0423 - acc: 0.6669\n",
      "Epoch 00020: val_loss improved from 0.77277 to 0.74925, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_4_conv_checkpoint/020-0.7492.hdf5\n",
      "36805/36805 [==============================] - 29s 800us/sample - loss: 1.0423 - acc: 0.6669 - val_loss: 0.7492 - val_acc: 0.7782\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.0155 - acc: 0.6727\n",
      "Epoch 00021: val_loss improved from 0.74925 to 0.73226, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_4_conv_checkpoint/021-0.7323.hdf5\n",
      "36805/36805 [==============================] - 29s 801us/sample - loss: 1.0155 - acc: 0.6727 - val_loss: 0.7323 - val_acc: 0.7871\n",
      "Epoch 22/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9974 - acc: 0.6818\n",
      "Epoch 00022: val_loss improved from 0.73226 to 0.71731, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_4_conv_checkpoint/022-0.7173.hdf5\n",
      "36805/36805 [==============================] - 29s 800us/sample - loss: 0.9974 - acc: 0.6818 - val_loss: 0.7173 - val_acc: 0.7859\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9822 - acc: 0.6863\n",
      "Epoch 00023: val_loss improved from 0.71731 to 0.69532, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_4_conv_checkpoint/023-0.6953.hdf5\n",
      "36805/36805 [==============================] - 29s 800us/sample - loss: 0.9823 - acc: 0.6862 - val_loss: 0.6953 - val_acc: 0.7934\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9687 - acc: 0.6905\n",
      "Epoch 00024: val_loss improved from 0.69532 to 0.68837, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_4_conv_checkpoint/024-0.6884.hdf5\n",
      "36805/36805 [==============================] - 30s 802us/sample - loss: 0.9688 - acc: 0.6904 - val_loss: 0.6884 - val_acc: 0.7987\n",
      "Epoch 25/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9527 - acc: 0.6971\n",
      "Epoch 00025: val_loss improved from 0.68837 to 0.66050, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_4_conv_checkpoint/025-0.6605.hdf5\n",
      "36805/36805 [==============================] - 29s 800us/sample - loss: 0.9527 - acc: 0.6969 - val_loss: 0.6605 - val_acc: 0.8064\n",
      "Epoch 26/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9394 - acc: 0.7015\n",
      "Epoch 00026: val_loss improved from 0.66050 to 0.65309, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_4_conv_checkpoint/026-0.6531.hdf5\n",
      "36805/36805 [==============================] - 29s 796us/sample - loss: 0.9389 - acc: 0.7017 - val_loss: 0.6531 - val_acc: 0.8095\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9231 - acc: 0.7058\n",
      "Epoch 00027: val_loss improved from 0.65309 to 0.62368, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_4_conv_checkpoint/027-0.6237.hdf5\n",
      "36805/36805 [==============================] - 29s 798us/sample - loss: 0.9232 - acc: 0.7057 - val_loss: 0.6237 - val_acc: 0.8153\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9088 - acc: 0.7146\n",
      "Epoch 00028: val_loss did not improve from 0.62368\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 0.9089 - acc: 0.7145 - val_loss: 0.6587 - val_acc: 0.8043\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8980 - acc: 0.7159\n",
      "Epoch 00029: val_loss did not improve from 0.62368\n",
      "36805/36805 [==============================] - 29s 797us/sample - loss: 0.8981 - acc: 0.7159 - val_loss: 0.6360 - val_acc: 0.8132\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8834 - acc: 0.7186\n",
      "Epoch 00030: val_loss improved from 0.62368 to 0.60285, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_4_conv_checkpoint/030-0.6028.hdf5\n",
      "36805/36805 [==============================] - 29s 798us/sample - loss: 0.8834 - acc: 0.7186 - val_loss: 0.6028 - val_acc: 0.8190\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8707 - acc: 0.7252\n",
      "Epoch 00031: val_loss improved from 0.60285 to 0.59022, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_4_conv_checkpoint/031-0.5902.hdf5\n",
      "36805/36805 [==============================] - 29s 800us/sample - loss: 0.8707 - acc: 0.7252 - val_loss: 0.5902 - val_acc: 0.8255\n",
      "Epoch 32/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8661 - acc: 0.7266\n",
      "Epoch 00032: val_loss improved from 0.59022 to 0.58694, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_4_conv_checkpoint/032-0.5869.hdf5\n",
      "36805/36805 [==============================] - 29s 793us/sample - loss: 0.8659 - acc: 0.7267 - val_loss: 0.5869 - val_acc: 0.8300\n",
      "Epoch 33/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8592 - acc: 0.7310\n",
      "Epoch 00033: val_loss improved from 0.58694 to 0.58120, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_4_conv_checkpoint/033-0.5812.hdf5\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 0.8590 - acc: 0.7311 - val_loss: 0.5812 - val_acc: 0.8307\n",
      "Epoch 34/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8363 - acc: 0.7353\n",
      "Epoch 00034: val_loss improved from 0.58120 to 0.57041, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_4_conv_checkpoint/034-0.5704.hdf5\n",
      "36805/36805 [==============================] - 29s 793us/sample - loss: 0.8369 - acc: 0.7351 - val_loss: 0.5704 - val_acc: 0.8328\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8397 - acc: 0.7376\n",
      "Epoch 00035: val_loss did not improve from 0.57041\n",
      "36805/36805 [==============================] - 29s 796us/sample - loss: 0.8397 - acc: 0.7376 - val_loss: 0.5744 - val_acc: 0.8304\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8327 - acc: 0.7398\n",
      "Epoch 00036: val_loss improved from 0.57041 to 0.55203, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_4_conv_checkpoint/036-0.5520.hdf5\n",
      "36805/36805 [==============================] - 29s 793us/sample - loss: 0.8326 - acc: 0.7398 - val_loss: 0.5520 - val_acc: 0.8367\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8177 - acc: 0.7413\n",
      "Epoch 00037: val_loss improved from 0.55203 to 0.54622, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_4_conv_checkpoint/037-0.5462.hdf5\n",
      "36805/36805 [==============================] - 29s 799us/sample - loss: 0.8177 - acc: 0.7413 - val_loss: 0.5462 - val_acc: 0.8430\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8113 - acc: 0.7443\n",
      "Epoch 00038: val_loss improved from 0.54622 to 0.54400, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_4_conv_checkpoint/038-0.5440.hdf5\n",
      "36805/36805 [==============================] - 29s 799us/sample - loss: 0.8112 - acc: 0.7443 - val_loss: 0.5440 - val_acc: 0.8376\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8108 - acc: 0.7468\n",
      "Epoch 00039: val_loss improved from 0.54400 to 0.53313, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_4_conv_checkpoint/039-0.5331.hdf5\n",
      "36805/36805 [==============================] - 29s 798us/sample - loss: 0.8108 - acc: 0.7468 - val_loss: 0.5331 - val_acc: 0.8411\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7993 - acc: 0.7494\n",
      "Epoch 00040: val_loss improved from 0.53313 to 0.53083, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_4_conv_checkpoint/040-0.5308.hdf5\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 0.7993 - acc: 0.7494 - val_loss: 0.5308 - val_acc: 0.8484\n",
      "Epoch 41/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7935 - acc: 0.7513\n",
      "Epoch 00041: val_loss improved from 0.53083 to 0.52469, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_4_conv_checkpoint/041-0.5247.hdf5\n",
      "36805/36805 [==============================] - 29s 798us/sample - loss: 0.7933 - acc: 0.7513 - val_loss: 0.5247 - val_acc: 0.8467\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7882 - acc: 0.7530\n",
      "Epoch 00042: val_loss improved from 0.52469 to 0.51634, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_4_conv_checkpoint/042-0.5163.hdf5\n",
      "36805/36805 [==============================] - 29s 797us/sample - loss: 0.7881 - acc: 0.7530 - val_loss: 0.5163 - val_acc: 0.8467\n",
      "Epoch 43/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7834 - acc: 0.7554\n",
      "Epoch 00043: val_loss improved from 0.51634 to 0.51416, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_4_conv_checkpoint/043-0.5142.hdf5\n",
      "36805/36805 [==============================] - 29s 801us/sample - loss: 0.7834 - acc: 0.7554 - val_loss: 0.5142 - val_acc: 0.8488\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7760 - acc: 0.7572\n",
      "Epoch 00044: val_loss did not improve from 0.51416\n",
      "36805/36805 [==============================] - 29s 801us/sample - loss: 0.7760 - acc: 0.7572 - val_loss: 0.5329 - val_acc: 0.8435\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7720 - acc: 0.7557\n",
      "Epoch 00045: val_loss improved from 0.51416 to 0.50593, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_4_conv_checkpoint/045-0.5059.hdf5\n",
      "36805/36805 [==============================] - 29s 798us/sample - loss: 0.7721 - acc: 0.7557 - val_loss: 0.5059 - val_acc: 0.8516\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7600 - acc: 0.7617\n",
      "Epoch 00046: val_loss did not improve from 0.50593\n",
      "36805/36805 [==============================] - 29s 797us/sample - loss: 0.7599 - acc: 0.7617 - val_loss: 0.5088 - val_acc: 0.8484\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7564 - acc: 0.7665\n",
      "Epoch 00047: val_loss improved from 0.50593 to 0.48981, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_4_conv_checkpoint/047-0.4898.hdf5\n",
      "36805/36805 [==============================] - 29s 797us/sample - loss: 0.7563 - acc: 0.7665 - val_loss: 0.4898 - val_acc: 0.8551\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7495 - acc: 0.7638\n",
      "Epoch 00048: val_loss improved from 0.48981 to 0.48766, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_4_conv_checkpoint/048-0.4877.hdf5\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 0.7496 - acc: 0.7638 - val_loss: 0.4877 - val_acc: 0.8523\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7446 - acc: 0.7679\n",
      "Epoch 00049: val_loss improved from 0.48766 to 0.48736, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_4_conv_checkpoint/049-0.4874.hdf5\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 0.7446 - acc: 0.7679 - val_loss: 0.4874 - val_acc: 0.8565\n",
      "Epoch 50/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7428 - acc: 0.7676\n",
      "Epoch 00050: val_loss did not improve from 0.48736\n",
      "36805/36805 [==============================] - 29s 792us/sample - loss: 0.7430 - acc: 0.7677 - val_loss: 0.4899 - val_acc: 0.8586\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7389 - acc: 0.7693\n",
      "Epoch 00051: val_loss improved from 0.48736 to 0.47861, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_4_conv_checkpoint/051-0.4786.hdf5\n",
      "36805/36805 [==============================] - 29s 798us/sample - loss: 0.7388 - acc: 0.7694 - val_loss: 0.4786 - val_acc: 0.8581\n",
      "Epoch 52/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7340 - acc: 0.7686\n",
      "Epoch 00052: val_loss did not improve from 0.47861\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 0.7337 - acc: 0.7687 - val_loss: 0.4831 - val_acc: 0.8581\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7322 - acc: 0.7688\n",
      "Epoch 00053: val_loss improved from 0.47861 to 0.47495, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_4_conv_checkpoint/053-0.4750.hdf5\n",
      "36805/36805 [==============================] - 29s 796us/sample - loss: 0.7321 - acc: 0.7688 - val_loss: 0.4750 - val_acc: 0.8626\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7265 - acc: 0.7719\n",
      "Epoch 00054: val_loss did not improve from 0.47495\n",
      "36805/36805 [==============================] - 29s 793us/sample - loss: 0.7265 - acc: 0.7719 - val_loss: 0.4852 - val_acc: 0.8567\n",
      "Epoch 55/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7215 - acc: 0.7734\n",
      "Epoch 00055: val_loss improved from 0.47495 to 0.47020, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_4_conv_checkpoint/055-0.4702.hdf5\n",
      "36805/36805 [==============================] - 29s 801us/sample - loss: 0.7213 - acc: 0.7734 - val_loss: 0.4702 - val_acc: 0.8602\n",
      "Epoch 56/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7211 - acc: 0.7727\n",
      "Epoch 00056: val_loss did not improve from 0.47020\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 0.7212 - acc: 0.7727 - val_loss: 0.4730 - val_acc: 0.8593\n",
      "Epoch 57/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7162 - acc: 0.7782\n",
      "Epoch 00057: val_loss improved from 0.47020 to 0.46863, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_4_conv_checkpoint/057-0.4686.hdf5\n",
      "36805/36805 [==============================] - 29s 797us/sample - loss: 0.7165 - acc: 0.7780 - val_loss: 0.4686 - val_acc: 0.8619\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7105 - acc: 0.7773\n",
      "Epoch 00058: val_loss improved from 0.46863 to 0.45553, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_4_conv_checkpoint/058-0.4555.hdf5\n",
      "36805/36805 [==============================] - 29s 799us/sample - loss: 0.7105 - acc: 0.7773 - val_loss: 0.4555 - val_acc: 0.8640\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7057 - acc: 0.7810\n",
      "Epoch 00059: val_loss improved from 0.45553 to 0.44948, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_4_conv_checkpoint/059-0.4495.hdf5\n",
      "36805/36805 [==============================] - 29s 798us/sample - loss: 0.7057 - acc: 0.7810 - val_loss: 0.4495 - val_acc: 0.8689\n",
      "Epoch 60/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7061 - acc: 0.7789\n",
      "Epoch 00060: val_loss did not improve from 0.44948\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 0.7065 - acc: 0.7788 - val_loss: 0.4689 - val_acc: 0.8607\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7015 - acc: 0.7794\n",
      "Epoch 00061: val_loss did not improve from 0.44948\n",
      "36805/36805 [==============================] - 29s 797us/sample - loss: 0.7016 - acc: 0.7794 - val_loss: 0.4725 - val_acc: 0.8593\n",
      "Epoch 62/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6993 - acc: 0.7810\n",
      "Epoch 00062: val_loss did not improve from 0.44948\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 0.6991 - acc: 0.7810 - val_loss: 0.4595 - val_acc: 0.8677\n",
      "Epoch 63/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6884 - acc: 0.7835\n",
      "Epoch 00063: val_loss improved from 0.44948 to 0.44735, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_4_conv_checkpoint/063-0.4473.hdf5\n",
      "36805/36805 [==============================] - 29s 800us/sample - loss: 0.6884 - acc: 0.7835 - val_loss: 0.4473 - val_acc: 0.8679\n",
      "Epoch 64/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6958 - acc: 0.7822\n",
      "Epoch 00064: val_loss did not improve from 0.44735\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 0.6955 - acc: 0.7823 - val_loss: 0.4571 - val_acc: 0.8619\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6865 - acc: 0.7840\n",
      "Epoch 00065: val_loss did not improve from 0.44735\n",
      "36805/36805 [==============================] - 29s 797us/sample - loss: 0.6865 - acc: 0.7840 - val_loss: 0.4476 - val_acc: 0.8677\n",
      "Epoch 66/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6840 - acc: 0.7871\n",
      "Epoch 00066: val_loss improved from 0.44735 to 0.43322, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_4_conv_checkpoint/066-0.4332.hdf5\n",
      "36805/36805 [==============================] - 29s 793us/sample - loss: 0.6846 - acc: 0.7869 - val_loss: 0.4332 - val_acc: 0.8712\n",
      "Epoch 67/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6882 - acc: 0.7832\n",
      "Epoch 00067: val_loss did not improve from 0.43322\n",
      "36805/36805 [==============================] - 29s 798us/sample - loss: 0.6880 - acc: 0.7832 - val_loss: 0.4405 - val_acc: 0.8703\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6767 - acc: 0.7870\n",
      "Epoch 00068: val_loss did not improve from 0.43322\n",
      "36805/36805 [==============================] - 29s 799us/sample - loss: 0.6767 - acc: 0.7870 - val_loss: 0.4370 - val_acc: 0.8696\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6768 - acc: 0.7885\n",
      "Epoch 00069: val_loss improved from 0.43322 to 0.42995, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_4_conv_checkpoint/069-0.4299.hdf5\n",
      "36805/36805 [==============================] - 29s 796us/sample - loss: 0.6770 - acc: 0.7885 - val_loss: 0.4299 - val_acc: 0.8749\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6785 - acc: 0.7863\n",
      "Epoch 00070: val_loss did not improve from 0.42995\n",
      "36805/36805 [==============================] - 29s 797us/sample - loss: 0.6785 - acc: 0.7863 - val_loss: 0.4395 - val_acc: 0.8682\n",
      "Epoch 71/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6743 - acc: 0.7883\n",
      "Epoch 00071: val_loss did not improve from 0.42995\n",
      "36805/36805 [==============================] - 29s 794us/sample - loss: 0.6745 - acc: 0.7881 - val_loss: 0.4386 - val_acc: 0.8724\n",
      "Epoch 72/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6652 - acc: 0.7913\n",
      "Epoch 00072: val_loss improved from 0.42995 to 0.42925, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_4_conv_checkpoint/072-0.4293.hdf5\n",
      "36805/36805 [==============================] - 29s 794us/sample - loss: 0.6654 - acc: 0.7912 - val_loss: 0.4293 - val_acc: 0.8737\n",
      "Epoch 73/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6646 - acc: 0.7934\n",
      "Epoch 00073: val_loss improved from 0.42925 to 0.42398, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_4_conv_checkpoint/073-0.4240.hdf5\n",
      "36805/36805 [==============================] - 29s 798us/sample - loss: 0.6644 - acc: 0.7935 - val_loss: 0.4240 - val_acc: 0.8754\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6638 - acc: 0.7924\n",
      "Epoch 00074: val_loss did not improve from 0.42398\n",
      "36805/36805 [==============================] - 29s 796us/sample - loss: 0.6639 - acc: 0.7923 - val_loss: 0.4394 - val_acc: 0.8714\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6617 - acc: 0.7913\n",
      "Epoch 00075: val_loss improved from 0.42398 to 0.42116, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_4_conv_checkpoint/075-0.4212.hdf5\n",
      "36805/36805 [==============================] - 29s 799us/sample - loss: 0.6617 - acc: 0.7913 - val_loss: 0.4212 - val_acc: 0.8751\n",
      "Epoch 76/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6540 - acc: 0.7943\n",
      "Epoch 00076: val_loss did not improve from 0.42116\n",
      "36805/36805 [==============================] - 29s 797us/sample - loss: 0.6538 - acc: 0.7943 - val_loss: 0.4247 - val_acc: 0.8747\n",
      "Epoch 77/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6604 - acc: 0.7949\n",
      "Epoch 00077: val_loss did not improve from 0.42116\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 0.6605 - acc: 0.7949 - val_loss: 0.4335 - val_acc: 0.8754\n",
      "Epoch 78/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6537 - acc: 0.7946\n",
      "Epoch 00078: val_loss improved from 0.42116 to 0.41702, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_4_conv_checkpoint/078-0.4170.hdf5\n",
      "36805/36805 [==============================] - 29s 799us/sample - loss: 0.6540 - acc: 0.7946 - val_loss: 0.4170 - val_acc: 0.8761\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6571 - acc: 0.7933\n",
      "Epoch 00079: val_loss did not improve from 0.41702\n",
      "36805/36805 [==============================] - 29s 796us/sample - loss: 0.6571 - acc: 0.7933 - val_loss: 0.4195 - val_acc: 0.8775\n",
      "Epoch 80/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6448 - acc: 0.7983\n",
      "Epoch 00080: val_loss did not improve from 0.41702\n",
      "36805/36805 [==============================] - 29s 796us/sample - loss: 0.6449 - acc: 0.7984 - val_loss: 0.4224 - val_acc: 0.8784\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6485 - acc: 0.7942\n",
      "Epoch 00081: val_loss did not improve from 0.41702\n",
      "36805/36805 [==============================] - 29s 799us/sample - loss: 0.6484 - acc: 0.7942 - val_loss: 0.4224 - val_acc: 0.8747\n",
      "Epoch 82/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6454 - acc: 0.7966\n",
      "Epoch 00082: val_loss did not improve from 0.41702\n",
      "36805/36805 [==============================] - 29s 797us/sample - loss: 0.6450 - acc: 0.7967 - val_loss: 0.4229 - val_acc: 0.8770\n",
      "Epoch 83/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6432 - acc: 0.7983\n",
      "Epoch 00083: val_loss improved from 0.41702 to 0.41436, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_4_conv_checkpoint/083-0.4144.hdf5\n",
      "36805/36805 [==============================] - 29s 799us/sample - loss: 0.6429 - acc: 0.7983 - val_loss: 0.4144 - val_acc: 0.8810\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6393 - acc: 0.8007\n",
      "Epoch 00084: val_loss did not improve from 0.41436\n",
      "36805/36805 [==============================] - 29s 797us/sample - loss: 0.6393 - acc: 0.8007 - val_loss: 0.4253 - val_acc: 0.8761\n",
      "Epoch 85/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6345 - acc: 0.8008\n",
      "Epoch 00085: val_loss improved from 0.41436 to 0.41433, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_4_conv_checkpoint/085-0.4143.hdf5\n",
      "36805/36805 [==============================] - 29s 796us/sample - loss: 0.6348 - acc: 0.8007 - val_loss: 0.4143 - val_acc: 0.8817\n",
      "Epoch 86/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6356 - acc: 0.8008\n",
      "Epoch 00086: val_loss improved from 0.41433 to 0.40389, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_4_conv_checkpoint/086-0.4039.hdf5\n",
      "36805/36805 [==============================] - 29s 798us/sample - loss: 0.6354 - acc: 0.8008 - val_loss: 0.4039 - val_acc: 0.8838\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6343 - acc: 0.7998\n",
      "Epoch 00087: val_loss did not improve from 0.40389\n",
      "36805/36805 [==============================] - 29s 798us/sample - loss: 0.6344 - acc: 0.7998 - val_loss: 0.4174 - val_acc: 0.8796\n",
      "Epoch 88/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6292 - acc: 0.8022\n",
      "Epoch 00088: val_loss did not improve from 0.40389\n",
      "36805/36805 [==============================] - 29s 796us/sample - loss: 0.6293 - acc: 0.8021 - val_loss: 0.4064 - val_acc: 0.8821\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6289 - acc: 0.8032\n",
      "Epoch 00089: val_loss did not improve from 0.40389\n",
      "36805/36805 [==============================] - 29s 798us/sample - loss: 0.6289 - acc: 0.8032 - val_loss: 0.4341 - val_acc: 0.8705\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6258 - acc: 0.8037\n",
      "Epoch 00090: val_loss improved from 0.40389 to 0.40350, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_4_conv_checkpoint/090-0.4035.hdf5\n",
      "36805/36805 [==============================] - 29s 797us/sample - loss: 0.6259 - acc: 0.8037 - val_loss: 0.4035 - val_acc: 0.8819\n",
      "Epoch 91/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6229 - acc: 0.8041\n",
      "Epoch 00091: val_loss did not improve from 0.40350\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 0.6228 - acc: 0.8040 - val_loss: 0.4152 - val_acc: 0.8782\n",
      "Epoch 92/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6229 - acc: 0.8055\n",
      "Epoch 00092: val_loss improved from 0.40350 to 0.39986, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_4_conv_checkpoint/092-0.3999.hdf5\n",
      "36805/36805 [==============================] - 29s 799us/sample - loss: 0.6230 - acc: 0.8056 - val_loss: 0.3999 - val_acc: 0.8810\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6218 - acc: 0.8070\n",
      "Epoch 00093: val_loss did not improve from 0.39986\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 0.6218 - acc: 0.8070 - val_loss: 0.4141 - val_acc: 0.8756\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6271 - acc: 0.8048\n",
      "Epoch 00094: val_loss did not improve from 0.39986\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 0.6271 - acc: 0.8048 - val_loss: 0.4160 - val_acc: 0.8796\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6177 - acc: 0.8080\n",
      "Epoch 00095: val_loss improved from 0.39986 to 0.39525, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_4_conv_checkpoint/095-0.3953.hdf5\n",
      "36805/36805 [==============================] - 29s 794us/sample - loss: 0.6176 - acc: 0.8080 - val_loss: 0.3953 - val_acc: 0.8840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6156 - acc: 0.8087\n",
      "Epoch 00096: val_loss improved from 0.39525 to 0.39482, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_4_conv_checkpoint/096-0.3948.hdf5\n",
      "36805/36805 [==============================] - 29s 790us/sample - loss: 0.6156 - acc: 0.8087 - val_loss: 0.3948 - val_acc: 0.8849\n",
      "Epoch 97/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6128 - acc: 0.8081\n",
      "Epoch 00097: val_loss did not improve from 0.39482\n",
      "36805/36805 [==============================] - 29s 796us/sample - loss: 0.6131 - acc: 0.8080 - val_loss: 0.4047 - val_acc: 0.8817\n",
      "Epoch 98/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6044 - acc: 0.8095\n",
      "Epoch 00098: val_loss did not improve from 0.39482\n",
      "36805/36805 [==============================] - 29s 792us/sample - loss: 0.6049 - acc: 0.8093 - val_loss: 0.3952 - val_acc: 0.8842\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6083 - acc: 0.8108\n",
      "Epoch 00099: val_loss did not improve from 0.39482\n",
      "36805/36805 [==============================] - 29s 792us/sample - loss: 0.6082 - acc: 0.8108 - val_loss: 0.3980 - val_acc: 0.8817\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6096 - acc: 0.8096\n",
      "Epoch 00100: val_loss improved from 0.39482 to 0.39292, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_4_conv_checkpoint/100-0.3929.hdf5\n",
      "36805/36805 [==============================] - 29s 794us/sample - loss: 0.6096 - acc: 0.8096 - val_loss: 0.3929 - val_acc: 0.8859\n",
      "Epoch 101/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6020 - acc: 0.8096\n",
      "Epoch 00101: val_loss did not improve from 0.39292\n",
      "36805/36805 [==============================] - 29s 792us/sample - loss: 0.6020 - acc: 0.8096 - val_loss: 0.4158 - val_acc: 0.8775\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6033 - acc: 0.8102\n",
      "Epoch 00102: val_loss did not improve from 0.39292\n",
      "36805/36805 [==============================] - 29s 792us/sample - loss: 0.6033 - acc: 0.8102 - val_loss: 0.4021 - val_acc: 0.8810\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6029 - acc: 0.8111\n",
      "Epoch 00103: val_loss did not improve from 0.39292\n",
      "36805/36805 [==============================] - 29s 790us/sample - loss: 0.6029 - acc: 0.8111 - val_loss: 0.3974 - val_acc: 0.8852\n",
      "Epoch 104/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6042 - acc: 0.8107\n",
      "Epoch 00104: val_loss improved from 0.39292 to 0.38891, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_4_conv_checkpoint/104-0.3889.hdf5\n",
      "36805/36805 [==============================] - 29s 791us/sample - loss: 0.6045 - acc: 0.8106 - val_loss: 0.3889 - val_acc: 0.8863\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5941 - acc: 0.8137\n",
      "Epoch 00105: val_loss improved from 0.38891 to 0.38615, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_4_conv_checkpoint/105-0.3861.hdf5\n",
      "36805/36805 [==============================] - 29s 790us/sample - loss: 0.5941 - acc: 0.8137 - val_loss: 0.3861 - val_acc: 0.8870\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5996 - acc: 0.8129\n",
      "Epoch 00106: val_loss improved from 0.38615 to 0.38178, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_4_conv_checkpoint/106-0.3818.hdf5\n",
      "36805/36805 [==============================] - 29s 790us/sample - loss: 0.5996 - acc: 0.8129 - val_loss: 0.3818 - val_acc: 0.8896\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5959 - acc: 0.8108\n",
      "Epoch 00107: val_loss did not improve from 0.38178\n",
      "36805/36805 [==============================] - 29s 791us/sample - loss: 0.5959 - acc: 0.8109 - val_loss: 0.3990 - val_acc: 0.8845\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5933 - acc: 0.8149\n",
      "Epoch 00108: val_loss did not improve from 0.38178\n",
      "36805/36805 [==============================] - 29s 792us/sample - loss: 0.5933 - acc: 0.8148 - val_loss: 0.3871 - val_acc: 0.8875\n",
      "Epoch 109/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5968 - acc: 0.8112\n",
      "Epoch 00109: val_loss did not improve from 0.38178\n",
      "36805/36805 [==============================] - 29s 790us/sample - loss: 0.5970 - acc: 0.8111 - val_loss: 0.3889 - val_acc: 0.8833\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5932 - acc: 0.8135\n",
      "Epoch 00110: val_loss did not improve from 0.38178\n",
      "36805/36805 [==============================] - 29s 791us/sample - loss: 0.5932 - acc: 0.8135 - val_loss: 0.3931 - val_acc: 0.8835\n",
      "Epoch 111/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5853 - acc: 0.8160\n",
      "Epoch 00111: val_loss did not improve from 0.38178\n",
      "36805/36805 [==============================] - 29s 789us/sample - loss: 0.5850 - acc: 0.8161 - val_loss: 0.3857 - val_acc: 0.8859\n",
      "Epoch 112/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5904 - acc: 0.8145\n",
      "Epoch 00112: val_loss improved from 0.38178 to 0.38117, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_4_conv_checkpoint/112-0.3812.hdf5\n",
      "36805/36805 [==============================] - 29s 792us/sample - loss: 0.5904 - acc: 0.8145 - val_loss: 0.3812 - val_acc: 0.8894\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5901 - acc: 0.8163\n",
      "Epoch 00113: val_loss did not improve from 0.38117\n",
      "36805/36805 [==============================] - 29s 792us/sample - loss: 0.5901 - acc: 0.8163 - val_loss: 0.3838 - val_acc: 0.8898\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5827 - acc: 0.8176\n",
      "Epoch 00114: val_loss did not improve from 0.38117\n",
      "36805/36805 [==============================] - 29s 790us/sample - loss: 0.5827 - acc: 0.8176 - val_loss: 0.3830 - val_acc: 0.8866\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5788 - acc: 0.8167\n",
      "Epoch 00115: val_loss did not improve from 0.38117\n",
      "36805/36805 [==============================] - 29s 792us/sample - loss: 0.5788 - acc: 0.8167 - val_loss: 0.3929 - val_acc: 0.8863\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5884 - acc: 0.8143\n",
      "Epoch 00116: val_loss did not improve from 0.38117\n",
      "36805/36805 [==============================] - 29s 793us/sample - loss: 0.5884 - acc: 0.8143 - val_loss: 0.3898 - val_acc: 0.8863\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5784 - acc: 0.8189\n",
      "Epoch 00117: val_loss did not improve from 0.38117\n",
      "36805/36805 [==============================] - 29s 791us/sample - loss: 0.5783 - acc: 0.8189 - val_loss: 0.3889 - val_acc: 0.8863\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5800 - acc: 0.8185\n",
      "Epoch 00118: val_loss did not improve from 0.38117\n",
      "36805/36805 [==============================] - 29s 791us/sample - loss: 0.5800 - acc: 0.8185 - val_loss: 0.3853 - val_acc: 0.8863\n",
      "Epoch 119/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5800 - acc: 0.8186\n",
      "Epoch 00119: val_loss did not improve from 0.38117\n",
      "36805/36805 [==============================] - 29s 798us/sample - loss: 0.5798 - acc: 0.8186 - val_loss: 0.3863 - val_acc: 0.8866\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5742 - acc: 0.8200\n",
      "Epoch 00120: val_loss did not improve from 0.38117\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 0.5743 - acc: 0.8200 - val_loss: 0.3836 - val_acc: 0.8852\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5744 - acc: 0.8199\n",
      "Epoch 00121: val_loss improved from 0.38117 to 0.37224, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_4_conv_checkpoint/121-0.3722.hdf5\n",
      "36805/36805 [==============================] - 29s 797us/sample - loss: 0.5744 - acc: 0.8199 - val_loss: 0.3722 - val_acc: 0.8938\n",
      "Epoch 122/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5673 - acc: 0.8219\n",
      "Epoch 00122: val_loss did not improve from 0.37224\n",
      "36805/36805 [==============================] - 29s 796us/sample - loss: 0.5676 - acc: 0.8218 - val_loss: 0.3775 - val_acc: 0.8873\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 123/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5748 - acc: 0.8190\n",
      "Epoch 00123: val_loss did not improve from 0.37224\n",
      "36805/36805 [==============================] - 29s 797us/sample - loss: 0.5748 - acc: 0.8189 - val_loss: 0.3945 - val_acc: 0.8833\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5726 - acc: 0.8200\n",
      "Epoch 00124: val_loss did not improve from 0.37224\n",
      "36805/36805 [==============================] - 29s 794us/sample - loss: 0.5726 - acc: 0.8200 - val_loss: 0.3885 - val_acc: 0.8873\n",
      "Epoch 125/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5696 - acc: 0.8212\n",
      "Epoch 00125: val_loss did not improve from 0.37224\n",
      "36805/36805 [==============================] - 29s 798us/sample - loss: 0.5694 - acc: 0.8212 - val_loss: 0.3789 - val_acc: 0.8912\n",
      "Epoch 126/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5691 - acc: 0.8206\n",
      "Epoch 00126: val_loss did not improve from 0.37224\n",
      "36805/36805 [==============================] - 29s 793us/sample - loss: 0.5692 - acc: 0.8206 - val_loss: 0.3779 - val_acc: 0.8889\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5622 - acc: 0.8248\n",
      "Epoch 00127: val_loss did not improve from 0.37224\n",
      "36805/36805 [==============================] - 29s 790us/sample - loss: 0.5621 - acc: 0.8248 - val_loss: 0.3865 - val_acc: 0.8849\n",
      "Epoch 128/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5637 - acc: 0.8222\n",
      "Epoch 00128: val_loss did not improve from 0.37224\n",
      "36805/36805 [==============================] - 29s 796us/sample - loss: 0.5639 - acc: 0.8222 - val_loss: 0.3888 - val_acc: 0.8863\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5663 - acc: 0.8229\n",
      "Epoch 00129: val_loss did not improve from 0.37224\n",
      "36805/36805 [==============================] - 29s 794us/sample - loss: 0.5663 - acc: 0.8230 - val_loss: 0.3885 - val_acc: 0.8852\n",
      "Epoch 130/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5594 - acc: 0.8239\n",
      "Epoch 00130: val_loss did not improve from 0.37224\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 0.5594 - acc: 0.8239 - val_loss: 0.3773 - val_acc: 0.8889\n",
      "Epoch 131/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5618 - acc: 0.8239\n",
      "Epoch 00131: val_loss did not improve from 0.37224\n",
      "36805/36805 [==============================] - 29s 796us/sample - loss: 0.5617 - acc: 0.8238 - val_loss: 0.3736 - val_acc: 0.8901\n",
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5609 - acc: 0.8234\n",
      "Epoch 00132: val_loss did not improve from 0.37224\n",
      "36805/36805 [==============================] - 29s 796us/sample - loss: 0.5610 - acc: 0.8234 - val_loss: 0.3853 - val_acc: 0.8845\n",
      "Epoch 133/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5611 - acc: 0.8224\n",
      "Epoch 00133: val_loss did not improve from 0.37224\n",
      "36805/36805 [==============================] - 29s 794us/sample - loss: 0.5610 - acc: 0.8224 - val_loss: 0.3769 - val_acc: 0.8880\n",
      "Epoch 134/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5624 - acc: 0.8217\n",
      "Epoch 00134: val_loss did not improve from 0.37224\n",
      "36805/36805 [==============================] - 29s 798us/sample - loss: 0.5623 - acc: 0.8217 - val_loss: 0.3808 - val_acc: 0.8898\n",
      "Epoch 135/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5588 - acc: 0.8219\n",
      "Epoch 00135: val_loss did not improve from 0.37224\n",
      "36805/36805 [==============================] - 29s 798us/sample - loss: 0.5587 - acc: 0.8219 - val_loss: 0.3785 - val_acc: 0.8917\n",
      "Epoch 136/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5563 - acc: 0.8261\n",
      "Epoch 00136: val_loss did not improve from 0.37224\n",
      "36805/36805 [==============================] - 29s 797us/sample - loss: 0.5562 - acc: 0.8262 - val_loss: 0.3732 - val_acc: 0.8898\n",
      "Epoch 137/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5544 - acc: 0.8233\n",
      "Epoch 00137: val_loss did not improve from 0.37224\n",
      "36805/36805 [==============================] - 29s 797us/sample - loss: 0.5545 - acc: 0.8232 - val_loss: 0.3727 - val_acc: 0.8908\n",
      "Epoch 138/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5490 - acc: 0.8289\n",
      "Epoch 00138: val_loss did not improve from 0.37224\n",
      "36805/36805 [==============================] - 29s 797us/sample - loss: 0.5489 - acc: 0.8289 - val_loss: 0.3793 - val_acc: 0.8863\n",
      "Epoch 139/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5455 - acc: 0.8285\n",
      "Epoch 00139: val_loss improved from 0.37224 to 0.37145, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_4_conv_checkpoint/139-0.3714.hdf5\n",
      "36805/36805 [==============================] - 29s 800us/sample - loss: 0.5455 - acc: 0.8285 - val_loss: 0.3714 - val_acc: 0.8926\n",
      "Epoch 140/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5422 - acc: 0.8292\n",
      "Epoch 00140: val_loss improved from 0.37145 to 0.37071, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_4_conv_checkpoint/140-0.3707.hdf5\n",
      "36805/36805 [==============================] - 29s 799us/sample - loss: 0.5420 - acc: 0.8293 - val_loss: 0.3707 - val_acc: 0.8910\n",
      "Epoch 141/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5459 - acc: 0.8274\n",
      "Epoch 00141: val_loss did not improve from 0.37071\n",
      "36805/36805 [==============================] - 29s 797us/sample - loss: 0.5458 - acc: 0.8275 - val_loss: 0.3784 - val_acc: 0.8880\n",
      "Epoch 142/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5472 - acc: 0.8285\n",
      "Epoch 00142: val_loss did not improve from 0.37071\n",
      "36805/36805 [==============================] - 29s 797us/sample - loss: 0.5470 - acc: 0.8286 - val_loss: 0.3798 - val_acc: 0.8840\n",
      "Epoch 143/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5419 - acc: 0.8276\n",
      "Epoch 00143: val_loss improved from 0.37071 to 0.36762, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_4_conv_checkpoint/143-0.3676.hdf5\n",
      "36805/36805 [==============================] - 29s 797us/sample - loss: 0.5419 - acc: 0.8276 - val_loss: 0.3676 - val_acc: 0.8926\n",
      "Epoch 144/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5422 - acc: 0.8278\n",
      "Epoch 00144: val_loss did not improve from 0.36762\n",
      "36805/36805 [==============================] - 29s 794us/sample - loss: 0.5419 - acc: 0.8278 - val_loss: 0.3694 - val_acc: 0.8901\n",
      "Epoch 145/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5422 - acc: 0.8265\n",
      "Epoch 00145: val_loss improved from 0.36762 to 0.36024, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_4_conv_checkpoint/145-0.3602.hdf5\n",
      "36805/36805 [==============================] - 29s 790us/sample - loss: 0.5423 - acc: 0.8266 - val_loss: 0.3602 - val_acc: 0.8919\n",
      "Epoch 146/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5459 - acc: 0.8288\n",
      "Epoch 00146: val_loss did not improve from 0.36024\n",
      "36805/36805 [==============================] - 29s 797us/sample - loss: 0.5459 - acc: 0.8288 - val_loss: 0.3711 - val_acc: 0.8891\n",
      "Epoch 147/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5433 - acc: 0.8308\n",
      "Epoch 00147: val_loss did not improve from 0.36024\n",
      "36805/36805 [==============================] - 29s 797us/sample - loss: 0.5433 - acc: 0.8309 - val_loss: 0.3777 - val_acc: 0.8838\n",
      "Epoch 148/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5458 - acc: 0.8280\n",
      "Epoch 00148: val_loss did not improve from 0.36024\n",
      "36805/36805 [==============================] - 29s 797us/sample - loss: 0.5460 - acc: 0.8280 - val_loss: 0.3707 - val_acc: 0.8928\n",
      "Epoch 149/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5363 - acc: 0.8312\n",
      "Epoch 00149: val_loss did not improve from 0.36024\n",
      "36805/36805 [==============================] - 29s 798us/sample - loss: 0.5364 - acc: 0.8312 - val_loss: 0.3671 - val_acc: 0.8901\n",
      "Epoch 150/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5420 - acc: 0.8274\n",
      "Epoch 00150: val_loss did not improve from 0.36024\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 0.5418 - acc: 0.8276 - val_loss: 0.3887 - val_acc: 0.8856\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 151/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5370 - acc: 0.8318\n",
      "Epoch 00151: val_loss did not improve from 0.36024\n",
      "36805/36805 [==============================] - 29s 798us/sample - loss: 0.5369 - acc: 0.8317 - val_loss: 0.3726 - val_acc: 0.8896\n",
      "Epoch 152/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5357 - acc: 0.8309\n",
      "Epoch 00152: val_loss did not improve from 0.36024\n",
      "36805/36805 [==============================] - 29s 797us/sample - loss: 0.5356 - acc: 0.8309 - val_loss: 0.3790 - val_acc: 0.8875\n",
      "Epoch 153/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5348 - acc: 0.8307\n",
      "Epoch 00153: val_loss did not improve from 0.36024\n",
      "36805/36805 [==============================] - 29s 797us/sample - loss: 0.5350 - acc: 0.8306 - val_loss: 0.3771 - val_acc: 0.8884\n",
      "Epoch 154/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5397 - acc: 0.8312\n",
      "Epoch 00154: val_loss did not improve from 0.36024\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 0.5398 - acc: 0.8312 - val_loss: 0.3741 - val_acc: 0.8931\n",
      "Epoch 155/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5315 - acc: 0.8324\n",
      "Epoch 00155: val_loss did not improve from 0.36024\n",
      "36805/36805 [==============================] - 29s 790us/sample - loss: 0.5315 - acc: 0.8324 - val_loss: 0.3709 - val_acc: 0.8901\n",
      "Epoch 156/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5318 - acc: 0.8311\n",
      "Epoch 00156: val_loss did not improve from 0.36024\n",
      "36805/36805 [==============================] - 29s 789us/sample - loss: 0.5319 - acc: 0.8311 - val_loss: 0.3833 - val_acc: 0.8884\n",
      "Epoch 157/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5368 - acc: 0.8314\n",
      "Epoch 00157: val_loss did not improve from 0.36024\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 0.5369 - acc: 0.8314 - val_loss: 0.3675 - val_acc: 0.8896\n",
      "Epoch 158/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5305 - acc: 0.8327\n",
      "Epoch 00158: val_loss did not improve from 0.36024\n",
      "36805/36805 [==============================] - 29s 787us/sample - loss: 0.5304 - acc: 0.8327 - val_loss: 0.3605 - val_acc: 0.8940\n",
      "Epoch 159/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5231 - acc: 0.8335\n",
      "Epoch 00159: val_loss did not improve from 0.36024\n",
      "36805/36805 [==============================] - 29s 787us/sample - loss: 0.5230 - acc: 0.8335 - val_loss: 0.3646 - val_acc: 0.8947\n",
      "Epoch 160/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5248 - acc: 0.8328\n",
      "Epoch 00160: val_loss did not improve from 0.36024\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 0.5247 - acc: 0.8328 - val_loss: 0.3640 - val_acc: 0.8894\n",
      "Epoch 161/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5265 - acc: 0.8341\n",
      "Epoch 00161: val_loss did not improve from 0.36024\n",
      "36805/36805 [==============================] - 29s 789us/sample - loss: 0.5264 - acc: 0.8342 - val_loss: 0.3640 - val_acc: 0.8912\n",
      "Epoch 162/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5270 - acc: 0.8362\n",
      "Epoch 00162: val_loss did not improve from 0.36024\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 0.5269 - acc: 0.8362 - val_loss: 0.3686 - val_acc: 0.8910\n",
      "Epoch 163/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5263 - acc: 0.8347\n",
      "Epoch 00163: val_loss did not improve from 0.36024\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 0.5266 - acc: 0.8346 - val_loss: 0.3665 - val_acc: 0.8924\n",
      "Epoch 164/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5253 - acc: 0.8337\n",
      "Epoch 00164: val_loss did not improve from 0.36024\n",
      "36805/36805 [==============================] - 29s 789us/sample - loss: 0.5254 - acc: 0.8337 - val_loss: 0.3614 - val_acc: 0.8917\n",
      "Epoch 165/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5198 - acc: 0.8363\n",
      "Epoch 00165: val_loss did not improve from 0.36024\n",
      "36805/36805 [==============================] - 29s 789us/sample - loss: 0.5200 - acc: 0.8362 - val_loss: 0.3769 - val_acc: 0.8891\n",
      "Epoch 166/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5280 - acc: 0.8336\n",
      "Epoch 00166: val_loss did not improve from 0.36024\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 0.5277 - acc: 0.8337 - val_loss: 0.3651 - val_acc: 0.8898\n",
      "Epoch 167/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5166 - acc: 0.8343\n",
      "Epoch 00167: val_loss did not improve from 0.36024\n",
      "36805/36805 [==============================] - 29s 797us/sample - loss: 0.5165 - acc: 0.8344 - val_loss: 0.3724 - val_acc: 0.8910\n",
      "Epoch 168/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5164 - acc: 0.8363\n",
      "Epoch 00168: val_loss did not improve from 0.36024\n",
      "36805/36805 [==============================] - 29s 797us/sample - loss: 0.5163 - acc: 0.8363 - val_loss: 0.3687 - val_acc: 0.8915\n",
      "Epoch 169/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5279 - acc: 0.8323\n",
      "Epoch 00169: val_loss did not improve from 0.36024\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 0.5279 - acc: 0.8323 - val_loss: 0.3824 - val_acc: 0.8838\n",
      "Epoch 170/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5154 - acc: 0.8361\n",
      "Epoch 00170: val_loss did not improve from 0.36024\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 0.5157 - acc: 0.8360 - val_loss: 0.3704 - val_acc: 0.8935\n",
      "Epoch 171/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5145 - acc: 0.8360\n",
      "Epoch 00171: val_loss did not improve from 0.36024\n",
      "36805/36805 [==============================] - 29s 797us/sample - loss: 0.5145 - acc: 0.8360 - val_loss: 0.3670 - val_acc: 0.8924\n",
      "Epoch 172/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5186 - acc: 0.8371\n",
      "Epoch 00172: val_loss did not improve from 0.36024\n",
      "36805/36805 [==============================] - 29s 797us/sample - loss: 0.5186 - acc: 0.8371 - val_loss: 0.3649 - val_acc: 0.8931\n",
      "Epoch 173/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5133 - acc: 0.8372\n",
      "Epoch 00173: val_loss did not improve from 0.36024\n",
      "36805/36805 [==============================] - 29s 797us/sample - loss: 0.5132 - acc: 0.8372 - val_loss: 0.3825 - val_acc: 0.8845\n",
      "Epoch 174/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5128 - acc: 0.8364\n",
      "Epoch 00174: val_loss improved from 0.36024 to 0.35862, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_4_conv_checkpoint/174-0.3586.hdf5\n",
      "36805/36805 [==============================] - 29s 798us/sample - loss: 0.5128 - acc: 0.8364 - val_loss: 0.3586 - val_acc: 0.8952\n",
      "Epoch 175/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5167 - acc: 0.8357\n",
      "Epoch 00175: val_loss did not improve from 0.35862\n",
      "36805/36805 [==============================] - 29s 796us/sample - loss: 0.5168 - acc: 0.8357 - val_loss: 0.3749 - val_acc: 0.8891\n",
      "Epoch 176/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5132 - acc: 0.8389\n",
      "Epoch 00176: val_loss did not improve from 0.35862\n",
      "36805/36805 [==============================] - 29s 797us/sample - loss: 0.5133 - acc: 0.8389 - val_loss: 0.3655 - val_acc: 0.8891\n",
      "Epoch 177/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5094 - acc: 0.8384\n",
      "Epoch 00177: val_loss did not improve from 0.35862\n",
      "36805/36805 [==============================] - 29s 796us/sample - loss: 0.5092 - acc: 0.8385 - val_loss: 0.3623 - val_acc: 0.8954\n",
      "Epoch 178/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5112 - acc: 0.8388\n",
      "Epoch 00178: val_loss did not improve from 0.35862\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 0.5112 - acc: 0.8387 - val_loss: 0.3649 - val_acc: 0.8942\n",
      "Epoch 179/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5102 - acc: 0.8358\n",
      "Epoch 00179: val_loss did not improve from 0.35862\n",
      "36805/36805 [==============================] - 29s 793us/sample - loss: 0.5102 - acc: 0.8358 - val_loss: 0.3697 - val_acc: 0.8903\n",
      "Epoch 180/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5063 - acc: 0.8398\n",
      "Epoch 00180: val_loss did not improve from 0.35862\n",
      "36805/36805 [==============================] - 29s 798us/sample - loss: 0.5065 - acc: 0.8398 - val_loss: 0.3598 - val_acc: 0.8961\n",
      "Epoch 181/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5096 - acc: 0.8397\n",
      "Epoch 00181: val_loss did not improve from 0.35862\n",
      "36805/36805 [==============================] - 29s 790us/sample - loss: 0.5095 - acc: 0.8398 - val_loss: 0.3615 - val_acc: 0.8949\n",
      "Epoch 182/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5052 - acc: 0.8404\n",
      "Epoch 00182: val_loss did not improve from 0.35862\n",
      "36805/36805 [==============================] - 29s 789us/sample - loss: 0.5052 - acc: 0.8404 - val_loss: 0.3660 - val_acc: 0.8905\n",
      "Epoch 183/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5103 - acc: 0.8362\n",
      "Epoch 00183: val_loss did not improve from 0.35862\n",
      "36805/36805 [==============================] - 29s 794us/sample - loss: 0.5106 - acc: 0.8362 - val_loss: 0.3681 - val_acc: 0.8891\n",
      "Epoch 184/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5015 - acc: 0.8395\n",
      "Epoch 00184: val_loss did not improve from 0.35862\n",
      "36805/36805 [==============================] - 29s 799us/sample - loss: 0.5014 - acc: 0.8395 - val_loss: 0.3676 - val_acc: 0.8931\n",
      "Epoch 185/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5056 - acc: 0.8403\n",
      "Epoch 00185: val_loss did not improve from 0.35862\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 0.5057 - acc: 0.8402 - val_loss: 0.3656 - val_acc: 0.8905\n",
      "Epoch 186/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5014 - acc: 0.8392\n",
      "Epoch 00186: val_loss improved from 0.35862 to 0.35191, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_4_conv_checkpoint/186-0.3519.hdf5\n",
      "36805/36805 [==============================] - 29s 796us/sample - loss: 0.5011 - acc: 0.8393 - val_loss: 0.3519 - val_acc: 0.8952\n",
      "Epoch 187/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4998 - acc: 0.8406\n",
      "Epoch 00187: val_loss did not improve from 0.35191\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 0.4997 - acc: 0.8406 - val_loss: 0.3586 - val_acc: 0.8931\n",
      "Epoch 188/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5003 - acc: 0.8431\n",
      "Epoch 00188: val_loss did not improve from 0.35191\n",
      "36805/36805 [==============================] - 29s 790us/sample - loss: 0.5003 - acc: 0.8431 - val_loss: 0.3555 - val_acc: 0.8952\n",
      "Epoch 189/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4937 - acc: 0.8418\n",
      "Epoch 00189: val_loss did not improve from 0.35191\n",
      "36805/36805 [==============================] - 29s 783us/sample - loss: 0.4939 - acc: 0.8417 - val_loss: 0.3578 - val_acc: 0.8921\n",
      "Epoch 190/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5048 - acc: 0.8387\n",
      "Epoch 00190: val_loss did not improve from 0.35191\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 0.5049 - acc: 0.8387 - val_loss: 0.3552 - val_acc: 0.8924\n",
      "Epoch 191/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4996 - acc: 0.8423\n",
      "Epoch 00191: val_loss did not improve from 0.35191\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 0.4996 - acc: 0.8424 - val_loss: 0.3526 - val_acc: 0.8954\n",
      "Epoch 192/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5040 - acc: 0.8413\n",
      "Epoch 00192: val_loss did not improve from 0.35191\n",
      "36805/36805 [==============================] - 29s 796us/sample - loss: 0.5038 - acc: 0.8413 - val_loss: 0.3577 - val_acc: 0.8938\n",
      "Epoch 193/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4997 - acc: 0.8405\n",
      "Epoch 00193: val_loss did not improve from 0.35191\n",
      "36805/36805 [==============================] - 29s 797us/sample - loss: 0.4998 - acc: 0.8405 - val_loss: 0.3549 - val_acc: 0.8938\n",
      "Epoch 194/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4987 - acc: 0.8414\n",
      "Epoch 00194: val_loss did not improve from 0.35191\n",
      "36805/36805 [==============================] - 29s 796us/sample - loss: 0.4987 - acc: 0.8415 - val_loss: 0.3556 - val_acc: 0.8938\n",
      "Epoch 195/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4932 - acc: 0.8434\n",
      "Epoch 00195: val_loss did not improve from 0.35191\n",
      "36805/36805 [==============================] - 29s 791us/sample - loss: 0.4934 - acc: 0.8434 - val_loss: 0.3644 - val_acc: 0.8933\n",
      "Epoch 196/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4910 - acc: 0.8425\n",
      "Epoch 00196: val_loss did not improve from 0.35191\n",
      "36805/36805 [==============================] - 29s 794us/sample - loss: 0.4911 - acc: 0.8425 - val_loss: 0.3592 - val_acc: 0.8954\n",
      "Epoch 197/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5000 - acc: 0.8432\n",
      "Epoch 00197: val_loss improved from 0.35191 to 0.35144, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_4_conv_checkpoint/197-0.3514.hdf5\n",
      "36805/36805 [==============================] - 29s 789us/sample - loss: 0.4997 - acc: 0.8433 - val_loss: 0.3514 - val_acc: 0.8968\n",
      "Epoch 198/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4898 - acc: 0.8442\n",
      "Epoch 00198: val_loss did not improve from 0.35144\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 0.4899 - acc: 0.8442 - val_loss: 0.3611 - val_acc: 0.8908\n",
      "Epoch 199/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4971 - acc: 0.8424\n",
      "Epoch 00199: val_loss did not improve from 0.35144\n",
      "36805/36805 [==============================] - 29s 797us/sample - loss: 0.4972 - acc: 0.8424 - val_loss: 0.3720 - val_acc: 0.8908\n",
      "Epoch 200/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4908 - acc: 0.8440\n",
      "Epoch 00200: val_loss did not improve from 0.35144\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 0.4909 - acc: 0.8440 - val_loss: 0.3602 - val_acc: 0.8963\n",
      "Epoch 201/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4900 - acc: 0.8472\n",
      "Epoch 00201: val_loss did not improve from 0.35144\n",
      "36805/36805 [==============================] - 29s 787us/sample - loss: 0.4899 - acc: 0.8472 - val_loss: 0.3550 - val_acc: 0.8926\n",
      "Epoch 202/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4936 - acc: 0.8459\n",
      "Epoch 00202: val_loss did not improve from 0.35144\n",
      "36805/36805 [==============================] - 29s 787us/sample - loss: 0.4936 - acc: 0.8459 - val_loss: 0.3561 - val_acc: 0.8987\n",
      "Epoch 203/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4885 - acc: 0.8462\n",
      "Epoch 00203: val_loss did not improve from 0.35144\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 0.4884 - acc: 0.8462 - val_loss: 0.3575 - val_acc: 0.8921\n",
      "Epoch 204/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4964 - acc: 0.8429\n",
      "Epoch 00204: val_loss did not improve from 0.35144\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 0.4964 - acc: 0.8429 - val_loss: 0.3612 - val_acc: 0.8928\n",
      "Epoch 205/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4929 - acc: 0.8430\n",
      "Epoch 00205: val_loss did not improve from 0.35144\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 0.4931 - acc: 0.8430 - val_loss: 0.3624 - val_acc: 0.8880\n",
      "Epoch 206/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4852 - acc: 0.8455\n",
      "Epoch 00206: val_loss did not improve from 0.35144\n",
      "36805/36805 [==============================] - 29s 797us/sample - loss: 0.4852 - acc: 0.8455 - val_loss: 0.3555 - val_acc: 0.8949\n",
      "Epoch 207/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4798 - acc: 0.8448\n",
      "Epoch 00207: val_loss did not improve from 0.35144\n",
      "36805/36805 [==============================] - 29s 797us/sample - loss: 0.4803 - acc: 0.8447 - val_loss: 0.3606 - val_acc: 0.8908\n",
      "Epoch 208/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4856 - acc: 0.8453\n",
      "Epoch 00208: val_loss did not improve from 0.35144\n",
      "36805/36805 [==============================] - 29s 799us/sample - loss: 0.4856 - acc: 0.8453 - val_loss: 0.3740 - val_acc: 0.8889\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 209/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4859 - acc: 0.8443\n",
      "Epoch 00209: val_loss did not improve from 0.35144\n",
      "36805/36805 [==============================] - 29s 799us/sample - loss: 0.4859 - acc: 0.8443 - val_loss: 0.3544 - val_acc: 0.8921\n",
      "Epoch 210/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4883 - acc: 0.8435\n",
      "Epoch 00210: val_loss did not improve from 0.35144\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 0.4883 - acc: 0.8435 - val_loss: 0.3556 - val_acc: 0.8919\n",
      "Epoch 211/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4874 - acc: 0.8445\n",
      "Epoch 00211: val_loss did not improve from 0.35144\n",
      "36805/36805 [==============================] - 29s 801us/sample - loss: 0.4874 - acc: 0.8445 - val_loss: 0.3518 - val_acc: 0.8954\n",
      "Epoch 212/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4800 - acc: 0.8463\n",
      "Epoch 00212: val_loss did not improve from 0.35144\n",
      "36805/36805 [==============================] - 29s 798us/sample - loss: 0.4794 - acc: 0.8466 - val_loss: 0.3599 - val_acc: 0.8949\n",
      "Epoch 213/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4801 - acc: 0.8471\n",
      "Epoch 00213: val_loss did not improve from 0.35144\n",
      "36805/36805 [==============================] - 29s 798us/sample - loss: 0.4801 - acc: 0.8471 - val_loss: 0.3583 - val_acc: 0.8933\n",
      "Epoch 214/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4804 - acc: 0.8470\n",
      "Epoch 00214: val_loss did not improve from 0.35144\n",
      "36805/36805 [==============================] - 29s 797us/sample - loss: 0.4803 - acc: 0.8470 - val_loss: 0.3523 - val_acc: 0.8933\n",
      "Epoch 215/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4811 - acc: 0.8463\n",
      "Epoch 00215: val_loss did not improve from 0.35144\n",
      "36805/36805 [==============================] - 29s 798us/sample - loss: 0.4809 - acc: 0.8463 - val_loss: 0.3668 - val_acc: 0.8931\n",
      "Epoch 216/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4815 - acc: 0.8457\n",
      "Epoch 00216: val_loss did not improve from 0.35144\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 0.4818 - acc: 0.8457 - val_loss: 0.3627 - val_acc: 0.8903\n",
      "Epoch 217/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4807 - acc: 0.8478\n",
      "Epoch 00217: val_loss did not improve from 0.35144\n",
      "36805/36805 [==============================] - 29s 797us/sample - loss: 0.4804 - acc: 0.8479 - val_loss: 0.3589 - val_acc: 0.8945\n",
      "Epoch 218/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4792 - acc: 0.8467\n",
      "Epoch 00218: val_loss improved from 0.35144 to 0.35115, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_4_conv_checkpoint/218-0.3512.hdf5\n",
      "36805/36805 [==============================] - 29s 797us/sample - loss: 0.4791 - acc: 0.8468 - val_loss: 0.3512 - val_acc: 0.8963\n",
      "Epoch 219/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4764 - acc: 0.8477\n",
      "Epoch 00219: val_loss did not improve from 0.35115\n",
      "36805/36805 [==============================] - 29s 797us/sample - loss: 0.4766 - acc: 0.8477 - val_loss: 0.3620 - val_acc: 0.8954\n",
      "Epoch 220/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4827 - acc: 0.8462\n",
      "Epoch 00220: val_loss did not improve from 0.35115\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 0.4825 - acc: 0.8463 - val_loss: 0.3563 - val_acc: 0.8898\n",
      "Epoch 221/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4746 - acc: 0.8490\n",
      "Epoch 00221: val_loss did not improve from 0.35115\n",
      "36805/36805 [==============================] - 29s 790us/sample - loss: 0.4746 - acc: 0.8491 - val_loss: 0.3515 - val_acc: 0.8954\n",
      "Epoch 222/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4727 - acc: 0.8480\n",
      "Epoch 00222: val_loss did not improve from 0.35115\n",
      "36805/36805 [==============================] - 29s 796us/sample - loss: 0.4727 - acc: 0.8480 - val_loss: 0.3561 - val_acc: 0.8931\n",
      "Epoch 223/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4717 - acc: 0.8492\n",
      "Epoch 00223: val_loss did not improve from 0.35115\n",
      "36805/36805 [==============================] - 29s 798us/sample - loss: 0.4717 - acc: 0.8492 - val_loss: 0.3593 - val_acc: 0.8919\n",
      "Epoch 224/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4733 - acc: 0.8489\n",
      "Epoch 00224: val_loss did not improve from 0.35115\n",
      "36805/36805 [==============================] - 29s 790us/sample - loss: 0.4735 - acc: 0.8489 - val_loss: 0.3556 - val_acc: 0.8954\n",
      "Epoch 225/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4792 - acc: 0.8474\n",
      "Epoch 00225: val_loss did not improve from 0.35115\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 0.4791 - acc: 0.8474 - val_loss: 0.3546 - val_acc: 0.8924\n",
      "Epoch 226/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4693 - acc: 0.8518\n",
      "Epoch 00226: val_loss did not improve from 0.35115\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 0.4694 - acc: 0.8517 - val_loss: 0.3592 - val_acc: 0.8926\n",
      "Epoch 227/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4745 - acc: 0.8493\n",
      "Epoch 00227: val_loss did not improve from 0.35115\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 0.4744 - acc: 0.8493 - val_loss: 0.3522 - val_acc: 0.8938\n",
      "Epoch 228/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4715 - acc: 0.8478\n",
      "Epoch 00228: val_loss improved from 0.35115 to 0.35022, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_4_conv_checkpoint/228-0.3502.hdf5\n",
      "36805/36805 [==============================] - 29s 798us/sample - loss: 0.4720 - acc: 0.8477 - val_loss: 0.3502 - val_acc: 0.8970\n",
      "Epoch 229/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4735 - acc: 0.8487\n",
      "Epoch 00229: val_loss did not improve from 0.35022\n",
      "36805/36805 [==============================] - 29s 794us/sample - loss: 0.4734 - acc: 0.8487 - val_loss: 0.3503 - val_acc: 0.8949\n",
      "Epoch 230/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4752 - acc: 0.8468\n",
      "Epoch 00230: val_loss did not improve from 0.35022\n",
      "36805/36805 [==============================] - 29s 797us/sample - loss: 0.4752 - acc: 0.8468 - val_loss: 0.3579 - val_acc: 0.8935\n",
      "Epoch 231/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4711 - acc: 0.8498\n",
      "Epoch 00231: val_loss did not improve from 0.35022\n",
      "36805/36805 [==============================] - 29s 796us/sample - loss: 0.4710 - acc: 0.8499 - val_loss: 0.3546 - val_acc: 0.8963\n",
      "Epoch 232/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4661 - acc: 0.8506\n",
      "Epoch 00232: val_loss did not improve from 0.35022\n",
      "36805/36805 [==============================] - 29s 796us/sample - loss: 0.4661 - acc: 0.8506 - val_loss: 0.3559 - val_acc: 0.8926\n",
      "Epoch 233/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4650 - acc: 0.8525\n",
      "Epoch 00233: val_loss did not improve from 0.35022\n",
      "36805/36805 [==============================] - 29s 796us/sample - loss: 0.4652 - acc: 0.8524 - val_loss: 0.3508 - val_acc: 0.8915\n",
      "Epoch 234/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4651 - acc: 0.8508\n",
      "Epoch 00234: val_loss did not improve from 0.35022\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 0.4651 - acc: 0.8508 - val_loss: 0.3543 - val_acc: 0.8942\n",
      "Epoch 235/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4713 - acc: 0.8514\n",
      "Epoch 00235: val_loss did not improve from 0.35022\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 0.4709 - acc: 0.8515 - val_loss: 0.3573 - val_acc: 0.8896\n",
      "Epoch 236/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4647 - acc: 0.8519\n",
      "Epoch 00236: val_loss did not improve from 0.35022\n",
      "36805/36805 [==============================] - 29s 799us/sample - loss: 0.4647 - acc: 0.8519 - val_loss: 0.3703 - val_acc: 0.8889\n",
      "Epoch 237/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4593 - acc: 0.8545\n",
      "Epoch 00237: val_loss did not improve from 0.35022\n",
      "36805/36805 [==============================] - 29s 796us/sample - loss: 0.4593 - acc: 0.8545 - val_loss: 0.3589 - val_acc: 0.8926\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 238/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4739 - acc: 0.8472\n",
      "Epoch 00238: val_loss did not improve from 0.35022\n",
      "36805/36805 [==============================] - 29s 798us/sample - loss: 0.4739 - acc: 0.8472 - val_loss: 0.3579 - val_acc: 0.8924\n",
      "Epoch 239/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4627 - acc: 0.8513\n",
      "Epoch 00239: val_loss did not improve from 0.35022\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 0.4626 - acc: 0.8513 - val_loss: 0.3580 - val_acc: 0.8928\n",
      "Epoch 240/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4640 - acc: 0.8516\n",
      "Epoch 00240: val_loss did not improve from 0.35022\n",
      "36805/36805 [==============================] - 29s 798us/sample - loss: 0.4639 - acc: 0.8515 - val_loss: 0.3553 - val_acc: 0.8940\n",
      "Epoch 241/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4623 - acc: 0.8532\n",
      "Epoch 00241: val_loss did not improve from 0.35022\n",
      "36805/36805 [==============================] - 29s 787us/sample - loss: 0.4619 - acc: 0.8533 - val_loss: 0.3564 - val_acc: 0.8945\n",
      "Epoch 242/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4666 - acc: 0.8506\n",
      "Epoch 00242: val_loss did not improve from 0.35022\n",
      "36805/36805 [==============================] - 29s 790us/sample - loss: 0.4666 - acc: 0.8506 - val_loss: 0.3664 - val_acc: 0.8898\n",
      "Epoch 243/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4610 - acc: 0.8522\n",
      "Epoch 00243: val_loss did not improve from 0.35022\n",
      "36805/36805 [==============================] - 29s 796us/sample - loss: 0.4610 - acc: 0.8522 - val_loss: 0.3561 - val_acc: 0.8924\n",
      "Epoch 244/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4625 - acc: 0.8526\n",
      "Epoch 00244: val_loss did not improve from 0.35022\n",
      "36805/36805 [==============================] - 29s 798us/sample - loss: 0.4624 - acc: 0.8526 - val_loss: 0.3595 - val_acc: 0.8901\n",
      "Epoch 245/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4623 - acc: 0.8508\n",
      "Epoch 00245: val_loss did not improve from 0.35022\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 0.4621 - acc: 0.8509 - val_loss: 0.3590 - val_acc: 0.8956\n",
      "Epoch 246/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4657 - acc: 0.8524\n",
      "Epoch 00246: val_loss did not improve from 0.35022\n",
      "36805/36805 [==============================] - 29s 796us/sample - loss: 0.4658 - acc: 0.8524 - val_loss: 0.3517 - val_acc: 0.8959\n",
      "Epoch 247/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4597 - acc: 0.8525\n",
      "Epoch 00247: val_loss did not improve from 0.35022\n",
      "36805/36805 [==============================] - 29s 797us/sample - loss: 0.4599 - acc: 0.8524 - val_loss: 0.3531 - val_acc: 0.8947\n",
      "Epoch 248/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4592 - acc: 0.8526\n",
      "Epoch 00248: val_loss did not improve from 0.35022\n",
      "36805/36805 [==============================] - 29s 797us/sample - loss: 0.4594 - acc: 0.8527 - val_loss: 0.3587 - val_acc: 0.8938\n",
      "Epoch 249/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4632 - acc: 0.8528\n",
      "Epoch 00249: val_loss did not improve from 0.35022\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 0.4631 - acc: 0.8528 - val_loss: 0.3530 - val_acc: 0.8970\n",
      "Epoch 250/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4601 - acc: 0.8522\n",
      "Epoch 00250: val_loss did not improve from 0.35022\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 0.4601 - acc: 0.8522 - val_loss: 0.3565 - val_acc: 0.8926\n",
      "Epoch 251/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4589 - acc: 0.8521\n",
      "Epoch 00251: val_loss did not improve from 0.35022\n",
      "36805/36805 [==============================] - 29s 797us/sample - loss: 0.4588 - acc: 0.8521 - val_loss: 0.3640 - val_acc: 0.8898\n",
      "Epoch 252/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4526 - acc: 0.8547\n",
      "Epoch 00252: val_loss did not improve from 0.35022\n",
      "36805/36805 [==============================] - 29s 797us/sample - loss: 0.4526 - acc: 0.8547 - val_loss: 0.3628 - val_acc: 0.8917\n",
      "Epoch 253/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4610 - acc: 0.8533\n",
      "Epoch 00253: val_loss did not improve from 0.35022\n",
      "36805/36805 [==============================] - 29s 794us/sample - loss: 0.4610 - acc: 0.8533 - val_loss: 0.3682 - val_acc: 0.8921\n",
      "Epoch 254/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4528 - acc: 0.8548\n",
      "Epoch 00254: val_loss did not improve from 0.35022\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 0.4529 - acc: 0.8547 - val_loss: 0.3619 - val_acc: 0.8912\n",
      "Epoch 255/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4498 - acc: 0.8569\n",
      "Epoch 00255: val_loss did not improve from 0.35022\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 0.4496 - acc: 0.8570 - val_loss: 0.3608 - val_acc: 0.8931\n",
      "Epoch 256/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4597 - acc: 0.8529\n",
      "Epoch 00256: val_loss did not improve from 0.35022\n",
      "36805/36805 [==============================] - 29s 797us/sample - loss: 0.4597 - acc: 0.8529 - val_loss: 0.3672 - val_acc: 0.8884\n",
      "Epoch 257/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4550 - acc: 0.8525\n",
      "Epoch 00257: val_loss did not improve from 0.35022\n",
      "36805/36805 [==============================] - 29s 794us/sample - loss: 0.4550 - acc: 0.8525 - val_loss: 0.3557 - val_acc: 0.8963\n",
      "Epoch 258/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4569 - acc: 0.8538\n",
      "Epoch 00258: val_loss did not improve from 0.35022\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 0.4569 - acc: 0.8538 - val_loss: 0.3517 - val_acc: 0.8947\n",
      "Epoch 259/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4543 - acc: 0.8521\n",
      "Epoch 00259: val_loss did not improve from 0.35022\n",
      "36805/36805 [==============================] - 29s 797us/sample - loss: 0.4540 - acc: 0.8522 - val_loss: 0.3638 - val_acc: 0.8873\n",
      "Epoch 260/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4564 - acc: 0.8544\n",
      "Epoch 00260: val_loss did not improve from 0.35022\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 0.4563 - acc: 0.8544 - val_loss: 0.3684 - val_acc: 0.8905\n",
      "Epoch 261/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4554 - acc: 0.8537\n",
      "Epoch 00261: val_loss did not improve from 0.35022\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 0.4552 - acc: 0.8538 - val_loss: 0.3521 - val_acc: 0.8956\n",
      "Epoch 262/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4528 - acc: 0.8551\n",
      "Epoch 00262: val_loss did not improve from 0.35022\n",
      "36805/36805 [==============================] - 29s 787us/sample - loss: 0.4529 - acc: 0.8551 - val_loss: 0.3608 - val_acc: 0.8933\n",
      "Epoch 263/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4502 - acc: 0.8549\n",
      "Epoch 00263: val_loss did not improve from 0.35022\n",
      "36805/36805 [==============================] - 29s 794us/sample - loss: 0.4502 - acc: 0.8549 - val_loss: 0.3587 - val_acc: 0.8921\n",
      "Epoch 264/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4490 - acc: 0.8553\n",
      "Epoch 00264: val_loss did not improve from 0.35022\n",
      "36805/36805 [==============================] - 29s 796us/sample - loss: 0.4490 - acc: 0.8553 - val_loss: 0.3566 - val_acc: 0.8949\n",
      "Epoch 265/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4492 - acc: 0.8543\n",
      "Epoch 00265: val_loss did not improve from 0.35022\n",
      "36805/36805 [==============================] - 29s 789us/sample - loss: 0.4491 - acc: 0.8543 - val_loss: 0.3534 - val_acc: 0.8956\n",
      "Epoch 266/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4518 - acc: 0.8551\n",
      "Epoch 00266: val_loss did not improve from 0.35022\n",
      "36805/36805 [==============================] - 29s 787us/sample - loss: 0.4518 - acc: 0.8550 - val_loss: 0.3543 - val_acc: 0.8966\n",
      "Epoch 267/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4437 - acc: 0.8571\n",
      "Epoch 00267: val_loss did not improve from 0.35022\n",
      "36805/36805 [==============================] - 29s 796us/sample - loss: 0.4440 - acc: 0.8570 - val_loss: 0.3511 - val_acc: 0.8963\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 268/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4468 - acc: 0.8557\n",
      "Epoch 00268: val_loss did not improve from 0.35022\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 0.4468 - acc: 0.8556 - val_loss: 0.3520 - val_acc: 0.8947\n",
      "Epoch 269/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4444 - acc: 0.8558\n",
      "Epoch 00269: val_loss improved from 0.35022 to 0.34918, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_4_conv_checkpoint/269-0.3492.hdf5\n",
      "36805/36805 [==============================] - 29s 798us/sample - loss: 0.4442 - acc: 0.8559 - val_loss: 0.3492 - val_acc: 0.8984\n",
      "Epoch 270/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4451 - acc: 0.8563\n",
      "Epoch 00270: val_loss did not improve from 0.34918\n",
      "36805/36805 [==============================] - 29s 796us/sample - loss: 0.4451 - acc: 0.8563 - val_loss: 0.3554 - val_acc: 0.8966\n",
      "Epoch 271/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4459 - acc: 0.8563\n",
      "Epoch 00271: val_loss did not improve from 0.34918\n",
      "36805/36805 [==============================] - 29s 797us/sample - loss: 0.4457 - acc: 0.8564 - val_loss: 0.3618 - val_acc: 0.8942\n",
      "Epoch 272/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4451 - acc: 0.8581\n",
      "Epoch 00272: val_loss did not improve from 0.34918\n",
      "36805/36805 [==============================] - 29s 796us/sample - loss: 0.4450 - acc: 0.8581 - val_loss: 0.3550 - val_acc: 0.8921\n",
      "Epoch 273/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4426 - acc: 0.8589\n",
      "Epoch 00273: val_loss did not improve from 0.34918\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 0.4429 - acc: 0.8589 - val_loss: 0.3630 - val_acc: 0.8928\n",
      "Epoch 274/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4464 - acc: 0.8562\n",
      "Epoch 00274: val_loss did not improve from 0.34918\n",
      "36805/36805 [==============================] - 29s 796us/sample - loss: 0.4464 - acc: 0.8562 - val_loss: 0.3638 - val_acc: 0.8912\n",
      "Epoch 275/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4523 - acc: 0.8560\n",
      "Epoch 00275: val_loss did not improve from 0.34918\n",
      "36805/36805 [==============================] - 29s 796us/sample - loss: 0.4521 - acc: 0.8561 - val_loss: 0.3629 - val_acc: 0.8938\n",
      "Epoch 276/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4403 - acc: 0.8605\n",
      "Epoch 00276: val_loss did not improve from 0.34918\n",
      "36805/36805 [==============================] - 29s 796us/sample - loss: 0.4402 - acc: 0.8605 - val_loss: 0.3556 - val_acc: 0.8956\n",
      "Epoch 277/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4468 - acc: 0.8576\n",
      "Epoch 00277: val_loss did not improve from 0.34918\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 0.4471 - acc: 0.8575 - val_loss: 0.3581 - val_acc: 0.8933\n",
      "Epoch 278/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4406 - acc: 0.8577\n",
      "Epoch 00278: val_loss did not improve from 0.34918\n",
      "36805/36805 [==============================] - 29s 797us/sample - loss: 0.4405 - acc: 0.8577 - val_loss: 0.3668 - val_acc: 0.8859\n",
      "Epoch 279/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4452 - acc: 0.8560\n",
      "Epoch 00279: val_loss did not improve from 0.34918\n",
      "36805/36805 [==============================] - 29s 797us/sample - loss: 0.4450 - acc: 0.8561 - val_loss: 0.3658 - val_acc: 0.8917\n",
      "Epoch 280/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4438 - acc: 0.8558\n",
      "Epoch 00280: val_loss did not improve from 0.34918\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 0.4438 - acc: 0.8558 - val_loss: 0.3567 - val_acc: 0.8938\n",
      "Epoch 281/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4412 - acc: 0.8578\n",
      "Epoch 00281: val_loss did not improve from 0.34918\n",
      "36805/36805 [==============================] - 29s 796us/sample - loss: 0.4409 - acc: 0.8578 - val_loss: 0.3543 - val_acc: 0.8947\n",
      "Epoch 282/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4443 - acc: 0.8570\n",
      "Epoch 00282: val_loss did not improve from 0.34918\n",
      "36805/36805 [==============================] - 29s 789us/sample - loss: 0.4443 - acc: 0.8569 - val_loss: 0.3527 - val_acc: 0.8928\n",
      "Epoch 283/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4367 - acc: 0.8598\n",
      "Epoch 00283: val_loss did not improve from 0.34918\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 0.4368 - acc: 0.8597 - val_loss: 0.3509 - val_acc: 0.8952\n",
      "Epoch 284/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4379 - acc: 0.8597\n",
      "Epoch 00284: val_loss did not improve from 0.34918\n",
      "36805/36805 [==============================] - 29s 797us/sample - loss: 0.4379 - acc: 0.8597 - val_loss: 0.3589 - val_acc: 0.8915\n",
      "Epoch 285/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4397 - acc: 0.8587\n",
      "Epoch 00285: val_loss did not improve from 0.34918\n",
      "36805/36805 [==============================] - 29s 798us/sample - loss: 0.4396 - acc: 0.8587 - val_loss: 0.3643 - val_acc: 0.8912\n",
      "Epoch 286/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4361 - acc: 0.8595\n",
      "Epoch 00286: val_loss did not improve from 0.34918\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 0.4359 - acc: 0.8596 - val_loss: 0.3584 - val_acc: 0.8949\n",
      "Epoch 287/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4397 - acc: 0.8572\n",
      "Epoch 00287: val_loss did not improve from 0.34918\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 0.4396 - acc: 0.8572 - val_loss: 0.3596 - val_acc: 0.8905\n",
      "Epoch 288/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4376 - acc: 0.8594\n",
      "Epoch 00288: val_loss did not improve from 0.34918\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 0.4376 - acc: 0.8594 - val_loss: 0.3635 - val_acc: 0.8931\n",
      "Epoch 289/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4344 - acc: 0.8607\n",
      "Epoch 00289: val_loss did not improve from 0.34918\n",
      "36805/36805 [==============================] - 29s 796us/sample - loss: 0.4344 - acc: 0.8608 - val_loss: 0.3597 - val_acc: 0.8931\n",
      "Epoch 290/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4371 - acc: 0.8589\n",
      "Epoch 00290: val_loss did not improve from 0.34918\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 0.4373 - acc: 0.8589 - val_loss: 0.3560 - val_acc: 0.8938\n",
      "Epoch 291/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4350 - acc: 0.8607\n",
      "Epoch 00291: val_loss did not improve from 0.34918\n",
      "36805/36805 [==============================] - 29s 794us/sample - loss: 0.4350 - acc: 0.8607 - val_loss: 0.3577 - val_acc: 0.8961\n",
      "Epoch 292/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4338 - acc: 0.8618\n",
      "Epoch 00292: val_loss did not improve from 0.34918\n",
      "36805/36805 [==============================] - 29s 794us/sample - loss: 0.4338 - acc: 0.8618 - val_loss: 0.3614 - val_acc: 0.8921\n",
      "Epoch 293/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4371 - acc: 0.8578\n",
      "Epoch 00293: val_loss did not improve from 0.34918\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 0.4372 - acc: 0.8578 - val_loss: 0.3567 - val_acc: 0.8921\n",
      "Epoch 294/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4296 - acc: 0.8615\n",
      "Epoch 00294: val_loss did not improve from 0.34918\n",
      "36805/36805 [==============================] - 29s 796us/sample - loss: 0.4296 - acc: 0.8616 - val_loss: 0.3558 - val_acc: 0.8931\n",
      "Epoch 295/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4287 - acc: 0.8618\n",
      "Epoch 00295: val_loss did not improve from 0.34918\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 0.4287 - acc: 0.8618 - val_loss: 0.3609 - val_acc: 0.8935\n",
      "Epoch 296/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4318 - acc: 0.8604\n",
      "Epoch 00296: val_loss did not improve from 0.34918\n",
      "36805/36805 [==============================] - 29s 794us/sample - loss: 0.4321 - acc: 0.8604 - val_loss: 0.3541 - val_acc: 0.8945\n",
      "Epoch 297/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4325 - acc: 0.8590\n",
      "Epoch 00297: val_loss did not improve from 0.34918\n",
      "36805/36805 [==============================] - 29s 796us/sample - loss: 0.4325 - acc: 0.8590 - val_loss: 0.3592 - val_acc: 0.8947\n",
      "Epoch 298/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4416 - acc: 0.8579\n",
      "Epoch 00298: val_loss did not improve from 0.34918\n",
      "36805/36805 [==============================] - 29s 794us/sample - loss: 0.4414 - acc: 0.8579 - val_loss: 0.3546 - val_acc: 0.8959\n",
      "Epoch 299/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4275 - acc: 0.8617\n",
      "Epoch 00299: val_loss did not improve from 0.34918\n",
      "36805/36805 [==============================] - 29s 797us/sample - loss: 0.4275 - acc: 0.8618 - val_loss: 0.3625 - val_acc: 0.8928\n",
      "Epoch 300/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4326 - acc: 0.8605\n",
      "Epoch 00300: val_loss did not improve from 0.34918\n",
      "36805/36805 [==============================] - 29s 796us/sample - loss: 0.4324 - acc: 0.8606 - val_loss: 0.3618 - val_acc: 0.8912\n",
      "Epoch 301/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4246 - acc: 0.8622\n",
      "Epoch 00301: val_loss did not improve from 0.34918\n",
      "36805/36805 [==============================] - 29s 794us/sample - loss: 0.4246 - acc: 0.8622 - val_loss: 0.3620 - val_acc: 0.8901\n",
      "Epoch 302/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4304 - acc: 0.8614\n",
      "Epoch 00302: val_loss did not improve from 0.34918\n",
      "36805/36805 [==============================] - 29s 787us/sample - loss: 0.4305 - acc: 0.8615 - val_loss: 0.3632 - val_acc: 0.8880\n",
      "Epoch 303/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4330 - acc: 0.8598\n",
      "Epoch 00303: val_loss did not improve from 0.34918\n",
      "36805/36805 [==============================] - 29s 784us/sample - loss: 0.4333 - acc: 0.8598 - val_loss: 0.3672 - val_acc: 0.8896\n",
      "Epoch 304/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4324 - acc: 0.8596\n",
      "Epoch 00304: val_loss did not improve from 0.34918\n",
      "36805/36805 [==============================] - 29s 789us/sample - loss: 0.4328 - acc: 0.8595 - val_loss: 0.3601 - val_acc: 0.8952\n",
      "Epoch 305/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4284 - acc: 0.8597\n",
      "Epoch 00305: val_loss did not improve from 0.34918\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 0.4290 - acc: 0.8596 - val_loss: 0.3613 - val_acc: 0.8908\n",
      "Epoch 306/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4245 - acc: 0.8632\n",
      "Epoch 00306: val_loss did not improve from 0.34918\n",
      "36805/36805 [==============================] - 29s 789us/sample - loss: 0.4246 - acc: 0.8633 - val_loss: 0.3575 - val_acc: 0.8954\n",
      "Epoch 307/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4345 - acc: 0.8605\n",
      "Epoch 00307: val_loss did not improve from 0.34918\n",
      "36805/36805 [==============================] - 29s 786us/sample - loss: 0.4345 - acc: 0.8606 - val_loss: 0.3638 - val_acc: 0.8884\n",
      "Epoch 308/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4277 - acc: 0.8610\n",
      "Epoch 00308: val_loss did not improve from 0.34918\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 0.4276 - acc: 0.8610 - val_loss: 0.3588 - val_acc: 0.8938\n",
      "Epoch 309/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4252 - acc: 0.8627\n",
      "Epoch 00309: val_loss did not improve from 0.34918\n",
      "36805/36805 [==============================] - 29s 796us/sample - loss: 0.4252 - acc: 0.8628 - val_loss: 0.3559 - val_acc: 0.8933\n",
      "Epoch 310/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4265 - acc: 0.8625- ETA: 2s \n",
      "Epoch 00310: val_loss did not improve from 0.34918\n",
      "36805/36805 [==============================] - 29s 799us/sample - loss: 0.4267 - acc: 0.8625 - val_loss: 0.3623 - val_acc: 0.8915\n",
      "Epoch 311/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4243 - acc: 0.8620\n",
      "Epoch 00311: val_loss did not improve from 0.34918\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 0.4243 - acc: 0.8620 - val_loss: 0.3640 - val_acc: 0.8917\n",
      "Epoch 312/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4243 - acc: 0.8629\n",
      "Epoch 00312: val_loss did not improve from 0.34918\n",
      "36805/36805 [==============================] - 29s 789us/sample - loss: 0.4243 - acc: 0.8629 - val_loss: 0.3573 - val_acc: 0.8928\n",
      "Epoch 313/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4228 - acc: 0.8641\n",
      "Epoch 00313: val_loss did not improve from 0.34918\n",
      "36805/36805 [==============================] - 29s 793us/sample - loss: 0.4227 - acc: 0.8641 - val_loss: 0.3621 - val_acc: 0.8905\n",
      "Epoch 314/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4270 - acc: 0.8632\n",
      "Epoch 00314: val_loss did not improve from 0.34918\n",
      "36805/36805 [==============================] - 29s 792us/sample - loss: 0.4268 - acc: 0.8632 - val_loss: 0.3602 - val_acc: 0.8940\n",
      "Epoch 315/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4222 - acc: 0.8631\n",
      "Epoch 00315: val_loss did not improve from 0.34918\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 0.4220 - acc: 0.8631 - val_loss: 0.3600 - val_acc: 0.8961\n",
      "Epoch 316/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4267 - acc: 0.8648\n",
      "Epoch 00316: val_loss did not improve from 0.34918\n",
      "36805/36805 [==============================] - 29s 793us/sample - loss: 0.4267 - acc: 0.8648 - val_loss: 0.3665 - val_acc: 0.8917\n",
      "Epoch 317/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4274 - acc: 0.8621\n",
      "Epoch 00317: val_loss did not improve from 0.34918\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 0.4274 - acc: 0.8622 - val_loss: 0.3610 - val_acc: 0.8924\n",
      "Epoch 318/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4213 - acc: 0.8635\n",
      "Epoch 00318: val_loss improved from 0.34918 to 0.34916, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_4_conv_checkpoint/318-0.3492.hdf5\n",
      "36805/36805 [==============================] - 29s 797us/sample - loss: 0.4213 - acc: 0.8636 - val_loss: 0.3492 - val_acc: 0.8935\n",
      "Epoch 319/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4221 - acc: 0.8637\n",
      "Epoch 00319: val_loss did not improve from 0.34916\n",
      "36805/36805 [==============================] - 29s 796us/sample - loss: 0.4221 - acc: 0.8637 - val_loss: 0.3614 - val_acc: 0.8905\n",
      "Epoch 320/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4186 - acc: 0.8653\n",
      "Epoch 00320: val_loss did not improve from 0.34916\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 0.4185 - acc: 0.8652 - val_loss: 0.3591 - val_acc: 0.8935\n",
      "Epoch 321/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4186 - acc: 0.8657\n",
      "Epoch 00321: val_loss did not improve from 0.34916\n",
      "36805/36805 [==============================] - 29s 794us/sample - loss: 0.4186 - acc: 0.8657 - val_loss: 0.3617 - val_acc: 0.8928\n",
      "Epoch 322/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4204 - acc: 0.8642\n",
      "Epoch 00322: val_loss did not improve from 0.34916\n",
      "36805/36805 [==============================] - 29s 796us/sample - loss: 0.4204 - acc: 0.8642 - val_loss: 0.3527 - val_acc: 0.8928\n",
      "Epoch 323/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4218 - acc: 0.8635\n",
      "Epoch 00323: val_loss did not improve from 0.34916\n",
      "36805/36805 [==============================] - 29s 786us/sample - loss: 0.4215 - acc: 0.8636 - val_loss: 0.3527 - val_acc: 0.8977\n",
      "Epoch 324/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4206 - acc: 0.8635\n",
      "Epoch 00324: val_loss did not improve from 0.34916\n",
      "36805/36805 [==============================] - 29s 789us/sample - loss: 0.4207 - acc: 0.8634 - val_loss: 0.3564 - val_acc: 0.8931\n",
      "Epoch 325/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4181 - acc: 0.8651\n",
      "Epoch 00325: val_loss did not improve from 0.34916\n",
      "36805/36805 [==============================] - 29s 797us/sample - loss: 0.4180 - acc: 0.8651 - val_loss: 0.3625 - val_acc: 0.8903\n",
      "Epoch 326/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4207 - acc: 0.8637\n",
      "Epoch 00326: val_loss did not improve from 0.34916\n",
      "36805/36805 [==============================] - 29s 796us/sample - loss: 0.4208 - acc: 0.8636 - val_loss: 0.3612 - val_acc: 0.8942\n",
      "Epoch 327/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4202 - acc: 0.8638\n",
      "Epoch 00327: val_loss did not improve from 0.34916\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 0.4200 - acc: 0.8639 - val_loss: 0.3641 - val_acc: 0.8901\n",
      "Epoch 328/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4175 - acc: 0.8661\n",
      "Epoch 00328: val_loss did not improve from 0.34916\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 0.4175 - acc: 0.8661 - val_loss: 0.3609 - val_acc: 0.8924\n",
      "Epoch 329/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4265 - acc: 0.8615\n",
      "Epoch 00329: val_loss did not improve from 0.34916\n",
      "36805/36805 [==============================] - 29s 796us/sample - loss: 0.4269 - acc: 0.8613 - val_loss: 0.3590 - val_acc: 0.8959\n",
      "Epoch 330/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4191 - acc: 0.8628\n",
      "Epoch 00330: val_loss did not improve from 0.34916\n",
      "36805/36805 [==============================] - 29s 796us/sample - loss: 0.4191 - acc: 0.8628 - val_loss: 0.3549 - val_acc: 0.8963\n",
      "Epoch 331/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4198 - acc: 0.8643\n",
      "Epoch 00331: val_loss did not improve from 0.34916\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 0.4198 - acc: 0.8643 - val_loss: 0.3616 - val_acc: 0.8921\n",
      "Epoch 332/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4152 - acc: 0.8666\n",
      "Epoch 00332: val_loss did not improve from 0.34916\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 0.4149 - acc: 0.8666 - val_loss: 0.3595 - val_acc: 0.8938\n",
      "Epoch 333/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4145 - acc: 0.8654\n",
      "Epoch 00333: val_loss did not improve from 0.34916\n",
      "36805/36805 [==============================] - 29s 798us/sample - loss: 0.4146 - acc: 0.8653 - val_loss: 0.3582 - val_acc: 0.8926\n",
      "Epoch 334/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4171 - acc: 0.8660\n",
      "Epoch 00334: val_loss did not improve from 0.34916\n",
      "36805/36805 [==============================] - 29s 797us/sample - loss: 0.4172 - acc: 0.8659 - val_loss: 0.3651 - val_acc: 0.8919\n",
      "Epoch 335/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4132 - acc: 0.8662\n",
      "Epoch 00335: val_loss did not improve from 0.34916\n",
      "36805/36805 [==============================] - 29s 798us/sample - loss: 0.4132 - acc: 0.8662 - val_loss: 0.3587 - val_acc: 0.8940\n",
      "Epoch 336/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4130 - acc: 0.8671\n",
      "Epoch 00336: val_loss did not improve from 0.34916\n",
      "36805/36805 [==============================] - 29s 798us/sample - loss: 0.4130 - acc: 0.8671 - val_loss: 0.3588 - val_acc: 0.8935\n",
      "Epoch 337/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4148 - acc: 0.8649\n",
      "Epoch 00337: val_loss did not improve from 0.34916\n",
      "36805/36805 [==============================] - 29s 794us/sample - loss: 0.4148 - acc: 0.8649 - val_loss: 0.3596 - val_acc: 0.8954\n",
      "Epoch 338/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4115 - acc: 0.8665\n",
      "Epoch 00338: val_loss did not improve from 0.34916\n",
      "36805/36805 [==============================] - 29s 798us/sample - loss: 0.4115 - acc: 0.8665 - val_loss: 0.3561 - val_acc: 0.8949\n",
      "Epoch 339/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4048 - acc: 0.8709\n",
      "Epoch 00339: val_loss did not improve from 0.34916\n",
      "36805/36805 [==============================] - 29s 796us/sample - loss: 0.4052 - acc: 0.8708 - val_loss: 0.3707 - val_acc: 0.8915\n",
      "Epoch 340/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4150 - acc: 0.8643\n",
      "Epoch 00340: val_loss did not improve from 0.34916\n",
      "36805/36805 [==============================] - 29s 794us/sample - loss: 0.4150 - acc: 0.8643 - val_loss: 0.3710 - val_acc: 0.8882\n",
      "Epoch 341/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4116 - acc: 0.8667\n",
      "Epoch 00341: val_loss did not improve from 0.34916\n",
      "36805/36805 [==============================] - 29s 794us/sample - loss: 0.4118 - acc: 0.8666 - val_loss: 0.3654 - val_acc: 0.8933\n",
      "Epoch 342/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4123 - acc: 0.8670\n",
      "Epoch 00342: val_loss did not improve from 0.34916\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 0.4123 - acc: 0.8670 - val_loss: 0.3632 - val_acc: 0.8908\n",
      "Epoch 343/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4172 - acc: 0.8643\n",
      "Epoch 00343: val_loss did not improve from 0.34916\n",
      "36805/36805 [==============================] - 29s 789us/sample - loss: 0.4170 - acc: 0.8644 - val_loss: 0.3570 - val_acc: 0.8940\n",
      "Epoch 344/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4086 - acc: 0.8671\n",
      "Epoch 00344: val_loss did not improve from 0.34916\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 0.4086 - acc: 0.8671 - val_loss: 0.3660 - val_acc: 0.8947\n",
      "Epoch 345/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4132 - acc: 0.8659\n",
      "Epoch 00345: val_loss did not improve from 0.34916\n",
      "36805/36805 [==============================] - 29s 787us/sample - loss: 0.4138 - acc: 0.8658 - val_loss: 0.3595 - val_acc: 0.8887\n",
      "Epoch 346/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4127 - acc: 0.8661\n",
      "Epoch 00346: val_loss did not improve from 0.34916\n",
      "36805/36805 [==============================] - 29s 789us/sample - loss: 0.4126 - acc: 0.8662 - val_loss: 0.3590 - val_acc: 0.8931\n",
      "Epoch 347/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4088 - acc: 0.8672\n",
      "Epoch 00347: val_loss did not improve from 0.34916\n",
      "36805/36805 [==============================] - 29s 788us/sample - loss: 0.4091 - acc: 0.8672 - val_loss: 0.3660 - val_acc: 0.8915\n",
      "Epoch 348/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4135 - acc: 0.8669\n",
      "Epoch 00348: val_loss did not improve from 0.34916\n",
      "36805/36805 [==============================] - 29s 787us/sample - loss: 0.4134 - acc: 0.8669 - val_loss: 0.3686 - val_acc: 0.8915\n",
      "Epoch 349/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4109 - acc: 0.8668\n",
      "Epoch 00349: val_loss did not improve from 0.34916\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 0.4108 - acc: 0.8668 - val_loss: 0.3577 - val_acc: 0.8940\n",
      "Epoch 350/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4069 - acc: 0.8683\n",
      "Epoch 00350: val_loss did not improve from 0.34916\n",
      "36805/36805 [==============================] - 29s 796us/sample - loss: 0.4070 - acc: 0.8683 - val_loss: 0.3654 - val_acc: 0.8931\n",
      "Epoch 351/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4111 - acc: 0.8653\n",
      "Epoch 00351: val_loss did not improve from 0.34916\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 0.4112 - acc: 0.8652 - val_loss: 0.3670 - val_acc: 0.8882\n",
      "Epoch 352/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4043 - acc: 0.8695\n",
      "Epoch 00352: val_loss did not improve from 0.34916\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 0.4044 - acc: 0.8695 - val_loss: 0.3566 - val_acc: 0.8910\n",
      "Epoch 353/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4086 - acc: 0.8673\n",
      "Epoch 00353: val_loss did not improve from 0.34916\n",
      "36805/36805 [==============================] - 29s 793us/sample - loss: 0.4086 - acc: 0.8674 - val_loss: 0.3590 - val_acc: 0.8928\n",
      "Epoch 354/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4072 - acc: 0.8680\n",
      "Epoch 00354: val_loss did not improve from 0.34916\n",
      "36805/36805 [==============================] - 29s 796us/sample - loss: 0.4073 - acc: 0.8680 - val_loss: 0.3634 - val_acc: 0.8919\n",
      "Epoch 355/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4032 - acc: 0.8698\n",
      "Epoch 00355: val_loss did not improve from 0.34916\n",
      "36805/36805 [==============================] - 29s 796us/sample - loss: 0.4029 - acc: 0.8699 - val_loss: 0.3536 - val_acc: 0.8968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 356/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4040 - acc: 0.8704\n",
      "Epoch 00356: val_loss did not improve from 0.34916\n",
      "36805/36805 [==============================] - 29s 796us/sample - loss: 0.4040 - acc: 0.8704 - val_loss: 0.3760 - val_acc: 0.8870\n",
      "Epoch 357/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4052 - acc: 0.8693\n",
      "Epoch 00357: val_loss did not improve from 0.34916\n",
      "36805/36805 [==============================] - 29s 794us/sample - loss: 0.4055 - acc: 0.8693 - val_loss: 0.3620 - val_acc: 0.8917\n",
      "Epoch 358/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4113 - acc: 0.8674\n",
      "Epoch 00358: val_loss did not improve from 0.34916\n",
      "36805/36805 [==============================] - 29s 799us/sample - loss: 0.4114 - acc: 0.8673 - val_loss: 0.3626 - val_acc: 0.8915\n",
      "Epoch 359/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4108 - acc: 0.8674\n",
      "Epoch 00359: val_loss did not improve from 0.34916\n",
      "36805/36805 [==============================] - 29s 799us/sample - loss: 0.4108 - acc: 0.8674 - val_loss: 0.3769 - val_acc: 0.8898\n",
      "Epoch 360/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4086 - acc: 0.8690\n",
      "Epoch 00360: val_loss did not improve from 0.34916\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 0.4085 - acc: 0.8691 - val_loss: 0.3670 - val_acc: 0.8940\n",
      "Epoch 361/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4037 - acc: 0.8689\n",
      "Epoch 00361: val_loss did not improve from 0.34916\n",
      "36805/36805 [==============================] - 29s 800us/sample - loss: 0.4036 - acc: 0.8688 - val_loss: 0.3601 - val_acc: 0.8928\n",
      "Epoch 362/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4059 - acc: 0.8690\n",
      "Epoch 00362: val_loss did not improve from 0.34916\n",
      "36805/36805 [==============================] - 29s 797us/sample - loss: 0.4058 - acc: 0.8690 - val_loss: 0.3644 - val_acc: 0.8928\n",
      "Epoch 363/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4010 - acc: 0.8683\n",
      "Epoch 00363: val_loss did not improve from 0.34916\n",
      "36805/36805 [==============================] - 29s 797us/sample - loss: 0.4010 - acc: 0.8683 - val_loss: 0.3644 - val_acc: 0.8901\n",
      "Epoch 364/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4135 - acc: 0.8648\n",
      "Epoch 00364: val_loss did not improve from 0.34916\n",
      "36805/36805 [==============================] - 29s 787us/sample - loss: 0.4133 - acc: 0.8649 - val_loss: 0.3657 - val_acc: 0.8921\n",
      "Epoch 365/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4039 - acc: 0.8695\n",
      "Epoch 00365: val_loss did not improve from 0.34916\n",
      "36805/36805 [==============================] - 29s 789us/sample - loss: 0.4039 - acc: 0.8695 - val_loss: 0.3669 - val_acc: 0.8935\n",
      "Epoch 366/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4048 - acc: 0.8701\n",
      "Epoch 00366: val_loss did not improve from 0.34916\n",
      "36805/36805 [==============================] - 29s 795us/sample - loss: 0.4048 - acc: 0.8702 - val_loss: 0.3623 - val_acc: 0.8905\n",
      "Epoch 367/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4033 - acc: 0.8692\n",
      "Epoch 00367: val_loss did not improve from 0.34916\n",
      "36805/36805 [==============================] - 29s 794us/sample - loss: 0.4033 - acc: 0.8692 - val_loss: 0.3603 - val_acc: 0.8926\n",
      "Epoch 368/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4024 - acc: 0.8693\n",
      "Epoch 00368: val_loss did not improve from 0.34916\n",
      "36805/36805 [==============================] - 29s 786us/sample - loss: 0.4024 - acc: 0.8694 - val_loss: 0.3660 - val_acc: 0.8889\n",
      "\n",
      "1D_CNN_custom_multi_2_GMP_DO_4_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 365us/sample - loss: 0.4098 - acc: 0.8737\n",
      "Loss: 0.40975068577972407 Accuracy: 0.8737279\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.4416 - acc: 0.1819\n",
      "Epoch 00001: val_loss improved from inf to 1.97699, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_5_conv_checkpoint/001-1.9770.hdf5\n",
      "36805/36805 [==============================] - 31s 850us/sample - loss: 2.4416 - acc: 0.1819 - val_loss: 1.9770 - val_acc: 0.4037\n",
      "Epoch 2/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.9268 - acc: 0.3596\n",
      "Epoch 00002: val_loss improved from 1.97699 to 1.54799, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_5_conv_checkpoint/002-1.5480.hdf5\n",
      "36805/36805 [==============================] - 30s 808us/sample - loss: 1.9267 - acc: 0.3596 - val_loss: 1.5480 - val_acc: 0.5365\n",
      "Epoch 3/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.6319 - acc: 0.4689\n",
      "Epoch 00003: val_loss improved from 1.54799 to 1.20269, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_5_conv_checkpoint/003-1.2027.hdf5\n",
      "36805/36805 [==============================] - 30s 809us/sample - loss: 1.6315 - acc: 0.4690 - val_loss: 1.2027 - val_acc: 0.6676\n",
      "Epoch 4/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.3883 - acc: 0.5458\n",
      "Epoch 00004: val_loss improved from 1.20269 to 0.99362, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_5_conv_checkpoint/004-0.9936.hdf5\n",
      "36805/36805 [==============================] - 30s 809us/sample - loss: 1.3884 - acc: 0.5457 - val_loss: 0.9936 - val_acc: 0.7165\n",
      "Epoch 5/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.2133 - acc: 0.6104\n",
      "Epoch 00005: val_loss improved from 0.99362 to 0.82593, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_5_conv_checkpoint/005-0.8259.hdf5\n",
      "36805/36805 [==============================] - 30s 809us/sample - loss: 1.2128 - acc: 0.6106 - val_loss: 0.8259 - val_acc: 0.7589\n",
      "Epoch 6/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 1.0841 - acc: 0.6544\n",
      "Epoch 00006: val_loss improved from 0.82593 to 0.75648, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_5_conv_checkpoint/006-0.7565.hdf5\n",
      "36805/36805 [==============================] - 30s 810us/sample - loss: 1.0840 - acc: 0.6544 - val_loss: 0.7565 - val_acc: 0.7857\n",
      "Epoch 7/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9874 - acc: 0.6869\n",
      "Epoch 00007: val_loss improved from 0.75648 to 0.65601, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_5_conv_checkpoint/007-0.6560.hdf5\n",
      "36805/36805 [==============================] - 30s 807us/sample - loss: 0.9873 - acc: 0.6870 - val_loss: 0.6560 - val_acc: 0.8120\n",
      "Epoch 8/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.9147 - acc: 0.7106\n",
      "Epoch 00008: val_loss improved from 0.65601 to 0.58852, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_5_conv_checkpoint/008-0.5885.hdf5\n",
      "36805/36805 [==============================] - 30s 808us/sample - loss: 0.9142 - acc: 0.7109 - val_loss: 0.5885 - val_acc: 0.8279\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8506 - acc: 0.7330\n",
      "Epoch 00009: val_loss improved from 0.58852 to 0.56034, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_5_conv_checkpoint/009-0.5603.hdf5\n",
      "36805/36805 [==============================] - 30s 807us/sample - loss: 0.8505 - acc: 0.7331 - val_loss: 0.5603 - val_acc: 0.8393\n",
      "Epoch 10/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.8002 - acc: 0.7477\n",
      "Epoch 00010: val_loss improved from 0.56034 to 0.51004, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_5_conv_checkpoint/010-0.5100.hdf5\n",
      "36805/36805 [==============================] - 30s 808us/sample - loss: 0.8006 - acc: 0.7475 - val_loss: 0.5100 - val_acc: 0.8526\n",
      "Epoch 11/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7590 - acc: 0.7618\n",
      "Epoch 00011: val_loss improved from 0.51004 to 0.48866, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_5_conv_checkpoint/011-0.4887.hdf5\n",
      "36805/36805 [==============================] - 30s 807us/sample - loss: 0.7591 - acc: 0.7618 - val_loss: 0.4887 - val_acc: 0.8593\n",
      "Epoch 12/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.7181 - acc: 0.7776\n",
      "Epoch 00012: val_loss improved from 0.48866 to 0.46230, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_5_conv_checkpoint/012-0.4623.hdf5\n",
      "36805/36805 [==============================] - 30s 808us/sample - loss: 0.7180 - acc: 0.7777 - val_loss: 0.4623 - val_acc: 0.8698\n",
      "Epoch 13/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6908 - acc: 0.7854\n",
      "Epoch 00013: val_loss improved from 0.46230 to 0.45051, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_5_conv_checkpoint/013-0.4505.hdf5\n",
      "36805/36805 [==============================] - 30s 810us/sample - loss: 0.6906 - acc: 0.7855 - val_loss: 0.4505 - val_acc: 0.8633\n",
      "Epoch 14/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6555 - acc: 0.7985\n",
      "Epoch 00014: val_loss improved from 0.45051 to 0.43841, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_5_conv_checkpoint/014-0.4384.hdf5\n",
      "36805/36805 [==============================] - 30s 809us/sample - loss: 0.6554 - acc: 0.7985 - val_loss: 0.4384 - val_acc: 0.8672\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6275 - acc: 0.8062\n",
      "Epoch 00015: val_loss improved from 0.43841 to 0.42081, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_5_conv_checkpoint/015-0.4208.hdf5\n",
      "36805/36805 [==============================] - 30s 808us/sample - loss: 0.6274 - acc: 0.8062 - val_loss: 0.4208 - val_acc: 0.8726\n",
      "Epoch 16/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.6086 - acc: 0.8108\n",
      "Epoch 00016: val_loss improved from 0.42081 to 0.41291, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_5_conv_checkpoint/016-0.4129.hdf5\n",
      "36805/36805 [==============================] - 30s 809us/sample - loss: 0.6084 - acc: 0.8108 - val_loss: 0.4129 - val_acc: 0.8744\n",
      "Epoch 17/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5925 - acc: 0.8166\n",
      "Epoch 00017: val_loss improved from 0.41291 to 0.36511, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_5_conv_checkpoint/017-0.3651.hdf5\n",
      "36805/36805 [==============================] - 30s 810us/sample - loss: 0.5927 - acc: 0.8166 - val_loss: 0.3651 - val_acc: 0.8866\n",
      "Epoch 18/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5693 - acc: 0.8250\n",
      "Epoch 00018: val_loss improved from 0.36511 to 0.36153, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_5_conv_checkpoint/018-0.3615.hdf5\n",
      "36805/36805 [==============================] - 30s 808us/sample - loss: 0.5694 - acc: 0.8250 - val_loss: 0.3615 - val_acc: 0.8935\n",
      "Epoch 19/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5623 - acc: 0.8273\n",
      "Epoch 00019: val_loss improved from 0.36153 to 0.35245, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_5_conv_checkpoint/019-0.3525.hdf5\n",
      "36805/36805 [==============================] - 30s 809us/sample - loss: 0.5620 - acc: 0.8274 - val_loss: 0.3525 - val_acc: 0.8975\n",
      "Epoch 20/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5408 - acc: 0.8316\n",
      "Epoch 00020: val_loss improved from 0.35245 to 0.34888, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_5_conv_checkpoint/020-0.3489.hdf5\n",
      "36805/36805 [==============================] - 30s 808us/sample - loss: 0.5407 - acc: 0.8317 - val_loss: 0.3489 - val_acc: 0.8912\n",
      "Epoch 21/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5278 - acc: 0.8372\n",
      "Epoch 00021: val_loss improved from 0.34888 to 0.33294, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_5_conv_checkpoint/021-0.3329.hdf5\n",
      "36805/36805 [==============================] - 30s 808us/sample - loss: 0.5279 - acc: 0.8373 - val_loss: 0.3329 - val_acc: 0.9012\n",
      "Epoch 22/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5168 - acc: 0.8410\n",
      "Epoch 00022: val_loss did not improve from 0.33294\n",
      "36805/36805 [==============================] - 30s 808us/sample - loss: 0.5168 - acc: 0.8410 - val_loss: 0.3607 - val_acc: 0.8980\n",
      "Epoch 23/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.5037 - acc: 0.8455\n",
      "Epoch 00023: val_loss improved from 0.33294 to 0.32497, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_5_conv_checkpoint/023-0.3250.hdf5\n",
      "36805/36805 [==============================] - 30s 810us/sample - loss: 0.5035 - acc: 0.8456 - val_loss: 0.3250 - val_acc: 0.9106\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4942 - acc: 0.8488\n",
      "Epoch 00024: val_loss improved from 0.32497 to 0.31383, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_5_conv_checkpoint/024-0.3138.hdf5\n",
      "36805/36805 [==============================] - 30s 808us/sample - loss: 0.4943 - acc: 0.8488 - val_loss: 0.3138 - val_acc: 0.9099\n",
      "Epoch 25/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4793 - acc: 0.8525\n",
      "Epoch 00025: val_loss did not improve from 0.31383\n",
      "36805/36805 [==============================] - 30s 806us/sample - loss: 0.4799 - acc: 0.8522 - val_loss: 0.3154 - val_acc: 0.9094\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4758 - acc: 0.8533\n",
      "Epoch 00026: val_loss improved from 0.31383 to 0.30527, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_5_conv_checkpoint/026-0.3053.hdf5\n",
      "36805/36805 [==============================] - 30s 808us/sample - loss: 0.4758 - acc: 0.8533 - val_loss: 0.3053 - val_acc: 0.9068\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4649 - acc: 0.8570\n",
      "Epoch 00027: val_loss improved from 0.30527 to 0.29146, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_5_conv_checkpoint/027-0.2915.hdf5\n",
      "36805/36805 [==============================] - 30s 807us/sample - loss: 0.4649 - acc: 0.8571 - val_loss: 0.2915 - val_acc: 0.9126\n",
      "Epoch 28/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4565 - acc: 0.8591\n",
      "Epoch 00028: val_loss improved from 0.29146 to 0.28162, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_5_conv_checkpoint/028-0.2816.hdf5\n",
      "36805/36805 [==============================] - 30s 806us/sample - loss: 0.4562 - acc: 0.8591 - val_loss: 0.2816 - val_acc: 0.9166\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4467 - acc: 0.8632\n",
      "Epoch 00029: val_loss did not improve from 0.28162\n",
      "36805/36805 [==============================] - 30s 807us/sample - loss: 0.4468 - acc: 0.8631 - val_loss: 0.2941 - val_acc: 0.9138\n",
      "Epoch 30/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4385 - acc: 0.8643\n",
      "Epoch 00030: val_loss did not improve from 0.28162\n",
      "36805/36805 [==============================] - 30s 807us/sample - loss: 0.4383 - acc: 0.8643 - val_loss: 0.2881 - val_acc: 0.9129\n",
      "Epoch 31/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4311 - acc: 0.8676\n",
      "Epoch 00031: val_loss did not improve from 0.28162\n",
      "36805/36805 [==============================] - 30s 808us/sample - loss: 0.4309 - acc: 0.8677 - val_loss: 0.2822 - val_acc: 0.9138\n",
      "Epoch 32/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4326 - acc: 0.8668\n",
      "Epoch 00032: val_loss improved from 0.28162 to 0.28023, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_5_conv_checkpoint/032-0.2802.hdf5\n",
      "36805/36805 [==============================] - 30s 808us/sample - loss: 0.4322 - acc: 0.8670 - val_loss: 0.2802 - val_acc: 0.9192\n",
      "Epoch 33/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4237 - acc: 0.8697\n",
      "Epoch 00033: val_loss improved from 0.28023 to 0.26918, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_5_conv_checkpoint/033-0.2692.hdf5\n",
      "36805/36805 [==============================] - 30s 807us/sample - loss: 0.4237 - acc: 0.8697 - val_loss: 0.2692 - val_acc: 0.9185\n",
      "Epoch 34/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4156 - acc: 0.8709\n",
      "Epoch 00034: val_loss improved from 0.26918 to 0.26517, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_5_conv_checkpoint/034-0.2652.hdf5\n",
      "36805/36805 [==============================] - 30s 807us/sample - loss: 0.4157 - acc: 0.8709 - val_loss: 0.2652 - val_acc: 0.9245\n",
      "Epoch 35/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4057 - acc: 0.8763\n",
      "Epoch 00035: val_loss did not improve from 0.26517\n",
      "36805/36805 [==============================] - 30s 806us/sample - loss: 0.4058 - acc: 0.8763 - val_loss: 0.2693 - val_acc: 0.9220\n",
      "Epoch 36/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.4025 - acc: 0.8777\n",
      "Epoch 00036: val_loss improved from 0.26517 to 0.25653, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_5_conv_checkpoint/036-0.2565.hdf5\n",
      "36805/36805 [==============================] - 30s 808us/sample - loss: 0.4025 - acc: 0.8778 - val_loss: 0.2565 - val_acc: 0.9236\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3955 - acc: 0.8772\n",
      "Epoch 00037: val_loss did not improve from 0.25653\n",
      "36805/36805 [==============================] - 30s 808us/sample - loss: 0.3956 - acc: 0.8772 - val_loss: 0.2575 - val_acc: 0.9250\n",
      "Epoch 38/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3994 - acc: 0.8758- ETA: 1s - loss: 0.3985 -\n",
      "Epoch 00038: val_loss did not improve from 0.25653\n",
      "36805/36805 [==============================] - 30s 806us/sample - loss: 0.3994 - acc: 0.8757 - val_loss: 0.2604 - val_acc: 0.9224\n",
      "Epoch 39/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3885 - acc: 0.8813\n",
      "Epoch 00039: val_loss did not improve from 0.25653\n",
      "36805/36805 [==============================] - 30s 805us/sample - loss: 0.3885 - acc: 0.8813 - val_loss: 0.2767 - val_acc: 0.9126\n",
      "Epoch 40/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3781 - acc: 0.8822\n",
      "Epoch 00040: val_loss did not improve from 0.25653\n",
      "36805/36805 [==============================] - 30s 807us/sample - loss: 0.3782 - acc: 0.8821 - val_loss: 0.2763 - val_acc: 0.9182\n",
      "Epoch 41/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3784 - acc: 0.8839\n",
      "Epoch 00041: val_loss did not improve from 0.25653\n",
      "36805/36805 [==============================] - 30s 807us/sample - loss: 0.3782 - acc: 0.8839 - val_loss: 0.2580 - val_acc: 0.9213\n",
      "Epoch 42/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3786 - acc: 0.8855\n",
      "Epoch 00042: val_loss improved from 0.25653 to 0.24160, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_5_conv_checkpoint/042-0.2416.hdf5\n",
      "36805/36805 [==============================] - 30s 807us/sample - loss: 0.3785 - acc: 0.8855 - val_loss: 0.2416 - val_acc: 0.9294\n",
      "Epoch 43/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3675 - acc: 0.8860\n",
      "Epoch 00043: val_loss did not improve from 0.24160\n",
      "36805/36805 [==============================] - 30s 807us/sample - loss: 0.3675 - acc: 0.8860 - val_loss: 0.2540 - val_acc: 0.9234\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3650 - acc: 0.8870\n",
      "Epoch 00044: val_loss improved from 0.24160 to 0.23127, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_5_conv_checkpoint/044-0.2313.hdf5\n",
      "36805/36805 [==============================] - 30s 809us/sample - loss: 0.3651 - acc: 0.8869 - val_loss: 0.2313 - val_acc: 0.9306\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3645 - acc: 0.8893\n",
      "Epoch 00045: val_loss did not improve from 0.23127\n",
      "36805/36805 [==============================] - 30s 810us/sample - loss: 0.3645 - acc: 0.8893 - val_loss: 0.2391 - val_acc: 0.9299\n",
      "Epoch 46/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3547 - acc: 0.8898\n",
      "Epoch 00046: val_loss did not improve from 0.23127\n",
      "36805/36805 [==============================] - 30s 807us/sample - loss: 0.3549 - acc: 0.8897 - val_loss: 0.2482 - val_acc: 0.9290\n",
      "Epoch 47/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3566 - acc: 0.8892\n",
      "Epoch 00047: val_loss did not improve from 0.23127\n",
      "36805/36805 [==============================] - 30s 807us/sample - loss: 0.3566 - acc: 0.8891 - val_loss: 0.2338 - val_acc: 0.9308\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3541 - acc: 0.8889\n",
      "Epoch 00048: val_loss did not improve from 0.23127\n",
      "36805/36805 [==============================] - 30s 808us/sample - loss: 0.3545 - acc: 0.8887 - val_loss: 0.2346 - val_acc: 0.9297\n",
      "Epoch 49/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3431 - acc: 0.8944\n",
      "Epoch 00049: val_loss did not improve from 0.23127\n",
      "36805/36805 [==============================] - 30s 806us/sample - loss: 0.3428 - acc: 0.8944 - val_loss: 0.2457 - val_acc: 0.9252\n",
      "Epoch 50/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3398 - acc: 0.8956\n",
      "Epoch 00050: val_loss did not improve from 0.23127\n",
      "36805/36805 [==============================] - 30s 804us/sample - loss: 0.3398 - acc: 0.8956 - val_loss: 0.2355 - val_acc: 0.9297\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3441 - acc: 0.8936\n",
      "Epoch 00051: val_loss did not improve from 0.23127\n",
      "36805/36805 [==============================] - 30s 805us/sample - loss: 0.3442 - acc: 0.8935 - val_loss: 0.2411 - val_acc: 0.9301\n",
      "Epoch 52/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3358 - acc: 0.8959\n",
      "Epoch 00052: val_loss did not improve from 0.23127\n",
      "36805/36805 [==============================] - 30s 802us/sample - loss: 0.3357 - acc: 0.8960 - val_loss: 0.2383 - val_acc: 0.9306\n",
      "Epoch 53/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3363 - acc: 0.8970\n",
      "Epoch 00053: val_loss improved from 0.23127 to 0.22990, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_5_conv_checkpoint/053-0.2299.hdf5\n",
      "36805/36805 [==============================] - 30s 805us/sample - loss: 0.3365 - acc: 0.8969 - val_loss: 0.2299 - val_acc: 0.9292\n",
      "Epoch 54/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3267 - acc: 0.8985\n",
      "Epoch 00054: val_loss improved from 0.22990 to 0.22335, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_5_conv_checkpoint/054-0.2233.hdf5\n",
      "36805/36805 [==============================] - 30s 815us/sample - loss: 0.3268 - acc: 0.8985 - val_loss: 0.2233 - val_acc: 0.9348\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3235 - acc: 0.8991\n",
      "Epoch 00055: val_loss did not improve from 0.22335\n",
      "36805/36805 [==============================] - 30s 810us/sample - loss: 0.3235 - acc: 0.8991 - val_loss: 0.2360 - val_acc: 0.9329\n",
      "Epoch 56/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3284 - acc: 0.8972\n",
      "Epoch 00056: val_loss did not improve from 0.22335\n",
      "36805/36805 [==============================] - 30s 809us/sample - loss: 0.3284 - acc: 0.8972 - val_loss: 0.2250 - val_acc: 0.9322\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3201 - acc: 0.9001\n",
      "Epoch 00057: val_loss improved from 0.22335 to 0.22083, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_5_conv_checkpoint/057-0.2208.hdf5\n",
      "36805/36805 [==============================] - 30s 811us/sample - loss: 0.3201 - acc: 0.9000 - val_loss: 0.2208 - val_acc: 0.9341\n",
      "Epoch 58/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3209 - acc: 0.9002\n",
      "Epoch 00058: val_loss did not improve from 0.22083\n",
      "36805/36805 [==============================] - 30s 810us/sample - loss: 0.3207 - acc: 0.9003 - val_loss: 0.2332 - val_acc: 0.9311\n",
      "Epoch 59/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3157 - acc: 0.9025\n",
      "Epoch 00059: val_loss did not improve from 0.22083\n",
      "36805/36805 [==============================] - 30s 810us/sample - loss: 0.3159 - acc: 0.9025 - val_loss: 0.2230 - val_acc: 0.9366\n",
      "Epoch 60/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3116 - acc: 0.9037\n",
      "Epoch 00060: val_loss improved from 0.22083 to 0.21248, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_5_conv_checkpoint/060-0.2125.hdf5\n",
      "36805/36805 [==============================] - 30s 812us/sample - loss: 0.3119 - acc: 0.9036 - val_loss: 0.2125 - val_acc: 0.9345\n",
      "Epoch 61/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3049 - acc: 0.9052\n",
      "Epoch 00061: val_loss did not improve from 0.21248\n",
      "36805/36805 [==============================] - 30s 810us/sample - loss: 0.3050 - acc: 0.9052 - val_loss: 0.2236 - val_acc: 0.9357\n",
      "Epoch 62/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3094 - acc: 0.9036\n",
      "Epoch 00062: val_loss did not improve from 0.21248\n",
      "36805/36805 [==============================] - 30s 811us/sample - loss: 0.3093 - acc: 0.9036 - val_loss: 0.2214 - val_acc: 0.9338\n",
      "Epoch 63/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3099 - acc: 0.9038\n",
      "Epoch 00063: val_loss did not improve from 0.21248\n",
      "36805/36805 [==============================] - 30s 809us/sample - loss: 0.3097 - acc: 0.9038 - val_loss: 0.2135 - val_acc: 0.9399\n",
      "Epoch 64/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3041 - acc: 0.9059\n",
      "Epoch 00064: val_loss did not improve from 0.21248\n",
      "36805/36805 [==============================] - 30s 811us/sample - loss: 0.3040 - acc: 0.9059 - val_loss: 0.2148 - val_acc: 0.9378\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3008 - acc: 0.9058\n",
      "Epoch 00065: val_loss did not improve from 0.21248\n",
      "36805/36805 [==============================] - 30s 809us/sample - loss: 0.3008 - acc: 0.9058 - val_loss: 0.2127 - val_acc: 0.9364\n",
      "Epoch 66/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3016 - acc: 0.9072\n",
      "Epoch 00066: val_loss improved from 0.21248 to 0.20753, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_5_conv_checkpoint/066-0.2075.hdf5\n",
      "36805/36805 [==============================] - 30s 814us/sample - loss: 0.3014 - acc: 0.9073 - val_loss: 0.2075 - val_acc: 0.9397\n",
      "Epoch 67/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2937 - acc: 0.9088\n",
      "Epoch 00067: val_loss did not improve from 0.20753\n",
      "36805/36805 [==============================] - 30s 813us/sample - loss: 0.2939 - acc: 0.9088 - val_loss: 0.2087 - val_acc: 0.9392\n",
      "Epoch 68/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2950 - acc: 0.9071\n",
      "Epoch 00068: val_loss did not improve from 0.20753\n",
      "36805/36805 [==============================] - 30s 813us/sample - loss: 0.2952 - acc: 0.9071 - val_loss: 0.2155 - val_acc: 0.9378\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2987 - acc: 0.9073\n",
      "Epoch 00069: val_loss did not improve from 0.20753\n",
      "36805/36805 [==============================] - 30s 810us/sample - loss: 0.2987 - acc: 0.9073 - val_loss: 0.2098 - val_acc: 0.9411\n",
      "Epoch 70/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2868 - acc: 0.9095\n",
      "Epoch 00070: val_loss did not improve from 0.20753\n",
      "36805/36805 [==============================] - 30s 811us/sample - loss: 0.2872 - acc: 0.9094 - val_loss: 0.2199 - val_acc: 0.9357\n",
      "Epoch 71/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2889 - acc: 0.9116\n",
      "Epoch 00071: val_loss did not improve from 0.20753\n",
      "36805/36805 [==============================] - 30s 811us/sample - loss: 0.2887 - acc: 0.9117 - val_loss: 0.2102 - val_acc: 0.9399\n",
      "Epoch 72/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2853 - acc: 0.9110\n",
      "Epoch 00072: val_loss did not improve from 0.20753\n",
      "36805/36805 [==============================] - 30s 810us/sample - loss: 0.2853 - acc: 0.9110 - val_loss: 0.2173 - val_acc: 0.9378\n",
      "Epoch 73/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2766 - acc: 0.9132\n",
      "Epoch 00073: val_loss did not improve from 0.20753\n",
      "36805/36805 [==============================] - 30s 810us/sample - loss: 0.2768 - acc: 0.9132 - val_loss: 0.2116 - val_acc: 0.9411\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2858 - acc: 0.9100\n",
      "Epoch 00074: val_loss did not improve from 0.20753\n",
      "36805/36805 [==============================] - 30s 810us/sample - loss: 0.2859 - acc: 0.9100 - val_loss: 0.2087 - val_acc: 0.9397\n",
      "Epoch 75/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2798 - acc: 0.9130\n",
      "Epoch 00075: val_loss improved from 0.20753 to 0.20543, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_5_conv_checkpoint/075-0.2054.hdf5\n",
      "36805/36805 [==============================] - 30s 810us/sample - loss: 0.2795 - acc: 0.9131 - val_loss: 0.2054 - val_acc: 0.9399\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2755 - acc: 0.9133\n",
      "Epoch 00076: val_loss improved from 0.20543 to 0.20197, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_5_conv_checkpoint/076-0.2020.hdf5\n",
      "36805/36805 [==============================] - 30s 811us/sample - loss: 0.2757 - acc: 0.9132 - val_loss: 0.2020 - val_acc: 0.9394\n",
      "Epoch 77/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2776 - acc: 0.9117\n",
      "Epoch 00077: val_loss improved from 0.20197 to 0.19813, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_5_conv_checkpoint/077-0.1981.hdf5\n",
      "36805/36805 [==============================] - 30s 812us/sample - loss: 0.2776 - acc: 0.9117 - val_loss: 0.1981 - val_acc: 0.9401\n",
      "Epoch 78/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2723 - acc: 0.9144\n",
      "Epoch 00078: val_loss did not improve from 0.19813\n",
      "36805/36805 [==============================] - 30s 807us/sample - loss: 0.2723 - acc: 0.9144 - val_loss: 0.1985 - val_acc: 0.9434\n",
      "Epoch 79/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2737 - acc: 0.9134\n",
      "Epoch 00079: val_loss did not improve from 0.19813\n",
      "36805/36805 [==============================] - 30s 811us/sample - loss: 0.2738 - acc: 0.9133 - val_loss: 0.2049 - val_acc: 0.9427\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2653 - acc: 0.9169\n",
      "Epoch 00080: val_loss did not improve from 0.19813\n",
      "36805/36805 [==============================] - 30s 809us/sample - loss: 0.2653 - acc: 0.9169 - val_loss: 0.2066 - val_acc: 0.9385\n",
      "Epoch 81/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2663 - acc: 0.9152\n",
      "Epoch 00081: val_loss did not improve from 0.19813\n",
      "36805/36805 [==============================] - 30s 810us/sample - loss: 0.2664 - acc: 0.9152 - val_loss: 0.2024 - val_acc: 0.9411\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2607 - acc: 0.9173\n",
      "Epoch 00082: val_loss did not improve from 0.19813\n",
      "36805/36805 [==============================] - 30s 810us/sample - loss: 0.2607 - acc: 0.9173 - val_loss: 0.2037 - val_acc: 0.9420\n",
      "Epoch 83/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2632 - acc: 0.9176\n",
      "Epoch 00083: val_loss did not improve from 0.19813\n",
      "36805/36805 [==============================] - 30s 811us/sample - loss: 0.2631 - acc: 0.9176 - val_loss: 0.2051 - val_acc: 0.9390\n",
      "Epoch 84/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2633 - acc: 0.9162\n",
      "Epoch 00084: val_loss did not improve from 0.19813\n",
      "36805/36805 [==============================] - 30s 808us/sample - loss: 0.2631 - acc: 0.9162 - val_loss: 0.2063 - val_acc: 0.9394\n",
      "Epoch 85/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2595 - acc: 0.9182\n",
      "Epoch 00085: val_loss improved from 0.19813 to 0.19729, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_5_conv_checkpoint/085-0.1973.hdf5\n",
      "36805/36805 [==============================] - 30s 812us/sample - loss: 0.2597 - acc: 0.9182 - val_loss: 0.1973 - val_acc: 0.9446\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2551 - acc: 0.9189\n",
      "Epoch 00086: val_loss improved from 0.19729 to 0.19566, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_5_conv_checkpoint/086-0.1957.hdf5\n",
      "36805/36805 [==============================] - 30s 811us/sample - loss: 0.2551 - acc: 0.9189 - val_loss: 0.1957 - val_acc: 0.9443\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2541 - acc: 0.9205\n",
      "Epoch 00087: val_loss did not improve from 0.19566\n",
      "36805/36805 [==============================] - 30s 813us/sample - loss: 0.2543 - acc: 0.9204 - val_loss: 0.2068 - val_acc: 0.9394\n",
      "Epoch 88/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2561 - acc: 0.9190\n",
      "Epoch 00088: val_loss did not improve from 0.19566\n",
      "36805/36805 [==============================] - 30s 812us/sample - loss: 0.2558 - acc: 0.9191 - val_loss: 0.2022 - val_acc: 0.9399\n",
      "Epoch 89/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2512 - acc: 0.9200\n",
      "Epoch 00089: val_loss did not improve from 0.19566\n",
      "36805/36805 [==============================] - 30s 810us/sample - loss: 0.2512 - acc: 0.9200 - val_loss: 0.2103 - val_acc: 0.9413\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2519 - acc: 0.9201\n",
      "Epoch 00090: val_loss did not improve from 0.19566\n",
      "36805/36805 [==============================] - 30s 811us/sample - loss: 0.2519 - acc: 0.9201 - val_loss: 0.1986 - val_acc: 0.9420\n",
      "Epoch 91/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2545 - acc: 0.9208\n",
      "Epoch 00091: val_loss did not improve from 0.19566\n",
      "36805/36805 [==============================] - 30s 812us/sample - loss: 0.2546 - acc: 0.9207 - val_loss: 0.1991 - val_acc: 0.9429\n",
      "Epoch 92/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2499 - acc: 0.9221\n",
      "Epoch 00092: val_loss did not improve from 0.19566\n",
      "36805/36805 [==============================] - 30s 809us/sample - loss: 0.2499 - acc: 0.9220 - val_loss: 0.2011 - val_acc: 0.9422\n",
      "Epoch 93/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2477 - acc: 0.9229\n",
      "Epoch 00093: val_loss did not improve from 0.19566\n",
      "36805/36805 [==============================] - 30s 808us/sample - loss: 0.2477 - acc: 0.9230 - val_loss: 0.2084 - val_acc: 0.9387\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2433 - acc: 0.9241- ETA: 1s - loss: 0\n",
      "Epoch 00094: val_loss did not improve from 0.19566\n",
      "36805/36805 [==============================] - 30s 810us/sample - loss: 0.2432 - acc: 0.9241 - val_loss: 0.1967 - val_acc: 0.9436\n",
      "Epoch 95/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2408 - acc: 0.9231\n",
      "Epoch 00095: val_loss did not improve from 0.19566\n",
      "36805/36805 [==============================] - 30s 808us/sample - loss: 0.2409 - acc: 0.9231 - val_loss: 0.2038 - val_acc: 0.9415\n",
      "Epoch 96/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2357 - acc: 0.9266\n",
      "Epoch 00096: val_loss did not improve from 0.19566\n",
      "36805/36805 [==============================] - 30s 811us/sample - loss: 0.2356 - acc: 0.9267 - val_loss: 0.2121 - val_acc: 0.9399\n",
      "Epoch 97/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2359 - acc: 0.9254\n",
      "Epoch 00097: val_loss did not improve from 0.19566\n",
      "36805/36805 [==============================] - 30s 810us/sample - loss: 0.2359 - acc: 0.9254 - val_loss: 0.2000 - val_acc: 0.9427\n",
      "Epoch 98/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2441 - acc: 0.9211\n",
      "Epoch 00098: val_loss did not improve from 0.19566\n",
      "36805/36805 [==============================] - 30s 812us/sample - loss: 0.2438 - acc: 0.9212 - val_loss: 0.2094 - val_acc: 0.9404\n",
      "Epoch 99/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2369 - acc: 0.9257\n",
      "Epoch 00099: val_loss did not improve from 0.19566\n",
      "36805/36805 [==============================] - 30s 811us/sample - loss: 0.2370 - acc: 0.9257 - val_loss: 0.2087 - val_acc: 0.9422\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2342 - acc: 0.9258\n",
      "Epoch 00100: val_loss did not improve from 0.19566\n",
      "36805/36805 [==============================] - 30s 812us/sample - loss: 0.2342 - acc: 0.9258 - val_loss: 0.2005 - val_acc: 0.9446\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2328 - acc: 0.9260\n",
      "Epoch 00101: val_loss did not improve from 0.19566\n",
      "36805/36805 [==============================] - 30s 811us/sample - loss: 0.2328 - acc: 0.9260 - val_loss: 0.1978 - val_acc: 0.9436\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2283 - acc: 0.9274\n",
      "Epoch 00102: val_loss did not improve from 0.19566\n",
      "36805/36805 [==============================] - 30s 809us/sample - loss: 0.2283 - acc: 0.9274 - val_loss: 0.2080 - val_acc: 0.9408\n",
      "Epoch 103/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2287 - acc: 0.9273\n",
      "Epoch 00103: val_loss did not improve from 0.19566\n",
      "36805/36805 [==============================] - 30s 810us/sample - loss: 0.2290 - acc: 0.9272 - val_loss: 0.2030 - val_acc: 0.9429\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 104/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2312 - acc: 0.9268\n",
      "Epoch 00104: val_loss did not improve from 0.19566\n",
      "36805/36805 [==============================] - 30s 813us/sample - loss: 0.2311 - acc: 0.9269 - val_loss: 0.1987 - val_acc: 0.9434\n",
      "Epoch 105/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2274 - acc: 0.9275\n",
      "Epoch 00105: val_loss did not improve from 0.19566\n",
      "36805/36805 [==============================] - 30s 811us/sample - loss: 0.2274 - acc: 0.9275 - val_loss: 0.1975 - val_acc: 0.9434\n",
      "Epoch 106/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2277 - acc: 0.9272\n",
      "Epoch 00106: val_loss improved from 0.19566 to 0.19248, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_5_conv_checkpoint/106-0.1925.hdf5\n",
      "36805/36805 [==============================] - 30s 814us/sample - loss: 0.2274 - acc: 0.9273 - val_loss: 0.1925 - val_acc: 0.9439\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2191 - acc: 0.9304\n",
      "Epoch 00107: val_loss did not improve from 0.19248\n",
      "36805/36805 [==============================] - 30s 813us/sample - loss: 0.2192 - acc: 0.9304 - val_loss: 0.2018 - val_acc: 0.9387\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2286 - acc: 0.9275\n",
      "Epoch 00108: val_loss did not improve from 0.19248\n",
      "36805/36805 [==============================] - 30s 812us/sample - loss: 0.2287 - acc: 0.9275 - val_loss: 0.2021 - val_acc: 0.9427\n",
      "Epoch 109/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2216 - acc: 0.9288\n",
      "Epoch 00109: val_loss did not improve from 0.19248\n",
      "36805/36805 [==============================] - 30s 811us/sample - loss: 0.2215 - acc: 0.9288 - val_loss: 0.1943 - val_acc: 0.9448\n",
      "Epoch 110/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2197 - acc: 0.9288\n",
      "Epoch 00110: val_loss did not improve from 0.19248\n",
      "36805/36805 [==============================] - 30s 811us/sample - loss: 0.2197 - acc: 0.9288 - val_loss: 0.2003 - val_acc: 0.9434\n",
      "Epoch 111/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2173 - acc: 0.9312\n",
      "Epoch 00111: val_loss did not improve from 0.19248\n",
      "36805/36805 [==============================] - 30s 812us/sample - loss: 0.2173 - acc: 0.9312 - val_loss: 0.2036 - val_acc: 0.9434\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2245 - acc: 0.9268\n",
      "Epoch 00112: val_loss did not improve from 0.19248\n",
      "36805/36805 [==============================] - 30s 815us/sample - loss: 0.2245 - acc: 0.9268 - val_loss: 0.1967 - val_acc: 0.9439\n",
      "Epoch 113/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2216 - acc: 0.9289\n",
      "Epoch 00113: val_loss did not improve from 0.19248\n",
      "36805/36805 [==============================] - 30s 813us/sample - loss: 0.2216 - acc: 0.9289 - val_loss: 0.2036 - val_acc: 0.9427\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2162 - acc: 0.9306- ETA: 1s - loss: \n",
      "Epoch 00114: val_loss did not improve from 0.19248\n",
      "36805/36805 [==============================] - 30s 813us/sample - loss: 0.2162 - acc: 0.9306 - val_loss: 0.1981 - val_acc: 0.9443\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2139 - acc: 0.9315\n",
      "Epoch 00115: val_loss did not improve from 0.19248\n",
      "36805/36805 [==============================] - 30s 813us/sample - loss: 0.2140 - acc: 0.9315 - val_loss: 0.2010 - val_acc: 0.9436\n",
      "Epoch 116/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2106 - acc: 0.9322\n",
      "Epoch 00116: val_loss did not improve from 0.19248\n",
      "36805/36805 [==============================] - 30s 811us/sample - loss: 0.2106 - acc: 0.9322 - val_loss: 0.2023 - val_acc: 0.9434\n",
      "Epoch 117/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2074 - acc: 0.9338\n",
      "Epoch 00117: val_loss did not improve from 0.19248\n",
      "36805/36805 [==============================] - 30s 812us/sample - loss: 0.2075 - acc: 0.9337 - val_loss: 0.2080 - val_acc: 0.9436\n",
      "Epoch 118/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2172 - acc: 0.9292\n",
      "Epoch 00118: val_loss did not improve from 0.19248\n",
      "36805/36805 [==============================] - 30s 813us/sample - loss: 0.2169 - acc: 0.9293 - val_loss: 0.2024 - val_acc: 0.9415\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2120 - acc: 0.9322\n",
      "Epoch 00119: val_loss did not improve from 0.19248\n",
      "36805/36805 [==============================] - 30s 810us/sample - loss: 0.2120 - acc: 0.9322 - val_loss: 0.2053 - val_acc: 0.9434\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2070 - acc: 0.9349\n",
      "Epoch 00120: val_loss did not improve from 0.19248\n",
      "36805/36805 [==============================] - 30s 812us/sample - loss: 0.2072 - acc: 0.9349 - val_loss: 0.1942 - val_acc: 0.9434\n",
      "Epoch 121/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2065 - acc: 0.9330\n",
      "Epoch 00121: val_loss did not improve from 0.19248\n",
      "36805/36805 [==============================] - 30s 812us/sample - loss: 0.2067 - acc: 0.9329 - val_loss: 0.1944 - val_acc: 0.9471\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2043 - acc: 0.9348\n",
      "Epoch 00122: val_loss did not improve from 0.19248\n",
      "36805/36805 [==============================] - 30s 810us/sample - loss: 0.2044 - acc: 0.9348 - val_loss: 0.1973 - val_acc: 0.9469\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2082 - acc: 0.9354\n",
      "Epoch 00123: val_loss did not improve from 0.19248\n",
      "36805/36805 [==============================] - 30s 812us/sample - loss: 0.2082 - acc: 0.9354 - val_loss: 0.2015 - val_acc: 0.9418\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2015 - acc: 0.9341\n",
      "Epoch 00124: val_loss improved from 0.19248 to 0.19009, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_5_conv_checkpoint/124-0.1901.hdf5\n",
      "36805/36805 [==============================] - 30s 814us/sample - loss: 0.2015 - acc: 0.9341 - val_loss: 0.1901 - val_acc: 0.9464\n",
      "Epoch 125/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2019 - acc: 0.9342- ETA: 1s - loss: 0.\n",
      "Epoch 00125: val_loss did not improve from 0.19009\n",
      "36805/36805 [==============================] - 30s 813us/sample - loss: 0.2024 - acc: 0.9341 - val_loss: 0.1958 - val_acc: 0.9432\n",
      "Epoch 126/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.2041 - acc: 0.9344\n",
      "Epoch 00126: val_loss did not improve from 0.19009\n",
      "36805/36805 [==============================] - 30s 810us/sample - loss: 0.2043 - acc: 0.9342 - val_loss: 0.2019 - val_acc: 0.9420\n",
      "Epoch 127/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1991 - acc: 0.9348- ETA: 1s - loss: 0.1\n",
      "Epoch 00127: val_loss did not improve from 0.19009\n",
      "36805/36805 [==============================] - 30s 809us/sample - loss: 0.1996 - acc: 0.9347 - val_loss: 0.1914 - val_acc: 0.9469\n",
      "Epoch 128/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1996 - acc: 0.9341\n",
      "Epoch 00128: val_loss did not improve from 0.19009\n",
      "36805/36805 [==============================] - 30s 809us/sample - loss: 0.1994 - acc: 0.9341 - val_loss: 0.2004 - val_acc: 0.9436\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1969 - acc: 0.9367\n",
      "Epoch 00129: val_loss did not improve from 0.19009\n",
      "36805/36805 [==============================] - 30s 812us/sample - loss: 0.1969 - acc: 0.9367 - val_loss: 0.1930 - val_acc: 0.9460\n",
      "Epoch 130/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1989 - acc: 0.9354\n",
      "Epoch 00130: val_loss did not improve from 0.19009\n",
      "36805/36805 [==============================] - 30s 814us/sample - loss: 0.1988 - acc: 0.9355 - val_loss: 0.1974 - val_acc: 0.9446\n",
      "Epoch 131/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1991 - acc: 0.9348\n",
      "Epoch 00131: val_loss did not improve from 0.19009\n",
      "36805/36805 [==============================] - 30s 811us/sample - loss: 0.1991 - acc: 0.9348 - val_loss: 0.1992 - val_acc: 0.9413\n",
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1952 - acc: 0.9381\n",
      "Epoch 00132: val_loss did not improve from 0.19009\n",
      "36805/36805 [==============================] - 30s 811us/sample - loss: 0.1952 - acc: 0.9381 - val_loss: 0.1901 - val_acc: 0.9467\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 133/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1897 - acc: 0.9383\n",
      "Epoch 00133: val_loss did not improve from 0.19009\n",
      "36805/36805 [==============================] - 30s 812us/sample - loss: 0.1896 - acc: 0.9383 - val_loss: 0.1997 - val_acc: 0.9427\n",
      "Epoch 134/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1941 - acc: 0.9374\n",
      "Epoch 00134: val_loss did not improve from 0.19009\n",
      "36805/36805 [==============================] - 30s 812us/sample - loss: 0.1941 - acc: 0.9374 - val_loss: 0.1931 - val_acc: 0.9450\n",
      "Epoch 135/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1876 - acc: 0.9384\n",
      "Epoch 00135: val_loss did not improve from 0.19009\n",
      "36805/36805 [==============================] - 30s 811us/sample - loss: 0.1878 - acc: 0.9383 - val_loss: 0.1974 - val_acc: 0.9434\n",
      "Epoch 136/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1850 - acc: 0.9394\n",
      "Epoch 00136: val_loss did not improve from 0.19009\n",
      "36805/36805 [==============================] - 30s 811us/sample - loss: 0.1850 - acc: 0.9394 - val_loss: 0.2050 - val_acc: 0.9420\n",
      "Epoch 137/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1915 - acc: 0.9374\n",
      "Epoch 00137: val_loss did not improve from 0.19009\n",
      "36805/36805 [==============================] - 30s 811us/sample - loss: 0.1915 - acc: 0.9374 - val_loss: 0.1954 - val_acc: 0.9439\n",
      "Epoch 138/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1866 - acc: 0.9391\n",
      "Epoch 00138: val_loss did not improve from 0.19009\n",
      "36805/36805 [==============================] - 30s 810us/sample - loss: 0.1868 - acc: 0.9390 - val_loss: 0.1985 - val_acc: 0.9474\n",
      "Epoch 139/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1899 - acc: 0.9390\n",
      "Epoch 00139: val_loss did not improve from 0.19009\n",
      "36805/36805 [==============================] - 30s 810us/sample - loss: 0.1899 - acc: 0.9390 - val_loss: 0.2053 - val_acc: 0.9425\n",
      "Epoch 140/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1880 - acc: 0.9398\n",
      "Epoch 00140: val_loss did not improve from 0.19009\n",
      "36805/36805 [==============================] - 30s 814us/sample - loss: 0.1880 - acc: 0.9397 - val_loss: 0.1983 - val_acc: 0.9446\n",
      "Epoch 141/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1840 - acc: 0.9398\n",
      "Epoch 00141: val_loss did not improve from 0.19009\n",
      "36805/36805 [==============================] - 30s 809us/sample - loss: 0.1838 - acc: 0.9399 - val_loss: 0.1934 - val_acc: 0.9474\n",
      "Epoch 142/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1878 - acc: 0.9381\n",
      "Epoch 00142: val_loss did not improve from 0.19009\n",
      "36805/36805 [==============================] - 30s 809us/sample - loss: 0.1879 - acc: 0.9380 - val_loss: 0.2015 - val_acc: 0.9441\n",
      "Epoch 143/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1807 - acc: 0.9399\n",
      "Epoch 00143: val_loss did not improve from 0.19009\n",
      "36805/36805 [==============================] - 30s 810us/sample - loss: 0.1807 - acc: 0.9400 - val_loss: 0.2047 - val_acc: 0.9404\n",
      "Epoch 144/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1805 - acc: 0.9420\n",
      "Epoch 00144: val_loss did not improve from 0.19009\n",
      "36805/36805 [==============================] - 30s 812us/sample - loss: 0.1805 - acc: 0.9420 - val_loss: 0.2069 - val_acc: 0.9413\n",
      "Epoch 145/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1801 - acc: 0.9415\n",
      "Epoch 00145: val_loss did not improve from 0.19009\n",
      "36805/36805 [==============================] - 30s 811us/sample - loss: 0.1800 - acc: 0.9415 - val_loss: 0.1996 - val_acc: 0.9450\n",
      "Epoch 146/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1772 - acc: 0.9413\n",
      "Epoch 00146: val_loss did not improve from 0.19009\n",
      "36805/36805 [==============================] - 30s 810us/sample - loss: 0.1771 - acc: 0.9413 - val_loss: 0.1937 - val_acc: 0.9448\n",
      "Epoch 147/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1780 - acc: 0.9421\n",
      "Epoch 00147: val_loss did not improve from 0.19009\n",
      "36805/36805 [==============================] - 30s 813us/sample - loss: 0.1780 - acc: 0.9421 - val_loss: 0.1945 - val_acc: 0.9467\n",
      "Epoch 148/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1816 - acc: 0.9408\n",
      "Epoch 00148: val_loss did not improve from 0.19009\n",
      "36805/36805 [==============================] - 30s 812us/sample - loss: 0.1816 - acc: 0.9408 - val_loss: 0.1991 - val_acc: 0.9446\n",
      "Epoch 149/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1776 - acc: 0.9422\n",
      "Epoch 00149: val_loss did not improve from 0.19009\n",
      "36805/36805 [==============================] - 30s 811us/sample - loss: 0.1777 - acc: 0.9422 - val_loss: 0.2044 - val_acc: 0.9462\n",
      "Epoch 150/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1714 - acc: 0.9439\n",
      "Epoch 00150: val_loss did not improve from 0.19009\n",
      "36805/36805 [==============================] - 30s 811us/sample - loss: 0.1714 - acc: 0.9439 - val_loss: 0.1982 - val_acc: 0.9448\n",
      "Epoch 151/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1745 - acc: 0.9424\n",
      "Epoch 00151: val_loss did not improve from 0.19009\n",
      "36805/36805 [==============================] - 30s 813us/sample - loss: 0.1744 - acc: 0.9425 - val_loss: 0.2023 - val_acc: 0.9464\n",
      "Epoch 152/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1737 - acc: 0.9430\n",
      "Epoch 00152: val_loss did not improve from 0.19009\n",
      "36805/36805 [==============================] - 30s 812us/sample - loss: 0.1737 - acc: 0.9430 - val_loss: 0.1991 - val_acc: 0.9425\n",
      "Epoch 153/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1769 - acc: 0.9427\n",
      "Epoch 00153: val_loss did not improve from 0.19009\n",
      "36805/36805 [==============================] - 30s 813us/sample - loss: 0.1769 - acc: 0.9427 - val_loss: 0.2046 - val_acc: 0.9432\n",
      "Epoch 154/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1733 - acc: 0.9438\n",
      "Epoch 00154: val_loss did not improve from 0.19009\n",
      "36805/36805 [==============================] - 30s 810us/sample - loss: 0.1733 - acc: 0.9438 - val_loss: 0.2034 - val_acc: 0.9455\n",
      "Epoch 155/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1749 - acc: 0.9418\n",
      "Epoch 00155: val_loss did not improve from 0.19009\n",
      "36805/36805 [==============================] - 30s 812us/sample - loss: 0.1751 - acc: 0.9418 - val_loss: 0.2011 - val_acc: 0.9439\n",
      "Epoch 156/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1720 - acc: 0.9425\n",
      "Epoch 00156: val_loss did not improve from 0.19009\n",
      "36805/36805 [==============================] - 30s 811us/sample - loss: 0.1718 - acc: 0.9425 - val_loss: 0.2074 - val_acc: 0.9411\n",
      "Epoch 157/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1730 - acc: 0.9426\n",
      "Epoch 00157: val_loss did not improve from 0.19009\n",
      "36805/36805 [==============================] - 30s 811us/sample - loss: 0.1730 - acc: 0.9427 - val_loss: 0.2118 - val_acc: 0.9406\n",
      "Epoch 158/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1682 - acc: 0.9450\n",
      "Epoch 00158: val_loss did not improve from 0.19009\n",
      "36805/36805 [==============================] - 30s 811us/sample - loss: 0.1683 - acc: 0.9449 - val_loss: 0.2008 - val_acc: 0.9448\n",
      "Epoch 159/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1651 - acc: 0.9465\n",
      "Epoch 00159: val_loss did not improve from 0.19009\n",
      "36805/36805 [==============================] - 30s 812us/sample - loss: 0.1650 - acc: 0.9466 - val_loss: 0.1964 - val_acc: 0.9443\n",
      "Epoch 160/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1666 - acc: 0.9440\n",
      "Epoch 00160: val_loss did not improve from 0.19009\n",
      "36805/36805 [==============================] - 30s 809us/sample - loss: 0.1667 - acc: 0.9439 - val_loss: 0.2031 - val_acc: 0.9425\n",
      "Epoch 161/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1711 - acc: 0.9427\n",
      "Epoch 00161: val_loss did not improve from 0.19009\n",
      "36805/36805 [==============================] - 30s 811us/sample - loss: 0.1710 - acc: 0.9427 - val_loss: 0.2286 - val_acc: 0.9364\n",
      "Epoch 162/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1695 - acc: 0.9446\n",
      "Epoch 00162: val_loss did not improve from 0.19009\n",
      "36805/36805 [==============================] - 30s 812us/sample - loss: 0.1695 - acc: 0.9446 - val_loss: 0.2012 - val_acc: 0.9467\n",
      "Epoch 163/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1675 - acc: 0.9444\n",
      "Epoch 00163: val_loss did not improve from 0.19009\n",
      "36805/36805 [==============================] - 30s 814us/sample - loss: 0.1674 - acc: 0.9444 - val_loss: 0.2087 - val_acc: 0.9401\n",
      "Epoch 164/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1649 - acc: 0.945 - ETA: 0s - loss: 0.1646 - acc: 0.9459\n",
      "Epoch 00164: val_loss did not improve from 0.19009\n",
      "36805/36805 [==============================] - 30s 816us/sample - loss: 0.1645 - acc: 0.9459 - val_loss: 0.2066 - val_acc: 0.9427\n",
      "Epoch 165/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1679 - acc: 0.9457\n",
      "Epoch 00165: val_loss did not improve from 0.19009\n",
      "36805/36805 [==============================] - 30s 826us/sample - loss: 0.1679 - acc: 0.9457 - val_loss: 0.2061 - val_acc: 0.9422\n",
      "Epoch 166/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1641 - acc: 0.9464\n",
      "Epoch 00166: val_loss did not improve from 0.19009\n",
      "36805/36805 [==============================] - 30s 815us/sample - loss: 0.1642 - acc: 0.9463 - val_loss: 0.2064 - val_acc: 0.9415\n",
      "Epoch 167/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1599 - acc: 0.9476\n",
      "Epoch 00167: val_loss did not improve from 0.19009\n",
      "36805/36805 [==============================] - 30s 817us/sample - loss: 0.1599 - acc: 0.9476 - val_loss: 0.2048 - val_acc: 0.9443\n",
      "Epoch 168/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1590 - acc: 0.9474\n",
      "Epoch 00168: val_loss did not improve from 0.19009\n",
      "36805/36805 [==============================] - 30s 817us/sample - loss: 0.1589 - acc: 0.9474 - val_loss: 0.2018 - val_acc: 0.9436\n",
      "Epoch 169/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1634 - acc: 0.9456\n",
      "Epoch 00169: val_loss did not improve from 0.19009\n",
      "36805/36805 [==============================] - 30s 814us/sample - loss: 0.1633 - acc: 0.9456 - val_loss: 0.2118 - val_acc: 0.9418\n",
      "Epoch 170/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1601 - acc: 0.9464\n",
      "Epoch 00170: val_loss did not improve from 0.19009\n",
      "36805/36805 [==============================] - 30s 817us/sample - loss: 0.1601 - acc: 0.9464 - val_loss: 0.2058 - val_acc: 0.9425\n",
      "Epoch 171/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1645 - acc: 0.9451\n",
      "Epoch 00171: val_loss did not improve from 0.19009\n",
      "36805/36805 [==============================] - 30s 817us/sample - loss: 0.1644 - acc: 0.9451 - val_loss: 0.1943 - val_acc: 0.9471\n",
      "Epoch 172/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1578 - acc: 0.9470\n",
      "Epoch 00172: val_loss did not improve from 0.19009\n",
      "36805/36805 [==============================] - 30s 815us/sample - loss: 0.1578 - acc: 0.9470 - val_loss: 0.2157 - val_acc: 0.9420\n",
      "Epoch 173/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1580 - acc: 0.9478\n",
      "Epoch 00173: val_loss did not improve from 0.19009\n",
      "36805/36805 [==============================] - 30s 816us/sample - loss: 0.1580 - acc: 0.9478 - val_loss: 0.2080 - val_acc: 0.9411\n",
      "Epoch 174/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1601 - acc: 0.9470\n",
      "Epoch 00174: val_loss did not improve from 0.19009\n",
      "36805/36805 [==============================] - 30s 816us/sample - loss: 0.1601 - acc: 0.9470 - val_loss: 0.2010 - val_acc: 0.9425\n",
      "\n",
      "1D_CNN_custom_multi_2_GMP_DO_5_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 391us/sample - loss: 0.2356 - acc: 0.9290\n",
      "Loss: 0.23562480327620067 Accuracy: 0.92897195\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.2563 - acc: 0.2597\n",
      "Epoch 00001: val_loss improved from inf to 1.49416, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_6_conv_checkpoint/001-1.4942.hdf5\n",
      "36805/36805 [==============================] - 33s 909us/sample - loss: 2.2563 - acc: 0.2597 - val_loss: 1.4942 - val_acc: 0.5616\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.4912 - acc: 0.5155\n",
      "Epoch 00002: val_loss improved from 1.49416 to 0.97026, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_6_conv_checkpoint/002-0.9703.hdf5\n",
      "36805/36805 [==============================] - 31s 840us/sample - loss: 1.4912 - acc: 0.5155 - val_loss: 0.9703 - val_acc: 0.7149\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1527 - acc: 0.6318\n",
      "Epoch 00003: val_loss improved from 0.97026 to 0.73355, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_6_conv_checkpoint/003-0.7336.hdf5\n",
      "36805/36805 [==============================] - 31s 836us/sample - loss: 1.1527 - acc: 0.6319 - val_loss: 0.7336 - val_acc: 0.7871\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9640 - acc: 0.6945\n",
      "Epoch 00004: val_loss improved from 0.73355 to 0.61612, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_6_conv_checkpoint/004-0.6161.hdf5\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.9639 - acc: 0.6946 - val_loss: 0.6161 - val_acc: 0.8304\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8343 - acc: 0.7344\n",
      "Epoch 00005: val_loss improved from 0.61612 to 0.53868, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_6_conv_checkpoint/005-0.5387.hdf5\n",
      "36805/36805 [==============================] - 31s 841us/sample - loss: 0.8343 - acc: 0.7344 - val_loss: 0.5387 - val_acc: 0.8500\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.7304 - acc: 0.7706\n",
      "Epoch 00006: val_loss improved from 0.53868 to 0.45279, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_6_conv_checkpoint/006-0.4528.hdf5\n",
      "36805/36805 [==============================] - 31s 835us/sample - loss: 0.7304 - acc: 0.7706 - val_loss: 0.4528 - val_acc: 0.8668\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6554 - acc: 0.7935\n",
      "Epoch 00007: val_loss improved from 0.45279 to 0.43393, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_6_conv_checkpoint/007-0.4339.hdf5\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.6553 - acc: 0.7935 - val_loss: 0.4339 - val_acc: 0.8658\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5973 - acc: 0.8134\n",
      "Epoch 00008: val_loss improved from 0.43393 to 0.40894, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_6_conv_checkpoint/008-0.4089.hdf5\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.5973 - acc: 0.8134 - val_loss: 0.4089 - val_acc: 0.8744\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5555 - acc: 0.8262\n",
      "Epoch 00009: val_loss improved from 0.40894 to 0.33438, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_6_conv_checkpoint/009-0.3344.hdf5\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.5555 - acc: 0.8262 - val_loss: 0.3344 - val_acc: 0.9029\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5252 - acc: 0.8370\n",
      "Epoch 00010: val_loss improved from 0.33438 to 0.30029, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_6_conv_checkpoint/010-0.3003.hdf5\n",
      "36805/36805 [==============================] - 31s 836us/sample - loss: 0.5252 - acc: 0.8370 - val_loss: 0.3003 - val_acc: 0.9087\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4853 - acc: 0.8470\n",
      "Epoch 00011: val_loss improved from 0.30029 to 0.29034, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_6_conv_checkpoint/011-0.2903.hdf5\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.4853 - acc: 0.8470 - val_loss: 0.2903 - val_acc: 0.9152\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4647 - acc: 0.8561\n",
      "Epoch 00012: val_loss improved from 0.29034 to 0.26893, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_6_conv_checkpoint/012-0.2689.hdf5\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.4647 - acc: 0.8561 - val_loss: 0.2689 - val_acc: 0.9168\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4401 - acc: 0.8629\n",
      "Epoch 00013: val_loss did not improve from 0.26893\n",
      "36805/36805 [==============================] - 31s 831us/sample - loss: 0.4401 - acc: 0.8629 - val_loss: 0.2850 - val_acc: 0.9157\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4186 - acc: 0.8688\n",
      "Epoch 00014: val_loss improved from 0.26893 to 0.24571, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_6_conv_checkpoint/014-0.2457.hdf5\n",
      "36805/36805 [==============================] - 31s 840us/sample - loss: 0.4186 - acc: 0.8688 - val_loss: 0.2457 - val_acc: 0.9308\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4080 - acc: 0.8730\n",
      "Epoch 00015: val_loss improved from 0.24571 to 0.24489, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_6_conv_checkpoint/015-0.2449.hdf5\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.4080 - acc: 0.8731 - val_loss: 0.2449 - val_acc: 0.9241\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3889 - acc: 0.8795\n",
      "Epoch 00016: val_loss did not improve from 0.24489\n",
      "36805/36805 [==============================] - 31s 835us/sample - loss: 0.3890 - acc: 0.8794 - val_loss: 0.2652 - val_acc: 0.9189\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3732 - acc: 0.8833\n",
      "Epoch 00017: val_loss improved from 0.24489 to 0.23885, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_6_conv_checkpoint/017-0.2389.hdf5\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.3733 - acc: 0.8832 - val_loss: 0.2389 - val_acc: 0.9243\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3643 - acc: 0.8876- ETA: 1s - \n",
      "Epoch 00018: val_loss improved from 0.23885 to 0.21229, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_6_conv_checkpoint/018-0.2123.hdf5\n",
      "36805/36805 [==============================] - 31s 834us/sample - loss: 0.3643 - acc: 0.8876 - val_loss: 0.2123 - val_acc: 0.9390\n",
      "Epoch 19/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3494 - acc: 0.8899\n",
      "Epoch 00019: val_loss did not improve from 0.21229\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.3492 - acc: 0.8900 - val_loss: 0.2182 - val_acc: 0.9334\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3316 - acc: 0.8968\n",
      "Epoch 00020: val_loss improved from 0.21229 to 0.19553, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_6_conv_checkpoint/020-0.1955.hdf5\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.3317 - acc: 0.8968 - val_loss: 0.1955 - val_acc: 0.9420\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3279 - acc: 0.8979\n",
      "Epoch 00021: val_loss improved from 0.19553 to 0.18978, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_6_conv_checkpoint/021-0.1898.hdf5\n",
      "36805/36805 [==============================] - 31s 835us/sample - loss: 0.3278 - acc: 0.8979 - val_loss: 0.1898 - val_acc: 0.9434\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3140 - acc: 0.9032\n",
      "Epoch 00022: val_loss improved from 0.18978 to 0.18794, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_6_conv_checkpoint/022-0.1879.hdf5\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.3140 - acc: 0.9032 - val_loss: 0.1879 - val_acc: 0.9432\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.3028 - acc: 0.9031\n",
      "Epoch 00023: val_loss did not improve from 0.18794\n",
      "36805/36805 [==============================] - 31s 835us/sample - loss: 0.3027 - acc: 0.9032 - val_loss: 0.2020 - val_acc: 0.9369\n",
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2956 - acc: 0.9077\n",
      "Epoch 00024: val_loss did not improve from 0.18794\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.2955 - acc: 0.9077 - val_loss: 0.1885 - val_acc: 0.9450\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2869 - acc: 0.9099\n",
      "Epoch 00025: val_loss improved from 0.18794 to 0.17513, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_6_conv_checkpoint/025-0.1751.hdf5\n",
      "36805/36805 [==============================] - 31s 834us/sample - loss: 0.2869 - acc: 0.9099 - val_loss: 0.1751 - val_acc: 0.9457\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2798 - acc: 0.9127\n",
      "Epoch 00026: val_loss improved from 0.17513 to 0.17139, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_6_conv_checkpoint/026-0.1714.hdf5\n",
      "36805/36805 [==============================] - 31s 836us/sample - loss: 0.2799 - acc: 0.9127 - val_loss: 0.1714 - val_acc: 0.9515\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2782 - acc: 0.9120\n",
      "Epoch 00027: val_loss improved from 0.17139 to 0.17014, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_6_conv_checkpoint/027-0.1701.hdf5\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.2783 - acc: 0.9120 - val_loss: 0.1701 - val_acc: 0.9490\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2730 - acc: 0.9148\n",
      "Epoch 00028: val_loss did not improve from 0.17014\n",
      "36805/36805 [==============================] - 31s 835us/sample - loss: 0.2730 - acc: 0.9148 - val_loss: 0.1827 - val_acc: 0.9415\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2606 - acc: 0.9181\n",
      "Epoch 00029: val_loss improved from 0.17014 to 0.16412, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_6_conv_checkpoint/029-0.1641.hdf5\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.2605 - acc: 0.9181 - val_loss: 0.1641 - val_acc: 0.9525\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2562 - acc: 0.9195\n",
      "Epoch 00030: val_loss improved from 0.16412 to 0.15772, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_6_conv_checkpoint/030-0.1577.hdf5\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.2562 - acc: 0.9195 - val_loss: 0.1577 - val_acc: 0.9564\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2442 - acc: 0.9224\n",
      "Epoch 00031: val_loss did not improve from 0.15772\n",
      "36805/36805 [==============================] - 31s 836us/sample - loss: 0.2441 - acc: 0.9224 - val_loss: 0.1594 - val_acc: 0.9522\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2460 - acc: 0.9217\n",
      "Epoch 00032: val_loss improved from 0.15772 to 0.15630, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_6_conv_checkpoint/032-0.1563.hdf5\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.2459 - acc: 0.9217 - val_loss: 0.1563 - val_acc: 0.9513\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2403 - acc: 0.9238\n",
      "Epoch 00033: val_loss did not improve from 0.15630\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.2403 - acc: 0.9238 - val_loss: 0.1642 - val_acc: 0.9450\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2370 - acc: 0.9250\n",
      "Epoch 00034: val_loss improved from 0.15630 to 0.15178, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_6_conv_checkpoint/034-0.1518.hdf5\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.2370 - acc: 0.9250 - val_loss: 0.1518 - val_acc: 0.9520\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2305 - acc: 0.9260\n",
      "Epoch 00035: val_loss did not improve from 0.15178\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.2306 - acc: 0.9259 - val_loss: 0.1538 - val_acc: 0.9511\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2266 - acc: 0.9291\n",
      "Epoch 00036: val_loss improved from 0.15178 to 0.14641, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_6_conv_checkpoint/036-0.1464.hdf5\n",
      "36805/36805 [==============================] - 31s 844us/sample - loss: 0.2265 - acc: 0.9291 - val_loss: 0.1464 - val_acc: 0.9548\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2231 - acc: 0.9299\n",
      "Epoch 00037: val_loss did not improve from 0.14641\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.2232 - acc: 0.9299 - val_loss: 0.1544 - val_acc: 0.9513\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2193 - acc: 0.9319\n",
      "Epoch 00038: val_loss did not improve from 0.14641\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.2193 - acc: 0.9319 - val_loss: 0.1577 - val_acc: 0.9469\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2129 - acc: 0.9328\n",
      "Epoch 00039: val_loss improved from 0.14641 to 0.14522, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_6_conv_checkpoint/039-0.1452.hdf5\n",
      "36805/36805 [==============================] - 31s 840us/sample - loss: 0.2129 - acc: 0.9328 - val_loss: 0.1452 - val_acc: 0.9536\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2158 - acc: 0.9313\n",
      "Epoch 00040: val_loss did not improve from 0.14522\n",
      "36805/36805 [==============================] - 31s 840us/sample - loss: 0.2158 - acc: 0.9313 - val_loss: 0.1493 - val_acc: 0.9515\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2053 - acc: 0.9358\n",
      "Epoch 00041: val_loss improved from 0.14522 to 0.14211, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_6_conv_checkpoint/041-0.1421.hdf5\n",
      "36805/36805 [==============================] - 31s 842us/sample - loss: 0.2053 - acc: 0.9359 - val_loss: 0.1421 - val_acc: 0.9536\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2020 - acc: 0.9357\n",
      "Epoch 00042: val_loss did not improve from 0.14211\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.2019 - acc: 0.9357 - val_loss: 0.1438 - val_acc: 0.9555\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2015 - acc: 0.9373\n",
      "Epoch 00043: val_loss improved from 0.14211 to 0.14011, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_6_conv_checkpoint/043-0.1401.hdf5\n",
      "36805/36805 [==============================] - 31s 847us/sample - loss: 0.2015 - acc: 0.9373 - val_loss: 0.1401 - val_acc: 0.9548\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1953 - acc: 0.9389\n",
      "Epoch 00044: val_loss improved from 0.14011 to 0.13416, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_6_conv_checkpoint/044-0.1342.hdf5\n",
      "36805/36805 [==============================] - 31s 840us/sample - loss: 0.1953 - acc: 0.9389 - val_loss: 0.1342 - val_acc: 0.9576\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1908 - acc: 0.9379\n",
      "Epoch 00045: val_loss did not improve from 0.13416\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.1908 - acc: 0.9379 - val_loss: 0.1546 - val_acc: 0.9522\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1909 - acc: 0.9391\n",
      "Epoch 00046: val_loss did not improve from 0.13416\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.1909 - acc: 0.9391 - val_loss: 0.1386 - val_acc: 0.9560\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1893 - acc: 0.9404\n",
      "Epoch 00047: val_loss did not improve from 0.13416\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.1893 - acc: 0.9404 - val_loss: 0.1427 - val_acc: 0.9546\n",
      "Epoch 48/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1839 - acc: 0.9423\n",
      "Epoch 00048: val_loss did not improve from 0.13416\n",
      "36805/36805 [==============================] - 31s 836us/sample - loss: 0.1840 - acc: 0.9422 - val_loss: 0.1350 - val_acc: 0.9576\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1836 - acc: 0.9417\n",
      "Epoch 00049: val_loss improved from 0.13416 to 0.12889, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_6_conv_checkpoint/049-0.1289.hdf5\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.1836 - acc: 0.9417 - val_loss: 0.1289 - val_acc: 0.9597\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1793 - acc: 0.9437\n",
      "Epoch 00050: val_loss did not improve from 0.12889\n",
      "36805/36805 [==============================] - 31s 840us/sample - loss: 0.1793 - acc: 0.9436 - val_loss: 0.1361 - val_acc: 0.9564\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1743 - acc: 0.9452\n",
      "Epoch 00051: val_loss did not improve from 0.12889\n",
      "36805/36805 [==============================] - 31s 836us/sample - loss: 0.1742 - acc: 0.9452 - val_loss: 0.1315 - val_acc: 0.9569\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1741 - acc: 0.9439\n",
      "Epoch 00052: val_loss did not improve from 0.12889\n",
      "36805/36805 [==============================] - 31s 836us/sample - loss: 0.1741 - acc: 0.9439 - val_loss: 0.1385 - val_acc: 0.9569\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1690 - acc: 0.9460\n",
      "Epoch 00053: val_loss did not improve from 0.12889\n",
      "36805/36805 [==============================] - 31s 835us/sample - loss: 0.1690 - acc: 0.9459 - val_loss: 0.1360 - val_acc: 0.9588\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1668 - acc: 0.9468\n",
      "Epoch 00054: val_loss did not improve from 0.12889\n",
      "36805/36805 [==============================] - 31s 836us/sample - loss: 0.1668 - acc: 0.9469 - val_loss: 0.1300 - val_acc: 0.9583\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1636 - acc: 0.9479\n",
      "Epoch 00055: val_loss did not improve from 0.12889\n",
      "36805/36805 [==============================] - 31s 835us/sample - loss: 0.1636 - acc: 0.9479 - val_loss: 0.1310 - val_acc: 0.9597\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1632 - acc: 0.9482\n",
      "Epoch 00056: val_loss did not improve from 0.12889\n",
      "36805/36805 [==============================] - 31s 835us/sample - loss: 0.1632 - acc: 0.9482 - val_loss: 0.1490 - val_acc: 0.9541\n",
      "Epoch 57/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1594 - acc: 0.9485\n",
      "Epoch 00057: val_loss did not improve from 0.12889\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.1592 - acc: 0.9485 - val_loss: 0.1346 - val_acc: 0.9581\n",
      "Epoch 58/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1599 - acc: 0.9483\n",
      "Epoch 00058: val_loss improved from 0.12889 to 0.12778, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_6_conv_checkpoint/058-0.1278.hdf5\n",
      "36805/36805 [==============================] - 31s 842us/sample - loss: 0.1599 - acc: 0.9483 - val_loss: 0.1278 - val_acc: 0.9597\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1529 - acc: 0.9499\n",
      "Epoch 00059: val_loss improved from 0.12778 to 0.12057, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_6_conv_checkpoint/059-0.1206.hdf5\n",
      "36805/36805 [==============================] - 31s 840us/sample - loss: 0.1529 - acc: 0.9499 - val_loss: 0.1206 - val_acc: 0.9625\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1546 - acc: 0.9492\n",
      "Epoch 00060: val_loss did not improve from 0.12057\n",
      "36805/36805 [==============================] - 31s 840us/sample - loss: 0.1546 - acc: 0.9492 - val_loss: 0.1379 - val_acc: 0.9583\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1523 - acc: 0.9513\n",
      "Epoch 00061: val_loss did not improve from 0.12057\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.1523 - acc: 0.9513 - val_loss: 0.1255 - val_acc: 0.9618\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1502 - acc: 0.9513\n",
      "Epoch 00062: val_loss did not improve from 0.12057\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.1502 - acc: 0.9513 - val_loss: 0.1223 - val_acc: 0.9602\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1486 - acc: 0.9523\n",
      "Epoch 00063: val_loss improved from 0.12057 to 0.12012, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_6_conv_checkpoint/063-0.1201.hdf5\n",
      "36805/36805 [==============================] - 31s 841us/sample - loss: 0.1486 - acc: 0.9522 - val_loss: 0.1201 - val_acc: 0.9613\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1461 - acc: 0.9526\n",
      "Epoch 00064: val_loss did not improve from 0.12012\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.1460 - acc: 0.9526 - val_loss: 0.1230 - val_acc: 0.9627\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1461 - acc: 0.9531\n",
      "Epoch 00065: val_loss did not improve from 0.12012\n",
      "36805/36805 [==============================] - 31s 834us/sample - loss: 0.1461 - acc: 0.9531 - val_loss: 0.1313 - val_acc: 0.9588\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1434 - acc: 0.9534\n",
      "Epoch 00066: val_loss improved from 0.12012 to 0.11537, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_6_conv_checkpoint/066-0.1154.hdf5\n",
      "36805/36805 [==============================] - 31s 840us/sample - loss: 0.1434 - acc: 0.9534 - val_loss: 0.1154 - val_acc: 0.9648\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1384 - acc: 0.9554\n",
      "Epoch 00067: val_loss did not improve from 0.11537\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.1384 - acc: 0.9554 - val_loss: 0.1258 - val_acc: 0.9597\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1408 - acc: 0.9536\n",
      "Epoch 00068: val_loss did not improve from 0.11537\n",
      "36805/36805 [==============================] - 31s 835us/sample - loss: 0.1407 - acc: 0.9536 - val_loss: 0.1221 - val_acc: 0.9616\n",
      "Epoch 69/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1373 - acc: 0.9561\n",
      "Epoch 00069: val_loss did not improve from 0.11537\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.1374 - acc: 0.9561 - val_loss: 0.1317 - val_acc: 0.9588\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1385 - acc: 0.9550\n",
      "Epoch 00070: val_loss did not improve from 0.11537\n",
      "36805/36805 [==============================] - 31s 841us/sample - loss: 0.1385 - acc: 0.9550 - val_loss: 0.1309 - val_acc: 0.9597\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1348 - acc: 0.9557\n",
      "Epoch 00071: val_loss did not improve from 0.11537\n",
      "36805/36805 [==============================] - 31s 836us/sample - loss: 0.1348 - acc: 0.9557 - val_loss: 0.1247 - val_acc: 0.9627\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1314 - acc: 0.9581\n",
      "Epoch 00072: val_loss did not improve from 0.11537\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.1314 - acc: 0.9581 - val_loss: 0.1200 - val_acc: 0.9625\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1307 - acc: 0.9570\n",
      "Epoch 00073: val_loss did not improve from 0.11537\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.1307 - acc: 0.9570 - val_loss: 0.1218 - val_acc: 0.9623\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1305 - acc: 0.9567\n",
      "Epoch 00074: val_loss did not improve from 0.11537\n",
      "36805/36805 [==============================] - 31s 835us/sample - loss: 0.1305 - acc: 0.9567 - val_loss: 0.1308 - val_acc: 0.9611\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1264 - acc: 0.9588\n",
      "Epoch 00075: val_loss did not improve from 0.11537\n",
      "36805/36805 [==============================] - 31s 834us/sample - loss: 0.1264 - acc: 0.9588 - val_loss: 0.1188 - val_acc: 0.9623\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1241 - acc: 0.9583\n",
      "Epoch 00076: val_loss did not improve from 0.11537\n",
      "36805/36805 [==============================] - 31s 836us/sample - loss: 0.1241 - acc: 0.9583 - val_loss: 0.1204 - val_acc: 0.9625\n",
      "Epoch 77/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1269 - acc: 0.9582\n",
      "Epoch 00077: val_loss did not improve from 0.11537\n",
      "36805/36805 [==============================] - 31s 835us/sample - loss: 0.1268 - acc: 0.9583 - val_loss: 0.1249 - val_acc: 0.9606\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1248 - acc: 0.9582\n",
      "Epoch 00078: val_loss did not improve from 0.11537\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.1248 - acc: 0.9582 - val_loss: 0.1277 - val_acc: 0.9620\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1200 - acc: 0.9608\n",
      "Epoch 00079: val_loss did not improve from 0.11537\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.1200 - acc: 0.9608 - val_loss: 0.1656 - val_acc: 0.9504\n",
      "Epoch 80/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1218 - acc: 0.9596\n",
      "Epoch 00080: val_loss did not improve from 0.11537\n",
      "36805/36805 [==============================] - 31s 834us/sample - loss: 0.1218 - acc: 0.9596 - val_loss: 0.1222 - val_acc: 0.9625\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1215 - acc: 0.9608\n",
      "Epoch 00081: val_loss did not improve from 0.11537\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.1215 - acc: 0.9608 - val_loss: 0.1183 - val_acc: 0.9611\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1224 - acc: 0.9589\n",
      "Epoch 00082: val_loss improved from 0.11537 to 0.11176, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_6_conv_checkpoint/082-0.1118.hdf5\n",
      "36805/36805 [==============================] - 31s 841us/sample - loss: 0.1224 - acc: 0.9589 - val_loss: 0.1118 - val_acc: 0.9634\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1152 - acc: 0.9616\n",
      "Epoch 00083: val_loss did not improve from 0.11176\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.1151 - acc: 0.9616 - val_loss: 0.1224 - val_acc: 0.9613\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1127 - acc: 0.9629\n",
      "Epoch 00084: val_loss did not improve from 0.11176\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.1127 - acc: 0.9629 - val_loss: 0.1185 - val_acc: 0.9630\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1135 - acc: 0.9618\n",
      "Epoch 00085: val_loss did not improve from 0.11176\n",
      "36805/36805 [==============================] - 31s 836us/sample - loss: 0.1135 - acc: 0.9618 - val_loss: 0.1291 - val_acc: 0.9611\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1169 - acc: 0.9612\n",
      "Epoch 00086: val_loss did not improve from 0.11176\n",
      "36805/36805 [==============================] - 31s 836us/sample - loss: 0.1169 - acc: 0.9612 - val_loss: 0.1223 - val_acc: 0.9632\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1084 - acc: 0.9643\n",
      "Epoch 00087: val_loss did not improve from 0.11176\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.1084 - acc: 0.9643 - val_loss: 0.1157 - val_acc: 0.9632\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1117 - acc: 0.9632\n",
      "Epoch 00088: val_loss did not improve from 0.11176\n",
      "36805/36805 [==============================] - 31s 836us/sample - loss: 0.1116 - acc: 0.9632 - val_loss: 0.1317 - val_acc: 0.9599\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1080 - acc: 0.9646\n",
      "Epoch 00089: val_loss did not improve from 0.11176\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.1080 - acc: 0.9647 - val_loss: 0.1273 - val_acc: 0.9630\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1068 - acc: 0.9648\n",
      "Epoch 00090: val_loss did not improve from 0.11176\n",
      "36805/36805 [==============================] - 31s 835us/sample - loss: 0.1068 - acc: 0.9648 - val_loss: 0.1263 - val_acc: 0.9618\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1047 - acc: 0.9649\n",
      "Epoch 00091: val_loss did not improve from 0.11176\n",
      "36805/36805 [==============================] - 31s 840us/sample - loss: 0.1047 - acc: 0.9649 - val_loss: 0.1410 - val_acc: 0.9595\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1055 - acc: 0.9653\n",
      "Epoch 00092: val_loss did not improve from 0.11176\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.1055 - acc: 0.9653 - val_loss: 0.1213 - val_acc: 0.9641\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1028 - acc: 0.9664\n",
      "Epoch 00093: val_loss did not improve from 0.11176\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.1028 - acc: 0.9664 - val_loss: 0.1215 - val_acc: 0.9639\n",
      "Epoch 94/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.1011 - acc: 0.9659\n",
      "Epoch 00094: val_loss did not improve from 0.11176\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.1009 - acc: 0.9660 - val_loss: 0.1152 - val_acc: 0.9651\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1019 - acc: 0.9667\n",
      "Epoch 00095: val_loss did not improve from 0.11176\n",
      "36805/36805 [==============================] - 31s 836us/sample - loss: 0.1019 - acc: 0.9667 - val_loss: 0.1211 - val_acc: 0.9611\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0990 - acc: 0.9674\n",
      "Epoch 00096: val_loss did not improve from 0.11176\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.0990 - acc: 0.9674 - val_loss: 0.1293 - val_acc: 0.9585\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0954 - acc: 0.9690\n",
      "Epoch 00097: val_loss did not improve from 0.11176\n",
      "36805/36805 [==============================] - 31s 836us/sample - loss: 0.0954 - acc: 0.9690 - val_loss: 0.1217 - val_acc: 0.9620\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0998 - acc: 0.9668\n",
      "Epoch 00098: val_loss did not improve from 0.11176\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.0998 - acc: 0.9668 - val_loss: 0.1203 - val_acc: 0.9620\n",
      "Epoch 99/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0952 - acc: 0.9682\n",
      "Epoch 00099: val_loss did not improve from 0.11176\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.0952 - acc: 0.9682 - val_loss: 0.1273 - val_acc: 0.9623\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1005 - acc: 0.9668\n",
      "Epoch 00100: val_loss did not improve from 0.11176\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.1004 - acc: 0.9668 - val_loss: 0.1141 - val_acc: 0.9632\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0961 - acc: 0.9690\n",
      "Epoch 00101: val_loss did not improve from 0.11176\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.0961 - acc: 0.9690 - val_loss: 0.1161 - val_acc: 0.9630\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0925 - acc: 0.9693\n",
      "Epoch 00102: val_loss did not improve from 0.11176\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.0925 - acc: 0.9693 - val_loss: 0.1248 - val_acc: 0.9604\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0918 - acc: 0.9692\n",
      "Epoch 00103: val_loss did not improve from 0.11176\n",
      "36805/36805 [==============================] - 31s 836us/sample - loss: 0.0918 - acc: 0.9692 - val_loss: 0.1169 - val_acc: 0.9632\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0875 - acc: 0.9702\n",
      "Epoch 00104: val_loss did not improve from 0.11176\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.0875 - acc: 0.9702 - val_loss: 0.1265 - val_acc: 0.9620\n",
      "Epoch 105/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0912 - acc: 0.9699\n",
      "Epoch 00105: val_loss did not improve from 0.11176\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.0912 - acc: 0.9699 - val_loss: 0.1178 - val_acc: 0.9641\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0903 - acc: 0.9696\n",
      "Epoch 00106: val_loss did not improve from 0.11176\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.0903 - acc: 0.9696 - val_loss: 0.1207 - val_acc: 0.9630\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0906 - acc: 0.9691\n",
      "Epoch 00107: val_loss did not improve from 0.11176\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.0906 - acc: 0.9691 - val_loss: 0.1169 - val_acc: 0.9655\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0880 - acc: 0.9707\n",
      "Epoch 00108: val_loss did not improve from 0.11176\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.0880 - acc: 0.9707 - val_loss: 0.1248 - val_acc: 0.9641\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0887 - acc: 0.9707\n",
      "Epoch 00109: val_loss did not improve from 0.11176\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.0887 - acc: 0.9706 - val_loss: 0.1316 - val_acc: 0.9637\n",
      "Epoch 110/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0917 - acc: 0.9695\n",
      "Epoch 00110: val_loss did not improve from 0.11176\n",
      "36805/36805 [==============================] - 31s 840us/sample - loss: 0.0917 - acc: 0.9695 - val_loss: 0.1219 - val_acc: 0.9616\n",
      "Epoch 111/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0832 - acc: 0.9717\n",
      "Epoch 00111: val_loss did not improve from 0.11176\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.0832 - acc: 0.9717 - val_loss: 0.1314 - val_acc: 0.9604\n",
      "Epoch 112/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0854 - acc: 0.9718\n",
      "Epoch 00112: val_loss did not improve from 0.11176\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.0854 - acc: 0.9718 - val_loss: 0.1377 - val_acc: 0.9618\n",
      "Epoch 113/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0824 - acc: 0.9718\n",
      "Epoch 00113: val_loss did not improve from 0.11176\n",
      "36805/36805 [==============================] - 31s 836us/sample - loss: 0.0823 - acc: 0.9718 - val_loss: 0.1167 - val_acc: 0.9672\n",
      "Epoch 114/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0834 - acc: 0.9719\n",
      "Epoch 00114: val_loss did not improve from 0.11176\n",
      "36805/36805 [==============================] - 31s 836us/sample - loss: 0.0834 - acc: 0.9719 - val_loss: 0.1296 - val_acc: 0.9609\n",
      "Epoch 115/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0852 - acc: 0.9711\n",
      "Epoch 00115: val_loss did not improve from 0.11176\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.0852 - acc: 0.9711 - val_loss: 0.1379 - val_acc: 0.9604\n",
      "Epoch 116/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0800 - acc: 0.9726\n",
      "Epoch 00116: val_loss did not improve from 0.11176\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.0800 - acc: 0.9726 - val_loss: 0.1246 - val_acc: 0.9613\n",
      "Epoch 117/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0829 - acc: 0.9718\n",
      "Epoch 00117: val_loss did not improve from 0.11176\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.0829 - acc: 0.9718 - val_loss: 0.1203 - val_acc: 0.9655\n",
      "Epoch 118/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0832 - acc: 0.9715\n",
      "Epoch 00118: val_loss did not improve from 0.11176\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.0831 - acc: 0.9716 - val_loss: 0.1195 - val_acc: 0.9634\n",
      "Epoch 119/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0796 - acc: 0.9724\n",
      "Epoch 00119: val_loss did not improve from 0.11176\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.0796 - acc: 0.9724 - val_loss: 0.1259 - val_acc: 0.9641\n",
      "Epoch 120/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0775 - acc: 0.9738\n",
      "Epoch 00120: val_loss did not improve from 0.11176\n",
      "36805/36805 [==============================] - 31s 840us/sample - loss: 0.0775 - acc: 0.9738 - val_loss: 0.1279 - val_acc: 0.9632\n",
      "Epoch 121/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0782 - acc: 0.9735\n",
      "Epoch 00121: val_loss did not improve from 0.11176\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.0782 - acc: 0.9735 - val_loss: 0.1214 - val_acc: 0.9651\n",
      "Epoch 122/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0789 - acc: 0.9733\n",
      "Epoch 00122: val_loss did not improve from 0.11176\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.0789 - acc: 0.9733 - val_loss: 0.1198 - val_acc: 0.9646\n",
      "Epoch 123/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0750 - acc: 0.9751\n",
      "Epoch 00123: val_loss did not improve from 0.11176\n",
      "36805/36805 [==============================] - 31s 837us/sample - loss: 0.0750 - acc: 0.9751 - val_loss: 0.1337 - val_acc: 0.9609\n",
      "Epoch 124/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0758 - acc: 0.9752\n",
      "Epoch 00124: val_loss did not improve from 0.11176\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.0758 - acc: 0.9752 - val_loss: 0.1191 - val_acc: 0.9634\n",
      "Epoch 125/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0744 - acc: 0.9752\n",
      "Epoch 00125: val_loss did not improve from 0.11176\n",
      "36805/36805 [==============================] - 31s 834us/sample - loss: 0.0744 - acc: 0.9752 - val_loss: 0.1308 - val_acc: 0.9627\n",
      "Epoch 126/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0714 - acc: 0.9762\n",
      "Epoch 00126: val_loss did not improve from 0.11176\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.0714 - acc: 0.9762 - val_loss: 0.1269 - val_acc: 0.9644\n",
      "Epoch 127/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0743 - acc: 0.9750\n",
      "Epoch 00127: val_loss did not improve from 0.11176\n",
      "36805/36805 [==============================] - 31s 836us/sample - loss: 0.0743 - acc: 0.9750 - val_loss: 0.1406 - val_acc: 0.9606\n",
      "Epoch 128/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0750 - acc: 0.9748\n",
      "Epoch 00128: val_loss did not improve from 0.11176\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.0750 - acc: 0.9748 - val_loss: 0.1295 - val_acc: 0.9627\n",
      "Epoch 129/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0712 - acc: 0.9751\n",
      "Epoch 00129: val_loss did not improve from 0.11176\n",
      "36805/36805 [==============================] - 31s 836us/sample - loss: 0.0712 - acc: 0.9751 - val_loss: 0.1372 - val_acc: 0.9611\n",
      "Epoch 130/500\n",
      "36736/36805 [============================>.] - ETA: 0s - loss: 0.0709 - acc: 0.9749\n",
      "Epoch 00130: val_loss did not improve from 0.11176\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.0709 - acc: 0.9749 - val_loss: 0.1337 - val_acc: 0.9625\n",
      "Epoch 131/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0687 - acc: 0.9762\n",
      "Epoch 00131: val_loss did not improve from 0.11176\n",
      "36805/36805 [==============================] - 31s 838us/sample - loss: 0.0687 - acc: 0.9762 - val_loss: 0.1288 - val_acc: 0.9648\n",
      "Epoch 132/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0691 - acc: 0.9768\n",
      "Epoch 00132: val_loss did not improve from 0.11176\n",
      "36805/36805 [==============================] - 31s 839us/sample - loss: 0.0691 - acc: 0.9768 - val_loss: 0.1305 - val_acc: 0.9623\n",
      "\n",
      "1D_CNN_custom_multi_2_GMP_DO_6_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 424us/sample - loss: 0.1583 - acc: 0.9516\n",
      "Loss: 0.1583239990974141 Accuracy: 0.95160955\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.0439 - acc: 0.3379\n",
      "Epoch 00001: val_loss improved from inf to 1.12394, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_7_conv_checkpoint/001-1.1239.hdf5\n",
      "36805/36805 [==============================] - 34s 937us/sample - loss: 2.0439 - acc: 0.3379 - val_loss: 1.1239 - val_acc: 0.6830\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.1353 - acc: 0.6391\n",
      "Epoch 00002: val_loss improved from 1.12394 to 0.65201, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_7_conv_checkpoint/002-0.6520.hdf5\n",
      "36805/36805 [==============================] - 32s 867us/sample - loss: 1.1353 - acc: 0.6391 - val_loss: 0.6520 - val_acc: 0.8234\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.8118 - acc: 0.7457\n",
      "Epoch 00003: val_loss improved from 0.65201 to 0.52260, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_7_conv_checkpoint/003-0.5226.hdf5\n",
      "36805/36805 [==============================] - 32s 860us/sample - loss: 0.8118 - acc: 0.7457 - val_loss: 0.5226 - val_acc: 0.8577\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6489 - acc: 0.7967\n",
      "Epoch 00004: val_loss improved from 0.52260 to 0.39187, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_7_conv_checkpoint/004-0.3919.hdf5\n",
      "36805/36805 [==============================] - 32s 867us/sample - loss: 0.6488 - acc: 0.7967 - val_loss: 0.3919 - val_acc: 0.8894\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5579 - acc: 0.8248\n",
      "Epoch 00005: val_loss improved from 0.39187 to 0.34270, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_7_conv_checkpoint/005-0.3427.hdf5\n",
      "36805/36805 [==============================] - 32s 866us/sample - loss: 0.5580 - acc: 0.8248 - val_loss: 0.3427 - val_acc: 0.9003\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5098 - acc: 0.8411\n",
      "Epoch 00006: val_loss improved from 0.34270 to 0.30970, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_7_conv_checkpoint/006-0.3097.hdf5\n",
      "36805/36805 [==============================] - 32s 867us/sample - loss: 0.5098 - acc: 0.8411 - val_loss: 0.3097 - val_acc: 0.9131\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4617 - acc: 0.8556\n",
      "Epoch 00007: val_loss improved from 0.30970 to 0.29437, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_7_conv_checkpoint/007-0.2944.hdf5\n",
      "36805/36805 [==============================] - 32s 869us/sample - loss: 0.4616 - acc: 0.8556 - val_loss: 0.2944 - val_acc: 0.9161\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4267 - acc: 0.8658\n",
      "Epoch 00008: val_loss improved from 0.29437 to 0.25722, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_7_conv_checkpoint/008-0.2572.hdf5\n",
      "36805/36805 [==============================] - 32s 868us/sample - loss: 0.4267 - acc: 0.8658 - val_loss: 0.2572 - val_acc: 0.9241\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3879 - acc: 0.8800\n",
      "Epoch 00009: val_loss improved from 0.25722 to 0.22451, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_7_conv_checkpoint/009-0.2245.hdf5\n",
      "36805/36805 [==============================] - 32s 863us/sample - loss: 0.3879 - acc: 0.8800 - val_loss: 0.2245 - val_acc: 0.9345\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3661 - acc: 0.8853\n",
      "Epoch 00010: val_loss did not improve from 0.22451\n",
      "36805/36805 [==============================] - 32s 857us/sample - loss: 0.3661 - acc: 0.8853 - val_loss: 0.2647 - val_acc: 0.9168\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3447 - acc: 0.8935\n",
      "Epoch 00011: val_loss improved from 0.22451 to 0.20579, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_7_conv_checkpoint/011-0.2058.hdf5\n",
      "36805/36805 [==============================] - 32s 864us/sample - loss: 0.3448 - acc: 0.8935 - val_loss: 0.2058 - val_acc: 0.9376\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3238 - acc: 0.8984\n",
      "Epoch 00012: val_loss improved from 0.20579 to 0.20282, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_7_conv_checkpoint/012-0.2028.hdf5\n",
      "36805/36805 [==============================] - 32s 866us/sample - loss: 0.3238 - acc: 0.8984 - val_loss: 0.2028 - val_acc: 0.9390\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3040 - acc: 0.9037\n",
      "Epoch 00013: val_loss improved from 0.20282 to 0.17918, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_7_conv_checkpoint/013-0.1792.hdf5\n",
      "36805/36805 [==============================] - 32s 859us/sample - loss: 0.3039 - acc: 0.9037 - val_loss: 0.1792 - val_acc: 0.9511\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2875 - acc: 0.9085\n",
      "Epoch 00014: val_loss did not improve from 0.17918\n",
      "36805/36805 [==============================] - 32s 859us/sample - loss: 0.2874 - acc: 0.9085 - val_loss: 0.2033 - val_acc: 0.9355\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2765 - acc: 0.9123\n",
      "Epoch 00015: val_loss improved from 0.17918 to 0.16108, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_7_conv_checkpoint/015-0.1611.hdf5\n",
      "36805/36805 [==============================] - 32s 858us/sample - loss: 0.2767 - acc: 0.9122 - val_loss: 0.1611 - val_acc: 0.9564\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2636 - acc: 0.9166\n",
      "Epoch 00016: val_loss improved from 0.16108 to 0.15901, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_7_conv_checkpoint/016-0.1590.hdf5\n",
      "36805/36805 [==============================] - 32s 866us/sample - loss: 0.2637 - acc: 0.9166 - val_loss: 0.1590 - val_acc: 0.9532\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2528 - acc: 0.9210\n",
      "Epoch 00017: val_loss did not improve from 0.15901\n",
      "36805/36805 [==============================] - 32s 857us/sample - loss: 0.2528 - acc: 0.9210 - val_loss: 0.1598 - val_acc: 0.9527\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2410 - acc: 0.9238\n",
      "Epoch 00018: val_loss did not improve from 0.15901\n",
      "36805/36805 [==============================] - 32s 870us/sample - loss: 0.2411 - acc: 0.9238 - val_loss: 0.1607 - val_acc: 0.9499\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2287 - acc: 0.9275\n",
      "Epoch 00019: val_loss did not improve from 0.15901\n",
      "36805/36805 [==============================] - 33s 888us/sample - loss: 0.2287 - acc: 0.9275 - val_loss: 0.1654 - val_acc: 0.9481\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2225 - acc: 0.9296\n",
      "Epoch 00020: val_loss improved from 0.15901 to 0.14702, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_7_conv_checkpoint/020-0.1470.hdf5\n",
      "36805/36805 [==============================] - 33s 889us/sample - loss: 0.2225 - acc: 0.9296 - val_loss: 0.1470 - val_acc: 0.9555\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2121 - acc: 0.9321\n",
      "Epoch 00021: val_loss improved from 0.14702 to 0.13777, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_7_conv_checkpoint/021-0.1378.hdf5\n",
      "36805/36805 [==============================] - 33s 885us/sample - loss: 0.2121 - acc: 0.9321 - val_loss: 0.1378 - val_acc: 0.9583\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2047 - acc: 0.9346\n",
      "Epoch 00022: val_loss did not improve from 0.13777\n",
      "36805/36805 [==============================] - 33s 884us/sample - loss: 0.2047 - acc: 0.9347 - val_loss: 0.1382 - val_acc: 0.9602\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1982 - acc: 0.9370\n",
      "Epoch 00023: val_loss improved from 0.13777 to 0.13179, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_7_conv_checkpoint/023-0.1318.hdf5\n",
      "36805/36805 [==============================] - 33s 886us/sample - loss: 0.1982 - acc: 0.9370 - val_loss: 0.1318 - val_acc: 0.9583\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1899 - acc: 0.9402\n",
      "Epoch 00024: val_loss did not improve from 0.13179\n",
      "36805/36805 [==============================] - 33s 888us/sample - loss: 0.1899 - acc: 0.9403 - val_loss: 0.1339 - val_acc: 0.9613\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1874 - acc: 0.9412\n",
      "Epoch 00025: val_loss improved from 0.13179 to 0.12770, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_7_conv_checkpoint/025-0.1277.hdf5\n",
      "36805/36805 [==============================] - 33s 887us/sample - loss: 0.1875 - acc: 0.9412 - val_loss: 0.1277 - val_acc: 0.9625\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1801 - acc: 0.9435\n",
      "Epoch 00026: val_loss improved from 0.12770 to 0.12202, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_7_conv_checkpoint/026-0.1220.hdf5\n",
      "36805/36805 [==============================] - 33s 890us/sample - loss: 0.1801 - acc: 0.9435 - val_loss: 0.1220 - val_acc: 0.9623\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1750 - acc: 0.9429\n",
      "Epoch 00027: val_loss did not improve from 0.12202\n",
      "36805/36805 [==============================] - 33s 887us/sample - loss: 0.1750 - acc: 0.9429 - val_loss: 0.1291 - val_acc: 0.9606\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1709 - acc: 0.9445\n",
      "Epoch 00028: val_loss did not improve from 0.12202\n",
      "36805/36805 [==============================] - 33s 885us/sample - loss: 0.1709 - acc: 0.9445 - val_loss: 0.1243 - val_acc: 0.9632\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1643 - acc: 0.9479\n",
      "Epoch 00029: val_loss did not improve from 0.12202\n",
      "36805/36805 [==============================] - 33s 883us/sample - loss: 0.1643 - acc: 0.9479 - val_loss: 0.1433 - val_acc: 0.9557\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1612 - acc: 0.9476\n",
      "Epoch 00030: val_loss improved from 0.12202 to 0.12084, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_7_conv_checkpoint/030-0.1208.hdf5\n",
      "36805/36805 [==============================] - 33s 892us/sample - loss: 0.1611 - acc: 0.9476 - val_loss: 0.1208 - val_acc: 0.9641\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1523 - acc: 0.9504\n",
      "Epoch 00031: val_loss did not improve from 0.12084\n",
      "36805/36805 [==============================] - 33s 887us/sample - loss: 0.1523 - acc: 0.9504 - val_loss: 0.1317 - val_acc: 0.9592\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1489 - acc: 0.9519\n",
      "Epoch 00032: val_loss did not improve from 0.12084\n",
      "36805/36805 [==============================] - 33s 886us/sample - loss: 0.1489 - acc: 0.9519 - val_loss: 0.1302 - val_acc: 0.9609\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1446 - acc: 0.9532\n",
      "Epoch 00033: val_loss did not improve from 0.12084\n",
      "36805/36805 [==============================] - 33s 886us/sample - loss: 0.1446 - acc: 0.9532 - val_loss: 0.1228 - val_acc: 0.9609\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1433 - acc: 0.9526\n",
      "Epoch 00034: val_loss improved from 0.12084 to 0.11945, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_7_conv_checkpoint/034-0.1195.hdf5\n",
      "36805/36805 [==============================] - 33s 890us/sample - loss: 0.1433 - acc: 0.9525 - val_loss: 0.1195 - val_acc: 0.9616\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1377 - acc: 0.9558\n",
      "Epoch 00035: val_loss did not improve from 0.11945\n",
      "36805/36805 [==============================] - 33s 884us/sample - loss: 0.1377 - acc: 0.9558 - val_loss: 0.1286 - val_acc: 0.9611\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1359 - acc: 0.9557\n",
      "Epoch 00036: val_loss did not improve from 0.11945\n",
      "36805/36805 [==============================] - 33s 886us/sample - loss: 0.1359 - acc: 0.9557 - val_loss: 0.1220 - val_acc: 0.9672\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1344 - acc: 0.9557\n",
      "Epoch 00037: val_loss improved from 0.11945 to 0.11193, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_7_conv_checkpoint/037-0.1119.hdf5\n",
      "36805/36805 [==============================] - 33s 888us/sample - loss: 0.1344 - acc: 0.9557 - val_loss: 0.1119 - val_acc: 0.9658\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1246 - acc: 0.9589\n",
      "Epoch 00038: val_loss did not improve from 0.11193\n",
      "36805/36805 [==============================] - 33s 883us/sample - loss: 0.1246 - acc: 0.9589 - val_loss: 0.1164 - val_acc: 0.9637\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1231 - acc: 0.9597\n",
      "Epoch 00039: val_loss improved from 0.11193 to 0.11045, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_7_conv_checkpoint/039-0.1104.hdf5\n",
      "36805/36805 [==============================] - 33s 887us/sample - loss: 0.1232 - acc: 0.9597 - val_loss: 0.1104 - val_acc: 0.9655\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1182 - acc: 0.9609\n",
      "Epoch 00040: val_loss did not improve from 0.11045\n",
      "36805/36805 [==============================] - 33s 885us/sample - loss: 0.1182 - acc: 0.9608 - val_loss: 0.1199 - val_acc: 0.9651\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1194 - acc: 0.9598\n",
      "Epoch 00041: val_loss did not improve from 0.11045\n",
      "36805/36805 [==============================] - 33s 884us/sample - loss: 0.1194 - acc: 0.9598 - val_loss: 0.1136 - val_acc: 0.9660\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1171 - acc: 0.9614\n",
      "Epoch 00042: val_loss did not improve from 0.11045\n",
      "36805/36805 [==============================] - 33s 886us/sample - loss: 0.1171 - acc: 0.9614 - val_loss: 0.1172 - val_acc: 0.9644\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1109 - acc: 0.9636\n",
      "Epoch 00043: val_loss did not improve from 0.11045\n",
      "36805/36805 [==============================] - 33s 883us/sample - loss: 0.1109 - acc: 0.9636 - val_loss: 0.1105 - val_acc: 0.9651\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1122 - acc: 0.9626\n",
      "Epoch 00044: val_loss improved from 0.11045 to 0.10815, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_7_conv_checkpoint/044-0.1082.hdf5\n",
      "36805/36805 [==============================] - 33s 888us/sample - loss: 0.1121 - acc: 0.9626 - val_loss: 0.1082 - val_acc: 0.9669\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1076 - acc: 0.9657\n",
      "Epoch 00045: val_loss did not improve from 0.10815\n",
      "36805/36805 [==============================] - 32s 882us/sample - loss: 0.1076 - acc: 0.9657 - val_loss: 0.1181 - val_acc: 0.9634\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1048 - acc: 0.9658\n",
      "Epoch 00046: val_loss did not improve from 0.10815\n",
      "36805/36805 [==============================] - 33s 887us/sample - loss: 0.1048 - acc: 0.9658 - val_loss: 0.1091 - val_acc: 0.9660\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1004 - acc: 0.9667\n",
      "Epoch 00047: val_loss did not improve from 0.10815\n",
      "36805/36805 [==============================] - 33s 894us/sample - loss: 0.1004 - acc: 0.9667 - val_loss: 0.1170 - val_acc: 0.9639\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0979 - acc: 0.9674\n",
      "Epoch 00048: val_loss did not improve from 0.10815\n",
      "36805/36805 [==============================] - 33s 885us/sample - loss: 0.0979 - acc: 0.9674 - val_loss: 0.1139 - val_acc: 0.9662\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1001 - acc: 0.9668\n",
      "Epoch 00049: val_loss did not improve from 0.10815\n",
      "36805/36805 [==============================] - 33s 885us/sample - loss: 0.1001 - acc: 0.9668 - val_loss: 0.1082 - val_acc: 0.9660\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0943 - acc: 0.9689\n",
      "Epoch 00050: val_loss did not improve from 0.10815\n",
      "36805/36805 [==============================] - 33s 886us/sample - loss: 0.0943 - acc: 0.9689 - val_loss: 0.1092 - val_acc: 0.9683\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0950 - acc: 0.9683\n",
      "Epoch 00051: val_loss did not improve from 0.10815\n",
      "36805/36805 [==============================] - 32s 881us/sample - loss: 0.0949 - acc: 0.9683 - val_loss: 0.1101 - val_acc: 0.9681\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0928 - acc: 0.9685\n",
      "Epoch 00052: val_loss did not improve from 0.10815\n",
      "36805/36805 [==============================] - 33s 883us/sample - loss: 0.0928 - acc: 0.9685 - val_loss: 0.1090 - val_acc: 0.9676\n",
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0893 - acc: 0.9712\n",
      "Epoch 00053: val_loss did not improve from 0.10815\n",
      "36805/36805 [==============================] - 33s 884us/sample - loss: 0.0892 - acc: 0.9712 - val_loss: 0.1166 - val_acc: 0.9679\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0857 - acc: 0.9709\n",
      "Epoch 00054: val_loss did not improve from 0.10815\n",
      "36805/36805 [==============================] - 33s 885us/sample - loss: 0.0857 - acc: 0.9709 - val_loss: 0.1187 - val_acc: 0.9667\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0827 - acc: 0.9730\n",
      "Epoch 00055: val_loss did not improve from 0.10815\n",
      "36805/36805 [==============================] - 32s 883us/sample - loss: 0.0826 - acc: 0.9730 - val_loss: 0.1176 - val_acc: 0.9681\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0847 - acc: 0.9714\n",
      "Epoch 00056: val_loss did not improve from 0.10815\n",
      "36805/36805 [==============================] - 33s 884us/sample - loss: 0.0847 - acc: 0.9714 - val_loss: 0.1193 - val_acc: 0.9637\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0814 - acc: 0.9720\n",
      "Epoch 00057: val_loss did not improve from 0.10815\n",
      "36805/36805 [==============================] - 33s 884us/sample - loss: 0.0814 - acc: 0.9720 - val_loss: 0.1257 - val_acc: 0.9644\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0809 - acc: 0.9724\n",
      "Epoch 00058: val_loss did not improve from 0.10815\n",
      "36805/36805 [==============================] - 32s 880us/sample - loss: 0.0809 - acc: 0.9724 - val_loss: 0.1128 - val_acc: 0.9676\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0762 - acc: 0.9751\n",
      "Epoch 00059: val_loss improved from 0.10815 to 0.10812, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_7_conv_checkpoint/059-0.1081.hdf5\n",
      "36805/36805 [==============================] - 32s 876us/sample - loss: 0.0762 - acc: 0.9751 - val_loss: 0.1081 - val_acc: 0.9674\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0776 - acc: 0.9739\n",
      "Epoch 00060: val_loss did not improve from 0.10812\n",
      "36805/36805 [==============================] - 32s 882us/sample - loss: 0.0776 - acc: 0.9739 - val_loss: 0.1154 - val_acc: 0.9660\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0764 - acc: 0.9741\n",
      "Epoch 00061: val_loss did not improve from 0.10812\n",
      "36805/36805 [==============================] - 33s 886us/sample - loss: 0.0765 - acc: 0.9741 - val_loss: 0.1090 - val_acc: 0.9648\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0736 - acc: 0.9747\n",
      "Epoch 00062: val_loss did not improve from 0.10812\n",
      "36805/36805 [==============================] - 33s 888us/sample - loss: 0.0735 - acc: 0.9747 - val_loss: 0.1246 - val_acc: 0.9686\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0747 - acc: 0.9742\n",
      "Epoch 00063: val_loss did not improve from 0.10812\n",
      "36805/36805 [==============================] - 33s 885us/sample - loss: 0.0747 - acc: 0.9742 - val_loss: 0.1253 - val_acc: 0.9660\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0706 - acc: 0.9758\n",
      "Epoch 00064: val_loss did not improve from 0.10812\n",
      "36805/36805 [==============================] - 32s 881us/sample - loss: 0.0706 - acc: 0.9757 - val_loss: 0.1101 - val_acc: 0.9700\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0706 - acc: 0.9763\n",
      "Epoch 00065: val_loss did not improve from 0.10812\n",
      "36805/36805 [==============================] - 33s 887us/sample - loss: 0.0706 - acc: 0.9763 - val_loss: 0.1115 - val_acc: 0.9662\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0692 - acc: 0.9755\n",
      "Epoch 00066: val_loss did not improve from 0.10812\n",
      "36805/36805 [==============================] - 33s 885us/sample - loss: 0.0692 - acc: 0.9755 - val_loss: 0.1139 - val_acc: 0.9695\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0644 - acc: 0.9778\n",
      "Epoch 00067: val_loss did not improve from 0.10812\n",
      "36805/36805 [==============================] - 33s 886us/sample - loss: 0.0644 - acc: 0.9778 - val_loss: 0.1217 - val_acc: 0.9683\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0654 - acc: 0.9784\n",
      "Epoch 00068: val_loss did not improve from 0.10812\n",
      "36805/36805 [==============================] - 33s 883us/sample - loss: 0.0654 - acc: 0.9784 - val_loss: 0.1180 - val_acc: 0.9667\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0623 - acc: 0.9787\n",
      "Epoch 00069: val_loss did not improve from 0.10812\n",
      "36805/36805 [==============================] - 33s 885us/sample - loss: 0.0623 - acc: 0.9787 - val_loss: 0.1173 - val_acc: 0.9653\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0631 - acc: 0.9787\n",
      "Epoch 00070: val_loss did not improve from 0.10812\n",
      "36805/36805 [==============================] - 33s 883us/sample - loss: 0.0631 - acc: 0.9787 - val_loss: 0.1221 - val_acc: 0.9667\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0621 - acc: 0.9792\n",
      "Epoch 00071: val_loss did not improve from 0.10812\n",
      "36805/36805 [==============================] - 33s 885us/sample - loss: 0.0621 - acc: 0.9792 - val_loss: 0.1167 - val_acc: 0.9695\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0570 - acc: 0.9811\n",
      "Epoch 00072: val_loss did not improve from 0.10812\n",
      "36805/36805 [==============================] - 32s 881us/sample - loss: 0.0570 - acc: 0.9811 - val_loss: 0.1307 - val_acc: 0.9648\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0598 - acc: 0.9796\n",
      "Epoch 00073: val_loss did not improve from 0.10812\n",
      "36805/36805 [==============================] - 32s 882us/sample - loss: 0.0598 - acc: 0.9796 - val_loss: 0.1169 - val_acc: 0.9681\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0572 - acc: 0.9806\n",
      "Epoch 00074: val_loss did not improve from 0.10812\n",
      "36805/36805 [==============================] - 33s 884us/sample - loss: 0.0571 - acc: 0.9806 - val_loss: 0.1169 - val_acc: 0.9683\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0575 - acc: 0.9799\n",
      "Epoch 00075: val_loss did not improve from 0.10812\n",
      "36805/36805 [==============================] - 33s 888us/sample - loss: 0.0575 - acc: 0.9799 - val_loss: 0.1183 - val_acc: 0.9697\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0509 - acc: 0.9829\n",
      "Epoch 00076: val_loss did not improve from 0.10812\n",
      "36805/36805 [==============================] - 33s 884us/sample - loss: 0.0509 - acc: 0.9828 - val_loss: 0.1315 - val_acc: 0.9655\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0588 - acc: 0.9805\n",
      "Epoch 00077: val_loss did not improve from 0.10812\n",
      "36805/36805 [==============================] - 33s 888us/sample - loss: 0.0589 - acc: 0.9805 - val_loss: 0.1137 - val_acc: 0.9711\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0539 - acc: 0.9819\n",
      "Epoch 00078: val_loss did not improve from 0.10812\n",
      "36805/36805 [==============================] - 33s 887us/sample - loss: 0.0539 - acc: 0.9819 - val_loss: 0.1180 - val_acc: 0.9693\n",
      "Epoch 79/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0549 - acc: 0.9816\n",
      "Epoch 00079: val_loss did not improve from 0.10812\n",
      "36805/36805 [==============================] - 33s 884us/sample - loss: 0.0549 - acc: 0.9816 - val_loss: 0.1193 - val_acc: 0.9706\n",
      "Epoch 80/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0506 - acc: 0.9827\n",
      "Epoch 00080: val_loss did not improve from 0.10812\n",
      "36805/36805 [==============================] - 33s 891us/sample - loss: 0.0506 - acc: 0.9826 - val_loss: 0.1268 - val_acc: 0.9716\n",
      "Epoch 81/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0498 - acc: 0.9832\n",
      "Epoch 00081: val_loss did not improve from 0.10812\n",
      "36805/36805 [==============================] - 33s 886us/sample - loss: 0.0498 - acc: 0.9832 - val_loss: 0.1146 - val_acc: 0.9686\n",
      "Epoch 82/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0501 - acc: 0.9836\n",
      "Epoch 00082: val_loss did not improve from 0.10812\n",
      "36805/36805 [==============================] - 33s 888us/sample - loss: 0.0501 - acc: 0.9836 - val_loss: 0.1268 - val_acc: 0.9665\n",
      "Epoch 83/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0490 - acc: 0.9838\n",
      "Epoch 00083: val_loss did not improve from 0.10812\n",
      "36805/36805 [==============================] - 33s 887us/sample - loss: 0.0490 - acc: 0.9838 - val_loss: 0.1214 - val_acc: 0.9679\n",
      "Epoch 84/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0500 - acc: 0.9833\n",
      "Epoch 00084: val_loss did not improve from 0.10812\n",
      "36805/36805 [==============================] - 33s 885us/sample - loss: 0.0499 - acc: 0.9833 - val_loss: 0.1208 - val_acc: 0.9706\n",
      "Epoch 85/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0473 - acc: 0.9846\n",
      "Epoch 00085: val_loss did not improve from 0.10812\n",
      "36805/36805 [==============================] - 33s 884us/sample - loss: 0.0473 - acc: 0.9846 - val_loss: 0.1200 - val_acc: 0.9702\n",
      "Epoch 86/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0480 - acc: 0.9839\n",
      "Epoch 00086: val_loss did not improve from 0.10812\n",
      "36805/36805 [==============================] - 33s 889us/sample - loss: 0.0480 - acc: 0.9839 - val_loss: 0.1286 - val_acc: 0.9697\n",
      "Epoch 87/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0480 - acc: 0.9835\n",
      "Epoch 00087: val_loss did not improve from 0.10812\n",
      "36805/36805 [==============================] - 33s 887us/sample - loss: 0.0480 - acc: 0.9835 - val_loss: 0.1261 - val_acc: 0.9669\n",
      "Epoch 88/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0452 - acc: 0.9847\n",
      "Epoch 00088: val_loss did not improve from 0.10812\n",
      "36805/36805 [==============================] - 33s 884us/sample - loss: 0.0452 - acc: 0.9847 - val_loss: 0.1215 - val_acc: 0.9709\n",
      "Epoch 89/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0449 - acc: 0.9848\n",
      "Epoch 00089: val_loss did not improve from 0.10812\n",
      "36805/36805 [==============================] - 33s 886us/sample - loss: 0.0449 - acc: 0.9848 - val_loss: 0.1188 - val_acc: 0.9665\n",
      "Epoch 90/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0439 - acc: 0.9851\n",
      "Epoch 00090: val_loss did not improve from 0.10812\n",
      "36805/36805 [==============================] - 33s 888us/sample - loss: 0.0439 - acc: 0.9851 - val_loss: 0.1236 - val_acc: 0.9700\n",
      "Epoch 91/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0506 - acc: 0.9838\n",
      "Epoch 00091: val_loss did not improve from 0.10812\n",
      "36805/36805 [==============================] - 33s 887us/sample - loss: 0.0506 - acc: 0.9838 - val_loss: 0.1258 - val_acc: 0.9690\n",
      "Epoch 92/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0401 - acc: 0.9868\n",
      "Epoch 00092: val_loss did not improve from 0.10812\n",
      "36805/36805 [==============================] - 33s 886us/sample - loss: 0.0401 - acc: 0.9868 - val_loss: 0.1282 - val_acc: 0.9676\n",
      "Epoch 93/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0406 - acc: 0.9863\n",
      "Epoch 00093: val_loss did not improve from 0.10812\n",
      "36805/36805 [==============================] - 33s 884us/sample - loss: 0.0406 - acc: 0.9863 - val_loss: 0.1295 - val_acc: 0.9679\n",
      "Epoch 94/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0431 - acc: 0.9853\n",
      "Epoch 00094: val_loss did not improve from 0.10812\n",
      "36805/36805 [==============================] - 33s 886us/sample - loss: 0.0431 - acc: 0.9853 - val_loss: 0.1268 - val_acc: 0.9695\n",
      "Epoch 95/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0408 - acc: 0.9869\n",
      "Epoch 00095: val_loss did not improve from 0.10812\n",
      "36805/36805 [==============================] - 33s 885us/sample - loss: 0.0408 - acc: 0.9869 - val_loss: 0.1337 - val_acc: 0.9681\n",
      "Epoch 96/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0427 - acc: 0.9862\n",
      "Epoch 00096: val_loss did not improve from 0.10812\n",
      "36805/36805 [==============================] - 33s 885us/sample - loss: 0.0427 - acc: 0.9862 - val_loss: 0.1432 - val_acc: 0.9683\n",
      "Epoch 97/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0390 - acc: 0.9867\n",
      "Epoch 00097: val_loss did not improve from 0.10812\n",
      "36805/36805 [==============================] - 33s 886us/sample - loss: 0.0390 - acc: 0.9867 - val_loss: 0.1392 - val_acc: 0.9665\n",
      "Epoch 98/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0415 - acc: 0.9862\n",
      "Epoch 00098: val_loss did not improve from 0.10812\n",
      "36805/36805 [==============================] - 33s 888us/sample - loss: 0.0415 - acc: 0.9862 - val_loss: 0.1281 - val_acc: 0.9665\n",
      "Epoch 99/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0381 - acc: 0.9866\n",
      "Epoch 00099: val_loss did not improve from 0.10812\n",
      "36805/36805 [==============================] - 33s 889us/sample - loss: 0.0381 - acc: 0.9866 - val_loss: 0.1335 - val_acc: 0.9704\n",
      "Epoch 100/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0348 - acc: 0.9887\n",
      "Epoch 00100: val_loss did not improve from 0.10812\n",
      "36805/36805 [==============================] - 33s 886us/sample - loss: 0.0349 - acc: 0.9887 - val_loss: 0.1463 - val_acc: 0.9669\n",
      "Epoch 101/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0412 - acc: 0.9861\n",
      "Epoch 00101: val_loss did not improve from 0.10812\n",
      "36805/36805 [==============================] - 33s 887us/sample - loss: 0.0412 - acc: 0.9861 - val_loss: 0.1267 - val_acc: 0.9690\n",
      "Epoch 102/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0394 - acc: 0.9866\n",
      "Epoch 00102: val_loss did not improve from 0.10812\n",
      "36805/36805 [==============================] - 33s 885us/sample - loss: 0.0394 - acc: 0.9866 - val_loss: 0.1265 - val_acc: 0.9693\n",
      "Epoch 103/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0359 - acc: 0.9882\n",
      "Epoch 00103: val_loss did not improve from 0.10812\n",
      "36805/36805 [==============================] - 33s 884us/sample - loss: 0.0359 - acc: 0.9882 - val_loss: 0.1318 - val_acc: 0.9688\n",
      "Epoch 104/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0365 - acc: 0.9879\n",
      "Epoch 00104: val_loss did not improve from 0.10812\n",
      "36805/36805 [==============================] - 33s 884us/sample - loss: 0.0365 - acc: 0.9879 - val_loss: 0.1533 - val_acc: 0.9686\n",
      "Epoch 105/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0400 - acc: 0.9864\n",
      "Epoch 00105: val_loss did not improve from 0.10812\n",
      "36805/36805 [==============================] - 32s 883us/sample - loss: 0.0400 - acc: 0.9864 - val_loss: 0.1439 - val_acc: 0.9690\n",
      "Epoch 106/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0331 - acc: 0.9887\n",
      "Epoch 00106: val_loss did not improve from 0.10812\n",
      "36805/36805 [==============================] - 33s 885us/sample - loss: 0.0331 - acc: 0.9888 - val_loss: 0.1468 - val_acc: 0.9681\n",
      "Epoch 107/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0345 - acc: 0.9887\n",
      "Epoch 00107: val_loss did not improve from 0.10812\n",
      "36805/36805 [==============================] - 33s 884us/sample - loss: 0.0345 - acc: 0.9887 - val_loss: 0.1448 - val_acc: 0.9669\n",
      "Epoch 108/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0313 - acc: 0.9894\n",
      "Epoch 00108: val_loss did not improve from 0.10812\n",
      "36805/36805 [==============================] - 33s 887us/sample - loss: 0.0313 - acc: 0.9894 - val_loss: 0.1382 - val_acc: 0.9702\n",
      "Epoch 109/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0347 - acc: 0.9880\n",
      "Epoch 00109: val_loss did not improve from 0.10812\n",
      "36805/36805 [==============================] - 33s 886us/sample - loss: 0.0347 - acc: 0.9880 - val_loss: 0.1353 - val_acc: 0.9686\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_multi_2_GMP_DO_7_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 456us/sample - loss: 0.1550 - acc: 0.9541\n",
      "Loss: 0.1550227659780567 Accuracy: 0.95410174\n",
      "\n",
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 1.8834 - acc: 0.3926\n",
      "Epoch 00001: val_loss improved from inf to 0.91575, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_8_conv_checkpoint/001-0.9157.hdf5\n",
      "36805/36805 [==============================] - 38s 1ms/sample - loss: 1.8834 - acc: 0.3926 - val_loss: 0.9157 - val_acc: 0.7049\n",
      "Epoch 2/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9510 - acc: 0.6966\n",
      "Epoch 00002: val_loss improved from 0.91575 to 0.56343, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_8_conv_checkpoint/002-0.5634.hdf5\n",
      "36805/36805 [==============================] - 33s 898us/sample - loss: 0.9510 - acc: 0.6966 - val_loss: 0.5634 - val_acc: 0.8355\n",
      "Epoch 3/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6785 - acc: 0.7852\n",
      "Epoch 00003: val_loss improved from 0.56343 to 0.41379, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_8_conv_checkpoint/003-0.4138.hdf5\n",
      "36805/36805 [==============================] - 33s 900us/sample - loss: 0.6784 - acc: 0.7852 - val_loss: 0.4138 - val_acc: 0.8782\n",
      "Epoch 4/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5549 - acc: 0.8254\n",
      "Epoch 00004: val_loss improved from 0.41379 to 0.33872, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_8_conv_checkpoint/004-0.3387.hdf5\n",
      "36805/36805 [==============================] - 33s 899us/sample - loss: 0.5549 - acc: 0.8254 - val_loss: 0.3387 - val_acc: 0.8945\n",
      "Epoch 5/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4802 - acc: 0.8487\n",
      "Epoch 00005: val_loss improved from 0.33872 to 0.31103, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_8_conv_checkpoint/005-0.3110.hdf5\n",
      "36805/36805 [==============================] - 33s 898us/sample - loss: 0.4803 - acc: 0.8487 - val_loss: 0.3110 - val_acc: 0.9038\n",
      "Epoch 6/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4220 - acc: 0.8660\n",
      "Epoch 00006: val_loss improved from 0.31103 to 0.26855, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_8_conv_checkpoint/006-0.2685.hdf5\n",
      "36805/36805 [==============================] - 33s 897us/sample - loss: 0.4220 - acc: 0.8659 - val_loss: 0.2685 - val_acc: 0.9199\n",
      "Epoch 7/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3814 - acc: 0.8800\n",
      "Epoch 00007: val_loss improved from 0.26855 to 0.24106, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_8_conv_checkpoint/007-0.2411.hdf5\n",
      "36805/36805 [==============================] - 33s 899us/sample - loss: 0.3813 - acc: 0.8800 - val_loss: 0.2411 - val_acc: 0.9273\n",
      "Epoch 8/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3438 - acc: 0.8910\n",
      "Epoch 00008: val_loss improved from 0.24106 to 0.23494, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_8_conv_checkpoint/008-0.2349.hdf5\n",
      "36805/36805 [==============================] - 33s 901us/sample - loss: 0.3438 - acc: 0.8910 - val_loss: 0.2349 - val_acc: 0.9259\n",
      "Epoch 9/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3206 - acc: 0.8987\n",
      "Epoch 00009: val_loss improved from 0.23494 to 0.20311, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_8_conv_checkpoint/009-0.2031.hdf5\n",
      "36805/36805 [==============================] - 33s 901us/sample - loss: 0.3205 - acc: 0.8987 - val_loss: 0.2031 - val_acc: 0.9334\n",
      "Epoch 10/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2957 - acc: 0.9063\n",
      "Epoch 00010: val_loss improved from 0.20311 to 0.18688, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_8_conv_checkpoint/010-0.1869.hdf5\n",
      "36805/36805 [==============================] - 33s 907us/sample - loss: 0.2956 - acc: 0.9063 - val_loss: 0.1869 - val_acc: 0.9406\n",
      "Epoch 11/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2759 - acc: 0.9134\n",
      "Epoch 00011: val_loss did not improve from 0.18688\n",
      "36805/36805 [==============================] - 33s 898us/sample - loss: 0.2758 - acc: 0.9134 - val_loss: 0.2001 - val_acc: 0.9348\n",
      "Epoch 12/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2595 - acc: 0.9187\n",
      "Epoch 00012: val_loss improved from 0.18688 to 0.17363, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_8_conv_checkpoint/012-0.1736.hdf5\n",
      "36805/36805 [==============================] - 33s 899us/sample - loss: 0.2595 - acc: 0.9187 - val_loss: 0.1736 - val_acc: 0.9471\n",
      "Epoch 13/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2442 - acc: 0.9214\n",
      "Epoch 00013: val_loss did not improve from 0.17363\n",
      "36805/36805 [==============================] - 33s 896us/sample - loss: 0.2441 - acc: 0.9214 - val_loss: 0.1761 - val_acc: 0.9474\n",
      "Epoch 14/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2328 - acc: 0.9254\n",
      "Epoch 00014: val_loss improved from 0.17363 to 0.15646, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_8_conv_checkpoint/014-0.1565.hdf5\n",
      "36805/36805 [==============================] - 33s 898us/sample - loss: 0.2328 - acc: 0.9254 - val_loss: 0.1565 - val_acc: 0.9488\n",
      "Epoch 15/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2190 - acc: 0.9299\n",
      "Epoch 00015: val_loss did not improve from 0.15646\n",
      "36805/36805 [==============================] - 33s 897us/sample - loss: 0.2190 - acc: 0.9299 - val_loss: 0.1710 - val_acc: 0.9434\n",
      "Epoch 16/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2081 - acc: 0.9323\n",
      "Epoch 00016: val_loss did not improve from 0.15646\n",
      "36805/36805 [==============================] - 33s 893us/sample - loss: 0.2080 - acc: 0.9323 - val_loss: 0.1582 - val_acc: 0.9490\n",
      "Epoch 17/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1999 - acc: 0.9357\n",
      "Epoch 00017: val_loss improved from 0.15646 to 0.14895, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_8_conv_checkpoint/017-0.1490.hdf5\n",
      "36805/36805 [==============================] - 33s 896us/sample - loss: 0.1999 - acc: 0.9356 - val_loss: 0.1490 - val_acc: 0.9525\n",
      "Epoch 18/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1897 - acc: 0.9397\n",
      "Epoch 00018: val_loss improved from 0.14895 to 0.14430, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_8_conv_checkpoint/018-0.1443.hdf5\n",
      "36805/36805 [==============================] - 33s 898us/sample - loss: 0.1897 - acc: 0.9397 - val_loss: 0.1443 - val_acc: 0.9567\n",
      "Epoch 19/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1790 - acc: 0.9418\n",
      "Epoch 00019: val_loss improved from 0.14430 to 0.14114, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_8_conv_checkpoint/019-0.1411.hdf5\n",
      "36805/36805 [==============================] - 33s 900us/sample - loss: 0.1790 - acc: 0.9418 - val_loss: 0.1411 - val_acc: 0.9550\n",
      "Epoch 20/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1707 - acc: 0.9445\n",
      "Epoch 00020: val_loss improved from 0.14114 to 0.13898, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_8_conv_checkpoint/020-0.1390.hdf5\n",
      "36805/36805 [==============================] - 33s 896us/sample - loss: 0.1707 - acc: 0.9445 - val_loss: 0.1390 - val_acc: 0.9578\n",
      "Epoch 21/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1679 - acc: 0.9455\n",
      "Epoch 00021: val_loss did not improve from 0.13898\n",
      "36805/36805 [==============================] - 33s 894us/sample - loss: 0.1678 - acc: 0.9455 - val_loss: 0.1447 - val_acc: 0.9529\n",
      "Epoch 22/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1535 - acc: 0.9502\n",
      "Epoch 00022: val_loss improved from 0.13898 to 0.13883, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_8_conv_checkpoint/022-0.1388.hdf5\n",
      "36805/36805 [==============================] - 33s 898us/sample - loss: 0.1536 - acc: 0.9502 - val_loss: 0.1388 - val_acc: 0.9557\n",
      "Epoch 23/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1506 - acc: 0.9515\n",
      "Epoch 00023: val_loss did not improve from 0.13883\n",
      "36805/36805 [==============================] - 33s 896us/sample - loss: 0.1505 - acc: 0.9516 - val_loss: 0.1502 - val_acc: 0.9541\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1469 - acc: 0.9526\n",
      "Epoch 00024: val_loss improved from 0.13883 to 0.13833, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_8_conv_checkpoint/024-0.1383.hdf5\n",
      "36805/36805 [==============================] - 33s 897us/sample - loss: 0.1469 - acc: 0.9526 - val_loss: 0.1383 - val_acc: 0.9571\n",
      "Epoch 25/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1385 - acc: 0.9554\n",
      "Epoch 00025: val_loss did not improve from 0.13833\n",
      "36805/36805 [==============================] - 33s 900us/sample - loss: 0.1385 - acc: 0.9554 - val_loss: 0.1404 - val_acc: 0.9560\n",
      "Epoch 26/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1344 - acc: 0.9560\n",
      "Epoch 00026: val_loss improved from 0.13833 to 0.12808, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_8_conv_checkpoint/026-0.1281.hdf5\n",
      "36805/36805 [==============================] - 33s 900us/sample - loss: 0.1344 - acc: 0.9560 - val_loss: 0.1281 - val_acc: 0.9604\n",
      "Epoch 27/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1290 - acc: 0.9570\n",
      "Epoch 00027: val_loss did not improve from 0.12808\n",
      "36805/36805 [==============================] - 33s 893us/sample - loss: 0.1290 - acc: 0.9570 - val_loss: 0.1351 - val_acc: 0.9557\n",
      "Epoch 28/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1271 - acc: 0.9583\n",
      "Epoch 00028: val_loss improved from 0.12808 to 0.12316, saving model to model/checkpoint/1D_CNN_custom_multi_2_GMP_DO_8_conv_checkpoint/028-0.1232.hdf5\n",
      "36805/36805 [==============================] - 33s 895us/sample - loss: 0.1271 - acc: 0.9583 - val_loss: 0.1232 - val_acc: 0.9599\n",
      "Epoch 29/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1166 - acc: 0.9616\n",
      "Epoch 00029: val_loss did not improve from 0.12316\n",
      "36805/36805 [==============================] - 33s 892us/sample - loss: 0.1166 - acc: 0.9616 - val_loss: 0.1353 - val_acc: 0.9569\n",
      "Epoch 30/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1102 - acc: 0.9635\n",
      "Epoch 00030: val_loss did not improve from 0.12316\n",
      "36805/36805 [==============================] - 33s 894us/sample - loss: 0.1102 - acc: 0.9635 - val_loss: 0.1313 - val_acc: 0.9616\n",
      "Epoch 31/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1082 - acc: 0.9644\n",
      "Epoch 00031: val_loss did not improve from 0.12316\n",
      "36805/36805 [==============================] - 33s 891us/sample - loss: 0.1082 - acc: 0.9644 - val_loss: 0.1353 - val_acc: 0.9599\n",
      "Epoch 32/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1030 - acc: 0.9666\n",
      "Epoch 00032: val_loss did not improve from 0.12316\n",
      "36805/36805 [==============================] - 33s 894us/sample - loss: 0.1030 - acc: 0.9666 - val_loss: 0.1318 - val_acc: 0.9588\n",
      "Epoch 33/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0987 - acc: 0.9667\n",
      "Epoch 00033: val_loss did not improve from 0.12316\n",
      "36805/36805 [==============================] - 33s 892us/sample - loss: 0.0987 - acc: 0.9667 - val_loss: 0.1355 - val_acc: 0.9611\n",
      "Epoch 34/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0954 - acc: 0.9689\n",
      "Epoch 00034: val_loss did not improve from 0.12316\n",
      "36805/36805 [==============================] - 33s 893us/sample - loss: 0.0953 - acc: 0.9689 - val_loss: 0.1498 - val_acc: 0.9583\n",
      "Epoch 35/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0963 - acc: 0.9685\n",
      "Epoch 00035: val_loss did not improve from 0.12316\n",
      "36805/36805 [==============================] - 33s 894us/sample - loss: 0.0963 - acc: 0.9685 - val_loss: 0.1271 - val_acc: 0.9637\n",
      "Epoch 36/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0850 - acc: 0.9723\n",
      "Epoch 00036: val_loss did not improve from 0.12316\n",
      "36805/36805 [==============================] - 33s 895us/sample - loss: 0.0850 - acc: 0.9723 - val_loss: 0.1392 - val_acc: 0.9604\n",
      "Epoch 37/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0887 - acc: 0.9704\n",
      "Epoch 00037: val_loss did not improve from 0.12316\n",
      "36805/36805 [==============================] - 33s 894us/sample - loss: 0.0887 - acc: 0.9704 - val_loss: 0.1372 - val_acc: 0.9583\n",
      "Epoch 38/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0837 - acc: 0.9722\n",
      "Epoch 00038: val_loss did not improve from 0.12316\n",
      "36805/36805 [==============================] - 33s 893us/sample - loss: 0.0837 - acc: 0.9722 - val_loss: 0.1336 - val_acc: 0.9606\n",
      "Epoch 39/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0789 - acc: 0.9740\n",
      "Epoch 00039: val_loss did not improve from 0.12316\n",
      "36805/36805 [==============================] - 33s 893us/sample - loss: 0.0789 - acc: 0.9740 - val_loss: 0.1672 - val_acc: 0.9539\n",
      "Epoch 40/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0799 - acc: 0.9735\n",
      "Epoch 00040: val_loss did not improve from 0.12316\n",
      "36805/36805 [==============================] - 33s 891us/sample - loss: 0.0799 - acc: 0.9735 - val_loss: 0.1311 - val_acc: 0.9627\n",
      "Epoch 41/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0726 - acc: 0.9758\n",
      "Epoch 00041: val_loss did not improve from 0.12316\n",
      "36805/36805 [==============================] - 33s 895us/sample - loss: 0.0726 - acc: 0.9758 - val_loss: 0.1644 - val_acc: 0.9564\n",
      "Epoch 42/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0749 - acc: 0.9750\n",
      "Epoch 00042: val_loss did not improve from 0.12316\n",
      "36805/36805 [==============================] - 33s 898us/sample - loss: 0.0749 - acc: 0.9750 - val_loss: 0.1324 - val_acc: 0.9611\n",
      "Epoch 43/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0731 - acc: 0.9750\n",
      "Epoch 00043: val_loss did not improve from 0.12316\n",
      "36805/36805 [==============================] - 33s 894us/sample - loss: 0.0731 - acc: 0.9750 - val_loss: 0.1340 - val_acc: 0.9616\n",
      "Epoch 44/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0677 - acc: 0.9771\n",
      "Epoch 00044: val_loss did not improve from 0.12316\n",
      "36805/36805 [==============================] - 33s 896us/sample - loss: 0.0677 - acc: 0.9771 - val_loss: 0.1336 - val_acc: 0.9606\n",
      "Epoch 45/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0684 - acc: 0.9778\n",
      "Epoch 00045: val_loss did not improve from 0.12316\n",
      "36805/36805 [==============================] - 33s 894us/sample - loss: 0.0684 - acc: 0.9778 - val_loss: 0.1361 - val_acc: 0.9637\n",
      "Epoch 46/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0617 - acc: 0.9795\n",
      "Epoch 00046: val_loss did not improve from 0.12316\n",
      "36805/36805 [==============================] - 33s 892us/sample - loss: 0.0618 - acc: 0.9794 - val_loss: 0.1485 - val_acc: 0.9616\n",
      "Epoch 47/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0622 - acc: 0.9791\n",
      "Epoch 00047: val_loss did not improve from 0.12316\n",
      "36805/36805 [==============================] - 33s 897us/sample - loss: 0.0622 - acc: 0.9791 - val_loss: 0.1348 - val_acc: 0.9630\n",
      "Epoch 48/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0576 - acc: 0.9798\n",
      "Epoch 00048: val_loss did not improve from 0.12316\n",
      "36805/36805 [==============================] - 33s 894us/sample - loss: 0.0576 - acc: 0.9798 - val_loss: 0.1438 - val_acc: 0.9613\n",
      "Epoch 49/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0589 - acc: 0.9796\n",
      "Epoch 00049: val_loss did not improve from 0.12316\n",
      "36805/36805 [==============================] - 33s 895us/sample - loss: 0.0589 - acc: 0.9796 - val_loss: 0.1459 - val_acc: 0.9620\n",
      "Epoch 50/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0559 - acc: 0.9811\n",
      "Epoch 00050: val_loss did not improve from 0.12316\n",
      "36805/36805 [==============================] - 33s 896us/sample - loss: 0.0559 - acc: 0.9811 - val_loss: 0.1438 - val_acc: 0.9620\n",
      "Epoch 51/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0530 - acc: 0.9815\n",
      "Epoch 00051: val_loss did not improve from 0.12316\n",
      "36805/36805 [==============================] - 33s 895us/sample - loss: 0.0530 - acc: 0.9815 - val_loss: 0.1407 - val_acc: 0.9641\n",
      "Epoch 52/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0567 - acc: 0.9807\n",
      "Epoch 00052: val_loss did not improve from 0.12316\n",
      "36805/36805 [==============================] - 33s 895us/sample - loss: 0.0567 - acc: 0.9807 - val_loss: 0.1424 - val_acc: 0.9634\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0510 - acc: 0.9835\n",
      "Epoch 00053: val_loss did not improve from 0.12316\n",
      "36805/36805 [==============================] - 33s 894us/sample - loss: 0.0510 - acc: 0.9835 - val_loss: 0.1426 - val_acc: 0.9620\n",
      "Epoch 54/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0494 - acc: 0.9830\n",
      "Epoch 00054: val_loss did not improve from 0.12316\n",
      "36805/36805 [==============================] - 33s 896us/sample - loss: 0.0494 - acc: 0.9830 - val_loss: 0.1478 - val_acc: 0.9627\n",
      "Epoch 55/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0464 - acc: 0.9850\n",
      "Epoch 00055: val_loss did not improve from 0.12316\n",
      "36805/36805 [==============================] - 33s 896us/sample - loss: 0.0464 - acc: 0.9850 - val_loss: 0.1604 - val_acc: 0.9611\n",
      "Epoch 56/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0470 - acc: 0.9836\n",
      "Epoch 00056: val_loss did not improve from 0.12316\n",
      "36805/36805 [==============================] - 33s 895us/sample - loss: 0.0470 - acc: 0.9836 - val_loss: 0.1529 - val_acc: 0.9634\n",
      "Epoch 57/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0485 - acc: 0.9838\n",
      "Epoch 00057: val_loss did not improve from 0.12316\n",
      "36805/36805 [==============================] - 33s 890us/sample - loss: 0.0485 - acc: 0.9838 - val_loss: 0.1582 - val_acc: 0.9623\n",
      "Epoch 58/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0418 - acc: 0.9859\n",
      "Epoch 00058: val_loss did not improve from 0.12316\n",
      "36805/36805 [==============================] - 33s 894us/sample - loss: 0.0418 - acc: 0.9859 - val_loss: 0.1497 - val_acc: 0.9639\n",
      "Epoch 59/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0444 - acc: 0.9848\n",
      "Epoch 00059: val_loss did not improve from 0.12316\n",
      "36805/36805 [==============================] - 33s 896us/sample - loss: 0.0444 - acc: 0.9848 - val_loss: 0.1640 - val_acc: 0.9604\n",
      "Epoch 60/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0450 - acc: 0.9854\n",
      "Epoch 00060: val_loss did not improve from 0.12316\n",
      "36805/36805 [==============================] - 33s 892us/sample - loss: 0.0450 - acc: 0.9854 - val_loss: 0.1585 - val_acc: 0.9604\n",
      "Epoch 61/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0448 - acc: 0.9848\n",
      "Epoch 00061: val_loss did not improve from 0.12316\n",
      "36805/36805 [==============================] - 33s 896us/sample - loss: 0.0448 - acc: 0.9848 - val_loss: 0.1627 - val_acc: 0.9564\n",
      "Epoch 62/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0388 - acc: 0.9868\n",
      "Epoch 00062: val_loss did not improve from 0.12316\n",
      "36805/36805 [==============================] - 33s 895us/sample - loss: 0.0389 - acc: 0.9867 - val_loss: 0.1548 - val_acc: 0.9644\n",
      "Epoch 63/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0394 - acc: 0.9868\n",
      "Epoch 00063: val_loss did not improve from 0.12316\n",
      "36805/36805 [==============================] - 33s 901us/sample - loss: 0.0394 - acc: 0.9868 - val_loss: 0.1573 - val_acc: 0.9625\n",
      "Epoch 64/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0431 - acc: 0.9856\n",
      "Epoch 00064: val_loss did not improve from 0.12316\n",
      "36805/36805 [==============================] - 33s 906us/sample - loss: 0.0431 - acc: 0.9856 - val_loss: 0.1613 - val_acc: 0.9616\n",
      "Epoch 65/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0374 - acc: 0.9875\n",
      "Epoch 00065: val_loss did not improve from 0.12316\n",
      "36805/36805 [==============================] - 33s 895us/sample - loss: 0.0374 - acc: 0.9875 - val_loss: 0.1574 - val_acc: 0.9623\n",
      "Epoch 66/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0363 - acc: 0.9877\n",
      "Epoch 00066: val_loss did not improve from 0.12316\n",
      "36805/36805 [==============================] - 33s 893us/sample - loss: 0.0363 - acc: 0.9877 - val_loss: 0.1674 - val_acc: 0.9611\n",
      "Epoch 67/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0407 - acc: 0.9866\n",
      "Epoch 00067: val_loss did not improve from 0.12316\n",
      "36805/36805 [==============================] - 33s 896us/sample - loss: 0.0407 - acc: 0.9866 - val_loss: 0.1641 - val_acc: 0.9639\n",
      "Epoch 68/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0317 - acc: 0.9891\n",
      "Epoch 00068: val_loss did not improve from 0.12316\n",
      "36805/36805 [==============================] - 33s 900us/sample - loss: 0.0317 - acc: 0.9891 - val_loss: 0.1644 - val_acc: 0.9627\n",
      "Epoch 69/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0348 - acc: 0.9887\n",
      "Epoch 00069: val_loss did not improve from 0.12316\n",
      "36805/36805 [==============================] - 33s 895us/sample - loss: 0.0348 - acc: 0.9887 - val_loss: 0.1496 - val_acc: 0.9639\n",
      "Epoch 70/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0325 - acc: 0.9886\n",
      "Epoch 00070: val_loss did not improve from 0.12316\n",
      "36805/36805 [==============================] - 33s 896us/sample - loss: 0.0325 - acc: 0.9886 - val_loss: 0.1617 - val_acc: 0.9651\n",
      "Epoch 71/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0349 - acc: 0.9881\n",
      "Epoch 00071: val_loss did not improve from 0.12316\n",
      "36805/36805 [==============================] - 33s 897us/sample - loss: 0.0349 - acc: 0.9881 - val_loss: 0.1714 - val_acc: 0.9639\n",
      "Epoch 72/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0339 - acc: 0.9886\n",
      "Epoch 00072: val_loss did not improve from 0.12316\n",
      "36805/36805 [==============================] - 33s 894us/sample - loss: 0.0339 - acc: 0.9886 - val_loss: 0.1567 - val_acc: 0.9665\n",
      "Epoch 73/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0314 - acc: 0.9898\n",
      "Epoch 00073: val_loss did not improve from 0.12316\n",
      "36805/36805 [==============================] - 33s 903us/sample - loss: 0.0314 - acc: 0.9898 - val_loss: 0.1675 - val_acc: 0.9634\n",
      "Epoch 74/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0299 - acc: 0.9899\n",
      "Epoch 00074: val_loss did not improve from 0.12316\n",
      "36805/36805 [==============================] - 33s 896us/sample - loss: 0.0299 - acc: 0.9899 - val_loss: 0.1715 - val_acc: 0.9637\n",
      "Epoch 75/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0325 - acc: 0.9893\n",
      "Epoch 00075: val_loss did not improve from 0.12316\n",
      "36805/36805 [==============================] - 33s 894us/sample - loss: 0.0325 - acc: 0.9893 - val_loss: 0.1701 - val_acc: 0.9639\n",
      "Epoch 76/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0323 - acc: 0.9896\n",
      "Epoch 00076: val_loss did not improve from 0.12316\n",
      "36805/36805 [==============================] - 33s 896us/sample - loss: 0.0323 - acc: 0.9896 - val_loss: 0.1746 - val_acc: 0.9618\n",
      "Epoch 77/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0280 - acc: 0.9912\n",
      "Epoch 00077: val_loss did not improve from 0.12316\n",
      "36805/36805 [==============================] - 33s 892us/sample - loss: 0.0280 - acc: 0.9912 - val_loss: 0.1815 - val_acc: 0.9632\n",
      "Epoch 78/500\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0331 - acc: 0.9892\n",
      "Epoch 00078: val_loss did not improve from 0.12316\n",
      "36805/36805 [==============================] - 33s 895us/sample - loss: 0.0331 - acc: 0.9892 - val_loss: 0.1509 - val_acc: 0.9646\n",
      "\n",
      "1D_CNN_custom_multi_2_GMP_DO_8_conv Model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 468us/sample - loss: 0.1815 - acc: 0.9429\n",
      "Loss: 0.18153870397267682 Accuracy: 0.9428868\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base = '1D_CNN_custom_multi_2_GMP_DO'\n",
    "\n",
    "for i in range(3, 9):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    model = build_cnn(conv_num=i)\n",
    "#         model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4),\n",
    "          metrics=['accuracy'])\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model_filename = model_path+'{epoch:03d}-{val_loss:.4f}.hdf5'\n",
    "    checkpointer = ModelCheckpoint(filepath = model_filename, monitor = \"val_loss\", \n",
    "                                   verbose=1, save_best_only=True)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=50)\n",
    "    hist = model.fit(x_train_abs, y_train_onehot, batch_size=64, epochs=500, \n",
    "                     validation_data=[x_val_abs, y_val_onehot], shuffle=True, \n",
    "                     callbacks = [checkpointer, early_stopping])\n",
    "\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "    ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "    ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "    ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    png_path = 'visualization/learning_curve/'\n",
    "    filename = model_name+'.png'\n",
    "    os.makedirs(png_path, exist_ok=True)\n",
    "    fig.savefig(png_path+filename, transparent=True)\n",
    "\n",
    "    model.save(model_path+'000_last.hdf5')\n",
    "    del(model)\n",
    "    \n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "    model = load_model(model_filename)\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "    print()\n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_multi_2_GMP_DO_3_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_33 (Conv1D)              (None, 16000, 64)    384         input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 16000, 64)    0           conv1d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling1D) (None, 5333, 64)     0           activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_34 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_33[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 5333, 64)     0           conv1d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling1D) (None, 1777, 64)     0           activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_35 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 1777, 64)     0           conv1d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling1D) (None, 592, 64)      0           activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_12 (Global (None, 64)           0           max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_13 (Global (None, 64)           0           max_pooling1d_35[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 128)          0           global_max_pooling1d_12[0][0]    \n",
      "                                                                 global_max_pooling1d_13[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 128)          0           concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 16)           2064        dropout_6[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 43,536\n",
      "Trainable params: 43,536\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 2s 434us/sample - loss: 0.6528 - acc: 0.8073\n",
      "Loss: 0.6527872024047907 Accuracy: 0.807269\n",
      "\n",
      "1D_CNN_custom_multi_2_GMP_DO_4_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_8 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_36 (Conv1D)              (None, 16000, 64)    384         input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 16000, 64)    0           conv1d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling1D) (None, 5333, 64)     0           activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_37 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_36[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 5333, 64)     0           conv1d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling1D) (None, 1777, 64)     0           activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_38 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_37[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 1777, 64)     0           conv1d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling1D) (None, 592, 64)      0           activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_39 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 592, 64)      0           conv1d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling1D) (None, 197, 64)      0           activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_14 (Global (None, 64)           0           max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_15 (Global (None, 64)           0           max_pooling1d_39[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 128)          0           global_max_pooling1d_14[0][0]    \n",
      "                                                                 global_max_pooling1d_15[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 128)          0           concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 16)           2064        dropout_7[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 64,080\n",
      "Trainable params: 64,080\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 2s 451us/sample - loss: 0.4098 - acc: 0.8737\n",
      "Loss: 0.40975068577972407 Accuracy: 0.8737279\n",
      "\n",
      "1D_CNN_custom_multi_2_GMP_DO_5_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_40 (Conv1D)              (None, 16000, 64)    384         input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 16000, 64)    0           conv1d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling1D) (None, 5333, 64)     0           activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_41 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_40[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 5333, 64)     0           conv1d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling1D) (None, 1777, 64)     0           activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_42 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_41[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 1777, 64)     0           conv1d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling1D) (None, 592, 64)      0           activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_43 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_42[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 592, 64)      0           conv1d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling1D) (None, 197, 64)      0           activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_44 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 197, 128)     0           conv1d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling1D) (None, 65, 128)      0           activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_16 (Global (None, 64)           0           max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_17 (Global (None, 128)          0           max_pooling1d_44[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 192)          0           global_max_pooling1d_16[0][0]    \n",
      "                                                                 global_max_pooling1d_17[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 192)          0           concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 16)           3088        dropout_8[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 106,192\n",
      "Trainable params: 106,192\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 472us/sample - loss: 0.2356 - acc: 0.9290\n",
      "Loss: 0.23562480327620067 Accuracy: 0.92897195\n",
      "\n",
      "1D_CNN_custom_multi_2_GMP_DO_6_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_10 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_45 (Conv1D)              (None, 16000, 64)    384         input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 16000, 64)    0           conv1d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling1D) (None, 5333, 64)     0           activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_46 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_45[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 5333, 64)     0           conv1d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling1D) (None, 1777, 64)     0           activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_47 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_46[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 1777, 64)     0           conv1d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling1D) (None, 592, 64)      0           activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_48 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_47[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 592, 64)      0           conv1d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling1D) (None, 197, 64)      0           activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_49 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_48[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 197, 128)     0           conv1d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling1D) (None, 65, 128)      0           activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_50 (Conv1D)              (None, 65, 128)      82048       max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 65, 128)      0           conv1d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling1D) (None, 21, 128)      0           activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_18 (Global (None, 128)          0           max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_19 (Global (None, 128)          0           max_pooling1d_50[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 256)          0           global_max_pooling1d_18[0][0]    \n",
      "                                                                 global_max_pooling1d_19[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 256)          0           concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 16)           4112        dropout_9[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 189,264\n",
      "Trainable params: 189,264\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 2s 490us/sample - loss: 0.1583 - acc: 0.9516\n",
      "Loss: 0.1583239990974141 Accuracy: 0.95160955\n",
      "\n",
      "1D_CNN_custom_multi_2_GMP_DO_7_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_51 (Conv1D)              (None, 16000, 64)    384         input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 16000, 64)    0           conv1d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling1D) (None, 5333, 64)     0           activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_52 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_51[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 5333, 64)     0           conv1d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling1D) (None, 1777, 64)     0           activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_53 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_52[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 1777, 64)     0           conv1d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling1D) (None, 592, 64)      0           activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_54 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_53[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 592, 64)      0           conv1d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling1D) (None, 197, 64)      0           activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_55 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_54[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 197, 128)     0           conv1d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling1D) (None, 65, 128)      0           activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_56 (Conv1D)              (None, 65, 128)      82048       max_pooling1d_55[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 65, 128)      0           conv1d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling1D) (None, 21, 128)      0           activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_57 (Conv1D)              (None, 21, 128)      82048       max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 21, 128)      0           conv1d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling1D) (None, 7, 128)       0           activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_20 (Global (None, 128)          0           max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_21 (Global (None, 128)          0           max_pooling1d_57[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 256)          0           global_max_pooling1d_20[0][0]    \n",
      "                                                                 global_max_pooling1d_21[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 256)          0           concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 16)           4112        dropout_10[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 271,312\n",
      "Trainable params: 271,312\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 2s 510us/sample - loss: 0.1550 - acc: 0.9541\n",
      "Loss: 0.1550227659780567 Accuracy: 0.95410174\n",
      "\n",
      "1D_CNN_custom_multi_2_GMP_DO_8_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_12 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_58 (Conv1D)              (None, 16000, 64)    384         input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 16000, 64)    0           conv1d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling1D) (None, 5333, 64)     0           activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_59 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_58[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 5333, 64)     0           conv1d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling1D) (None, 1777, 64)     0           activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_60 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_59[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 1777, 64)     0           conv1d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling1D) (None, 592, 64)      0           activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_61 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_60[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 592, 64)      0           conv1d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling1D) (None, 197, 64)      0           activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_62 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_61[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 197, 128)     0           conv1d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling1D) (None, 65, 128)      0           activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_63 (Conv1D)              (None, 65, 128)      82048       max_pooling1d_62[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 65, 128)      0           conv1d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling1D) (None, 21, 128)      0           activation_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_64 (Conv1D)              (None, 21, 128)      82048       max_pooling1d_63[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 21, 128)      0           conv1d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling1D) (None, 7, 128)       0           activation_64[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_65 (Conv1D)              (None, 7, 128)       82048       max_pooling1d_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 7, 128)       0           conv1d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling1D) (None, 2, 128)       0           activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_22 (Global (None, 128)          0           max_pooling1d_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_23 (Global (None, 128)          0           max_pooling1d_65[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 256)          0           global_max_pooling1d_22[0][0]    \n",
      "                                                                 global_max_pooling1d_23[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 256)          0           concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 16)           4112        dropout_11[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 353,360\n",
      "Trainable params: 353,360\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 521us/sample - loss: 0.1815 - acc: 0.9429\n",
      "Loss: 0.18153870397267682 Accuracy: 0.9428868\n"
     ]
    }
   ],
   "source": [
    "log_dir = 'log'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "base = '1D_CNN_custom_multi_2_GMP_DO'\n",
    "\n",
    "with open(path.join(log_dir, base), 'w') as log_file:\n",
    "    for i in range(3, 9):\n",
    "        model_name = base+'_{}_conv'.format(i)\n",
    "        print()\n",
    "        print(model_name, 'Model')\n",
    "        model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "        model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "\n",
    "        model = load_model(model_filename)\n",
    "        model.summary()\n",
    "\n",
    "        [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "        print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "        del(model)\n",
    "\n",
    "        log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D_CNN_custom_multi_2_GMP_DO_3_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_33 (Conv1D)              (None, 16000, 64)    384         input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 16000, 64)    0           conv1d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling1D) (None, 5333, 64)     0           activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_34 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_33[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 5333, 64)     0           conv1d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling1D) (None, 1777, 64)     0           activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_35 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 1777, 64)     0           conv1d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling1D) (None, 592, 64)      0           activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_12 (Global (None, 64)           0           max_pooling1d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_13 (Global (None, 64)           0           max_pooling1d_35[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 128)          0           global_max_pooling1d_12[0][0]    \n",
      "                                                                 global_max_pooling1d_13[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 128)          0           concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 16)           2064        dropout_6[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 43,536\n",
      "Trainable params: 43,536\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 2s 484us/sample - loss: 0.6510 - acc: 0.8081\n",
      "Loss: 0.6510124325380889 Accuracy: 0.8080997\n",
      "\n",
      "1D_CNN_custom_multi_2_GMP_DO_4_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_8 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_36 (Conv1D)              (None, 16000, 64)    384         input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 16000, 64)    0           conv1d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling1D) (None, 5333, 64)     0           activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_37 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_36[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 5333, 64)     0           conv1d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling1D) (None, 1777, 64)     0           activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_38 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_37[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 1777, 64)     0           conv1d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling1D) (None, 592, 64)      0           activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_39 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 592, 64)      0           conv1d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling1D) (None, 197, 64)      0           activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_14 (Global (None, 64)           0           max_pooling1d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_15 (Global (None, 64)           0           max_pooling1d_39[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 128)          0           global_max_pooling1d_14[0][0]    \n",
      "                                                                 global_max_pooling1d_15[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 128)          0           concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 16)           2064        dropout_7[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 64,080\n",
      "Trainable params: 64,080\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 2s 505us/sample - loss: 0.4293 - acc: 0.8679\n",
      "Loss: 0.42927111028263254 Accuracy: 0.86791277\n",
      "\n",
      "1D_CNN_custom_multi_2_GMP_DO_5_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_40 (Conv1D)              (None, 16000, 64)    384         input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 16000, 64)    0           conv1d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling1D) (None, 5333, 64)     0           activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_41 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_40[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 5333, 64)     0           conv1d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling1D) (None, 1777, 64)     0           activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_42 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_41[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 1777, 64)     0           conv1d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling1D) (None, 592, 64)      0           activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_43 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_42[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 592, 64)      0           conv1d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling1D) (None, 197, 64)      0           activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_44 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 197, 128)     0           conv1d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling1D) (None, 65, 128)      0           activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_16 (Global (None, 64)           0           max_pooling1d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_17 (Global (None, 128)          0           max_pooling1d_44[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 192)          0           global_max_pooling1d_16[0][0]    \n",
      "                                                                 global_max_pooling1d_17[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 192)          0           concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 16)           3088        dropout_8[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 106,192\n",
      "Trainable params: 106,192\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 529us/sample - loss: 0.2519 - acc: 0.9254\n",
      "Loss: 0.25190773231837116 Accuracy: 0.9254413\n",
      "\n",
      "1D_CNN_custom_multi_2_GMP_DO_6_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_10 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_45 (Conv1D)              (None, 16000, 64)    384         input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 16000, 64)    0           conv1d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling1D) (None, 5333, 64)     0           activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_46 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_45[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 5333, 64)     0           conv1d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling1D) (None, 1777, 64)     0           activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_47 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_46[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 1777, 64)     0           conv1d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling1D) (None, 592, 64)      0           activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_48 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_47[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 592, 64)      0           conv1d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling1D) (None, 197, 64)      0           activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_49 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_48[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 197, 128)     0           conv1d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling1D) (None, 65, 128)      0           activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_50 (Conv1D)              (None, 65, 128)      82048       max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 65, 128)      0           conv1d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling1D) (None, 21, 128)      0           activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_18 (Global (None, 128)          0           max_pooling1d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_19 (Global (None, 128)          0           max_pooling1d_50[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 256)          0           global_max_pooling1d_18[0][0]    \n",
      "                                                                 global_max_pooling1d_19[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 256)          0           concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 16)           4112        dropout_9[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 189,264\n",
      "Trainable params: 189,264\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 543us/sample - loss: 0.1840 - acc: 0.9522\n",
      "Loss: 0.1839770991413207 Accuracy: 0.9522326\n",
      "\n",
      "1D_CNN_custom_multi_2_GMP_DO_7_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_51 (Conv1D)              (None, 16000, 64)    384         input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 16000, 64)    0           conv1d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling1D) (None, 5333, 64)     0           activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_52 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_51[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 5333, 64)     0           conv1d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling1D) (None, 1777, 64)     0           activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_53 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_52[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 1777, 64)     0           conv1d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling1D) (None, 592, 64)      0           activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_54 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_53[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 592, 64)      0           conv1d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling1D) (None, 197, 64)      0           activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_55 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_54[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 197, 128)     0           conv1d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling1D) (None, 65, 128)      0           activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_56 (Conv1D)              (None, 65, 128)      82048       max_pooling1d_55[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 65, 128)      0           conv1d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling1D) (None, 21, 128)      0           activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_57 (Conv1D)              (None, 21, 128)      82048       max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 21, 128)      0           conv1d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling1D) (None, 7, 128)       0           activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_20 (Global (None, 128)          0           max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_21 (Global (None, 128)          0           max_pooling1d_57[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 256)          0           global_max_pooling1d_20[0][0]    \n",
      "                                                                 global_max_pooling1d_21[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 256)          0           concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 16)           4112        dropout_10[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 271,312\n",
      "Trainable params: 271,312\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 3s 551us/sample - loss: 0.1815 - acc: 0.9580\n",
      "Loss: 0.18145450511492733 Accuracy: 0.95804775\n",
      "\n",
      "1D_CNN_custom_multi_2_GMP_DO_8_conv Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_12 (InputLayer)           (None, 16000, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_58 (Conv1D)              (None, 16000, 64)    384         input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 16000, 64)    0           conv1d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling1D) (None, 5333, 64)     0           activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_59 (Conv1D)              (None, 5333, 64)     20544       max_pooling1d_58[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 5333, 64)     0           conv1d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_59 (MaxPooling1D) (None, 1777, 64)     0           activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_60 (Conv1D)              (None, 1777, 64)     20544       max_pooling1d_59[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 1777, 64)     0           conv1d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling1D) (None, 592, 64)      0           activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_61 (Conv1D)              (None, 592, 64)      20544       max_pooling1d_60[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 592, 64)      0           conv1d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling1D) (None, 197, 64)      0           activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_62 (Conv1D)              (None, 197, 128)     41088       max_pooling1d_61[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 197, 128)     0           conv1d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling1D) (None, 65, 128)      0           activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_63 (Conv1D)              (None, 65, 128)      82048       max_pooling1d_62[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 65, 128)      0           conv1d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling1D) (None, 21, 128)      0           activation_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_64 (Conv1D)              (None, 21, 128)      82048       max_pooling1d_63[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 21, 128)      0           conv1d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_64 (MaxPooling1D) (None, 7, 128)       0           activation_64[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_65 (Conv1D)              (None, 7, 128)       82048       max_pooling1d_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 7, 128)       0           conv1d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_65 (MaxPooling1D) (None, 2, 128)       0           activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_22 (Global (None, 128)          0           max_pooling1d_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_23 (Global (None, 128)          0           max_pooling1d_65[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 256)          0           global_max_pooling1d_22[0][0]    \n",
      "                                                                 global_max_pooling1d_23[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 256)          0           concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 16)           4112        dropout_11[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 353,360\n",
      "Trainable params: 353,360\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "4815/4815 [==============================] - 3s 575us/sample - loss: 0.2285 - acc: 0.9510\n",
      "Loss: 0.22845164054305014 Accuracy: 0.9509865\n"
     ]
    }
   ],
   "source": [
    "# log_dir = 'log'\n",
    "# os.makedirs(log_dir, exist_ok=True)\n",
    "# base = '1D_CNN_custom_DO'\n",
    "\n",
    "# with open(path.join(log_dir, base), 'w') as log_file:\n",
    "for i in range(3, 9):\n",
    "    model_name = base+'_{}_conv'.format(i)\n",
    "    print()\n",
    "    print(model_name, 'Model')\n",
    "    model_path = 'model/checkpoint/'+model_name+'_checkpoint/'\n",
    "    model_filename = model_path + '000_last.hdf5'\n",
    "\n",
    "    model = load_model(model_filename)\n",
    "    model.summary()\n",
    "\n",
    "    [loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "    print('Loss:', loss, 'Accuracy:', accuracy)\n",
    "\n",
    "    del(model)\n",
    "\n",
    "#         log_file.write('\\t'.join([model_name, str(accuracy), str(loss)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
