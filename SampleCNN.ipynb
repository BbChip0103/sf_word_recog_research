{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uN7hQRZsDbgI"
   },
   "source": [
    "(1) Importing dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "3lPxjI5BDAkX",
    "outputId": "88280284-3c51-485b-adfa-4c428507fb92"
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import maxabs_scale\n",
    "\n",
    "import librosa\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import os\n",
    "import os.path as path\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(13)\n",
    "import random\n",
    "random.seed(13)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Input,InputLayer, Dense, Activation, \\\n",
    "                                    BatchNormalization, Flatten, Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D, Dropout\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler, \\\n",
    "                                        EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "py5KMVLnDZsC"
   },
   "source": [
    "(2) Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = 'data'\n",
    "data_dir = path.join(base_dir, 'data_speech_commands_v0.02')\n",
    " \n",
    "train_txt = path.join(data_dir, 'wav_train_16words.txt')\n",
    "val_txt = path.join(data_dir, 'wav_validation_16words.txt')\n",
    "test_txt = path.join(data_dir, 'wav_test_16words.txt')\n",
    "\n",
    "train_data = np.load(path.join(data_dir, 'wav_train_data.npz'))\n",
    "val_data = np.load(path.join(data_dir, 'wav_validation_data.npz'))\n",
    "test_data = np.load(path.join(data_dir, 'wav_test_data.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36805, 16000, 1),\n",
       " (36805,),\n",
       " (4293, 16000, 1),\n",
       " (4293,),\n",
       " (4815, 16000, 1),\n",
       " (4815,),\n",
       " (16, 2))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = train_data['x_train']\n",
    "y_train = train_data['y_train']\n",
    "x_val = val_data['x_val']\n",
    "y_val = val_data['y_val']\n",
    "x_test = test_data['x_test']\n",
    "y_test = test_data['y_test']\n",
    "y_table = test_data['table']\n",
    "\n",
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape, y_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_test[0].shape\n",
    "output_size = y_table.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_abs = np.asarray([maxabs_scale(wav) for wav in x_train])\n",
    "y_train_onehot = np.asarray([to_categorical(label, output_size) for label in y_train])\n",
    "del x_train, y_train\n",
    "\n",
    "x_val_abs = np.asarray([maxabs_scale(wav) for wav in x_val])\n",
    "y_val_onehot = np.asarray([to_categorical(label, output_size) for label in y_val])\n",
    "del x_val, y_val\n",
    "\n",
    "x_test_abs = np.asarray([maxabs_scale(wav) for wav in x_test])\n",
    "y_test_onehot = np.asarray([to_categorical(label, output_size) for label in y_test])\n",
    "del x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(wav):\n",
    "    wav = sklearn.preprocessing.maxabs_scale(wav)\n",
    "    wav_mfcc = librosa.feature.mfcc(y=wav, n_mfcc=13)\n",
    "    wav_mfcc_std = StandardScaler().fit_transform(wav_mfcc)\n",
    "    wav_mfcc_std_mean = wav_mfcc_std.mean(axis=1)\n",
    "\n",
    "    features = np.concatenate([wav_mfcc_std_mean])\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "12cS85jvDnfS"
   },
   "source": [
    "(3) Create a sequential model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 985
    },
    "colab_type": "code",
    "id": "fs8Heys2Dm30",
    "outputId": "bad14ede-be9c-4a2f-d052-9d9a29a5e437"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 5333, 128)         512       \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 5333, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 5333, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 5333, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 5333, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 1777, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 1777, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 1777, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 592, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 592, 256)          98560     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 592, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 592, 256)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 197, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 197, 256)          196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 197, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 197, 256)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 65, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 65, 256)           196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 65, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 65, 256)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 21, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 21, 256)           196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 21, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 21, 256)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 7, 256)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 7, 256)            196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 7, 256)            1024      \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 7, 256)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 2, 256)            0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 2, 256)            0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                8208      \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 16)                0         \n",
      "=================================================================\n",
      "Total params: 999,952\n",
      "Trainable params: 996,624\n",
      "Non-trainable params: 3,328\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model=Sequential()\n",
    "\n",
    "# Layer 1\n",
    "model.add(Conv1D (kernel_size=3, filters=128, strides=3, padding='valid',\n",
    "                  kernel_initializer='he_uniform', input_shape=input_shape))                  \n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# Layer 2\n",
    "model.add(Conv1D (kernel_size=3, filters=128, padding='same', kernel_initializer='he_uniform'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling1D(pool_size=3, strides=3))\n",
    "\n",
    "# Layer 3\n",
    "model.add(Conv1D (kernel_size=3, filters=128, padding='same', kernel_initializer='he_uniform'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling1D(pool_size=3, strides=3))\n",
    "\n",
    "# Layer 4\n",
    "model.add(Conv1D (kernel_size=3, filters=256, padding='same', kernel_initializer='he_uniform'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling1D(pool_size=3, strides=3))\n",
    "\n",
    "# Layer 5\n",
    "model.add(Conv1D (kernel_size=3, filters=256, padding='same', kernel_initializer='he_uniform'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling1D(pool_size=3, strides=3))\n",
    "\n",
    "# Layer 6\n",
    "model.add(Conv1D (kernel_size=3, filters=256, padding='same', kernel_initializer='he_uniform'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling1D(pool_size=3, strides=3))\n",
    "\n",
    "# Layer 7\n",
    "model.add(Conv1D (kernel_size=3, filters=256, padding='same', kernel_initializer='he_uniform'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling1D(pool_size=3, strides=3))\n",
    "\n",
    "# Layer 8\n",
    "model.add(Conv1D (kernel_size=3, filters=256, padding='same', kernel_initializer='he_uniform'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling1D(pool_size=3, strides=3))\n",
    "\n",
    "# # Layer 9\n",
    "# model.add(Conv1D (kernel_size=3, filters=256, padding='same', kernel_initializer='he_uniform'))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(MaxPooling1D(pool_size=3, strides=3))\n",
    "\n",
    "# # Layer 10\n",
    "# model.add(Conv1D (kernel_size=3, filters=512, padding='same', kernel_initializer='he_uniform'))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(MaxPooling1D(pool_size=3, strides=3))\n",
    "\n",
    "# Layer 11\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Flatten())\n",
    "\n",
    "# Layer 12\n",
    "model.add(Dense(output_size))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RLxfqHNxDuJq"
   },
   "source": [
    "(4) Compile "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5jPB8IbZDxeJ"
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer=Adam(lr=1e-4),\n",
    "#               optimizer=SGD(lr=0.01, momentum=0.9, decay=1e-6, nesterov=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VUsuRj-7Dzxx"
   },
   "source": [
    "(5) Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'model/checkpoint/SampleCNN_8_conv_checkpoint/'\n",
    "os.makedirs(model_path, exist_ok=True)\n",
    "model_filename = model_path+'{epoch:03d}-{val_loss:.4f}.hdf5'\n",
    "checkpointer = ModelCheckpoint(filepath = model_filename, monitor = \"val_loss\", verbose=1, save_best_only=True)\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 404
    },
    "colab_type": "code",
    "id": "ZUVV71K2D2tZ",
    "outputId": "7a454152-003e-4615-acd8-cfdb60ef8170",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36805 samples, validate on 4293 samples\n",
      "Epoch 1/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 2.0131 - acc: 0.4151\n",
      "Epoch 00001: val_loss improved from inf to 0.90185, saving model to model/checkpoint/SampleCNN_8_conv_checkpoint/001-0.9019.hdf5\n",
      "36805/36805 [==============================] - 132s 4ms/step - loss: 2.0130 - acc: 0.4152 - val_loss: 0.9019 - val_acc: 0.7349\n",
      "Epoch 2/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.9675 - acc: 0.6957\n",
      "Epoch 00002: val_loss improved from 0.90185 to 0.57968, saving model to model/checkpoint/SampleCNN_8_conv_checkpoint/002-0.5797.hdf5\n",
      "36805/36805 [==============================] - 128s 3ms/step - loss: 0.9674 - acc: 0.6957 - val_loss: 0.5797 - val_acc: 0.8283\n",
      "Epoch 3/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.6600 - acc: 0.7941\n",
      "Epoch 00003: val_loss improved from 0.57968 to 0.37485, saving model to model/checkpoint/SampleCNN_8_conv_checkpoint/003-0.3749.hdf5\n",
      "36805/36805 [==============================] - 127s 3ms/step - loss: 0.6602 - acc: 0.7940 - val_loss: 0.3749 - val_acc: 0.8905\n",
      "Epoch 4/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.5168 - acc: 0.8404\n",
      "Epoch 00004: val_loss improved from 0.37485 to 0.31874, saving model to model/checkpoint/SampleCNN_8_conv_checkpoint/004-0.3187.hdf5\n",
      "36805/36805 [==============================] - 128s 3ms/step - loss: 0.5169 - acc: 0.8403 - val_loss: 0.3187 - val_acc: 0.9045\n",
      "Epoch 5/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.4205 - acc: 0.8692\n",
      "Epoch 00005: val_loss improved from 0.31874 to 0.27969, saving model to model/checkpoint/SampleCNN_8_conv_checkpoint/005-0.2797.hdf5\n",
      "36805/36805 [==============================] - 128s 3ms/step - loss: 0.4205 - acc: 0.8692 - val_loss: 0.2797 - val_acc: 0.9173\n",
      "Epoch 6/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3613 - acc: 0.8889\n",
      "Epoch 00006: val_loss improved from 0.27969 to 0.23128, saving model to model/checkpoint/SampleCNN_8_conv_checkpoint/006-0.2313.hdf5\n",
      "36805/36805 [==============================] - 127s 3ms/step - loss: 0.3612 - acc: 0.8890 - val_loss: 0.2313 - val_acc: 0.9331\n",
      "Epoch 7/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.3147 - acc: 0.9018\n",
      "Epoch 00007: val_loss improved from 0.23128 to 0.21529, saving model to model/checkpoint/SampleCNN_8_conv_checkpoint/007-0.2153.hdf5\n",
      "36805/36805 [==============================] - 127s 3ms/step - loss: 0.3147 - acc: 0.9019 - val_loss: 0.2153 - val_acc: 0.9380\n",
      "Epoch 8/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2715 - acc: 0.9150\n",
      "Epoch 00008: val_loss improved from 0.21529 to 0.20910, saving model to model/checkpoint/SampleCNN_8_conv_checkpoint/008-0.2091.hdf5\n",
      "36805/36805 [==============================] - 128s 3ms/step - loss: 0.2715 - acc: 0.9150 - val_loss: 0.2091 - val_acc: 0.9399\n",
      "Epoch 9/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2468 - acc: 0.9230\n",
      "Epoch 00009: val_loss did not improve from 0.20910\n",
      "36805/36805 [==============================] - 128s 3ms/step - loss: 0.2468 - acc: 0.9230 - val_loss: 0.3110 - val_acc: 0.9092\n",
      "Epoch 10/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.2235 - acc: 0.9292\n",
      "Epoch 00010: val_loss improved from 0.20910 to 0.18632, saving model to model/checkpoint/SampleCNN_8_conv_checkpoint/010-0.1863.hdf5\n",
      "36805/36805 [==============================] - 129s 3ms/step - loss: 0.2235 - acc: 0.9292 - val_loss: 0.1863 - val_acc: 0.9464\n",
      "Epoch 11/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1964 - acc: 0.9396\n",
      "Epoch 00011: val_loss improved from 0.18632 to 0.18043, saving model to model/checkpoint/SampleCNN_8_conv_checkpoint/011-0.1804.hdf5\n",
      "36805/36805 [==============================] - 129s 4ms/step - loss: 0.1964 - acc: 0.9396 - val_loss: 0.1804 - val_acc: 0.9455\n",
      "Epoch 12/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1776 - acc: 0.9440\n",
      "Epoch 00012: val_loss did not improve from 0.18043\n",
      "36805/36805 [==============================] - 130s 4ms/step - loss: 0.1776 - acc: 0.9440 - val_loss: 0.1950 - val_acc: 0.9427\n",
      "Epoch 13/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1640 - acc: 0.9471\n",
      "Epoch 00013: val_loss did not improve from 0.18043\n",
      "36805/36805 [==============================] - 129s 4ms/step - loss: 0.1641 - acc: 0.9470 - val_loss: 0.2191 - val_acc: 0.9350\n",
      "Epoch 14/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1516 - acc: 0.9518\n",
      "Epoch 00014: val_loss improved from 0.18043 to 0.17568, saving model to model/checkpoint/SampleCNN_8_conv_checkpoint/014-0.1757.hdf5\n",
      "36805/36805 [==============================] - 127s 3ms/step - loss: 0.1516 - acc: 0.9518 - val_loss: 0.1757 - val_acc: 0.9469\n",
      "Epoch 15/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1345 - acc: 0.9583\n",
      "Epoch 00015: val_loss improved from 0.17568 to 0.16389, saving model to model/checkpoint/SampleCNN_8_conv_checkpoint/015-0.1639.hdf5\n",
      "36805/36805 [==============================] - 129s 3ms/step - loss: 0.1345 - acc: 0.9583 - val_loss: 0.1639 - val_acc: 0.9497\n",
      "Epoch 16/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1311 - acc: 0.9599\n",
      "Epoch 00016: val_loss did not improve from 0.16389\n",
      "36805/36805 [==============================] - 128s 3ms/step - loss: 0.1311 - acc: 0.9599 - val_loss: 0.1883 - val_acc: 0.9436\n",
      "Epoch 17/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1159 - acc: 0.9639\n",
      "Epoch 00017: val_loss improved from 0.16389 to 0.15814, saving model to model/checkpoint/SampleCNN_8_conv_checkpoint/017-0.1581.hdf5\n",
      "36805/36805 [==============================] - 128s 3ms/step - loss: 0.1159 - acc: 0.9639 - val_loss: 0.1581 - val_acc: 0.9518\n",
      "Epoch 18/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.1065 - acc: 0.9677\n",
      "Epoch 00018: val_loss did not improve from 0.15814\n",
      "36805/36805 [==============================] - 127s 3ms/step - loss: 0.1066 - acc: 0.9677 - val_loss: 0.1936 - val_acc: 0.9448\n",
      "Epoch 19/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0994 - acc: 0.9690\n",
      "Epoch 00019: val_loss did not improve from 0.15814\n",
      "36805/36805 [==============================] - 126s 3ms/step - loss: 0.0994 - acc: 0.9690 - val_loss: 0.1753 - val_acc: 0.9504\n",
      "Epoch 20/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0922 - acc: 0.9713\n",
      "Epoch 00020: val_loss improved from 0.15814 to 0.15671, saving model to model/checkpoint/SampleCNN_8_conv_checkpoint/020-0.1567.hdf5\n",
      "36805/36805 [==============================] - 128s 3ms/step - loss: 0.0922 - acc: 0.9713 - val_loss: 0.1567 - val_acc: 0.9553\n",
      "Epoch 21/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0802 - acc: 0.9759\n",
      "Epoch 00021: val_loss did not improve from 0.15671\n",
      "36805/36805 [==============================] - 128s 3ms/step - loss: 0.0802 - acc: 0.9759 - val_loss: 0.1905 - val_acc: 0.9425\n",
      "Epoch 22/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0805 - acc: 0.9758\n",
      "Epoch 00022: val_loss did not improve from 0.15671\n",
      "36805/36805 [==============================] - 128s 3ms/step - loss: 0.0807 - acc: 0.9758 - val_loss: 0.2018 - val_acc: 0.9443\n",
      "Epoch 23/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0842 - acc: 0.9733\n",
      "Epoch 00023: val_loss improved from 0.15671 to 0.15410, saving model to model/checkpoint/SampleCNN_8_conv_checkpoint/023-0.1541.hdf5\n",
      "36805/36805 [==============================] - 127s 3ms/step - loss: 0.0842 - acc: 0.9733 - val_loss: 0.1541 - val_acc: 0.9543\n",
      "Epoch 24/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0647 - acc: 0.9800\n",
      "Epoch 00024: val_loss did not improve from 0.15410\n",
      "36805/36805 [==============================] - 127s 3ms/step - loss: 0.0648 - acc: 0.9800 - val_loss: 0.1607 - val_acc: 0.9553\n",
      "Epoch 25/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0646 - acc: 0.9798\n",
      "Epoch 00025: val_loss did not improve from 0.15410\n",
      "36805/36805 [==============================] - 128s 3ms/step - loss: 0.0646 - acc: 0.9798 - val_loss: 0.1779 - val_acc: 0.9499\n",
      "Epoch 26/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0544 - acc: 0.9839\n",
      "Epoch 00026: val_loss did not improve from 0.15410\n",
      "36805/36805 [==============================] - 127s 3ms/step - loss: 0.0544 - acc: 0.9839 - val_loss: 0.2095 - val_acc: 0.9427\n",
      "Epoch 27/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0568 - acc: 0.9829\n",
      "Epoch 00027: val_loss did not improve from 0.15410\n",
      "36805/36805 [==============================] - 128s 3ms/step - loss: 0.0569 - acc: 0.9829 - val_loss: 0.1898 - val_acc: 0.9450\n",
      "Epoch 28/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0703 - acc: 0.9777\n",
      "Epoch 00028: val_loss did not improve from 0.15410\n",
      "36805/36805 [==============================] - 129s 3ms/step - loss: 0.0703 - acc: 0.9777 - val_loss: 0.1661 - val_acc: 0.9562\n",
      "Epoch 29/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0470 - acc: 0.9864\n",
      "Epoch 00029: val_loss did not improve from 0.15410\n",
      "36805/36805 [==============================] - 130s 4ms/step - loss: 0.0470 - acc: 0.9864 - val_loss: 0.1763 - val_acc: 0.9520\n",
      "Epoch 30/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0435 - acc: 0.9879\n",
      "Epoch 00030: val_loss did not improve from 0.15410\n",
      "36805/36805 [==============================] - 129s 4ms/step - loss: 0.0435 - acc: 0.9879 - val_loss: 0.1629 - val_acc: 0.9543\n",
      "Epoch 31/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0407 - acc: 0.9883\n",
      "Epoch 00031: val_loss did not improve from 0.15410\n",
      "36805/36805 [==============================] - 131s 4ms/step - loss: 0.0407 - acc: 0.9883 - val_loss: 0.1837 - val_acc: 0.9518\n",
      "Epoch 32/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0418 - acc: 0.9873\n",
      "Epoch 00032: val_loss did not improve from 0.15410\n",
      "36805/36805 [==============================] - 130s 4ms/step - loss: 0.0418 - acc: 0.9873 - val_loss: 0.2024 - val_acc: 0.9499\n",
      "Epoch 33/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0410 - acc: 0.9882\n",
      "Epoch 00033: val_loss did not improve from 0.15410\n",
      "36805/36805 [==============================] - 130s 4ms/step - loss: 0.0411 - acc: 0.9881 - val_loss: 0.1738 - val_acc: 0.9534\n",
      "Epoch 34/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0518 - acc: 0.9840\n",
      "Epoch 00034: val_loss did not improve from 0.15410\n",
      "36805/36805 [==============================] - 129s 4ms/step - loss: 0.0518 - acc: 0.9841 - val_loss: 0.1607 - val_acc: 0.9569\n",
      "Epoch 35/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0344 - acc: 0.9901\n",
      "Epoch 00035: val_loss did not improve from 0.15410\n",
      "36805/36805 [==============================] - 129s 4ms/step - loss: 0.0346 - acc: 0.9901 - val_loss: 0.1633 - val_acc: 0.9539\n",
      "Epoch 36/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0413 - acc: 0.9879\n",
      "Epoch 00036: val_loss did not improve from 0.15410\n",
      "36805/36805 [==============================] - 127s 3ms/step - loss: 0.0413 - acc: 0.9879 - val_loss: 0.1721 - val_acc: 0.9543\n",
      "Epoch 37/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0344 - acc: 0.9898\n",
      "Epoch 00037: val_loss did not improve from 0.15410\n",
      "36805/36805 [==============================] - 127s 3ms/step - loss: 0.0347 - acc: 0.9897 - val_loss: 0.1828 - val_acc: 0.9513\n",
      "Epoch 38/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0507 - acc: 0.9838\n",
      "Epoch 00038: val_loss did not improve from 0.15410\n",
      "36805/36805 [==============================] - 128s 3ms/step - loss: 0.0507 - acc: 0.9838 - val_loss: 0.1701 - val_acc: 0.9553\n",
      "Epoch 39/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0286 - acc: 0.9917\n",
      "Epoch 00039: val_loss did not improve from 0.15410\n",
      "36805/36805 [==============================] - 131s 4ms/step - loss: 0.0286 - acc: 0.9917 - val_loss: 0.1797 - val_acc: 0.9550\n",
      "Epoch 40/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0285 - acc: 0.9912\n",
      "Epoch 00040: val_loss did not improve from 0.15410\n",
      "36805/36805 [==============================] - 129s 4ms/step - loss: 0.0287 - acc: 0.9912 - val_loss: 0.1911 - val_acc: 0.9511\n",
      "Epoch 41/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0350 - acc: 0.9892\n",
      "Epoch 00041: val_loss did not improve from 0.15410\n",
      "36805/36805 [==============================] - 127s 3ms/step - loss: 0.0350 - acc: 0.9892 - val_loss: 0.2316 - val_acc: 0.9406\n",
      "Epoch 42/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0231 - acc: 0.9935\n",
      "Epoch 00042: val_loss did not improve from 0.15410\n",
      "36805/36805 [==============================] - 129s 3ms/step - loss: 0.0232 - acc: 0.9935 - val_loss: 0.2043 - val_acc: 0.9497\n",
      "Epoch 43/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0363 - acc: 0.9891\n",
      "Epoch 00043: val_loss did not improve from 0.15410\n",
      "36805/36805 [==============================] - 128s 3ms/step - loss: 0.0362 - acc: 0.9891 - val_loss: 0.1968 - val_acc: 0.9497\n",
      "Epoch 44/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0226 - acc: 0.9937\n",
      "Epoch 00044: val_loss did not improve from 0.15410\n",
      "36805/36805 [==============================] - 128s 3ms/step - loss: 0.0226 - acc: 0.9937 - val_loss: 0.2110 - val_acc: 0.9483\n",
      "Epoch 45/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0244 - acc: 0.9929\n",
      "Epoch 00045: val_loss did not improve from 0.15410\n",
      "36805/36805 [==============================] - 128s 3ms/step - loss: 0.0244 - acc: 0.9929 - val_loss: 0.2054 - val_acc: 0.9504\n",
      "Epoch 46/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0216 - acc: 0.9939\n",
      "Epoch 00046: val_loss did not improve from 0.15410\n",
      "36805/36805 [==============================] - 128s 3ms/step - loss: 0.0216 - acc: 0.9939 - val_loss: 0.1786 - val_acc: 0.9553\n",
      "Epoch 47/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0268 - acc: 0.9917\n",
      "Epoch 00047: val_loss did not improve from 0.15410\n",
      "36805/36805 [==============================] - 128s 3ms/step - loss: 0.0269 - acc: 0.9917 - val_loss: 0.1636 - val_acc: 0.9562\n",
      "Epoch 48/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0375 - acc: 0.9879\n",
      "Epoch 00048: val_loss did not improve from 0.15410\n",
      "36805/36805 [==============================] - 128s 3ms/step - loss: 0.0375 - acc: 0.9879 - val_loss: 0.1993 - val_acc: 0.9511\n",
      "Epoch 49/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0183 - acc: 0.9949\n",
      "Epoch 00049: val_loss did not improve from 0.15410\n",
      "36805/36805 [==============================] - 127s 3ms/step - loss: 0.0183 - acc: 0.9949 - val_loss: 0.1638 - val_acc: 0.9592\n",
      "Epoch 50/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0208 - acc: 0.9938\n",
      "Epoch 00050: val_loss did not improve from 0.15410\n",
      "36805/36805 [==============================] - 127s 3ms/step - loss: 0.0208 - acc: 0.9938 - val_loss: 0.1693 - val_acc: 0.9590\n",
      "Epoch 51/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0166 - acc: 0.9957\n",
      "Epoch 00051: val_loss did not improve from 0.15410\n",
      "36805/36805 [==============================] - 127s 3ms/step - loss: 0.0166 - acc: 0.9957 - val_loss: 0.1908 - val_acc: 0.9515\n",
      "Epoch 52/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0256 - acc: 0.9919\n",
      "Epoch 00052: val_loss did not improve from 0.15410\n",
      "36805/36805 [==============================] - 127s 3ms/step - loss: 0.0258 - acc: 0.9919 - val_loss: 0.2000 - val_acc: 0.9481\n",
      "Epoch 53/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0341 - acc: 0.9895\n",
      "Epoch 00053: val_loss did not improve from 0.15410\n",
      "36805/36805 [==============================] - 127s 3ms/step - loss: 0.0341 - acc: 0.9895 - val_loss: 0.1772 - val_acc: 0.9564\n",
      "Epoch 54/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0176 - acc: 0.9949\n",
      "Epoch 00054: val_loss did not improve from 0.15410\n",
      "36805/36805 [==============================] - 127s 3ms/step - loss: 0.0176 - acc: 0.9949 - val_loss: 0.1594 - val_acc: 0.9569\n",
      "Epoch 55/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0136 - acc: 0.9960\n",
      "Epoch 00055: val_loss did not improve from 0.15410\n",
      "36805/36805 [==============================] - 127s 3ms/step - loss: 0.0136 - acc: 0.9960 - val_loss: 0.1869 - val_acc: 0.9564\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0169 - acc: 0.9951\n",
      "Epoch 00056: val_loss did not improve from 0.15410\n",
      "36805/36805 [==============================] - 127s 3ms/step - loss: 0.0169 - acc: 0.9951 - val_loss: 0.1939 - val_acc: 0.9520\n",
      "Epoch 57/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0177 - acc: 0.9950\n",
      "Epoch 00057: val_loss did not improve from 0.15410\n",
      "36805/36805 [==============================] - 128s 3ms/step - loss: 0.0177 - acc: 0.9950 - val_loss: 0.1555 - val_acc: 0.9613\n",
      "Epoch 58/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0184 - acc: 0.9944\n",
      "Epoch 00058: val_loss did not improve from 0.15410\n",
      "36805/36805 [==============================] - 128s 3ms/step - loss: 0.0184 - acc: 0.9944 - val_loss: 0.2322 - val_acc: 0.9495\n",
      "Epoch 59/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0179 - acc: 0.9946\n",
      "Epoch 00059: val_loss did not improve from 0.15410\n",
      "36805/36805 [==============================] - 128s 3ms/step - loss: 0.0179 - acc: 0.9946 - val_loss: 0.1911 - val_acc: 0.9532\n",
      "Epoch 60/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0205 - acc: 0.9937\n",
      "Epoch 00060: val_loss did not improve from 0.15410\n",
      "36805/36805 [==============================] - 128s 3ms/step - loss: 0.0205 - acc: 0.9937 - val_loss: 0.2633 - val_acc: 0.9334\n",
      "Epoch 61/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0174 - acc: 0.9951\n",
      "Epoch 00061: val_loss did not improve from 0.15410\n",
      "36805/36805 [==============================] - 128s 3ms/step - loss: 0.0174 - acc: 0.9951 - val_loss: 0.1610 - val_acc: 0.9606\n",
      "Epoch 62/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0168 - acc: 0.9949\n",
      "Epoch 00062: val_loss did not improve from 0.15410\n",
      "36805/36805 [==============================] - 128s 3ms/step - loss: 0.0168 - acc: 0.9949 - val_loss: 0.2196 - val_acc: 0.9504\n",
      "Epoch 63/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0201 - acc: 0.9945\n",
      "Epoch 00063: val_loss did not improve from 0.15410\n",
      "36805/36805 [==============================] - 128s 3ms/step - loss: 0.0201 - acc: 0.9945 - val_loss: 0.1791 - val_acc: 0.9613\n",
      "Epoch 64/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0124 - acc: 0.9967\n",
      "Epoch 00064: val_loss did not improve from 0.15410\n",
      "36805/36805 [==============================] - 128s 3ms/step - loss: 0.0124 - acc: 0.9967 - val_loss: 0.2043 - val_acc: 0.9539\n",
      "Epoch 65/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0166 - acc: 0.9952\n",
      "Epoch 00065: val_loss did not improve from 0.15410\n",
      "36805/36805 [==============================] - 128s 3ms/step - loss: 0.0166 - acc: 0.9952 - val_loss: 0.1795 - val_acc: 0.9576\n",
      "Epoch 66/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0153 - acc: 0.9954\n",
      "Epoch 00066: val_loss did not improve from 0.15410\n",
      "36805/36805 [==============================] - 128s 3ms/step - loss: 0.0154 - acc: 0.9954 - val_loss: 0.2220 - val_acc: 0.9511\n",
      "Epoch 67/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0314 - acc: 0.9907\n",
      "Epoch 00067: val_loss did not improve from 0.15410\n",
      "36805/36805 [==============================] - 126s 3ms/step - loss: 0.0314 - acc: 0.9907 - val_loss: 0.1727 - val_acc: 0.9588\n",
      "Epoch 68/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0104 - acc: 0.9972\n",
      "Epoch 00068: val_loss did not improve from 0.15410\n",
      "36805/36805 [==============================] - 126s 3ms/step - loss: 0.0104 - acc: 0.9972 - val_loss: 0.1843 - val_acc: 0.9585\n",
      "Epoch 69/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0109 - acc: 0.9969\n",
      "Epoch 00069: val_loss did not improve from 0.15410\n",
      "36805/36805 [==============================] - 126s 3ms/step - loss: 0.0109 - acc: 0.9969 - val_loss: 0.1662 - val_acc: 0.9592\n",
      "Epoch 70/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0214 - acc: 0.9930\n",
      "Epoch 00070: val_loss did not improve from 0.15410\n",
      "36805/36805 [==============================] - 126s 3ms/step - loss: 0.0214 - acc: 0.9930 - val_loss: 0.1745 - val_acc: 0.9569\n",
      "Epoch 71/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0091 - acc: 0.9973\n",
      "Epoch 00071: val_loss did not improve from 0.15410\n",
      "36805/36805 [==============================] - 126s 3ms/step - loss: 0.0091 - acc: 0.9973 - val_loss: 0.1747 - val_acc: 0.9595\n",
      "Epoch 72/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0105 - acc: 0.9972\n",
      "Epoch 00072: val_loss did not improve from 0.15410\n",
      "36805/36805 [==============================] - 126s 3ms/step - loss: 0.0105 - acc: 0.9972 - val_loss: 0.1731 - val_acc: 0.9604\n",
      "Epoch 73/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0122 - acc: 0.9968\n",
      "Epoch 00073: val_loss did not improve from 0.15410\n",
      "36805/36805 [==============================] - 126s 3ms/step - loss: 0.0123 - acc: 0.9968 - val_loss: 0.3018 - val_acc: 0.9355\n",
      "Epoch 74/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0275 - acc: 0.9918\n",
      "Epoch 00074: val_loss did not improve from 0.15410\n",
      "36805/36805 [==============================] - 126s 3ms/step - loss: 0.0275 - acc: 0.9918 - val_loss: 0.1617 - val_acc: 0.9623\n",
      "Epoch 75/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0106 - acc: 0.9971\n",
      "Epoch 00075: val_loss did not improve from 0.15410\n",
      "36805/36805 [==============================] - 126s 3ms/step - loss: 0.0106 - acc: 0.9971 - val_loss: 0.1580 - val_acc: 0.9644\n",
      "Epoch 76/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0092 - acc: 0.9974\n",
      "Epoch 00076: val_loss did not improve from 0.15410\n",
      "36805/36805 [==============================] - 126s 3ms/step - loss: 0.0092 - acc: 0.9974 - val_loss: 0.2194 - val_acc: 0.9529\n",
      "Epoch 77/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0101 - acc: 0.9974\n",
      "Epoch 00077: val_loss did not improve from 0.15410\n",
      "36805/36805 [==============================] - 126s 3ms/step - loss: 0.0101 - acc: 0.9974 - val_loss: 0.1858 - val_acc: 0.9599\n",
      "Epoch 78/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0135 - acc: 0.9961\n",
      "Epoch 00078: val_loss did not improve from 0.15410\n",
      "36805/36805 [==============================] - 126s 3ms/step - loss: 0.0135 - acc: 0.9961 - val_loss: 0.2312 - val_acc: 0.9539\n",
      "Epoch 79/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0120 - acc: 0.9965\n",
      "Epoch 00079: val_loss did not improve from 0.15410\n",
      "36805/36805 [==============================] - 126s 3ms/step - loss: 0.0120 - acc: 0.9965 - val_loss: 0.2372 - val_acc: 0.9515\n",
      "Epoch 80/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0156 - acc: 0.9955\n",
      "Epoch 00080: val_loss did not improve from 0.15410\n",
      "36805/36805 [==============================] - 126s 3ms/step - loss: 0.0156 - acc: 0.9955 - val_loss: 0.2058 - val_acc: 0.9557\n",
      "Epoch 81/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0137 - acc: 0.9960\n",
      "Epoch 00081: val_loss did not improve from 0.15410\n",
      "36805/36805 [==============================] - 126s 3ms/step - loss: 0.0138 - acc: 0.9959 - val_loss: 0.1819 - val_acc: 0.9571\n",
      "Epoch 82/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0166 - acc: 0.9949\n",
      "Epoch 00082: val_loss did not improve from 0.15410\n",
      "36805/36805 [==============================] - 126s 3ms/step - loss: 0.0166 - acc: 0.9949 - val_loss: 0.1875 - val_acc: 0.9562\n",
      "Epoch 83/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0104 - acc: 0.9970\n",
      "Epoch 00083: val_loss did not improve from 0.15410\n",
      "36805/36805 [==============================] - 126s 3ms/step - loss: 0.0105 - acc: 0.9970 - val_loss: 0.1702 - val_acc: 0.9583\n",
      "Epoch 84/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0145 - acc: 0.9956\n",
      "Epoch 00084: val_loss did not improve from 0.15410\n",
      "36805/36805 [==============================] - 126s 3ms/step - loss: 0.0145 - acc: 0.9956 - val_loss: 0.1597 - val_acc: 0.9604\n",
      "Epoch 85/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0104 - acc: 0.9971\n",
      "Epoch 00085: val_loss did not improve from 0.15410\n",
      "36805/36805 [==============================] - 126s 3ms/step - loss: 0.0104 - acc: 0.9971 - val_loss: 0.1817 - val_acc: 0.9620\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0064 - acc: 0.9985\n",
      "Epoch 00086: val_loss did not improve from 0.15410\n",
      "36805/36805 [==============================] - 126s 3ms/step - loss: 0.0064 - acc: 0.9985 - val_loss: 0.1881 - val_acc: 0.9606\n",
      "Epoch 87/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0110 - acc: 0.9969\n",
      "Epoch 00087: val_loss did not improve from 0.15410\n",
      "36805/36805 [==============================] - 126s 3ms/step - loss: 0.0111 - acc: 0.9969 - val_loss: 0.2176 - val_acc: 0.9560\n",
      "Epoch 88/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0226 - acc: 0.9930\n",
      "Epoch 00088: val_loss did not improve from 0.15410\n",
      "36805/36805 [==============================] - 126s 3ms/step - loss: 0.0226 - acc: 0.9930 - val_loss: 0.2021 - val_acc: 0.9562\n",
      "Epoch 89/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0086 - acc: 0.9975\n",
      "Epoch 00089: val_loss did not improve from 0.15410\n",
      "36805/36805 [==============================] - 126s 3ms/step - loss: 0.0087 - acc: 0.9975 - val_loss: 0.1803 - val_acc: 0.9595\n",
      "Epoch 90/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0218 - acc: 0.9934\n",
      "Epoch 00090: val_loss did not improve from 0.15410\n",
      "36805/36805 [==============================] - 126s 3ms/step - loss: 0.0218 - acc: 0.9934 - val_loss: 0.1832 - val_acc: 0.9590\n",
      "Epoch 91/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0058 - acc: 0.9985\n",
      "Epoch 00091: val_loss did not improve from 0.15410\n",
      "36805/36805 [==============================] - 126s 3ms/step - loss: 0.0058 - acc: 0.9985 - val_loss: 0.1937 - val_acc: 0.9583\n",
      "Epoch 92/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0100 - acc: 0.9970\n",
      "Epoch 00092: val_loss did not improve from 0.15410\n",
      "36805/36805 [==============================] - 126s 3ms/step - loss: 0.0100 - acc: 0.9970 - val_loss: 0.2471 - val_acc: 0.9488\n",
      "Epoch 93/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0096 - acc: 0.9974\n",
      "Epoch 00093: val_loss did not improve from 0.15410\n",
      "36805/36805 [==============================] - 126s 3ms/step - loss: 0.0096 - acc: 0.9974 - val_loss: 0.2045 - val_acc: 0.9536\n",
      "Epoch 94/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0124 - acc: 0.9963\n",
      "Epoch 00094: val_loss did not improve from 0.15410\n",
      "36805/36805 [==============================] - 126s 3ms/step - loss: 0.0124 - acc: 0.9963 - val_loss: 0.1562 - val_acc: 0.9632\n",
      "Epoch 95/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0099 - acc: 0.9971\n",
      "Epoch 00095: val_loss did not improve from 0.15410\n",
      "36805/36805 [==============================] - 126s 3ms/step - loss: 0.0099 - acc: 0.9971 - val_loss: 0.1803 - val_acc: 0.9620\n",
      "Epoch 96/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0076 - acc: 0.9978\n",
      "Epoch 00096: val_loss did not improve from 0.15410\n",
      "36805/36805 [==============================] - 126s 3ms/step - loss: 0.0076 - acc: 0.9978 - val_loss: 0.2553 - val_acc: 0.9511\n",
      "Epoch 97/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0107 - acc: 0.9969\n",
      "Epoch 00097: val_loss did not improve from 0.15410\n",
      "36805/36805 [==============================] - 126s 3ms/step - loss: 0.0107 - acc: 0.9969 - val_loss: 0.2554 - val_acc: 0.9436\n",
      "Epoch 98/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0096 - acc: 0.9972\n",
      "Epoch 00098: val_loss did not improve from 0.15410\n",
      "36805/36805 [==============================] - 126s 3ms/step - loss: 0.0096 - acc: 0.9972 - val_loss: 0.1743 - val_acc: 0.9623\n",
      "Epoch 99/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0098 - acc: 0.9968\n",
      "Epoch 00099: val_loss did not improve from 0.15410\n",
      "36805/36805 [==============================] - 126s 3ms/step - loss: 0.0098 - acc: 0.9968 - val_loss: 0.2797 - val_acc: 0.9504\n",
      "Epoch 100/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0073 - acc: 0.9980\n",
      "Epoch 00100: val_loss did not improve from 0.15410\n",
      "36805/36805 [==============================] - 126s 3ms/step - loss: 0.0073 - acc: 0.9980 - val_loss: 0.2126 - val_acc: 0.9583\n",
      "Epoch 101/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0119 - acc: 0.9961\n",
      "Epoch 00101: val_loss did not improve from 0.15410\n",
      "36805/36805 [==============================] - 127s 3ms/step - loss: 0.0119 - acc: 0.9961 - val_loss: 0.2259 - val_acc: 0.9534\n",
      "Epoch 102/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0077 - acc: 0.9976\n",
      "Epoch 00102: val_loss did not improve from 0.15410\n",
      "36805/36805 [==============================] - 130s 4ms/step - loss: 0.0079 - acc: 0.9976 - val_loss: 0.2784 - val_acc: 0.9418\n",
      "Epoch 103/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0245 - acc: 0.9925\n",
      "Epoch 00103: val_loss did not improve from 0.15410\n",
      "36805/36805 [==============================] - 129s 4ms/step - loss: 0.0245 - acc: 0.9925 - val_loss: 0.1719 - val_acc: 0.9609\n",
      "Epoch 104/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0069 - acc: 0.9979\n",
      "Epoch 00104: val_loss did not improve from 0.15410\n",
      "36805/36805 [==============================] - 129s 4ms/step - loss: 0.0069 - acc: 0.9979 - val_loss: 0.1760 - val_acc: 0.9627\n",
      "Epoch 105/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0065 - acc: 0.9979\n",
      "Epoch 00105: val_loss did not improve from 0.15410\n",
      "36805/36805 [==============================] - 127s 3ms/step - loss: 0.0065 - acc: 0.9979 - val_loss: 0.1756 - val_acc: 0.9613\n",
      "Epoch 106/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0061 - acc: 0.9983\n",
      "Epoch 00106: val_loss did not improve from 0.15410\n",
      "36805/36805 [==============================] - 129s 4ms/step - loss: 0.0061 - acc: 0.9983 - val_loss: 0.1884 - val_acc: 0.9630\n",
      "Epoch 107/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0098 - acc: 0.9970\n",
      "Epoch 00107: val_loss did not improve from 0.15410\n",
      "36805/36805 [==============================] - 128s 3ms/step - loss: 0.0098 - acc: 0.9970 - val_loss: 0.3124 - val_acc: 0.9341\n",
      "Epoch 108/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0085 - acc: 0.9976\n",
      "Epoch 00108: val_loss did not improve from 0.15410\n",
      "36805/36805 [==============================] - 129s 3ms/step - loss: 0.0085 - acc: 0.9976 - val_loss: 0.1817 - val_acc: 0.9581\n",
      "Epoch 109/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0095 - acc: 0.9976\n",
      "Epoch 00109: val_loss did not improve from 0.15410\n",
      "36805/36805 [==============================] - 127s 3ms/step - loss: 0.0095 - acc: 0.9976 - val_loss: 0.1781 - val_acc: 0.9595\n",
      "Epoch 110/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0056 - acc: 0.9987\n",
      "Epoch 00110: val_loss did not improve from 0.15410\n",
      "36805/36805 [==============================] - 127s 3ms/step - loss: 0.0056 - acc: 0.9987 - val_loss: 0.2323 - val_acc: 0.9506\n",
      "Epoch 111/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0109 - acc: 0.9968\n",
      "Epoch 00111: val_loss did not improve from 0.15410\n",
      "36805/36805 [==============================] - 127s 3ms/step - loss: 0.0110 - acc: 0.9968 - val_loss: 0.2843 - val_acc: 0.9495\n",
      "Epoch 112/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0192 - acc: 0.9939\n",
      "Epoch 00112: val_loss did not improve from 0.15410\n",
      "36805/36805 [==============================] - 128s 3ms/step - loss: 0.0192 - acc: 0.9939 - val_loss: 0.1727 - val_acc: 0.9639\n",
      "Epoch 113/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0079 - acc: 0.9980\n",
      "Epoch 00113: val_loss did not improve from 0.15410\n",
      "36805/36805 [==============================] - 130s 4ms/step - loss: 0.0079 - acc: 0.9980 - val_loss: 0.2380 - val_acc: 0.9518\n",
      "Epoch 114/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0071 - acc: 0.9980\n",
      "Epoch 00114: val_loss did not improve from 0.15410\n",
      "36805/36805 [==============================] - 128s 3ms/step - loss: 0.0071 - acc: 0.9980 - val_loss: 0.2074 - val_acc: 0.9539\n",
      "Epoch 115/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0044 - acc: 0.9989\n",
      "Epoch 00115: val_loss did not improve from 0.15410\n",
      "36805/36805 [==============================] - 127s 3ms/step - loss: 0.0044 - acc: 0.9988 - val_loss: 0.1945 - val_acc: 0.9599\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 116/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0118 - acc: 0.9963\n",
      "Epoch 00116: val_loss did not improve from 0.15410\n",
      "36805/36805 [==============================] - 128s 3ms/step - loss: 0.0119 - acc: 0.9963 - val_loss: 0.1824 - val_acc: 0.9609\n",
      "Epoch 117/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0187 - acc: 0.9943\n",
      "Epoch 00117: val_loss did not improve from 0.15410\n",
      "36805/36805 [==============================] - 128s 3ms/step - loss: 0.0187 - acc: 0.9943 - val_loss: 0.1993 - val_acc: 0.9564\n",
      "Epoch 118/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0052 - acc: 0.9987\n",
      "Epoch 00118: val_loss did not improve from 0.15410\n",
      "36805/36805 [==============================] - 127s 3ms/step - loss: 0.0052 - acc: 0.9987 - val_loss: 0.2203 - val_acc: 0.9578\n",
      "Epoch 119/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0042 - acc: 0.9988\n",
      "Epoch 00119: val_loss did not improve from 0.15410\n",
      "36805/36805 [==============================] - 127s 3ms/step - loss: 0.0042 - acc: 0.9988 - val_loss: 0.1766 - val_acc: 0.9632\n",
      "Epoch 120/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0064 - acc: 0.9985\n",
      "Epoch 00120: val_loss did not improve from 0.15410\n",
      "36805/36805 [==============================] - 127s 3ms/step - loss: 0.0066 - acc: 0.9985 - val_loss: 0.2261 - val_acc: 0.9564\n",
      "Epoch 121/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0171 - acc: 0.9944\n",
      "Epoch 00121: val_loss did not improve from 0.15410\n",
      "36805/36805 [==============================] - 128s 3ms/step - loss: 0.0171 - acc: 0.9944 - val_loss: 0.1896 - val_acc: 0.9599\n",
      "Epoch 122/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0043 - acc: 0.9989\n",
      "Epoch 00122: val_loss did not improve from 0.15410\n",
      "36805/36805 [==============================] - 130s 4ms/step - loss: 0.0043 - acc: 0.9989 - val_loss: 0.2074 - val_acc: 0.9623\n",
      "Epoch 123/10000\n",
      "36800/36805 [============================>.] - ETA: 0s - loss: 0.0064 - acc: 0.9982\n",
      "Epoch 00123: val_loss did not improve from 0.15410\n",
      "36805/36805 [==============================] - 128s 3ms/step - loss: 0.0064 - acc: 0.9982 - val_loss: 0.1884 - val_acc: 0.9637\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(x_train_abs, y_train_onehot, batch_size=64, epochs=10000, \n",
    "                 validation_data=[x_val_abs, y_val_onehot], shuffle=True, \n",
    "                 callbacks = [checkpointer, early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "ax.set_xlabel('epoch')\n",
    "ax.set_ylabel('loss')\n",
    "ax.legend(loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "png_path = 'visualization/learning_curve/'\n",
    "filename = 'SampleCNN_8_conv'+'.png'\n",
    "os.makedirs(png_path, exist_ok=True)\n",
    "fig.savefig(png_path+filename, transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(6) Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4815/4815 [==============================] - 6s 1ms/step\n",
      "Loss: 0.18324540446851856 Accuracy: 0.9426791277629936\n"
     ]
    }
   ],
   "source": [
    "model_path = 'model/checkpoint/SampleCNN_8_conv_checkpoint/'\n",
    "model_filename = model_path + sorted(os.listdir(model_path))[-1]\n",
    "model = load_model(model_filename)\n",
    "[loss, accuracy] = model.evaluate(x_test_abs, y_test_onehot)\n",
    "print('Loss:', loss, 'Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred = model.predict(x_test)\n",
    "test_f1_score = f1_score(y_test, pred > 0.5)\n",
    "print('F1 Score:', test_f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit -n 5 -r 5 model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "brains_on_beats_model_test",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
